- en: Chapter 9. Hooking Volume Baggage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter introduces data volumes and storage driver concepts, which are
    widely used in Docker to manage persistent or shared data. We''ll be also taking
    a deep dive into various storage drivers supported by Docker, and the basic commands
    associated with them for management. The three main use cases for Docker data
    volumes are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: To keep data persistent after a container is deleted
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To share data between the host and the Docker container
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To share data across Docker containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In order to understand a Docker volume, we need to understand how the Docker
    filesystem works. Docker images are stored as a series of read-only layers. When
    the container is started, the read-only image adds a read-write layer on top.
    If the current file needs to be modified, it is copied from the read-only layer
    to the read-write layer, where changes are applied. The version of the file in
    the read-write layer hides the underlying file but doesn't destroy it. Thus, when
    a Docker container is deleted, relaunching the image will start a fresh container
    with a fresh read-write layer and all the changes are lost. The combination of
    read-write layers on top of the read-only layer is termed the **Union File System**
    (**UFS**). In order to persist the data and be able to share it with the host
    and other containers, Docker has come up with the concept of volumes. Basically,
    volumes are directories that exist outside the UFS and behave as normal directories
    or files on the host filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some important features of Docker volumes are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Volumes can be initialized when the container is created
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data volumes can be reused and shared among other data containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data volumes persist the data even if a container is deleted
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Changes to the data volume are made directly, bypassing the UFS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Data-only containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hosting a mapped volume backed up by shared storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker storage driver performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoiding troubleshooting by understanding Docker volumes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll be looking at four ways to deal with data and Docker
    containers, which will help us to understand and achieve the preceding use cases
    mentioned with Docker volumes.
  prefs: []
  type: TYPE_NORMAL
- en: Default case storing data inside the Docker container
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this case, data is only visible inside the Docker containers and is not
    from the host system. The data is lost if the container is shut down or the Docker
    host dies. This case mostly works with services that are packaged in Docker containers
    and are not dependent on persistent data when they return:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'As seen in the preceding example, the `hello.txt` file only exists inside the
    container and will not be persisted once the container dies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Default case storing data inside the Docker container](graphics/image_09_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Data stored inside Docker Container
  prefs: []
  type: TYPE_NORMAL
- en: Data-only container
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Data can be stored outside the Docker UFS in a data-only container. The data
    will be visible inside the data-only container mount namespace. As the data is
    persisted outside the container, it remains even after the container is deleted.
    If any other container wants to connect to this data-only container, simply use
    the `--volumes-from` option to grab the container and apply it to the current
    container. Let''s try out data volume container:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Data-only container](graphics/image_09_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Using a data-only container
  prefs: []
  type: TYPE_NORMAL
- en: Creating a data-only container
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding command, we created an Ubuntu container and attached `/tmp`.
    It is a data-only container based on the Ubuntu image, and exists in the `/tmp`
    directory. If the new Ubuntu container needs to write some data to the `/tmp`
    directory of our data-only container, this can be achieved with help of `--volumes-from`
    option. Now, anything we write to the `/tmp` directory of the new container will
    be saved in the `/tmp` volume of the Ubuntu data container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Use a data-volume container in container-1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Use a data-volume container in container-2 to get the data shared by container-1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, container-2 gets the data written by container-1 in the `/tmp`
    space. These examples demonstrate the basic usage of data-only containers.
  prefs: []
  type: TYPE_NORMAL
- en: Sharing data between the host and the Docker container
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is a common use case where it is necessary to share files between the host
    and the Docker container. In this scenario, we don't need to create a data-only
    container; we can simply run a container of any Docker image and simply override
    one of its directories with content from the host system directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider an example where we want to access the logs of Docker NGINX from
    the host system. Currently, they are not available outside the host, but this
    can be achieved simply by mapping the `/var/log/nginx` from inside the container
    to a directory on the host system. In this scenario, we will run a copy of the
    NGINX image with a shared volume from the host system, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sharing data between the host and the Docker container](graphics/image_09_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Sharing data between the host and Docker container
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a `serverlogs` directory in the host system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the NGINX container and map `/home/serverlogs` to the `/var/log/nginx`
    directory inside the Docker container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Access `http://localhost:5000` from the host system, post this, logs will be
    generated, and they can be accessed on the host system in `/home/serverlogs` directory,
    which is mapped to `/var/log/nginx` inside the Docker container, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Host mapped volume backed up by shared storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker volume plugins allow us to mount a shared storage backend. The main advantage
    of this is that the user will never suffer data loss in the case of host failure,
    as it is backed by shared storage. In the preceding approaches, if we migrate
    the container, the volumes doesn't get migrated. It can be achieved with the help
    of external Docker volume plugins such **Flocker** and **Convy**, which make the
    volume portable and help to migrate the containers across hosts with volumes easily,
    as well as protecting the data, as it is not dependent on the host file system.
  prefs: []
  type: TYPE_NORMAL
- en: Flocker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Flocker is widely used to run containerized stateful services and applications
    that require persistent storage. Docker provides a very basic view of volume management,
    but Flocker enhances it by providing durability, failover, and high availability
    of the volumes. Flocker can be deployed manually with Docker Swarm and compose,
    or can be set up easily on AWS with the help of the CloudFormation template if
    the backed up storage has to be used in production set ups.
  prefs: []
  type: TYPE_NORMAL
- en: 'Flocker can be deployed easily on AWS with the help of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Log in to your AWS account and create a key pair in Amazon EC2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **CloudFormation** from the home page of AWS.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The Flocker cloud formation stack can be launched with the help of the template
    in the AWS S3 storage using the following link: `https://s3.amazonaws.com/installer.downloads.clusterhq.com/flocker-cluster.cloudformation.json`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select create stack; then select the second option and specify the Amazon S3
    template URL:![Flocker](graphics/image_09_004.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the next screen, specify the **Stack name**, **AmazonAccessKeyID**, and **AmazonSecretAccessKey**
    for the account:![Flocker](graphics/image_09_005.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Provide the key-value pairs to tag this Flocker stack, and provide the **IAM
    Role** for this stack if required:![Flocker](graphics/image_09_006.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the details and launch the Flocker cloud formation stack:![Flocker](graphics/image_09_007.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once, the stack deployment is completed from the outputs tab, get the IP address
    of the client node and control node. SSH into the client node using the key-value
    pair generated during the start of the Flocker stack deployment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: If the Flocker `status` and `ls` commands ran successfully, this means the Docker
    Swarm and Flocker have been successfully set up on the AWS.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Flocker volume can be easily set up and allows you to create a container
    that will persist beyond the lifecycle of the container or container host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: An external storage block will be created and mounted, to our host and the container
    directory will be bounded to it. If the container is deleted or the host crashes,
    the data remains secured. The alternate container can be brought up in the second
    host using the same command, and we will be able to access our shared storage.
    The preceding tutorial was to set up Flocker on the AWS for a production use case,
    but we can also test Flocker locally with the help of Docker Swarm setup. Let
    us consider a use case where you have two Docker Swarm nodes and a Flocker client
    node.
  prefs: []
  type: TYPE_NORMAL
- en: In the Flocker client node
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Create a `docker-compose.yml` file and define the containers `redis` and `clusterhq/flask`.
    Provide the respective configuration Docker image, names, ports, and data volumes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a file named `flocker-deploy.yml`, where we will define both containers
    that will be deployed on the same nodes-`node-1`; leave `node-2` blank as of now
    of the Swarm cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Deploy the containers using the preceding `.yml` files; we simply need to run
    the following command to do so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The cluster configuration has been updated. It may take a short while for the
    changes to take effect, in particular if Docker images need to be pulled.
  prefs: []
  type: TYPE_NORMAL
- en: 'Both containers can be observed running in `node-1`. Once the setup has been
    done, we can access the application on `http://node-1`. It will show the visit
    count of this webpage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Recreate the deployment file in order to move the container to `node-2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we''ll be migrating the container from `node-1` to `node-2`, and we''ll
    see that Flocker will auto handle the volume management. It will plug the existing
    volume to the Redis container when it comes up in `node-2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The cluster configuration has been updated. It may take a short while for the
    changes to take effect, in particular if Docker images need to be pulled.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can SSH into `node-2` and list the running Redis container. Try to access
    the application on `http://node2`; we''ll be able to see that the count is still
    persisted as it were in `node-1` and gets incremented by `1` as the application
    is accessed from `node-2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This example demonstrates how easily we can migrate the container with its data
    volume in a Flocker cluster from one node to another.
  prefs: []
  type: TYPE_NORMAL
- en: Convoy Docker volume plugin
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Convoy is the other Docker volume plugin that is widely used to provide storage
    backend. It is written in Go and the main advantage is that it can be deployed
    in standalone mode. Convoy will run as a Docker volume extension, and will behave
    like an intermediate container. The initial implementation of Convoy utilizes
    Linux devices and provides the following four Docker storage function for volumes:'
  prefs: []
  type: TYPE_NORMAL
- en: Thin provisioned volumes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Restore volumes across hosts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Take snapshots of volumes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Back up the volumes to external object stores such as **Amazon EBS**, **Virtual
    File System** (**VFS**), and **Network File System** (**NFS**):![Convoy Docker
    volume plugin](graphics/image_09_008.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Convoy volume plugin
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we''ll be running a local Convoy device mapper driver
    and showcasing the use of the Convoy volume plugin in between two containers for
    sharing the data:'
  prefs: []
  type: TYPE_NORMAL
- en: Verify the Docker version is above 1.8.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Install the Convoy plugin by locally downloading the plugin tar file and extracting
    it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We can go ahead and use the file backed loop device which acts as a pseudo
    device and makes file accessible as a block device in order to demo the Convoy
    device mapper driver:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the data and metadata device is setup start Convoy plugin daemon:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding Terminal, the Convoy daemon will start running; open the next
    Terminal instance and create a `busybox` Docker container, which uses the Convoy
    volume `test_volume` mounted at `/sample` directory inside the container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a sample file in the mounted directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Start a different container by using the volume driver as Convoy and mount
    the same Convoy volume:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'As we do `ls`, we''ll be able to see the file created in the previous container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Thus, the preceding example shows how Convoy can allow the sharing of volumes
    between containers residing in the same, or a different, host.
  prefs: []
  type: TYPE_NORMAL
- en: 'Basically, the volume driver should be used for persistent data such as WordPress
    MySQL DB:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, we have started the MySQL DB using the Convoy volume
    driver in order to provide persistence in case the host fails. We then linked
    the MySQL database in the WordPress Docker container.
  prefs: []
  type: TYPE_NORMAL
- en: Docker storage driver performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll be looking into the performance aspect and comparison
    of file systems supported by Docker. Pluggable storage driver architecture and
    the flexibility to plug in a volume is the best approach for containerized environments
    and production use cases. Docker supports the aufs, btrfs, devicemapper, vfs,
    zfs, and overlayfs filesystems.
  prefs: []
  type: TYPE_NORMAL
- en: UFS basics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As discussed previously, Docker uses UFS in order to have a read-only, layered
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: Docker uses UFS to combine several such layers into a single image. This section
    will take a deep dive into the basics of UFS and storage drivers supported by
    Docker.
  prefs: []
  type: TYPE_NORMAL
- en: UFS recursively merges several directories into a single virtual view. The fundamental
    desire of UFS is to have a read-only file system and some writable overlay on
    it. This gives the illusion that the file system has read-write access, even though
    it is read-only. UFS uses copy-on-write to support this feature. Also, UFS operates
    on directories instead of drives.
  prefs: []
  type: TYPE_NORMAL
- en: 'The underlying filesystem does not matter. UFS can combine directories from
    different underlying file systems. Combining different underlying filesystems
    is possible because UFS intercepts the operations bound to those file systems.
    The following diagram shows that the UFS lies between the user applications and
    filesystems. Examples of UFS are Union FS, Another Union FS (AUFS), and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '![UFS basics](graphics/B04534_09_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: UFS and underlying file systems for branches
  prefs: []
  type: TYPE_NORMAL
- en: UFS - terminology
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Branches in UFS are filesystems that are merged. Branches can have different
    access permissions, such as read-only, read-write, and so on. UFSs are stackable
    filesystems. The branches can also be assigned preferences, which determine the
    order in which operations will be performed on the filesystems. If a directory
    with the same file name exists in multiple branches, the contents of the directory
    appear to be merged in the UFS, but the operations on the files in those directories
    are redirected to respective filesystems.
  prefs: []
  type: TYPE_NORMAL
- en: UFS allows us to create a writable layer over a read-only file system and create
    new files/directories. It also allows the updating of existing files. The existing
    files are updated by copying the file to the writable layer and then making the
    changes. A file in a read-only file system is kept as it is, but the virtual view
    created by UFS will show an updated file. This phenomenon of copying a file to
    a writable layer to update it is called copy-up.
  prefs: []
  type: TYPE_NORMAL
- en: With copy-up in place, removing files becomes complex. When trying to delete
    a file, we have to delete all the copies from bottom to top. This can result in
    errors on read-only layers, which cannot remove the file. In such situations,
    the file is removed from writable layers, but still exists in the read-only layers
    below.
  prefs: []
  type: TYPE_NORMAL
- en: UFS - issues
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The most obvious problem with UFS is support for underlying filesystems. Since
    UFS wraps the necessary branches and their filesystems, the filesystem support
    has to be added in the UFS source code. The underlying filesystems do not change,
    but UFS has to add support for each one of them.
  prefs: []
  type: TYPE_NORMAL
- en: The whiteouts created after removing files also cause a lot of problems. First
    and foremost is that they pollute the filesystem namespace. This can be reduced
    by adding whiteouts in a single sub-directory, but that needs special handling.
    Also, because of whiteouts, `rmdir` performance degrades. Even if a directory
    seems empty, it might contain a lot of whiteouts, because of which `rmdir` cannot
    remove the directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Copy-up is an excellent feature in UFS, but it also has drawbacks. It reduces
    the performance for the first update, as it has to copy the complete file and
    directory hierarchy to a writable layer. Also, the time of directory copies needs
    to be decided. There are two choices: copy the whole directory hierarchy while
    updating, or do it when the directory is opened. Both techniques have their trade-offs.'
  prefs: []
  type: TYPE_NORMAL
- en: AuFS
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: AuFS is another UFS. AuFS is forked from the UFS file system. This caught the
    eye of developers, and is now way ahead of UFS. In fact, UFS is now following
    some of the design decisions taken while developing AuFS. Like any UFS, AuFS makes
    existing filesystem and overlays a unified view on it.
  prefs: []
  type: TYPE_NORMAL
- en: AuFS supports all the UFS features mentioned in the previous sections. You need
    to install the `aufs-tools` package on Ubuntu to use AuFS commands. More information
    about AuFS and its commands can be found on the AuFS man page.
  prefs: []
  type: TYPE_NORMAL
- en: Device Mapper
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Device Mapper** is a Linux kernel component; it provides a mechanism for
    mapping physical block devices onto virtual block devices. These mapped devices
    can be used as logical volumes. Device Mapper provides a generic way to create
    such mappings.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Device Mapper maintains a table, which defines device mappings. The table specifies
    how to map each range of logical sectors of the device. The table contains lines
    for the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`start`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`length`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mapping`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mapping_parameters`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `start` value for the first line is always zero. For other lines, start
    plus the length of the previous line should be equal to the `start` value of the
    current line. Device Mapper sizes are always specified in 512 byte sectors. There
    are different types of mapping targets, such as linear, striped, mirror, snapshot,
    snapshot-origin, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: How device-mapper is used by Docker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Docker uses the thin provisioning and snapshots features of Device Mapper. These
    features allow many virtual devices to be stored on the same data volume. Two
    separate devices are used for data and metadata. The data device is utilized for
    the pool itself and the metadata device contains information about volumes, snapshots,
    blocks in the storage pool, and mapping between the blocks of each snapshot. So,
    Docker creates a single large block device on which a thin pool is created. It
    then creates a base block device. Every image and container is formed from the
    snapshot of this base device.
  prefs: []
  type: TYPE_NORMAL
- en: BTRFS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**BTRFS** is a Linux filesystem that has the potential to replace the current
    default Linux filesystem, EXT3/EXT4\. BTRFS (also known as **butter FS**) is basically
    a copy-on-write filesystem. **Copy-on-Write** (**CoW**) means it never updates
    the data. Instead, it creates a new copy of that part of the data which is stored
    somewhere else on the disk, keeping the old part as it is. Anyone with decent
    filesystem knowledge will understand that CoW requires more space because it stores
    the old copies of data as well. Also, it has the problem of fragmentation. So,
    how can a CoW filesystem be used as a default Linux filesystem? Wouldn''t that
    reduce the performance? No need to mention the storage space problem. Let''s dive
    into BTRFS to understand why it has become so popular.'
  prefs: []
  type: TYPE_NORMAL
- en: The primary design goal of BTRFS was to develop a generic filesystem that can
    perform well with any use case and workload. Most filesystems perform well for
    a specific filesystem benchmark, and performance is not that great for other scenarios.
    Apart from this, BTRFS also supports snapshots, cloning, and RAID (Level 0, 1,
    5, 6, 10). This is more than anyone has previously bargained for from a filesystem.
    One can understand the design complexity, because Linux filesystems are deployed
    on all kinds of devices, from computers and smart phones to small embedded devices.
    The BTRFS layout is represented with B-trees, more like a forest of B-trees. These
    are copy-on-write-friendly B-trees. As CoW filesystems require a little more disk
    space, in general, BTRFS has a very sophisticated mechanism for space reclamation.
    It has a garbage collector, which makes use of reference counting to reclaim unused
    disk space. For data integrity, BTRFS uses check sums.
  prefs: []
  type: TYPE_NORMAL
- en: 'The storage driver can be selected by passing the `--storage-driver` option
    to the `dockerd` command line, or setting the `DOCKER_OPTS` option in the `/etc/default/docker`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We have considered the preceding three widely used filesystems with Docker
    in order to do performance analysis for the following Docker commands using micro
    benchmark tools; `fio` is the tool used to analyze the details of the filesystem,
    such as random write:'
  prefs: []
  type: TYPE_NORMAL
- en: '`commit`: This is used to create a Docker image out of a running container:![BTRFS](graphics/image_09_010.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chart depicting the time required to commit a large-size container containing
    a single large file
  prefs: []
  type: TYPE_NORMAL
- en: '`build`: This is used to build an image from using a Dockerfile which contains
    a set of steps to be followed to create an image from scratch containing a single
    large file:![BTRFS](graphics/image_09_011.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chart depicting the time required to build the container on different file systems
  prefs: []
  type: TYPE_NORMAL
- en: '`rm`: This is used to remove a stopped container:![BTRFS](graphics/image_09_012.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chart depicting the time required to remove the container holding many thousands
    of files using the rm command
  prefs: []
  type: TYPE_NORMAL
- en: '`rmi`: This is used to remove an image:![BTRFS](graphics/image_09_013.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chart depicting the time required to remove a large size container containing
    a single large file using the rmi command
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding tests, we can clearly see that AuFS and BTRFS perform extremely
    well for Docker commands, but BTRFS containers performing many small writes leads
    to poor use of the BTRFS chunk. This can ultimately lead to out-of-space conditions
    on the Docker host and stop working. Using the BTRFS storage driver closely monitors
    the free space on the BTRFS filesystem. Also, due to the BTRFS journaling technique,
    the sequential writes are affected and can halve performance.
  prefs: []
  type: TYPE_NORMAL
- en: Device Mapper performs badly, as each time the container updates existing data,
    the storage driver performs a CoW operation. The copy is from the image snapshot
    to the container's snapshot and can have a noticeable impact on container performance.
  prefs: []
  type: TYPE_NORMAL
- en: AuFS looks like a good choice for PaaS and other similar use-cases where container
    density plays an important role. AuFS efficiently shares images between running,
    enabling a fast container start time and minimal use of disk space. It also uses
    system page cache very efficiently. OverlayFS is a modern filesystem similar to
    AuFS, but with a simpler design and potentially faster. But currently, OverlayFS
    is not mature enough to be used in a production environment. It may be a successor
    to AuFS in the near future. No single driver is well suited for every use case.
    Users should either select the storage driver as per the use case and considering
    the stability required for the application, or go ahead with the default driver
    installed by the distribution's Docker package. If the host system is RHEL or
    a variation, Device Mapper is the default storage driver. For Ubuntu, AuFS is
    the default driver.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we took a deep dive into data volumes and storage driver concepts
    related to Docker. We discussed troubleshooting the data volumes with the help
    of the four approaches, as well as their pros and cons. The first case of storing
    data inside the Docker container is the most basic case, but doesn't provide the
    flexibility to manage and handle data in a production environment. The second
    and third cases are about storing the data using data-only containers or directly
    on the host. These cases help to provide reliability, but still depend on the
    availability of host. The fourth case, which is about using a third-party volume
    plugin such as Flocker or Convoy, solves all of the preceding issues by storing
    the data in a separate block, and provides the reliability with data, even if
    the container is transferred from one host to another or if the container dies.
    In the final section we discussed Docker storage drivers and the plugin architecture
    provided by Docker to use required filesystems such as AuFS, BTRFS, Device Mapper,
    vfs, zfs and OverlayFS. We looked in depth at AuFS, BTRFS, and Device Mapper,
    which are widely used filesystems. From the various tests we conducted using the
    basic Docker commands, AuFS and BTRFS provide a better performance than Device
    Mapper. Users should select a Docker storage driver as per their application use
    case and Docker daemon host system.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll discuss Docker deployment in a public cloud, AWS
    and Azure, and troubleshooting issues.
  prefs: []
  type: TYPE_NORMAL
