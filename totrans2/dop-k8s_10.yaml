- en: Kubernetes on GCP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Google Cloud Platform** (**GCP**) is getting popular in the public cloud
    industry that is provided by Google. GCP has similar concepts as AWS such as VPC,
    Compute Engine, Persistent Disk, Load Balancing, and several managed services.
    In this chapter, you will learn about GCP and how to set up Kubernetes on GCP
    through the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding GCP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using and understanding GCP components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using **Google Container Engine** (**GKE**), the hosted Kubernetes service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to GCP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GCP was officially launched in 2011\. But not like AWS; at the beginning, GCP
    provided **PaaS** (**Platform as a Service**) first. So you can deploy your application
    directly, instead of launching VM. After that, keep enhance functionality that
    supports a variety of services.
  prefs: []
  type: TYPE_NORMAL
- en: The most important service for Kubernetes users is GKE, which is a hosted Kubernetes
    service. So you can get some relief from Kubernetes installation, upgrade, and
    management. It has a pay–as–you–go style approach to use the Kubernetes cluster.
    GKE is also a very active service that keeps providing new versions of Kubernetes
    in a timely manner, and also keeps coming up with new features and management
    tools for Kubernetes as well.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at what kind of foundation and services are provided by GCP
    and then explore GKE.
  prefs: []
  type: TYPE_NORMAL
- en: GCP components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GCP provides a web console and **command-line interface** (**CLI**). Both are
    easy and straightforward to control GCP infrastructure, but Google accounts (such
    as Gmail) are required. Once you have a Google account, go to the GCP sign up
    page ([https://cloud.google.com/free/](https://cloud.google.com/free/)) to set
    up your GCP account creation.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to control via CLI, you need to install Cloud SDK ([https://cloud.google.com/sdk/gcloud/](https://cloud.google.com/sdk/gcloud/)),
    which is similar to AWS CLI that you can use to list, create, update, and delete
    GCP resources. After installing Cloud SDK, you need to configure it with the following
    command to associate it to a GCP account:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: VPC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: VPC in GCP is quite a different policy compared with AWS. First of all, you
    don't need to set CIDR prefix to VPC, in other words, you cannot set CIDR to VPC.
    Instead, you just add one or some subnets to the VPC. Because subnet is always
    coming with certain CIDR blocks, therefore, GCP VPC is identified as a logical
    group of subnets, and subnets within VPC can communicate with each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that GCP VPC has two modes, either **auto** or **custom**. If you choose
    auto, it will create some subnets on each region with predefined CIDR blocks.
    For example, if you type the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'It will create 11 subnets as shown in the following screenshot (because, as
    of August, 2017, GCP has 11 regions):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00130.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Auto mode VPC is probably good to start with. However, in auto mode, you can't
    specify CIDR prefix and 11 subnets from all regions might not fit with your use
    case. For example, if you want to integrate to your on–premise data center via
    VPN, or want to create subnets from a particular region only.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, choose custom mode VPC, then you can create subnets with desired
    CIDR prefix manually. Type the following command to create custom mode VPC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Because custom mode VPC won''t create any subnets as shown in the following
    screenshot, let''s add subnets onto this custom mode VPC:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00131.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Subnets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Subnet in GCP, its always across multiple zones (availability zone) within region.
    In other words, you can't create subnets on a single zone like AWS. You always
    need to specify entire regions when creating a subnet.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, there are no significant concepts of public and private subnets
    such as AWS (combination of route and internet gateway or NAT gateway to determine
    as a public or private subnet). This is because all subnets in GCP have a route
    to internet gateway.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of subnet level access control, GCP uses host (instance) level access
    control using **network tags** to ensure the network security. It will be described
    in more detail in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: It might make network administrators nervous, however, GCP best practice brings
    you much more simplified and scalable VPC administration, because you can add
    subnets anytime to expand entire network blocks.
  prefs: []
  type: TYPE_NORMAL
- en: Technically, you can launch VM instance to set up as a NAT gateway or HTTP proxy,
    and then create a custom priority route for the private subnet that points to
    the NAT/proxy instance to achieve an AWS–like private subnet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please refer to the following online document for details:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://cloud.google.com/compute/docs/vpc/special-configurations](https://cloud.google.com/compute/docs/vpc/special-configurations)'
  prefs: []
  type: TYPE_NORMAL
- en: 'One more thing, an interesting and unique concept of GCP VPC is that you can
    add different CIDR prefix network blocks to the single VPC. For example, if you
    have custom mode VPC then add the following three subnets:'
  prefs: []
  type: TYPE_NORMAL
- en: '`subnet-a` (`10.0.1.0/24`) from `us-west1`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`subnet-b` (`172.16.1.0/24`) from `us-east1`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`subnet-c` (`192.168.1.0/24`) from `asia-northeast1`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following commands will create three subnets from three different regions
    with different CIDR prefix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The result will be the following web console. If you are familiar with AWS VPC,
    you won't believe these combinations of CIDR prefixes within a single VPC! This
    means that, whenever you need to expand a network, you can feel free to assign
    another CIDR prefix to add to the VPC.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00132.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Firewall rules
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned previously, GCP firewall rule is important to achieve network security.
    But GCP firewall is more simple and flexible than AWS **security group** (**SG**).
    For example, in AWS, when you launch an EC2 instance, you have to assign at least
    one SG that is tight coupling with EC2 and SG. On the other hand, in GCP, you
    can't assign any firewall rules directly. Instead, firewall rule and VM instance
    are loosely coupled via **network tag**. Therefore, there is no direct association
    between firewall rule and VM instance. The following diagram is a comparison between
    AWS security group and GCP firewall rule. EC2 requires security group, on the
    other hand, GCP VM instance just sets a tag. This is regardless of whether the
    corresponding firewall has the same tag or not.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00133.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'For example, create a firewall rule for public host (use network tag `public`)
    and private host (use network tag `private`) as given in the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'It creates four firewall rules as shown in the following screenshot. Let''s
    create VM instances to use either the `public` or `private` network tag to see
    how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00134.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: VM instance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: VM instance in GCP is quite similar to AWS EC2\. You can choose from a variety
    of machine (instance) types that have different hardware configurations. As well
    as OS images that are Linux or Windows–based OS or your customized OS, you can
    choose.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned when talking about firewall rules, you can specify zero or more
    network tags. A tag is not necessary to be created beforehand. This means you
    can launch VM instances with network tags first, even though a firewall rule is
    not created. It is still valid, but no firewall rule is applied in this case.
    Then create a firewall rule to have a network tag. Eventually a firewall rule
    will be applied to the VM instances afterwards. This is why VM instances and firewall
    rules are loosely coupled, which provides flexibility to the user.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00135.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Before launching a VM instance, you need to create a ssh public key first,
    the same as AWS EC2\. The easiest way to do this is to run the following command
    to create and register a new key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now let's get started to launch a VM instance on GCP.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deploy two instances on both `subnet-a` and `subnet-b` as public instances
    (use the `public` network tag) and then launch another instance on the `subnet-a`
    as private instance (with a `private` network tag):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../images/00136.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'You can log in to those machines to check whether a firewall rule works as
    expected. First of all, you need to add a ssh key to the ssh-agent on your machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Then check whether an ICMP firewall rule can reject from external, because
    ICMP allows only public or private tagged hosts, so it must not allow ping from
    your machine as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00137.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: On the other hand, the public host allows ssh from your machine, because public-ssh
    rule allows any (`0.0.0.0/0`).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00138.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Of course, this host can ping and ssh to private hosts on `subnet-a` (`10.0.1.2`)
    through a private IP address, because of the `internal-icmp` rule and `private-ssh`
    rule.
  prefs: []
  type: TYPE_NORMAL
- en: Let's ssh to a private host and then install `tomcat8` and `tomcat8-examples`
    package (it will install the `/examples/` application to Tomcat).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00139.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Remember that `subnet-a` is `10.0.1.0/24` CIDR prefix, but `subnet-b` is `172.16.1.0/24`
    CIDR prefix. But within the same VPC, there is connectivity with each other. This
    is a great benefit and advantage of using GCP whereby you can expand a network
    address block whenever you need.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, install nginx to public hosts (`public-on-subnet-a` and `public-on-subnet-b`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'However, at this moment, you can''t access Tomcat on a private host. Even if
    it has a public IP address. This is because a private host doesn''t have any firewall
    rule that allows 8080/tcp yet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Moving forward, not to just creating a firewall rule for Tomcat but will also
    be setting up a LoadBalancer to configure both nginx and Tomcat access from a
    single LoadBalancer.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'GCP provides several types of load balancers as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Layer 4 TCP LoadBalancer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Layer 4 UDP LoadBalancer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Layer 7 HTTP(S) LoadBalancer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Layer 4, both TCP and UDP, LoadBalancers are similar to AWS Classic ELB. On
    the other hand, Layer 7 HTTP(S) LoadBalancer has content (context) based routing.
    For example, URL /img will forward to instance-a, everything else will forward
    to instance-b. So, it is more like an application level LoadBalancer.
  prefs: []
  type: TYPE_NORMAL
- en: AWS also provides **Application Load Balancer** (**ALB** or **ELBv2**), which
    is quite similar to GCP Layer 7 HTTP(S) LoadBalancer. For details, please visit
    [https://aws.amazon.com/blogs/aws/new-aws-application-load-balancer/](https://aws.amazon.com/blogs/aws/new-aws-application-load-balancer/).
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to set up LoadBalancer, unlike AWS ELB, there are several steps needed
    to configure some items beforehand:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Configuration item** | **Purpose** |'
  prefs: []
  type: TYPE_TB
- en: '| Instance group | Determine group of VM instances or VM template (OS image).
    |'
  prefs: []
  type: TYPE_TB
- en: '| Health check | Set health threshold (interval, timeout, and so on) to determine
    instance group health status. |'
  prefs: []
  type: TYPE_TB
- en: '| Backend service | Set load threshold (maximum CPU or request per second)
    and session affinity (sticky session) to the instance group and also associate
    to health check. |'
  prefs: []
  type: TYPE_TB
- en: '| url-maps (LoadBalancer) | This is an actual place holder to represent an
    L7 LoadBalancer that associates backend services and target HTTP(S) proxy |'
  prefs: []
  type: TYPE_TB
- en: '| Target HTTP(S) proxy | This is a connector that makes relationships between
    frontend forwarding rules to LoadBalancer |'
  prefs: []
  type: TYPE_TB
- en: '| Frontend forwarding rule | Associate IP address (ephemeral or static), port
    number to the target HTTP proxy |'
  prefs: []
  type: TYPE_TB
- en: '| External IP (static) | (Optional) Allocate static external IP address for
    LoadBalancer |'
  prefs: []
  type: TYPE_TB
- en: 'The following diagram is for all the preceding components'' association that
    constructs L7 LoadBalancer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00140.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Let's set up an instance group first. In this example, there are three instance
    groups to create. One for private host Tomcat instance (8080/tcp) and another
    two instance groups for public HTTP instances per zones.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do that, execute the following command to group three of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Health check
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s set standard settings by executing the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Backend service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First of all, we need to create a backend service that specifies health check.
    And then add each instance group with threshold with CPU utilization that utilizes
    up to 80% and max capacity as 100% for both HTTP and Tomcat:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Creating a LoadBalancer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The LoadBalancer needs to bind both `my-http-backend-service` and `my-tomcat-backend-service`.
    In this scenario, only `/examples` and `/examples/*` will be the forwarded traffic
    to `my-tomcat-backend-service`. Other than that, every URI forwards traffic to
    `my-http-backend-service`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: If you don't specify an `--address` option, it will create and assign an ephemeral
    external IP address.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, LoadBalancer has been created. However, one missing configuration is
    remaining. Private hosts don't have any firewall rules to allow Tomcat traffic
    (8080/tcp). This is why when you see LoadBalancer status, healthy status of `my-tomcat-backend-service`
    is kept down (0).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00141.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In this case, you need to add one more firewall rule that allows connection
    from LoadBalancer to a private subnet (use the `private` network tag). According
    to GCP documentation ([https://cloud.google.com/compute/docs/load-balancing/health-checks#https_ssl_proxy_tcp_proxy_and_internal_load_balancing](https://cloud.google.com/compute/docs/load-balancing/health-checks#https_ssl_proxy_tcp_proxy_and_internal_load_balancing)),
    health check heart beat will come from address range `130.211.0.0/22` and `35.191.0.0/16`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'After a few minutes, `my-tomcat-backend-service` healthy status will be up
    (`1`); now you can access LoadBalancer from a web browser. When access to `/`
    it should route to `my-http-backend-service`, which has nginx application on public
    hosts:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00142.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'On the other hand, if you access `/examples/` URL with the same LoadBalancer
    IP address, it will route to `my-tomcat-backend-service`, which is a Tomcat application
    on a private host, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00143.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Overall, there are some steps needed to be performed to set up LoadBalancer,
    but it is useful to integrate different HTTP applications onto a single LoadBalancer
    to deliver your service efficiently with minimum resources.
  prefs: []
  type: TYPE_NORMAL
- en: Persistent Disk
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GCE also has a storage service called **Persistent Disk** (**PD**) that is quite
    similar to AWS EBS. You can allocate desired size and types (either standard or
    SSD) on each zone and attach/detach to VM instances anytime.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create one PD and then attach to the VM instance. Note that when attaching
    PD to the VM instance, both must be sat in the same zones. This limitation is
    the same as AWS EBS. So before creating PD, check the VM instance location once
    again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s choose `us-west1-a` and then attach it to `public-on-subnet-a`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: You may see PD has been attached at `/dev/sdb`. Similar to AWS EBS, you have
    to format this disk. Because this is a Linux OS operation, the steps are exactly
    the same as described in [Chapter 9](part0226.html#6NGV40-6c8359cae3d4492eb9973d94ec3e4f1e),
    *Kubernetes on AWS*.
  prefs: []
  type: TYPE_NORMAL
- en: Google Container Engine (GKE)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overall, there are some GCP components that have been introduced in previous
    sections. Now you can start to set up Kubernetes on GCP VM instances using those
    components. You can even use kops that was also introduced in [Chapter 9](part0226.html#6NGV40-6c8359cae3d4492eb9973d94ec3e4f1e),
    *Kubernetes on AWS* too.
  prefs: []
  type: TYPE_NORMAL
- en: However, GCP has a managed Kubernetes service called GKE. Underneath, it uses
    some GCP components such as VPC, VM instances, PD, firewall rules, and LoadBalancers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, as usual, you can use the `kubectl` command to control your Kubernetes
    cluster on GKE, which is included Cloud SDK. If you don''t install the `kubectl`
    command on your machine yet, type the following command to install `kubectl` via
    Cloud SDK:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Setting up your first Kubernetes cluster on GKE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can set up a Kubernetes cluster on GKE using the `gcloud` command. It needs
    to specify several parameters to determine some configurations. One of the important
    parameters is network. You have to specify which VPC and subnet you will deploy.
    Although GKE supports multiple zones to deploy, you need to specify at least one
    zone for Kubernetes master node. This time, it uses the following parameters to
    launch a GKE cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Parameter** | **Description** | **Value** |'
  prefs: []
  type: TYPE_TB
- en: '| `--cluster-version` | Specify Kubernetes version | `1.6.7` |'
  prefs: []
  type: TYPE_TB
- en: '| `--machine-type` | VM instance type for Kubernetes Node | `f1-micro` |'
  prefs: []
  type: TYPE_TB
- en: '| `--num-nodes` | Initial number size of Kubernetes nodes | `3` |'
  prefs: []
  type: TYPE_TB
- en: '| `--network` | Specify GCP VPC | `my-custom-network` |'
  prefs: []
  type: TYPE_TB
- en: '| `--subnetwork` | Specify GCP Subnet if VPC is custom mode | `subnet-c` |'
  prefs: []
  type: TYPE_TB
- en: '| `--zone` | Specify single zone | `asia-northeast1-a` |'
  prefs: []
  type: TYPE_TB
- en: '| `--tags` | Network tags that will be assigned to Kubernetes nodes | `private`
    |'
  prefs: []
  type: TYPE_TB
- en: 'In this scenario, you need to type the following command to launch a Kubernetes
    cluster on GCP. It may take a few minutes to complete because, behind the scenes,
    it will launch several VM instances and set up Kubernetes master and nodes. Note
    that Kubernetes master and etcd will be fully managed by GCP. This means master
    node and etcd don''t consume your VM instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Note that we specify the `--tags private` option, so Kubernetes node VM instance
    has a network tag as `private`. Therefore, it behaves the same as other regular
    VM instances that have `private` tags. Therefore you can't ssh from public Internet
    and you can't HTTP from internet either. But you can ping and ssh from another
    VM instance which has a `public` network tag.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once all nodes are ready, let''s access Kubernetes UI, which is installed by
    default. To do that, use the `kubectl proxy` command to connect to your machine
    as a proxy. Then access the UI via proxy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![](../images/00144.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Node pool
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When launching the Kubernetes cluster, you can specify the number of nodes using
    the `--num-nodes` option. GKE manages a Kubernetes node as node pool. Which means
    you can manage one or more node pools that attach to your Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'What if you need to add more nodes or delete some nodes? GKE provides a functionality
    to resize the node pool by following the command to change Kubernetes node from
    3 to 5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Increasing the number of nodes will help if you need to scale out your node
    capacity. However, in this scenario, it still uses the smallest instance type
    (`f1-micro`, which has only 0.6 GB memory). It might not help if a single container
    needs more than 0.6 GB memory. In this case you need to scale up, which means
    you need to add a larger size of VM instance type.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, you have to add another set of node pools onto your cluster. Because
    within the same node pool, all VM instances are configured the same. So you can't
    change the instance type in the same node pool.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, add a new node pool that has two new sets of `g1-small` (1.7 GB memory)
    VM instance type to the cluster. Then you can expand Kubernetes nodes with different
    hardware configuration.
  prefs: []
  type: TYPE_NORMAL
- en: By default, there are some quotas that you can create a number limit of VM instances
    within one region (for example, up to eight cpu cores on `us-west1`). If you wish
    to increase this quota, you must change your account to be a paid account. Then
    request quota change to GCP. For more details, please read online documentation
    from [https://cloud.google.com/compute/quotas](https://cloud.google.com/compute/quotas)
    and [https://cloud.google.com/free/docs/frequently-asked-questions#how-to-upgrade](https://cloud.google.com/free/docs/frequently-asked-questions#how-to-upgrade).
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command that adds an additional node pool that has two instances
    of `g1-small` instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Now you have a total of seven CPU cores and 6.4 GB memory in your cluster that
    has more capacity. However, due to larger hardware types, Kubernetes scheduler
    will probably assign to deploy pod to the `large-mem-pool` first, because it has
    enough memory capacity.
  prefs: []
  type: TYPE_NORMAL
- en: However, you may want to preserve `large-mem-pool` node in case a big application
    needs large heap memory size (for example, Java application). Therefore, you may
    want to differentiate `default-pool` and `large-mem-pool`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, Kubernetes label `beta.kubernetes.io/instance-type` helps to
    distinguish instance type of node. Therefore, use `nodeSelector` to specify a
    desired node to the pod. For example, following `nodeSelector` parameter will
    force to use `f1-micro` node for nginx application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: If you want to specify a particular label instead of `beta.kubernetes.io/instance-type`,
    use `--node-labels` option to create a node pool. That assigns your desired label
    for the node pool.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more details, please read the following online document:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://cloud.google.com/sdk/gcloud/reference/container/node-pools/create](https://cloud.google.com/sdk/gcloud/reference/container/node-pools/create).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, you can feel free to remove a node pool if you no longer need it.
    To do that, run the following command to delete `default-pool` (`f1-micro` x 5
    instances). This operation will involve pod migration (terminate pod on `default-pool`
    and re-launch on `large-mem-pool`) automatically, if there are some pods running
    at `default-pool`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: You may have noticed that all of the preceding operations happened in a single
    zone (`asia-northeast1-a`). Therefore, if `asia-northeast1-a` zone gets an outage,
    your cluster will be down. In order to avoid zone failure, you may consider setting
    up a multi zone cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Multi zone cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GKE supports multi zone cluster that allows you to launch Kubernetes nodes on
    multiple zones, but limits within the same region. In previous examples, it has
    been provisioned at `asia-northeast1-a` only, so let's re-provision a cluster
    that has `asia-northeast1-a`, `asia-northeast1-b` and `asia-northeast1-c` in a
    total of three zones.
  prefs: []
  type: TYPE_NORMAL
- en: It is very simple; you just append an `--additional-zones` parameter when creating
    a new cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'As of August, 2017, there is a beta feature that supports to update existing
    clusters from single zones to multi zones. Use a beta command as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`$ gcloud beta container clusters update my-k8s-cluster --additional-zones=asia-northeast1-b,asia-northeast1-c`.'
  prefs: []
  type: TYPE_NORMAL
- en: To change an existing cluster to multi zone, it may need an additional SDK tool
    installation, but out of SLA.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s delete the previous cluster, and create a new cluster with an `--additional-zones`
    option:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, it will create two nodes per zones (`asia-northeast1-a`, `b`
    and `c`); therefore, a total of six nodes will be added:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: You may also distinguish node zone by Kubernetes label `failure-domain.beta.kubernetes.io/zone`
    so that you can specify desired zones to deploy a pod.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster upgrade
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you start to manage Kubernetes, you may encounter some difficulty when
    upgrading Kubernetes clusters. Because the Kubernetes project is very aggressive,
    around every three months, there is a new release, such as 1.6.0 (released on
    March 28^(th) 2017) to 1.7.0 (released on June 29^(th) 2017).
  prefs: []
  type: TYPE_NORMAL
- en: 'GKE also keeps adding new version support in a timely manner. It allows us
    to upgrade both master and nodes via the `gcloud` command. You can run the following
    command to see which Kubernetes version is supported by GKE:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'So, you may see the latest supported version is 1.7.3 on both master and node
    at this moment. Since the previous example installed is version 1.6.7, let''s
    update to 1.7.3\. First of all, you need to upgrade master first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'It takes around 10 minutes depending on environment, after that you can verify
    via the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you can upgrade all nodes to version 1.7.3\. Because GKE tries to perform
    rolling upgrade, it will perform the following steps per node one by one:'
  prefs: []
  type: TYPE_NORMAL
- en: Deregister a target node from the cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Delete old VM instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Provision a new VM instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up the node with the 1.7.3 version.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Register to master.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Therefore, it takes much longer than a master upgrade:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'During rolling upgrade, you can see node status as follows and it shows a mid
    process of rolling update (two nodes have upgraded to 1.7.3, one node is upgrading,
    three nodes are pending):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Kubernetes cloud provider
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GKE also integrates Kubernetes cloud provider out of box that deep integrate
    to GCP infrastructure; for example overlay network by VPC route, StorageClass
    by Persistent Disk, and Service by L4 LoadBalancer. The best part is ingress by
    L7 LoadBalancer. Let's take a look at how it works.
  prefs: []
  type: TYPE_NORMAL
- en: StorageClass
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As per as kops on AWS, GKE also sets up StorageClass by default, which uses
    Persistent Disk:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Therefore, when creating Persistent Volume Claim, it will allocate GCP Persistent
    Disk as Kubernetes Persistent Volume automatically. Regarding Persistent Volume
    Claim and Dynamic Provisioning, please refer to [Chapter 4](part0103.html#3279U0-6c8359cae3d4492eb9973d94ec3e4f1e),
    *Working with Storage and Resources*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: L4 LoadBalancer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to AWS cloud provider, GKE also supports using L4 LoadBalancer for Kubernetes
    Service. Just specify `Service.spec.type` as LoadBalancer, and then GKE will set
    up and configure L4 LoadBalancer automatically.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the corresponding firewall rule between L4 LoadBalancer to Kubernetes
    node can be created by cloud provider automatically. It is simple but powerful
    enough if you want to expose your application to the internet quickly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: L7 LoadBalancer (ingress)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GKE also supports Kubernetes ingress that can set up GCP L7 LoadBalancer to
    dispatch HTTP requests to the target service based on URL. You just need to set
    up one or more NodePort services and then create ingress rules to point to services.
    Behind the scenes, Kubernetes creates and configures firewall rules, health check,
    backend service, forwarding rules, and URL maps automatically.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create same examples that use nginx and Tomcat to deploy to the Kubernetes
    cluster first. These are using Kubernetes Services that bind to NodePort instead
    of LoadBalancer:'
  prefs: []
  type: TYPE_NORMAL
- en: '**![](../images/00145.jpeg)**'
  prefs: []
  type: TYPE_NORMAL
- en: At this moment, you cannot access service, because there are no firewall rules
    that allow access to the Kubernetes node from the internet yet. So, let's create
    Kubernetes ingress that points to these services.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use `kubectl port-forward <pod name> <your machine available port><:
    service port number>` to access via the Kubernetes API server. For the preceding
    case, use `kubectl port-forward tomcat-670632475-l6h8q 10080:8080.`.'
  prefs: []
  type: TYPE_NORMAL
- en: After that, open your web browser to `http://localhost:10080/` and then you
    can access Tomcat pod directly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes ingress definition is quite similar to GCP backend service definition
    as it needs to specify a combination of URL path, Kubernetes service name, and
    service port number. So in this scenario, URL `/` and `/*` point to nginx service,
    also URL `/examples` and `/examples/*` point to the Tomcat service as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'It takes around 10 minutes to fully configure GCP components such as health
    check, forwarding rule, backend services, and url-maps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also check the status on the web console as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00146.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Once you have completed the setup of L7 LoadBalancer, you can access the public
    IP address of LoadBalancer (`http://107.178.253.174/`) to see the nginx page.
    As well as access to `http://107.178.253.174/examples/` then you can see `tomcat
    example` page.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding steps, we created and assigned an ephemeral IP address for
    L7 LoadBalancer. However, the best practice to use L7 LoadBalancer is to assign
    a static IP address instead, because you can also associate DNS (FQDN) to the
    static IP address.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do that, update ingress setting to add an annotation `kubernetes.io/ingress.global-static-ip-name`
    to associate a GCP static IP address name as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: So, now you can access ingress via a static IP address as `http://35.186.227.252/`
    (nginx) and `http://35.186.227.252/examples/` (Tomcat) instead.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed Google Cloud Platform. The basic concept is similar
    to AWS, but some of the policies and concepts are different. Especially Google
    Container Engine, as it is a very powerful service to use Kubernetes as production
    grade. Kubernetes cluster and node management are quite easy, not only the installation,
    but also upgrade. Cloud provider is also fully integrated to GCP, especially Ingress
    as it can configure L7 LoadBalancer with one command. Therefore, it is highly
    recommended to try GKE if you plan to use Kubernetes on the public cloud.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will provide a sneak preview to some new features and alternative
    services to against Kubernetes.
  prefs: []
  type: TYPE_NORMAL
