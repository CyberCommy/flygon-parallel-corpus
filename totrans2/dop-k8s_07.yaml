- en: Continuous Delivery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Topics we''ve discussed so far enable us to run our services in Kubernetes.
    With the monitoring system, we''ve gained more confidence in our service. The
    next thing we''d like to achieve to set our service on course is how to deliver
    our latest features as well as ameliorations to our service continuously in Kubernetes,
    and we''ll learn it with the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Updating Kubernetes resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a delivery pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Techniques to improve the deployment process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Updating resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The property of Continuous Delivery is as what we described in [Chapter 1](part0022.html#KVCC0-6c8359cae3d4492eb9973d94ec3e4f1e),
    *Introduction to DevOps*, a set of operations including the **Continuous Integration**
    (**CI**) and ensuing deployment tasks. The CI flow comprises elements like version
    control systems, buildings, and different levels of automated tests. Tools to
    implement CI functions are usually at the application layer which can be independent
    to underlying infrastructure, but when it comes to achieving deployment, understanding
    and dealing with infrastructure is inevitable since the deployment tasks are tightly
    bound to the platform that our application is running on. In the environment that
    software runs on physical or virtual machines, we'd utilize configuration management
    tools, orchestrators, and scripts to deploy our software. However, if we're running
    our service on an application platform like Heroku, or even in the Serverless
    pattern, designing the deployment pipeline would be a totally different story.
    All in all, the goal of deployment tasks is about making sure our software works
    properly in the right places. In Kubernetes, it's about how to rightly update
    resources, in particular, pods.
  prefs: []
  type: TYPE_NORMAL
- en: Triggering updates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 3](part0070.html#22O7C0-6c8359cae3d4492eb9973d94ec3e4f1e), *Getting
    Started with Kubernetes*, we''ve discussed the rolling update mechanism of pods
    of a Deployment. Let''s recap what''d happen after the update process is triggered:'
  prefs: []
  type: TYPE_NORMAL
- en: The Deployment creates a new `ReplicaSet` with `0` pod according to the updated
    manifest.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The new `ReplicaSet` is scaled up gradually while the previous `ReplicaSet`
    keeps shrinking.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The process ends after all the old pods are replaced.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Such a mechanism is done automatically by Kubernetes, and it exempts us from
    supervising the updating process. To trigger it, all we need to do is inform Kubernetes
    that the pod specification of a Deployment is updated, that is to say, modifying
    the manifest of one resource in Kubernetes. Suppose we have a Deployment `my-app`
    (see `ex-deployment.yml` under the example directory for this section), we can
    modify the manifest with the sub commands of `kubectl` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kubectl patch`: Patches a manifest of an object partially according to the
    input JSON parameter. If we''d like to update the image of `my-app` from `alpine:3.5`
    to `alpine:3.6`, it''d be:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '`kubectl set`: Makes changes to certain properties of an object. This is a
    shortcut to change some properties directly, image of a Deployment is one of the
    properties it supports:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '`kubectl edit`: Opens an editor and dumps the current manifest so that we can
    edit it interactively. The modified one would take effect immediately after being
    saved.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kubectl replace`: Replaces one manifest with another submitted template file.
    If a resource is not created yet or contains properties that can''t be changed,
    it yields errors. For instance, there are two resources in our example template
    `ex-deployment.yml`, namely the Deployment `my-app` and its Service `my-app-svc`.
    Let''s replace them with a new specification file:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: After they are replaced, we'd see the error code would be `1` even though the
    result is expected, that is, updating the Deployment rather than the Service.
    Such behavior should be noticed especially when composing automation scripts for
    the CI/CD flow.
  prefs: []
  type: TYPE_NORMAL
- en: '`kubectl apply`: Applies the manifest file anyway. In other words, if a resource
    exists in Kubernetes, then it''d be updated, otherwise it''d be created. When
    `kubectl apply` is used to create resources, it''s roughly equal to `kubectl create
    --save-config` in functionality. The applied specification file would be saved
    to the annotation field `kubectl.kubernetes.io/last-applied- configuration` accordingly,
    and we can manipulate it with sub commands `edit-last-applied`, `set-last-applied`,
    and `view-last-applied`. For example, we can view the template we''ve submitted
    previously, no matter what the actual content of `ex-deployment.yml` become with:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The saved manifest information would exactly be the same as what we've sent,
    unlike the one we retrieve via `kubectl get -o yaml/json` which contains an object's
    live status, in addition to specifications.
  prefs: []
  type: TYPE_NORMAL
- en: Although in this section we only focus on manipulating a Deployment, but the
    commands here also work for updating all other Kubernetes resources like Service,
    Role, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Changes to `ConfigMap` and secret usually take seconds to propagate to pods.
  prefs: []
  type: TYPE_NORMAL
- en: 'The recommended way to interact with an Kubernetes'' API server is by `kubectl`.
    If you''re under a confined environment, there are also REST APIs for manipulating
    resources of Kubernetes. For example, the `kubectl patch` command we used before
    would become as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Here the variable `$KUBEAPI` is the endpoint of the API server. See API references
    for more information: [https://kubernetes.io/docs/api-reference/v1.7/](https://kubernetes.io/docs/api-reference/v1.7/).'
  prefs: []
  type: TYPE_NORMAL
- en: Managing rollouts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once the rollout process is triggered, Kubernetes would silently complete all
    tasks behind the backdrop. Let''s try some hands-on experiments. Again, the rolling
    update process won''t be triggered even if we''ve modified something with the
    commands mentioned earlier, unless the associated pod''s specification is changed.
    The example we prepared is a simple script that would respond to any request with
    its hostname and the Alpine version it runs on. We first create the Deployment,
    and check its response in another Terminal constantly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we change its image to another version and see what the responses are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Messages from version 3.5 and 3.6 are interleaved until the updating process
    ends. To immediately determine the status of updating processes from Kubernetes
    rather than polling the service endpoint, there''s `kubectl rollout` for managing
    the rolling update process, including inspecting the progress of ongoing updates.
    Let''s see the acting rollout with sub command `status`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'At this moment, the output at Terminal #2 should be all from version 3.6\.
    The sub command `history` allows us to review previous changes of the `deployment`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: However, the `CHANGE-CAUSE` field doesn't show any useful information that helps
    us to know the detail of the revision. To leverage it, add a flag `--record` after
    every command that leads to a change, such as what we've introduced earlier. Certainly,
    `kubectl create` also support the record flag.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s make some change to the Deployment, say, modifying the environment variable
    `DEMO` of pods of `my-app`. As it causes a change in the pod''s specification,
    a rollout would start right away. This sort of behavior allows us to trigger an
    update without building a new image. For simplicity''s sake, we use `patch` to
    modify the variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The `CHANGE-CAUSE` of `REVISION 3` notes the committed command clearly. Nonetheless,
    only the command would be recorded, which means any modification by `edit`/`apply`/`replace`
    wouldn't be marked down explicitly. If we'd like to get the manifest of former
    versions, we could retrieve the saved configuration as long as our changes are
    made with `apply`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For all kinds of reasons, sometimes we want to roll back our application even
    if the rollout is successful to a certain extent. It can be achieved by the sub
    command `undo`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The whole process is basically identical to updating, that is, applying the
    previous manifest, and performing a rolling update. Also, we can utilize the flag
    `--to-revision=<REVISION#>` to rollback to a specific version, but only retained
    revisions are able to be rolled back. Kubernetes determines how many revisions
    it would keep according to the `revisionHistoryLimit` parameter in Deployment
    objects.
  prefs: []
  type: TYPE_NORMAL
- en: The progress of an update is controlled by `kubectl rollout pause` and `kubectl
    rollout resume`. As their names indicate, they should be used in pairs. The pause
    of a Deployment implicates not only stopping of an ongoing rollout, but also freezing
    any rolling updates even if the specification is modified unless it's resumed.
  prefs: []
  type: TYPE_NORMAL
- en: Updating DaemonSet and StatefulSet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes supports various ways to orchestrate pods for different kinds of
    workloads. In addition to Deployments, there are `DaemonSet` and `StatefulSet`
    for long-running, non-batch workloads. As pods they spawned have more constraint
    than Deployments, we should know caveats on handling their updates
  prefs: []
  type: TYPE_NORMAL
- en: DaemonSet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`DaemonSet` is a controller designed for system daemons as its name suggests.
    Consequently, a `DaemonSet` launches and maintains exactly one pod per node, this
    is to say, the total number of pods by a `DaemonSet` is adhered to a number of
    nodes in a cluster. Due to such limitations, updating a `DaemonSet` is not as
    straightforward as updating a Deployment. For instance, Deployment has a `maxSurge`
    parameter (`.spec.strategy.rollingUpdate.maxSurge`) that controls how many redundant
    pods over desired numbers can be created during updates. But we can''t employ
    the same strategy for the pod as a `DaemonSet` usually occupies host''s resources
    like ports. It could result in errors if we have two or more system pods simultaneously
    on a node. As such, the update is in the form that a new pod is created after
    the old pod is terminated on a host.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes implements two update strategies for `DaemonSet`, namely `OnDelete`
    and `rollingUpdate`. An example demonstrates how to write a template of `DaemonSet`
    is at `7-1_updates/ex-daemonset.yml`. The update strategy is set at path `.spec.``updateStrategy``.type`,
    and its default is `OnDelete` in Kubernetes 1.7, and it becomes `rollingUpdate`
    since Kubernetes 1.8:'
  prefs: []
  type: TYPE_NORMAL
- en: '`OnDelete`: Pods are only updated after they are deleted manually.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rollingUpdate`: It actually works like `OnDelete` but the deletion of pods
    is performed by Kubernetes automatically. There is one optional parameter `.spec.updateStrategy.rollingUpdate.maxUnavailable`,
    which is akin to the one in Deployment. Its default value is `1`, which means
    Kubernetes replaces one pod at a time node by node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The trigger of the rolling update process is identical to a Deployment's. Moreover,
    we can also utilize `kubectl rollout` to manage rollouts of our `DaemonSet`. But
    `pause` and `resume` are not supported.
  prefs: []
  type: TYPE_NORMAL
- en: Rolling updates for `DaemonSet` are only available at Kubernetes 1.6 and onward.
  prefs: []
  type: TYPE_NORMAL
- en: StatefulSet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The updating of `StatefulSet` and `DaemonSet` are pretty much the same -- they
    don''t create redundant pods during an update, and their update strategies also
    behave in a similar way. There is also a template file at `7-1_updates/ex-statefulset.yml`
    for practice. The option of update strategy is set at path `.spec.``updateStrategy``.type`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`OnDelete`: Pods are only updated after they are manually deleted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rollingUpdate`: Like every rolling update, Kubernetes deletes and creates
    pods in a controlled fashion. But Kubernetes knows the order matters in `StatefulSet`,
    so it would replace pods in reverse ordinal. Say we have three pods in a `StatefulSet`,
    and they are `my-ss-0`, `my-ss-1`, `my-ss-2` respectively. The update order is
    then starting from `my-ss-2` to `my-ss-0`. The deletion process does not respect
    the pod management policy, that is to say, even if we set the pod management policies
    to `Parallel`, the updating would still be performed one by one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The only parameter for type `rollingUpdate` is partition (`.spec.updateStrategy.rollingUpdate.partition`).
    If it's specified, any pod with its ordinal less than the partition number would
    keep its current version and wouldn't be updated. For instance, if we set it to
    1 in a `StatefulSet` with 3 pods, only pod-1 and pod-2 would be updated after
    a rollout. This parameter allows us to control the progress at certain degrees
    and it's particularly handy for scenarios such as waiting for data synchronization,
    testing the release with a canary, or maybe we just want to stage an update.
  prefs: []
  type: TYPE_NORMAL
- en: Pod management policies and rolling updates are two features implemented in
    Kubernetes 1.7 and later.
  prefs: []
  type: TYPE_NORMAL
- en: Building a delivery pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Implementing a continuous delivery pipeline for containerized applications
    is quite simple. Let''s remember what we have learnt about Docker and Kubernetes
    so far and organize them into the CD pipeline. Suppose we''ve done our code, Dockerfile,
    and corresponding Kubernetes templates. To deploy them to our cluster, we''d go
    through the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '`docker build`: Produces an executable immutable artifact.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`docker run`: Verifies if the build works with some simple test.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`docker tag`: Tags the build with meaningful versions if it''s good.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`docker push`: Moves the build to the artifacts repository for distribution.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`kubectl apply`: Deploys the build to a desired environment.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`kubectl rollout status`: tracks the progress of deployment tasks.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: That's all for a simple but viable delivery pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To make the pipeline ship builds continuously, we need at least three kinds
    of tools, namely version control systems, build servers, and a repository for
    storing container artifacts. In this section, we will set a reference CD pipeline
    based on the SaaS tools we''ve introduced in previous chapters. They are *GitHub*
    ([https://github.com](https://github.com)), *Travis CI* ([https://travis- ci.org](https://travis-ci.org)),
    and *Docker Hub* ([https://hub.docker.com](https://hub.docker.com)), all of them
    are free to open source projects. There are numerous alternatives for each tool
    we used here, like GitLab for VCS, or hosting a Jenkins for CI. The following
    diagram is our CD flow based on the three services earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '>![](../images/00107.jpeg)'
  prefs: []
  type: TYPE_NORMAL
- en: The workflow begins from committing codes into a repository on GitHub, and the
    commit would invoke a build job on Travis CI. Our Docker image is built at this
    stage. Meanwhile, we often run different levels of tests on the CI server to ensure
    that the quality of build is solid. Further, as running an application stack by
    Docker Compose or Kubernetes is easier than ever, we are capable of running tests
    involving many components in a build job. Afterwards, the verified image is tagged
    with identifiers and pushed to the public Docker Registry service, Docker Hub.
  prefs: []
  type: TYPE_NORMAL
- en: No blocks in our pipeline are dedicated to deployment tasks. Instead, we rely
    on Travis CI to deploy our builds. As a matter of fact, the deployment task is
    merely applying Kubernetes templates on certain builds after the image is pushed.
    Finally, the delivery is finished after the rolling update process by Kubernetes
    ends.
  prefs: []
  type: TYPE_NORMAL
- en: Steps explained
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our example, `my-app` is a web service that echoes `OK` constantly, and the
    code as well as the files for deployment are committed in our repository over
    in GitHub here: ([https://github.com/DevOps-with-Kubernetes/my-app](https://github.com/DevOps-with-Kubernetes/my-app)).'
  prefs: []
  type: TYPE_NORMAL
- en: Before configuring our builds on Travis CI, let's create an image repository
    at Docker Hub first for later use. After signing in to Docker Hub, press the huge
    Create Repository at top-right, and then follow the steps on screen to create
    one. Image registry of `my-app` for pushing and pulling is at `devopswithkubernetes/my-app`
    ([https://hub.docker.com/r/devopswithkubernetes/my-app/](https://hub.docker.com/r/devopswithkubernetes/my-app/)).
  prefs: []
  type: TYPE_NORMAL
- en: Connecting Travis CI with a GitHub repository is quite simple, all we need to
    do is authorize Travis CI to access our GitHub repositories, and enable Travis
    CI to build the repository at the profile page ([https://travis-ci.org/profile](https://travis-ci.org/profile)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The definition of a job in Travis CI is configured in a file `.travis.yml`
    placed under the same repository. It''s a YAML format template consisting of blocks
    of shell scripts that tell what Travis CI should do during a build. Explanations
    on blocks of our `.travis.yml` ([https://github.com/DevOps-with-Kubernetes/my-app/blob/master/.travis.yml](https://github.com/DevOps-with-Kubernetes/my-app/blob/master/.travis.yml))
    are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: env
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section defines environment variables that are visible throughout a build:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we set some variables that might be changed like the namespace and the
    docker registry path to where the built image is going. Besides, there''re also
    metadata about a build passed from Travis CI in the form of environment variables,
    and they are documented here: [https://docs.travis-ci.com/user/environment-variables/#Default-Environment-
    Variables](https://docs.travis-ci.com/user/environment-variables/#Default-Environment-Variables).
    For example, `TRAVIS_BUILD_NUMBER` represents the number of the current build,
    and we use it as an identifier to distinguish our images across builds.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The other one source of environment variables is configured manually on Travis
    CI. Because the variables configured there would be hidden publicly, we stored
    some sensitive data such as credentials to Docker Hub and Kubernetes there:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00108.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Every CI tool has own best practices to deal with secrets. For instance, some
    CI tools also allow us to save variables in the CI server, but they would still
    be printed in the building logs, so we're unlikely to save secrets in the CI server
    in such cases.
  prefs: []
  type: TYPE_NORMAL
- en: script
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section is where we run builds and tests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: As we're on Docker, the build is only one line of script. Our test is quite
    simple as well--launching a container with the built image and making some requests
    against it to determine its correctness and integrity. Definitely, we can do everything
    such as adding unit tests, doing the multi-stage build, or running an automated
    integration test to better the resultant artifacts in this stage.
  prefs: []
  type: TYPE_NORMAL
- en: after_success
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This block is executed only if the previous stage ends without any error. Once
    it comes here, we are good to publish our image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Our image tag trivially uses the build number on Travis CI, but using the hash
    of a commit, or version numbers to tag an image is common, too. However, using
    the default tag `latest` is strongly discouraged as it could result in version
    confusion such as running two different images but they have the same name. The
    last conditional block is publishing the image on certain branch tags, and it's
    not actually needed, for we just want to keep building and releasing on a separate
    track. Remember to authenticate to Docker Hub before pushing an image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes decides whether the image should be pulled by the `imagePullPolicy`:
    [https://kubernetes.io/docs/concepts/containers/images/#updating-images](https://kubernetes.io/docs/concepts/containers/images/#updating-images).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Because we set our project deploys to actual machines only on a release, a
    build may stop and be returned at that moment. Let''s see the log of this build:
    [https://travis- ci.org/DevOps-with-Kubernetes/my-app/builds/268053332](https://travis-ci.org/DevOps-with-Kubernetes/my-app/builds/268053332).
    The log retains scripts that Travis CI executed and outputs from every line of
    the script:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00109.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'As we can see, our build is successful, so the image is then published here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://hub.docker.com/r/devopswithkubernetes/my-app/tags/](https://hub.docker.com/r/devopswithkubernetes/my-app/tags/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The build refers to tag `b1`, and we can run it outside the CI server now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: deploy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although we can achieve a fully automated pipeline from end to end, we'd often
    encounter situations to hold up deploying builds due to business reasons. As such,
    we tell Travis CI to run deployment scripts only when we release a new version.
  prefs: []
  type: TYPE_NORMAL
- en: 'To manipulate resources in our Kubernetes cluster from Travis CI, we''ll need
    to grant Travis CI sufficient permissions. Our example uses a service account
    `cd-agent` under RBAC mode to create and update our deployments on behalf of us.
    Later chapters will have more descriptions on RBAC. The templates for creating
    the account and permissions are at: [https://github.com/DevOps-with-Kubernetes/examples/tree/master/chapter7/7-
    2_service-account-for-ci-tool](https://github.com/DevOps-with-Kubernetes/examples/tree/master/chapter7/7-2_service-account-for-ci-tool).
    The account is created under namespace `cd`, and it''s authorized to create and
    modify most kinds of resources across namespaces.'
  prefs: []
  type: TYPE_NORMAL
- en: Here we use a service account that is able to read and modify most resources
    across namespaces, including secrets of the whole cluster. Due to security concerns,
    its always encouraged to restrict permissions of a service account to resources
    the account actually used, or it could be a potential vulnerability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because Travis CI sits outside our cluster, we have to export credentials from
    Kubernetes so that we can configure our CI job to use them. Here we provide a
    simple script to help export those credentials. The script is at: [https://github.com/DevOps-with-
    Kubernetes/examples/blob/master/chapter7/get-sa-token.sh](https://github.com/DevOps-with-Kubernetes/examples/blob/master/chapter7/get-sa-token.sh).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Corresponding variables of exported API endpoint, `ca.crt`, and `sa.token` are
    `CI_ENV_K8S_MASTER`, `CI_ENV_K8S_CA`, and `CI_ENV_K8S_SA_TOKEN` respectively.
    The client certificate (`ca.crt`) is encoded to base64 for portability, and it
    will be decoded at our deployment script.
  prefs: []
  type: TYPE_NORMAL
- en: The deployment script ([https://github.com/DevOps-with-Kubernetes/my- app/blob/master/deployment/deploy.sh](https://github.com/DevOps-with-Kubernetes/my-app/blob/master/deployment/deploy.sh))
    downloads `kubectl` first, and configures `kubectl` with environment variables
    accordingly. Afterwards, the image path of the current build is filled in the
    deployment template, and the templates are applied. Finally, after the rollout
    is finished, our deployment is done.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see the entire flow in action.
  prefs: []
  type: TYPE_NORMAL
- en: 'As soon as we publish a release at GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DevOps-with-Kubernetes/my-app/releases/tag/rel.0.3](https://github.com/DevOps-with-Kubernetes/my-app/releases/tag/rel.0.3)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00110.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Travis CI starts to build our job right after that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00111.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The built image is pushed onto Docker Hub after a while:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00112.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'At this point, Travis CI should start to run deployment tasks, let''s see the
    building log to know the status of our deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://travis-ci.org/DevOps-with-Kubernetes/my-app/builds/268107714](https://travis-ci.org/DevOps-with-Kubernetes/my-app/builds/268107714)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00113.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'As we can see, our application has rolled out successfully, and it should start
    to welcome everyone with `OK`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The pipeline we built and demonstrated in this section is a classical flow to
    deliver codes continuous in Kubernetes. Nonetheless, as the work style and cultures
    vary from team to team, designing a tailor-made continuously delivery pipeline
    for your team rewards efficiency boosts.
  prefs: []
  type: TYPE_NORMAL
- en: Gaining deeper understanding of pods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although the birth and the death are merely a wink during a pod's lifetime,
    they are the most fragile point of a service. Common situations in the real world
    such as routing requests to an unready box, or brutally cutting all in-flight
    connections to a terminating machine, are all what we want to avoid. As a result,
    even Kubernetes takes care of most things for us, and we should know how to configure
    it correctly to gain more confident in deploying.
  prefs: []
  type: TYPE_NORMAL
- en: Starting a pod
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By default, Kubernetes transfers a pod's state to Running as soon as a pod launches.
    If the pod is behind a service, the endpoint controller registers an endpoint
    to Kubernetes immediately. Later on kube-proxy observes the change of endpoints
    and add rules to iptables accordingly. Requests from the outside world now go
    to pods. Kubernetes makes the pod registration lightning fast, so the changes
    are that the request goes to pods prior to an application's readiness, especially
    on bulky software. On the other hand, if a pod fails while running, we should
    have an automatic way to remove it instantly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `minReadySeconds` field of Deployment and other controllers doesn''t postpone
    a pod from becoming ready. Instead, it delays a pod from becoming available, which
    is meaningful during a rollout process: a rollout is successful only when all
    pods are available.'
  prefs: []
  type: TYPE_NORMAL
- en: Liveness and readiness probes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A probe is an indicator to a container''s health. It judges the health through
    periodically performing a diagnostic action against a container via kubelet:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Liveness** **probe**: Indicates whether a container is alive or not. If a
    container fails on this probe, kubelet kills it and may restart it based on the
    `restartPolicy` of a pod.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Readiness probe**: Indicates whether a container is ready for incoming traffic.
    If a pod behind a service is not ready, its endpoint won''t be created until the
    pod is ready.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`retartPolicy` tells how Kubernetes treats a pod on failures or terminations.
    It has three modes: `Always`, `OnFailure`, or `Never`. Default is set to `Always`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Three kinds of action handlers can be configured to perform against a container:'
  prefs: []
  type: TYPE_NORMAL
- en: '`exec`: Executes a defined command inside the container. Considered to be successful
    if the exit code is `0`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tcpSocket`: Tests a given port via TCP, successful if the port is opened.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`httpGet`: Performs an `HTTP GET` to the IP address of target container. Headers
    in the request to be sent is customizable. This check is considered to be healthy
    if the status code satisfies: `400 > CODE >= 200`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Additionally, there are five parameters to define a probe''s behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: '`initialDelaySeconds`: How long kubelet should be waiting for before the first
    probing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`successThreshold`: A container is considered to be healthy when getting consecutive
    times of probing successes passed this threshold.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`failureThreshold`: Same as preceding but defines the negative side.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timeoutSeconds`: The time limitation of a single probe action.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`periodSeconds`: Intervals between probe actions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code snippet demonstrates the usage of a readiness probe, the
    full template is here: [https://github.com/DevOps-with- Kubernetes/examples/blob/master/chapter7/7-3_on_pods/probe.yml](https://github.com/DevOps-with-Kubernetes/examples/blob/master/chapter7/7-3_on_pods/probe.yml)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'How the probe behaves is illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00114.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The upper timeline is a pod''s real readiness, and another line below is its
    readiness from Kubernetes'' view. The first probing executes 10 seconds after
    the pod is created, and the pod is regarded as ready after 2 probing successes.
    A few seconds later, the pod goes out of service due to an unknown reason, and
    it becomes unready after the next three failures. Try to deploy the preceding
    example and observe its output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'In our example file, there is another pod `tester` which is constantly making
    requests to our service, and the log entries `/from-tester` in our service is
    caused by the tester thereof. From tester''s activity logs, we can observe that
    the traffic from the `tester` is stopped after our service becomes unready:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Since we didn't configure the liveness probe in our service, the unhealthy container
    wouldn't be restarted unless we kill it manually. Therefore, in general, we would
    use both probes together so as to make the healing process automated.
  prefs: []
  type: TYPE_NORMAL
- en: Init containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even though `initialDelaySeconds` allows us to block a pod for some time prior
    to receiving traffic, it's still limited. Imagine that if our application is serving
    a file that fetches from somewhere upon initializing, the ready time might differ
    a lot depending on the file size. Hence, the Init containers come in handy here.
  prefs: []
  type: TYPE_NORMAL
- en: Init containers are one or more containers that start prior to application containers
    and run one by one to completion in order. If any container fails, it's subject
    to the `restartPolicy` of a pod and starts over again, till all containers exited
    with code `0`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Defining Init containers is akin to regular containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'They only differ in:'
  prefs: []
  type: TYPE_NORMAL
- en: Init containers don't have readiness probes as they'd run to completion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The port defined in init containers wouldn't be captured by the service in front
    of the pod
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The request/limit of resources are calculated with `max(sum(regular containers),
    max(init containers))`, which means if one of init containers sets a higher resource
    limit than other init containers as well as the sum of resource limit of all regular
    containers, Kubernetes schedules the pod according to the init container's resource
    limit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The usefulness of init containers is more than blocking the application containers.
    For instance, we can utilize them to configure an image by sharing an `emptyDir`
    volume to Init containers and application containers, instead of building another
    image that only runs `awk`/`sed` on the base image, mounts and consume secrets
    in an Init container rather than in application containers.
  prefs: []
  type: TYPE_NORMAL
- en: Terminating a pod
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The sequence of shutdown events is similar to events while starting a pod. After
    receiving a deletion invocation, Kubernetes sends `SIGTERM` to the pod to be deleted,
    and the pod's state becomes Terminating. Meanwhile, Kubernetes removes the endpoint
    of that pod to stop further requests if the pod is backing a service. Occasionally,
    there are pods that aren't quitting at all. It could be the pods don't honor `SIGTERM`,
    or simply because their tasks aren't completed. Under such circumstances, Kubernetes
    would send a `SIGKILL` to forcibly kill those pods after the termination periods.
    The period length is set at `.spec.terminationGracePeriodSeconds` under pod specification.
    Nonetheless, even though Kubernetes has mechanisms to reclaim such pods anyway,
    we still should make sure our pods can be closed properly.
  prefs: []
  type: TYPE_NORMAL
- en: Besides, like in starting a pod, here we also need to take care of a case that
    might affect our service, that is, the process which is serving requests in a
    pod closed prior to the corresponding iptables rules are entirely removed.
  prefs: []
  type: TYPE_NORMAL
- en: Handling SIGTERM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Graceful termination is not a new idea, it's a common practice in programming,
    and especially important for business- critical missions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The implementation principally includes three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Add a handler to capture termination signals.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do everything required in the handler, such as returning resources, releasing
    distribution locks, or closing connections.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Program shutdown. Our previous example demonstrates the idea: closing the controller
    thread on `SIGTERM` in the handler `graceful_exit_handler`. The code can be found
    here ([https://github.com/DevOps-with-Kubernetes/my-app/blob/master/app.py](https://github.com/DevOps-with-Kubernetes/my-app/blob/master/app.py)).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As a matter of fact, common pitfalls that fail a graceful exit are not on the
    program side:'
  prefs: []
  type: TYPE_NORMAL
- en: SIGTERM is not forwarded to the container process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 2](part0047.html#1CQAE0-6c8359cae3d4492eb9973d94ec3e4f1e), *DevOps
    with Container*, we''ve learned that there are two forms to invoke our program
    when writing a Dockerfile, namely the shell form and the exec form, and the shell
    to run the shell form commands is default to `/bin/sh` on Linux containers. Let''s
    see the following example ([https://github.com/DevOps-with- Kubernetes/examples/tree/master/chapter7/7-3_on_pods/graceful_docker](https://github.com/DevOps-with-Kubernetes/examples/tree/master/chapter7/7-3_on_pods/graceful_docker)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We know that the signal sent to a container would be caught by the `PID 1` process
    inside the container, so let's build and run it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Our container is still there. Let''s see what happened inside the container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The `PID 1` process is the shell itself, and it doesn't forward our signal to
    the sub process apparently. In this example, we're using Alpine as the base image
    which uses `ash` as the default shell. If we execute anything with `/bin/sh`,
    it's linked to `ash` actually. Similarly, the default shell in Debian family is
    `dash`, which doesn't forward signals as well. There is still a shell that forwards
    signals, such as `bash`. To leverage `bash`, we can either install an extra shell,
    or switch the base image to distributions that use `bash`. But both of them are
    rather cumbersome.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, there are still options to fix the signal problem without using `bash`.
    One is running our program with `exec` in the shell form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Our process will replace the shell process and thus become the `PID 1` process.
    Another choice and also the recommended one is writing `Dockerfile` in EXEC form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s try the example again with the one in EXEC form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The EXEC form works like a charm. As we can see, the processes in the container
    is what we would anticipate, and our handler now receives `SIGTERM` correctly.
  prefs: []
  type: TYPE_NORMAL
- en: SIGTERM doesn't invoke the termination handler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In some cases, the termination handler of a process is not triggered by `SIGTERM`.
    For instance, sending a `SIGTERM` to nginx actually causes a fast shutdown. To
    gracefully close a nginx controller, we have to send `SIGQUIT` with `nginx -s
    quit` instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full list of supported actions on the signal of nginx is listed here: [http://nginx.org/en/docs/control.html](http://nginx.org/en/docs/control.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Now another problem arises--how do we send signals other than `SIGTERM` to a
    container on deleting a pod? We can modify the behavior of our program to trap
    SIGTERM, but there's nothing we can do about a popular tool like nginx. For such
    a situation, the life cycle hook is capable of solving the problem.
  prefs: []
  type: TYPE_NORMAL
- en: Container lifecycle hooks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Lifecycle hooks are event-aware actions performs against a container. They
    work like a single Kubernetes probing action, but they''ll only be fired at least
    once per event during a container''s lifetime. Right now, there are two events
    supported:'
  prefs: []
  type: TYPE_NORMAL
- en: '`PostStart`: Executes right after a container is created. Since this hook and
    the entry point of a container are fired asynchronously, there is no guarantee
    that the hook would be executed before the container starts. As such, we''re unlikely
    to use it to initialize resources for a container.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PreStop`: executes right before sending `SIGTERM` to a container. One difference
    to `PostStart` hook is that the `PreStop` hook is a synchronous call, in other
    words, `SIGTERM` is only sent after a `PreStop` hook exited.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, our nginx shutdown problem is able to be trivially solved with a `PreStop`
    hook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Additionally, an important property of hooks is they could affect the state
    of a pod in certain ways: a pod won''t be running unless its `PostStart` hook
    exited successfully; a pod is set to terminating immediately on deletion, but
    `SIGTERM` won''t be sent unless the `PreStop` hook exited successfully. Therefore,
    for the case we mentioned earlier, the container quits before its iptables rules
    are removed, we can resolve it by the `PreStop` hook. The following figure illustrates
    how to use the hook to eliminate the unwanted gap:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00115.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The implementation is just adding a hook that sleeps for few seconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Placing pods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of time we don't really care about which node our pods is running on as
    scheduling pods is a fundamental feature of Kubernetes. Nevertheless, Kubernetes
    is not aware of factors such as geographical location of a node, availability
    zones, or machine types when scheduling a pod. Moreover, at times we'd like to
    deploy pods that run testing builds in an isolated instance group. As such, to
    complete the scheduling, Kubernetes provides different levels of affinities that
    allows us to actively assign pods to certain nodes.
  prefs: []
  type: TYPE_NORMAL
- en: The node selector of a pod is the simplest way to place pods manually. It's
    similar to pod selectors of service. A pod would only be put on nodes with matching
    labels. The field is set at `.spec.nodeSelector`. For example, the following snippet
    of a pod `spec` schedules the pod to nodes with label `purpose=sandbox,disk=ssd`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Checking labels on nodes is the same as how we check other resources in Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, there are already labels on our node. Those labels are set by
    default, and the default labels are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kubernetes.io/hostname`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`failure-domain.beta.kubernetes.io/zone`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`failure-domain.beta.kubernetes.io/region`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`beta.kubernetes.io/instance-type`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`beta.kubernetes.io/os`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`beta.kubernetes.io/arch`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If we''d like to label a node to make our example pods scheduled, we can either
    update the manifest of the node or use the shortcut command `kubectl label`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Aside from placing pods to a node, a node is able to reject pods as well, that
    is, *taints and tolerations*, and we will learn it at the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've discussed topics not only on building a continuous delivery
    pipeline, but also on techniques to strengthen our every deployment task. The
    rolling update of pods is a powerful tool that performs updates in a controlled
    fashion. To trigger a rolling update, what we need to do is update the pod's specification.
    Although the update is managed by Kubernetes, we can still control it with `kubectl
    rollout`.
  prefs: []
  type: TYPE_NORMAL
- en: Later on, we fabricated an extensible continuous delivery pipeline by `GitHub/DockerHub/Travis-CI`.
    Next, we moved our steps to learn more about the life of pods to prevent any possible
    failure, including using the readiness and liveness probe to protect a pod, initializing
    a pod with Init containers, handling `SIGTERM` properly by writing `Dockerfile`
    in the exec form, leveraging life cycle hooks to stall a pod's readiness as well
    as its termination for the iptables rules to be removed at the right timing, and
    assigning pods to specific nodes with node selectors.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll learn how to segment our cluster with logical boundaries
    to share resource more stable and secure in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
