- en: Chapter 8. Managing Code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Working on a software project that involves more than one person is tough. Everything
    slows down and gets harder. This happens for several reasons. This chapter will
    expose these reasons and will try to provide some ways to fight against them.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter is divided into two parts, which explain:'
  prefs: []
  type: TYPE_NORMAL
- en: How to work with a version control system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to set up continuous development processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First of all, a code base evolves so much that it is important to track all
    the changes that are made, even more so when many developers work on it. That
    is the role of a **version control system**.
  prefs: []
  type: TYPE_NORMAL
- en: Next, several brains that are not directly wired together can still work on
    the same project. They have different roles and work on different aspects. Therefore,
    a lack of global visibility generates a lot of confusion about what is going on
    and what is being done by others. This is unavoidable, and some tools have to
    be used to provide continuous visibility and mitigate the problem. This is done
    by setting up a series of tools for continuous development processes such as **continuous
    integration** or **continuous delivery**.
  prefs: []
  type: TYPE_NORMAL
- en: Now we will discuss these two aspects in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Version control systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Version control systems** (**VCS**) provide a way to share, synchronize,
    and back up any kind of file. They are categorized into two families:'
  prefs: []
  type: TYPE_NORMAL
- en: Centralized systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Centralized systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A centralized version control system is based on a single server that holds
    the files and lets people check in and check out the changes that are made to
    those files. The principle is quite simple—everyone can get a copy of the files
    on his/her system and work on them. From there, every user can *commit* his/her
    changes to the server. They will be applied and the *revision* number will be
    raised. The other users will then be able to get those changes by synchronizing
    their *repository* copy through an *update*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The repository evolves through all the commits, and the system archives all
    revisions into a database to undo any change or provide information on what has
    been done:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Centralized systems](graphics/5295_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1
  prefs: []
  type: TYPE_NORMAL
- en: 'Every user in this centralized configuration is responsible for synchronizing
    his/her local repository with the main one in order to get the other user''s changes.
    This means that some conflicts can occur when a locally modified file has been
    changed and checked in by someone else. A conflict resolution mechanism is carried
    out, in this case on the user system, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Centralized systems](graphics/5295_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2
  prefs: []
  type: TYPE_NORMAL
- en: 'This will help you understand better:'
  prefs: []
  type: TYPE_NORMAL
- en: Joe checks in a change.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pamela attempts to check in a change on the same file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The server complains that her copy of the file is out of date.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pamela updates her local copy. The version control software may or may not be
    able to merge the two versions seamlessly (that is, without a conflict).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pamela commits a new version that contains the latest changes made by Joe and
    her own.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This process is perfectly fine on small-sized projects that involve a few developers
    and a small number of files. But it becomes problematic for bigger projects. For
    instance, a complex change involves a lot of files, which is time consuming, and
    keeping everything local before the whole work is done is unfeasible. The problems
    of such approach are:'
  prefs: []
  type: TYPE_NORMAL
- en: It is dangerous because the user may keep his/her computer changes that are
    not necessarily backed up
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is hard to share with others until it is checked in and sharing it before
    it is done would leave the repository in an unstable state, and so the other users
    would not want to share
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Centralized VCS has resolved this problem by providing *branches* and *merges*.
    It is possible to fork from the main stream of revisions to work on a separated
    line and then to get back to the main stream.
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 3*, Joe starts a new branch from revision 2 to work on a new feature.
    The revisions are incremented in the main stream and in his branch every time
    a change is checked in. At revision 7, Joe has finished his work and commits his
    changes into the trunk (the main branch). This requires, most of the time, some
    conflict resolution.
  prefs: []
  type: TYPE_NORMAL
- en: 'But in spite of their advantages, centralized VCS has several pitfalls:'
  prefs: []
  type: TYPE_NORMAL
- en: Branching and merging is quite hard to deal with. It can become a nightmare.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since the system is centralized, it is impossible to commit changes offline.
    This can lead to a huge, single commit to the server when the user gets back online.
    Lastly, it doesn't work very well for projects such as Linux, where many companies
    permanently maintain their own branch of the software and there is no central
    repository that everyone has an account on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the latter, some tools are making it possible to work offline, such as SVK,
    but a more fundamental problem is how the centralized VCS works.
  prefs: []
  type: TYPE_NORMAL
- en: '![Centralized systems](graphics/5295_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3
  prefs: []
  type: TYPE_NORMAL
- en: Despite these pitfalls, centralized VCS is still quite popular among many companies
    mainly due to inertia of corporate environments. The main examples of centralized
    VCSes used by many organizations are **Subversion** (**SVN**) and **Concurrent
    Version System** (**CVS**). The obvious issues with centralized architecture for
    version control systems is the reason why most of the open source communities
    have already switched to the more reliable architecture of **Distributed VCS**
    (**DVCS**).
  prefs: []
  type: TYPE_NORMAL
- en: Distributed systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Distributed VCS is the answer to the centralized VCS deficiencies. It does
    not rely on a main server that people work with, but on peer-to-peer principles.
    Everyone can hold and manage his/her own independent repository for a project
    and synchronize it with other repositories:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Distributed systems](graphics/5295_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 4*, we can see an example of such a system in use:'
  prefs: []
  type: TYPE_NORMAL
- en: Bill *pulls* the files from HAL's repository.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bill makes some changes on the files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Amina *pulls* the files from Bill's repository.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Amina changes the files too.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Amina *pushes* the changes to HAL.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kenny *pulls* the files from HAL.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kenny makes changes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kenny regularly *pushes* his changes to HAL.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The key concept is that people *push* and *pull* the files to or from other
    repositories, and this behavior changes according to the way people work and the
    way the project is managed. Since there is no main repository anymore, the maintainer
    of the project needs to define a strategy for people to *push* and *pull* the
    changes.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, people have to be a bit smarter when they work with several repositories.
    In most distributed version control systems, revision numbers are local to each
    repository, and there are no global revision numbers anyone can refer to. Therefore,
    *tags* have to be used to make things clearer. They are textual labels that can
    be attached to a revision. Lastly, users are responsible for backing up their
    own repositories, which is not the case in a centralized infrastructure where
    the administrator usually sets back up strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed strategies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A central server is, of course, still desirable with a DVCS if you're working
    in a company setting with everyone working toward the same goal. But the purpose
    of that server is completely different than in centralized VCS. It is simply a
    hub that allows all developers to share their changes in a single place rather
    than pull and push between each other's repositories. Such a single central repository
    (often called *upstream*) serves also as a backup for all the changes tracked
    in the individual repositories of all team members.
  prefs: []
  type: TYPE_NORMAL
- en: Different approaches can be applied to sharing code with the central repository
    in DVCS. The simplest one is to set up a server that acts like a regular centralized
    server, where every member of the project can push his/her changes into a common
    stream. But this approach is a bit simplistic. It does not take full advantage
    of the distributed system, since people will use push and pull commands in the
    same way as they would with a centralized system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another approach consists of providing several repositories on a server with
    different levels of access:'
  prefs: []
  type: TYPE_NORMAL
- en: An **unstable** **repository** is where everyone can push changes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **stable** **repository** is read-only for all members except the release
    managers. They are allowed to pull changes from the unstable repository and decide
    what should be merged.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Various **release** **repositories** correspond to the releases and are read-only,
    as we will see later in the chapter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This allows people to contribute, and managers to review, the changes before
    they make it to the stable repository. Anyway, depending on the tools used, this
    may be too much of an overhead. In many distributed version control systems, this
    can also be handled with a proper branching strategy.
  prefs: []
  type: TYPE_NORMAL
- en: The other strategies can be made up, since DVCS provides infinite combinations.
    For instance, the Linux Kernel, which is using Git ([http://git-scm.com/](http://git-scm.com/)),
    is based on a star model, where Linus Torvalds is maintaining the official repository
    and pulls the changes from a set of developers he trusts. In this model, people
    who wish to push changes to the kernel will, hopefully, try to push them to the
    trusted developers so that they reach Linus through them.
  prefs: []
  type: TYPE_NORMAL
- en: Centralized or distributed?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just forget about the centralized version control systems.
  prefs: []
  type: TYPE_NORMAL
- en: Let's be honest. Centralized version control systems are relict of the past.
    In a time when most of us have the opportunity to work remotely full-time, it
    is unreasonable to be constrained by all the deficiencies of centralized VCS.
    For instance, with CVS or SVN you can't track the changes when offline. And that's
    silly. What should you do when the Internet connection at your workplace is temporarily
    broken or the central repository goes down? Should you forget about all your workflow
    and just allow changes to pile up until the situation changes and then just commit
    it as a one huge blob of unstructured updates? No!
  prefs: []
  type: TYPE_NORMAL
- en: Also, most of the centralized version control systems do not handle branching
    schemes efficiently. And branching is a very useful technique that allows you
    to limit the number of merge conflicts in the projects where many people work
    on multiple features. Branching in SVN is so ridiculous that most of the developers
    try to avoid it at all costs. Instead, most of the centralized VCS provides some
    file-locking primitives that should be considered the anti-pattern for any version
    control system. The sad truth about every version control tool is that if it contains
    a dangerous option, someone in your team will start using it on a daily basis
    eventually. And locking is one such feature that in return of fewer merge conflicts
    will drastically reduce the productivity of your whole team. By choosing a version
    control system that does not allow for such bad workflows, you are making a situation,
    which makes it more likely that your developers will use it effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Use Git if you can
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Git is currently the most popular distributed version control system. It was
    created by Linus Torvalds for maintaining versions of the Linux kernel when its
    core developers needed to resign from proprietary BitKeeper that was used previously.
  prefs: []
  type: TYPE_NORMAL
- en: If you have not used any of the version control systems then you should start
    with Git from the beginning. If you already use some other tools for version control,
    learn Git anyway. You should definitely do that even if your organization is unwilling
    to switch to Git in the near future, otherwise you risk becoming a living fossil.
  prefs: []
  type: TYPE_NORMAL
- en: I'm not saying that Git is the ultimate and best DVCS version control system.
    It surely has some disadvantages. Most of all, it is not an easy-to-use tool and
    is very challenging for newcomers. Git's steep learning curve is already a source
    of many jokes online. There may be some version control systems that may perform
    better for a lot of projects and the full list of open source Git contenders would
    be quite long. Anyway, Git is currently the most popular DVCS, so the *network
    effect* really works in its favor.
  prefs: []
  type: TYPE_NORMAL
- en: Briefly speaking, the network effect causes that the overall benefit of using
    popular tools is greater than using others, even if slightly better, precisely
    due to its high popularity (this is how VHS killed Betamax). It is very probable
    that people in your organization, as well as new hires, are somewhat proficient
    with Git, so the cost of integrating exactly this DVCS will be lower than trying
    something less popular.
  prefs: []
  type: TYPE_NORMAL
- en: Anyway, it is still always good to know something more and familiarizing yourself
    with other DVCS won't hurt you. The most popular open source rivals of Git are
    Mercurial, Bazaar, and Fossil. The first one is especially neat because it is
    written in Python and was the official version control system for CPython sources.
    There are some signs that it may change in the near future, so CPython developers
    may already use Git by the time you read this book. But it really does not matter.
    Both systems are great. If there would be no Git, or it were less popular, I would
    definitely recommend Mercurial. There is evident beauty in its design. It's definitely
    not as powerful as Git, but a lot easier to master for beginners.
  prefs: []
  type: TYPE_NORMAL
- en: Git flow and GitHub flow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The very popular and standardized methodology for working with Git is simply
    called **Git flow**. Here is the brief description of the main rules of that flow:'
  prefs: []
  type: TYPE_NORMAL
- en: There is a main working branch, usually called `develop`, where all the developments
    for the latest version of the application occurs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: New project features are implemented in separate branches called *feature branches*
    that always start from the `develop` branch. When work on a feature is finished
    and the code is properly tested, this branch is merged back to `develop`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the code in `develop` is stabilized (without known bugs) and there is a
    need for new application release, a new *release branch* is created. This release
    branch usually requires additional tests (extensive QA tests, integration tests,
    and so on) so new bugs will be definitely found. If additional changes (such as
    bug fixes) are included in a release branch, they need to eventually be merged
    back to the `develop` branch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When code on a *release branch* is ready to be deployed/released, it is merged
    to the `master` branch and the latest commit on the `master` is labeled with an
    appropriate version tag. No other branches but `release` branches can be merged
    to the `master`. The only exceptions are hot fixes that need to be immediately
    deployed or released.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hot fixes that require urgent release are always implemented on separate branches
    that start from the `master`. When the fix is done, it is merged to both the `develop`
    and `master` branches. Merging of the hot fix branch is done like it were an ordinary
    release branch, so it must be properly tagged and the application version identifier
    should be modified accordingly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The visual example of *Git flow* in action is presented in *Figure 5*. For those
    that have never worked in such a way, and have also never used distributed version
    control systems, this may be a bit overwhelming. Anyway, it is really worth trying
    in your organization if you don't have any formalized workflow. It has multiple
    benefits and also solves real problems. It is especially useful for teams of multiple
    programmers that are working on many separate features and when continuous support
    for multiple releases needs to be provided.
  prefs: []
  type: TYPE_NORMAL
- en: This methodology is also handy if you want to implement continuous delivery
    using continuous deployment processes because it is always clear in your organization
    and which version of code represents a deliverable release of your application
    or service. It is also a great tool for open source projects because it provides
    great transparency to both the users and the active contributors.
  prefs: []
  type: TYPE_NORMAL
- en: '![Git flow and GitHub flow](graphics/5295_08_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5 Visual presentation of Git flow in action
  prefs: []
  type: TYPE_NORMAL
- en: So, if you think that this short summary of *Git flow* makes a bit of sense
    and it did not scare you yet, then you should dig deeper into online resources
    on that topic. It is really hard to say who the original author of the preceding
    workflow is, but most online sources point to Vincent Driessen. Thus, the best
    starting material to learn about *Git flow* is his online article titled *A successful
    Git* *branching model* (refer to [http://nvie.com/posts/a-successful-git-branching-model/](http://nvie.com/posts/a-successful-git-branching-model/)).
  prefs: []
  type: TYPE_NORMAL
- en: Like every other popular methodology, *Git flow* gained a lot of criticism over
    the Internet from programmers that do not like it. The most commented thing about
    Vincent Driessen's article is the rule (strictly technical) saying that every
    merge should create a new artificial commit representing that merge. Git has an
    option to do *fast forward* merges and Vincent discourages that option. This is,
    of course, an unsolvable problem because the best way to perform merges is a completely
    subjective matter to the organization Git is being used in. Anyway, the real issue
    of *Git flow* is that it is noticeably complicated. The full set of rules is really
    long, so it is easy to make some mistakes. It is very probable that you would
    like to choose something simpler.
  prefs: []
  type: TYPE_NORMAL
- en: 'One such flow is used at GitHub and described by Scott Chacon on his blog (refer
    to [http://scottchacon.com/2011/08/31/github-flow.html](http://scottchacon.com/2011/08/31/github-flow.html)).
    It is referred to as **GitHub flow** and is very similar to *Git flow*:'
  prefs: []
  type: TYPE_NORMAL
- en: Anything in the master branch is deployable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The new features are implemented on separate branches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main difference from *Git flow* is simplicity. There is only one main development
    branch (`master`) and it is always stable (in contrast to the `develop` branch
    in *Git flow*). There are also no release branches and a big emphasis is placed
    on tagging the code. There is no such need at GitHub because, as they say, when
    something is merged into the master it is usually deployed to production immediately.
    Diagram presenting an example b flow in action is shown in *Figure 6*.
  prefs: []
  type: TYPE_NORMAL
- en: 'GitHub flow seems like a good and lightweight workflow for teams that want
    to have a continuous deployment process setup for their project. Such a workflow
    is, of course, not viable for any project that has a strong notion of release
    (with strict version numbers)—at least without any modifications. It is important
    to know that the main assumption of the *always deployable* `master` branch is
    that it cannot be ensured without proper automated testing and a building procedure.
    This is what continuous integration systems take care of and we will discuss that
    a bit later. The following is a diagram presenting an example of GitHub flow in
    action:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Git flow and GitHub flow](graphics/5295_08_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6 Visual presentation of GitHub flow in action
  prefs: []
  type: TYPE_NORMAL
- en: Note that both *Git flow* and *GitHub flow* are only branching strategies, so
    despite having *Git* in their names, they are not limited to that single DVCS
    solution. It's true that the official article describing *Git flow* mentions specific
    `git` command parameters that should be used when performing a merge, but the
    general idea can be easily applied to almost any other distributed version control
    system. In fact, due to the way it is suggested to handle merges, Mercurial seems
    like a better tool to use for this specific branching strategy! The same applies
    to *GitHub flow*. This is the only branching strategy sprinkled with a bit of
    specific development culture, so it can be used in any version control system
    that allows you to easily create and merge branches of code.
  prefs: []
  type: TYPE_NORMAL
- en: As a last comment, remember that no methodology is carved in stone and no one
    forces you to use it. They are created to solve some existing problems and keep
    you from making common mistakes. You can take all of their rules or modify some
    of them to your own needs. They are great tools for beginners that may easily
    get into common pitfalls. If you are not familiar with any version control system,
    you should then start with a lightweight methodology like *GitHub flow* without
    any custom modification. You should start thinking about more complex workflows
    only when you get enough experience with Git, or any other tool of your choice.
    Anyway, as you will gain more and more proficiency, you will eventually realize
    that there is no perfect workflow that suits every project. What works well in
    one organization does not need to work well in others.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous development processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are some processes that can greatly streamline your development and reduce
    a time in getting the application ready to be released or deployed to the production
    environment. They often have `continuous` in their name, and we will discuss the
    most important and popular ones in this section. It is important to highlight
    that they are strictly technical processes, so they are almost unrelated to project
    management technologies, although they can highly dovetail with the latter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most important processes we will mention are:'
  prefs: []
  type: TYPE_NORMAL
- en: Continuous integration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous delivery
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The order of listing is important because each one of them is an extension of
    the previous one. Continuous deployment could be simply considered even a variation
    of continuous delivery. We will discuss them separately anyway, because what is
    only a minor difference for one organization may be critical in others.
  prefs: []
  type: TYPE_NORMAL
- en: The fact that these are technical processes means that their implementation
    strictly depends on the usage of proper tools. The general idea behind each of
    them is rather simple, so you could build your own continuous integration/delivery/deployment
    tools, but the best approach is to choose something that is already built. This
    way, you can focus more on building your product instead of the tool chain for
    continuous development.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous integration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Continuous integration**, often abbreviated as **CI**, is a process that
    takes benefit from automated testing and version control systems to provide a
    fully automatic integration environment. It can be used with centralized version
    control systems but in practice it spreads its wings only when a good DVCS tool
    is being used to manage the code.'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a repository is the first step towards continuous integration, which
    is a set of software practices that have emerged from **eXtreme** **Programming**
    (**XP**). The principles are clearly described on Wikipedia ([http://en.wikipedia.org/wiki/Continuous_integration#The_Practices](http://en.wikipedia.org/wiki/Continuous_integration#The_Practices))
    and define a way to make sure the software is easy to build, test, and deliver.
  prefs: []
  type: TYPE_NORMAL
- en: The first and most important requirement to implement continuous integration
    is to have a fully automated workflow that can test the whole application in the
    given revision in order to decide if it is technically correct. Technically correct
    means that it is free of known bugs and that all the features work as expected.
  prefs: []
  type: TYPE_NORMAL
- en: The general idea behind CI is that tests should always be run before merging
    to the mainstream development branch. This could be handled only through formal
    arrangements in the development team, but practice shows that this is not a reliable
    approach. The problem is that, as programmers, we tend to be overconfident and
    are unable to look critically at our code. If continuous integration is built
    only on team arrangements, it will inevitably fail because some of the developers
    will eventually skip their testing phase and commit possibly faulty code to the
    mainstream development branch that should always remain stable. And, in reality,
    even simple changes can introduce critical issues.
  prefs: []
  type: TYPE_NORMAL
- en: The obvious solution is to utilize a dedicated build server that automatically
    runs all the required application tests whenever the codebase changes. There are
    many tools that streamline this process and they can be easily integrated with
    version control hosting services such as GitHub or Bitbucket and self-hosted services
    such as GitLab. The benefit of using such tools is that the developer may locally
    run only the selected subset of tests (that, according to him, are related to
    his current work) and leave a potentially time consuming whole suite of integration
    tests for the build server. This really speeds up the development but still reduces
    the risk that new features will break the existing stable code found in the mainstream
    code branch.
  prefs: []
  type: TYPE_NORMAL
- en: Another plus of using a dedicated build server is that tests can be run in the
    environment that is closer to the production. Developers should also use environments
    that match the production as closely as possible and there are great tools for
    that (Vagrant, for instance); it is, however, hard to enforce this in any organization.
    You can easily do that on one dedicated build server or even on a cluster of build
    servers. Many CI tools make that even less problematic by utilizing various virtualization
    tools that help to ensure that tests are run always in the same, and completely
    fresh, testing environment.
  prefs: []
  type: TYPE_NORMAL
- en: Having a build server is also a must if you create desktop or mobile applications
    that must be delivered to users in binary form. The obvious thing to do is to
    always perform such a building procedure in the same environment. Almost every
    CI system takes into account the fact that applications often need to be downloaded
    in binary form after testing/building is done. Such building results are commonly
    referred to as **build artifacts**.
  prefs: []
  type: TYPE_NORMAL
- en: Because CI tools originated in times where most of the applications were written
    in compiled languages, they mostly use the term "building" to describe their main
    activity. For languages such as C or C++, this is obvious because applications
    cannot be run and tested if it is not built (compiled). For Python, this makes
    a bit less sense because most of the programs are distributed in a source form
    and can be run without any additional building step. So, in the scope of our language,
    the *building* and *testing* terms are often used interchangeably when talking
    about continuous integration.
  prefs: []
  type: TYPE_NORMAL
- en: Testing every commit
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The best approach to continuous integration is to perform the whole test suite
    on every change pushed to the central repository. Even if one programmer pushed
    a series of multiple commits in a single branch, it very often makes sense to
    test each change separately. If you decide to test only the latest changeset in
    a single repository push, then it will be harder to find sources of possible regression
    problems introduced somewhere in the middle.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, many DVCS such as Git or Mercurial allow you to limit time spent
    on searching regression sources by providing commands to *bisect* the history
    of changes, but in practice it is much more convenient to do that automatically
    as part of your continuous integration process.
  prefs: []
  type: TYPE_NORMAL
- en: Of course there is the issue of projects that have very long running test suites
    that may require tens of minutes or even hours to complete. One server may be
    not enough to perform all the builds on every commit made in the given time frame.
    This will make waiting for results even longer. In fact, long running tests is
    a problem on its own that will be described later in the *Problem 2 – too long
    building time* section. For now, you should know that you should always strive
    to test every commit pushed to the repository. If you have no power to do that
    on a single server, then set up the whole building cluster. If you are using a
    paid service, then pay for a higher pricing plan with more parallel builds. Hardware
    is cheap. Your developers' time is not. Eventually, you will save more money by
    having faster parallel builds and a more expensive CI setup than you would save
    on skipping tests for selected changes.
  prefs: []
  type: TYPE_NORMAL
- en: Merge testing through CI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Reality is complicated. If the code on a feature branch passes all the tests,
    it does not mean that the build will not fail when it is merged to a stable mainstream
    branch. Both of the popular branching strategies mentioned in the *Git flow and
    GitHub flow* sections assume that code merged to the `master` branch is always
    tested and deployable. But how can you be sure that this assumption is met if
    you have not perform the merge yet? This is a lesser problem for *Git flow* (if
    implemented well and used precisely) due to its emphasis on release branches.
    But it is a real problem for the simple *GitHub flow* where merging to `master`
    is often related with conflicts and is very likely to introduce regressions in
    tests. Even for *Git flow*, this is a serious concern. This is a complex branching
    model, so for sure people will make mistakes when using it. So, you can never
    be sure that the code on `master` will pass the tests after merging if you won't
    take the special precautions.
  prefs: []
  type: TYPE_NORMAL
- en: One of the solutions to this problem is to delegate the duty of merging feature
    branches into a stable mainstream branch to your CI system. In many CI tools,
    you can easily set up an on-demand building job that will locally merge a specific
    feature branch to the stable branch and push it to the central repository only
    if it passed all the tests. If the build fails, then such a merge will be reverted,
    leaving the stable branch untouched. Of course, this approach gets more complex
    in fast paced projects where many feature branches are developed simultaneously
    because there is a high risk of conflicts that can't be resolved automatically
    by any CI system. There are, of course, solutions to that problem, like rebasing
    in Git.
  prefs: []
  type: TYPE_NORMAL
- en: Such an approach to merging anything into the stable branch in a version control
    system is practically a must if you are thinking about going further and implementing
    continuous delivery processes. It is also required if you have a strict rule in
    your workflow stating that everything in a stable branch is releasable.
  prefs: []
  type: TYPE_NORMAL
- en: Matrix testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Matrix testing is a very useful tool if your code needs to be tested in different
    environments. Depending on your project needs, the direct support of such a feature
    in your CI solution may be less or more required.
  prefs: []
  type: TYPE_NORMAL
- en: The easiest way to explain the idea of matrix testing is to take the example
    of some open source Python package. Django, for instance, is the project that
    has a strictly specified set of supported Python language versions. The 1.9.3
    version lists the Python 2.7, Python 3.4, and Python 3.5 versions as required
    in order to run Django code. This means that every time Django core developers
    make a change to the project, the full tests suite must be executed on these three
    Python versions in order to back this claim. If even a single test fails on one
    environment, the whole build must be marked as failed because the backwards compatibility
    constraint was possibly broken. For such a simple case, you do not need any support
    from CI. There is a great Tox tool (refer to [https://tox.readthedocs.org/](https://tox.readthedocs.org/))
    that, among other features, allows you to easily run test suites in different
    Python versions in isolated virtual environments. This utility can also be easily
    used in local development.
  prefs: []
  type: TYPE_NORMAL
- en: 'But this was only the simplest example. It is not uncommon that the application
    must be tested in multiple environments where completely different parameters
    must be tested. To name a few:'
  prefs: []
  type: TYPE_NORMAL
- en: Different operating systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different versions of backing services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different types of filesystems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The full set of combinations forms a multi-dimensional environment parameter
    matrix, and this is why such a setup is called matrix testing. When you need such
    a deep testing workflow, it is very possible that you require some integrated
    support for matrix testing in your CI solution. With a large number of possible
    combinations, you will also require a highly parallelizable building process because
    every run over the matrix will require a large amount of work from your building
    server. In some cases, you will be forced to do some tradeoff if your test matrix
    has too many dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous delivery
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Continuous delivery is a simple extension of the continuous integration idea.
    This approach to software engineering aims to ensure that the application can
    be released reliably at any time. The goal of continuous delivery is to release
    software in short circles. It generally reduces both costs and the risk of releasing
    software by allowing the incremental delivery of changes to the application in
    production.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main prerequisites for building successful continuous delivery processes
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: A reliable continuous integration process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An automated process of deployment to the production environment (if the project
    has a notion of the production environment)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A well-defined version control system workflow or branching strategy that allows
    you to easily define what version of software represents releasable code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In many projects, the automated tests are not enough to reliably tell if the
    given version of the software is really ready to be released. In such cases, the
    additional manual user acceptance tests are usually performed by skilled QA staff.
    Depending on your project management methodology, this may also require some approval
    from the client. This does not mean that you can't use *Git flow*, *GitHub flow*,
    or a similar branching strategy, if some of your acceptance tests must be performed
    manually by humans. This only changes the semantics of your stable and release
    branches from *ready to be deployed* to *ready for user acceptance tests and approval*.
  prefs: []
  type: TYPE_NORMAL
- en: Also, the previous paragraph does not change the fact that code deployment should
    always be automated. We already discussed some of the tools and benefits of automation
    in [Chapter 6](ch06.html "Chapter 6. Deploying Code"), *Deploying Code*. As stated
    there, it will always reduce the cost and risk of a new release. Also, most of
    the available CI tools allow you to set up special build targets that, instead
    of testing, will perform automated deployment for you. In most continuous delivery
    processes, this is usually triggered manually (on demand) by authorized staff
    members when they are sure there is required approval and all acceptance tests
    ended with success.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Continuous deployment is a process that takes continuous delivery to the next
    level. It is a perfect approach for projects where all acceptance tests are automated
    and there is no need for manual approval from the client. In short, once code
    is merged to the stable branch (usually `master`), it is automatically deployed
    to the production environment.
  prefs: []
  type: TYPE_NORMAL
- en: This approach seems to be very nice and robust but is not often used because
    it is very hard to find a project that does not need manual QA testing and someone's
    approval before a new version is released. Anyway, it is definitely doable and
    some companies claim to be working in that way.
  prefs: []
  type: TYPE_NORMAL
- en: In order to implement continuous deployment, you need the same basic prerequisites
    as the continuous delivery process. Also, a more careful approach to merging into
    a stable branch is very often required. What gets merged into the `master` in
    continuous integration usually goes instantly to the production. Because of that,
    it is reasonable to handoff the merging task to your CI system, as explained in
    the *Merge testing through CI* section.
  prefs: []
  type: TYPE_NORMAL
- en: Popular tools for continuous integration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is a tremendous variety of choices for CI tools nowadays. They greatly
    vary on ease of use and available features, and almost each one of them has some
    unique features that others will lack. So, it is hard to give a good general recommendation
    because each project has completely different needs and also a different development
    workflow. There are, of course, some great free and open source projects, but
    paid hosted services are also worth researching. It's because although open source
    software such as Jenkins or Buildbot are freely available to install without any
    fee, it is false thinking that they are free to run. Both hardware and maintenance
    are added costs of having your own CI system. In some circumstances, it may be
    less expensive to pay for such a service instead of paying for additional infrastructure
    and spending time on resolving any issues in open source CI software. Still, you
    need to make sure that sending your code to any third-party service is in line
    with security policies at your company.
  prefs: []
  type: TYPE_NORMAL
- en: Here we will review some of the popular free open source tools, as well as paid
    hosted services. I really don't want to advertise any vendor, so we will discuss
    only those that are available without any fees for open source projects to justify
    this rather subjective selection. No best recommendation will be given, but we
    will point out both the good and bad sides of any solution. If you are still in
    doubt, the next section that describes common continuous integration pitfalls
    should help you in making good decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Jenkins
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Jenkins ([https://jenkins-ci.org](https://jenkins-ci.org)) seems to be the most
    popular tool for continuous integration. It is also one of the oldest open source
    projects in this field, in pair with Hudson (the development of these two projects
    split and Jenkins is a fork of Hudson).
  prefs: []
  type: TYPE_NORMAL
- en: '![Jenkins](graphics/5295_08_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7 Preview of Jenkins main interface
  prefs: []
  type: TYPE_NORMAL
- en: Jenkins is written in Java and was initially designed mainly for building projects
    written in the Java language. It means that for Java developers, it is a perfect
    CI system, but you will need to struggle a bit if you want to use it with other
    technology stack.
  prefs: []
  type: TYPE_NORMAL
- en: One big advantage of Jenkins is its very extensive list of features that Jenkins
    have implemented straight out of the box. The most important one, from the Python
    programmer's point of view, is the ability to understand test results. Instead
    of giving only plain binary information about build success, Jenkins is able to
    present the results of all tests that were executed during a run in the form of
    tables and graphs. This will, of course, not work automatically and you need to
    provide those results in a specific format (by default, Jenkins understands JUnit
    files) during your build. Fortunately, a lot of Python testing frameworks are
    able to export results in a machine-readable format.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example presentation of unit test results in Jenkins in
    its web UI:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Jenkins](graphics/5295_08_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8 Presentation of unit test results in Jenkins
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot illustrates how Jenkins presents additional build
    information such as trends or downloadable artifacts:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Jenkins](graphics/5295_08_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9 Test result trends graph on example Jenkins project
  prefs: []
  type: TYPE_NORMAL
- en: Surprisingly, most of Jenkins' power does not come from its built-in features
    but from a huge repository of free plugins. What is available from clean installation
    may be great for Java developers but programmers using different technologies
    will need to spend a lot of time to make it suited for their project. Even support
    for Git is provided by some plugin.
  prefs: []
  type: TYPE_NORMAL
- en: It is great that Jenkins is so easily extendable, but this has also some serious
    downsides. You will eventually depend on installed plugins to drive your continuous
    integration process and these are developed independently from Jenkins core. Most
    authors of popular plugins try to keep them up to date and compatible with the
    latest releases of Jenkins. Nevertheless, the extensions with smaller communities
    will be updated less frequently, and some day you may be either forced to resign
    from them or postpone the update of the core system. This may be a real problem
    when there is urgent need for an update (security fix, for instance), but some
    of the plugins that are critical for your CI process will not work with the new
    version.
  prefs: []
  type: TYPE_NORMAL
- en: The basic Jenkins installation that provides you with a master CI server is
    also capable of performing builds. This is different from other CI systems that
    put more emphasis on distribution and create a strict separation from master and
    slave build servers. This is both good and bad. On the one side, it allows you
    to set up a wholly working CI server in a few minutes. Jenkins, of course, supports
    deferring work to build slaves, so you can scale out in future whenever it is
    needed. On the other hand, it is very common that Jenkins is underperforming because
    it is deployed in single-server settings, and its users complain regarding performance
    without providing it enough resources. It is not hard to add new building nodes
    to the Jenkins cluster. It seems that this is rather a mental challenge than a
    technical problem for those that got used to the single-server setup.
  prefs: []
  type: TYPE_NORMAL
- en: Buildbot
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Buildbot ([http://buildbot.net/](http://buildbot.net/)) is a software written
    in Python that automates the compile and test cycles for any kind of software
    project. It is configurable in a way that every change made on a source code repository
    generates some builds and launches some tests and then provides some feedback:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Buildbot](graphics/5295_08_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10 Buildbot's Waterfall view for CPython 3.x branch
  prefs: []
  type: TYPE_NORMAL
- en: This tool is used, for instance, by CPython core and can be seen at [http://buildbot.python.org/all/waterfall?&category=3.x.stable](http://buildbot.python.org/all/waterfall?&category=3.x.stable).
  prefs: []
  type: TYPE_NORMAL
- en: 'The default Buildbot''s representation of build results is a Waterfall view,
    as shown in *Figure 10*. Each column corresponds to a **build** composed of **steps**
    and is associated with some **build** **slaves**. The whole system is driven by
    the build master:'
  prefs: []
  type: TYPE_NORMAL
- en: The build master centralizes and drives everything
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A build is a sequence of steps used to build an application and run tests over
    it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A **step** is an atomic command, for example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check out the files of a project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build the application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A build slave is a machine that is in charge of running a build. It can be located
    anywhere as long as it can reach the build master. Thanks to this architecture,
    Buildbot scales very well. All of heavy lifting is done on build slaves and you
    can have as many of them as you want.
  prefs: []
  type: TYPE_NORMAL
- en: Very simple and clear design makes Buildbot very flexible. Each build step is
    just a single command. Buildbot is written in Python but it is completely language
    agnostic. So the build step can be absolutely anything. The process exit code
    is used to decide if the step ended as a success and all standard output of the
    step command is captured by default. Most of the testing tools and compilers follow
    good design practices, and they indicate failures with proper exit codes and return
    readable error/warning messages on `sdout` or `stderr` output streams. If it's
    not true, you can usually easily wrap them with a Bash script. In most cases,
    this is a simple task. Thanks to this, a lot of projects can be integrated with
    Buildbot with only minimal effort.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next advantage of Buildbot is that it supports many version control systems
    out of the box without the need to install any additional plugins:'
  prefs: []
  type: TYPE_NORMAL
- en: CVS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Subversion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perforce
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bzr
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Darcs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Git
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mercurial
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monotone
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main disadvantage of Buildbot is its lack of higher-level presentation tools
    for presenting build results. For instance, other projects, such as Jenkins, can
    take the notion of unit tests run during the build. If you feed them with test
    results data presented in the proper format (usually XML), they can present all
    the tests in a readable form like tables and graphs. Buildbot does not have such
    a built-in feature and this is the price it pays for its flexibility and simplicity.
    If you need some extra bells and whistles, you need to build them by yourself
    or search for some custom extension. On the other hand, thanks to such simplicity,
    it is easier to reason about Buildbot's behavior and maintain it. So, there is
    always a tradeoff.
  prefs: []
  type: TYPE_NORMAL
- en: Travis CI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Travis CI ([https://travis-ci.org/](https://travis-ci.org/)) is a continuous
    integration system sold in Software as a Service form. It is a paid service for
    enterprises but can be used completely for free in open source projects hosted
    on GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: '![Travis CI](graphics/5295_08_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11 Travis CI page for django-userena project showing failed builds in
    its build matrix
  prefs: []
  type: TYPE_NORMAL
- en: Naturally, this is the free part of its pricing plan that made it very popular.
    Currently, it is one of the most popular CI solutions for projects hosted on GitHub.
    But the biggest advantage over older projects such as Buildbot or Jenkins, is
    how the build configuration is stored. All build definition is provided in a single
    `.travis.yml` file in the root of the project repository. Travis works only with
    GitHub, so if you have enabled such integration, your project will be tested on
    every commit if there is only a `.travis.yml` file.
  prefs: []
  type: TYPE_NORMAL
- en: Having the whole CI configuration for a project in its code repository is really
    a great approach. This makes the whole process a lot clearer for the developers
    and also allows for more flexibility. In systems where build configuration must
    be provided to build a server separately (using web interface or through server
    configuration), there is always some additional friction when something new needs
    to be added to the testing rig. In some organizations, where only selected staff
    are authorized to maintain the CI system, this really slows the process of adding
    new build steps down. Also, sometimes there is a need to test different branches
    of the code with completely different procedures. When build configuration is
    available in project sources, it is a lot easier to do so.
  prefs: []
  type: TYPE_NORMAL
- en: The other great feature of Travis is the emphasis it puts on running builds
    in clean environments. Every build is executed in a completely fresh virtual machine,
    so there is no risk of some persisted state that would affect build results. Travis
    uses a rather big virtual machine image, so you have a lot of open source software
    and programming environments available without the need of additional installs.
    In this isolated environment, you have full administrative rights so you can download
    and install anything you need to perform your build and the syntax of the `.travis.yml`
    file makes it very easy. Unfortunately, you do not have a lot of choice over the
    operating system available as the base of your testing environment. Travis does
    not allow to provide your own virtual machine images, so you must rely on the
    very limited options provided. Usually there is no choice at all and all the builds
    must be done in some version of Ubuntu or Mac OS X (still experimental at the
    time of writing the book). Sometimes there is an option to select some legacy
    version of the system or the preview of the new testing environment, but such
    a possibility is always temporary. There is always a way to bypass this. You can
    run another virtual machine inside of the one provided by Travis. This should
    be something that allows you to easily encode VM configuration in your project
    sources such as Vagrant or Docker. But this will add more time to your builds,
    so it is not the best approach you will take. Stacking virtual machines that way
    may not be the best and most efficient approach if you need to perform tests under
    different operating systems. If this is an important feature for you, then this
    is a sign that Travis is not a service for you.
  prefs: []
  type: TYPE_NORMAL
- en: The biggest downside of Travis is that it is completely locked to GitHub. If
    you would like to use it in your open source project, then this is not a big deal.
    For enterprises and closed source projects, this is mostly an unsolvable issue.
  prefs: []
  type: TYPE_NORMAL
- en: GitLab CI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: GitLab CI is a part of a larger GitLab project. It is available as both a paid
    service (Enterprise Edition) and an open source project that you may host on your
    own infrastructure (Community Edition). The open source edition lacks some of
    the paid service features, but in most cases is everything that any company needs
    from the software that manages version control repositories and continuous integration.
  prefs: []
  type: TYPE_NORMAL
- en: GitLab CI is very similar in feature sets to the Travis. It is even configured
    with a very similar YAML syntax stored in the `.gitlab-ci.yml` file. The biggest
    difference is that the GitLab Enterprise Edition pricing model does not provide
    you with free accounts for open source projects. The Community Edition is open
    source by itself but you need to have some own infrastructure in order to run
    it.
  prefs: []
  type: TYPE_NORMAL
- en: When compared with Travis, the GitLab has an obvious advantage of having more
    control over the execution environment. Unfortunately, in the area of environment
    isolation, the default build runner in GitLab is a bit inferior. The process called
    Gitlab Runner executes all the build steps in the same environment it is run in,
    so it works more like Jenkins' or Buildbot's slave servers. Fortunately, it plays
    well with Docker, so you can easily add more isolation with container-based virtualization,
    but this will require some effort and additional setup. In Travis, you get full
    isolation out of the box.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right tool and common pitfalls
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As already said, there is no perfect CI tool that will suit every project and,
    most importantly, every organization and workflow it uses. I can give only a single
    suggestion for open source projects hosted on GitHub. For small code bases with
    platform independent code, **Travis CI** seems like the best choice. It is easy
    to start with and will give you almost instant gratification with a minimal amount
    of work.
  prefs: []
  type: TYPE_NORMAL
- en: For projects with closed sources, the situation is completely different. It
    is possible that you will need to evaluate a few CI systems in various setups
    until you are able decide which one is best for you. We discussed only four of
    the popular tools but it should be a rather representative group. To make your
    decision a bit easier, we will discuss some of the common problems related to
    continuous integration systems. In some of the available CI systems, it is more
    possible to make certain kinds of mistakes than in others. On the other hand,
    some of the problems may not be important to every application. I hope that by
    combining the knowledge of your needs with this short summary, it will be easier
    to make your first decision the right one.
  prefs: []
  type: TYPE_NORMAL
- en: Problem 1 – too complex build strategies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Some organizations like to formalize and structure things beyond the reasonable
    levels. In companies that create computer software, this is especially true in
    two areas: project management tools and build strategies on CI servers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Excessive configuration of project management tools usually ends with issue
    processing workflows on JIRA (or any other management software) so complicated
    that they will never fit a single wall when expressed as graphs. If your manager
    has such configuration/control mania, you can either talk to him or switch him
    for another manager (read: quit your current job). Unfortunately, this does not
    reliably ensure any improvement in that matter.'
  prefs: []
  type: TYPE_NORMAL
- en: 'But when it comes to CI, we can do more. Continuous integration tools are usually
    maintained and configured by us: developers. These are OUR tools that are supposed
    to improve OUR work. If someone has irresistible temptation to toggle every switch
    and turn every knob possible, then he should be kept away from configuration of
    CI systems, especially if his main job is to talk the whole day and make decisions.'
  prefs: []
  type: TYPE_NORMAL
- en: There is really no need for making complex strategies to decide which commit
    or branch should be tested. No need to limit testing to specific tags. No need
    to queue commits in order to perform larger builds. No need to disable building
    via custom commit messages. Your continuous integration process should be simple
    to reason about. Test everything! Test always! That's all! If there are not enough
    hardware resources to test every commit, then add more hardware. Remember that
    the programmer's time is more expensive than silicon chips.
  prefs: []
  type: TYPE_NORMAL
- en: Problem 2 – too long building time
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Long building times is a thing that kills performance of any developer. If you
    need to wait hours to know if your work was done properly, then there is no way
    you can be productive. Of course, having something else to do when your feature
    is being tested helps a lot. Anyway, as humans, we are really terrible at multitasking.
    Switching between different problems takes time and, in the end, reduces our programming
    performance to zero. It's simply hard to keep focus when working on multiple problems
    at once.
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution is very simple: keep your builds fast at any price. At first,
    try to find bottlenecks and optimize them. If the performance of build servers
    is the problem, then try to scale out. If this does not help, then split each
    build into smaller parts and parallelize.'
  prefs: []
  type: TYPE_NORMAL
- en: There are plenty of solutions to speed up slow build tests, but sometimes nothing
    can be done about that problem. For instance, if you have automated browser tests
    or need to perform long running calls to external services, then it is very hard
    to improve performance beyond some hard limit. For instance, when speed of automated
    acceptance test in your CI becomes a problem, then you can loosen the *test everything,
    test always* rule a bit. What matters the most for programmers are usually unit
    tests and static analysis. So, depending on your workflow, the slow browser tests
    may be sometimes deferred in time to the moment when release is being prepared.
  prefs: []
  type: TYPE_NORMAL
- en: The other solution to slow build runs is rethinking the overall architecture
    design of your application. If testing the application takes a lot of time, it
    is very often a sign that it should be split into a few independent components
    that can be developed and tested separately. Writing software as huge monoliths
    is one of the shortest paths to failure. Usually any software engineering process
    breaks on software that is not modularized properly.
  prefs: []
  type: TYPE_NORMAL
- en: Problem 3 – external job definitions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some continuous integration systems, especially Jenkins, allow you to set up
    most of the build configurations and testing processes completely through web
    UI, without the need to touch the code repository. But you should really avoid
    putting anything more than simple entry points to the build steps/commands into
    externals systems. This is the kind of CI anti-pattern that can cause nothing
    more than troubles.
  prefs: []
  type: TYPE_NORMAL
- en: Your building and testing process is usually tightly tied to your codebase.
    If you store its whole definition in external system such as Jenkins or Buildbot,
    then it will be really hard to introduce changes to that process.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example of a problem introduced by global external build definition,
    let''s assume that we have some open source project. The initial development was
    hectic and we did not care for any style guidelines. Our project was successful,
    so the development required another major release. After some time, we moved from
    `0.x` version to `1.0` and decided to reformat all of your code to conform to
    PEP 8 guidelines. It is a good approach to have a static analysis check as part
    of CI builds, so we decided to add the execution of the `pep8` tool to our build
    definition. If we had only a global external build configuration, then there would
    be a problem if some improvement needs to be done to the code in older versions.
    Let''s say that there is a critical security issue that needs to be fixed in both
    branches of the application: `0.x` and `1.y`. We know that anything below version
    1.0 wasn''t compliant with the style guide and the newly introduced check against
    PEP 8 will mark the build as failed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution to the problem is to keep the definition of your build process
    as close to the source as possible. With some CI systems (Travis CI and GitLab
    CI), you get that workflow by default. With other solutions (Jenkins and Buildbot)
    you need to take additional care in order to ensure that most of the build processes
    are included in your code instead of some external tool configuration. Fortunately,
    you have a lot of choices that allow that kind of automation:'
  prefs: []
  type: TYPE_NORMAL
- en: Bash scripts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Makefiles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Problem 4 – lack of isolation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have discussed the importance of isolation when programming in Python many
    times already. We know that the best approach to isolate Python execution environment
    on the package level is to use virtual environments with `virtualenv` or `python
    -m venv`. Unfortunately, when testing code for the purpose of continuous integration
    processes, it is usually not enough. The testing environment should be as close
    as possible to the production environment and it is really hard to achieve that
    without additional system-level virtualization.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main issues you may experience when not ensuring proper system-level isolation
    when building your application are:'
  prefs: []
  type: TYPE_NORMAL
- en: Some state persisted between builds either on the filesystem or in backing services
    (caches, databases, and so on)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiple builds or tests interfacing with each other through the environment,
    filesystem or backing services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Problems that would occur due to specific characteristics of the production
    operating system not caught on the build server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The preceding issues are particularly troublesome if you need to perform concurrent
    builds of the same application or even parallelize single builds.
  prefs: []
  type: TYPE_NORMAL
- en: Some Python frameworks (mostly Django) provide some additional level of isolation
    for databases that try to ensure the storage will be cleaned before running tests.
    There is also quite a useful extension for `py.test` called `pytest-dbfixtures`
    (refer to [https://github.com/ClearcodeHQ/pytest-dbfixtures](https://github.com/ClearcodeHQ/pytest-dbfixtures))
    that allows you to achieve that even more reliably. Anyway, such solutions add
    even more complexity to your builds instead of reducing it. Always clearing the
    virtual machine on every build (in the style of Travis CI) seems like a more elegant
    and simpler approach.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have learned the following things in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the difference between centralized and distributed version control systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why you should prefer distributed version control systems over centralized
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why Git should be your first choice for DVCS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the common workflows and branching strategies for Git
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is continuous integration/delivery/deployment and what are the popular
    tools that allow you to implement these processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next chapter will explain how to clearly document your code.
  prefs: []
  type: TYPE_NORMAL
