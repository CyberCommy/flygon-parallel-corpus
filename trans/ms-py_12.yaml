- en: Chapter 12. Performance – Tracking and Reducing Your Memory and CPU Usage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we talk about performance, there is a quote by *Donald Knuth* you need
    to consider first:'
  prefs: []
  type: TYPE_NORMAL
- en: '*"The real problem is that programmers have spent far too much time worrying
    about efficiency in the wrong places and at the wrong times; premature optimization
    is the root of all evil (or at least most of it) in programming."*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Donald Knuth is often called the father of algorithm analysis. His book series,
    *The Art of Computer Programming*, can be considered the Bible of all fundamental
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: As long as you pick the correct data structures with the right algorithms, performance
    should not be something to worry about. That does not mean you should ignore performance
    entirely, but just make sure you pick the right battles and optimize only when
    it is actually needed. Micro/premature optimizations can definitely be fun, but
    only very rarely useful.
  prefs: []
  type: TYPE_NORMAL
- en: We have seen the performance characteristics of many data structures in [Chapter
    2](ch02.html "Chapter 2. Pythonic Syntax, Common Pitfalls, and Style Guide"),
    *Pythonic Syntax, Common Pitfalls, and Style Guide*, already, so we won't discuss
    that, but we will show you how performance can be measured and how problems can
    be detected. There are cases where micro optimizations make a difference, but
    you won't know until you measure the performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Within this chapter, we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Profiling CPU usage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Profiling memory usage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning how to correctly compare performance metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding and fixing memory leaks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is performance?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Performance is a very broad term. It has many different meanings and in many
    cases it is defined incorrectly. You have probably heard statements similar to
    "Language X is faster than Python". However, that statement is inherently wrong.
    Python is neither fast nor slow; Python is a programming language and a language
    has no performance metrics whatsoever. If one were to say that the CPython interpreter
    is faster or slower than interpreter Y for language X, that would be possible.
    The performance characteristics of code can vary greatly between different interpreters.
    Just take a look at this small test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Three different interpreters with all vastly different performance! All are
    Python but the interpreters obviously vary. Looking at this benchmark, you might
    be tempted to drop the CPython interpreter completely and only use Pypy. The danger
    with benchmarks such as these is that they rarely offer any meaningful results.
    For this limited example, the Pypy interpreter was about four times faster than
    the CPython3 interpreter, but that has no relevance whatsoever for the general
    case. The only conclusion that can safely be drawn here is that this specific
    version of the Pypy interpreter is more than four times faster than this specific
    version of CPython3 for this exact test. For any other test and interpreter version
    the results could be vastly different.
  prefs: []
  type: TYPE_NORMAL
- en: Timeit – comparing code snippet performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we can start improving performance, we need a reliable method to measure
    it. Python has a really nice module (`timeit`) with the specific purpose of measuring
    execution times of bits of code. It executes a bit of code many times to make
    sure there is as little variation as possible and to make the measurement fairly
    clean. It''s very useful if you want to compare a few code snippets. Following
    are example executions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'These few examples demonstrate the performance difference between `list.insert`,
    `list.append`, a list comprehension, and the `list` function. But more importantly,
    it demonstrates how to use the `timeit` command. Naturally, the command can be
    used with regular scripts as well, but the `timeit` module only accepts statements
    as strings to execute which is a bit of an annoyance. Luckily, you can easily
    work around that by wrapping your code in a function and just timing that function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'When executing this, you will get something along the following lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As you may have noticed, this script is still a bit basic. While the regular
    version keeps trying until it reaches 0.2 seconds or more, this script just has
    a fixed number of executions. Unfortunately, the `timeit` module wasn't entirely
    written with re-use in mind, so besides calling `timeit.main()` from your script
    there is not much you can do to re-use that logic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Personally, I recommend using IPython instead, as it makes measurements much
    easier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In this case, IPython automatically takes care of the string wrapping and passing
    of `globals()`. Still, this is all very limited and useful only for comparing
    multiple methods of doing the same thing. When it comes to full Python applications,
    there are more methods available.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To view the source of both IPython functions and regular modules, entering `object??`
    in the IPython shell returns the source. In this case just enter `timeit??` to
    view the `timeit` IPython function definition.
  prefs: []
  type: TYPE_NORMAL
- en: 'The easiest way you can implement the `%timeit` function yourself is to simply
    call `timeit.main`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The internals of the `timeit` module are nothing special. A basic version can
    be implemented with just an `eval` and a `time.perf_counter` (the highest resolution
    timer available in Python) combination:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The actual `timeit` code is a bit more advanced in terms of checking the input
    but this example roughly shows how the `timeit.repeat` function can be implemented.
  prefs: []
  type: TYPE_NORMAL
- en: 'To register your own function in IPython, you need to use some IPython magic.
    Note that the magic is not a pun. The IPython module that takes care of commands
    such as these is actually called `magic`. To demonstrate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: To learn more about custom magic in IPython, take a look at the IPython documentation
    at [https://ipython.org/ipython-doc/3/config/custommagics.html](https://ipython.org/ipython-doc/3/config/custommagics.html).
  prefs: []
  type: TYPE_NORMAL
- en: cProfile – finding the slowest components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `profile` module makes it easily possible to analyze the relative CPU cycles
    used in a script/application. Be very careful not to compare these with the results
    from the `timeit` module. While the `timeit` module tries as best as possible
    to give an accurate benchmark of the absolute amount of time it takes to execute
    a code snippet, the `profile` module is only useful for relative results. The
    reason is that the profiling code itself incurs such a slowdown that the results
    are not comparable with non-profiled code. There is a way to make it a bit more
    accurate however, but more about that later.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Within this section we will be talking about the `profile` module but in the
    examples we will actually use the `cProfile` module. The `cProfile` module is
    a high-performance emulation of the pure Python `profile` module.
  prefs: []
  type: TYPE_NORMAL
- en: First profiling run
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s profile our Fibonacci function from [Chapter 5](ch05.html "Chapter 5. Decorators
    – Enabling Code Reuse by Decorating"), *Decorators– Enabling Code Reuse by Decorating*,
    both with and without the cache function. First, the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For readabilities sake, all `cProfile` statistics will be stripped of the `percall`
    and `cumtime` columns in all `cProfile` outputs. These columns are irrelevant
    for the purposes of these examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'First we''ll execute the function without cache:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'That''s quite a lot of calls, isn''t it? Apparently, we called the `test_fibonacci`
    function nearly 3 million times. That is where the profiling modules provide a
    lot of insight. Let''s analyze the metrics a bit further:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ncalls**: The number of calls that were made to the function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tottime**: The total time spent in seconds within this function with all
    sub-functions excluded'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Percall, `tottime / ncalls`
  prefs: []
  type: TYPE_NORMAL
- en: '**Cumtime**: The total time spent within this function, including sub-functions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Percall, `cumtime / ncalls`
  prefs: []
  type: TYPE_NORMAL
- en: 'Which is the most useful depends on your use case. It''s quite simple to change
    the sort order using the `-s` parameter within the default output. But now let''s
    see what the result is with the cached version. Once again, with stripped output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This time we see a `tottime` of `0.000` because it's just too fast to measure.
    But also, while the `fibonacci_cached` function is still the most executed function,
    it's only being executed 31 times instead of 3 million.
  prefs: []
  type: TYPE_NORMAL
- en: Calibrating your profiler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To illustrate the difference between `profile` and `cProfile,` let''s try the
    uncached run again with the `profile` module instead. Just a heads up, this is
    much slower so don''t be surprised if it stalls a little:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Huge difference, isn''t it? Now the code is nearly 10 times slower and the
    only difference is using the pure Python `profile` module instead of the `cProfile`
    module. This does indicate a big problem with the `profile` module. The overhead
    from the module itself is great enough to skew the results, which means we should
    account for that offset. That''s what the `Profile.calibrate()` function takes
    care of, as it calculates the bias incurred by the profile module. To calculate
    the bias, we can use the following script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The numbers will vary slightly but you should be able to get a fair estimate
    of the bias using this code. If the numbers still vary a lot, you can increase
    the trials from `100000` to something even larger. This type of calibration only
    works for the profile module, but if you are looking for more accurate results
    and the `cProfile` module does not work for you due to inheritance or not being
    supported on your platform, you can use this code to set your bias globally and
    get more accurate results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'For a specific `Profile` instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that in general a smaller bias is better to use than a large one, because
    a large bias could cause very strange results. In some cases you will even get
    negative timings. Let''s give it a try for our Fibonacci code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'While running it, it indeed appears that I''ve used a bias that''s too large:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Still, it shows how the code can be used properly. You can even incorporate
    the bias calculation within the script using a snippet like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Selective profiling using decorators
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Calculating simple timings is easy enough using decorators, but profiling is
    also important. Both are useful but serve different goals. Let''s look at both
    the options:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The code is simple enough, just a basic timer and profiler printing some default
    statistics. Which functions best for you depends on your use-case of course, but
    they definitely both have their uses. The added advantage of this selective profiling
    is that the output is more limited which helps with readability:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the profiler still makes the code about twice as slow, but it's
    definitely usable.
  prefs: []
  type: TYPE_NORMAL
- en: Using profile statistics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To get some more intricate profiling results, we will profile the `pystone`
    script. The `pystone` script is an internal Python performance test which benchmarks
    the Python interpreter fairly thoroughly. First, let''s create the statistics
    using this script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'When executing the script, you should get something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the script, you should have a `pystone.profile` file containing
    the profiling results. These results can be And the `pystone.profile` file which
    contains all of the profiling statistics. These statistics can be viewed through
    the `pstats` module which is bundled with Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'In some cases, it can be interesting to combine the results from multiple measurements.
    That is possible by specifying multiple files or by using `stats.add(*filenames)`.
    But first, let''s look at the regular output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Obviously, the parameters can easily be modified to change the sort order and
    the number of output lines. But that is not the only possibility of the statistics.
    There are quite a few packages around which can parse these results and visualize
    them. One option is RunSnakeRun, which although useful does not run on Python
    3 currently. Also, we have QCacheGrind, a very nice visualizer for profile statistics
    but which requires some manual compiling to get running or some searching for
    binaries of course.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the output from QCacheGrind. In the case of Windows, the QCacheGrindWin
    package provides a binary, whereas within Linux it is most likely available through
    your package manager, and with OS X you can try `brew install qcachegrind --with-graphviz`.
    But there is one more package you will require: the `pyprof2calltree` package.
    It transforms the `profile` output into a format that QCacheGrind understands.
    So, after a simple `pip install pyprof2calltree,` we can now convert the `profile`
    file into a `callgrind` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in running of the `QCacheGrind` application. After switching to
    the appropriate tabs, you should see something like the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using profile statistics](images/4711_12_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For a simple script such as this, pretty much all output works. However, with
    full applications, a tool such as QCacheGrind is invaluable. Looking at the output
    generated by QCacheGrind, it is immediately obvious which process took the most
    time. The structure at the top right shows bigger rectangles if the amount of
    time taken was greater, which is a very useful visualization of the chunks of
    CPU time that were used. The list at the left is very similar to `cProfile` and
    therefore nothing new. The tree at the bottom right can be very valuable or very
    useless as it is in this case. It shows you the percentage of CPU time taken in
    a function and more importantly, the relationship of that function with the other
    functions.
  prefs: []
  type: TYPE_NORMAL
- en: Because these tools scale depending on the input the results are useful for
    just about any application. Whether a function takes 100 milliseconds or 100 minutes
    makes no difference, the output will show a clear overview of the slow parts,
    which is what we will try to fix.
  prefs: []
  type: TYPE_NORMAL
- en: Line profiler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`line_profiler` is actually not a package that''s bundled with Python, but
    it''s far too useful to ignore. While the regular `profile` module profiles all
    (sub)functions within a certain block, `line_profiler` allows for profiling line
    per line within a function. The Fibonacci function is not best suited here, but
    we can use a prime number generator instead. But first, install `line_profiler`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have installed the `line_profiler` module (and with that the `kernprof`
    command), let''s test `line_profiler`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'You might be wondering where the `profile` decorator is coming from. It originates
    from the `line_profiler` module, which is why we have to run the script with the
    `kernprof` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'As the command says, the results have been written to the `test_primes.py.lprof`
    file. So let''s look at the output of that with the `Time` column skipped for
    readability:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Wonderful output, isn't it? It makes it trivial to find the slow part within
    a bit of code. Within this code, the slowness is obviously originating from the
    loop, but within other code it might not be that clear.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This module can be added as an IPython extension as well, which enables the
    `%lprun` command within IPython. To load the extension, the `load_ext` command
    can be used from the IPython shell `%load_ext line_profiler`.
  prefs: []
  type: TYPE_NORMAL
- en: Improving performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Much can be said about performance optimization, but truthfully, if you have
    read the entire book up to this point, you know most of the Python-specific techniques
    to write fast code. The most important factor in application performance will
    always be the choice of algorithms, and by extension, the data structures. Searching
    for an item within `list` is almost always a worse idea than searching for an
    item in `dict` or `set`.
  prefs: []
  type: TYPE_NORMAL
- en: Using the right algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Within any application, the right choice of algorithm is by far the most important
    performance characteristic, which is why I am repeating it to illustrate the results
    of a bad choice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Checking whether an item is within a `list` is an `O(n)` operation and checking
    whether an item is within a `dict` is an `O(1)` operation. A huge difference when
    `n=1000000` obviously, in this simple test we can see that for 1 million items
    it's 500 times faster.
  prefs: []
  type: TYPE_NORMAL
- en: All other performance tips combined together might make your code twice as fast,
    but using the right algorithm for the job can cause a much greater improvement.
    Using an algorithm that takes `O(n)` time instead of `O(n^2)` time will make your
    code `1000` times faster for `n=1000,` and with a larger `n` the difference only
    grows further.
  prefs: []
  type: TYPE_NORMAL
- en: Global interpreter lock
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most obscure components of the CPython interpreter is the **global
    interpreter lock** (**GIL**), a **mutual exclusion lock** (**mutex**) required
    to prevent memory corruption. The Python memory manager is not thread-safe and
    that is why the GIL is needed. Without the GIL, multiple threads might alter memory
    at the same time, causing all sorts of unexpected and potentially dangerous results.
  prefs: []
  type: TYPE_NORMAL
- en: So what is the impact of the GIL in a real-life application? Within single-threaded
    applications it makes no difference whatsoever and is actually an extremely fast
    method for memory consistency. Within multithreaded applications however, it can
    slow your application down a bit, because only a single thread can access the
    GIL at a time. So if your code has to access the GIL a lot, it might benefit from
    some restructuring.
  prefs: []
  type: TYPE_NORMAL
- en: 'Luckily, Python offers a few other options for parallel processing: the `asyncio`
    module that we saw earlier and the `multiprocessing` library that we will see
    in [Chapter 13](ch13.html "Chapter 13. Multiprocessing – When a Single CPU Core
    Is Not Enough"), *Multiprocessing – When a Single CPU Core Is Not Enough*.'
  prefs: []
  type: TYPE_NORMAL
- en: Try versus if
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In many languages a `try/except` type of block incurs quite a performance hit,
    but within Python this is not the case. It's not that an `if` statement is heavy,
    but if you expect your `try/except` to succeed most of the time and only fail
    in rare cases, it's definitely a valid alternative. As always though, focus on
    readability and conveying the purpose of the code. If the intention of the code
    is clearer using an `if` statement, use the `if` statement. If `try/except` conveys
    the intention in a better way, use that.
  prefs: []
  type: TYPE_NORMAL
- en: Lists versus generators
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Evaluating code lazily using generators is almost always a better idea than
    calculating the entire dataset. The most important rule of performance optimization
    is probably that you shouldn't calculate anything you're not going to use. If
    you're not sure that you are going to need it, don't calculate it.
  prefs: []
  type: TYPE_NORMAL
- en: Don't forget that you can easily chain multiple generators, so everything is
    calculated only when it's actually needed. Do be careful that this won't result
    in recalculation though; `itertools.tee` is generally a better idea than recalculating
    your results completely.
  prefs: []
  type: TYPE_NORMAL
- en: String concatenation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You might have seen benchmarks saying that using `+=` is much slower than joining
    strings. At one point this made quite a lot of difference indeed. With Python
    3 however, most of the differences have vanished.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: There are still some differences of course, but they are so small that I recommend
    to simply ignore them and choose the most readable option instead.
  prefs: []
  type: TYPE_NORMAL
- en: Addition versus generators
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As is the case with string concatenation, once a significant difference now
    too small to mention.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: What does help though is letting Python handle everything internally using native
    functions, as can be seen in the last example.
  prefs: []
  type: TYPE_NORMAL
- en: Map versus generators and list comprehensions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once again, readability counts more than performance. There are a few cases
    where `map` is faster than list comprehensions and generators, but only if the
    `map` function can use a predefined function. As soon as you need to whip out
    `lambda,` it''s actually slower. Not that it matters much, since readability should
    be key anyhow, use generators or list comprehensions instead of `map`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the list comprehension is obviously quite a bit faster than
    the generator. In many cases I would still recommend the generator over the list
    comprehension though, if only because of the memory usage and the potential laziness.
    If for some reason you are only going to use the first 10 items, you're still
    wasting a lot of resources by calculating the full list of items.
  prefs: []
  type: TYPE_NORMAL
- en: Caching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have already covered the `functools.lru_cache` decorator in [Chapter 5](ch05.html
    "Chapter 5. Decorators – Enabling Code Reuse by Decorating"), *Decorators – Enabling
    Code Reuse by Decorating* but the importance should not be underestimated. Regardless
    of how fast and smart your code is, not having to calculate results is always
    better and that's what caching does. Depending on your use case, there are many
    options available. Within a simple script, `functools.lru_cache` is a very good
    contender, but between multiple executions of an application, the `cPickle` module
    can be a life saver as well.
  prefs: []
  type: TYPE_NORMAL
- en: If multiple servers are involved, I recommend taking a look at **Redis**. The
    Redis server is a single threaded in-memory server which is extremely fast and
    has many useful data structures available. If you see articles or tutorials about
    improving performance using Memcached, simply replace Memcached with Redis everywhere.
    Redis is superior to Memcached in every way and in its most basic form the API
    is compatible.
  prefs: []
  type: TYPE_NORMAL
- en: Lazy imports
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A common problem in application load times is that everything is loaded immediately
    at the start of the program, while with many applications this is actually not
    needed and certain parts of the application only require loading when they are
    actually used. To facilitate this, one can occasionally move the imports inside
    of functions so they can be loaded on demand.
  prefs: []
  type: TYPE_NORMAL
- en: 'While it''s a valid strategy in some cases, I don''t generally recommend it
    for two reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: It makes your code less clear; having all imports in the same style at the top
    of the file improves readability.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It doesn't make the code faster as it just moves the load time to a different
    part.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using optimized libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is actually a very broad tip, but useful nonetheless. If there''s a highly
    optimized library which suits your purpose, you most likely won''t be able to
    beat its performance without a significant amount of effort. Libraries such as
    `numpy`, `pandas`, `scipy`, and `sklearn` are highly optimized for performance
    and their native operations can be incredibly fast. If they suit your purpose,
    be sure to give them a try. Just to illustrate how fast `numpy` can be compared
    to plain Python, refer to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The `numpy` code does exactly the same as the Python code, except that it uses
    `numpy` arrays instead of Python lists. This little difference has made the code
    more than 25 times faster.
  prefs: []
  type: TYPE_NORMAL
- en: Just-in-time compiling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Just-in-time** (**JIT**) compiling is a method of dynamically compiling (parts)
    an application during runtime. Because there is much more information available
    at runtime, this can have a huge effect and make your application much faster.'
  prefs: []
  type: TYPE_NORMAL
- en: The `numba` package provides selective JIT compiling for you, allowing you to
    mark the functions that are JIT compiler compatible. Essentially, if your functions
    follow the functional programming paradigm of basing the calculations only on
    the input, then it will most likely work with the JIT compiler.
  prefs: []
  type: TYPE_NORMAL
- en: 'Basic example of how the `numba` JIT compiler can be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The use cases for these are limited, but if you are using `numpy` or pandas
    you will most likely benefit from `numba`.
  prefs: []
  type: TYPE_NORMAL
- en: Another very interesting fact to note is that `numba` supports not only CPU
    optimized execution but GPU as well. This means that for certain operations you
    can use the fast processor in your video card to process the results.
  prefs: []
  type: TYPE_NORMAL
- en: Converting parts of your code to C
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will see more about this in [Chapter 14](ch14.html "Chapter 14. Extensions
    in C/C++, System Calls, and C/C++ Libraries"), *Extensions in C/C++, System Calls,
    and C/C++ Libraries*, but if high performance is really required, then a native
    C function can help quite a lot. This doesn't even have to be that difficult.
    The Cython module makes it trivial to write parts of your code with performance
    very close to native C code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is an example from the Cython manual to approximate the value of
    pi:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: While there are some small differences such as `cdef` instead of `def` and type
    definitions for the values and parameters, the code is largely the same as regular
    Python would be, but certainly much faster.
  prefs: []
  type: TYPE_NORMAL
- en: Memory usage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far we have simply looked at the execution times and ignored the memory usage
    of the scripts. In many cases, the execution times are the most important, but
    memory usage should not be ignored. In almost all cases, CPU and memory are traded;
    a code either uses a lot of CPU or a lot of memory, which means that both do matter
    a lot.
  prefs: []
  type: TYPE_NORMAL
- en: Tracemalloc
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Monitoring memory usage used to be something that was only possible through
    external Python modules such as **Dowser** or **Heapy**. While those modules still
    work, they are largely obsolete now because of the `tracemalloc` module. Let''s
    give the `tracemalloc` module a try to see how easy memory usage monitoring is
    nowadays:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: You can easily see how every part of the code allocated memory and where it
    might be wasted. While it might still be unclear which part was actually causing
    the memory usage, there are options for that as well, as we will see in the following
    sections.
  prefs: []
  type: TYPE_NORMAL
- en: Memory profiler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `memory_profiler` module is very similar to `line_profiler` discussed earlier,
    but for memory usage instead. Installing it is as easy as `pip install memory_profiler`,
    but the optional `pip install psutil` is also highly recommended (and required
    in the case of Windows) as it increases your performance by a large amount. To
    test `line_profiler,` we will use the following script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we actually import the `memory_profiler` here although that is not
    strictly required. It can also be executed through `python3 -m memory_profiler
    your_scripts.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Even though everything runs as expected, you might be wondering about the varying
    amounts of memory used by the lines of code here. Why does `a` take `3.5 MiB`
    and `b` only `3.2 MiB`? This is caused by the Python memory allocation code; it
    reserves memory in larger blocks, which is subdivided and reused internally. Another
    problem is that `memory_profiler` takes snapshots internally, which results in
    memory being attributed to the wrong variables in some cases. The variations should
    be small enough to not make a large difference in the end, but some changes are
    to be expected.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This module can be added as an IPython extension as well, which enables the
    `%mprun` command within IPython. To load the extension, the `load_ext` command
    can be used from the IPython shell `%load_ext memory_profiler`. Another very useful
    command is `%memit` which is the memory equivalent of the `%timeit` command.
  prefs: []
  type: TYPE_NORMAL
- en: Memory leaks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The usage of these modules will generally be limited to the search for memory
    leaks. Especially the `tracemalloc` module has a few features to make that fairly
    easy. The Python memory management system is fairly simple; it just has a simple
    reference counter to see if an object is used. While this works great in most
    cases, it can easily introduce memory leaks when circular references are involved.
    The basic premise of a memory leak with leak detection code looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see how bad this code is actually leaking:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: In absolute memory usage the increase is not even that great, but it is definitely
    leaking a little. The first leak is negligible; at the last iteration we see an
    increase of 28 bytes which is next to nothing. The second leak however leaks a
    lot and peaks at a 18.3 megabyte increase. Those are memory leaks, the Python
    garbage collector (`gc`) is smart enough to clean circular references eventually
    but it won't clean them until a certain limit is reached. More about that soon.
  prefs: []
  type: TYPE_NORMAL
- en: 'Whenever you want to have a circular reference that does not cause memory leaks,
    the `weakref` module is available. It creates reference which don''t count towards
    the object reference count. Before we look at the `weakref` module, let''s take
    a look at the object references themselves through the eyes of the Python garbage
    collector (`gc`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'So let''s have a look at the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: As we can see here, until we manually call the garbage collector, the `Eggs`
    objects will stay in the memory. Even after explicitly deleting the objects. So
    does this mean you are always required to manually call `gc.collect()` to remove
    these references? Luckily that's not needed, as the Python garbage collector will
    automatically collect once the thresholds have been reached. By default, the thresholds
    for the Python garbage collector are set to `700, 10, 10` for the three generations
    of collected objects. The collector keeps track of all the memory allocations
    and deallocations in Python, and as soon as the number of allocations minus the
    number of deallocations reaches 700, the object is either removed if it's not
    referenced anymore or it is moved to the next generation if it still has a reference.
    The same is repeated for generation 2 and 3, albeit with the lower thresholds
    of 10.
  prefs: []
  type: TYPE_NORMAL
- en: 'This begs the question: where and when is it useful to manually call the garbage
    collector? Since the Python memory allocator reuses blocks of memory and only
    rarely releases it, for long running scripts the garbage collector can be very
    useful. That''s exactly where I recommend its usage: long running scripts in memory-strapped
    environments and specifically, right before you allocate a large amount of memory.'
  prefs: []
  type: TYPE_NORMAL
- en: 'More to the point however, the `gc` module can help you a lot when looking
    for memory leaks as well. The `tracemalloc` module can show you the parts that
    take the most memory in bytes but the `gc` module can help you find the most defined
    objects. Just be careful with setting the garbage collector debug settings such
    as `gc.set_debug(gc.DEBUG_LEAK)`; it returns a large amount of output even if
    you don''t reserve any memory yourself. Revisiting our `Spam` and `Eggs` script
    from earlier, let''s see where and how the memory is being used using the garbage
    collection module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is probably close to what you were already expecting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The large amount of `dict` objects is because of the internal state of the
    classes, but beyond that we simply see the `Eggs` objects just as we would expect.
    The `Spam` objects were properly removed by the garbage collector because they
    and all of the references were just removed. The `Eggs` objects couldn''t be removed
    because of the circular references. Now we will repeat the same example using
    the `weakref` module to see if it makes a difference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s see what remained this time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Nothing besides some standard built-in Python objects, which is exactly what
    we had hoped for. Be careful with weak references though, as they can easily blow
    up in your face if the referenced object has disappeared:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in one working reference and a dead one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Reducing memory usage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In general, memory usage probably won't be your biggest problem in Python, but
    it can still be useful to know what you can do to reduce memory usage. When trying
    to reduce memory usage, it's important to understand how Python allocates memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are four concepts which you need to know about within the Python memory
    manager:'
  prefs: []
  type: TYPE_NORMAL
- en: First we have the heap. The heap is the collection of all Python managed memory.
    Note that this is separate from the regular heap and mixing the two could result
    in corrupt memory and crashes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Second are the arenas. These are the chunks that Python requests from the system.
    These chunks have a fixed size of 256 KiB each and they are the objects that make
    up the heap.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Third we have the pools. These are the chunks of memory that make up the arenas.
    These chunks are 4 KiB each. Since the pools and arenas have fixed sizes, they
    are simple arrays.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fourth and last, we have the blocks. The Python objects get stored within these
    and every block has a specific format depending on the data type. Since an integer
    takes up more space than a character, for efficiency a different block size is
    used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we know how the memory is allocated, we can also understand how it
    can be returned to the operating system. Whenever an arena is completely empty,
    it can and will be freed. To increase the likelihood of this happening, some heuristics
    are used to maximize the usage of fuller arenas.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is important to note that the regular heap and Python heap are maintained
    separately as mixing them can result in corruption and/or crashing of applications.
    Unless you write your own extensions, you will probably never have to worry about
    manual memory allocation though.
  prefs: []
  type: TYPE_NORMAL
- en: Generators versus lists
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The most important tip is to use generators whenever possible. Python 3 has
    come a long way in replacing lists with generators already, but it really pays
    off to keep that in mind as it saves not only memory, but CPU as well when not
    all of that memory needs to be kept at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate the difference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: The `range()` generator takes such little memory that it doesn't even register,
    whereas the list of numbers takes `38.6 MiB`.
  prefs: []
  type: TYPE_NORMAL
- en: Recreating collections versus removing items
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One very important detail about collections in Python is that many of them
    can only grow; they won''t just shrink by themselves. To illustrate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: This is one of the most common memory usage mistakes made with lists and dictionaries.
    Besides recreating the objects, there is, of course, also the option of using
    generators instead so the memory is never allocated at all.
  prefs: []
  type: TYPE_NORMAL
- en: Using slots
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you've used Python for a long time you may have seen the `__slots__` feature
    of classes. It allows you to specify which fields you want to store in a class
    and it skips all the others by not implementing `instance.__dict__`. While this
    method does save a little bit of memory in your class definitions, I recommend
    against its usage as there are several downsides to using it. The most important
    one is that they make inheritance non-obvious (adding `__slots__` to a subclassed
    class that doesn't have `__slots__` has no effect). It also makes it impossible
    to modify class attributes on the fly and breaks `weakref` by default. And lastly,
    classes with slots cannot be pickled without defining a `__getstate__` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'For completeness however, here''s a demonstration of the slots feature and
    the difference in memory usage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'And the memory usage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: You might argue that this is not a fair comparison, since they both store a
    lot of data which skews the results. And you would indeed be right, because the
    "bare" comparison storing only `index` and nothing else gives `2 MiB` versus `4.5
    MiB`. But let's be honest, if you're not going to store data, then what's the
    point in creating class instances? That's why I recommend against the usage of
    `__slots__` and instead recommend the usage of tuples or `collections.namedtuple`
    if memory is that important. There is one more structure that's even more memory
    efficient, the `array` module. It stores the data in pretty much a bare memory
    array. Note that this is generally slower than lists and much less convenient
    to use.
  prefs: []
  type: TYPE_NORMAL
- en: Performance monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far we have seen how to measure and improve both CPU and memory performance,
    but there is one part we have completely skipped over. Performance changes due
    to external factors such as growing amounts of data are very hard to predict.
    In real life applications, bottlenecks aren't constant. They change all the time
    and code that was once extremely fast might bog down as soon as more load is applied.
  prefs: []
  type: TYPE_NORMAL
- en: Because of that I recommend implementing a monitoring solution that tracks the
    performance of anything and everything over time. The big problem with performance
    monitoring is that you can't know what will slow down in the future and what the
    cause is going to be. I've even had websites slow down because of Memcached and
    Redis calls. These are memory only caching servers that respond well within a
    millisecond which makes slowdowns highly unlikely, until you do over a 100 cache
    calls and the latency towards the cache server increases from 0.1 milliseconds
    to 2 milliseconds, and all of a sudden those 100 calls take 200 milliseconds instead
    of 10 milliseconds. Even though 200 milliseconds still sounds like very little,
    if your total page load time is generally below 100 milliseconds, that is all
    of a sudden an enormous increase and definitely noticeable.
  prefs: []
  type: TYPE_NORMAL
- en: To monitor performance and to be able to track changes over time and find the
    responsible components, I am personally a big fan of the Statsd statistic collection
    server together with the Graphite interface. Even though usability is a bit lacking,
    the result is a graphing interface which you can dynamically query to analyze
    when, where, and how your performance changed. To be able to use these you will
    have to send the metrics from your application towards the Statsd server. To do
    just that, I have written the Python-Statsd ([https://pypi.python.org/pypi/python-statsd](https://pypi.python.org/pypi/python-statsd))
    and Django-Statsd ([https://pypi.python.org/pypi/django-statsd](https://pypi.python.org/pypi/django-statsd))
    packages. These packages allow you to monitor your application from beginning
    to end, and in the case of Django you will be able to monitor your performance
    per application or view and within those see all of the components, such as the
    database, template, and caching layers. This way, you know exactly what is causing
    the slowdowns in your website (or application).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When it comes to performance, there is no holy grail, no single thing you can
    do to ensure peak performance in all cases. This shouldn't worry you however,
    as in most cases you will never need to tune the performance and if you do, a
    single tweak could probably fix your problem. You should be able to find performance
    problems and memory leaks in your code now which is what matters most, so just
    try to contain yourself and only tweak when it's actually needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most important outtakes from this chapter are:'
  prefs: []
  type: TYPE_NORMAL
- en: Test before you invest any effort. Making some functions faster seems like a
    great achievement but it is only rarely needed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing the correct data structure/algorithm is much more effective than any
    other performance optimization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Circular references drain the memory until the garbage collector starts cleaning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Slots are not worth the effort.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next chapter will discuss multiprocessing, a library which makes it trivial
    to employ multiple processors for your scripts. If you can't squeeze any more
    performance out of your script, multiprocessing might be your answer, as every
    (remote?) CPU core can make your script faster.
  prefs: []
  type: TYPE_NORMAL
