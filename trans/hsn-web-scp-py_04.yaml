- en: Python and the Web – Using urllib and Requests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From the previous chapter, we now have an idea about what web scraping is, what
    the core development technologies that exist are, and where or how we can plan
    to find the information we are looking for.
  prefs: []
  type: TYPE_NORMAL
- en: Web scraping requires tools and techniques to be implemented and deployed using
    scripts or programs. The Python programming language consists of a huge set of
    libraries that are fit for interacting with the web and for scraping purposes. In
    this chapter, we will communicate with web resources using Python; we'll also explore
    and search for the contents to be extracted from the web.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will also provide a detailed overview of using Python libraries
    such as `requests` and `urllib`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, we will learn about the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting Python and its required libraries, `requests` and `urllib`, to load
    URLs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A detailed overview of `requests` and `urllib`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing HTTP methods (`GET`/`POST`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We assume that you have some prior basic experience of using the Python programming
    language. If not, then please refer to Python tutorials from W3schools ([https://www.w3schools.com/python/default.asp](https://www.w3schools.com/python/default.asp)),
    Python course ([https://python-course.eu/](https://python-course.eu/)), or search
    Google for *learn Python programming*.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will be using Python 3.7.0, which has been installed on the Windows operating
    system. There are plenty of choices for code editors; choose one that is convenient
    to use and deal with the libraries that are used in this chapter's code examples.
    We will be using PyCharm (Community Edition [https://www.jetbrains.com/pycharm/download/download-thanks.html?platform=windows&code=PCC](https://www.jetbrains.com/pycharm/download/download-thanks.html?platform=windows&code=PCC)) from
    JetBrains and Python IDLE ([https://www.python.org/downloads/](https://www.python.org/downloads/)) side
    by side.
  prefs: []
  type: TYPE_NORMAL
- en: 'To follow along with this chapter, you will need to install the following applications:'
  prefs: []
  type: TYPE_NORMAL
- en: Python 3.7.* or the latest version that's appropriate for your OS: [https://www.python.org/downloads/](https://www.python.org/downloads/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `pip` Python package management: [https://packaging.python.org/tutorials/installing-packages/](https://packaging.python.org/tutorials/installing-packages/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Either Google Chrome or Mozilla Firefox
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JetBrains PyCharm or Visual Studio Code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Python libraries that are required for this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`requests`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`urllib`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code files for this chapter are available online on GitHub: [https://github.com/PacktPublishing/Hands-On-Web-Scraping-with-Python/tree/master/Chapter02](https://github.com/PacktPublishing/Hands-On-Web-Scraping-with-Python/tree/master/Chapter02).
  prefs: []
  type: TYPE_NORMAL
- en: Accessing the web with Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Python is a programming language that's used to code various types of applications,
    from simple scripts to AI algorithms and web frameworks. We will be writing scripts
    in Python to access the URLs that we are interested in from a data extraction
    or scraping perspective.
  prefs: []
  type: TYPE_NORMAL
- en: A number of Python libraries exist for HTTP communication and web-related purposes
    (including `http`, `cookielib`, `urllib`, `requests`, `html`, `socket`, `json`,
    `xmlrpc`, `httplib2`, and `urllib3`). We will explore and use a few of them that
    have been praised by the programmers' community for HTTP access or client-server
    communication. The `urllib` and `requests` Python modules are the ones we are
    interested in using. These libraries possess various functions that can be used
    to communicate with the web using Python and deal with HTTP requests and responses.
  prefs: []
  type: TYPE_NORMAL
- en: In order to start a few coding tasks and explore the Python-based modules straightaway,
    let's verify that we have installed all the Python resources we want before moving
    on.
  prefs: []
  type: TYPE_NORMAL
- en: Setting things up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is assumed that Python has been preinstalled. If not, please visit [https://www.python.org/downloads/](https://www.python.org/downloads/) and [https://www.python.org/download/other/](https://www.python.org/download/other/)
    for the latest Python version for your operating system. Regarding the general
    setup and installation procedure, please visit [https://realpython.com/installing-python/](https://realpython.com/installing-python/) to
    find out how to install Python on your chosen platform. We will be using the Windows
    operating system here.
  prefs: []
  type: TYPE_NORMAL
- en: To verify that we have all the required tools available, let's check that Python
    and `pip` are installed and are up to date.
  prefs: []
  type: TYPE_NORMAL
- en: '`pip` package management system is used to install and manage software packages
    written in Python. More on installing Python packages and `pip` can be found at [https://packaging.python.org/tutorials/installing-packages/](https://packaging.python.org/tutorials/installing-packages/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be using Python 3.7 on the Windows operating system. Press Windows
    + *R* to open the Run box and type `cmd` to get the command-line interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/c9e94300-8d7a-4f84-875d-c926ec32cce7.png)'
  prefs: []
  type: TYPE_IMG
- en: Opening the command-line interface on the Windows operating system
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, move to your root directory and type the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command will provide us with the Python version that we currently
    have on our system. Let''s get some information on the `pip` version that we are
    using. The following command will display the current `pip` version, as well as
    its location:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We are happy to proceed after seeing the preceding responses. If you encounter
    an error stating Application not found or `not recognized as an internal or external
    command`, then we need to reinstall Python or check for the proper drive that
    was used during installation.
  prefs: []
  type: TYPE_NORMAL
- en: It's always advisable to check for the system and library version and keep them
    updated unless a specific version is required.
  prefs: []
  type: TYPE_NORMAL
- en: 'To update `pip` to its latest release, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'You can verify the libraries we wish to use, that is, `requests` and `urllib`,
    either from the command line or by importing the Python IDE and getting details
    on the package using the `help()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the preceding code, we are trying to install `requests`, but the
    command returns `Requirement already satisfied`. The `pip` command checks for
    an existing installation on the system before installing a fresh library.
  prefs: []
  type: TYPE_NORMAL
- en: In the following code block, we will be using the Python IDE to import `urllib`.
    We'll view its details using Python's built-in `help()` method*.*
  prefs: []
  type: TYPE_NORMAL
- en: 'The `>>>` symbol in code represents use of the Python IDE; it accepts the code
    or instructions and displays the output on the next line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Similar to the previous code, lets import `requests` using the Python IDE:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'If we import `urllib `or `requests` and these libraries don''t exist, the result
    will throw an error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'For missing modules or in the previous case, install the module first; use
    `pip` as follows to install or upgrade. You can install it from your command line,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also upgrade the module version using the `--upgrade` argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Loading URLs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we've confirmed the required libraries and system requirements, we
    will proceed with loading the URLs. While looking for contents from a URL, it
    is also necessary to confirm and verify the exact URL that has been chosen for
    the required content. Contents can be found on single web pages or scattered across
    multiple pages, and it might not always be the HTML sources we are looking for.
  prefs: []
  type: TYPE_NORMAL
- en: We will load some URLs and explore the content using a couple of tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Before loading URLs using Python script, it's also advisable to verify the URLs
    are working properly and contain the detail we are looking for, using web browsers.
    Developer tools can also be used for similar scenarios, as discussed in [Chapter
    1](af7787bb-7fcf-4101-8680-9bad14bf22e1.xhtml), *Web Scraping Fundamentals*, in
    the *Developer tools* section.
  prefs: []
  type: TYPE_NORMAL
- en: '**Task 1**: To view data related to the listings of the most popular websites
    from Wikipedia. We will identify data from the Site, Domain, and Type columns
    in the page source.'
  prefs: []
  type: TYPE_NORMAL
- en: We will follow the steps at the following link to achieve our task (a data extraction-related
    activity will be done in [Chapter 3](9e1ad029-726f-4ed3-897a-c68bcd61f71e.xhtml),
    *Using LXML, XPath and CSS Selectors*): [https://en.wikipedia.org/wiki/List_of_most_popular_websites](https://en.wikipedia.org/wiki/List_of_most_popular_websites).
  prefs: []
  type: TYPE_NORMAL
- en: Search Wikipedia for the information we are looking for. The preceding link
    can be easily viewed in a web browser. The content is in tabular format (as shown
    in the following screenshot), and so the data can be collected by repeatedly using
    the select, copy, and paste actions, or by collecting all the text inside the
    table.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, such actions will not result in the content that we are interested
    in being in a desirable format, or it will require extra editing and formatting
    tasks being performed on the text to achieve the desired result. We are also not
    interested in the page source that''s obtained from the browser:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ff896cba-9530-485c-bdd9-130ac843833d.png)'
  prefs: []
  type: TYPE_IMG
- en: Page from Wikipedia, that is, https://en.wikipedia.org/wiki/List_of_most_popular_websites
  prefs: []
  type: TYPE_NORMAL
- en: 'After finalizing the link that contains the content we require, let''s load
    the link using Python. We are making a request to the link and willing to see
    the response returned by both libraries, that is, `urllib` and `requests`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use `urllib`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `urlopen()` function from `urllib.request` has been passed with the selected
    URL or request that has been made to the URL and `response` is received, that
    is, `HTTPResponse`. `response` that's received for the request made can be read
    using the `read()` method.
  prefs: []
  type: TYPE_NORMAL
- en: '2\. Now, let''s use `requests`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Here, we are using the `requests` module to load the page source, just like
    we did using `urllib`. `requests` with the `get()` method, which accepts a URL
    as a parameter. The `response` type for both examples has also been checked.
  prefs: []
  type: TYPE_NORMAL
- en: The output that's displayed in the preceding code blocks has been shortened.
    You can find the code files for this at [https://github.com/PacktPublishing/Hands-On-Web-Scraping-with-Python](https://github.com/PacktPublishing/Hands-On-Web-Scraping-with-Python).
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding examples, the page content—or the `response` object—contains
    the details we were looking for, that is, the Site, Domain, and Type columns.
  prefs: []
  type: TYPE_NORMAL
- en: We can choose any one library to deal with the HTTP request and response. Detailed
    information on these two Python libraries with examples is provided in the next
    section, *URL handling and operations with urllib and requests*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s have a look at the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/6ea4c570-0bf3-4c2b-b970-a0978b106125.png)'
  prefs: []
  type: TYPE_IMG
- en: Wikipedia.com page content, viewed using Python libraries
  prefs: []
  type: TYPE_NORMAL
- en: Further activities like processing and parsing can be applied to content like
    this in order to extract the required data. More details about further processing
    tools/techniques and parsing can be found in [Chapter 3](9e1ad029-726f-4ed3-897a-c68bcd61f71e.xhtml),
    *Using LXML, XP**ath, and CSS Selectors,* [Chapter 4](30c30342-63a5-4452-9f61-a05a2e69e256.xhtml),
    *Scraping Using pyquery – a Python Library*, and [Chapter 5](5869ee86-6c67-4e6f-8151-61093795d94f.xhtml),
    *Web Scraping Using Scrapy an**d Beautiful Soup*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Task 2**: Load and save the page content from [https://www.samsclub.com/robots.txt](https://www.samsclub.com/robots.txt)
    and [https://www.samsclub.com/sitemap.xml](https://www.samsclub.com/sitemap.xml)
    using `urllib` and `requests`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally, websites provide files in their root path (for more information
    on these files, please refer to [Chapter 1](af7787bb-7fcf-4101-8680-9bad14bf22e1.xhtml),
    *Web Scraping Fundamentals*, the *Data finding techniques for the web* section):'
  prefs: []
  type: TYPE_NORMAL
- en: '`robots.txt`: This contains information for the crawler, web agents, and so
    on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sitemap.xml`: This contains links to recently modified files, published files,
    and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'From *Task 1*, we were able to load the URL and retrieve its content. Saving
    the content to local files using libraries methods and using file handling concepts
    will be implemented in this task. Saving content to local files and working on
    content with tasks like parsing and traversing can be really quick and even reduce
    network resources:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load and save the content from [https://www.samsclub.com/robots.txt](https://www.samsclub.com/robots.txt)
    using `urllib`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The `urlretrieve()` function, that is, `urlretrieve(url, filename=None, reporthook=None,
    data=None)`, from `urllib.request` returns a tuple with the filename and HTTP
    headers. You can find this file in the `C:\\Users..Temp` directory if no path
    is given; otherwise, the file will be generated in the current working directory
    with the name provided to the `urlretrieve()` method as the second argument. This
    was `testrobots.txt` in the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we are reading the URL and writing the content found
    using a file handling concept.
  prefs: []
  type: TYPE_NORMAL
- en: 'Load and save the content from [https://www.samsclub.com/sitemap.xml](https://www.samsclub.com/sitemap.xml) using
    `requests`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In both cases, we were able to find the content from the respective URL and
    save it to individual files and locations. The contents from the preceding code
    was found as bytes literals, for example, `b'<!DOCTYPE …` or `b'<?xml`. Page content
    can also be retrieved in a text format, such as `requests.get(link).text`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the `decode()` method to convert bytes into a string and the `encode()` method
    to convert a string into bytes, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Identifying a proper character set or `charset` is important when dealing with
    various domains and type of documents. To identify a proper `charset` encoding
    type, we can seek help from the page source for the `<meta>` tag by using `content-type`
    or `charset`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `<meta>` tag with the `charset` attribute, that is, `<meta charset="utf-8"/>`, is
    identified from the page source, as shown in the following screenshot (or `<meta
    http-equiv="content-type" content="text/html; charset=utf-8">`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/a224b37e-d00d-44a2-a460-7fcb132b97eb.png)'
  prefs: []
  type: TYPE_IMG
- en: Identifying charset from the document response or page source
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, the content for `<meta http-equiv="content-type" content="text/html;
    charset=utf-8">` can be obtained from the response header, as highlighted in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/5b46e1f3-ee12-4967-8388-4b10b4f82ad1.png)Identifying charset through
    the browser DevTools, Network panel, Headers tab, and response headers'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using Python code, we can find `charset` in the HTTP header:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '`charset` that was identified will be used to encode and decode with `requests.get(link).content.decode(''utf-8'')`.'
  prefs: []
  type: TYPE_NORMAL
- en: Python 3.0 uses the concepts of *text* and (binary) *data* instead of Unicode
    strings and 8-bit strings. All text is Unicode; however, *encoded* Unicode is
    represented as binary data. The type that's used to hold text is `str` ([https://docs.python.org/3/library/stdtypes.html#str](https://docs.python.org/3/library/stdtypes.html#str)),
    and the type that's used to hold data is bytes ([https://docs.python.org/3/library/stdtypes.html#bytes](https://docs.python.org/3/library/stdtypes.html#bytes)). For
    more information on Python 3.0, please visit [https://docs.python.org/3/whatsnew/3.0.html](https://docs.python.org/3/whatsnew/3.0.html).
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we set up and verified our technical requirements, and also
    explored URL loading and content viewing. In the next section, we will explore
    Python libraries to find some useful functions and their attributes.
  prefs: []
  type: TYPE_NORMAL
- en: URL handling and operations with urllib and requests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For our primary motive of extracting data from a web page, it's necessary to
    work with URLs. In the examples we've seen so far, we have noticed some pretty
    simple URLs being used with Python to communicate with their source or contents.
    The web scraping process often requires the use of different URLs from various
    domains that do not exist in the same format or pattern.
  prefs: []
  type: TYPE_NORMAL
- en: Developers might also face many cases where there will be a requirement for
    URL manipulation (altering, cleaning) to access the resource quickly and conveniently.
    URL handling and operations are used to set up, alter query parameters, or clean
    up unnecessary parameters. It also passes the required request headers with the
    appropriate values and identification of the proper HTTP method for making requests.
    There will be many cases where you will find URL-related operations that are identified
    using browser DevTools or the Network panel.
  prefs: []
  type: TYPE_NORMAL
- en: The `urllib `and `requests` Python libraries, which we will be using throughout
    this book, deal with URL and network-based client-server communication. These
    libraries provide various easy to use functions and attributes, and we will be
    exploring a few important ones.
  prefs: []
  type: TYPE_NORMAL
- en: urllib
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `urllib` library is a standard Python package that collects several modules
    to work with HTTP-related communication models. Modules inside `urllib `are specially
    designed and contain functions and classes that deal with various types of client-server
    communication.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly named packages also exist, like `urllib2`, an extensible library,
    and `urllib3`, a powerful HTTP client that addresses missing features from Python
    standard libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two of the most important `urllib` modules that deal with URL requests and
    responses are as follows. We will be using these modules in this and upcoming
    chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`urllib.request`: Used for opening and reading URLs and requesting or accessing
    network resources (cookies, authentication, and so on)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`urllib.response`: This module is used to provide a response to the requests
    that are generated'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are a number of functions and public attributes that exist to handle request
    information and process response data that's relevant to HTTP requests, such as
    `urlopen()`, `urlretrieve()`, `getcode()`, `getheaders()`, `getheader()`, `geturl()`, `read()`,
    `readline()`, and many more.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use Python''s built-in `dir()` function to display a module''s content,
    such as its classes, functions, and attributes, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The `urlopen()` function accepts a URL or an `urllib.request.Request` object,
    such as `requestObj`, and returns a response through the `urllib.response` `read()`
    function, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The object types that are returned are different in the case of `linkRequest` and
    `requestObj` from the `urlopen()` function and class request, respectively. The `linkResponse` and `requestObjResponse` objects
    were also created, which holds the `urllib.response` information of the `read()` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally, `urlopen()` is used to read a response from the URL, while `urllib.request.Request`
    is used to send extra arguments like `data` or `headers`, and even to specify
    the HTTP method and retrieve a response. It can be used as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`urllib.request.Request(url, data=None, headers={}, origin_req_host=None, unverifiable=False,
    method=None)`'
  prefs: []
  type: TYPE_NORMAL
- en: '`urllib.response` and its functions, such as `read()` and `readline()`, are
    used with the `urllib.request` objects.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If the request that was made was successful and received a response from the
    proper URL, we can check the HTTP status code, the HTTP method that was used,
    as well as the returned URL to view a description:'
  prefs: []
  type: TYPE_NORMAL
- en: '`getcode()` returns a HTTP status code. The same result can also be achieved
    using the `code` and `status` public attributes, as shown in the following code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '`geturl()` returns current the URL. It is sometimes handy to verify whether
    any redirection occurred. The `url` attribute can be used for a similar purpose:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '**`_method`** returns a HTTP method; `GET` is the default response:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '`getheaders()` returns a list with tuples that contains HTTP headers. As we
    can see from the following code, we can determine values regarding cookie, content
    type, date, and so on from the output:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Individual request-based headers can also be retrieved when `getheader()` is
    passed with desired header element, as shown in the following code. Here, we can
    see we can obtain the value for the Content-Type header. The same result can also
    be achieved using the `info()` function:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We have used code blocks and found the output that's relevant to our request
    and response. Web browsers also allow us to trace request/response-related information
    using browser DevTools (browser-based developer tools).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot displays the Network panel and the Doc tab, which
    includes the Headers option. This contains various sections, such as General,
    Response Headers, and Request Headers. Basic request and response-related information
    can be found inside the Headers option:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/b497f7dc-d4f4-4734-93d2-a91aa064ef87.png)'
  prefs: []
  type: TYPE_IMG
- en: Network panel and Document tab with General and Request header information
  prefs: []
  type: TYPE_NORMAL
- en: 'Note `urllib.error` deals with the exceptions raised by `urllib.request`. Exceptions
    like `URLError` and `HTTPError` can be raised for a request.The following code
    demonstrates the use of `urllib.error`:'
  prefs: []
  type: TYPE_NORMAL
- en: Exception handling deals with error handling and management in programming.
    Code that uses exception handling is also considered an effective technique and
    is often prescribed to adapt.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '`urllib.parse` is used to encode/decode request(data) or links, add/update
    headers, and analyze, parse, and manipulate URLs. Parsed URL strings or objects
    are processed with `urllib.request`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, `urlencode()`, `urlparse()`, `urljoin()`, `urlsplit()`, `quote_plus()`
    are a few important functions that are available in `urllib.parse`, as shown in
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The `urlsplit()` function from `urllib.parse` splits the URL that''s passed
    into the `namedtuple` object. Each name in tuple identifies parts of the URL.
    These parts can be separated and retrieved in other variables and used as needed.
    The following code implements `urlsplit()` for `amazonUrl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `urlparse()` function from `urllib.parse` results in the `ParseResult` object.
    It differs in terms of the parameters (`params` and `path`) that are retrieved
    in he URL compared to `urlsplit()`. The following code prints the object from
    `urlparse()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s confirm the differences between `urlparse()` and `urlsplit()`. The `localUrl` that''s
    created is parsed with both `urlsplit()` and `urlparse()`. `params` is only available
    with `urlparse()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Basically, `urllib.request.Request` accepts data and headers-related information,
    and `headers` can be assigned to an object using `add_header()`*;* for example,
    `object.add_header('host','hostname')` or `object.add_header('referer','refererUrl')`.
  prefs: []
  type: TYPE_NORMAL
- en: In order to request `data`, `Query Information`, or `URL arguments` need to
    be used as key-value pair of information that are appended to the desired URL.
    Such a URL is usually processed with the HTTP GET method. Query information that's
    passed to the request object should be encoded using `urlencode()`.
  prefs: []
  type: TYPE_NORMAL
- en: '`urlencode()` ensures that arguments comply with the W3C standard and are accepted
    by the server. `parse_qs()` parses percent-encoded query strings to the Python
    dictionary. The following code demonstrates an example of using `urlencode()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'You may also need to encode the special characters in a URL before processing
    the request to the server:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that `urllib.parse` contains the `quote()`, `quote_plus()`, and `unquote()`
    functions, which permit error-free server requests:'
  prefs: []
  type: TYPE_NORMAL
- en: '`quote()` is generally applied to the URL path (listed with `urlsplit()` or `urlparse()`)
    or queried with reserved and special characters (defined by RFC 3986) before it''s
    passed to `urlencode()` to ensure that the server''s acceptable. Default encoding
    is done with `UTF-8`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`quote_plus()` also encodes special characters, spaces, and the URL separator,
    `/`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unquote()` and `unquote_plus()` are used to revert the encoding that''s applied
    by using `quote()` and `quote_plus()`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These functions are demonstrated in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The `urljoin()` function from `urllib.parse` helps obtain the URL from the
    provided arguments, as demonstrated in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '`urllib.robotparser`, as its name suggests, helps parse `robots.txt` and identifies
    agent-based rules. Please refer to [Chapter 1](af7787bb-7fcf-4101-8680-9bad14bf22e1.xhtml),
    *Web Scraping Fundamentals*, the *Data finding techniques for the web *section,
    for more detailed information on `robots.txt`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see in the following code, `par`, which is an object of `RobotFileParser`,
    can be used to set a URL via the `set_url()` function. It can also read contents
    with the `read()` function. Functions such as `can_fetch()` can return a Boolean answer
    for the evaluated condition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, `https://www.samsclub.com/friend` returns `False` when passed
    with the `can_fetch()` function, thus satisfying the `Disallow: /friend` directives
    found in `robots.txt`. Similarly, `https://www.samsclub.com/category` returns
    `True` as there are no listed directives that restrict the category URL.'
  prefs: []
  type: TYPE_NORMAL
- en: However, there are some limitations to using `urllib.request`. Connection-based
    delays can occur while using functions like `urlopen()` and `urlretrieve()`. These
    functions return raw data and need to be converted into the required type for
    the parser before they can be used in the scraping process.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying threads, or threading, is considered an effective technique when dealing
    with HTTP requests and responses.
  prefs: []
  type: TYPE_NORMAL
- en: requests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`requests` HTTP Python library released in 2011 and is one of the most renowned
    HTTP libraries for developers in recent times.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Requests is an elegant and simple HTTP library for Python, built for human
    beings*. (source: [https://2.python-requests.org/en/master/](https://2.python-requests.org/en/master/)).'
  prefs: []
  type: TYPE_NORMAL
- en: More information on `requests` can be found at [http://docs.python-requests.org/en/master/](http://docs.python-requests.org/en/master/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Compared to other HTTP libraries in Python, `requests` is rated highly in terms
    of its functioning capability with HTTP. A few of its capabilities are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Short, simple, and readable functions and attributes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to various HTTP methods (GET, POST, and PUT, to name a few)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gets rid of manual actions, like encoding form values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processes query strings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Custom headers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Session and cookie processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deals with JSON requests and content
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Proxy settings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploys encoding and compliance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: API-based link headers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raw socket response
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Timeouts and more...
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will be using the `requests` library and accessing some of its properties.
    The `get()` function from `requests` is used to send a GET HTTP request to the
    URL provided. The object that''s returned is of the `requests.model.Response`
    type, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The `requests` library also supports HTTP requests such as `PUT`, `POST`, `DELETE`,
    `HEAD`, and `OPTIONS` using the `put()`, `post()`, `delete()`, `head()`, and `options()` methods,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some `requests` attributes, along with a short explanation
    of each:'
  prefs: []
  type: TYPE_NORMAL
- en: '`url` outputs the current URL'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The HTTP status code is found using `status_code`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`history` is used to track redirection:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also obtain some details that are found when we use developer tools,
    such as HTTP Header, Encoding, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '`headers` returns response-related HTTP headers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`requests.header` returns request-related HTTP headers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoding` displays the `charset` that''s obtained from the content:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Page or response content can be retrieved using the `content` in bytes, whereas
    `text` returns a `str` string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Furthermore, `requests` also returns a `raw` socket response from the server
    by using the `stream` argument in a `get()` request. We can read a raw response
    using the `raw.read()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: A raw response that's received using the `raw` attribute is raw bytes of characters
    that haven't been transformed or automatically decoded.
  prefs: []
  type: TYPE_NORMAL
- en: '`requests` handles JSON data very effectively with its built-in decoder. As
    we can see, URLs with JSON content can be parsed with `requests` and used as required:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Note that, `requests` uses `urllib3` for session and for raw socket response. At
    the time of writing, `requests` version 2.21.0 was available.
  prefs: []
  type: TYPE_NORMAL
- en: Crawling the script might use any of the mentioned or available HTTP libraries
    to make web-based communications. Most of the time, functions and attributes from
    multiple libraries will make this task easy. In the next section, we will be using
    the `requests` library to implement the HTTP (`GET`/`POST`) methods.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing HTTP methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Generally, web-based interaction or communication between the web page and
    the user or reader is achieved as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The user or reader can access the web page to read or navigate through information
    that's presented to them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The user or reader can also submit certain information to the web page using
    the HTML form, such as by searching, logging in, user registration, password recovery,
    and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we will be using the `requests` Python library to implement
    common HTTP methods (`GET` and `POST`) that execute the HTTP-based communication
    scenario we listed previously.
  prefs: []
  type: TYPE_NORMAL
- en: GET
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A command way to request information is to use safe methods since the resource
    state is not altered. The `GET` parameters, also known as query strings, are visible
    in the URL. They are appended to the URL using `?` and are available as `key=value`
    pairs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally, a processed URLs without any specified HTTP methods are normally
    GET requests. A request that''s made using GET can be cached and bookmarked. There
    are also length restrictions while making a `GET` request. Some examples URLs
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.test-domain.com](http://www.test-domain.com)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://www.test-domain.com/indexes/](http://www.test-domain.com/indexes/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://www.test-domain.com/data file?id=1345322&display=yes](http://www.test-domain.com/data%20file?id=1345322&display=yes)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the preceding sections, requests were made to normal URLs such as `robots.txt` and `sitemap.xml`,
    both of which use the HTTP `GET` method. The `get()` function from `requests` accepts
    URLs, parameters, and headers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: POST
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These are known as secure requests that are made to a source. The requested
    resource state can be altered. Data that's posted or sent to the requested URL
    is not visible in the URL; instead, it's transferred to the request body. A request
    that's made using `POST` isn't cached or bookmarked and has no restrictions in
    terms of length.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, a simple HTTP request and response service<q> (</q>source:
    [http://httpbin.org/](http://httpbin.org/)) has been used to make a `POST` request.'
  prefs: []
  type: TYPE_NORMAL
- en: '`pageUrl` accepts data to be posted, as defined in `params` to `postUrl`. Custom
    headers are assigned as `headers`. The `post()` function from the `requests` library
    accepts URLs, data, and headers, and returns a response in JSON format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous code will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'For the `POST` request we attempted, we can find detailed information regarding
    Request Headers, Response Headers, HTTP Status, and `POST` data (params) using
    the DevTools Network panel, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ade91570-74a7-4bc6-99e7-708ac1ddb71e.png)'
  prefs: []
  type: TYPE_IMG
- en: POST data submitted and found as form data in the DevTools Network panelIt's
    always beneficial to learn and detect the request and response sequences that
    are made with URLs through the browser and the available DevTools.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about using Python libraries to make a request to
    a web resource and collect the response that was returned. This chapter's main
    objective was to demonstrate core features that are available through the `urllib`
    and `requests` Python libraries, plus exploring page contents that are found in
    various formats.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn and use a few techniques to identify and
    extract data from web contents.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: urllib:** [https://docs.python.org/3/library/urllib.html](https://docs.python.org/3/library/urllib.html) **
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Requests: [https://2.python-requests.org/en/master/](https://2.python-requests.org/en/master/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: urllib3 [https://urllib3.readthedocs.io/en/latest/index.html](https://urllib3.readthedocs.io/en/latest/index.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HTTP methods (GET/POST): [https://www.w3schools.com/tags/ref_httpmethods.asp](https://www.w3schools.com/tags/ref_httpmethods.asp)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing Python packages: [https://packaging.python.org/tutorials/installing-packages/](https://packaging.python.org/tutorials/installing-packages/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are DevTools? [https://developer.mozilla.org/en-US/docs/Learn/Common_questions/What_are_browser_developer_tools](https://developer.mozilla.org/en-US/docs/Learn/Common_questions/What_are_browser_developer_tools)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HTTP request and response service: [http://httpbin.org/](http://httpbin.org/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
