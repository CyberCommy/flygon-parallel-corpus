- en: Designing Lock-Based and Mutex-Free Concurrent Data Structures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will analyze the detailed process of designing and implementing
    two common types of data structure in concurrent programming: lock-based and mutex-free.
    The principal differences between the two data structures, as well as their respective
    usages in concurrent programming, will be discussed. Throughout the chapter, an
    analysis of the trade-off between the accuracy and speed of concurrent programs
    is also supplied. Through this analysis, readers will be able to apply the same
    trade-off analysis for their own concurrent applications.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Common problems with lock-based data structures, and how to address them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A detailed analysis of how to implement a lock-based data structure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The idea behind mutex-free data structures, along with their advantages and
    disadvantages, as compared to lock-based data structures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A detailed analysis of how to implement a mutex-free data structure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is a list of prerequisites for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Ensure that you have Python 3 installed on your computer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Download the GitHub repository at [https://github.com/PacktPublishing/Mastering-Concurrency-in-Python](https://github.com/PacktPublishing/Mastering-Concurrency-in-Python)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Throughout this chapter, we will be working with the subfolder named `Chapter16`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check out the following video to see the Code in Action: [http://bit.ly/2QhT3MS](http://bit.ly/2QhT3MS)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lock-based concurrent data structures in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous chapters that covered the usage of locks, you learned that locks
    don't lock anything; an insubstantial locking mechanism implemented on a data
    structure does not actually prevent external programs from accessing the data
    structure at the same time, by simply bypassing the lock imposed. One solution
    to this problem is to embed the lock into the data structure, so that it is impossible
    for the lock to be ignored by external entities.
  prefs: []
  type: TYPE_NORMAL
- en: In the first section of this chapter, we will consider the theories behind the
    preceding specific use of locks and lock-based data structures. Specifically,
    we will analyze the process of designing a concurrent counter that can be safely
    executed by different threads, using locks (or mutex) as the synchronization mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: LocklessCounter and race conditions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, let's simulate the problem encountered with a naive, lockless implementation
    of a counter class in a concurrent program. If you have already downloaded the
    code for this book from the GitHub page, go ahead and navigate to the `Chapter16`
    folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us take a look at the `Chapter16/example1.py` file—specifically, the implementation
    of the `LocklessCounter` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This is a simple counter that has an attribute called `value`, which contains
    the current value of the counter, assigned with `0` when the counter instance
    is first initialized. The `increment()` method of the class takes in an argument, `x`,
    and increases the current value of the calling `LocklessCounter` object by `x`.
    Notice that we are creating a small delay inside the `increment()` function, between
    the process of computing the new value of the counter and the process of assigning
    that new value to the counter object. The class also has a method called `get_value()`,
    which returns the current value of the calling counter.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is quite obvious why this implementation of the `LocklessCounter` class
    can create a race condition in a concurrent program: while a thread is in the
    middle of incrementing a shared counter, another thread also might access the
    counter to execute the `increment()` method, and the change to the counter value
    made by the first thread might be overwritten by the one made by the second thread.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a refresher, the following diagram shows how a race condition can occur
    in situations where multiple processes or threads access and mutate a shared resource
    at the same time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/c9c5bd97-d645-4f09-ac3e-f925f29357b6.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram of a race condition
  prefs: []
  type: TYPE_NORMAL
- en: 'To simulate this race condition, in our main program we are including a total
    of three threads, to increment a shared counter by 300 times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `concurrent.futures` module offers us an easy and high-level way to schedule
    a task through a pool of threads. Specifically, after initializing a shared counter
    object, we declare the variable `executor` as a pool of three threads (use a context
    manager), and that executor calls the `increment()` method on the shared counter
    300 times, each time incrementing the value of the counter by `1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'These tasks are to be executed among the three threads in the pool, using the
    `map()` method of the `ThreadPoolExecutor` class. At the end of the program, we
    simply print out the final value of the counter object. The following code shows
    my own output after running the script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: While it is possible to obtain a different value for the counter when executing
    the script on your own system, it is extremely unlikely that the final value of
    the counter will actually be 300, which is the correct value. Additionally, if
    you were to run the script over and over again, it would be possible to obtain
    different values for the counter, illustrating the non-deterministic nature of
    the program. Again, as some threads were overwriting the changes made by other
    threads, some increments got lost during the execution, resulting in the fact
    that the counter was only successfully incremented `101` times, in this case.
  prefs: []
  type: TYPE_NORMAL
- en: Embedding locks in the data structure of the counter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The goal of a good lock-based concurrent data structure is to have its locks
    internally implemented within its class attributes and methods, so that external
    functions and programs cannot bypass those locks and access a shared concurrent
    object simultaneously. For our counter data structure, we will be adding an additional
    attribute for the class, which will hold the `lock` object that corresponds to
    the value of the counter. Consider the following new implementation of the data
    structure in the `Chapter16/example2.py` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In this implementation of our counter data structure, a `lock` object is also
    initialized as an attribute of a `LockedCounter` instance, when that instance
    is initialized. Additionally, any time the value of the counter is accessed by
    a thread, whether for reading (the `get_value()` method) or updating (the `increment()`
    method), that `lock` attribute has to be acquired, to ensure that no other thread
    is also accessing it. This is done by using a context manager with the `lock`
    attribute.
  prefs: []
  type: TYPE_NORMAL
- en: 'Theoretically, this implementation should solve the problem of the race condition
    for us. In our main program, we are implementing the same thread pool that was
    used in the previous example. A shared counter will be created, and it will be
    incremented 300 times (each time by one unit), across three different threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the script, and the output produced by the program should be similar to
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the problem of the race condition has been addressed successfully:
    the final value of the counter is `300`, which corresponds perfectly to the number
    of increments that were executed. Furthermore, no matter how many times the program
    is run again, the value of the counter will always remain `300`. What we currently
    have is a working, correct data structure for concurrent counters.'
  prefs: []
  type: TYPE_NORMAL
- en: The concept of scalability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One aspect of programming that is essential to the application of concurrency
    is **scalability**. By scalability, we mean the changes in performance when the
    number of tasks to be processed by the program increases. Andre B. Bondi, founder
    and president of Software Performance and Scalability Consulting, LLC, defines
    the term scalability as <q>*"the capability of a system, network, or process to
    handle a growing amount of work, or its potential to be enlarged to accommodate
    that growth."*</q>
  prefs: []
  type: TYPE_NORMAL
- en: In concurrent programming, scalability is an important concept that always needs
    to be taken into account; the amount of work that grows in concurrent programming
    is typically the number of tasks to be executed, as well as the number of processes
    and threads active to execute those tasks. For example, the designing, implementing,
    and testing phases of a concurrent application usually involve fairly small amounts
    of work, to facilitate efficient and fast development. This means that a typical
    concurrent application will handle significantly more work in real-life situations
    than it did during the development stage. This is why an analysis of scalability
    is crucial in well-designed concurrent applications.
  prefs: []
  type: TYPE_NORMAL
- en: Since the execution of a process or thread is independent of the process execution
    of another, as long as the amount of work a single process/thread is responsible
    for remains the same, we would like changes in the number of processes/threads
    to not affect the performance of the general program. This characteristic is called
    **perfect scalability**, and is a desirable characteristic for a concurrent program;
    if the amount of work for a given perfectly scalable concurrent program increases,
    the program can simply create more active processes or threads, in order to absorb
    the increased amount of work. Its performance can then stay stable.
  prefs: []
  type: TYPE_NORMAL
- en: However, perfect scalability is virtually impossible to achieve most of the
    time, due to the overhead in creating threads and processes. That being said,
    if the performance of a concurrent program does not considerably worsen as the
    number of active processes or threads increases, then we can accept the scalability.
    The term **considerably worsen** is highly dependent on the types of task that
    the concurrent program is responsible for executing, as well as how large a decrease
    in program performance is permitted.
  prefs: []
  type: TYPE_NORMAL
- en: In this kind of analysis, we will consider a two-dimensional graph, representing
    the scalability of a given concurrent program. The *x *axis denotes the number
    of active threads or processes (again, each is responsible for executing a fixed
    amount of work throughout the program); the *y *axis denotes the speed of the
    program, with different numbers of active threads or processes. The graph under
    consideration will have a generally increasing trend; the more processes/threads
    the program has, the more time it will (most likely) take for the program to execute.
    Perfect scalability, on the other hand, will translate to a horizontal line, as
    no additional time is needed when the number of threads/processes increases.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram is an example of such a graph, for scalability analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/719e2141-ce98-4fb4-9b03-912088d34e31.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of scalability analysis (Source: stackoverflow.com/questions/10660990/c-sharp-server-scalability-issue-on-linux)
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding graph, the *x *axis indicates the number of executing threads/processes,
    and the *y *axis indicates the running time (in seconds, in this case). The different
    graphs indicate the scalability of specific setups (the operating system combined
    with multiple cores).
  prefs: []
  type: TYPE_NORMAL
- en: The steeper the slope of a graph is, the worse the corresponding concurrent
    model scales with an increasing number of threads/processes. For example, a horizontal
    line (the dark blue and lowest graph in this case) signifies perfect scalability,
    while the yellow (upper most) graph indicates an undesirable scalability.
  prefs: []
  type: TYPE_NORMAL
- en: Analysis of the scalability of the counter data structure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let's consider the scalability of our current counter data structure—specifically,
    with changing numbers of active threads. We had three threads increment a shared
    counter for a total of 300 times; so, in our scalability analysis, we will have
    each of the active threads increment a shared counter 100 times, while changing
    the number of active threads in our program. Following the aforementioned specification
    of scalability, we will look at how the performance (speed) of the program that
    uses the counter data structure changes when the number of threads increases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the `Chapter16/example3.py` file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding script, we are still using the same implementation of the `LockedCounter`
    class that we used in the previous example. In our main program, we are testing
    this class against various numbers of active threads; specifically, we are iterating
    over a `for` loop, to have the number of active threads go from 1 to 10\. In each
    iteration, we initialize a shared counter and create a pool of threads to process
    an appropriate number of tasks—in this case, incrementing the shared counter 100
    times for each thread.
  prefs: []
  type: TYPE_NORMAL
- en: We are also keeping track of the number of active threads, as well as the time
    it took for the pool of threads to finish its tasks in each iteration. This is
    our data for the scalability analysis process. We are printing this data out and
    plotting a scalability graph similar to what we saw in the preceding sample graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows my output from running the script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Additionally, the scalability graph that I obtained is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/e4a205f0-bac9-43e2-9c13-a4599f1d02a5.png)'
  prefs: []
  type: TYPE_IMG
- en: Scalability of lock-based counter data structures
  prefs: []
  type: TYPE_NORMAL
- en: 'Even if your own output varies in the specific duration of each iteration,
    the scalability trend should be relatively the same; in other words, your scalability
    graph should have the same slope as the preceding graph. As you can see from the
    kinds of output that we have, even though the counter in each iteration had the
    correct value, the current scalability of our counter data structure is highly
    undesirable: as more threads are added to the program to execute more tasks, the
    performance of the program decreases, almost linearly. Recall that the desired
    perfect scalability requires the performance to remain stable across different
    numbers of threads/processes. Our counter data structure increases the execution
    time of the program that we have by an amount that is proportional to the increase
    in the number of active threads.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Intuitively, this constraint in scalability results from our locking mechanism:
    since only one thread can access and increment the shared counter at any given
    time, the more increments the program has to execute, the longer it will take
    to finish all increment tasks. Of the biggest disadvantages to using locks as
    a synchronization mechanism, this is the second: locks can execute a concurrent
    program (again, the first disadvantage is the fact that locks don''t actually
    lock anything).'
  prefs: []
  type: TYPE_NORMAL
- en: Approximate counters as a solution for scalability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Given the complexity of designing and implementing a correct, yet fast, lock-based
    concurrent data structure, developing efficiently scalable locking mechanisms
    is a popular topic of research in computer science, and many approaches to solving
    the problem that we are facing have been proposed. In this section, we will discuss
    one of them: **approximate counters**.'
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind approximate counters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s think back to our current program and the reason why the locks are preventing
    us from achieving good performance in terms of speed: all of the active threads
    in our program interact with the same shared counter, which can only interact
    with one thread at a time. The solution to this problem is to isolate the interactions
    with a counter of separate threads. Specifically, the value of the counter that
    we are keeping track of will not be represented by only a single, shared counter
    object anymore; instead, we will use many **local counters**, one per thread/process,
    in addition to the shared **global counter** that we originally had.'
  prefs: []
  type: TYPE_NORMAL
- en: The basic idea behind this approach is to distribute the work (incrementing
    the shared global counter) across other low-level counters. When an active thread
    executes and wants to increment the global counter, first it has to increment
    its corresponding local counter. Interacting with individual local counters, unlike
    doing it with a single, shared counter, is highly scalable, as only one thread
    accesses and updates each local counter; in other words, there is no contention
    between different threads in interacting with the individual local counters.
  prefs: []
  type: TYPE_NORMAL
- en: As each thread interacts with its corresponding local counter, the local counters
    have to interact with the global counter. Specifically, each local counter will
    periodically acquire the lock for the global counter and increment it with respect
    to its current value; for example, if a local counter holding the value of six
    wants to increment the global counter, it will do it by six units, and set its
    own value back to zero. This is because all increments reported from the local
    counters are relative to the value of the global counter, meaning that, if a local
    counter holds the value of *x*, the global counter should increment its value
    by *x*.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can think of this design as a simple network, with the global counter being
    at the center node, and each local counter being a rear node. Each rear node interacts
    with the center node by sending its value to the center node and consequently
    resetting its value back to zero. The following diagram further illustrates this
    design:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/abc0fe6d-7219-4f13-8299-0dad44854165.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram of four-thread approximate counters
  prefs: []
  type: TYPE_NORMAL
- en: As discussed previously, if all of the active threads were to interact with
    the same lock-based counter, no additional speed could be gained from making the
    program concurrent, since the execution between separate threads cannot be overlapped.
    Now, with one separate counter object for each thread, the threads can update
    their corresponding local counters independently and simultaneously, creating
    overlaps that will result in better performance in speed for our program, making
    the program more scalable.
  prefs: []
  type: TYPE_NORMAL
- en: The name of the technique, **approximate counters**, comes from the fact that
    the value of the global counter is simply an approximation of the correct value.
    Specifically, the value of the global counter is calculated solely via the values
    of the local counters, and it becomes more accurate each time the global counter
    is incremented by one of the local counters.
  prefs: []
  type: TYPE_NORMAL
- en: There is, however, a specification in this design that deserves great consideration.
    How often should the local counters interact with the global counter and update
    its value? Surely it cannot be at the rate of every increment (incrementing the
    global counter every time a local counter is incremented), as that would be equivalent
    to using one shared lock, with even more over overhead (from the local counters).
  prefs: []
  type: TYPE_NORMAL
- en: A quantity called **threshold S** is used to denote the frequency in question;
    specifically, threshold S is defined as the upper boundary of the value of a local
    counter. So, if a local counter is incremented such that its value is greater
    than threshold S, it should update the global counter and reset its value to zero.
    The smaller threshold S is, the more frequently the local counters will update
    the global counter, and the less scalable our program will be, but the value of
    the global counter will be more up-to-date. Conversely, the larger threshold S
    is, the less frequently the value of the global counter will be updated, but the
    better the performance of our program will be.
  prefs: []
  type: TYPE_NORMAL
- en: There is, therefore, a trade-off between the accuracy of an approximate counter
    object and the scalability of a concurrent program using the data structure. Similar
    to other common trade-offs in computer science and programming, only through personal
    experimentation and testing can one determine the optimal threshold S for one's
    approximate counter data structure. In the next section, when we implement our
    own design for an approximate counter data structure, we will arbitrarily set
    the value of threshold S to 10.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing approximate counters in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With the concept of approximate counters in mind, let''s try to implement the
    data structure in Python, building on our previous design for the lock-based counter.
    Consider the following `Chapter16/example4.py` file—specifically, the `LockedCounter`
    class and the `ApproximateCounter` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: While the `LockedCounter` class remains the same as in our previous example
    (this class will be used to implement our global counter objects), the `ApproximateCounter`
    class, which contains the implementation of the approximate counter logic that
    we discussed previously, is of interest. A newly initialized `ApproximateCounter`
    object will be given a starting value of `0`, and it will also have a lock, as
    it is also a lock-based data structure. The important attributes of an `ApproximateCounter`
    object are the global counter that it needs to report to and the threshold that
    specifies the rate at which it reports to its corresponding global counter. As
    mentioned previously, here, we are simply choosing `10` as an arbitrary value
    for the threshold.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `increment()` method of the `ApproximateCounter` class, we can also
    see the same increment logic: the method takes in a parameter named `x` and increments
    the value of the counter by `x` while holding the lock of the calling approximate
    counter object. Additionally, the method also has to check whether the newly incremented
    value of the counter is past its threshold; if so, it will increment the value
    of its global counter by an amount that is equal to the current value of the local
    counter, and that value of the local counter will be set back to `0`. The `get_value()`
    method that is used to return the current value of the counter in this class is
    the same as what we saw previously.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s test and compare the scalability of the new data structure in our
    main program. First, we will regenerate the data for the scalability of our old
    single-lock counter data structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Just like in our previous example, we are using a `ThreadPoolExecutor` object
    to process tasks concurrently, in separate threads, while keeping track of the
    time it took for each iteration to finish; there is nothing surprising here. Next,
    we will generate the same data with a corresponding number of active threads in
    the iterations of the `for` loop, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Let's take some time to analyze the preceding code. First, we have an external
    `thread_increment()` function that takes in a counter and increments it by 1;
    this function will be used as refactored code later on, to individually increment
    our local counters.
  prefs: []
  type: TYPE_NORMAL
- en: Again, we will be iterating through a `for` loop to analyze the performance
    of this new data structure with a changing number of active threads. Inside each
    iteration, we first initialize a `LockedCounter` object as our global counter,
    together with a list of local counters, which are instances of the `ApproximateCounter`
    class. All of them are associated with the same global counter (which was passed
    in the initialization method), as they need to report to the same counter.
  prefs: []
  type: TYPE_NORMAL
- en: Next, similar to what we have been doing to schedule tasks for multiple threads,
    we are using a context manager to create a thread pool, inside of which we will
    be distributing the tasks (incrementing the local counters) via a nested `for`
    loop. The reason we are looping through another `for` loop is to simulate the
    number of tasks consistent with what we implemented in the previous example, and
    also to distribute those tasks across all of the local counters concurrently.
    We are also printing out the final value of the global counter in each iteration,
    to ensure that our new data structure is working correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, in our main program, we will be plotting the data points that are
    generated from the two `for` loops, to compare the scalability of the two data
    structures via their respective performances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the script, and the first output that you will receive will include the
    individual final values of the global counters in our second `for` loop, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the final values that we obtained from the global counters
    are all correct, proving that our data structure is working as intended. Additionally,
    you will obtain a graph similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/d0be6423-ae92-4f2a-96fa-b5ad1ab64ea7.png)'
  prefs: []
  type: TYPE_IMG
- en: Scalability of single-lock counter and approximate counters
  prefs: []
  type: TYPE_NORMAL
- en: The blue line indicates the changes in speed of the single-lock counter data
    structure, while the red line indicates those of the approximate counter data
    structure. As you can see, even though the performance of the approximate counter
    does worsen somewhat as the number of threads increases (due to overheads such
    as creating individual local counters and distributing an increasing number of
    increment tasks), our new data structure is highly scalable, especially in comparison
    to our previous single-lock counter data structure.
  prefs: []
  type: TYPE_NORMAL
- en: A few considerations for approximate counter designs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One thing that you may have noticed is that, even though only a single thread
    interacts with a single local counter, the data structure still has a `lock` attribute
    in its initialization. This is because it is, in fact, possible for multiple threads
    to share the same local counters. There are situations in which it is inefficient
    to create one local counter for every active thread, so the developer can have
    two or more share the same local counter instead, and individual counters can
    still report to the same global counter.
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose that there are 20 threads executing in a concurrent counter
    program; we can only have 10 local counters reporting to one global counter. From
    what we have seen, this setup will have a lower level of scalability than one
    with an individual local counter for each thread, but the advantage of this approach
    that it uses less memory space and avoids the overhead of creating more local
    counters.
  prefs: []
  type: TYPE_NORMAL
- en: There is another possible variation to the way in which a program that utilizes
    approximate counters can be designed. Instead of having only one layer of local
    counters, we can also implement semi-global counters that local counters report
    to, which, in turn, report to the global counters that are one level higher than
    themselves. When using the approximate counter data structure, the developer not
    only has to find, as discussed previously, an appropriate threshold of reporting,
    but he or she also needs to optimize the number of threads associated with one
    single local counter, as well as the number of layers in our design.
  prefs: []
  type: TYPE_NORMAL
- en: Mutex-free concurrent data structures in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous subsection concluded our discussion of designing a lock-based concurrent
    data structure in Python, and the complexities involved therein. We will now move
    on to one approach to the theoretical design of mutex-free concurrent data structures.
  prefs: []
  type: TYPE_NORMAL
- en: The term **mutex-free** in concurrent data structures indicates the lack of
    a locking mechanism to protect the integrity of the data structure. This does
    not mean that the data structure simply disregards the protection of its data;
    instead, the data structure has to employ other synchronization mechanisms. In
    this section, we will analyze one such mechanism, known as **read-copy-update**,
    and discuss how to apply it to a Python data structure.
  prefs: []
  type: TYPE_NORMAL
- en: The impossibility of being lock-free in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The opposite of a lock-based data structure is a lock-free one. Here we will
    be discussing its definition and the reason why the characteristic of being lock-free
    is actually impossible in Python, and why the closest we can get to it is being
    mutex-free.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike a lock-based data structure, a data structure that is lock-free not only
    does not employ any locking mechanism (like mutex-free data structures), but also
    requires that any given thread or process cannot be waiting to execute indefinitely.
    This means that, if a lock-free data structure is successfully implemented, applications
    utilizing that data structure will never encounter the problems of deadlock and
    starvation. For this reason, lock-free data structures are widely considered a
    more advanced technique in concurrent programming, and consequently, they are
    significantly more difficult to implement.
  prefs: []
  type: TYPE_NORMAL
- en: The characteristic of being lock-free, however, is actually impossible to implement
    in Python (or in the CPython interpreter, to be more specific). As you have probably
    guessed, this is due to the existence of the GIL, which prevents more than one
    thread from executing in the CPU at any given time. To learn more about the GIL,
    navigate to [Chapter 15](0e30892f-4bb1-4196-93c5-5df1d57428b8.xhtml), *The Global
    Interpreter Lock*, and read the in-depth analysis on the GIL, if you have not
    already. All in all, having a purely lock-free data structure implemented in CPython
    is a logical impossibility.
  prefs: []
  type: TYPE_NORMAL
- en: However, this does not mean that concurrent programs in Python cannot benefit
    from the design of lock-free data structures. As mentioned previously, mutex-free
    Python data structures (which can be considered a subset of lock-free data structures)
    are entirely possible to implement. In fact, mutex-free data structures still
    result in the successful avoidance of deadlock and starvation problems. However,
    they cannot fully take advantage of the purely lock-free execution that would
    result in better speed.
  prefs: []
  type: TYPE_NORMAL
- en: In the next subsections, we will take a look at a custom data structure in Python,
    analyze the problem that it raises if used concurrently, and, finally, try to
    apply a mutex-free logic to the underlying data structure.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to the network data structure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data structure that we are implementing resembles a network of nodes, one
    of which is the primary node. Additionally, each node contains a key and a value
    for the node. You can think of this data structure as a Python dictionary (in
    other words, a set of keys and values respectively paired together), but one of
    these key and value pairs is called the primary node of the network.
  prefs: []
  type: TYPE_NORMAL
- en: A good way to visualize this data structure is to analyze a situation in which
    the data structure is utilized. Suppose that you have been asked to implement
    the request handling logic of a popular website, which is also, unfortunately,
    a common target for **denial of service (DoS)** attacks. Since it is highly possible
    that the website will be taken down fairly frequently, despite the efforts of
    the cybersecurity team, an approach that you could take to guarantee that clients
    of the website will still be able to access it is to keep more than one working
    copy of the website, in addition to the main website, on the server.
  prefs: []
  type: TYPE_NORMAL
- en: These copies are equivalent to the main website in every way, and the main website
    can therefore be completely replaced by any of the copies at any time. Now, if
    and when the main website is taken down by a DoS attack, you, as the server administrator,
    can simply allow the main website to go down and switch the address of the new
    main website to one of the copies that you have ready. The clients of the website
    will therefore experience no difficulty or inconsistency when accessing the data
    from the website, since the copies are identical to the main website that was
    taken down. Servers that do not implement this mechanism, on the other hand, will
    most likely have to spend some time recovering from a DoS attack (isolating the
    attack, building back the interrupted or corrupted data, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, a connection between this method of web administration and the
    aforementioned network data structure can be made. In fact, the network data structure
    is, in essence, a high-level abstraction of the method; the data structure is
    a set of nodes or pairs of values (the website address and the data, in the preceding
    case), while keeping track of a primary node that can also be replaced by any
    other node (clients accessing the website are directed to a new website when the
    main website is attacked). We will call this processing **refreshing the primary**
    in our data structure, which is illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/5091dce0-bb4e-424e-a79b-4e1ab81f7030.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram of network primary refreshing
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding diagram, we have three separate notes of data in our network
    data structure (visualized as a dictionary, denoted by a pair of curly braces):
    key **A**, pointing to some data; key **B**, pointing to its own data; and, finally,
    key **C**, also pointing to its own data. Additionally, we have a pointer indicating
    the primary key of our dictionary network, pointing to key **A**. As the primary
    refresh process takes place, we will stop keeping track of key **A** (which is
    the primary key) and its own, and then have the primary pointer pointing to another
    node in the network (in this case, key **B**).'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a simple network data structure in Python and race conditions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s consider a starting implementation of this data structure in Python.
    Navigate to the `Chapter16/network.py` file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This file contains the `Network` class, which implements the logic that we discussed
    previously. Upon initialization, each instance of this class will have at least
    one node in its network (stored in the `data` attribute) that is its primary node;
    we are also using Python's dictionary data structure to implement this network
    design. Each object also has to keep track of the key of its primary data, stored
    in its `primary_key` attribute.
  prefs: []
  type: TYPE_NORMAL
- en: In this class, we also have an `add_node()` method that is used to add a new
    node of data to a network object; note that each node has to have a key and a
    value. Recall our web administration example—this corresponds to an internet address
    and the data that the website has. The class also has a `refresh_primary()` method
    that simulates refreshing the primary process (which deletes the reference to
    the previous primary data and pseudo-randomly selects a new primary node from
    the remaining nodes). Keep in mind that the precondition for this method is that
    the calling network object has to have at least two nodes left .
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we have an accessor method, called `get_primary_value()`, that returns
    the value that the primary key of the calling network object points to. Here,
    we add in a slight delay in the execution of the method, to simulate the race
    condition that will occur from using this naive data structure. (Additionally,
    we are overwriting the default `__str__()` method, for easy debugging.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s turn our attention to the `Chapter16/example5.py` file, where we
    import this data structure and use it in a concurrent program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'First of all, we implement a function called `print_network_primary_value()`,
    which accesses and obtains the primary data of a network object that is also a
    global variable, using the aforementioned `get_primary_value()` method. In our
    main program, we then initialize a network object with a starting node, with `A`
    as the node key and `1` as the node data (this node also automatically becomes
    the primary node). We then add two more nodes to this network: `B`, pointing to
    `1`, and `C`, pointing to `1`, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, two threads are initialized and started, the first of which calls the `print_network_primary_value()`
    function to print out the current primary data of the network. The second calls
    the `refresh_primary()` method from the network object. We are also printing out
    the current state of the network object at various points throughout the program.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is quite easy to spot the race condition that will likely occur here: since
    the first thread is trying to access the primary data while the second thread
    is trying to refresh the data of the network (in essence, deleting the current
    primary data at that time), the first thread will most likely cause an error in
    its execution. Specifically, the following is my output after running the script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Just like we discussed, we encountered a `KeyError` that resulted from the
    fact that, by the time the first thread obtained the primary key, that key and
    the primary data had already been deleted from the data structure by its execution
    in the second thread. The following diagram further illustrates this point:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/f7229d9b-ab3e-4217-bd3b-0b9a75781c70.png)'
  prefs: []
  type: TYPE_IMG
- en: Race condition with network data structure
  prefs: []
  type: TYPE_NORMAL
- en: As you saw in previous chapters, we are using the `time.sleep()` function in
    the source code of the data structure, to ensure that the race condition will
    occur. Most of the time, the execution will be fast enough that an error will
    not occur, yet the race condition will still be there, and this is a problem in
    our current data structure that we need to address.
  prefs: []
  type: TYPE_NORMAL
- en: RCU as a solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The root of the race condition that we are encountering is, as we know, the
    fact that the network object that we are working with is being shared between
    different threads, which are mutating and reading the data from the data structure
    simultaneously. Specifically, the second thread in our program was mutating the
    data (by calling the `refresh_primary()` method), while the first thread was reading
    from the same data.
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, we can simply apply locking as the synchronization mechanism for
    this data structure. However, we know that the tasks of acquiring and releasing
    locks involve a slight cost that will become substantial as the data structure
    is widely used across a system. As popular websites and systems (namely, MongoDB)
    use this abstraction to design and structure their servers, a considerably high
    level of traffic will make the cost of using locks apparent, and cause the performance
    to decrease. Implementing a variation of an approximate data structure could help
    with this issue, but the complexity of the implementation might prove to be too
    difficult to follow through.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we arrive at the goal of using a mutex-free approach as our synchronization
    mechanism—in this case, **read-copy-update** (**RCU**). To protect the integrity
    of your data structure, RCU is, in essence, a synchronization mechanism that creates
    and maintains another version of the data structure when a thread or process requests
    read or write access to it. By isolating the interaction between the data structure
    and threads/processes within a separate copy, RCU ensures that no conflicting
    data can occur. As a thread or a process has mutated the information in the copy
    of the data structure that it is assigned to, that update can then be reported
    to the original data structure.
  prefs: []
  type: TYPE_NORMAL
- en: In short, when a shared data structure has threads or processes requesting access
    to it (the read process), it needs to return a copy of itself, instead of letting
    the threads/processes access its own data (the copy process); finally, if there
    are any changes in the copies of the data structure, they will need to be updated
    back to the shared data structure (the update process).
  prefs: []
  type: TYPE_NORMAL
- en: 'RCU is particularly useful for data structures that have to handle a single
    updater and multiple readers at the same time, which is the typical case of the
    server network that we discussed previously (multiple clients constantly accessing
    and requesting data, but only occasional, periodic attacks). But how would this
    apply to our current network data structure? Theoretically, the accessor method
    of our data structure (the `get_primary_value()` method), which is, again, the
    root of the race condition, needs to create a copy of the data structure before
    reading the data from a thread. This specification is implemented in the accessor
    method, in the `Chapter16/concurrent_network.py` file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we are using the built-in `deepcopy` method from the copy module, which
    returns a separate copy of our network in a different memory location. Then, we
    only read the data from this copy of the network object, and not the original
    object itself. This process is illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/9a43e409-4c9d-4035-bdd4-a834e1d80192.png)'
  prefs: []
  type: TYPE_IMG
- en: RCU addressing the race condition
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, we can see that no conflict will occur in terms of
    data, as the two threads now deal with different copies of the data structure.
    Let us see this implementation in action in the `Chapter16/example6.py` file,
    which contains the same instructions as the previous `example5.py` file (initializing
    a network object, calling two threads at the same time—one to access the primary
    data of the network, the other to refresh the same primary data), only now the
    program is using our new data structure from the `concurrent_network.py` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'After running the script, your output should be the same as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, not only does the program obtain the correct value of the primary
    data in the first thread without evoking any errors, it also holds the correct
    network at the end of the program (without the previously deleted node, with the
    key `A`). The RCU method does, indeed, solve the problem of the race condition,
    without the use of any locking mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: One thing that you might have also noticed is that RCU could also be applied
    for our counter example in the previous section. It is true that both RCU and
    approximate counters are reasonable approaches to the counter problem, and the
    question of which one is the better solution for a specific concurrent problem
    can only be answered by empirical, hands-on analysis such as scalability analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Building on simple data structures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this chapter, we have worked with a number of simple, concurrent
    data structures, such as counters and networks. For this reason, we were able
    to truly get to the bottom of the problems that we encountered in the concurrent
    programs that utilize these data structures, and were able perform in-depth analyses
    of how to improve their structures and design.
  prefs: []
  type: TYPE_NORMAL
- en: As you work on more complex concurrent data structures in your work and projects,
    you will see that their designs and structures, and the problems that accompany
    them, are, in fact, fundamentally similar to those that we saw in the data structures
    we analyzed. By truly understanding the underlying architecture of the data structures,
    as well as the root of problems that can occur in the programs that use them,
    you can build on this knowledge and design data structures that are more complex
    in instruction but equivalent in logic.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we studied the theoretical differences between lock-based
    and mutex-free data structures: a lock-based data structure uses a locking mechanism
    to protect the integrity of its data, while a mutex-free one does not. We analyzed
    the problem of race conditions that can occur in poorly-designed data structures,
    and looked at how to address it in both situations.'
  prefs: []
  type: TYPE_NORMAL
- en: In our example of the concurrent lock-based counter data structure, we considered
    the design of approximate counters, as well as the improved scalability that the
    design can offer. In our analysis of the concurrent network data structure, we
    studied the RCU technique, which isolates reading instructions from updating instructions,
    with the goal of maintaining the integrity of the concurrent data structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will look at another set of advanced concepts in Python
    concurrent programming: memory models and operations on atomic types. You will
    learn more about Python memory management, as well as the definition and uses
    of atomic types.'
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is the main approach to solving the problem that locks don't lock anything?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describe the concept of scalability in the context of concurrent programming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does a naive locking mechanism affect the scalability of a concurrent program?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are approximate counters, and how do they help with the problem of scalability
    in concurrent programming?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are lock-free data structures possible in Python? Why or why not?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is a mutex-free concurrent data structure, and how is it different from
    a concurrent lock-based one?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the RCU technique, and what problem does it solve for mutex-free concurrent
    data structures?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information, you can refer to the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Operating systems: Three easy pieces*. Vol. 151\. Wisconsin: Arpaci-Dusseau
    Books, 2014, by Arpaci-Dusseau, Remzi H. and Andrea C. Arpaci-Dusseau'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The Secret Life of Concurrent Data Structures* ([addthis.com/blog/2013/04/25/the-secret-life-of-concurrent-data-structures/](https://www.addthis.com/blog/2013/04/25/the-secret-life-of-concurrent-data-structures/#.W7onwBNKiAw)),
    by Michael Spiegel'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*What is RCU, fundamentally? *Linux Weekly News (LWN. net) (2007), McKenney,
    Paul E. and Jonathan Walpole'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Wasp''s Nest: The Read-Copy-Update Pattern in Python* ([emptysqua.re/blog/wasps-nest-read-copy-update-python/](https://emptysqua.re/blog/wasps-nest-read-copy-update-python/)), Davis, A.
    Jesse Jiryu'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Characteristics of scalability and their impact on performance*, proceedings
    of the second international **workshop on software and performance** (**WOSP**)
    ''00\. p. 195, André B'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
