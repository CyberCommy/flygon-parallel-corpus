- en: Chapter 6. Deploying Code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even the perfect code (if it exists) is useless if it is not being run. So in
    order to serve a purpose, our code needs to be installed on the target machine
    (computer) and executed. The process of making a specific version of your application
    or service available to the end users is called deployment.
  prefs: []
  type: TYPE_NORMAL
- en: In case of desktop applications, this seems to be simple—your job ends on providing
    a downloadable package with optional installer, if necessary. It is the user's
    responsibility to download and install it in his/her environment. Your responsibility
    is to make this process as easy and convenient as possible. Proper packaging is
    still not a simple task, but some tools were already explained in the previous
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Surprisingly, things get more complicated when your code is not a product per
    se. If your application only provides a service that is being sold to the users,
    then it is your responsibility to run it on your own infrastructure. This scenario
    is typical for a web application or any "X as a Service" product. In such a situation,
    the code is deployed to set off remote machines that usually are hardly physically
    accessible to the developers. This is especially true if you are already a user
    of cloud computing services such as **Amazon Web** **Services** (**AWS**) or Heroku.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will concentrate on the aspect of code deployment to remote
    hosts because of the very high popularity of Python in the field of building various
    web-related services and products. Despite the high portability of this language,
    it has no specific quality that would make its code easily deployable. What matters
    the most is how your application is built and what processes you use to deploy
    it to the target environments. So this chapter will focus on the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What are the main challenges in deploying the code to remote environments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to build applications in Python that are easily deployable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to reload web services without downtime
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to leverage Python packaging ecosystem in code deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to properly monitor and instrument code that runs remotely
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Twelve-Factor App
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main requirement for painless deployment is building your application in
    a way that ensures that this process will be simple and as streamlined as possible.
    This is mostly about removing obstacles and encouraging well-established practices.
    Following such common practices is especially important in organizations where
    only specific people are responsible for development (developers team or Dev for
    short) and different people are responsible for deploying and maintaining the
    execution environments (operations team or Ops for short).
  prefs: []
  type: TYPE_NORMAL
- en: All tasks related to server maintenance, monitoring, deployment, configuration,
    and so on are often put to one single bag called operations. Even in organizations
    that have no separate teams for operational tasks, it is common that only some
    of the developers are authorized to do deployment tasks and maintain the remote
    servers. The common name for such a position is DevOps. Also, it isn't such an
    unusual situation that every member of the development team is responsible for
    operations, so everyone in such a team can be called DevOps. Anyway, no matter
    how your organization is structured and what the responsibilities of each developer
    are, everyone should know how operations work and how code is deployed to the
    remote servers because, in the end, the execution environment and its configuration
    is a hidden part of the product you are building.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following common practices and conventions are important mainly for the
    following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: At every company people quit and new ones are hired. By using best approaches,
    you are making it easier for fresh team members to jump into the project. You
    can never be sure that new employees are already familiar with common practices
    for system configuration and running applications in a reliable way, but you at
    least make their fast adaptation more probable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In organizations where only some people are responsible for deployments, it
    simply reduces the friction between the operations and development teams.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A good source of such practices that encourage building easily deployable apps
    is a manifesto called **Twelve-Factor App**. It is a general language-agnostic
    methodology for building software-as-a-service apps. One of its purposes is making
    applications easier to deploy, but it also highlights other topics, such as maintainability
    and making applications easier to scale.
  prefs: []
  type: TYPE_NORMAL
- en: 'As its name says, the Twelve-Factor App consists of 12 rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Codebase**: One codebase tracked in revision control, many deploys'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dependencies**: Explicitly declare and isolate dependencies'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Config**: Store config in the environment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Backing services**: Treat backing services as attached resources'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Build, release, run**: Strictly separate build and run stages'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Processes**: Execute the app as one or more stateless processes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Port binding**: Export services via port binding'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Concurrency**: Scale out via the process model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Disposability**: Maximize robustness with fast startup and graceful shutdown'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dev/prod parity**: Keep development, staging, and production as similar as
    possible'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Logs**: Treat logs as event streams'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Admin processes**: Run admin/management tasks as one-off processes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extending each of these rules here is a bit pointless because the official page
    of Twelve-Factor App methodology ([http://12factor.net/](http://12factor.net/))
    contains extensive rationale for every app factor with examples of tools for different
    frameworks and environments.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter tries to stay consistent with the above manifesto, so we will discuss
    some of them in detail when necessary. The techniques and examples that are presented
    may sometimes slightly diverge from these 12 factors, but remember that these
    rules are not carved in stone. They are great as long as they serve the purpose.
    In the end, what matters is the working application (product) and not being compatible
    with some arbitrary methodology.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment automation using Fabric
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For very small projects, it may be possible to do deploy your code "by hand",
    that is, by manually typing the sequence of commands through the remote shell
    that are necessary to install a new version of code and execute it on a remote
    shell. Anyway, even for an average-sized project, this is error prone, tedious,
    and should be considered a waste of most the precious resource you have, your
    own time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution for that is automation. The simple rule of thumb could be if you
    needed to perform the same task manually at least twice, you should automate it
    so you won''t need to do it for the third time. There are various tools that allow
    you to automate different things:'
  prefs: []
  type: TYPE_NORMAL
- en: Remote execution tools such as Fabric are used for on-demand automated execution
    of code on multiple remote hosts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuration management tools such as Chef, Puppet, CFEngine, Salt, and Ansible
    are designed for automatized configuration of remote hosts (execution environments).
    They can be used to set up backing services (databases, caches, and so on), system
    permissions, users, and so on. Most of them can be used also as a tool for remote
    execution like Fabric, but depending on their architecture, this may be more or
    less easy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuration management solutions is a complex topic that deserves a separate
    book. The truth is that the simplest remote execution frameworks have the lowest
    entry barrier and are the most popular choice, at least for small projects. In
    fact, every configuration management tool that provides a way to declaratively
    specify configuration of your machines has a remote execution layer implemented
    somewhere deep inside.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, depending on some of the tools, thanks to their design, it may not be
    best suited for actual automated code deployment. One such example is Puppet,
    which really discourages the explicit running of any shell commands. This is why
    many people choose to use both types of solution to complement each other: configuration
    management for setting up system-level environment and on-demand remote execution
    for application deployment.'
  prefs: []
  type: TYPE_NORMAL
- en: Fabric ([http://www.fabfile.org/](http://www.fabfile.org/)) is so far the most
    popular solution used by Python developers to automate remote execution. It is
    a Python library and command-line tool for streamlining the use of SSH for application
    deployment or systems administration tasks. We will focus on it because it is
    relatively easy to start with. Be aware that, depending on your needs, it may
    not be the best solution to your problems. Anyway, it is a great example of a
    utility that can add some automation to your operations, if you don't have any
    yet.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Fabric and Python 3**'
  prefs: []
  type: TYPE_NORMAL
- en: This book encourages you to develop only in Python 3 (if it is possible) with
    notes about older syntax features and compatibility caveats only to make the eventual
    version switch a bit more painless. Unfortunately, Fabric, at the time of writing
    this book, still has not been officially ported to Python 3\. Enthusiasts of this
    tool are being told for at least a few years that there is ongoing Fabric 2 development
    that will bring a compatibility update. This is said to be a total rewrite with
    a lot of new features but there is no official open repository for Fabric 2 and
    almost no one has seen its code. Core Fabric developers do not accept any pull
    requests for Python 3 compatibility in the current development branch of this
    project and close every feature request for it. Such an approach to the development
    of popular open source projects is at best disturbing. The history of this issue
    does not give us a high chance of seeing the official release of Fabric 2 soon.
    Such secret development of a new Fabric release raises many questions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Regardless of anyone''s opinions, this fact does not diminish the usefulness
    of Fabric in its current state. So there are two options if you already decided
    to stick with Python 3: use a fully compatible and independent fork ([https://github.com/mathiasertl/fabric/](https://github.com/mathiasertl/fabric/))
    or write your application in Python 3 and maintain Fabric scripts in Python 2\.
    The best approach would be to do it in a separate code repository.'
  prefs: []
  type: TYPE_NORMAL
- en: You could of course automate all the work using only Bash scripts, but this
    is very tedious and error-prone. Python has more convenient ways of string processing
    and encourages code modularization. Fabric is in fact only a tool for gluing execution
    of commands via SSH, so some knowledge about how the command-line interface and
    its utilities work in your environment is still required.
  prefs: []
  type: TYPE_NORMAL
- en: To start working with Fabric, you need to install the `fabric` package (using
    `pip`) and create a script named `fabfile.py` that is usually located in the root
    of your project. Note that `fabfile` can be considered a part of your project
    configuration. So if you want to strictly follow the Twelve-Factor App methodology,
    you should not maintain its code in the source tree of the deployed application.
    Complex projects are in fact very often built from various components maintained
    as separate codebases, so it is another reason why it is a good approach to have
    one separate repository for all of the project component configurations and Fabric
    scripts. This makes deployment of different services more consistent and encourages
    good code reuse.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example `fabfile` that defines a simple deployment procedure will look like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Every function decorated with `@task` is treated as an available subcommand
    to the `fab` utility provided with the `fabric` package. You can list all the
    available subcommands using the `-l` or `--list` switch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you can deploy the application to the given environment type with just
    a single shell command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the preceding `fabfile` serves only illustrative purposes. In your
    own code, you might want to provide extensive failure handling and also try to
    reload the application without the need to restart the web worker process. Also,
    some of the techniques presented here may be obvious right now but will be explained
    later in this chapter. These are:'
  prefs: []
  type: TYPE_NORMAL
- en: Deploying an application using the private package repository
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Circus for process supervision on the remote host
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your own package index or index mirror
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are three main reasons why you might want to run your own index of Python
    packages:'
  prefs: []
  type: TYPE_NORMAL
- en: The official Python Package Index does not have any availability guarantees.
    It is run by Python Software Foundation thanks to numerous donations. Because
    of that, it very often means that this site can be down. You don't want to stop
    your deployment or packaging process in the middle due to PyPI outage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is useful to have reusable components written in Python properly packaged
    even for the closed source that will never be published publicly. It simplifies
    the code base because packages that are used across the company for different
    projects do not need to be vendored. You can simply install them from the repository.
    This simplifies maintenance for such shared code and might reduce development
    costs for the whole company if it has many teams working on different projects.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is very good practice to have your entire project packaged using `setuptools`.
    Then, deployment of the new application version is often as simple as running
    `pip install --update my-application`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Code vendoring**'
  prefs: []
  type: TYPE_NORMAL
- en: Code vendoring is a practice of including sources of the external package in
    the source code (repository) of other projects. It is usually done when the project's
    code depends on a specific version of some external package that may also be required
    by other packages (and in a completely different version). For instance, the popular
    `requests` package vendors some version of `urllib3` in its source tree because
    it is very tightly coupled to it and is also very unlikely to work with any other
    version of `urllib3`. An example of a module that is particularly often vendored
    by others is `six`. It can be found in sources of numerous popular projects such
    as Django (`django.utils.six`), Boto (`boto.vedored.six`), or Matplotlib (`matplotlib.externals.six`).
  prefs: []
  type: TYPE_NORMAL
- en: Although vendoring is practiced even by some large and successful open source
    projects, it should be avoided if possible. This has justifiable usage only in
    certain circumstances and should not be treated as a substitute for package dependency
    management.
  prefs: []
  type: TYPE_NORMAL
- en: PyPI mirroring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The problem of PyPI outages can be somehow mitigated by allowing the installation
    tools to download packages from one of its mirrors. In fact, the official Python
    Package Index is already served through **CDN** (**Content Delivery Network**),
    so it is intrinsically mirrored. This does not change the fact that it seems to
    have some bad days from time to time when any attempt to download a package fails.
    Using unofficial mirrors is not a solution here because it might raise some security
    concerns.
  prefs: []
  type: TYPE_NORMAL
- en: The best solution is to have your own PyPI mirror that will have all the packages
    you need. The only party that will use it is you, so it will be much easier to
    ensure proper availability. The other advantage is that whenever this service
    gets down, you don't need to rely on someone else to bring it up. The mirroring
    tool maintained and recommended by PyPA is **bandersnatch** ([https://pypi.python.org/pypi/bandersnatch](https://pypi.python.org/pypi/bandersnatch)).
    It allows you to mirror the whole content of Python Package Index and it can be
    provided as the `index-url` option for the repository section in the `.pypirc`
    file (as explained in the previous chapter). This mirror does not accept uploads
    and does not have the web part of PyPI. Anyway, beware! A full mirror might require
    hundreds of gigabytes of storage and its size will continue to grow over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'But why stop on a simple mirror while we have a much better alternative? There
    is a very small chance that you will require a mirror of the whole package index.
    Even with a project that has hundreds of dependencies, it will be only a minor
    fraction of all the available packages. Also, not being able to upload your own
    private package is a huge limitation of such a simple mirror. It seems that the
    added value of using bandersnatch is very low for such a high price. And this
    is true in most situations. If the package mirror is to be maintained only for
    single of few projects, a much better approach is to use **devpi** ([http://doc.devpi.net/](http://doc.devpi.net/)).
    It is a PyPI-compatible package index implementation that provides both:'
  prefs: []
  type: TYPE_NORMAL
- en: A private index to upload nonpublic packages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Index mirroring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main advantage of devpi over bandersnatch is how it handles mirroring. It
    can of course do a full generic mirror of other indexes, like bandersnatch does,
    but it is not its default behavior. Instead of doing rather expensive backup of
    the whole repository, it maintains mirrors for packages that were already requested
    by clients. So whenever a package is requested by the installation tool (`pip`,
    `setuptools`, and `easyinstall`), if it does not exist in the local mirror, the
    devpi server will attempt to download it from the mirrored index (usually PyPI)
    and serve. Once the package is downloaded, the devpi will periodically check for
    its updates to maintain a fresh state of its mirror.
  prefs: []
  type: TYPE_NORMAL
- en: The mirroring approach leaves a slight risk of failure when you request a new
    package that has not yet been mirrored and the upstream package index has an outage.
    Anyway, this risk is reduced thanks to the fact that in most deploys you will
    depend only on packages that were already mirrored in the index. The mirror state
    for packages that were already requested has eventual consistency with PyPI and
    new versions will be downloaded automatically. This seems to be a very reasonable
    tradeoff.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment using a package
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Modern web applications have a lot of dependencies and often require a lot
    of steps to properly install on the remote host. For instance, the typical bootstrapping
    process for a new version of the application on a remote host consists of the
    following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create new virtual environment for isolation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Move the project code to the execution environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Install the latest project requirements (usually from the `requirements.txt`
    file)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synchronize or migrate the database schema
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collect static files from project sources and external packages to the desired
    location
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compile localization files for applications available in different languages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For more complex sites, there might be lot of additional tasks mostly related
    to frontend code:'
  prefs: []
  type: TYPE_NORMAL
- en: Generate CSS files using preprocessors such as SASS or LESS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform minification, obfuscation, and/or concatenation of static files (JavaScript
    and CSS files)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compile code written in JavaScript superset languages (CoffeeScript, TypeScript,
    and so on) to native JS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocess response template files (minification, style inlining, and so on)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All of these steps can be easily automated using tools such as Bash, Fabric,
    or Ansible but it is not a good idea to do everything on remote hosts where the
    application is being installed. Here are the reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Some of the popular tools for processing static assets can be either CPU- or
    memory-intensive. Running them in production environments can destabilize your
    application execution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These tools very often will require additional system dependencies that may
    not be required for normal operation of your projects. These are mostly additional
    runtime environments such as JVM, Node, or Ruby. This adds complexity to configuration
    management and increases the overall maintenance costs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are deploying your application to multiple servers (tenths, hundredths,
    thousands), you are simply repeating a lot of work that could be done once. If
    you have your own infrastructure, then you may not experience a huge increase
    in costs, especially if you perform deployments in periods of low traffic. But
    if you run cloud computing services in the pricing model that charges you extra
    for spikes in load or generally for execution time, then this additional cost
    may be substantial on a proper scale.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most of these steps just take a lot of time. You are installing your code on
    a remote server, so the last thing you want is to have your connection interrupted
    by some network issue. By keeping the deployment process quick, you are lowering
    the chance of deploy interruption.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For obvious reasons, the results of the mentioned deployment steps can't be
    included in your application code repository. Simply, there are things that must
    be done with every release and you can't change that. It is obviously a place
    for proper automation but the clue is to do it in the right place and at the right
    time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most of the things such as static collection and code/asset preprocessing can
    be done locally or in a dedicated environment, so the actual code that is deployed
    to the remote server requires only a minimal amount of on-site processing. The
    most notable deployment steps either in the process of building a distribution
    or installing a package are:'
  prefs: []
  type: TYPE_NORMAL
- en: Installation of Python dependencies and transferring static assets (CSS files
    and JavaScript) to the desired location can be handled as a part of the `install`
    command of the `setup.py` script
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocessing code (processing JavaScript supersets, minification/obfuscation/concatenation
    of assets, and running SASS or LESS) and things such as localized text compilation
    (for example, `compilemessages` in Django) can be a part of the `sdist`/`bdist`
    command of the `setup.py` script
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inclusion of preprocessed code other than Python can be easily handled with
    the proper `MANIFEST.in` file. Dependencies are of course best provided as an
    `install_requires` argument of the `setup()` function call from the `setuptools`
    package.
  prefs: []
  type: TYPE_NORMAL
- en: Packaging the whole application of course will require some additional work
    from you like providing your own custom `setuptools` commands or overriding the
    existing ones, but gives you a lot of advantages and makes project deployment
    a lot faster and reliable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use a Django-based project (in Django 1.9 version) as an example. I
    have chosen this framework because it seems to be the most popular Python project
    of this type, so there is a high chance that you already know it a bit. A typical
    structure of files in such a project might look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note that this slightly differs from the usual Django project template. By default,
    the package that contains the WSGI application, the settings module, and the URL
    configuration has the same name as the project. Because we decided to take the
    packaging approach, this would be named `webxample`. This can cause some confusion,
    so it is better to rename it `conf`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Without digging into the possible implementation details, let''s just make
    a few simple assumptions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our example application has some external dependencies. Here, it will be two
    popular Django packages: `djangorestframework` and `django-allauth`, plus one
    non-Django package: `gunicorn`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`djangorestframework` and `django-allauth` are provided as `INSTALLED_APPS`
    in the `webexample.webexample.settings` module.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The application is localized in three languages (German, English, and Polish)
    but we don't want to store the compiled `gettext` messages in the repository.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are tired of vanilla CSS syntax, so we decided to use the more powerful SCSS
    language that we translate to CSS using SASS.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Knowing the structure of the project, we can write our `setup.py` script in
    a way that make `setuptools` handle:'
  prefs: []
  type: TYPE_NORMAL
- en: Compilation of SCSS files under `webxample/myapp/static/scss`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compilation of `gettext` messages under `webexample/locale` from `.po` to `.mo`
    format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installation of requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A new script that provides an entry point to the package, so we will have the
    custom command instead of the `manage.py` script
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We have a bit of luck here. Python binding for `libsass`, a C/C++ port of SASS
    engine, provides a handful integration with `setuptools` and `distutils`. With
    only little configuration, it provides a custom `setup.py` command for running
    the SASS compilation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: So instead of running the `sass` command manually or executing a subprocess
    in the `setup.py` script we can type `python setup.py build_scss` and have our
    SCSS files compiled to CSS. This is still not enough. It makes our life a bit
    easier but we want the whole distribution to be fully automated so there is only
    one step for creating new releases. To achieve this goal, we are forced to override
    a bit some of the existing `setuptools` distribution commands.
  prefs: []
  type: TYPE_NORMAL
- en: 'The example `setup.py` file that handles some of the project preparation steps
    through packaging might look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'With such an implementation, we can build all assets and create source distribution
    of a package for the `webxample` project using this single terminal command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'If you already have your own package index (created with `devpi`) you can add
    the `install` subcommand or use `twine` so this package will be available for
    installation with `pip` in your organization. If we look into a structure of source
    distribution created with our `setup.py` script, we can see that it contains the
    compiled `gettext` messages and CSS style sheets generated from SCSS files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The additional benefit of using this approach is that we were able to provide
    our own entry point for the project in place of Django''s default `manage.py`
    script. Now we can run any Django management command using this entry point, for
    instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This required a little change in the `manage.py` script for compatibility with
    the `entry_points` argument in `setup()`, so the main part of its code is wrapped
    with the `main()` function call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Unfortunately, a lot of frameworks (including Django) are not designed with
    the idea of packaging your projects that way in mind. It means that depending
    on the advancement of your application, converting it to a package may require
    a lot of changes. In Django, this often means rewriting many of the implicit imports
    and updating a lot of configuration variables in your settings file.
  prefs: []
  type: TYPE_NORMAL
- en: The other problem is the consistency of releases created using Python packaging.
    If different team members are authorized to create application distribution, it
    is crucial that this process takes place in the same replicable environment, especially
    when you do a lot of asset preprocessing; it is possible that the package created
    in two different environments will not look the same even if created from the
    same code base. This may be due to different version of tools used during the
    build process. The best practice is to move the distribution responsibility to
    a continuous integration/delivery system such as Jenkins or Buildbot. The additional
    advantage is that you can assert that the package passes all required tests before
    going to distribution. You can even make the automated deployment as a part of
    such continuous delivery system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite this, distributing your code as Python packages using `setuptools`
    is not simple and effortless; it will greatly simplify your deployments, so it
    is definitely worth trying. Note that this is also in line with the detailed recommendation
    of the sixth rule in the Twelve-Factor App: execute the app as one or more stateless
    processes ([http://12factor.net/processes](http://12factor.net/processes)).'
  prefs: []
  type: TYPE_NORMAL
- en: Common conventions and practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is a set of common conventions and practices for deployment that not every
    developer may know but that are obvious for anyone who has done some operations
    in their life. As explained in the chapter introduction, it is crucial to know
    at least a few of them even if you are not responsible for code deployment and
    operations because it will allow you to make better design decisions during the
    development.
  prefs: []
  type: TYPE_NORMAL
- en: The filesystem hierarchy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The most obvious conventions that may come into your mind are probably about
    filesystem hierarchy and user naming. If you are looking for such suggestions
    here, then you will be disappointed. There is of course a **Filesystem Hierarchy
    Standard** that defines the directory structure and directory contents in Unix
    and Unix-like operating systems, but it is really hard to find an actual OS distribution
    that is fully compliant with FHS. If system designers and programmers cannot obey
    such standards, it is very hard to expect the same from its administrators. In
    my experience, I''ve seen application code deployed almost everywhere where it
    is possible, including nonstandard custom directories in the root filesystem level.
    Almost always, the people behind such decisions had really strong arguments for
    doing so. The only suggestions in this matter that I can give to you are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Choose wisely and avoid surprises
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be consistent across all the available infrastructure of your project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try to be consistent across your organization (the company you work in)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What really helps is to document conventions for your project. Just remember
    to make sure that this documentation is accessible for every interested team member
    and that everyone knows such a document exists.
  prefs: []
  type: TYPE_NORMAL
- en: Isolation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reasons for isolation as well as recommended tools were already discussed in
    [Chapter 1](ch01.html "Chapter 1. Current Status of Python"), *Current Status
    of Python*. For the purpose of deployments, there is only one important thing
    to add. You should always isolate project dependencies for each release of your
    application. In practice it means that whenever you deploy a new version of the
    application, you should create a new isolated environment for this release (using
    `virtualenv` or `venv`). Old environments should be left for some time on your
    hosts, so in case of issues you can easily perform a rollback to one of the older
    versions of your application.
  prefs: []
  type: TYPE_NORMAL
- en: Creating fresh environments for each release helps in managing their clean state
    and compliance with a list of provided dependencies. By fresh environment we mean
    creating a new directory tree in the filesystem instead of updating already existing
    files. Unfortunately, it may make it a bit harder to perform things such as a
    graceful reload of services, which is much easier to achieve if the environment
    is updated in-place.
  prefs: []
  type: TYPE_NORMAL
- en: Using process supervision tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Applications on remote servers usually are never expected to quit. If it is
    the web application, its HTTP server process will indefinitely wait for new connections
    and requests and will exit only if some unrecoverable error occurs.
  prefs: []
  type: TYPE_NORMAL
- en: It is of course not possible to run it manually in the shell and have a never-ending
    SSH connection. Using `nohup`, `screen`, or `tmux` to semi-daemonize the process
    is not an option. Doing so is like designing your service to fail.
  prefs: []
  type: TYPE_NORMAL
- en: 'What you need is to have some process supervision tool that can start and manage
    your application process. Before choosing the right one you need to make sure
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: Restarts the service if it quits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reliably tracks its state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Captures its `stdout`/`stderr` streams for logging purposes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Runs a process with specific user/group permissions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configures system environment variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most of the Unix and Linux distributions have some built-in tools/subsystems
    for process supervision, such as `initd` scripts, `upstart`, and `runit`. Unfortunately,
    in most cases they are not well suited for running user-level application code
    and are really hard to maintain. Especially writing reliable `init.d` scripts
    is a real challenge because it requires a lot of Bash scripting that is hard to
    do right. Some Linux distributions such as Gentoo have a redesigned approach to
    `init.d` scripts, so writing them is a lot easier. Anyway, locking yourself to
    a specific OS distribution just for the purpose of a single process supervision
    tool is not a good idea.
  prefs: []
  type: TYPE_NORMAL
- en: Two popular tools in the Python community for managing application processes
    are Supervisor ([http://supervisord.org](http://supervisord.org)) and Circus ([https://circus.readthedocs.org/en/latest/](https://circus.readthedocs.org/en/latest/)).
    They are both very similar in configuration and usage. Circus is a bit younger
    than Supervisor because it was created to address some weaknesses of the latter.
    They both can be configured in simple INI-like configuration format. They are
    not limited to running Python processes and can be configured to manage any application.
    It is hard to say which one is better because they both provide very similar functionality.
  prefs: []
  type: TYPE_NORMAL
- en: Anyway, Supervisor does not run on Python 3, so it does not get our blessing.
    While it is not a problem to run Python 3 processes under Supervisor's control,
    I will take it as an excuse and feature only the example of the Circus configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s assume that we want to run the webxample application (presented previously
    in this chapter) using `gunicorn` webserver under Circus control. In production,
    we would probably run Circus under applicable system-level process supervision
    tools (`initd`, `upstart`, and `runit`), especially if it was installed from the
    system packages repository. For the sake of simplicity, we will run this locally
    inside of the virtual environment. The minimal configuration file (here named
    `circus.ini`) that allows us to run our application in Circus looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the `circus` process can be run with this configuration file as the execution
    argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you can use the `circusctl` command to run an interactive session and control
    all managed processes using simple commands. Here is an example of such a session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Of course, both of the mentioned tools have a lot more features available. All
    of them are explained in their documentation, so before making your choice, you
    should read them carefully.
  prefs: []
  type: TYPE_NORMAL
- en: Application code should be run in user space
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Your application code should be always run in user space. This means it must
    not be executed under super-user privileges. If you designed your application
    following Twelve-Factor App, it is possible to run your application under a user
    that has almost no privileges. The conventional name for a user that owns no files
    and is in no privileged groups is `nobody`, anyway the actual recommendation is
    to create a separate user for each application daemon. The reason for that is
    system security. It is to limit the damage that a malicious user can do if it
    gains control over your application process. In Linux, processes of the same user
    can interact with each other, so it is important to have different applications
    separated at the user level.
  prefs: []
  type: TYPE_NORMAL
- en: Using reverse HTTP proxies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Multiple Python WSGI-compliant web servers can easily serve HTTP traffic all
    by themselves without the need for any other web server on top of them. It is
    still very common to hide them behind a reverse proxy such as Nginx for various
    reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: TLS/SSL termination is usually better handled by top-level web servers such
    as Nginx and Apache. The Python application can then speak only simple HTTP protocol
    (instead of HTTPS), so complexity and configuration of secure communication channels
    is left for the reverse proxy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unprivileged users cannot bind low ports (in the range of 0-1000), but HTTP
    protocol should be served to the users on port 80, and HTTPS should be served
    on port 443\. To do this, you must run the process with super-user privileges.
    Usually, it is safer to have your application serving on high port or on Unix
    Domain Socket and use that as an upstream for a reverse proxy that is run under
    the more privileged user.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Usually, Nginx can serve static assets (images, JS, CSS, and other media) more
    efficiently than Python code. If you configure it as a reverse proxy, then it
    is only few more lines of configuration to serve static files through it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When single host needs to serve multiple applications from different domains,
    Apache or Nginx are indispensable for creating virtual hosts for different domains
    served on the same port.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reverse proxies can improve performance by adding an additional caching layer
    or can be configured as simple load-balancers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some of the web servers actually are recommended to be run behind a proxy, such
    as Nginx. For example, `gunicorn` is a very robust WSGI-based server that can
    give exceptional performance results if its clients are fast as well. On the other
    hand, it does not handle slow clients well, so it is easily susceptible to denial-of-service
    attacks based on slow client connection. Using a proxy server that is able to
    buffer slow clients is the best way to solve this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Reloading processes gracefully
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The ninth rule of the Twelve-Factor App methodology deals with process disposability
    and says that you should maximize robustness with fast startup times and graceful
    shutdowns. While a fast startup time is quite self-explanatory, graceful shutdowns
    require some additional discussion.
  prefs: []
  type: TYPE_NORMAL
- en: In the scope of web applications, if you terminate the server process in a nongraceful
    way, it will quit immediately without time to finish processing requests and reply
    with the proper responses to connected clients. In the best case scenario, if
    you use some kind of reverse proxy, then the proxy might reply to the connected
    clients with some generic error response (for example, 502 Bad Gateway), even
    though it is not the right way to notify users that you have restarted your application
    and have deployed a new release.
  prefs: []
  type: TYPE_NORMAL
- en: According to the Twelve-Factor App, the web serving process should be able to
    quit gracefully upon receiving Unix `SIGTERM` signal (for example, `kill -TERM
    <process-id>`). This means the server should stop accepting new connections, finish
    processing all the pending requests, and then quit with some exit code when there
    is nothing more to do.
  prefs: []
  type: TYPE_NORMAL
- en: 'Obviously, when all of the serving processes quit or start their shutdown procedure,
    you are not able to process new requests any longer. This means your service will
    still experience an outage, so there is an additional step you need to perform—start
    new workers that will be able to accept new connections while the old ones are
    gracefully quitting. Various Python WSGI-compliant web server implementations
    allow reloading the service gracefully without any downtime. The most popular
    are Gunicorn and uWSGI:'
  prefs: []
  type: TYPE_NORMAL
- en: Gunicorn's master process, upon receiving the `SIGHUP` signal (`kill -HUP <process-pid>`),
    will start new workers (with new code and configuration) and attempt a graceful
    shutdown on the old ones.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: uWSGI has at least three independent schemes for doing graceful reloads. Each
    of them is too complex to explain briefly, but its official documentation provides
    full information on all the possible options.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Graceful reloads are today a standard in deploying web applications. Gunicorn
    seems to have an approach that is the easiest to use but also leaves you with
    the least flexibility. Graceful reloads in uWSGI on the other hand allow much
    better control on reloads but require more effort to automate and setup. Also,
    how you handle graceful reloads in your automated deploys is also affected on
    what supervision tools you use and how they are configured. For instance, in Gunicorn,
    graceful reloads are as simple as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: But if you want to properly isolate project distributions by separating virtual
    environments for each release and configure process supervision using symbolic
    links (as presented in the `fabfile` example earlier), you will shortly notice
    that this does not work as expected. For more complex deployments, there is still
    no solution available that will just work for you out-of-the-box. You will always
    need to do a bit of hacking and sometimes this will require a substantial level
    of knowledge about low-level system implementation details.
  prefs: []
  type: TYPE_NORMAL
- en: Code instrumentation and monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our work does not end with writing an application and deploying it to the target
    execution environment. It is possible to write an application that after deployment
    will not require any further maintenance, although it is very unlikely. In reality,
    we need to ensure that it is properly observed for errors and performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'To be sure that our product works as expected, we need to properly handle application
    logs and monitor the necessary application metrics. This often includes:'
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring web application access logs for various HTTP status codes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A collection of process logs that may contain information about runtime errors
    and various warnings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring system resources (CPU load, memory, and network traffic) on remote
    hosts where the application is run
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring application-level performance and metrics that are business performance
    indicators (customer acquisition, revenue, and so on)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luckily there are a lot of free tools available for instrumenting your code
    and monitoring its performance. Most of them are very easy to integrate.
  prefs: []
  type: TYPE_NORMAL
- en: Logging errors – sentry/raven
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: No matter how precisely your application is tested, the truth is painful. Your
    code will eventually fail at some point. This can be anything—unexpected exception,
    resource exhaustion, some backing service crashing, network outage, or simply
    an issue in the external library. Some of the possible issues, such as resource
    exhaustion, can be predicted and prevented with proper monitoring, but there will
    be always something that passes your defences no matter how much you try.
  prefs: []
  type: TYPE_NORMAL
- en: What you can do is be well prepared for such scenarios and make sure that no
    error passes unnoticed. In most cases, any unexpected failure scenario results
    in an exception raised by the application and logged through the logging system.
    This can be `stdout`, `sderr`, file, or whatever output you have configured for
    logging. Depending on your implementation, this may or may not result in the application
    quitting with some system exit code.
  prefs: []
  type: TYPE_NORMAL
- en: You could, of course, depend only on such logs stored in files for finding and
    monitoring your application errors. Unfortunately, observing errors in textual
    logs is quite painful and does not scale well beyond anything more complex than
    running code in development. You will eventually be forced to use some services
    designed for log collection and analysis. Proper log processing is very important
    for other reasons that will be explained a bit later but does not work well for
    tracking and debugging production errors. The reason is simple. The most common
    form of error logs is just Python stack trace. If you stop only on that, you will
    soon realize that it is not enough to find the root cause of your issues—especially
    when errors occur in unknown patterns or in certain load conditions.
  prefs: []
  type: TYPE_NORMAL
- en: What you really need is as much context information about error occurrence as
    possible. It is also very useful to have a full history of errors that have occurred
    in the production environment that you can browse and search in some convenient
    way. One of the most common tools that gives such capabilities is Sentry ([https://getsentry.com](https://getsentry.com)).
    It is a battle-tested service for tracking exceptions and collecting crash reports.
    It is available as open source, is written in Python, and originated as a tool
    for backend web developers. Now it has outgrown its initial ambitions and has
    support for many more languages, including PHP, Ruby, and JavaScript, but still
    stays the most popular tool of choice for most Python web developers.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Exception stack tracebacks in web applications**'
  prefs: []
  type: TYPE_NORMAL
- en: It is common that web applications do not exit on unhandled exceptions because
    HTTP servers are obliged to return an error response with a status code from the
    5XX group if any server error occurs. Most Python web frameworks do such things
    by default. In such cases, the exception is in fact handled but on a lower framework-level.
    Anyway, this, in most cases, will still result in the exception stack trace being
    printed (usually on standard output).
  prefs: []
  type: TYPE_NORMAL
- en: 'Sentry is available in a paid software-as-a-service model, but it is open source,
    so it can be hosted for free on your own infrastructure. The library that provides
    integration with Sentry is `raven` (available on PyPI). If you haven''t worked
    with it yet, want to test it but have no access to your own Sentry server, then
    you can easily signup for a free trial on Sentry''s on-premise service site. Once
    you have access to a Sentry server and have created a new project, you will obtain
    a string called DSN, or Data Source Name. This DSN string is the minimal configuration
    setting needed to integrate your application with sentry. It contains protocol,
    credentials, server location, and your organization/project identifier in the
    following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you have DSN, the integration is pretty straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Raven has numerous integrations with the most popular Python frameworks, such
    as Django, Flask, Celery, and Pyramid, to make integration easier. These integrations
    will automatically provide additional context that is specific to the given framework.
    If your web framework of choice does not have dedicated support, the `raven` package
    provides generic WSGI middleware that makes it compatible with any WSGI-based
    web servers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The other notable integration is the ability to track messages logged through
    Python''s built-in `logging` module. Enabling such support requires only a few
    additional lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Capturing `logging` messages may have some not obvious caveats, so make sure
    to read the official documentation on that topic if you are interested in such
    a feature. This should save you from unpleasant surprises.
  prefs: []
  type: TYPE_NORMAL
- en: The last note is about running your own Sentry as a way to save some money.
    "There ain't no such thing as a free lunch." You will eventually pay additional
    infrastructure costs and Sentry will be just another service to maintain. *Maintenance
    = additional work = costs*! As your application grows, the number of exceptions
    grow, so you will be forced to scale Sentry as you scale your product. Fortunately,
    this is a very robust project, but will not give you any value if overwhelmed
    with too much load. Also, keeping Sentry prepared for a catastrophic failure scenario
    where thousands of crash reports per second can be sent is a real challenge. So
    you must decide which option is really cheaper for you, and whether you have enough
    resources and wit to do all of this by yourself. There is of course no such dilemma
    if security policies in your organization deny sending any data to third parties.
    If so, just host it on your own infrastructure. There are costs of course, but
    ones that are definitely worth paying.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring system and application metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When it comes to monitoring performance, the amount of tools to choose from
    may be overwhelming. If you have high expectations, then it is possible that you
    will need to use a few of them at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: '**Munin** ([http://munin-monitoring.org](http://munin-monitoring.org)) is one
    of the popular choices used by many organizations regardless of the technology
    stack they use. It is a great tool for analyzing resource trends and provides
    a lot of useful information even with default installation without additional
    configuration. Its installation consists of two main components:'
  prefs: []
  type: TYPE_NORMAL
- en: The Munin master that collects metrics from other nodes and serves metrics graphs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Munin node that is installed on a monitored host, which gathers local metrics
    and sends it to the Munin master
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Master, node, and most of the plugins are written in Perl. There are also node
    implementations in other languages: `munin-node-c` is written in C ([https://github.com/munin-monitoring/munin-c](https://github.com/munin-monitoring/munin-c))
    and `munin-node-python` is written in Python ([https://github.com/agroszer/munin-node-python](https://github.com/agroszer/munin-node-python)).
    Munin comes with a huge number of plugins available in its `contrib` repository.
    This means it provides out-of-the-box support for most of the popular databases
    and system services. There are even plugins for monitoring popular Python web
    servers such as uWSGI, and Gunicorn.'
  prefs: []
  type: TYPE_NORMAL
- en: The main drawback of Munin is the fact it serves graphs as static images and
    actual plotting configuration is included in specific plugin configurations. This
    does not help in creating flexible monitoring dashboards and comparing metric
    values from different sources at the same graph. But this is the price we need
    to pay for simple installation and versatility. Writing your own plugins is quite
    simple. There is the `munin-python` package ([http://python-munin.readthedocs.org/en/latest/](http://python-munin.readthedocs.org/en/latest/))
    that helps writing Munin plugins in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, the architecture of Munin that assumes that there is always a
    separate monitoring daemon process on every host that is responsible for collection
    of metrics may not be the best solution for monitoring custom application performance
    metrics. It is indeed very easy to write your own Munin plugins, but under the
    assumption that the monitoring process can already report its performance statistics
    in some way. If you want to collect some custom application-level metrics, it
    might be necessary to aggregate and store them in some temporary storage until
    reporting to a custom Munin plugin. It makes creation of custom metrics more complicated,
    so you might want to consider other solutions for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: 'The other popular solution that makes it especially easy to collect custom
    metrics is StatsD ([https://github.com/etsy/statsd](https://github.com/etsy/statsd)).
    It''s a network daemon written in Node.js that listens to various statistics such
    as counters, timers, and gauges. It is very easy to integrate, thanks to the simple
    protocol based on UDP. It is also easy to use the Python package named `statsd`
    for sending metrics to the StatsD daemon:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Because UDP is connectionless, it has a very low performance overhead on the
    application code so it is very suitable for tracking and measuring custom events
    inside the application code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, StatsD is the only metrics collection daemon, so it does not
    provide any reporting features. You need other processes that are able to process
    data from StatsD in order to see the actual metrics graphs. The most popular choice
    is Graphite ([http://graphite.readthedocs.org](http://graphite.readthedocs.org)).
    It does mainly two things:'
  prefs: []
  type: TYPE_NORMAL
- en: Stores numeric time-series data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Renders graphs of this data on demand
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graphite provides you with the ability to save graph presets that are highly
    customizable. You can also group many graphs into thematic dashboards. Graphs
    are, similarly to Munin, rendered as static images, but there is also the JSON
    API that allows other frontends to read graph data and render it by other means.
    One of the great dashboard plugins integrated with Graphite is Grafana ([http://grafana.org](http://grafana.org)).
    It is really worth trying because it has way better usability than plain Graphite
    dashboards. Graphs provided in Grafana are fully interactive and easier to manage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Graphite is unfortunately a bit of a complex project. It is not a monolithic
    service and consists of three separate components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Carbon**: This is a daemon written using Twisted that listens for time-series
    data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**whisper**: This is a simple database library for storing time-series data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**graphite webapp**: This is a Django web application that renders graphs on-demand
    as static images (using Cairo library) or as JSON data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When used with the StatsD project, the `statsd` daemon sends its data to `carbon`
    daemon. This makes the full solution a rather complex stack of various applications,
    where each of them is written using a completely different technology. Also, there
    are no preconfigured graphs, plugins, and dashboards available, so you will need
    to configure everything by yourself. This is a lot of work at the beginning and
    it is very easy to miss something important. This is the reason why it might be
    a good idea to use Munin as a monitoring backup even if you decide to have Graphite
    as your core monitoring service.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with application logs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While solutions such as Sentry are usually way more powerful than ordinary textual
    output stored in files, logs will never die. Writing some information to a standard
    output or file is one of the simplest things that an application can do and this
    should never be underestimated. There is a risk that messages sent to Sentry by
    raven will not get delivered. The network can fail. Sentry's storage can get exhausted
    or may not be able to handle incoming load. Your application might crash before
    any message is sent (with segmentation fault, for example). These are only a few
    of the possible scenarios. What is less likely is your application won't be able
    to log messages that are going to be written to the filesystem. It is still possible,
    but let's be honest. If you face such a condition where logging fails, probably
    you have a lot more burning issues than some missing log messages.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that logs are not only about errors. Many developers used to think
    about logs only as a source of data that is useful when debugging issues and/or
    which can be used to perform some kind of forensics. Definitely, less of them
    try to use it as a source for generating application metrics or to do some statistical
    analysis. But logs may be a lot more useful than that. They can be even the core
    of the product implementation. A great example of building a product with logs
    is the Amazon article presenting an example architecture for a real-time bidding
    service, where everything is centered around access log collection and processing.
    See [https://aws.amazon.com/blogs/aws/real-time-ad-impression-bids-using-dynamodb/](https://aws.amazon.com/blogs/aws/real-time-ad-impression-bids-using-dynamodb/).
  prefs: []
  type: TYPE_NORMAL
- en: Basic low-level log practices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Twelve-Factor App says that logs should be treated as event streams. So
    a log file is not a log per se, but only an output format. The fact that they
    are streams means they represent time ordered events. In raw, they are typically
    in a text format with one line per event, although in some cases they may span
    multiple lines. This is typical for any backtraces related to run-time errors.
  prefs: []
  type: TYPE_NORMAL
- en: According to the Twelve-Factor App methodology, the application should never
    be aware of the format in which logs are stored. This means that writing to the
    file, or log rotation and retention should never be maintained by the application
    code. These are the responsibilities of the environment in which applications
    are run. This may be confusing because a lot of frameworks provide functions and
    classes for managing log files as well as rotation, compression, and retention
    utilities. It is tempting to use them because everything can be contained in your
    application codebase, but actually it is an anti-pattern that should be really
    avoided.
  prefs: []
  type: TYPE_NORMAL
- en: 'The best conventions for dealing with logs can be closed in a few rules:'
  prefs: []
  type: TYPE_NORMAL
- en: The application should always write logs unbuffered to the standard output (`stdout`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The execution environment should be responsible for the collection and routing
    of logs to the final destination
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main part of the mentioned execution environment is usually some kind of
    process supervision tool. The popular Python solutions, such as Supervisor or
    Circus, are the first ones responsible for dealing with log collection and routing.
    If logs are to be stored in the local filesystem, then only they should write
    to actual log files.
  prefs: []
  type: TYPE_NORMAL
- en: Both Supervisor and Circus are also capable of handling log rotation and retention
    for managed processes but you should really consider whether this is a path that
    you want to go. Successful operations are mostly about simplicity and consistency.
    Logs of your own application are probably not the only ones that you want to process
    and archive. If you use Apache or Nginx as a reverse proxy, you might want to
    collect their access logs. You might also want to store and process logs for caches
    and databases. If you are running some popular Linux distribution, then the chances
    are very high that each of these services have their own log files processed (rotated,
    compressed, and so on) by the popular utility named `logrotate`. My strong recommendation
    is to forget about Supervisor's and Circus' log rotation capabilities for the
    sake of consistency with other system services. `logrotate` is way more configurable
    and also supports compression.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**logrotate and Supervisor/Circus**'
  prefs: []
  type: TYPE_NORMAL
- en: There is an important thing to know when using `logrotate` with Supervisor or
    Circus. The rotation of logs will always happen while the process Supervisor still
    has an open descriptor to rotated logs. If you don't take proper countermeasures,
    then new events will be still written to file descriptor that was already deleted
    by `logrotate`. As a result, nothing more will be stored in a filesystem. Solutions
    to this problem are quite simple. Configure `logrotate` for log files of processes
    managed by Supervisor or Circus with the `copytruncate` option. Instead of moving
    the log file after rotation, it will copy it and truncate the original file to
    zero size in-place. This approach does not invalidate any of existing file descriptors
    and processes that are already running can write to log files uninterrupted. The
    Supervisor can also accept the `SIGUSR2` signal that will make it reopen all the
    file descriptors. It may be included as the `postrotate` script in the `logrotate`
    configuration. This second approach is more economical in the terms of I/O operations
    but is also less reliable and harder to maintain.
  prefs: []
  type: TYPE_NORMAL
- en: Tools for log processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you have no experience of working with large amounts of logs, you will eventually
    gain it when working with a product that has some substantial load. You will shortly
    notice that a simple approach based on storing them in files and backing in some
    persistent storage for later retrieval is not enough. Without proper tools, this
    will become crude and expensive. Simple utilities such as `logrotate` help you
    only to ensure that the hard disk is not overflown by the ever-increasing amount
    of new events, but splitting and compressing log files only helps in the data
    archival process but does not make data retrieval or analysis simpler.
  prefs: []
  type: TYPE_NORMAL
- en: When working with distributed systems that span multiple nodes, it is nice to
    have a single central point from which all logs can be retrieved and analyzed.
    This requires a log processing flow that goes way beyond simple compression and
    backing up. Fortunately this is a well-known problem so there are many tools available
    that aim to solve it.
  prefs: []
  type: TYPE_NORMAL
- en: One of the popular choices among many developers is **Logstash**. This is the
    log collection daemon that can observe active log files, parse log entries and
    send them to the backing service in a structured form. The choice of backing stays
    almost always the same—**Elasticsearch**. Elasticsearch is the search engine built
    on top of Lucene. Among text search capabilities, it has a unique data aggregation
    framework that fits extremely well into the purpose of log analysis.
  prefs: []
  type: TYPE_NORMAL
- en: The other addition to this pair of tools is **Kibana**. It is a very versatile
    monitoring, analysis, and visualization platform for Elasticsearch. The way how
    these three tools complement each other is the reason why they are almost always
    used together as a single stack for log processing.
  prefs: []
  type: TYPE_NORMAL
- en: The integration of existing services with Logstash is very simple because it
    can listen on existing log files changes for the new events with only minimal
    changes in your logging configuration. It parses logs in textual form and has
    preconfigured support for some of the popular log formats, such as Apache/Nginx
    access logs. The only problem with Logstash is that it does not handle log rotation
    well, and this is a bit surprising. Forcing a process to reopen its file descriptors
    by sending one of the defined Unix signals (usually `SIGHUP` or `SIGUSR1`) is
    a pretty well-established pattern. It seems that every application that deals
    with logs (exclusively) should know that and be able to process various log file
    rotation scenarios. Sadly, Logstash is not one of them, so if you want to manage
    log retention with the `logrotate` utility, remember to rely heavily on its `copytruncate`
    option. The Logstash process can't handle situations when the original log file
    was moved or deleted, so without the `copytruncate` option it wouldn't be able
    to receive new events after log rotation. Logstash can of course handle different
    inputs of log streams such as UDP packets, TCP connections, or HTTP requests.
  prefs: []
  type: TYPE_NORMAL
- en: The other solution that seems to fill some of Logstash gaps is Fluentd. It is
    an alternative log collection daemon that can be used interchangeably with Logstash
    in the mentioned log monitoring stack. It also has an option to listen and parse
    log events directly in log files, so minimal integration requires only a little
    effort. In contrast to Logstash, it handles reloads very well and does not even
    need to be signaled if log files were rotated. Anyway, the biggest advantage comes
    from using one of its alternative log collection options that will require some
    substantial changes to logging configuration in your application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fluentd really treats logs as event streams (as recommended by the Twelve-Factor
    App). The file-based integration is still possible but it is only kind of backward
    compatibility for legacy applications that treat logs mainly as files. Every log
    entry is an event and it should be structured. Fluentd can parse textual logs
    and has multiple plugin options to handle:'
  prefs: []
  type: TYPE_NORMAL
- en: Common formats (Apache, Nginx, and syslog)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arbitrary formats specified using regular expressions or handled with custom
    parsing plugins
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generic formats for structured messages such as JSON
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The best event format for Fluentd is JSON because it adds the least amount of
    overhead. Messages in JSON can be also passed almost without any change to the
    backing service like Elasticsearch or the database.
  prefs: []
  type: TYPE_NORMAL
- en: 'The other very useful feature of Fluentd is the ability to pass event streams
    using transports other than a log file written to the disk. Most notable built-in
    input plugins are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`in_udp`: With this plugin every log event is sent as UDP packets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`in_tcp`: With this plugin events are sent through TCP connection'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`in_unix`: With this plugin events are sent through Unix Domain Socket (names
    socket)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`in_http`: With this plugin events are sent as HTTP POST requests'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`in_exec`: With this plugin Fluentd process executes an external command periodically
    to pull events in the JSON or MessagePack format'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`in_tail`: With this plugin Fluentd process listens for an event in a textual
    file'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alternative transports for log events may be especially useful in situations
    where you need to deal with poor I/O performance of machine storage. It is very
    often on cloud computing services that the default disk storage has a very low
    number of **IOPS** (**Input Output Operations Per Second**) and you need to pay
    a lot of money for better disk performance. If your application outputs large
    amount of log messages, you can easily saturate your I/O capabilities even if
    the data size is not very high. With alternate transports, you can use your hardware
    more efficiently because you leave the responsibility of data buffering only to
    a single process—log collector. When configured to buffer messages in memory instead
    of disk, you can even completely get rid of disk writes for logs, although this
    may greatly reduce the consistency guarantees of collected logs.
  prefs: []
  type: TYPE_NORMAL
- en: Using different transports seems to be slightly against the 11th rule of the
    Twelve-Factor App methodology. Treat logs as event streams when explained in detail
    suggests that the application should always log only through a single standard
    output stream (`stdout`). It is still possible to use alternate transports without
    breaking this rule. Writing to `stdout` does not necessarily mean that this stream
    must be written to file. You can leave your application logging that way and wrap
    it with an external process that will capture this stream and pass it directly
    to Logstash or Fluentd without engaging the filesystem. This is an advanced pattern
    that may not be suitable for every project. It has an obvious disadvantage of
    higher complexity, so you need to consider by yourself whether it is really worth
    doing.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Code deployment is not a simple topic and you should already know that after
    reading this chapter. Extensive discussion of this problem could easily take a
    few books. Even though we limited our scope exclusively to web application, we
    have barely scratched the surface. This chapter takes as a basis the Twelve-Factor
    App methodology. We discussed in detail only a few of them: log treatment, managing
    dependencies, and separating build/run stages.'
  prefs: []
  type: TYPE_NORMAL
- en: After reading this chapter, you should know how to properly automate your deployment
    process, taking into consideration best practices, and be able to add proper instrumentation
    and monitoring for code that is run on your remote hosts.
  prefs: []
  type: TYPE_NORMAL
