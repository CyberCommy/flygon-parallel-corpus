- en: Using LXML, XPath, and CSS Selectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have learned about web-development technologies, data-finding techniques,
    and accessing web content using the Python programming language.
  prefs: []
  type: TYPE_NORMAL
- en: Web-based content exists in parts or elements using some predefined document
    expressions. Analyzing these parts for patterns is a major task for processing
    convenient scraping. Elements can be searched and identified with XPath and CSS
    selectors that are processed with scraping logic for required content. lxml will
    be used to process elements inside markup documents. We will be using browser-based
    development tools for content reading and element identification.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to XPath and CSS selectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using browser developer tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning and scraping using the Python lxml library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A web browser (Google Chrome or Mozilla Firefox) is required and we will be
    using the following Python libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: lxml
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Requests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the preceding libraries do not exist with the current Python setup, for setting
    up or installation, refer to the *Setting things up* section in the last chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Code files are available online on GitHub: [https://github.com/PacktPublishing/Hands-On-Web-Scraping-with-Python/tree/master/Chapter03](https://github.com/PacktPublishing/Hands-On-Web-Scraping-with-Python/tree/master/Chapter03).'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to XPath and CSS selector
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the *Understanding web development and technologies* section in [Chapter
    1](af7787bb-7fcf-4101-8680-9bad14bf22e1.xhtml), *Web Scraping Fundamentals*, we
    introduced XML as a document that contains data that is exchangeable and distributable
    across various technologies related to the web and documents. XML carries user-defined
    tags, also known as nodes, which hold data in a tree-like structure.
  prefs: []
  type: TYPE_NORMAL
- en: A tree-type structure (also known as an element-tree) is a base model for most
    markup languages and is often referred to as the **Document Object Model** (**DOM**).
    With the help of the DOM and its defined conventions, we can access, traverse,
    and manipulate elements.
  prefs: []
  type: TYPE_NORMAL
- en: Elements are structured inside some parent elements, which are inside their
    own parent and so on; this describes a parent-child relationship that is the most
    significant feature of markup language. Many applications that support XML or
    markup language supports the DOM and even contain a parser to use.
  prefs: []
  type: TYPE_NORMAL
- en: For extraction, it is necessary to identify the exact location of information.
    Information can be found nested inside a tree structure and could possess some
    additional attributes to represent the content. XPath and CSS selectors are both
    used to navigate along the DOM and search for desired elements or nodes found
    in the document.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will introduce both XPath and CSS selectors, and
    use them for a web-scraping purpose with a supportive Python library.
  prefs: []
  type: TYPE_NORMAL
- en: XPath
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **XML Path** (**XPath**) language is a part of XML-based technologies (XML,
    XSLT, and XQuery), which deal with navigating through DOM elements or locating
    nodes in XML (or HTML) documents using expressions also known as XPath expressions.
    XPath is normally a path that identifies nodes in documents. XPath is also a **W3C**
    (short for **World Wide Web Consortium**) recommendation ([https://www.w3.org/TR/xpath/all/](https://www.w3.org/TR/xpath/all/))[.](https://www.w3.org/TR/xpath/all/)
  prefs: []
  type: TYPE_NORMAL
- en: 'XPath or XPath expressions are also identified as absolute and relative:'
  prefs: []
  type: TYPE_NORMAL
- en: The absolute path is an expression that represents a complete path from the
    root element to the desired element. It begins with `/html` and looks like `/html/body/div[1]/div/div[1]/div/div[1]/div[2]/div[2]/div/span/b[1]`.
    Individual elements are identified with their position and represented by an index
    number.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The relative path represents an expression chosen from certain selected elements
    to the desired element. Relative paths are shorter and readable in comparison
    to absolute paths and look like `//*[@id="answer"]/div/span/b[@class="text"]`*.*
    A relative path is often preferred over an absolute path as element indexes, attributes,
    logical expressions, and so on can be combined and articulated in a single expression.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With XPath expressions, we can navigate hierarchically through elements and
    reach the targeted one. XPath is also implemented by various programming languages,
    such as JavaScript, Java, PHP, Python, and C++. Web applications and browsers
    also have built-in support to XPath.
  prefs: []
  type: TYPE_NORMAL
- en: Expressions can be built using a number of built-in functions available for
    various data types. Operations related to general math (+, -, *, /), comparison
    (<, >, =, !=, >=, <=), and combination operators (`and`, `or`, and `mod`) can
    also be used to build expression. XPath is also a core block for XML technologies
    such as XQuery and **eXtensible Stylesheet Language Transformations** (**XSLT**).
  prefs: []
  type: TYPE_NORMAL
- en: '**XML Query** (**XQuery**) is a query language that uses XPath expressions
    to extract data from XML document.'
  prefs: []
  type: TYPE_NORMAL
- en: XSLT is used to render XML in a more readable format.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s explore a few XPath expressions from the XML content as seen in the
    following from the `food.xml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/84e3a184-9a6f-49a1-a378-0de23cc11d41.png)'
  prefs: []
  type: TYPE_IMG
- en: XML content
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, we will be using XPath-Tester from Code Beautify ([https://codebeautify.org/Xpath-Tester](https://codebeautify.org/Xpath-Tester)).
    Use the XML source URL provided earlier to fetch the XML content and use it with
    the Code Beautify XPath-Tester.
  prefs: []
  type: TYPE_NORMAL
- en: You can use [https://codebeautify.org/Xpath-Tester](https://codebeautify.org/Xpath-Tester), [https://www.freeformatter.com/xpath-tester.htm](https://www.freeformatter.com/xpath-tester.html),
    or any other XPath tester tools that are available free on the web.
  prefs: []
  type: TYPE_NORMAL
- en: Everything is a node in an XML document, for example, `menus`, `food`, and `price`.
    An XML node can be an element itself (elements are types or entities that have
    start and end tags).
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding XML document can also be read as inherited element blocks. Parent
    node `menus` contain multiple child nodes `food`, which distinguishes child elements
    for appropriate values and proper data types. The XPath expression, `//food`,
    as shown in the following screenshot, displays the result for the selected node
    `food`. Node selection also retrieves the child nodes within the parents, as seen
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/595692c6-3dd2-4fad-bd74-d17dd48e3a45.png)'
  prefs: []
  type: TYPE_IMG
- en: Result for XPath //food (using https://codebeautify.org/Xpath-Tester)
  prefs: []
  type: TYPE_NORMAL
- en: 'The XPath expression in the following screenshot selects the child node, `price`,
    found inside all parent nodes `food`. There are six child `food` nodes available,
    each of them containing `price`*,* `name`*,* `description`, `feedback`, and `rating`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/b527b33b-10c7-46e0-ac56-29da6549590c.png)'
  prefs: []
  type: TYPE_IMG
- en: Result for XPath //food/price (using https://codebeautify.org/Xpath-Tester)
  prefs: []
  type: TYPE_NORMAL
- en: As we can see from the two preceding XPaths tested, expressions are created
    almost like a filesystem (command line or Terminal path), which we use in various
    OS. XPath expressions contain code patterns, functions, and conditional statements
    and support the use of predicates.
  prefs: []
  type: TYPE_NORMAL
- en: Predicates are used to identify a specific node or element. Predicate expressions
    are written using square brackets that are similar to Python lists or array expressions.
  prefs: []
  type: TYPE_NORMAL
- en: 'A brief explanation of the XPath expression given in the preceding XML is listed
    in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **XPath expression** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `//` | Selects nodes in the document, no matter where they are located |'
  prefs: []
  type: TYPE_TB
- en: '| `//*` | Selects all elements in the document |'
  prefs: []
  type: TYPE_TB
- en: '| `//food` | Selects the element `food` |'
  prefs: []
  type: TYPE_TB
- en: '| `*` | Selects all elements |'
  prefs: []
  type: TYPE_TB
- en: '| `//food/name &#124; //food/price` | Selects the `name` and `price` elements
    found in the `food` node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `//food/name` | Selects all the `name` elements inside `food`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `//food/name/text()` | Selects the `text` only for all `food/name` elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `//food/name &#124; //rating` | Selects all `name` elements from `food` and
    `rating` found in document:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `//food[1]/name` | Selects the `name` element for the first `food` node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `//food[feedback<9]` | Select the `food` node and all of its elements where
    the predicate condition, `feedback<9`, is true:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `//food[feedback<9]/name` | Selects the `food` node and the `name` element
    that matches the condition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `//food[last()]/name` | Selects the `name` element from the last `food` node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `//food[last()]/name/text()` | Selects `text` for the `name` element from
    the last `food` node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `sum(//food/feedback)` | Provides the sum of feedback found in all `food`:nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `//food[rating>3 and rating<5]/name` | Selects the `name` of `food` that
    fulfills the predicate condition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `//food/name[contains(.,"Juice")]` | Selects the `name` of `food` that contains
    the `Juice` string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `//food/description[starts-with(.,"Fresh")]/text()` | Selects the node description
    that starts with `Fresh`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `//food/description[starts-with(.,"Fresh")]` | Selects `text` from `description` node
    that starts with `Fresh`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `//food[position()<3]` | Selects the first and second food according to its
    position:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: XPath predicates can contain a numeric index that starts from `1` (not `0`)
    and conditional statements, for example, `//food[1]` or `//food[last()]/price`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have tested the preceding XML with various XPath expressions, let''s
    consider a simple XML with some attributes. Attributes are extra properties that
    identify certain parameters for a given node or element. A single element can
    contain a unique attributes set. Attributes found in XML nodes or HTML elements
    help to identify the unique element with the value it contains. As we can see
    in the code in the following XML, attributes are found as a `key=value` pair of
    information, for example `id="1491946008"`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: XPath expression accepts `key` attributes by adding the `@` character in front
    of the key name. Listed in the following table are a few examples of XPath using
    attributes with a brief description.
  prefs: []
  type: TYPE_NORMAL
- en: '| **XPath** **e****xpression** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `//book/@price` | Selects the `price` attribute for a `book`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `//book` | Selects the `book` field and its elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `//book[@price>30]` | Selects all elements in `book` the `price` attribute
    of which is greater than `30`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `//book[@price<30]/title` | Selects `title` from books where the `price`
    attribute is less than `30`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `//book/@id` | Selects the `id` attribute and its value. The `//@id` expression
    also results in the same output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `//book[@id=1491939362]/author` | Selects `author` from `book` where `id=1491939362`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: We have tried to explore and learn a few basic features about XPath and writing
    expressions to retrieve the desired content. In the *Scraping using lxml - a Python
    library* section, we will use Python programming libraries to further explore
    deploying code using XPath to scrape provided documents (XML or HTML) and learn
    to generate or create XPath expressions using browser tools. For more information
    on XPaths please refer to the links in the *Further reading* section.
  prefs: []
  type: TYPE_NORMAL
- en: CSS selectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 1](af7787bb-7fcf-4101-8680-9bad14bf22e1.xhtml), *Web Scraping Fundamentals*,
    under the *Understanding web development and technologies* section, we learned
    about CSS and its use to style HTML elements plus we learned about using global
    attributes. CSS is normally used to style HTML and there are various ways to apply
    CSS to the HTML.
  prefs: []
  type: TYPE_NORMAL
- en: CSS selectors (also referred to as CSS query or CSS selector query) are defined
    patterns used by CSS to select HTML elements, using the element name or global
    attributes (`ID`, and `Class`). CSS selectors, as the name suggests, select or
    provide the option to select HTML elements in various ways.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example code, we can visualize a few elements found in `<body>`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`<h1>` is an element and a selector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `<p>` element or selector has the `class` attribute with the `header` style
    type. When it comes to selecting, `<p>` we can use either the element name, the
    attribute name, or just the type name.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Multiple `<a>` are found inside `<div>`, but they differ with their `class`
    attribute, `id`, and value for the `href` property:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The distinguishable patterns we have identified in the preceding code can be
    used to select those particular elements individually or in groups. Numbers of
    DOM parsers are available online, which provide a CSS query-related facility.
    One of them, as shown in the following screenshot, is [https://try.jsoup.org/](https://try.jsoup.org/):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/c543073e-d7dd-4219-a1ce-fa384b2f66b8.png)'
  prefs: []
  type: TYPE_IMG
- en: Evaluating CSS query from https://try.jsoup.org/The DOM parser converts provided
    XML or HTML into a DOM object or tree type of structure, which facilitates accessing
    and manipulating element or tree nodes. For more detail information on the DOM,
    please visit [https://dom.spec.whatwg.org/](https://dom.spec.whatwg.org/).
  prefs: []
  type: TYPE_NORMAL
- en: 'In a CSS query, various symbols, as listed in the following code text, represent
    certain characteristics and can be used inside a CSS query:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The global `id` attribute and `class` are represented by `#` and `.`, respectively,
    as seen in this query:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`a#link`: `<a id="link" href="mailto:xyz@domain.com">Email Link1!</a>`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`a.plan`: `<a class="plan" href="*.pdf">Document Places</a>`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Combinators (showing the relationship between elements) are also used, such
    as `+`, `>`, `~`, and the space character, as seen in the query here:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`h1 + p`: `<p class=”header”>Page Header</p>`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`div.links a.plan`: `<a class="plan" href="*.pdf">Document Places</a>`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Operators, such as `^`, `*`, `$` are used for positioning and selecting, as
    seen in this query:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`a[href$="pdf"]`: `<a class="plan" href="*.pdf">Document Places</a>`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`a[href^="mailto"]`: `<a id="link" href="mailto:xyz@domain.com">Email Link1!</a><a
    href="mailto:abc@domain.com">Email Link2!</a>`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These symbols are used and explained side-by-side, referring to the preceding
    HTML code with various types of selectors, in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Element selectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Element selectors are basic selectors that choose elements from HTML. Most
    often, these elements are the basic tags of HTML. The following table lists some
    of the selectors and their usage for this category:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **CSS query** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `h1` | Selects `<h1>` elements |'
  prefs: []
  type: TYPE_TB
- en: '| `a` | Selects all of the `<a>` elements |'
  prefs: []
  type: TYPE_TB
- en: '| `*` | Selects all elements in the HTML code |'
  prefs: []
  type: TYPE_TB
- en: '| `body *` | Selects all `<h1>`, `<p>`, `<div>`, and `<a>` elements inside
    `<body>` |'
  prefs: []
  type: TYPE_TB
- en: '| `div a` | Selects all `<a>` inside `<div>` (using space character in between)
    |'
  prefs: []
  type: TYPE_TB
- en: '| `h1 + p` | Selects immediate `<p>` elements after `<h1>` |'
  prefs: []
  type: TYPE_TB
- en: '| `h1 ~ p` | Selects every `<p>` elements preceded by `<h1>` |'
  prefs: []
  type: TYPE_TB
- en: '| `h1,p` | Selects all `<h1>` and `<p>` elements |'
  prefs: []
  type: TYPE_TB
- en: '| `div > a` | Selects all `<a>` elements that are a direct child of `<div>`
    |'
  prefs: []
  type: TYPE_TB
- en: ID and class selectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ID and class selectors are additional features available with element selectors.
    We can find HTML tags with the `class` and `id` attributes. These are also known
    as global attributes. These attributes are mostly preferred over other attributes
    as they define the tags for structure and with identification.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more details on global attributes, please refer to [Chapter 1](af7787bb-7fcf-4101-8680-9bad14bf22e1.xhtml),
    *Web Scraping Fundamentals*, the *Global attributes* section. The following table
    lists the usage of this category of selectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **CSS query** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `.header` | Selects an element with `class=header` |'
  prefs: []
  type: TYPE_TB
- en: '| `.plan` | Selects `<a>` with `class=plan` |'
  prefs: []
  type: TYPE_TB
- en: '| `div.links` | Selects `<div>` with `class=plan` |'
  prefs: []
  type: TYPE_TB
- en: '| `#link` | Selects an element with `id=link` |'
  prefs: []
  type: TYPE_TB
- en: '| `a#link` | Selects `<a>` elements with `id=link` |'
  prefs: []
  type: TYPE_TB
- en: '| `a.plan` | Selects `<a>` elements with `class=plan` |'
  prefs: []
  type: TYPE_TB
- en: Attribute selectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Attribute selectors are used to define selectors with the available attributes.
    HTML tags contain an attribute that helps to identify a particular element with
    the attribute and the value that it carries.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table lists a few ways to show the usage of attribute selectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **CSS query** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `a[href*="domain"]` | Selects `<a>` elements that contain the `domain` substring
    in its `href`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `a[href^="mailto"]` | Selects `<a>` elements that start with the `mailto`
    substring of the `href` attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `a[href$="pdf"]` | Selects `<a>` elements that have a `pdf` substring at
    the end of its `href` attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `[href~=do]` | Selects all elements with the `href` attribute and matches
    `do` in values. The two `<a>` elements listed in the following both contain `do`
    inside of their `href` value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `[class]` | Selects all elements or `<p>`, `<div>`, and `<a>` with the `class`
    attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `[class=plan]` | Selects `<a>` with `class=plan`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Pseudo selectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pseudo selectors are a set of handy choices when it comes to identifying or
    selecting the elements based on their position.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table lists some of the ways these types of selectors might be
    used, with a brief description:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **CSS query** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `a:gt(0)` | Selects all `<a>` elements except those indexed at a `0` position:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `a:eq(2)` | Selects `<a>` element which are indexed at `2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `a:first-child` | Selects every `<a>` element that is the first child of
    its parent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `a:last-child` | Selects every `<a>` element that is the last child of its
    parent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `a:last-of-type` | Selects the last element `<a>` of its parent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `:not(p)` | Selects all elements except `<p>`. |'
  prefs: []
  type: TYPE_TB
- en: '| `a:nth-child(1)` | Selects every `<a>` from the first child of its parent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `a:nth-last-child(3)` | Selects every third `<a>` from the last child of
    its parent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `a:nth-of-type(3)` | Selects every third `<a>` element of its parent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `a:nth-last-of-type(3)` | Selects every `<a>` element, at the third position
    from last, of its parent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: CSS selectors are used as a convenient alternative to XPath expressions for
    selecting elements, as they are shorter in length compared to absolute XPath and
    use simple patterns in expressions that are easy to read and manage. CSS selectors
    can be converted into XPath expressions, but not vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are also a number of tools available online, which allow the conversion
    of a CSS selector query into an XPath expression; one of these is [https://css-selector-to-xpath.appspot.com/](https://css-selector-to-xpath.appspot.com/),
    as seen in the following screenshot; we shouldn''t always trust the tools available
    and results should be tested before applying them in code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/c7f2e6a1-0110-4135-822c-97efabd35133.png)'
  prefs: []
  type: TYPE_IMG
- en: CSS selector to XPath converter
  prefs: []
  type: TYPE_NORMAL
- en: As described in the preceding screenshot, CSS selectors are used to select elements
    from a data extraction perspective and can be used in `Scraper` codes or even
    while applying styles to selected elements from a styling perspective.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned about the most popular web-related pattern-finding
    techniques of XPath and CSS selectors. In the next section, we will explore browser-based
    developer tools (DevTools) and learn to use the features inside DevTools. DevTools
    can be used to search, analyze, identify, and select elements and obtain XPath
    expressions and CSS selectors.
  prefs: []
  type: TYPE_NORMAL
- en: Using web browser developer tools for accessing web content
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 1](af7787bb-7fcf-4101-8680-9bad14bf22e1.xhtml), *Web Scraping Fundamentals*,
    under the *Data finding techniques* (s*eeking data from the web)* section and
    inside *Developer tools (DevTools),* we introduced browser-based DevTools to locate
    content and explore the various panels found. DevTools offers various functional
    panels, which provide us with supportive tools to manage related resources.
  prefs: []
  type: TYPE_NORMAL
- en: In this particular section, our motive will be specific to identifying the particular
    elements that hold the content we are looking for. This identification-based information,
    such as XPath expression, CSS query, or even DOM-based navigation flow will be
    beneficial while coding `Scraper`.
  prefs: []
  type: TYPE_NORMAL
- en: We will explore web pages using Google Chrome. Chrome has a built-in developer
    tool with plenty of features (available for element identification, selection,
    DOM navigation, and so on). In the following sections, we will explore and use
    these features.
  prefs: []
  type: TYPE_NORMAL
- en: HTML elements and DOM navigation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will be using [http://books.toscrape.com/](http://books.toscrape.com/) from
    [http://toscrape.com/](http://toscrape.com/). `toscrape` provides resources related
    to web scraping for beginners and developers to learn and implement `Scraper`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s open the [http://books.toscrape.com](http://books.toscrape.com) URL
    using the web browser, Google Chrome, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/55b6f040-3c78-4824-857a-b759ba4254d6.png)Inspect view of books.toscrape.com'
  prefs: []
  type: TYPE_NORMAL
- en: As the page content is successfully loaded, we can load DevTools with a right-click
    on the page and press the option Inspect or by pressing *Ctrl* + *Shift* + *I*.
    If accessing through the Chrome menu, click More Tools and Developer Tools. The
    browser should look similar to the content in the preceding screenshot.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the preceding screenshot, in inspect mode, the following
    is loaded:'
  prefs: []
  type: TYPE_NORMAL
- en: Panel elements are default on the left-hand side.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CSS styles-based content is on the right-hand side.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We notice the DOM navigation or elements path in the bottom left-hand corner,
    for example, `html.no-js body .... div.page_inner div.row`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We have covered a basic overview of such panels in [Chapter 1](af7787bb-7fcf-4101-8680-9bad14bf22e1.xhtml),
    *Web Scraping Fundamentals*, in the *Developer Tools* section. As developer tools
    get loaded, we can find a pointer-icon listed, at first, from the left; this is
    used for selecting elements from the page, as shown in the following screenshot;
    this element selector (inspector) can be turned ON/OFF using *Ctrl* + *Shift*
    + *C*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/5a5ec3a7-22fb-4e7e-bdf1-86827e38dd81.png)'
  prefs: []
  type: TYPE_IMG
- en: Element selector (inspector) on inspect bar
  prefs: []
  type: TYPE_NORMAL
- en: 'We can move the mouse on the page loaded after turning ON the element selector.
    Basically, we are searching for the exact HTML element that we are pointing to
    using the mouse:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/a9235959-772d-4060-b3f5-e1735651d65b.png)'
  prefs: []
  type: TYPE_IMG
- en: Using element selector on the book image
  prefs: []
  type: TYPE_NORMAL
- en: 'As seen in the preceding screenshot, the element has been selected and, as
    we move the mouse over the first book picture available, this action results in
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The `div.image_container` element is displayed and selected in the page itself.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inside the elements panel source, we can find the particular HTML code, `<div
    class="image_container">`, being highlighted too. This information (where the
    book picture is located) can also be found using right-click + page source or
    *Ctrl* + *U* and searching for the specific content.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The same action can be repeated for various sections of HTML content that we
    wish to scrape, as in the following examples:'
  prefs: []
  type: TYPE_NORMAL
- en: The price for a listed book is found inside the `div.product_price` element.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The star-rating is found inside `p.star-rating`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The book title is found inside `*<*h3>`, found before `div.product_price` or
    after `p.star-rating`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The book detail link is found inside `<a>`, which exists inside `<h3>`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'From the following screenshot, it''s also clear that the previously listed
    elements are all found inside `article.product_prod`. Also, at the bottom of the
    following screenshot, we can identify the DOM path as `article.product_prod`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](assets/be32e5d0-f89c-4306-9df6-88b96ef47857.png)'
  prefs: []
  type: TYPE_IMG
- en: Element selection under inspect mode
  prefs: []
  type: TYPE_NORMAL
- en: DOM navigation, as found in the preceding screenshots, can be beneficial while
    dealing with XPath expressions, and can verify the content using the page source,
    if the path or element displayed by the element inspector actually exists (inside
    the obtained page source).
  prefs: []
  type: TYPE_NORMAL
- en: DOM elements, navigation paths, and elements found using the elements inspector
    or selectors should be cross-verified for their existence in page sources or inside
    resources that are found in Network panels, to be sure.
  prefs: []
  type: TYPE_NORMAL
- en: XPath and CSS selectors using DevTools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will be collecting XPath expressions and CSS queries for
    the required element. In a similar way to how we explored the Page Inspector and
    Elements panel in the preceding section, let''s proceed with the following steps
    to obtain an XPath expression and CSS query for the selected element:'
  prefs: []
  type: TYPE_NORMAL
- en: Choose the Element Selector and obtain the element code
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Right-click the mouse on the element code obtained
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose the Copy option from the menu
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the sub-menu options, choose Copy XPath for XPath expression for chosen
    element
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Or choose Copy selector for the CSS selector (query)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As seen in the following screenshot, we select various sections of a single
    book item and obtain respective CSS selectors or XPath expressions, accessing
    the menu options:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/7f53f2c4-ef0a-4b88-ba6d-56beb60c066f.png)'
  prefs: []
  type: TYPE_IMG
- en: Copying XPath and CSS selector using page inspect
  prefs: []
  type: TYPE_NORMAL
- en: The following are some XPath and CSS selectors collected using DevTools for
    items available with products such as book title and price.
  prefs: []
  type: TYPE_NORMAL
- en: '**XPath selectors** using DevTools:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Book title: `//*[@id="default"]/div/div/div/div/section/div[2]/ol/li[1]/article/h3/a`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Price: `//*[@id="default"]/div/div/div/div/section/div[2]/ol/li[1]/article/div[2]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Image: `//*[@id="default"]/div/div/div/div/section/div[2]/ol/li[1]/article/div[1]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stock information: `//*[@id="default"]/div/div/div/div/section/div[2]/ol/li[1]/article/div[2]/p[2]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Star rating: `//*[@id="default"]/div/div/div/div/section/div[2]/ol/li[1]/article/p`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CSS query selectors** using DevTools:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Book title: `#default > div > div > div > div > section > div:nth-child(2)
    > ol > li:nth-child(1) > article > h3 > a`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Price: `#default > div > div > div > div > section > div:nth-child(2) > ol
    > li:nth-child(1) > article > div.product_price`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Image: `#default > div > div > div > div > section > div:nth-child(2) > ol
    > li:nth-child(1) > article > div.image_container`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stock info: `#default > div > div > div > div > section > div:nth-child(2)
    > ol > li:nth-child(1) > article > div.product_price > p.instock.availability`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Star rating: `#default > div > div > div > div > section > div:nth-child(2)
    > ol > li:nth-child(1) > article > p.star-rating`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly other essential XPath or CSS selectors will also be collected as required.
    After collection and verification or cleaning (shortening) of these expressions
    and queries, scraping logic is applied using Python programming to automate the
    data collection.
  prefs: []
  type: TYPE_NORMAL
- en: Again, there's no particular way out of following the steps as discussed in
    the previous section. The XPath or CSS selector can also be determined or formed
    revealing the HTML source or page source; there are also lots of browser-based
    extensions that support similar tasks. It's the developer's choice to be comfortable
    with any way out that we have discussed to deal with the XPath and CSS selectors.
  prefs: []
  type: TYPE_NORMAL
- en: One of the recently listed browser-based extensions to generate XPath and CSS
    selectors for Google Chrome is ChroPath ([https://autonomiq.io/chropath/](https://autonomiq.io/chropath/)[)](https://autonomiq.io/chropath/).
    Writing customized expressions and queries is advised for self-practice and knowledge.
    Extensions and other similar applications should be used while processing a large
    information source.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we inspected and explored the Elements panel for element identification
    and DOM navigation: modifying, removing elements, altering scripts, and so on.
    Related options also exist in the Elements panel. In the following section, we
    will be using the Python library, `lxml`, to code `Scraper` and collect data from
    the chosen website using XPath and CSS selector.'
  prefs: []
  type: TYPE_NORMAL
- en: Scraping using lxml, a Python library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: lxml is a XML toolkit, with a rich library set to process XML and HTML. lxml
    is preferred over other XML-based libraries in Python for its high speed and effective
    memory management. It also contains various other features to handle both small
    or large XML files. Python programmers use lxml to process XML and HTML documents.
    For more detailed information on lxml and its library support, please visit [https://lxml.de/.](https://lxml.de/)
  prefs: []
  type: TYPE_NORMAL
- en: lxml provides native support to XPath and XSLT and is built on powerful C libraries: `libxml2`
    and `libxslt`. Its library set is used normally with XML or HTML to access XPath,
    parsing, validating, serializing, transforming, and extending features from ElementTree
    ([http://effbot.org/zone/element-index.htm#documentation](http://effbot.org/zone/element-index.htm#documentation)).
    Parsing, traversing ElementTree, XPath, and CSS selector-like features from lxml makes
    it handy enough for a task such as web scraping. lxml is also used as a parser
    engine in Python Beautiful Soup ([https://www.crummy.com/software/BeautifulSoup/bs4/doc/](https://www.crummy.com/software/BeautifulSoup/bs4/doc/))
    and pandas ([https://pandas.pydata.org/](https://pandas.pydata.org/)).
  prefs: []
  type: TYPE_NORMAL
- en: Elements of a markup language such as XML and HTML have start and close tags;
    tags can also have attributes and contain other elements. ElementTree is a wrapper
    that loads XML files as trees of elements. The Python built-in library, ElementTree
    (etree), is used to search, parse elements, and build a document tree. Element
    objects also exhibit various accessible properties related to Python lists and
    dictionaries.
  prefs: []
  type: TYPE_NORMAL
- en: XSLT is a language to transform an XML document into HTML, XHML, text, and so
    on. XSLT uses XPath to navigate in XML documents. XSLT is a template type of structure
    that is used to transform XML document into new documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'The lxml library contains important modules, as listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`lxml.etree` ([https://lxml.de/api/lxml.etree-module.html](https://lxml.de/api/lxml.etree-module.html)):
    Parsing and implementing ElementTree; supports XPath, iterations, and more'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lxml.html` ([https://lxml.de/api/lxml.html-module.html](https://lxml.de/api/lxml.html-module.html)):
    Parses HTML, supports XPath, CSSSelect, HTML form, and form submission'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lxml.cssselect` ([https://lxml.de/api/lxml.cssselect-module.html](https://lxml.de/api/lxml.cssselect-module.html)):
    Converts CSS selectors into XPath expressions; accepts a CSS selector or CSS Query
    as an expression'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: lxml by examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: lxml has a large module set, and, in this section, we will learn to explore
    lxml using most of its features with examples before moving into scraping tasks.
    The examples are geared toward extraction activity rather than development.
  prefs: []
  type: TYPE_NORMAL
- en: Example 1 – reading XML from file and traversing through its elements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we will be reading the XML content available from the `food.xml` file.
    We will use XML content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The XML response obtained from the preceding code needs to be parsed and traversed
    using `lxml.etree.XML()`. The `XML()` function parses the XML document and returns
    the `menus` root node, in this case. Please refer to [https://lxml.de/api/lxml.etree-module.html](https://lxml.de/api/lxml.etree-module.html)
    for more detailed information on `lxml.etree`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The functions `fromstring()` and `parse()` functions, found in the preceding
    code, also provide content to a default or chosen parser used by `lxml.etree`.
  prefs: []
  type: TYPE_NORMAL
- en: A number of parsers are provided by lxml (XMLParser and HTMLParser) and the
    default one used in code can be found using `>>> etree.get_default_parser()`.
    In the preceding case, it results in `<lxml.etree.XMLParser>`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s verify `tree` received after parsing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The preceding two statements confirm that `tree` is an XML root element of the
    `lxml.etree._Element` type. For traversing through all elements inside a tree,
    tree iteration can be used, which results in elements in their found order.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tree iteration is performed using the `iter()` function*.* The elements'' tag
    name can be accessed using the element property, `tag`; similarly, elements''
    text can be accessed by the `text` property, as shown in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding tree iteration will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We, too, can pass child elements as an argument to the tree iterator (`price`
    and `name`) to obtain selected element-based responses. After passing the child
    element to `tree.iter()`*,* we can obtain `Tag` and `Text` or `Content` child
    elements using `element.tag` and `element.text`, respectively, as shown in the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Also to be noted is that the `food.xml` file has been opened in `rb` mode and
    not in `r` mode. While dealing with local file-based content and files having
    encoding declarations, such as `<?xml version="1.0" encoding="UTF-8"?>`*,* there''s
    a possibility of encountering the error as `ValueError: Unicode strings with encoding
    declaration are not supported. Please use bytes input or XML fragments without
    declaration`***.*** Encoding/decoding the content might solve the issue mentioned,
    which is also based on the file mode.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To deal with the preceding condition or reading the content from file, HTTP
    URL, or FTP, `parse()` is a really effective approach. It uses the default parser
    unless specified; one is supplied to it as an extra argument. The following code
    demonstrates the use of the `parse()` function, which is being iterated for the
    element name to obtain its text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code results in the following output: `Butter Milk with Vanilla`,
    `Fish and Chips`, and so on, which are obtained from the `name` element and from
    the `food.xml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'A multiple-tree element can also be iterated, as seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Example 2 – reading HTML documents using lxml.html
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we will be using the `lxml.html` module to traverse through
    the elements from [http://httpbin.org/forms/post](http://httpbin.org/forms/post):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We are using `parse()` from `lxml.html` to load the given URL content. `parse()`
    acts similarly to `lxml.etree` but, in this case, `root` obtained is of the HTML
    type. The `getroot()` method returns the document root. The object type can be
    compared for `root` and `tree`, as shown in the preceding code. We are interested
    in `root` or HTMLElement for this example. The content parsed as `root` is shown
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/59036331-0920-4468-84a4-9a59392938a4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Page source: http://httpbin.org/forms/post'
  prefs: []
  type: TYPE_NORMAL
- en: 'HTMLElement `root` has various properties, as listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s find `<p>` from `root`; `find()` can be used to locate the first element
    by the path. Text can be retrieved using the `text_content()` function. The `findtext()`
    function can also be used for similar cases, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see in the following code*,* `findall()` is used to find and iterate
    through all of the elements in `root`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code lists the text from finding all `p` tags, as seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The HTMLElement `root` also supports XPath and CSSSelect:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the output seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'CSSSelect translates CSS selectors into XPath expressions and is used with
    a related object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code demonstrates the HTML `<form>` element being explored for
    its attributes and properties. We are targeting the `<form>` element first, which
    is found in `root`, that is, `<form method="post" action="/post">`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see from the preceding code, outputs are displayed as in-line comments:'
  prefs: []
  type: TYPE_NORMAL
- en: '`action` returns the URL value for the `key` attribute, `action`. The URL obtained
    is actually a link that will process the information submitted or options chosen.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`items()` returns the list of tuples containing the element''s key and value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keys()` returns the list of element keys.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`method` returns the value for the attribute method, that is, HTTP request
    or HTTP methods. For more information on HTTP methods, please refer to [Chapter
    1](af7787bb-7fcf-4101-8680-9bad14bf22e1.xhtml), *Web Scraping Fundamentals*, the *Understanding
    web development and technologies* section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example 3 – reading and parsing HTML for retrieving HTML form type element attributes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we will be reading HTML from the [http://httpbin.org/forms/post](http://httpbin.org/forms/post) URL,
    which contains HTML-based form elements. Form elements have various predefined
    attributes such as type, value, and name and can exist with manual attributes.
    In the preceding examples, we tried to implement various functions—XPath and CSSSelect—to
    retrieve content from the desired element.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we will try to collect the attributes and their values found in HTML-form
    elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, the `response.text` and a `str` type object is obtained
    for the given URL. The `fromstring()` function parses the provided string object
    and returns the root node or the HTMLElement `tree` type.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we are iterating the `input` element or `<input...>` and are
    looking to identify the attributes each input possesses.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding code results in the output shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'There are a number of functions and properties used with the `<input>` element
    in the code resulting from the output. Listed in the following in some of the
    code used in the example with an explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: '`element.tag`: This r'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: eturns the element `tag` name (for example, `input`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`element.values()`: The attributes of HTML form element exist as a `key:value`
    pair. The `value` attribute holds the exact data for the particular element. `values()`
    returns the `value` attribute for the chosen element in the `List` object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`element.attrib`: `attrib` returns a `Dict` type object (dictionary) with a `key:value`
    pair.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`element.items()`: `items()` returns a `List` object with a tuple possessing
    a key and value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`element.keys()`: Similar to'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`items()`, `keys()` returns the attributes `key` in the `List` object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With a general overview on lxml and its features explained through the preceding
    examples, we will now perform a few web scraping tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Web scraping using lxml
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will utilize most of the techniques and concepts learned
    throughout the chapters so far and implement some scraping tasks.
  prefs: []
  type: TYPE_NORMAL
- en: For the task ahead, we will first select the URLs required. In this case, it
    will be [http://books.toscrape.com/](http://books.toscrape.com/), but by targeting
    a music category, which is [http://books.toscrape.com/catalogue/category/books/music_14/index.html](http://books.toscrape.com/catalogue/category/books/music_14/index.html).
    With the chosen target URL, its time now to explore the web page and identify
    the content that we are willing to extract.
  prefs: []
  type: TYPE_NORMAL
- en: We are willing to collect certain information such as the title, price, availability,
    `imageUrl`, and rating found for each individual item (that is, the `Article`
    element) listed in the page. We will attempt different techniques using lxml and
    XPath to scrape data from single and multiple pages, plus the use of CSS selectors.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding element identification, XPath, CSS selectors and using DevTools, please
    refer to the *Using web browser developer tools for accessing web content* section.
  prefs: []
  type: TYPE_NORMAL
- en: Example 1 – extracting selected data from a single page using lxml.html.xpath
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we will use XPath to collect information from the provided
    URL and use lxml features.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, a `musicUrl` string object contains a link to the page.
    `musicUrl` is parsed using the `parse()` function, which results in the `doc`
    and `lxml.etree.ElementTree` objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have an ElementTree `doc` available; we will be collecting the XPath
    expressions for the chosen fields such as title and price, found on the `musicUrl`
    page. For generating XPath expressions, please refer to the *XPath and CSS selectors
    using DevTools* section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: The XPath for the preceding `articles` posseses all of the fields that are available
    inside  `<article>`, such as `title`, `price`, `availability`, `imageUrl`, and
    `starRating`. The `articles` field is an expression of a type of parent element
    with child elements. Also, individual XPath expressions for child elements are
    also declared, such as the `title` field, that is, `title = articles.xpath("//h3/a/text()")`*.*
    We can notice the use of `articles` in the expression.
  prefs: []
  type: TYPE_NORMAL
- en: It is also to be noticed in child expressions that element attributes or key
    names such as `class` or `src` can also be used as `@class` and `@src`, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the individual expressions have been set up, we can print the items
    that collect all of the found information for available expressions and return
    those in the Python list. The cleaning and formatting for data received has also
    been done with the `map()`, `replace()`, and `strip()` Python functions and Lambda
    operator, as seen in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Collected or extracted data might require the additional task of cleaning, that
    is, removing unwanted characters, white spaces, and so on. It might also require
    formatting or transforming data into the desired format such as converting string
    date and time into numerical values, and so on. These two actions help to maintain
    some predefined or same-structured data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final output for the preceding code is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/12cb33b8-3a35-4e8d-aa7e-64ccc854f962.png)'
  prefs: []
  type: TYPE_IMG
- en: Python lists with various data from the selected page
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see from the preceding screenshot, there is an individual collection
    of targeted data. Data collected in such a way can be merged into a single Python
    object as shown in the following code or can be written into external files such
    as CSV or JSON for further processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '`dataSet` in the preceding code is generated using the `zip()` Python function.
    `zip()` collects individual indexes from all provided list objects and appends
    them as a tuple. The final output from `dataSet` has particular values for each
    `<article>`, as shown in the previous code.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 2 – looping with XPath and scraping data from multiple pages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In example 1, we tried the simple XPath-based technique for a URL with a limited
    number of results on a single page. In this case, we will be targeting a *food
    and drink* category, that is, [http://books.toscrape.com/catalogue/category/books/food-and-drink_33/index.html](http://books.toscrape.com/catalogue/category/books/food-and-drink_33/index.html),
    which has its content across pages. An XPath-based looping operation will be used
    in this example, which supports a more effective collection of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we will be dealing with multiple pages, it''s good practice to check for
    a few individual page URLs that can be found in the browser while moving through
    the listed pages. Most of the time, it might contain some patterns that can solve
    the puzzle easily, as used in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '`bookUrl` is the main URL we are interested in; it also contains the page link
    for the next page, which contains a pattern, as found in `pageUrl`, for example,
    `page-2.html`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: An empty `dataSet` list is defined to hold data found from each article across
    pages.
  prefs: []
  type: TYPE_NORMAL
- en: 'An individual page URL is obtained by concatenating `pageUrl` with a page number,
    and `.html`. `totalPages` is found after calculating `totalArticles` and `perPageArticles`
    as traced from the page itself. `totalPages` obtained will give an exact loop
    count and is more manageable to apply in the loop (the `while` loop is found in
    the code):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: As we can see in the previous code, `articles` is the major XPath expression
    used to loop for finding individual elements inside the `<article>` field. The
    expression should contain a certain condition that can be fulfilled to preform
    a loop; in this case, we identified that the `<article>` field exists inside of
    the `<ol><li>` element.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we can perform a loop with `li[position()>0]` that identifies each `<article>`
    field found inside `<li>` until it exists in `<ol>` with its traced position,
    that is, `articles = XPath("//*[@id=''default'']//ol/li[position()>0]")`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Individual elements of the XPath expression are defined as the `titlePath`
    element, the `imagePath` element, and so on, targeting particular elements whose
    data is to be obtained. Finally, the expression set for articles is looped into
    the HTMLElement obtained for each page, that is, the `doc` element and collects
    the first occurrence of each `title` and `image` element and the other elements
    found. These collected data are appended to the `dataSet` field as a list with
    the cleaning and formatting done, which results in the output shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/059c3514-4cf0-4d47-ae5e-c51f3cba78f3.png)'
  prefs: []
  type: TYPE_IMG
- en: Output with paging information and dataSet contents
  prefs: []
  type: TYPE_NORMAL
- en: Example 3 – using lxml.cssselect to scrape content from a page
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CSS selectors have a broad range of query options as described in the *Introduction
    to XPath and CSS selector* section, and is often used as an easy alternative to
    XPath. In the two preceding examples, we explored the XPath to collect the desired
    information. In this example, we will be using `cssselect` from lxml to collect
    relevant data from a single page available on [https://developer.ibm.com/announcements/category/data-science/?fa=date%3ADESC&fb=](https://developer.ibm.com/announcements/category/data-science/?fa=date%3ADESC&fb=).
  prefs: []
  type: TYPE_NORMAL
- en: 'To identify a CSS query, we can browse through the page source or use the DevTools.
    For more detail on using DevTools, refer to the *XPath and CSS selectors using
    DevTools* section. In this case, we are identifying and collecting CSS Query using
    DevTools, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/f06876ef-04f3-4456-aa83-13e9d499b882.png)'
  prefs: []
  type: TYPE_IMG
- en: Using DevTools and selecting selector from https://developer.ibm.com/announcements
  prefs: []
  type: TYPE_NORMAL
- en: 'From the preceding screenshot, we can see that the individual announcements
    are a block identified by `a.ibm--card__block_link` found inside `div.ibm--card`,
    which possesses HTML elements with classes, such as `ibm--card__body`, and `ibm--card__type`.
    The CSS selector is copied using the described process and will result in the
    following list for `a.ibm--card__block_link` and `div.ibm--card__body`, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '`#content > div > div.code_main > div > div.cpt-content > div > div.bx--grid.no-pad.cpt--item__row
    > div:nth-child(1) > div:nth-child(1) > div > a`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`#content > div > div.code_main > div > div.cpt-content > div > div.bx--grid.no-pad.cpt--item__row
    > div:nth-child(1) > div:nth-child(1) > div > a > div.ibm--card__body`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s deploy the preceding concept using Python code, as shown in the following
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'The required Python library and URLs are declared and the page content `url_get`
    is parsed with `lxml.html`. With `lxml.html.HTMLElement` obtained, we can now
    select and navigate to the desired element in the tree with the XPath or CSS selector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '`articles` is a defined main CSS query and is looped for all available `articles`
    found in the page as `article`. Each article has different elements for type,
    date, title, category, and so on. Element data or attributes are collected using
    `text`, `text_content()`, and `get()`. `cssselect` returns the Python list objects,
    hence, indexing, such as `[0]`, is used to collect particular element content.'
  prefs: []
  type: TYPE_NORMAL
- en: '`category` in the preceding code doesn''t have any indexing, as it contains
    a multiple `<span>` element whose value is being extracted using a list comprehension
    technique, while appending or using indexing as shown in the comments. Output
    obtained for the code is shown in the following screenshot. Minor cleaning of
    data has been attempted, but the final list still contains the obtained raw data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/3cfdddb1-72ed-4663-8f8e-bce9310f1c49.png)'
  prefs: []
  type: TYPE_IMG
- en: Output from list announcements obtained using lxml.cssselectIt's also to be
    noted that CSS selector queries copied or obtained using DevTools and used in
    the example code seem to be different in expression and length. DevTools-provided
    queries contain details and linked expressions from the parent element found for
    all chosen elements. In code, we have used the CSS query for only the particular
    elements identified.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Element identification, DOM-based navigation, using browser-based developer
    tools, deploying data-extraction techniques, and an overview on XPath and CSS
    selectors, plus the use of lxml in a Python library, were the main topics explored
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We have also explored various examples using lxml, implementing different techniques
    plus library features to deal with the element and ElementTree. Finally, web Scraping
    techniques were explored through examples focusing on different situations that
    might arise in real cases.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn more about web scraping techniques and some
    new Python libraries deploying these techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The DOM: [https://dom.spec.whatwg.org/](https://dom.spec.whatwg.org/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XPath: [https://www.w3.org/TR/xpath/](https://www.w3.org/TR/xpath/), [https://www.w3.org/TR/2017/REC-xpath-31-20170321/](https://www.w3.org/TR/2017/REC-xpath-31-20170321/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XML DOM: [https://www.w3schools.com/xml/dom_intro.asp](https://www.w3schools.com/xml/dom_intro.asp)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XPath introduction: [https://www.w3schools.com/xml/xpath_intro.asp](https://www.w3schools.com/xml/xpath_intro.asp)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XPath tester: [https://freeformatter.com/xpath-tester.html](https://freeformatter.com/xpath-tester.html),
    [http://www.xpathtester.com/xslt](http://www.xpathtester.com/xslt), [https://codebeautify.org/Xpath-Tester](https://codebeautify.org/Xpath-Tester)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XPath tutorial: [https://doc.scrapy.org/en/xpath-tutorial/topics/xpath-tutorial.html](https://doc.scrapy.org/en/xpath-tutorial/topics/xpath-tutorial.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CSS Selector reference: [https://www.w3schools.com/cssref/css_selectors.asp](https://www.w3schools.com/cssref/css_selectors.asp)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CSS pseudo class and elements: [https://www.w3schools.com/css/css_pseudo_elements.asp](https://www.w3schools.com/css/css_pseudo_elements.asp)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CSS information: [http://www.css3.info/](http://www.css3.info/), [https://developer.mozilla.org/en-US/docs/Web/CSS](https://developer.mozilla.org/en-US/docs/Web/CSS)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CSS query parser: [https://try.jsoup.org/](https://try.jsoup.org/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CSS Selector to XPath: [https://css-selector-to-xpath.appspot.com](https://css-selector-to-xpath.appspot.com)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ElementTree overview: [http://effbot.org/zone/element-index.htm](http://effbot.org/zone/element-index.htm)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
