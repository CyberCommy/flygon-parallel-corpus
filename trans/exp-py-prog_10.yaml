- en: Chapter 10. Test-Driven Development
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Test-Driven Development** (**TDD**) is a simple technique to produce high
    quality software. It is widely used in the Python community, but it is also very
    popular in other communities.'
  prefs: []
  type: TYPE_NORMAL
- en: Testing is especially important in Python due to its dynamic nature. It lacks
    static typing so many, even minute, errors won't be noticed until the code is
    run and each of its line is executed. But the problem is not only how types in
    Python work. Remember that most bugs are not related to bad syntax usage, but
    rather to logical errors and subtle misunderstandings that can lead to major failures.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter is split into two parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '*I don''t test*, which advocates TDD and quickly describes how to do it with
    the standard library'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*I do test*, which is intended for developers who practice tests and wish to
    get more out of them'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I don't test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have already been convinced to TDD, you should move to the next section.
    It will focus on advanced techniques and tools for making your life easier when
    working with tests. This part is mainly intended for those who are not using this
    approach and tries to advocate its usage.
  prefs: []
  type: TYPE_NORMAL
- en: Test-driven development principles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The test-driven development process, in its simplest form, consists of three
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Writing automated tests for a new functionality or improvement that has not
    been implemented yet.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Providing minimal code that just passes all the defined tests.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Refactoring code to meet the desired quality standards.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The most important fact to remember about this development cycle is that tests
    should be written before implementation. It is not an easy task for unexperienced
    developers, but it is the only approach which guarantees that the code you are
    going to write will be testable.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, a developer who is asked to write a function that checks whether
    the given number is a prime number, writes a few examples on how to use it and
    what the expected results are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The developer that implements the feature does not need to be the only one responsible
    for providing tests. The examples can be provided by another person as well. For
    instance, very often the official specifications of network protocols or cryptography
    algorithms provide test vectors that are intended to verify correctness of implementation.
    These are a perfect basis for test cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'From there, the function can be implemented until the preceding examples work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'A bug or an unexpected result is a new example of usage the function should
    be able to deal with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The code can be changed accordingly, until the new test passes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'And more cases show that the implementation is still incomplete:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The updated code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'From there, all tests can be gathered in a test function, which is run every
    time the code evolves:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Every time we come up with a new requirement, the `test_is_prime()` function
    should be updated first to define the expected behavior of the `is_prime()` function.
    Then, the test is run to check if the implementation delivers the desired results.
    Only if the tests are known to be failing, there is a need to update code for
    the tested function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Test-driven development provides a lot of benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: It helps to prevent software regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It improves software quality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It provides a kind of low-level documentation of code behavior
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It allows you to produce robust code faster in short development cycles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The best convention to deal in with test is to gather all of them in a single
    module or package (usually named `tests`) and have an easy way to run the whole
    suite using a single shell command. Fortunately, there is no need to build whole
    test tool chains all by yourself. Both Python standard library and Python Package
    Index come with plenty of test frameworks and utilities that allow you to build,
    discover, and run tests in a convenient way. We will discuss the most notable
    examples of such packages and modules later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Preventing software regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We all face software regression issues in our developer lives. Software regression
    is a new bug introduced by a change. It manifests when features or functionalities
    that were known to be working in the previous versions of the software get broken
    and stop working at some point during project development.
  prefs: []
  type: TYPE_NORMAL
- en: The main reason for regressions is high complexity of software. At some point,
    it is impossible to guess what a single change in the codebase might lead to.
    Changing some code might break some other features and sometimes lead to vicious
    side effects, such as silently corrupting data. And high complexity is not only
    the problem of huge codebases. There is, of course, obvious correlation between
    the amount of code and its complexity, but even small projects (few hundredths/thousands
    lines of code) may have such convoluted architecture that it is hard to predict
    all consequences of relatively small changes.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid regression, the whole set of features the software provides should
    be tested every time a change occurs. Without this, you are not able to reliably
    tell difference between bugs that have always existed in your software from the
    new ones introduced to parts that were working correctly just some time ago.
  prefs: []
  type: TYPE_NORMAL
- en: Opening up a codebase to several developers amplifies the problem, since each
    person will not be fully aware of all the development activities. While having
    a version control system prevents conflicts, it does not prevent all unwanted
    interactions.
  prefs: []
  type: TYPE_NORMAL
- en: TDD helps reduce software regression. The whole software can be automatically
    tested after each change. This will work as long as each feature has the proper
    set of tests. When TDD is properly done, the testbase grows together with the
    codebase.
  prefs: []
  type: TYPE_NORMAL
- en: Since a full test campaign can last for quite a long time, it is a good practice
    to delegate it to some continuous integration system which can do the work in
    the background. We discussed such solutions already in [Chapter 8](ch08.html "Chapter 8. Managing
    Code"), *Managing Code*. Nevertheless, the local re-launching of the tests should
    be performed manually by the developer too, at least for the concerned modules.
    Relying only on continuous integration will have a negative effect on the developers'
    productivity. Programmers should be able to run selections of tests easily in
    their environments. This is the reason why you should carefully choose testing
    tools for the project.
  prefs: []
  type: TYPE_NORMAL
- en: Improving code quality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When a new module, class, or a function is written, a developer focuses on
    how to write it and how to produce the best piece of code he or she can. But while
    he or she is concentrating on algorithms, he or she might lose the user''s point
    of view: How and when will his or her function be used? Are the arguments easy
    and logical to use? Is the name of the API right?'
  prefs: []
  type: TYPE_NORMAL
- en: This is done by applying the tips described in the previous chapters, such as
    [Chapter 4](ch04.html "Chapter 4. Choosing Good Names"), *Choosing Good Names*.
    But the only way to do it efficiently is to write usage examples. This is the
    moment when the developer realizes if the code he or she wrote is logical and
    easy to use. Often, the first refactoring occurs right after the module, class,
    or function is finished.
  prefs: []
  type: TYPE_NORMAL
- en: Writing tests, which are use cases for the code, helps in having a user point
    of view. Developers will, therefore, often produce a better code when they use
    TDD. It is difficult to test gigantic functions and huge monolithic classes. Code
    that is written with testing in mind tends to be architected more cleanly and
    modularly.
  prefs: []
  type: TYPE_NORMAL
- en: Providing the best developer documentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tests are the best place for a developer to learn how software works. They are
    the use cases the code was primarily created for. Reading them provides a quick
    and deep insight into how the code works. Sometimes an example is worth a thousand
    words.
  prefs: []
  type: TYPE_NORMAL
- en: The fact that these tests are always up to date with the codebase makes them
    the best developer documentation that a piece of software can have. Tests don't
    go stale in the same way documentation does, otherwise they would fail.
  prefs: []
  type: TYPE_NORMAL
- en: Producing robust code faster
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Writing without testing leads to long debugging sessions. A consequence of a
    bug in one module might manifest itself in a completely different part of the
    software. Since you don't know who to blame, you spend an inordinate amount of
    time debugging. It's better to fight small bugs one at a time when a test fails,
    because you'll have a better clue as to where the real problem is. And testing
    is often more fun than debugging because it is coding.
  prefs: []
  type: TYPE_NORMAL
- en: If you measure the time taken to fix the code together with the time taken to
    write it, it will usually be longer than the time a TDD approach would take. This
    is not obvious when you start a new piece of code. This is because the time taken
    to set up a test environment and write the first few tests is extremely long compared
    to the time taken just to write the first pieces of code.
  prefs: []
  type: TYPE_NORMAL
- en: But there are some test environments that are really hard to set up. For instance,
    when your code interacts with an LDAP or an SQL server, writing tests is not obvious
    at all. This is covered in the *Fakes and mocks* section in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: What kind of tests?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are several kinds of tests that can be made on any software. The main
    ones are **acceptance tests** (or **functional tests**) and **unit tests**, and
    these are the ones that most people think of when discussing the topic of software
    testing. But there are a few other kinds of tests that you can use in your project.
    We will discuss some of them shortly in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Acceptance tests
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An acceptance test focuses on a feature and deals with the software like a black
    box. It just makes sure that the software really does what it is supposed to do,
    using the same media as that of the users and controlling the output. These tests
    are usually written out of the development cycle to validate that the application
    meets the requirements. They are usually run as a checklist over the software.
    Often, these tests are not done through TDD and are built by managers, QA staff,
    or even customers. In that case, they are often called **user acceptance tests**.
  prefs: []
  type: TYPE_NORMAL
- en: Still, they can and they should be done with TDD principles. Tests can be provided
    before the features are written. Developers get a pile of acceptance tests, usually
    made out of the functional specifications, and their job is to make sure the code
    will pass all of them.
  prefs: []
  type: TYPE_NORMAL
- en: 'The tools used to write those tests depend on the user interface the software
    provides. Some popular tools used by Python developers are:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Application type | Tool |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Web application | Selenium (for Web UI with JavaScript) |'
  prefs: []
  type: TYPE_TB
- en: '| Web application | `zope.testbrowser` (doesn''t test JS) |'
  prefs: []
  type: TYPE_TB
- en: '| WSGI application | `paste.test.fixture` (doesn''t test JS) |'
  prefs: []
  type: TYPE_TB
- en: '| Gnome Desktop application | dogtail |'
  prefs: []
  type: TYPE_TB
- en: '| Win32 Desktop application | pywinauto |'
  prefs: []
  type: TYPE_TB
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For an extensive list of functional testing tools, Grig Gheorghiu maintains
    a wiki page at [https://wiki.python.org/moin/PythonTestingToolsTaxonomy](https://wiki.python.org/moin/PythonTestingToolsTaxonomy).
  prefs: []
  type: TYPE_NORMAL
- en: Unit tests
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unit tests are low-level tests that perfectly fit test-driven development. As
    the name suggests, they focus on testing software units. A software unit can be
    understood as the smallest testable piece of the application code. Depending on
    the application, the size may vary from whole modules to a single method or function,
    but usually unit tests are written for the smallest fragments of code possible.
    Unit tests usually isolate the tested unit (module, class, function, and so on)
    from the rest of the application and other units. When external dependencies are
    required, such as web APIs or databases, they are often replaced by fake objects
    or mocks.
  prefs: []
  type: TYPE_NORMAL
- en: Functional tests
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Functional tests focus on whole features and functionalities instead of small
    code units. They are similar in their purpose to acceptance tests. The main difference
    is that functional tests do not necessarily need to use the same interface that
    a user does. For instance, when testing web applications, some of the user interactions
    (or its consequences) can be simulated by synthetic HTTP requests or direct database
    access, instead of simulating real page loading and mouse clicks.
  prefs: []
  type: TYPE_NORMAL
- en: This approach is often easier and faster than testing with tools used in *user
    acceptance tests*. The downside of limited functional tests is that they tend
    not to cover enough parts of the application where different abstraction layers
    and components meet. Tests that focus on such *meeting points* are often called
    integration tests.
  prefs: []
  type: TYPE_NORMAL
- en: Integration tests
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Integration tests represent a higher level of testing than unit tests. They
    test bigger parts of code and focus on situations where many application layers
    or components meet and interact with each other. The form and scope of integration
    tests varies depending on the project's architecture and complexity. For example,
    in small and monolithic projects, this may be as simple as running more complex
    functional tests and allowing them to interact with real backing services (databases,
    caches, and so on) instead of mocking or faking them. For complex scenarios or
    products that are built from multiple services, the real integration tests may
    be very extensive and even require running the whole project in a big distributed
    environment that mirrors the production.
  prefs: []
  type: TYPE_NORMAL
- en: Integration tests are often very similar to functional tests and the border
    between them is very blurry. It is very common that integration tests are also
    logically testing separate functionalities and features.
  prefs: []
  type: TYPE_NORMAL
- en: Load and performance testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Load tests and performance tests provide objective information about code efficiency
    rather than its correctness. The terms of load testing and performance testing
    are used by some interchangeably but the first one in fact refers to a limited
    aspect of performance. Load testing focuses on measuring how code behaves under
    some artificial demand (load). This is a very popular way of testing web applications
    where load is understood as web traffic from real users or programmatic clients.
    It is important to note that load tests tend to cover whole requests to the application
    so are very similar to integration and functional tests. This makes it important
    to be sure that tested application components are fully verified to be working
    correctly. Performance tests are generally all the tests that aim to measure code
    performance and can target even small units of code. So, load tests are only a
    specific subtype of performance tests.
  prefs: []
  type: TYPE_NORMAL
- en: They are special kind of tests because they do not provide binary results (failure/success)
    but only some performance quality measurement. This means that single results
    need to be interpreted and/or compared with results of different test runs. In
    some cases, the project requirements may set some hard time or resource constraints
    on the code but this does not change the fact that there is always some arbitrary
    interpretation involved in these kinds of testing approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Load performance tests are a great tool during the development of any software
    that needs to fulfill some **Service** **Level Agreements** because it helps to
    reduce the risk of compromising the performance of critical code paths. Anyway,
    it should not be overused.
  prefs: []
  type: TYPE_NORMAL
- en: Code quality testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Code quality does not have the arbitrary scale that would say for definite
    if it is bad or good. Unfortunately, the abstract concept of code quality cannot
    be measured and expressed in the form of numbers. But instead, we can measure
    various metrics of the software that are known to be highly correlated with the
    quality of code. To name a few:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of code style violations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The amount of documentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Complexity metrics, such as McCabe's cyclomatic complexity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of static code analysis warnings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many projects use code quality testing in their continuous integration workflows.
    The good and popular approach is to test at least basic metrics (static code analysis
    and code style violations) and not allow the merging of any code to the mainstream
    that makes these metrics lower.
  prefs: []
  type: TYPE_NORMAL
- en: Python standard test tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Python provides two main modules in the standard library to write tests:'
  prefs: []
  type: TYPE_NORMAL
- en: '`unittest` ([https://docs.python.org/3/library/unittest.html](https://docs.python.org/3/library/unittest.html)):
    This is the standard and most common Python unit testing framework based on Java''s
    JUnit and was originally written by Steve Purcell (formerly `PyUnit`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`doctest` ([https://docs.python.org/3/library/doctest.html](https://docs.python.org/3/library/doctest.html)):
    This is a literate programing testing tool with interactive usage examples'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: unittest
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`unittest` basically provides what JUnit does for Java. It offers a base class
    called `TestCase`, which has an extensive set of methods to verify the output
    of function calls and statements.'
  prefs: []
  type: TYPE_NORMAL
- en: This module was created to write unit tests, but acceptance tests can also be
    written with it as long as the test uses the user interface. For instance, some
    testing frameworks provide helpers to drive tools such as Selenium on top of `unittest`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Writing a simple unit test for a module using `unittest` is done by subclassing
    `TestCase` and writing methods with the `test` prefix. The final example from
    the *Test-driven development principles* section will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The `unittest.main()` function is the utility that allows to make the whole
    module to be executable as a test suite:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `unittest.main()` function scans the context of the current module and looks
    for classes that subclass `TestCase`. It instantiates them, then runs all methods
    that start with the `test` prefix.
  prefs: []
  type: TYPE_NORMAL
- en: 'A good test suite follows the common and consistent naming conventions. For
    instance, if the `is_prime` function is included in the `primes.py` module, the
    test class could be called `PrimesTests` and put into the `test_primes.py` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: From there, every time the `utils` module evolves, the `test_utils` module gets
    more tests.
  prefs: []
  type: TYPE_NORMAL
- en: In order to work, the `test_primes` module needs to have the `primes` module
    available in the context. This can be achieved either by having both modules in
    the same package by adding a tested module explicitly to the Python path. In practice,
    the `develop` command of `setuptools` is very helpful here.
  prefs: []
  type: TYPE_NORMAL
- en: Running tests over the whole application presupposes that you have a script
    that builds a **test campaign** out of all test modules. `unittest` provides a
    `TestSuite` class that can aggregate tests and run them as a test campaign, as
    long as they are all instances of `TestCase` or `TestSuite`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Python''s past, there was convention that test module provides a `test_suite`
    function that returns a `TestSuite` instance either used in the `__main__` section,
    when the module is called by Command Prompt, or used by a test runner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this module from the shell will print the test campaign output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding approach was required in the older versions of Python when the
    `unittest` module did not have proper test discovery utilities. Usually, running
    of all tests was done by a global script that browses the code tree looking for
    tests and runs them. This is called **test discovery** and will be covered more
    extensively later in this chapter. For now, you should only know that `unittest`
    provides a simple command that can discover all tests from modules and packages
    with a `test` prefix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: If you use the preceding command, then there is no requirement to manually define
    the `__main__` sections and invoke the `unittest.main()` function.
  prefs: []
  type: TYPE_NORMAL
- en: doctest
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`doctest` is a module that extracts snippets in the form of interactive prompt
    sessions from docstrings or text files and replays them to check whether the example
    output is the same as the real one.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, the text file with the following content could be run as a test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s assume this documentation file is stored in the filesystem under `test.rst`
    name. The `doctest` module provides some functions to extract and run the tests
    from such documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Using `doctest` has many advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Packages can be documented and tested through examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation examples are always up to date
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using examples in doctests to write a package helps to maintain the user's point
    of view
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, doctests do not make unit tests obsolete; they should be used only
    to provide human-readable examples in documents. In other words, when the tests
    are concerning low-level matters or need complex test fixtures that would obfuscate
    the document, they should not be used.
  prefs: []
  type: TYPE_NORMAL
- en: Some Python frameworks such as Zope use doctests extensively, and they are at
    times criticized by people who are new to the code. Some doctests are really hard
    to read and understand, since the examples break one of the rules of technical
    writing—they cannot be taken and run in a simple prompt, and they need extensive
    knowledge. So, documents that are supposed to help newcomers are really hard to
    read because the code examples, which are doctests built through TDD, are based
    on complex test fixtures or even specific test APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As explained in [Chapter 9](ch09.html "Chapter 9. Documenting Your Project"),
    *Documenting Your Project*, when you use doctests that are part of the documentation
    of your packages, be careful to follow the seven rules of technical writing.
  prefs: []
  type: TYPE_NORMAL
- en: At this stage, you should have a good overview of what TDD brings. If you are
    still not convinced, you should give it a try over a few modules. Write a package
    using TDD and measure the time spent on building, debugging, and then refactoring.
    You should find out quickly that it is truly superior.
  prefs: []
  type: TYPE_NORMAL
- en: I do test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are coming from the *I don't test* section and are now convinced to do
    test-driven development, then congratulations! You know the basics of test-driven
    development, but there are some more things you should learn before you will be
    able to efficiently use this methodology.
  prefs: []
  type: TYPE_NORMAL
- en: This section describes a few problems developers bump into when they write tests
    and some ways to solve them. It also provides a quick review of popular test runners
    and tools available in the Python community.
  prefs: []
  type: TYPE_NORMAL
- en: unittest pitfalls
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `unittest` module was introduced in Python 2.1 and has been massively used
    by developers since then. But some alternative test frameworks were created in
    the community by people who were frustrated with the weaknesses and limitations
    of `unittest`.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the common criticisms that are often made:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The **framework is heavy to use** because:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You have to write all your tests in subclasses of `TestCase`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You have to prefix the method names with `test`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You are encouraged to use assertion methods provided in `TestCase` instead of
    plain `assert` statements and existing methods may not cover every use case
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The framework is hard to extend because it requires massive subclassing of its
    base classes or tricks such as decorators.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test fixtures are sometimes hard to organize because the `setUp` and `tearDown`
    facilities are tied to the `TestCase` level, though they run once per test. In
    other words, if a test fixture concerns many test modules, it is not simple to
    organize its creation and cleanup.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is not easy to run a test campaign over Python software. The default test
    runner (`python -m unittest`) indeed provides some test discovery but does not
    provide enough filtering capabilities. In practice, extra scripts have to be written
    to collect the tests, aggregate them, and then run them in a convenient way.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A lighter approach is needed to write tests without suffering from the rigidity
    of a framework that looks too much like its big Java brother, JUnit. Since Python
    does not require working with a 100% class-based environment, it is preferable
    to provide a more Pythonic test framework that is not based on subclassing.
  prefs: []
  type: TYPE_NORMAL
- en: 'A common approach would be:'
  prefs: []
  type: TYPE_NORMAL
- en: To provide a simple way to mark any function or any class as a test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To extend the framework through a plug-in system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To provide a complete test fixture environment for all test levels: the whole
    campaign, a group of tests at module level, and at test level'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To provide a test runner based on test discovery with an extensive set of options
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: unittest alternatives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some third-party tools try to solve the problems just mentioned by providing
    extra features in the shape of `unittest` extensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Python wiki provides a very long list of various testing utilities and frameworks
    (refer to [https://wiki.python.org/moin/PythonTestingToolsTaxonomy](https://wiki.python.org/moin/PythonTestingToolsTaxonomy)),
    but there are just two projects that are especially popular:'
  prefs: []
  type: TYPE_NORMAL
- en: '`nose`: [http://nose.readthedocs.org](http://nose.readthedocs.org)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`py.test`: [http://pytest.org](http://pytest.org)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: nose
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`nose` is mainly a test runner with powerful discovery features. It has extensive
    options that allow running all kind of test campaigns in a Python application.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is not a part of standard library but is available on PyPI and can be easily
    installed with pip:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Test runner
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'After installing nose, a new command called `nosetests` is available at the
    prompt. Running the tests presented in the first section of the chapter can be
    done directly with it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '`nose` takes care of discovering the tests by recursively browsing the current
    directory and building a test suite on its own. The preceding example at first
    glance does not look like any improvement over the simple `python -m unittest`.
    The difference will be noticeable if you run this command with the `--help` switch.
    You will notice that nose provides tens of parameters that allow you to control
    test discovery and execution.'
  prefs: []
  type: TYPE_NORMAL
- en: Writing tests
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`nose` goes a step further by running all classes and functions whose name
    matches the regular expression `((?:^|[b_.-])[Tt]est)` located in modules that
    match it too. Roughly, all callables that start with `test` and are located in
    a module that match the pattern will also be executed as a test.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, this `test_ok.py` module will be recognized and run by `nose`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Regular `TestCase` classes and `doctests` are executed as well.
  prefs: []
  type: TYPE_NORMAL
- en: Last, `nose` provides assertion functions that are similar to `TestCase` methods.
    But these are provided as functions that follow the PEP 8 naming conventions,
    rather than using the Java convention `unittest` uses (refer to [http://nose.readthedocs.org/](http://nose.readthedocs.org/)).
  prefs: []
  type: TYPE_NORMAL
- en: Writing test fixtures
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`nose` supports three levels of fixtures:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Package level**: The `setup` and `teardown` functions can be added in the
    `__init__.py` module of a test''s package containing all test modules'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Module level**: A test module can have its own `setup` and `teardown` functions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test level**: The callable can also have fixture functions using the `with_setup`
    decorator provided'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For instance, to set a test fixture at the module and test level, use this
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Integration with setuptools and a plug-in system
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Last, `nose` integrates smoothly with `setuptools` and so the `test` command
    can be used with it (`python setup.py test`). This integration is done by adding
    the `test_suite` metadata in the `setup.py` script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '`nose` also uses `setuptool''s` entry point machinery for developers to write
    `nose` plugins. This allows you to override or modify every aspect of the tool
    from test discovery to output formatting.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A list of `nose` plugins is maintained at [https://nose-plugins.jottit.com](https://nose-plugins.jottit.com).
  prefs: []
  type: TYPE_NORMAL
- en: Wrap-up
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`nose` is a complete testing tool that fixes many of the issues `unittest`
    has. It is still designed to use implicit prefix names for tests, which remains
    a constraint for some developers. While this prefix can be customized, it still
    requires one to follow a convention.'
  prefs: []
  type: TYPE_NORMAL
- en: This convention over configuration statement is not bad and is a lot better
    than the boiler-plate code required in `unittest`. But using explicit decorators,
    for example, could be a nice way to get rid of the `test` prefix.
  prefs: []
  type: TYPE_NORMAL
- en: Also, the ability to extend `nose` with plugins makes it very flexible and allows
    a developer to customize the tool to meet his/her needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'If your testing workflow requires overriding a lot of nose parameters, you
    can easily add a `.noserc` or a `nose.cfg` file in your home directory or project
    root. It will specify the default set of options for the `nosetests` command.
    For instance, a good practice is to automatically look for doctests during test
    run. An example of the `nose` configuration file that enables running doctests
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: py.test
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`py.test` is very similar to `nose`. In fact, the latter was inspired by `py.test`,
    so we will focus mainly on details that make these tools different from each other.
    The tool was born as part of a larger package called `py` but now these are developed
    separately.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Like every third-party package mentioned in this book, `py.test` is available
    on PyPI and can be installed with `pip` as `pytest`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'From there, a new `py.test` command is available at the prompt that can be
    used exactly like `nosetests`. The tool uses similar pattern-matching and a test
    discovery algorithm to catch tests to be run. The pattern is stricter than that
    which `nose` uses and will only catch:'
  prefs: []
  type: TYPE_NORMAL
- en: Classes that start with `Test`, in a file that starts with `test`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Functions that start with `test`, in a file that starts with `test`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Be careful to use the right character case. If a function starts with a capital
    "T", it will be taken as a class, and thus ignored. And if a class starts with
    a lowercase "t", `py.test` will break because it will try to deal with it as a
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The advantages of `py.test` are:'
  prefs: []
  type: TYPE_NORMAL
- en: The ability to easily disable some test classes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A flexible and original mechanism for dealing with fixtures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to distribute tests among several computers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing test fixtures
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`py.test` supports two mechanisms to deal with fixtures. The first one, modeled
    after xUnit framework, is similar to `nose`. Of course semantics differ a bit.
    `py.test` will look for three levels of fixtures in each test module as shown
    in following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Each function will get the current module, class, or method as an argument.
    The test fixture will, therefore, be able to work on the context without having
    to look for it, as with `nose`.
  prefs: []
  type: TYPE_NORMAL
- en: The alternative mechanism for writing fixtures with `py.test` is to build on
    the concept of dependency injection, allowing to maintain the test state in a
    more modular and scalable way. The non xUnit-style fixtures (setup/teardown procedures)
    always have unique names and need to be explicitly activated by declaring their
    use in test functions, methods, and modules in classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest implementation of fixtures takes the form of a named function
    declared with the `pytest.fixture()` decorator. To mark a fixture as used in the
    test, it needs to be declared as a function or method argument. To make it more
    clear, consider the previous example of the test module for the `is_prime` function
    rewritten with the use of `py.test` fixtures:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Disabling test functions and classes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`py.test` provides a simple mechanism to disable some tests upon certain conditions.
    This is called skipping, and the `pytest` package provides the `.skipif` decorator
    for that purpose. If a single test function or a whole test class decorator needs
    to be skipped upon certain conditions, it needs to be defined with this decorator
    and with some value provided that verifies if the expected condition was met.
    Here is an example from the official documentation that skips running the whole
    test case class on Windows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'You can, of course, predefine the skipping conditions in order to share them
    across your testing modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'If a test is marked in such a way, it will not be executed at all. However,
    in some cases, you want to run such a test and want to execute it, but you know,
    it is expected to fail under known conditions. For this purpose, a different decorator
    is provided. It is `@mark.xfail` and ensures that the test is always run, but
    it should fail at some point if the predefined condition occurs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Using `xfail` is much stricter than `skipif`. Test is always executed and if
    it does not fail when it is expected to, then the whole `py.test` run will result
    in a failure.
  prefs: []
  type: TYPE_NORMAL
- en: Automated distributed tests
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: An interesting feature of `py.test` is its ability to distribute the tests across
    several computers. As long as the computers are reachable through SSH, `py.test`
    will be able to drive each computer by sending tests to be performed.
  prefs: []
  type: TYPE_NORMAL
- en: However, this feature relies on the network; if the connection is broken, the
    slave will not be able to continue working since it is fully driven by the master.
  prefs: []
  type: TYPE_NORMAL
- en: Buildbot, or other continuous integration tools, is preferable when a project
    has long test campaigns. But the `py.test` distributed model can be used for the
    ad hoc distribution of tests when you are working on an application that consumes
    a lot of resources to run the tests.
  prefs: []
  type: TYPE_NORMAL
- en: Wrap-up
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`py.test` is very similar to `nose` since no boilerplate code is needed to
    aggregate the tests in it. It also has a good plugin system and there are a great
    number of extensions available on PyPI.'
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, `py.test` focuses on making the tests run fast and is truly superior
    compared to the other tools in this area. The other notable feature is the original
    approach to fixtures that really helps in managing a reusable library of fixtures.
    Some people may argue that there is too much magic involved but it really streamlines
    the development of a test suite. This single advantage of `py.test` makes it my
    tool of choice, so I really recommend it.
  prefs: []
  type: TYPE_NORMAL
- en: Testing coverage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Code coverage** is a very useful metric that provides objective information
    on how well project code is tested. It is simply a measurement of how many and
    which lines of code are executed during all test executions. It is often expressed
    as a percentage and 100% coverage means that every line of code was executed during
    tests.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The most popular code coverage tool is called simply coverage and is freely
    available on PyPI. The usage is very simple and consists only of two steps. The
    first step is to run the coverage run command in your shell with the path to your
    script/program that runs all the tests as an argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The coverage run also accepts `-m` parameter that specifies a runnable module
    name instead of a program path that may be better for some testing frameworks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to generate a human-readable report of your code coverage
    from results cashed in the `.coverage` file. The `coverage` package supports a
    few output formats and the simplest one just prints an ASCII table in your terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The other useful coverage report format is HTML that can be browsed in your
    web browser:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The default output folder of this HTML report is `htmlcov/` in your working
    directory. The real advantage of the `coverage html` output is that you can browse
    annotated sources of your project with highlighted parts that have missing test
    coverage (as shown in *Figure 1*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing coverage](graphics/B05295_10_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1 Example of annotated sources in coverage HTML report
  prefs: []
  type: TYPE_NORMAL
- en: You should remember that while you should always strive to ensure 100% test
    coverage, it is never a guarantee that code is tested perfectly and there is no
    place where code can break. It means only that every line of code was reached
    during execution, but not necessarily every possible condition was tested. In
    practice, it may be relatively easy to ensure full code coverage, but it is really
    hard to make sure that every branch of code was reached. This is especially true
    for the testing of functions that may have multiple combinations of `if` statements
    and specific language constructs like `list`/`dict`/`set` comprehensions. You
    should always care for good test coverage, but you should never treat its measurement
    as the final answer of how good your testing suite is.
  prefs: []
  type: TYPE_NORMAL
- en: Fakes and mocks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Writing unit tests presupposes that you isolate the unit of code that is being
    tested. Tests usually feed the function or method with some data and verify its
    return value and/or the side effects of its execution. This is mainly to make
    sure the tests:'
  prefs: []
  type: TYPE_NORMAL
- en: Are concerning an atomic part of the application, which can be a function, method,
    class, or interface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide deterministic, reproducible results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sometimes, the proper isolation of the program component is not obvious. For
    instance, if the code sends e-mails, it will probably call Python's `smtplib`
    module, which will work with the SMTP server through a network connection. If
    we want our tests to be reproducible and are just testing if e-mails have the
    desired content, then probably this should not happen. Ideally, unit tests should
    run on any computer with no external dependencies and side effects.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to Python's dynamic nature, it is possible to use **monkey patching**
    to modify the runtime code from the test fixture (that is, modify software dynamically
    at runtime without touching the source code) to **fake** the behavior of a third-party
    code or library.
  prefs: []
  type: TYPE_NORMAL
- en: Building a fake
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A fake behavior in the tests can be created by discovering the minimal set of
    interactions needed for the tested code to work with the external parts. Then,
    the output is manually returned or uses a real pool of data that has been previously
    recorded.
  prefs: []
  type: TYPE_NORMAL
- en: This is done by starting an empty class or function and using it as a replacement.
    The test is then launched, and the fake is iteratively updated until it behaves
    correctly. This is possible thanks to nature of a Python type system. The object
    is considered compatible with the given type as long as it behaves as the expected
    type and does not need to be its ancestor via subclassing. This approach to typing
    in Python is called duck typing—if something behaves like a duck, it can be treated
    like a duck.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take an example with a function called `send` in a module called `mailer`
    that sends e-mails:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`py.test` will be used to demonstrate fakes and mocks in this section.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The corresponding test can be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'This test will pass and work as long as there is an SMTP server on the local
    host. If not, it will fail like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'A patch can be added to fake the SMTP class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we have used a new `pytest.yield_fixture()` decorator.
    It allows us to use a generator syntax to provide both setup and teardown procedures
    in a single fixture function. Now our test suite can be run again with the patched
    version of `smtplib`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see from the preceding transcript, our `FakeSMTP` class implementation
    is not complete. We need to update its interface to match the original SMTP class.
    According to the duck typing principle, we need only to provide interfaces that
    are required by the tested `send()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Of course, the fake class can evolve with new tests to provide more complex
    behaviors. But it should be as short and simple as possible. The same principle
    can be used with more complex outputs, by recording them to serve them back through
    the fake API. This is often done for third-party servers such as LDAP or SQL.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important to know that special care should be taken when monkey patching
    any built-in or third-party module. If not done properly, such an approach might
    leave unwanted side effects that will propagate between tests. Fortunately, many
    testing frameworks and tools provide proper utilities that make the patching of
    any code units safe and easy. In our example, we did everything manually and provided
    a custom `patch_smtplib()` fixture function with separated setup and teardown
    steps. A typical solution in `py.test` is much easier. This framework comes with
    a built-in monkey patch fixture that should satisfy most of our patching needs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: You should know that *fakes* have real limitations. If you decide to fake an
    external dependency, you might introduce bugs or unwanted behaviors the real server
    wouldn't have or vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: Using mocks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Mock objects are generic fake objects that can be used to isolate the tested
    code. They automate the building process of the object's input and output. There
    is a greater use of mock objects in statically typed languages, where monkey patching
    is harder, but they are still useful in Python to shorten the code to mimic external
    APIs.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a lot of mock libraries available in Python, but the most recognized
    one is `unittest.mock` and is provided in the standard library. It was created
    as a third-party package and not as a part of the Python distribution but was
    shortly included into the standard library as a provisional package (refer to
    [https://docs.python.org/dev/glossary.html#term-provisional-api](https://docs.python.org/dev/glossary.html#term-provisional-api)).
    For Python versions older than 3.3, you will need to install it from PyPI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'In our example, using `unittest.mock` to patch SMTP is way simpler than creating
    a fake from scratch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The `return_value` argument of a mock object or method allows you to define
    what value is returned by the call. When the mock object is used, every time an
    attribute is called by the code, it creates a new mock object for the attribute
    on the fly. Thus, no exception is raised. This is the case (for instance) for
    the `quit` method we wrote earlier that does not need to be defined anymore.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding example, we have in fact created two mocks:'
  prefs: []
  type: TYPE_NORMAL
- en: The first one that mocks the SMTP class object and not its instance. This allows
    you to easily create a new object regardless of the expected `__init__()` method.
    Mocks by default return new `Mock()` objects if treated as callable. This is why
    we needed to provide another mock as its `return_value` keyword argument to have
    control on the instance interface.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second one that mocks the actual instance returned on the patched `smtplib.SMTP()`
    call. In this mock, we control the behavior of the `sendmail()`method.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In our example, we have used the monkey-patching utility available from the
    `py.test` framework, but `unittest.mock` provides its own patching utilities.
    In some situations (like patching class objects), it may be simpler and faster
    to use them instead of your framework-specific tools. Here is example of monkey
    patching with the `patch()` context manager provided by `unittest.mock` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Testing environment and dependency compatibility
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The importance of environment isolation has already been mentioned in this book
    many times. By isolating your execution environment on both application level
    (virtual environments) and system level (system virtualization), you are able
    to ensure that your tests run under repeatable conditions. This way, you protect
    yourself from rare and obscure problems caused by broken dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: The best way to allow the proper isolation of a test environment is to use good
    continuous integration systems that support system virtualization. There are good
    free solutions for open source projects such as Travis CI (Linux and OS X) or
    AppVeyor (Windows), but if you need such a thing for testing proprietary software,
    it is very likely that you will need to spend some time on building such a solution
    by yourself on top of some existing open source CI tools (GitLab CI, Jenkins,
    and Buildbot).
  prefs: []
  type: TYPE_NORMAL
- en: Dependency matrix testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Testing matrixes for open source Python projects in most cases focus only on
    different Python versions and rarely on different operating systems. Not doing
    your tests and builds on different systems is completely OK for projects that
    are purely Python and where there are no expected system interoperability issues.
    But some projects, especially when distributed as compiled Python extensions,
    should be definitely tested on various target operating systems. For open source
    projects, you may even be forced to use a few independent CI systems to provide
    builds for just the three most popular ones (Windows, Linux, and Mac OS X). If
    you are looking for a good example, you can take a look at the small pyrilla project
    (refer to [https://github.com/swistakm/pyrilla](https://github.com/swistakm/pyrilla))
    that is a simple C audio extension for Python. It uses both Travis CI and AppVeyor
    in order to provide compiled builds for Windows and Mac OS X and a large range
    of CPython versions.
  prefs: []
  type: TYPE_NORMAL
- en: 'But dimensions of test matrixes do not end on systems and Python versions.
    Packages that provide integration with other software such as caches, databases,
    or system services very often should be tested on various versions of integrated
    applications. A good tool that makes such testing easy is tox (refer to [http://tox.readthedocs.org](http://tox.readthedocs.org)).
    It provides a simple way to configure multiple testing environments and run all
    tests with a single `tox` command. It is a very powerful and flexible tool but
    is also very easy to use. The best way to present its usage is to show you an
    example of a configuration file that is in fact the core of tox. Here is the `tox.ini`
    file from the django-userena project (refer to [https://github.com/bread-and-pepper/django-userena](https://github.com/bread-and-pepper/django-userena)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: This configuration allows you to test `django-userena` on five different versions
    of Django and six versions of Python. Not every Django version will work on every
    Python version and the `tox.ini` file makes it relatively easy to define such
    dependency constraints. In practice, the whole build matrix consists of 21 unique
    environments (including a special environment for code coverage collection). It
    would require tremendous effort to create each testing environment manually or
    even using shell scripts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tox is great but its usage gets more complicated if we would like to change
    other elements of the testing environment that are not plain Python dependencies.
    This is a situation when we need to test under different versions of system packages
    and backing services. The best way to solve this problem is again to use good
    continuous integration systems that allow you to easily define matrixes of environment
    variables and install system software on virtual machines. A good example of doing
    that using Travis CI is provided by the `ianitor` project (refer to [https://github.com/ClearcodeHQ/ianitor/](https://github.com/ClearcodeHQ/ianitor/))
    that was already mentioned in [Chapter 9](ch09.html "Chapter 9. Documenting Your
    Project"), *Documenting Your Project*. It is a simple utility for the Consul discovery
    service. The Consul project has a very active community and many new versions
    of its code are released every year. This makes it very reasonable to test against
    various versions of that service. This makes sure that the `ianitor` project is
    still up to date with the latest version of that software but also does not break
    compatibility with previous Consul versions. Here is the content of the `.travis.yml`
    configuration file for Travis CI that allows you to test against three different
    Consul versions and four Python interpreter versions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The preceding example provides 14 unique test environments (including `pep8`
    and `coverage` builds) for the `ianitor` code. This configuration also uses tox
    to create actual testing virtual environments on Travis VMs. This is actually
    a very popular approach to integrate tox with different CI systems. By moving
    as much of a test environment configuration as possible to tox, you are reducing
    the risk of locking yourself to a single vendor. Things like the installation
    of new services or defining system environment variables are supported by most
    of the Travis CI competitors, so it should be relatively easy to switch to a different
    service provider if there is a better product available on the market or Travis
    will change their pricing model for open source projects.
  prefs: []
  type: TYPE_NORMAL
- en: Document-driven development
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*doctests* are a real advantage in Python compared to other languages. The
    fact that documentation can use code examples that are also runnable as tests
    changes the way TDD can be done. For instance, a part of the documentation can
    be done through `doctests` during the development cycle. This approach also ensures
    that the provided examples are always up to date and really working.'
  prefs: []
  type: TYPE_NORMAL
- en: Building software through doctests rather than regular unit tests is called
    **Document-Driven Development** (**DDD**). Developers explain what the code is
    doing in plain English while they are implementing it.
  prefs: []
  type: TYPE_NORMAL
- en: Writing a story
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Writing doctests in DDD is done by building a story about how a piece of code
    works and should be used. The principles are described in plain English and then
    a few code usage examples are distributed throughout the text. A good practice
    is to start to write text on how the code works and then add some code examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see an example of doctests in practice, let''s look at the `atomisator`
    package (refer to [https://bitbucket.org/tarek/atomisator](https://bitbucket.org/tarek/atomisator)).
    The documentation text for its `atomisator.parser` subpackage (under `packages/atomisator.parser/atomisator/parser/docs/README.txt`)
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Later, the doctest will evolve to take into account new elements or the required
    changes. This doctest is also a good documentation for developers who want to
    use the package and should be changed with this usage in mind.
  prefs: []
  type: TYPE_NORMAL
- en: A common pitfall in writing tests in a document is to transform it into an unreadable
    piece of text. If this happens, it should not be considered as part of the documentation
    anymore.
  prefs: []
  type: TYPE_NORMAL
- en: 'That said, some developers that are working exclusively through doctests often
    group their doctests into two categories: the ones that are readable and usable
    so that they can be a part of the package documentation, and the ones that are
    unreadable and are just used to build and test the software.'
  prefs: []
  type: TYPE_NORMAL
- en: Many developers think that doctests should be dropped for the latter in favor
    of regular unit tests. Others even use dedicated doctests for bug fixes.
  prefs: []
  type: TYPE_NORMAL
- en: So, the balance between doctests and regular tests is a matter of taste and
    is up to the team, as long as the published part of the doctests is readable.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When DDD is used in a project, focus on the readability and decide which doctests
    are eligible to be a part of the published documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter advocated the usage of TDD and provided more information on:'
  prefs: []
  type: TYPE_NORMAL
- en: '`unittest` pitfalls'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Third-party tools: `nose` and `py.test`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to build fakes and mocks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation-driven development
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since we already know how to build, package, and test software, in the next
    two chapters we will focus on ways to find performance bottlenecks and optimize
    your programs.
  prefs: []
  type: TYPE_NORMAL
