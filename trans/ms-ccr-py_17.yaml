- en: Memory Models and Operations on Atomic Types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The considerations that need to be made during concurrent programming processes,
    and the problems that follow, are all connected to the way in which Python manages
    its memory. A deep understanding of how variables and values are stored and referenced
    in Python, therefore, would not only help to pinpoint the low-level bugs that
    cause the concurrent program to malfunction but also helps to optimize the concurrent
    codes. In this chapter, we will take an in-depth look into the Python memory model
    as well as its atomic types, specifically their places in the Python concurrency
    ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: The Python memory model, its components that support memory allocation on various
    levels, and the general philosophy in managing memory in Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The definition of atomic operations, the roles they play in concurrent programming,
    and how to use them in Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the technical requirements for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Have Python 3 installed on your computer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Download the GitHub repository at [https://github.com/PacktPublishing/Mastering-Concurrency-in-Python](https://github.com/PacktPublishing/Mastering-Concurrency-in-Python)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During this chapter, we will be working with the subfolder titled `Chapter17`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check out the following video to see the Code in Action: [http://bit.ly/2AiToVy](http://bit.ly/2AiToVy)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python memory model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You might remember the brief discussion on the method of memory management in
    Python in [Chapter 15](0e30892f-4bb1-4196-93c5-5df1d57428b8.xhtml), *The Global
    Interpreter Lock*. In this section, we will look at the Python memory model in
    greater depth by comparing its memory management mechanism to those of Java and
    C++ and discuss how it relates to the practices of concurrent programming in Python.
  prefs: []
  type: TYPE_NORMAL
- en: The components of Python memory manager
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data in Python is stored in memory in a particular way. To gain an in-depth
    understanding on a high level, regarding how data is handled in concurrent programs,
    we first need to dive deep into the theoretical structure of Python memory allocation.
    In this section, we will discuss how data is allocated in a private heap, and
    the handling of this data via the **Python memory manager***—*an overarching entity
    that ensures the integrity of the data.
  prefs: []
  type: TYPE_NORMAL
- en: The Python memory manager consists of a number of components that interact with
    different entities and support different functionalities. For example, one component
    handles the allocation of memory at a low level by interacting with the memory
    manager of the operating system that Python is running on; it is called the **raw
    memory allocator**.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the higher levels, there are also a number of other memory allocators that
    interact with the aforementioned private heap of objects and values. These components
    of the Python memory manager handle object-specific allocations that execute memory
    operations that are specific to the given data and object types: integers have
    to be handled and managed by a different allocator to one that manages strings,
    or one for dictionaries or tuples. As storing and reading instructions varies
    between these data types, these different object-specific memory allocators are
    implemented to gain additional speed while sacrificing some processing space.'
  prefs: []
  type: TYPE_NORMAL
- en: One step lower than the aforementioned raw memory allocator are the system allocators
    from the standard C library (assuming that the Python interpreter under consideration
    is CPython). Sometimes known as general-purpose allocators, these written-in-C
    entities are responsible for helping the raw memory allocator interact with the
    memory manager of the operating system.
  prefs: []
  type: TYPE_NORMAL
- en: 'The entire model of the Python memory manager described previously can be illustrated
    by the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/24811f24-5c55-448f-bea1-fecc4ca01cc0.png)'
  prefs: []
  type: TYPE_IMG
- en: Python memory manager components
  prefs: []
  type: TYPE_NORMAL
- en: Memory model as a labeled directed graph
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have learned about the general process of memory allocation in Python, so
    in this section, let's think about how data is stored and referenced in Python.
    Many programmers often think about the memory model in Python as one object graph
    with a label at each node and the edges are directed—in short, it is a labeled
    directed object graph. This memory model was first put into use with the second
    oldest computer programming language, **Lisp** (previously known as LISP).
  prefs: []
  type: TYPE_NORMAL
- en: 'It is often thought of as a directed graph because its memory model keeps track
    of its data and variables via nothing but pointers: the value of every variable
    is a pointer, and this point can be pointing to a symbol, a number, or a subroutine.
    So, these pointers are the directed edges in the object graph, and the actual
    values (symbols, numbers, subroutines) are the nodes in the graph. The following
    diagram is a simplification of the Lisp memory model in its early stages:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/09d714f4-5438-4c69-9731-f4a2c5b29b54.png)'
  prefs: []
  type: TYPE_IMG
- en: Lisp memory model as an object graph
  prefs: []
  type: TYPE_NORMAL
- en: With this object-graph memory model come a number of advantageous characteristics
    for memory management. First of all, the model offers a significant degree of
    flexibility in terms of reusability; it is possible, and in fact quite easy, to
    write a data structure or a set of instructions for one kind of data type or object
    and then also reuse it on other kinds. In contrast, C is a programming language
    that utilizes a different memory model that does not offer this flexibility, and
    its programmers are usually required to spend a significant amount of time rewriting
    the same data structures and algorithms for different kinds of data types and
    objects.
  prefs: []
  type: TYPE_NORMAL
- en: Another form of flexibility that this memory model provides is the fact that
    every object can be referenced by any number of pointers (or ultimately variables)
    and therefore be mutated by any of them. We have already seen the effect of this
    characteristic in a sample Python program in [Chapter 15](0e30892f-4bb1-4196-93c5-5df1d57428b8.xhtml), *The
    Global Interpreter Lock*, if two variables reference the same (mutable) object
    (achieved when one variable is assigned to another) and one successfully mutates
    the object via its reference, then the change will also be reflected through the
    reference of the second variable.
  prefs: []
  type: TYPE_NORMAL
- en: As also discussed in [Chapter 15](0e30892f-4bb1-4196-93c5-5df1d57428b8.xhtml), *The
    Global Interpreter Lock*, this is not similar to the memory management in C++.
    For example, as when a variable (that is not a pointer or a reference) is assigned
    with a specific value, the programming language will copy that specific value
    to the memory location that contains the original variable. Additionally, when
    a variable is assigned with another variable, the memory location of the latter
    will be copied to that of the former; no further connection between these two
    variables is maintained after the assignment.
  prefs: []
  type: TYPE_NORMAL
- en: However, some argue that this can, in fact, be a disadvantage in programming,
    especially concurrent programming, as uncoordinated attempts to mutate a shared
    object can lead to undesirable results. As experienced Python programmers, you
    might have also noticed that type errors (when a variable expected to be one specific
    type is referencing an object of a different, noncompatible type) are quite common
    in Python programming. This is also a direct result of this memory model, because,
    again, a reference pointer can point to anything.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the theoretical basics of the Python memory model in mind, how can we expect
    it to affect the ecosystem of Python concurrent programming? Fortunately, the
    Python memory model works in favor of concurrent programming in the sense that
    it allows thinking and reasoning about concurrency that is easier and more intuitive. Specifically,
    Python implements its memory model and executes its program instructions in the
    same way that we conventionally expect it to.
  prefs: []
  type: TYPE_NORMAL
- en: To understand this advantage that Python possesses, let's first consider concurrency
    in the Java programming language. To achieve better performance in terms of speed
    in concurrent programs (specifically multithreading programs), Java allows CPUs
    to rearrange the order in which given operations included in Java code are to
    be executed. The rearrangement, however, is made in an arbitrary way so that we
    cannot easily reason the order of execution from just the sequential ordering
    of the code when multiple threads are executing. This leads to the fact that if
    a concurrent program in Java executes in a way that is not intended, the developer
    would need to spend a significant amount of time determining the order of execution
    of the program to pinpoint the bug in their program.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike Java, Python has its memory model structured in a way that maintains
    the sequential consistency of its instructions. This means that the order in which
    the instructions are arranged in the Python code specifies the order of their
    execution—no arbitrary rearrangement of the code and, therefore, no surprising
    behavior from the concurrent programs. However, since the rearrangement in Java
    concurrency is implemented in order to achieve better speed for the programs,
    this means that Python is sacrificing its performance to keep its execution simpler
    and more intuitive.
  prefs: []
  type: TYPE_NORMAL
- en: Atomic operations in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another important topic regarding memory management is atomic operations. In
    this subsection, we will be exploring the definition of being atomic in programming,
    the roles that atomic operations have in the context of concurrent programming,
    and finally how to use atomic operations in Python programs.
  prefs: []
  type: TYPE_NORMAL
- en: What does it mean to be atomic?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's first examine the actual characteristic of being atomic. If an operation
    is atomic in a concurrent program, then it cannot be interrupted by other entities
    in the program during its execution; an atomic operation can also be called linearizable,
    indivisible, or uninterruptible. Given the nature of race conditions and how common
    they are in concurrent programs, it is quite intuitive to conclude that atomicity
    is a desirable characteristic of a program, as it guarantees the integrity of
    the shared data, and protects it from uncoordinated mutations.
  prefs: []
  type: TYPE_NORMAL
- en: The term "atomic" refers to the fact that an atomic operation appears instantaneous
    to the rest of the program that it is in. This means that the operation has to
    be executed in a continuous, uninterrupted manner. The most common method of implementing
    atomicity, as you could probably guess, is via mutual exclusion, or locks. Locks,
    as we have seen, require interactions with a shared resource to take place one
    thread or process at a time, thus protecting those interactions of one thread/process
    from being interrupted and potentially corrupted by other competing threads or
    processes.
  prefs: []
  type: TYPE_NORMAL
- en: If a programmer allows some of the operations in their concurrent program to
    be nonatomic, they would also need to allow those operations to be careful and
    flexible (in the sense of interacting and mutating data) enough so that no errors
    should result from them being interrupted by other operations. If, however, irregular
    and erroneous behaviors were to take place when these operations are interrupted
    during their execution, it would be quite difficult for the programmer to actually
    reproduce and debug those behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: The GIL reconsidered
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the major elements in the context of Python atomic operations is, of
    course, the GIL; there are additionally common misconceptions as well as complexities
    regarding the role the GIL plays in atomic operations.
  prefs: []
  type: TYPE_NORMAL
- en: For example, as reading about the definition of atomic operations, some tend
    to argue that all operations in Python are actually atomic, as the GIL actually
    requires threads to execute in a coordinated manner, with only one being able
    to run at any given point. This is, in fact, a false statement. The requirement
    of the GIL that only one thread can execute Python code at a given time does not
    lead to the atomicity of all Python operations; one operation can still be interrupted
    by another, and errors can still result from the mishandling and corruption of
    shared data.
  prefs: []
  type: TYPE_NORMAL
- en: At a lower level, the Python interpreter handles the switching between threads
    in a Python concurrent program. This process is done with respect to bytecode
    instructions, which are compiled Python code that are interpretable and executable
    by machines. Specifically, Python maintains a fixed frequency specifying how often
    the interpreter should switch between one active thread to another and this frequency
    can be set using the built-in `sys.setswitchinterval()` method. Any nonatomic
    operation can be interrupted during its execution by this thread switching event.
  prefs: []
  type: TYPE_NORMAL
- en: In Python 2, the default value for this frequency is 1,000 bytecode instructions,
    which means that after a thread has successfully executed 1,000 bytecode instructions,
    the Python interpreter will look for other active threads that are waiting to
    be executed. If there is at least one other waiting thread, the interpreter will
    have the currently running thread to release the GIL and have the waiting thread
    acquire it and thus start the execution of the latter thread.
  prefs: []
  type: TYPE_NORMAL
- en: In Python 3, the frequency is fundamentally different. The unit used for the
    frequency is now time-based, specifically in seconds. With the default value of
    15 milliseconds, this frequency specifies that if a thread has been executing
    for at least the amount of time equal to the threshold, then the thread switching
    event (together with the releasing and acquiring of the GIL) will take place as
    soon as the thread finishes the execution of the current bytecode instruction.
  prefs: []
  type: TYPE_NORMAL
- en: Innate atomicity in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned previously, an operation can be interrupted during its execution
    if the thread executing it has passed its executing limit (for example, 15 milliseconds
    in Python 3 by default), at which point the operation has to finish its current
    bytecode instruction and give back the GIL to another thread that is waiting.
    This means that the thread-switching event only takes place between bytecode instructions.
  prefs: []
  type: TYPE_NORMAL
- en: There are operations in Python that can be executed in one single bytecode instruction
    and are therefore atomic in nature without the help of external mechanisms, such
    as mutual exclusion. Specifically, if an operation in a thread completes its execution
    in one single bytecode, it cannot be interrupted by the thread-switching event
    as the event only takes place after the current bytecode instruction is completed.
    This characteristic of innate atomicity is very useful, as it allows the operations
    that have it to execute their instructions freely even if no synchronization method
    is being utilized, while still guaranteeing that they will not be interrupted
    and have their data corrupted.
  prefs: []
  type: TYPE_NORMAL
- en: Atomic versus nonatomic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is important to note that it can be surprising for programmers to learn which
    operations in Python are atomic and which are not. Some might assume that since
    simple operations take less bytecode than complex ones, the simpler an operation
    is, the more likely it is to be innately atomic. However, this is not the case,
    and the only way to determine with certainty which operations are atomic in nature
    is to perform further analyses.
  prefs: []
  type: TYPE_NORMAL
- en: 'According to the documentation of Python 3 (which can be found via this link:
    [docs.python.org/3/faq/library.html#what-kinds-of-global-value-mutation-are-thread-safe](https://docs.python.org/3/faq/library.html#what-kinds-of-global-value-mutation-are-thread-safe)),
    some examples of innately atomic operations include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Appending a predefined object to a list
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extending a list with another list
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fetching an element from a list
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '"Popping" from a list'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sorting a list
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assigning a variable to another variable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assigning a variable to an attribute of an object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a new entry for a dictionary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Updating a dictionary with another dictionary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some operations that are not innately atomic include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Incrementing an integer, including using `+=`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Updating an element in a list by referencing another element in that list
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Updating an entry in a dictionary via referencing another entry in that dictionary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simulation in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s analyze the difference between an atomic operation and a nonatomic one
    in an actual Python concurrent program. If you already have the code for this
    book downloaded from the GitHub page, go ahead and navigate to the `Chapter17`
    folder. For this example, we are considering the `Chapter17/example1.py` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: First of all, we are resetting the thread-switching frequency of the Python
    interpreter to 0.000001 seconds—this is to have the thread switching event take
    place more often than usual and thus amplify any race condition that might be
    in our program.
  prefs: []
  type: TYPE_NORMAL
- en: The gist of the program is to increment a simple global counter (`n`) with 1,000
    separate threads, each incrementing the counter once via the `foo()` function.
    Since the counter was originally initialized as `0`, if the program executed correctly,
    we would have that counter holding the value of 1,000 at the end of the program.
    However, we know that the increment operator that we are using in the `foo()`
    function (`+=`) is not an atomic operation, which means it can be interrupted
    by a thread-switching event when applied on a global variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'After running the script multiple times, we can observe that there is, in fact,
    a race condition existing in our code. This is illustrated by incorrect values
    of the counter that are less than 1,000\. For example, the following is an output
    I obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This is consistent with what we have previously discussed, that is, since the
    `+=` operator is not atomic, it would need other synchronization mechanisms to
    ensure the integrity of the data that it interacts with from multiple threads
    concurrently. Let's now simulate the same experiment with an operation that we
    know is atomic, specifically **appending a predefined object to a list**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `Chapter17/example2.py` file, we have the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Instead of a global counter, we now have a global list that was originally empty.
    The new `foo()` function now takes this global list and appends the integer `1`
    to it. In the rest of the program, we are still creating and running 1,000 separate
    threads, each of which calls the `foo()` function once. At the end of the program,
    we will print out the length of the global list to see if the list has been successfully
    mutated 1,000 times. Specifically, if the length of the list is less than 1,000,
    we will know that there is a race condition in our code, similar to what we saw
    in the previous example.
  prefs: []
  type: TYPE_NORMAL
- en: 'As the `list.append()` method is an atomic operation, however, it is guaranteed
    that there is no race condition when the threads call the `foo()` function and
    interact with the global list. This is illustrated by the length of the list at
    the end of the program. No matter how many times we run the program, the list
    will always have a length of 1,000:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Even though some operations in Python are innately atomic, it can be quite difficult
    to tell whether a given operation is atomic on its own or not. Since the application
    of nonatomic operations on shared data can lead to race conditions and thus erroneous
    results, it is always recommended that programmers utilize synchronization mechanisms
    to ensure the integrity of the shared data within a concurrent program.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have examined the underlying structure of the Python memory
    model, as well as how the language manages its values and variables in a concurrent
    programming context. Given the way memory management in Python is structured and
    implemented, the reasoning for the behaviors of a concurrent program can be significantly
    easier than doing the same in another programming language. The ease in understanding
    and debugging concurrent programs in Python, however, also comes with a decrease
    in performance.
  prefs: []
  type: TYPE_NORMAL
- en: Atomic operations are instructions that cannot be interrupted during their execution.
    Atomicity is a desirable characteristic of concurrent operations, as it guarantees
    the safety of data shared across different threads. While there are operations
    in Python that are innately atomic, synchronization mechanisms such as locking
    are always recommended to guarantee the atomicity of a given operation.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will be looking into how to build a concurrent server
    from scratch. Through this process, we will learn more about implementing communication
    protocols as well as applying concurrency to an existing Python application.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are the main components of the Python memory manager?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does the Python memory model resemble a labeled directed graph?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the advantages and disadvantages of the Python memory model in terms
    of developing concurrent applications in Python?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is an atomic operation, and why is it desirable in concurrent programming?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Give three examples of innately atomic operations in Python.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information you can refer the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The memory models that underlie programming languages* ([http://canonical.org/~kragen/memory-models/](http://canonical.org/~kragen/memory-models/)),K.
    J. Sitaker'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Grok the GIL: How to write fast and thread-safe Python* ([opensource.com/article/17/4/grok-gil](https://opensource.com/article/17/4/grok-gil)), A.
    Jesse Jiryu Davis'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Thread Synchronization Mechanisms in Python* ([http://effbot.org/zone/thread-synchronization.htm#atomic-operations](http://effbot.org/zone/thread-synchronization.htm#atomic-operations)),
    Fredrik Lundh'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Memory* *Management* ([https://docs.python.org/3/c-api/memory.html](https://docs.python.org/3/c-api/memory.html)), Python
    Documentation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Concurrency* ([jython.org/jythonbook/en/1.0/Concurrency](http://www.jython.org/jythonbook/en/1.0/Concurrency.html)), Jython
    Documentation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Memory management in Python* ([anubnair.wordpress.com/2014/09/30/memory-management-in-python/](https://anubnair.wordpress.com/2014/09/30/memory-management-in-python/)), Anu
    B Nair'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
