- en: Language-Level Concurrency and Parallelism in C++
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: C++ has had excellent support for concurrent programming ever since the C++
    11 language standard came out. Until then, threading was an affair that was handled
    by platform-specific libraries. The Microsoft Corporation had its own threading
    libraries, and other platforms (GNU Linux/macOS X) supported the POSIX threading
    model. A threading mechanism as part of the language has helped C++ programmers
    write portable code that runs on multiple platforms.
  prefs: []
  type: TYPE_NORMAL
- en: The original C++ standard was published in 1998, and the language design committee
    firmly believed that threading, filesystems, GUI libraries, and so on are better
    left to the platform-specific libraries. Herb Sutter published an influential
    article in the Dr. Dobbs Journal titled, *The Free Lunch Is Over*, where he advocated
    programming techniques to exploit multiple cores available in the processors of
    those days. While writing parallel code, functional programming models are well-suited
    for the task. Features such as threads, Lambda functions and expressions, move
    semantics, and memory guarantee helps people write concurrent or parallel code
    without much hassle. This chapter aims to enable developers to leverage thread
    libraries and their best practices.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What is concurrency?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A characteristic Hello World program using multiple threads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to manage the lifetime and resources of threads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharing data between threads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to write a thread-safe data structure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is concurrency?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At a basic level, concurrency stands for more than one activity happening at
    the same time. We can correlate concurrency to many of our real-life situations,
    such as eating popcorn while we watch a movie or using two hands for separate
    functions at the same time, and so on. Well then, what is concurrency in a computer?
  prefs: []
  type: TYPE_NORMAL
- en: Computer systems were enabled to do task switching decades ago, and multitasking
    operating systems have been in existence for a long time. Why is there renewed
    interest in concurrency all of a sudden in the computing realm? The microprocessor
    manufacturers were increasing computing power by cramming more and more silicon
    into a processor. At a certain stage in the process, they could not cram more
    things into the same area as they reached fundamental physical limits. The CPUs
    of those eras had a single path of execution at a time and they were running multiple
    paths of instructions by switching tasks (stream of instructions). At the CPU
    level, only one instruction stream was getting executed, and as things happen
    very fast (compared to human perception), the users felt actions were happening
    at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: Around the year 2005, Intel announced their new multicore processors (which
    support multiple paths of execution at the hardware level), which was a game changer.
    Instead of one processor doing every task by switching between them, multicore
    processors came as a solution to actually perform them in parallel. But this introduced
    another challenge to the programmers; to write their code to leverage hardware-level
    concurrency. Also, the issue of the actual hardware concurrency behaving differently
    compared to the illusion created by the task switches arose. Until the multicore
    processors came to light, the chip manufacturers were in a race to increase their
    computing power, expecting that it might reach 10 GHz before the end of the first
    decade of the 21st century. As Herb Sutter said in *The Free Lunch is Over* ([http://www.gotw.ca/publications/concurrency-ddj.htm](http://www.gotw.ca/publications/concurrency-ddj.htm)),
    "*If software is to take advantage of this increased computing power, it must
    be designed to run multiple tasks concurrently*". Herb warned the programmers
    that those who ignored concurrency must also take that into account while writing
    a program.
  prefs: []
  type: TYPE_NORMAL
- en: The modern C++ standard libraries provide a set of mechanisms to support concurrency
    and parallelism. First and foremost, `std::thread`, along with the synchronization
    objects (such as `std::mutex`, `std::lock_guards`**,** `std::unique_lock`, `std::condition_variables`,
    and so on) empowers the programmers to write a concurrent multithreaded code using
    standard C++. Secondly, to use task-based parallelism (as in .NET and Java), C++
    introduced the classes `std::future` and `std::promise`, which work in pairs to
    separate the function invocation and wait for results.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, to avoid the additional overhead of managing threads, C++ introduced
    a class called `std::async`, which will be covered in detail in the following
    chapter where the focus of discussion will be writing lock-free concurrent programs
    (well, at least minimizing locks, wherever possible).
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency is when two or more threads or execution paths can start, run, and
    complete in overlapping time periods (in some kind of interleaved execution).
    Parallelism means two tasks can run at the same time (like you see on a multicore
    CPU). Concurrency is about response time and parallelism is mostly about exploiting
    available resources.
  prefs: []
  type: TYPE_NORMAL
- en: Hello World of concurrency (using std::thread)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s get started with our first program using the `std::thread` library.
    You are expected to have C++ 11 or later to compile the programs we are going
    to discuss in this chapter. Let''s take a simple, classic Hello World example
    as a reference before going into a multi-threaded Hello World:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This program simply writes Hello World into the standard output stream (mainly
    the console). Now, let''s see another example that does the same stuff, but using
    a background thread (often called a worker thread instead):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The first difference with traditional code is the inclusion of the `<thread>`
    standard header file. All of the multithreading support functions and classes
    are declared in this new header. But to achieve synchronization and shared data
    protection, the supporting classes are available in other headers. If you are
    familiar with platform-level threads in Windows or POSIX systems, all threads
    require an initial function. The same concept is what the standard library is
    also following. In this example, the `thread_proc` function is the initial function
    of a thread that's declared in the main function. The initial function (through
    the function pointer) is specified in the constructor of the `std::thread` object
    `t`, and construction starts the execution of the thread.
  prefs: []
  type: TYPE_NORMAL
- en: The most notable difference is that now the application writes the message into
    a standard output stream from a new thread (background thread), which results
    in having two threads or a path of execution in this application. Once the new
    thread has been launched, the main thread continues its execution. If the main
    thread is not waiting for the newly started thread to finish, the `main()` function
    would end and thus that would be the end of the application—even before the new
    thread has had the chance to finish its execution. This is the reason for calling
    `join()` before the main thread finishes, in order to wait for the new thread, `t`,
    which is started here.
  prefs: []
  type: TYPE_NORMAL
- en: Managing threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At runtime, the execution starts at the user entry point `main()` (after the
    execution of the start-up code), and it will be executing in a default thread
    that's been created. So, every program will have at least one thread of execution.
    During the execution of the program, an arbitrary number of threads can be created
    through a standard library or platform-specific libraries. These threads can run
    in parallel if the CPU cores are available to execute them. If the number of threads
    are more than the number of CPU cores, even though there is parallelism, we cannot
    run all of the threads simultaneously. So, thread switching happens here as well.
    A program can launch any number of threads from the main thread, and those threads
    run concurrently on the initial thread. As we can see, the initial function for
    a program thread is `main()`, and the program ends when the main returns from
    its execution. This terminates all the parallel threads. Therefore, the main thread
    needs to wait until all the children threads finish execution. So, let's see how
    the launch and join of threads occurs.
  prefs: []
  type: TYPE_NORMAL
- en: Thread launch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous example, we saw that the initialization function is passed as
    an argument to the `std::thread` constructor, and the thread gets launched. This
    function runs on its own thread. The thread launch happens during the thread object's
    construction, but the initialization functions can have other alternatives as
    well. A function object is another possible argument in a thread class. The C++
    standard library ensures that the `std::thread` works with any callable type.
  prefs: []
  type: TYPE_NORMAL
- en: 'The modern C++ standard supports threads to be initialized through:'
  prefs: []
  type: TYPE_NORMAL
- en: Function pointers (as in the previous section)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An object that implements the call operator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lambdas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Any callable entity is a candidate for initializing a thread. This enables
    the `std::thread` to accept a class object with an overloaded function call operator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the newly created thread copies the object into its storage, hence the
    copy behavior must be ensured. Here, we can also use `std::move` to avoid problems
    related to copying:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'If you pass temporary (an rvalue) instead of a function object, the syntax
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This code can be interpreted by the compiler as a declaration of a function
    that accepts a function pointer and returns a `std::thread` object. However, we
    can avoid this by using the new uniform initialization syntax, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'An extra set of parenthesis, as given in the following code snippet, can also
    avoid the interpretation of `std::thread` object declaration into a function declaration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Another interesting way to launch a thread is by giving the C++ Lambdas as an
    argument into a `std::thread` constructor. Lambdas can capture local variables
    and thus avoid unnecessary usage of any arguments. Lambdas are very useful when
    it comes to writing anonymous functions, but that doesn't mean that they should
    be used everywhere.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Lambda function can be used along with a thread declaration as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Thread join
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the Hello World example, you might have noticed the use of `t.join()` at
    the end of `main()` before leaving from the function. The call to `join()` on
    the associated thread instance ensures that the launched function will wait until
    the background thread completes its execution. In the absence of join, the thread
    will be terminated before the thread starts until the current context is finished
    (their child threads will also be terminated).
  prefs: []
  type: TYPE_NORMAL
- en: '`join()` is a direct function, either waiting for the thread to finish or not.
    To get more control over the thread, we have other mechanisms such as mutex, condition
    variables, and futures, and they will be discussed in the later sections of this
    chapter and the next chapter. The call to `join()` cleans up the storage associated
    with the thread, and so it ensures that the object is no longer associated with
    the thread that was launched. This asserts that the `join()` function can only
    be called once per thread; the call to `joinable()` will always return false after
    a call to `join()`. The previous example with a function object can be modified
    as follows to understand `join()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In this case, at the end of the `func()` function, the thread object is verified
    to confirm whether the thread is still in execution. We call `joinable()` to see
    its return value before we place the join call.
  prefs: []
  type: TYPE_NORMAL
- en: 'To prevent the wait on `func()`, there is a mechanism that was introduced by
    the standard to continue execution, even if the parent function finishes its execution.
    This can be achieved using another standard function, `detach()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: There are a couple of things that we need to consider before detaching a thread;
    the `t` thread will probably still be running when `func()` exits. As per the
    implementation given in the preceding example, the thread is using the reference
    of a local variable created in `func()`, which is not a good idea since the old
    stack variables can be overwritten at any time on most architectures. These situations
    must always be addressed while using `detach()` in your code. The most common
    way of handling this situation is making a thread self-contained and copying the
    data into the thread instead of sharing it.
  prefs: []
  type: TYPE_NORMAL
- en: Passing arguments into a thread
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So, we have figured out how to launch and wait over a thread. Now, let''s see
    how to pass arguments into a thread initialization function. Let''s look at an
    example to find the factorial of a number:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'From this example, it is clear that passing arguments into a thread function
    or a thread callable object can be achieved by passing additional arguments into
    an `std::thread()` declaration. One thing we must keep in mind; *the arguments
    passed are copied into the thread''s internal storage for further execution*.
    It is important for a thread''s execution to have its own copy of arguments, as
    we have seen the problems associated with local variables going out of scope.
    To discuss passing arguments into a thread further, let''s go back to our first
    Hello World example from this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, the `thread_proc()` function takes `std::string` as an argument,
    but we are passing a `const char*` as an argument to the thread function. Only
    in the case of a thread is the argument passed, converted, and copied into the
    thread''s internal storage. Here, `const char*` will be converted to `std::string`.
    The type of argument supplied to a thread must be chosen while keeping this in
    mind. Let''s see what happens if a pointer is supplied to the thread as an argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, the argument supplied to the thread is a pointer to
    the local variable `buf`. There is a probable chance that the `func()` function
    will exit before the conversion of `buf` to an `std::string` happens on the thread.
    This could lead to an undefined behavior. This problem can be resolved by casting
    the `buf` variable into `std::string` in the declaration itself, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s look at the cases where you want a reference to get updated in
    the thread. In a typical scenario, the thread copies the value supplied to the
    thread to ensure a safe execution, but the standard library has also provided
    a means to pass the argument by reference to a thread. In many practical systems,
    you might have seen that a shared data structure is getting updated inside a thread.
    The following example shows how to achieve pass by reference in a thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, wrapping the arguments passed into the `std::thread`
    constructor with `std::ref` ensures that the variable supplied inside the thread
    is referenced to the actual parameters. You might have noticed that the function
    prototype of the thread initialization function is accepting a reference to the
    `shared_data` object, but why do you still need an `std::ref()` wrapping for thread
    invocation? Consider the following code for thread invocation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the `update_data()` function expects the `shared_data` argument
    to be treated as a reference to actual parameters. But when used as a thread initialization
    function, arguments are simply copied internally. When the call to `update_data()`
    happens, it will pass a reference to the internal copies of arguments and not
    a reference to the actual parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Using Lambdas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s see the usefulness of Lambda expressions for multithreading. In
    the following code, we are going to create five threads and put those into a vector
    container. Each thread will be using a Lambda function as the initialization function.
    The threads initialized in the following code are capturing the loop index by
    value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The vector container threads store five threads that have been created inside
    the loop. They are joined at the end of the `main()` function once the execution
    is over. The output for the preceding code may look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The output of the program could be different for each run. This program is a
    good example to showcase the non-determinism associated with concurrent programming.
    In the following section, we will discuss the move properties of a `std::thread`
    object.
  prefs: []
  type: TYPE_NORMAL
- en: Ownership management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From the examples discussed so far in this chapter, you might have noticed that
    the function that launches the thread has to wait for the thread to complete its
    execution using the `join()` function, otherwise it will call `detach()` with
    a cost of the program losing control over the thread. In modern C++, many standard
    types are movable, but cannot be copied; `std::thread` is one of them. This means
    that the ownership of a thread's execution can be moved between `std::thread`
    instances with the help of move semantics.
  prefs: []
  type: TYPE_NORMAL
- en: There are many situations where we want to move the ownership to another thread,
    for example, if we want the thread to run in the background without waiting for
    it on the function that created the thread. This can be achieved by passing the
    thread ownership to a calling function rather than waiting for it to complete
    in the created function. In another instance, pass the ownership to some other
    function, which will wait for the thread to complete its execution. Both of these
    cases can be achieved by passing the ownership from one thread instance to another.
  prefs: []
  type: TYPE_NORMAL
- en: 'To explain further, let us define two functions to use as the thread functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look into the main function that spawns threads from previously declared
    functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, a new thread started with `t1` in the first line of
    `main()`. Ownership is then transferred to `t2` using the `std::move()` function,
    which is invoking the move constructor of `std::thread`, which is associated with
    `t2`. Now, the t1 instance has no associated thread of execution. The initialization
    function `function1()` is now associated with `t2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, a new thread is started using an rvalue, which invokes the move assignment
    operator of `std::thread`, which is associated with `t1`. Since we are using an
    rvalue, an explicit call to `std::move()` is not required:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '`t3` was instantiated without any thread of execution, which means it is invoking
    the default constructor. The ownership currently associated with `t2` is then
    transferred to `t3` by the move assignment operator, by explicitly calling the
    `std::move()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Finally, the `std::thread` instances with an associated thread of execution
    are joined before the program exits. Here, `t1` and `t3` are the instances with
    an associated thread of execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s assume that the following code is present before the threads `join()` in
    the preceding example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Here, the instance `t1` is already associated with a running function (`function2`).
    When `std::move()` attempts to transfer the ownership of `function1` back to `t1`,
    `std::terminate()` is called to terminate the program. This guarantees the consistency
    of the `std::thread` destructor.
  prefs: []
  type: TYPE_NORMAL
- en: 'The move support in `std::thread` helps in transferring the ownership of a
    thread out of a function. The following example demonstrates such a scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Here, the `thread_creator()` function returns the `std::thread` associated with
    the `func()` function. The `thread_wait_func()` function calls `thread_creator()`,
    and then returns the thread object, which is an rvalue that is assigned to an
    `std::thread` object. This transfers the ownership of the thread into the `std::thread`
    object `t`, and object `t` is waiting for the completion of thread execution in
    the transferred function.
  prefs: []
  type: TYPE_NORMAL
- en: Sharing data between threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen how to start a thread and different methods of managing them. Now,
    let's discuss how to share data between threads. One key feature of concurrency
    is its ability to share data between the threads in action. First, let's see what
    the problems associated with threads accessing common (shared) data are.
  prefs: []
  type: TYPE_NORMAL
- en: There won't be a problem if the data shared between threads is immutable (read-only),
    because the data read by one thread is unaffected by whether the other threads
    are reading the same data or not. The moment threads start modifying shared data
    is when problems begin to emerge.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if the threads are accessing a common data structure, the invariants
    associated with the data structure are broken if an update is happening. In this
    case, the number of elements is stored in the data structure, which usually requires
    the modification of more than one value. Consider the delete operation of a self-balancing
    tree or a doubly linked list. If you don't do anything special to ensure otherwise,
    if one thread is reading the data structure, while another is removing a node,
    it is quite possible for the reading thread to see the data structure with a partially
    removed node, so the invariant is broken. This might end up corrupting the data
    structure permanently and could lead to the program crashing.
  prefs: []
  type: TYPE_NORMAL
- en: An invariant is a set of assertions that must always be true during the execution
    of a program or lifetime of an object. Placing proper assertion within the code
    to see whether invariants have been violated will result in robust code. This
    is a great way to document software as well as a good mechanism to prevent regression
    bugs. More can be read about this in the following Wikipedia article: [https://en.wikipedia.org/wiki/Invariant_(computer_science)](https://en.wikipedia.org/wiki/Invariant_(computer_science)).
  prefs: []
  type: TYPE_NORMAL
- en: This often leads to a situation called *race condition*, which is the most common
    cause of bugs in concurrent programs. In multithreading, race condition means
    that the threads race to perform their respective operations. Here, the outcome
    depends on the relative ordering of the execution of an operation in two or more
    threads. Usually, the term race condition means a problematic race condition;
    normal race conditions don't cause any bugs. Problematic race conditions usually
    occur where the completion of an operation requires modification of two or more
    bits of data, such as deletion of a node in a tree data structure or a doubly
    linked list. Because the modification must access separate pieces of data, these
    must be modified in separate instructions when another thread is trying to access
    the data structure. This occurs when half of the previous modifications have been
    completed.
  prefs: []
  type: TYPE_NORMAL
- en: Race conditions are often very hard to find and hard to duplicate because they
    occur in a very short window of execution. For software that uses concurrency,
    the major complexity of implementation comes from avoiding problematic race conditions.
  prefs: []
  type: TYPE_NORMAL
- en: There are many ways to deal with problematic race conditions. The common and
    simplest option is to use *synchronization primitives*, which are lock-based protection
    mechanisms. This wraps the data structure by using some locking mechanisms to
    prevent the access of other threads during its execution. We will discuss the
    available synchronization primitives and their uses in detail in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Another option is to alter the design of your data structure and its invariants
    so that the modification guarantees the sequential consistency of your code, even
    across multiple threads. This is a difficult way of writing programs and is commonly
    referred to as *lock-free programming*. Lock-free programming and the C++ memory
    model will be covered in [Chapter 4](80c4a483-89c7-45fc-a83f-736a1817126e.xhtml), *Asynchronous
    and Lock-Free Programming in C++*.
  prefs: []
  type: TYPE_NORMAL
- en: Then, there are other mechanisms such as handling the updates to a data structure
    as a transaction, as updates to databases are done within transactions. Currently,
    this topic is not in the scope of this book, and therefore it won't be covered.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's consider the most basic mechanism in C++ standard for protecting
    shared data, which is the *mutex*.
  prefs: []
  type: TYPE_NORMAL
- en: Mutexes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A mutex is a mechanism used in concurrency control to prevent race conditions.
    The function of a mutex is to prevent a thread of execution to enter its *critical
    section* at the same time another concurrent thread enters its own critical section.
    It is a lockable object designed to signal when the critical sections of code
    need exclusive access, thereby restricting other concurrent threads with the same
    protection in execution as well as memory access. The C++ 11 standard introduced
    an `std::mutex` class into the standard library to achieve data protection across
    concurrent threads.
  prefs: []
  type: TYPE_NORMAL
- en: The `std::mutex` class consist of the `lock()` and `unlock()` functions to create
    a critical section in code. One thing to keep in mind while using the member functions
    to create critical sections is that you should never skip an unlock function associated
    with a lock function to mark the critical section in code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s discuss the same code we used for discussing Lambdas with threads.
    There, we observed that the output of the program was scrambled due to a race
    condition with a common resource, `std::cout`, and `std::ostream` operators. That
    code is now being rewritten using `std::mutex` to print the thread index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for the preceding code may look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, the mutex is used to protect the shared resource, which
    is the `std::cout` and cascaded `std::ostream` operators. Unlike the older example,
    the addition of a mutex in the code now avoids the scrambled output, but it will
    appear in a random order. The use of `lock()` and `unlock()` functions in the `std::mutex`
    class guarantees the output is not garbled. However, the practice to call member
    functions directly is not recommended, because you need to call unlock on every
    code path in the function, including the exception scenarios as well. Instead,
    C++ standard introduced a new template class, `std::lock_guard`, which implemented
    the **Resource Acquisition Is Initialization** (**RAII**) idiom for a mutex. It
    locks the supplied mutex in the constructor and unlocks it in the destructor.
    The implementation of this template class is available in the `<mutex>` standard
    header library. The previous example can be rewritten using `std::lock_guard`
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, the mutex that protects the critical section is at global
    scope and the `std::lock_guard` object is local to the Lambda each time thread
    execution happens. This way, as soon as the object is constructed, the mutex acquires
    the lock. It unlocks the mutex with the call to destructor when the Lambda execution
    is over.
  prefs: []
  type: TYPE_NORMAL
- en: RAII is a C++ idiom where the lifetime of entities such as database/file handles,
    socket handles, mutexes, dynamically allocated memory on the heap, and so on are
    bounded to the life cycle of the object holding it. You can read more about RAII
    at the following Wikipedia page: [https://en.wikipedia.org/wiki/Resource_acquisition_is_initialization](https://en.wikipedia.org/wiki/Resource_acquisition_is_initialization).
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding deadlock
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While dealing with mutexes, the biggest problem that can arise is a deadlock.
    To understand what deadlock is, just imagine an iPod. For an iPod to achieve its
    purpose, it requires both an iPod as well as an earpiece. If two siblings share
    one iPod, there are situations where both want to listen to music at the same
    time. Imagine one person got their hands on the iPod and the other got the earpiece,
    and neither of them is willing to share the item they possess. Now they are stuck,
    unless one of them tries to be nice and lets the other person listen to music.
  prefs: []
  type: TYPE_NORMAL
- en: Here, the siblings are arguing over an iPod and an earpiece, but coming back
    to our situation, threads argue over the locks on mutexes. Here, each thread has
    one mutex and is waiting for the other. No mutex can proceed here, because each
    thread is waiting for the other thread to release its mutex. This scenario is
    called **deadlock**.
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding deadlock is sometimes quite straightforward because different mutexes
    serve different purposes, but there are instances where handling such situations
    is not that obvious. The best advice I can give you to avoid deadlock is to always
    lock multiple mutexes in the same order. Then, you will never get deadlock situations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider an example of a program with two threads; each thread is intended
    to print odd numbers and even numbers alone. Since the intentions of the two threads
    are different, the program uses two mutexes to control each thread. The shared
    resource between the two threads is `std::cout`. Let''s look at the following
    program with a deadlock situation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The `printEven()` function is defined to print all the positive even numbers
    into the standard console which are less than the `max` value. Similarly, let
    us define a `printOdd()` function to print all the positive odd numbers less than
    `max`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s write the `main` function to spawn two independent threads to print
    odd and even numbers using the previously defined functions as the thread functions
    for each operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, `std::cout` is protected with two mutexes, `printEven` and
    `printOdd`, which perform locking in a different order. With this code, we always
    ends up in deadlock, since each thread is clearly waiting for the mutex locked
    by the other thread. Running this code would result in a hang. As mentioned previously,
    deadlock can be avoided by locking them in the same order, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'But this code is clearly not clean. You already know that using a mutex with
    the RAII idiom makes the code cleaner and safer, but to ensure the order of locking,
    the C++ standard library has introduced a new function, `std::lock`—a function
    that can lock two or more mutexes in one go without deadlock risk. The following
    example shows how to use this for our previous odd-even program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: In this case, as soon as the thread execution enters the loop, the call to `std::lock`
    locks the two mutexes. Two `std::lock_guard` instances are constructed for each
    mutex. The `std::adopt_lock` parameter is supplied in addition to the mutex instance
    to `std::lock_guard` to indicate that the mutexes are already locked, and they
    should just adopt the ownership of the existing lock on the mutex rather than
    attempt to lock the mutex in the constructor. This guarantees safe unlocking,
    even in exceptional cases.
  prefs: []
  type: TYPE_NORMAL
- en: However, `std::lock` can help you to avoid deadlocks in cases where the program
    demands the locking of two or more mutexes at the same time; it doesn't help if
    they are acquired separately. Deadlocks are one of the hardest problems that can
    occur in a multithreaded program. It ultimately relies on the discipline of a
    programmer to not get into any deadlock situations.
  prefs: []
  type: TYPE_NORMAL
- en: Locking with std::unique_lock
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Compared to `std::lock_guard`, `std::unique_lock` provides a bit more flexibility
    in operations. An `std::unique_lock` instance doesn't always own a mutex associated
    with it. Firstly, you can pass `std::adopt_lock` as a second argument to the constructor
    to manage a lock on a mutex similar to `std::lock_guard`. Secondly, the mutex
    can remain unlocked during construction by passing `std::defer_lock` as a second
    argument to the constructor. So, later in the code, a lock can be acquired by
    calling `lock()` on the same `std::unique_lock` object. But the flexibility available
    with `std::unique_lock` comes with a price; it is a bit slower than `lock_guard`
    in regards to storing this extra information and is in need of an update. Therefore,
    it is recommended to use `lock_guard` unless there is a real need for the flexibility
    that `std::unique_lock` offers.
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting feature about `std::unique_lock` is its ability to transfer
    ownership. Since `std::unique_lock` must own its associated mutexes, this results
    in the ownership transfer of mutexes. Similar to `std::thread`, the `std::unique_lock`
    class is also a move only type. All of the move semantic language nuances and
    rvalue reference handling available in the C++ standard library applies to `std::unique_lock`
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'The availability of member functions such as `lock()` and `unlock()`, similar
    to `std::mutex`, increases the flexibility of its use in code compared to `std::lock_guard`.
    The ability to release the lock before an `std::unique_lock` instance is destroyed,
    meaning that you can optionally release it anywhere in the code if it''s obvious
    that the lock is no longer required. Holding down the lock unnecessarily can drop
    the performance of the application drastically, since the threads waiting for
    locks are prevented from executing for longer than is necessary. Hence, `std::unique_lock`
    is a very handy feature introduced by the C++ standard library, which supports
    RAII idiom, and it can effectively minimize the size of a critical section of
    the applicable code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, you can see the fine-grained locking achieved by leveraging
    the flexibility of `std::unique_lock`. As the function starts its execution, an
    `std::unique_lock` object is constructed with `global_mutex` in an unlocked state.
    Immediately, data is prepared with params, which don't require exclusive access;
    it is executing freely. Before retrieving the prepared data, the `local_lock`
    is marking the beginning of a critical section using the lock member function
    in `std::unique_lock`. As soon as the data retrieval is over, the lock is released,
    marking the end of the critical section. Followed by that, a call to the `process_data()`
    function, which again does not require exclusive access, is getting executed freely.
    Finally, before the execution of the `store_result()` function, the mutex is locked
    to protect the write operation, which updates the processed result. When exiting
    the function, the lock gets released when the local instance of `std::unique_lock`
    is destroyed.
  prefs: []
  type: TYPE_NORMAL
- en: Condition variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We already know that mutexes can be used to share common resources and synchronize
    operations between threads. But synchronization using mutexes is a little complex
    and deadlock-prone if you are not careful. In this section, we will discuss how
    to wait for events with condition variables and how to use them for synchronization
    in an easier way.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to synchronization using mutexes, if the waiting thread has acquired
    a lock over a mutex, it can't be locked by any other thread. Also, waiting for
    one thread to complete its execution by checking on a status flag periodically
    that is protected by a mutex is a waste of CPU resources. This is because these
    resources can be effectively utilized by other threads in the system rather than
    having to wait for a longer time.
  prefs: []
  type: TYPE_NORMAL
- en: 'To address these problems, the C++ standard library has provided two implementations
    of conditional variables: `std::condition_variable` and `std::condition_variable_any`.
    Both are declared inside the `<condition_variable>` library header, and both the
    implementations need to work with a mutex to synchronize threads. The implementation
    of `std::condition_variable` is limited to working with `std::mutex`. On the other
    hand, `std::condition_variable_any` can work with anything that meets mutex-like
    criteria (mutex-like semantics), hence `suffix _any`. Because of its generic behavior,
    `std::condition_variable_any` ends up consuming more memory and degrades performance.
    It is not recommended unless a real, tailored requirement is in place.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following program is an implementation of odd-even threads that we discussed
    when we talked about mutexes, which is now being re-implemented using condition
    variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The program starts with the declaration of a mutex, a conditional variable,
    and two Boolean flags globally so that we can synchronize them between two threads.
    The `printEven` function gets executed in a worker thread and prints only even
    numbers starting from 0\. Here, when it enters the loop, the mutex is protected
    with `std::unique_lock` instead of `std::lock_guard`; we will see the reason for
    that in a moment. The thread then calls the `wait()` function in `std::condition_variable`,
    passing the lock object and a Lambda predicate function that expresses the condition
    being waited for. This can be replaced with any callable object that returns bool.
    In this function, the predicate function returns the `bEvenReady` flag, so that
    the function continues execution when it becomes true. If the predicate returns
    false, the `wait()` function will unlock the mutex and wait for another thread
    to notify it, hence the `std::unique_lock` object comes handy here with the provided
    flexibility to lock and unlock.
  prefs: []
  type: TYPE_NORMAL
- en: 'As soon as `std::cout` prints the loop index, the `bEvenReady` flag is raised
    to false and `bOddReady` is raised to true. Then, the call to the `notify_one()`
    function associated with `syncCond` signals the waiting odd thread to write an
    odd number into the standard output stream:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The `printOdd` function gets executed in another worker thread and prints only
    odd numbers starting from `1`. Like the `printEven` function, a loop iterates
    and prints the index that is protected by the globally declared conditional variable
    and mutex. Unlike the `printEven` function, the predicate used in the `wait()`
    function of a condition variable returns `bOddReady`, and the `bEvenReady` flag
    is raised to `true` and the `bOddReady` flag is raised to `false`. Followed by
    that, calling the `notify_one()` function associated with `syncCond` signals the
    waiting even thread to write an even number into the standard output stream. This
    interleaved printing of even and odd numbers continues until the max value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The main function launches two background threads, `t1`, which is associated
    with the `printEven` function and `t2`, which is associated with the `printOdd`
    function. The output starts when even parity is confirmed by raising the `bEvenReady`
    flag to true before the threads are launched.
  prefs: []
  type: TYPE_NORMAL
- en: A thread-safe stack data structure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have discussed how to launch and manage a thread, and how to synchronize
    the operations between concurrent threads. But, when it comes to actual systems,
    the data is represented in the form of data structures, which must be chosen appropriately
    for the situation to guarantee the performance of the program. In this section,
    we are going to discuss how to design a concurrent stack using conditional variables
    and mutexes. The following program is a wrapper to `std::stack`, which is declared
    under the library header `<stack>`, and the stack wrapper will be available with
    different overloads for pop and push functionalities (this has been done to keep
    the listing small, and this also demonstrates how we can adapt a sequential data
    structure to work in a concurrent context):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Stack` class contains an object to the template class `std::stack`, along
    with member variables for `std::mutex` and `std::condition_variable`. The constructor
    and destructor of the class are marked as default, letting the compiler generate
    a default implementation for those, and the copy assignment operator is marked
    as delete to prevent the invocation of the assignment operator of this class at
    compile time itself. The copy constructor is defined, which copies the `std::stack`
    member object `myData`, by invoking its own copy assignment operator, which is
    protected by the right-hand side object''s mutex:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The member function `push()` is wrapping the `push` function of `std::stack
    container`. As you can see, the mutex member variable, `myMutex`, is locked by
    an `std::lock_guard` object to safeguard the `push` operation that follows in
    the next line. Followed by that, the `notify_one()` function is invoked using
    the member `std::condition_variable` object to raise an event to notify the waiting
    threads over this same condition variable. There are two overloads of the `pop`
    operation that you will see in the following code listings, which wait over this
    condition variable to get signaled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The `try_pop()` function takes a template argument as a reference. Since the
    implementation never waits for the stack to fill at least one element, this uses
    the `std::lock_guard` object to protect the thread. The function returns `false`
    if the stack is empty, otherwise it returns `true`. Here, the output is assigned
    to input a reference argument by invoking the `top()` function of `std::stack`,
    which returns the topmost element in the stack, followed by the `pop()` function
    to clear the topmost element from the stack. All overloads for the `pop` function
    invoke the `top()` function followed by a call to the `pop()` function of `std::stack`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'This is another overload of the `try_pop()` function, which returns an instance
    of `std::shared_ptr` (smart pointer) of the template type. As you have already
    seen, the `try_pop` function overloads, and never waits for a stack to fill at
    least one element; therefore, this implementation uses `std::lock_guard`. If the
    internal stack is empty, the function returns an instance of `std::shared_ptr`
    and holds no element of the stack. Otherwise, a `std::shared_ptr` instance that
    holds the top element of the stack is returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: So far, the overloads of the `pop` function are not waiting for the stack to
    fill at least one element if it is empty. To achieve that, two more overloads
    of the `pop` function are added, which uses the wait function associated with
    `std::condition_variable`. The first implementation returns the template value
    as an output argument, and the second one returns an `std::shared_ptr` instance.
    Both functions use `std::unique_lock` to control the mutex in order to supply
    the `wait()` function of `std::condition_variable`. In the `wait` function, the
    `predicate` function is checking whether the stack is empty or not. If the stack
    is empty, then the `wait()` function unlocks the mutex and continues to wait until
    a notification is received from the `push()` function. As soon as the push is
    called, the predicate will return true, and `wait_n_pop` continues its execution.
    The function overload takes the template reference and assigns the top element
    into the input argument, and the latter implementation returns an `std::shared_ptr`
    instance, holding the top element.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed the threading library available in C++ standard
    libraries. We saw how to launch and manage a thread, and discussed different aspects
    of the threading library, such as how to pass arguments into a thread, ownership
    management of a thread object, sharing of data between threads, and so on. The
    C++ standard threading library can execute most callable objects as threads! We
    have seen the importance of all the available callable objects in association
    with threads, such as `std::function`, Lambdas, and functors. We discussed the
    synchronization primitives available in the C++ standard library, starting with
    the simple `std::mutex`, the use of the RAII idiom to protect mutexes from unhandled
    exit cases to avoid explicit unlock, and using classes such as `std::lock_guard`
    and `std::unique_lock`. We also discussed condition variables (`std::condition_variable`)
    in the context of thread synchronization. This chapter lays a good foundation
    for concurrency support introduced in modern C++ to kickstart the journey of this
    book into functional idioms.
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapter, we will be covering more concurrency library features
    in C++, such as task-based parallelism and lock-free programming.
  prefs: []
  type: TYPE_NORMAL
