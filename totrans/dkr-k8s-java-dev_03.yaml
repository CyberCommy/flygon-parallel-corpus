- en: Working with Microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After reading the previous two chapters, you should now have an understanding
    of the Docker architecture and its concepts. Before we go further on our Java,
    Docker, and Kubernetes journey, let's get to know the concept of microservices.
  prefs: []
  type: TYPE_NORMAL
- en: By reading this chapter, you will find out why a transition to microservices
    and cloud development is necessary and why monolithic architecture is not an option
    anymore. The microservices architecture is also where Docker and Kubernetes will
    be especially useful.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to microservices and comparison to a monolithic architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How Docker and Kubernetes fits into the microservices world
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When to use microservices architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before we actually create the Java microservice and deploy it using Docker and
    Kubernetes, let's start with an explanation of the microservices idea and compare
    it to the monolithic architecture.
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By definition, microservices, also known as the **Microservice Architecture**
    (**MSA** ), is an architectural style and design pattern which says that an application
    should consist of a collection of loosely-coupled services. This architecture
    decomposes business domain models into smaller, consistent pieces implemented
    by services. In other words, each of the services will have its own responsibilities,
    independent of others, each one of them will provide a specific functionality.
  prefs: []
  type: TYPE_NORMAL
- en: These services should be isolated and autonomous. Yet, they of course need to
    communicate to provide some piece of business functionality. They usually communicate
    using `REST` exposures or by publishing and subscribing events in the publish/subscribe
    way.
  prefs: []
  type: TYPE_NORMAL
- en: The best way of explaining the reasoning behind the idea of microservices is
    to compare them with an old, traditional approach for building large applications,
    the monolithic design.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the following diagram presenting the monolithic application
    and distributed application consisting of microservices:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00056.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see on the previous diagram, the monolithic application differs totally
    from an application created using the microservices architecture. Let's compare
    the two approaches and point out their advantages and flaws.
  prefs: []
  type: TYPE_NORMAL
- en: Monolithic versus microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We begin the comparison by starting with the description of the monolithic architecture
    to present its characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: The monolithic architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the past, we used to create applications as complete, massive, and uniform
    pieces of code. Let''s take a web MVC application for example. A simplified architecture
    of such an application is presented in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00057.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, the diagram presents the typical web application, a fragment
    of a banking system in this case. It''s the **Model** **View Controller** (**MVC**
    ) application, consisting of models, views, and controllers to serve up HTML content
    back to the client''s browser. It could probably also accept and send the JSON
    content via the REST endpoints. This kind of an application is built as a single
    unit. As you can see, we have a couple of layers here. Enterprise Applications
    are built in three parts usually: a client-side user interface (consisting of
    HTML pages and JavaScript running in a browser), a server-side part handling the
    `HTTP` requests (probably constructed using some spring-like controllers), then
    we have a service layer, which could probably be implemented using EJBs or Spring
    services. The service layer executes the domain specific business logic, and retrieves/updates
    data in the database, eventually. This is a very typical web application which
    every one of us has probably created once in a while. The whole application is
    a monolith, a single logical executable. To make any changes to the system, we
    must build and deploy an updated version of the whole server-side application;
    this kind of application is packaged into single WAR or EAR archive, altogether
    with all the static content such as HTML and JavaScript files. When deployed,
    all the application code runs in the same machine. Scaling this kind of application
    usually requires deploying multiple copies of the exact same application code
    to multiple machines in a cluster, behind some load balancer perhaps.'
  prefs: []
  type: TYPE_NORMAL
- en: This design wasn't too bad, we had our applications up and running, after all.
    But the world, especially when using Agile methodologies, changes fast. Businesses
    have started asking to release software faster than ever. ASAP has become a very
    common word in the IT development language dictionary. The specification fluctuates,
    so the code changes often and grows over time. If the team working on the application
    is large (and it probably will be in case of complex, huge applications) everyone
    must be very careful not to destroy each other's work. With every added feature,
    our applications become more and more complex. The compile and build times become
    longer, sooner or later it will become tricky to test the whole thing using unit
    or integration tests. Also, the point of entry for new members coming to the team
    can be daunting, they will need to checkout the whole project from the source
    code repository. Then they need to build it in their IDE (which is not always
    that easy in case of huge applications), and analyze and understand the component
    structure to get their job done. Additionally, people working on the user interface
    part will need to communicate with developers working on the middle-tier, with
    people modelling the database, DBAs, and so on. The team structure will often
    begin to mimic the application architecture over time. There's a risk that a developer
    working on the specific layer will tend to put as much logic into the layer he
    controls as he can. As a result, the code can become unmaintainable over time.
    We all have been there and done that, haven't we?
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, the scaling of monolithic systems is not as easy as putting a WAR or
    EAR in another application server and then booting it. Because all the application
    code runs in the same process on the server, it''s often almost impossible to
    scale individual portions of the application. Take this example: we have an application
    which integrates with the VOIP external service. We don''t have many users of
    our application, but then there is a lot of events coming from the VOIP service
    we need to process. To handle the increasing load, we need to scale our application
    and, in the case of a monolithic system, we need to scale the whole system. That''s
    because the application is a single, big, working unit. If just one of the application''s
    services is CPU or resource hungry, the whole server must be provisioned with
    enough memory and CPU to handle the load. This can be expensive. Every server
    needs a fast CPU and enough RAM to be able to run the most demanding component
    of our application.'
  prefs: []
  type: TYPE_NORMAL
- en: 'All monolithic applications have these characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: They are rather large, often involving a lot of people working on them. This
    can be a problem when loading your project into the IDE, despite having powerful
    machines and a great development environment, such as IntelliJ IDEA, for example.
    But it's not only about the hundreds, thousands, or millions of lines of code.
    It's about the complexity of the solution, such as communication problems between
    team members. Problems with communication could lead to multiple solutions for
    the same problem in different parts of the application. And this will make it
    even bigger, it can easily evolve into a big ball of mud where no one can understand
    the whole system any longer. Moreover, people can be afraid of introducing substantial
    changes to the system, because something at an opposite end could suddenly stop
    working. Too bad if this is reported by the users, on a production system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They have a long release cycle, we all know the process of release management,
    permissions, regression testing, and so on. It's almost impossible to create a
    continuous delivery flow having a huge, monolith application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are difficult to scale; it typically takes a considerable amount of work
    to put in a new application instance in the cluster by the operations team. Scaling
    the specific feature is impossible, the only option you have is to multiply the
    instances of the whole system in the cluster. This makes scaling up and down a
    big challenge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In case of deployment failure, the whole system is unavailable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You are locked into the specific programming language or technology stack. Of
    course, with Java, parts of the system can be developed in one or more languages
    that run on JVM, such as Scala, Kotlin, or Groovy, but if you need to integrate
    with a `.net` library, here begins the trouble. This also means that you will
    not always be able to use the right tool for the job. Imagine a scenario in which
    you would like to store a lot of complex documents in the database. They often
    have different structures. MongoDB as a document database should be suitable,
    right? Yes, but our system is running on Oracle.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It's not well suited well for agile development processes, where we need to
    implement changes all the time, release to production almost at once, and be ready
    for the next iteration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As you can see, monolithic applications are only good for small scale teams
    and small projects. If you need something that has a larger scale and involves
    many teams, it''s better to look at the alternative. But what to do with the existing
    monolithic system you may enjoy dealing with? You may realize that it can be handy
    to outsource some parts of the system outside, into small services. This will
    speed up the development process and increase testability. It will also make you
    application easier to scale. While the monolithic application still retains the
    core functionality, many pieces can be outsourced into small side services supporting
    the core module. This approach is presented in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00058.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this, let's say intermediary solution, the main business logic will stay
    in your application monolith. Things such as integrations, background jobs, or
    other small subsystems that can be triggered by messages, for example, can be
    moved to their own services. You can even put those services into the cloud, to
    limit the necessity for managing infrastructure around them even further. This
    approach allows you to gradually move your existing monolith application into
    a fully service-oriented architecture. Let's look at the microservices approach.
  prefs: []
  type: TYPE_NORMAL
- en: The microservices architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The microservices architecture is designed to address the issues we''ve mentioned
    with monolithic applications. The main difference is that the services defined
    in the monolithic application are decomposed into individual services. Best of
    all, they are deployed separately from one another on separate hosts. Take a look
    at the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00059.jpg)'
  prefs: []
  type: TYPE_IMG
- en: When creating an application using the microservices architecture, each microservice
    is responsible for a single, specific business function and contains only the
    implementation that is required to perform exactly that specific business logic.
    It's same as a **divide** and **conquer** way of creating a system. This may seem
    similar to the SOA-oriented architecture. In fact, traditional SOA and microservices
    architecture share some common features. Both organize fragments of the application
    into services and both define clear boundaries at which a service can be decoupled
    from the other. SOA, however, has its roots in the need to integrate monolithic
    applications with another one. This has been done, usually, using an API that
    was usually SOAP-based, using heavy XML messaging. In SOA, this integration was
    relying heavily on some kind of middleware in between, usually **Enterprise Service
    Bus** (**ESB** ). Microservices architecture can also utilize the message bus,
    with significant differences. In microservices architecture there is no logic
    in the messaging layer at all, it is purely used as a transport for messages from
    one service to another. This is a total contrast to ESB, which needed a lot of
    logic for message routing, schema validation, message translation, and so on.
    As a result, microservices architecture is less cumbersome than traditional SOA.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to scaling, there's a huge difference when comparing microservices
    to monolithic applications. The key advantage of microservices is that a single
    service can be scaled individually, depending on the resource requirements. That's
    because they are self-sufficient and independent. As a microservice is usually
    deployed on smaller (in terms of resources) host; the host needs to contain only
    resources that are required for a service to function properly. As the resource
    requirement grows, scaling is easy both ways, horizontally and vertically. To
    scale horizontally, you just deploy as many instances as you need to handle load
    on a specific component.
  prefs: []
  type: TYPE_NORMAL
- en: We will get back to this concept in the coming chapters, when we will be getting
    to know Kubernetes. Scaling vertically is also a lot easier and cheaper in comparison
    to the monolithic systems, you upgrade only a host on which your microservice
    is being deployed. Also, introducing new versions of the service is easy, you
    don't need to stop the whole system just to upgrade a piece of functionality.
    In fact, you can do it on the fly. When deployed, microservices improve the fault
    tolerance for the entire application. For example, if there is a memory leak in
    one service or some other problem, only this service will be affected and can
    then be fixed and upgraded without interfering with the rest of the system. This
    is not the case with monolithic architecture, where one faulty component can bring
    down the entire application.
  prefs: []
  type: TYPE_NORMAL
- en: From a developer's perspective, having your application split into separate
    pieces deployed individually gives a huge advantage. A developer skilled in server-side
    JavaScript can develop its piece `node.js` , while the rest of the system will
    be developed in Java. It's all related to the API exposed by each microservice;
    apart from this API, each microservice doesn't need to know anything about the
    rest of the services. This makes the development process a lot easier. Separate
    microservices can be developed and tested independently. Basically, the microservices
    approach dictates that instead of having one giant code base that all developers
    are working on, which often becomes tricky to manage, there are several smaller
    code bases managed by small and agile teams. The only dependency services have
    on one another is their exposed APIs. There's a difference in storing data as
    well. As we have said before, each microservice should be responsible for storing
    its own data, because again, it should be independent. This leads to another feature
    of the microservices architecture, a possibility to have a polyglot persistence.
    Microservices should own their data.
  prefs: []
  type: TYPE_NORMAL
- en: While microservices communicate and exchange data with other microservices using
    REST endpoints or events, they can store their own data in the form that is best
    suitable for the job. If the data is relational, the service will be using a traditional,
    relational database such as MySQL or PostgreSQL. If a document database is better
    suited for the job, a microservice can use MongoDB for example, or Neo4j if it's
    graph as data. That leads to another conclusion, by implementing the microservices
    architecture we can now only choose the programming language or framework that
    will be best suited for the job, this applies to the data storage as well. Of
    course, having its own data can lead to a challenge in the microservices architecture,
    data consistency. We are going to cover this subject in a while in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s summarize the benefits of using the microservices architecture from
    the development process perspective:'
  prefs: []
  type: TYPE_NORMAL
- en: Services can be written using a variety of languages, frameworks, and their
    versions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each microservice is relatively small, easier to understand by the developer
    (which results in less bugs), easy to develop, and testable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The deployment and start up time is fast, which makes developers more productive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each service can consist of multiple service instances for increased throughput
    and availability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each service can be deployed independently of other services, easier to deploy
    new versions of services frequently
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is easier to organize the development process; each team owns and is responsible
    for one or more service and can develop, release, or scale their service independently
    of all of the other teams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can choose whatever programming language or framework you think is best
    for the job. There is no long-term commitment to a technology stack. If needed,
    the service can be rewritten in the new technology stack, and if there are no
    API changes, this will be transparent for the rest of the system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is better for continuous delivery as small units are easier to manage, test,
    and deploy. As long as each team maintains backwards and forward API compatibility,
    it can work in release cycles that are decoupled from other teams. There are some
    scenarios where these release cycles are coupled, but this is not the common case
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maintaining data consistency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Services must be loosely coupled so that they can be developed, deployed, and
    scaled independently. They of course, need to communicate, but they are independent
    of each other. They, have well defined interfaces and encapsulate implementation
    details. But what about data? In the real world and in non-trivial applications
    (and microservice applications will probably be non-trivial), business transactions
    must often span multiple services. If you, for example, create a banking application,
    before you execute the customer''s money transfer order, you need to ensure that
    it will not exceed his account balance. The single database that comes with a
    monolith application gives us a lot of convenience: atomic transactions, a single
    place to look for data, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, in the microservices world, different services need to be
    independent. This also means that they can have different data storage requirements.
    For some services, it will be a relational database, others might need a document
    database such as MongoDB, which is good at storing complex, unstructured data.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, when building microservices and thus splitting up our database into multiple
    smaller databases, how do we manage these challenges? We have also said that services
    should own their data. That is, every microservice should depend only on its own
    database. The service''s database is effectively part of the implementation of
    that service. This leads to quite an interesting challenge when designing the
    microservices architecture. As Martin Fowler says in his `Microservice trade-offs`
    column: Maintaining strong consistency is extremely difficult for a distributed
    system, which means everyone has to manage eventual consistency. How do we deal
    with this? Well, it''s all about boundaries.'
  prefs: []
  type: TYPE_NORMAL
- en: Microservices should have clearly defined responsibilities and boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Microservices need to be grouped according to their business domain. Also,
    in practice, you will need to design your microservices in such a way that they
    cannot directly connect to a database owned by another service. The loose coupling
    means microservices should expose clear API interfaces that model the data and
    access patterns related to this data. They must stick to those interfaces, when
    changes are necessary, you will probably introduce a versioning mechanism and
    create another version of the microservice. You could use a publish/subscribe
    pattern to dispatch events from one microservice to be processed by others, as
    you can see in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Image00060.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The publish/subscribe mechanism you would want to use should provide retry and
    rollback features for the event processing. In a publish/subscribe scenario, the
    service that modifies or generates the data allows other services to subscribe
    to events. The subscribed services receive the event saying that the data has
    been modified. It's often the case that the event contains the data that has been
    modified. Of course, the event publish/subscribe pattern can be used not only
    in relation to data changes, it can be used as a generic communication mechanism
    between services. This is a simple and effective approach but it has a downside,
    there is a possibility to lose an event.
  prefs: []
  type: TYPE_NORMAL
- en: When creating distributed applications, you may want to consider that there
    will be data inconsistency for some amount of time. When an application changes
    data items on one machine, that change needs to be propagated to the other replicas.
    Since the change propagation is not instant, there's a time interval during which
    some of the copies will have the most recent change, but others won't. However,
    the change will be propagated to all the copies, eventually. That's why this is
    called eventual consistency. Your services would need to assume that the data
    will be in an inconsistent state for a while and need to deal with the situation
    by using the data as is, postponing the operation, or even ignoring certain pieces
    of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, there are a lot of challenges, but also a lot of advantages
    behind using microservices architecture. You should be warned, though, there are
    more challenges we need to address. As services are independent of each other,
    they can be implemented in different programming languages. This means the deployment
    process of each may vary: it will be totally different for a Java web application
    and for a `node.js` application. This can make the deployment to a server complex.
    This is precisely the point where Docker comes to the rescue.'
  prefs: []
  type: TYPE_NORMAL
- en: The Docker role
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you remember from the previous chapters, Docker utilizes the concept of containerization.
    You simply put your application (in this context, the application will be a microservice)
    no matter what language and technology it uses, into a single, deployable and
    runnable piece of software, called the image. We are going to cover the process
    of packaging a Java application into the image in detail in the [Chapter 4](text00063.html)
    , *Creating Java Microservices* . The Docker image will contain everything our
    service needs to work, it can be a Java Virtual Machine with all required libraries
    and an application server, or it can also be a `node.js` application packaged
    together with the `node.js` runtime with all the needed `node.js` modules, such
    as `express.js` or whatever the `node.js` service needs to run. A microservice
    might consist of two containers,   one running the service code and another running
    a database to keep the service's own data.
  prefs: []
  type: TYPE_NORMAL
- en: Docker isolates containers to one process or service. In effect, all the pieces
    of our application will just be a bunch of black boxes, packaged and ready to
    use Docker images. Containers operate as fully isolated sandboxes, with only the
    minimal kernel of the operating system present for each container. Docker uses
    the Linux kernel and makes use of kernel interfaces such as cnames and namespaces,
    which allow multiple containers to share the same kernel while running in complete
    isolation from one another.
  prefs: []
  type: TYPE_NORMAL
- en: Because the system resources of the underlying system are shared, you can run
    your services at optimal performance, the footprint is substantially smaller in
    comparison to traditional virtual machines. Because containers are portable, as
    we have said in [Chapter 2](text00037.html) , *Networking and Persistent Storage*
    , they can run everywhere the Docker engine can run. This makes the process of
    deployment of microservices easy. To deploy a new version of a service running
    on a given host, the running container can simply be stopped and a new container
    started that is based on a Docker image using the latest version of the service
    code. We are going to cover the process of creating new versions of the image
    later in this book. Of course, all the other containers running on the host will
    not be affected by this change.
  prefs: []
  type: TYPE_NORMAL
- en: As microservices need to communicate using the `REST` protocol, our Docker containers
    (or, to be more precise, your Java microservices packaged and running from within
    the Docker container) also need to communicate using the network. As you remember
    from [Chapter 2](text00037.html) , *Networking and Persistent Storage* , about
    networking, it's very easy to expose and map a network port for the Docker container.
    It seems that Docker containerization is ideal for the purposes of microservice
    architecture. You can package the microservice into a portable box and expose
    the needed network ports, enabling it to communicate to the outside world. When
    needed, you can run as many of those boxes as you want.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s summarize the Docker features that are useful when dealing with microservices:'
  prefs: []
  type: TYPE_NORMAL
- en: It is easy to scale up and scale down a service, you just change the running
    container instances count
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The container hides the details of the technology behind each of the services.
    All containers with our services are started and stopped in exactly the same way,
    no matter what technology stack they use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each service instance is isolated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can limit the runtime constraints on the CPU and memory consumed by a container
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Containers are fast to build and start. As you remember from [Chapter 1](text00022.html)
    , *Introduction to Docker* , there's minimal overhead in comparison to traditional
    virtualization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker image layers are cached, this gives you another speed boost when creating
    a new version of the service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Doesn''t it fit perfectly for the definition of the microservices architecture?
    Sure it does, but there''s one problem. Because our microservices are spread out
    across multiple hosts, it can be difficult to track which hosts are running certain
    services and also monitor which of them need more resources or, in the worst case,
    are dead and not functioning properly. Also, we need to group services that belong
    to the specific application or feature. This is the missing element in our puzzle:
    container management and orchestration. A lot of frameworks emerged for the sole
    purpose of handling more complex scenarios: managing single services in a cluster
    or multiple instances in a service across hosts, or how to coordinate between
    multiple services on a deployment and management level. One of these tools is
    Kubernetes.'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes' role
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While Docker provides the lifecycle management of containers, Kubernetes takes
    it to the next level by providing orchestration and managing clusters of containers.
    As you know, your application created using the microservice architecture will
    contain a couple of separated, independent services. How do we orchestrate and
    manage them? Kubernetes is an open-source tool that's perfect for this scenario.
    It defines a set of building blocks which provide mechanisms for deploying, maintaining,
    and scaling applications. The basic scheduling unit in Kubernetes is called a
    pod. Containers in a pod run on the same host, share the same IP address, and
    find each other via localhost. They can also communicate with each other using
    standard inter-process communications, such as shared memory or semaphores. A
    pod adds another level of abstraction to containerized components. A pod consists
    of one or more containers that are guaranteed to be co-located on the host machine
    and can share resources. It's same as a logical collection of containers that
    belong to an application.
  prefs: []
  type: TYPE_NORMAL
- en: For traditional services, such as a REST endpoint together with the corresponding
    database (our complete microservice, in fact), Kubernetes provides a concept of
    service. A service defines a logical group of pods and also enforces rules for
    accessing such logical groups from the outside world. Kubernetes uses the concept
    of Labels for pods and other resources (services, deployments, and so on). These
    are simple the key-value pairs that can be attached to resources at creation time
    and then added and modified at any time. We will be using labels later on, to
    organize and to select subsets of resources (pods, for example) to manage them
    as one entity.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes can place your container or a group of containers in the specific
    host automatically. To find a suitable host (the one with the smallest workload),
    it will analyze the current workload of the hosts and different colocation and
    availability constraints. Of course, you will be able to specify the host manually,
    but having this automatic feature can make the best of the processing power and
    resources available. Kubernetes can monitor the resource usage (CPU and RAM) at
    the container, pod, and cluster level. The resource usage and performance analysis
    agent runs on each node, auto-discovers containers on the node, and collects CPU,
    memory, filesystem, and network usage statistics.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes also manages the lifecycle of your container instances. If there
    are too many of them, some of them will be stopped. If the workload increases,
    new containers will be started automatically. This feature is called container
    auto-scaling. It will automatically change the number of running containers, based
    on memory, CPU utilization, or other metrics you define for your services, as
    the number of queries per second, for example.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you remember from [Chapter 2](text00037.html) , *Networking and Persistent
    Storage* , Docker operates volumes to persist your application data. Kubernetes
    also supports two kinds of volume: regular which has the same lifecycle as the
    pod, and persistent with a lifecycle independent of any pod. Volume types are
    implemented the same way as in Docker in the form of plugins. This extensible
    design enables you to have almost any type of volume you need. It currently contains
    storage plugins such as Google Cloud Platform volume, AWS elastic block storage
    volume, and others.'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes can monitor the health of your services, it can do it by executing
    a specified `HTTP` method (the same as `GET` for example) for the specified URL
    and analyzing the `HTTP` status code given back in response. Also, a TCP probe
    can check if a specified port is open which can also be used to monitor the health
    of your service. Last, but not least, you can specify the command that can be
    executed in the container, and some actions that could be taken based on the command's
    response. If the specified probe method signals that something is wrong with the
    container, it can be automatically restarted. When you need to update your software,
    Kubernetes supports rolling updates. This feature allows you to update a deployed,
    containerized application with minimal downtime. The rolling update feature lets
    you specify the number of old replicas that may be down while they are being updated.
    Upgrading containerized software with Docker is especially easy, as you already
    know, it will just be a new image version for the container. I guess you are now
    getting the complete picture. Deployments can be updated, rolled out, or rolled
    back. Load balancing, service discovery, all the features you would probably need
    when orchestrating and managing your herd of microservices running from within
    Docker containers are available in Kubernetes. Initially made by Google for big
    scale, Kubernetes is nowadays widely used by organizations of various sizes to
    run containers in production.
  prefs: []
  type: TYPE_NORMAL
- en: When to use the microservice architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The microservice architecture is a new way to think about structuring applications
    At the beginning, when you begin creating a system and it's relatively small,
    there will probably be no need to use the microservices approach. Of course, it's
    nothing wrong with the basic web application. When doing basic web applications
    for the people in your office, going with the microservice architecture may be
    overkill. On the other hand, if you plan to develop a new, super internet service
    that will be used by millions of mobile clients, I would consider going with microservices
    from the start. Joking aside, you get the picture, always try to pick the best
    tool for the job. In the end, the goal is to provide business value.
  prefs: []
  type: TYPE_NORMAL
- en: However, you should keep in mind the whole picture of your system after some
    time. If your application is growing larger in features and functionality than
    you expected, or you know that from the beginning, you may want to start breaking
    features off into microservices. You should try to do the functional decomposition
    and point out the fragments of your systems that have clear boundaries and which
    would need scaling, and separate deployments in the future. If there's a lot of
    people working on a project, having them developing the separate, independent
    pieces of an application will give the development process a huge boost. There
    can be a mix of technology stacks used each service can be implemented in a different
    programming language or framework and store its own data in the most suitable
    data storage. It's all about API and the way services communicate with each other.
    Having this architecture will result in a faster time to market the build, test,
    and deployment time is highly reduced in comparison to a monolith architecture.
    If you need to scale only the service that needs to handle higher workload. Having
    Docker and Kubernetes available, there is no reason not to go into the microservice
    architecture; it will pay off in the future, for sure.
  prefs: []
  type: TYPE_NORMAL
- en: The microservice architecture is not just a new trendy buzzword, it's generally
    accepted as a better way to build applications today. The birth of the microservice
    idea has been driven by the need to make better use of compute resources and the
    need to maintain more and more complex web-based applications.
  prefs: []
  type: TYPE_NORMAL
- en: Java is an excellent choice when building microservices. You can create a microservice
    as a single executable JAR, self-contained Spring Boot application, or fully featured
    web application deployed on an application server such as Wildfly or Tomcat. Depending
    on your use case and the responsibilities and features of your microservices,
    any of the previous will do. Docker Repository contains a lot of useful images
    you can use freely as a base for your microservice. Many images present in The
    Docker Hub are created by private individuals, some extending official images
    and customizing them to their needs, but others are entire platform configurations
    customized from base images. The base image can be as simple as pure JDK or it
    can be a fully configured Wildfly ready to run. This gives a serious development
    performance boost.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have compared monolithic and microservices architectures.
    I hope you see the advantages of using the latter. We have also learned how Docker
    and Kubernetes fits into the whole picture when deploying containerized applications,
    making this process a lot more easy and pleasant. Java is a proven ecosystem for
    implementing microservices. The software you are going to create will consist
    of small, highly testable, and efficient modules. In fact, in [Chapter 4](text00063.html)
    , *Creating Java Microservices* , we are going to get our hands dirty and create
    such a microservice.
  prefs: []
  type: TYPE_NORMAL
