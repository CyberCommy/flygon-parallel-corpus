- en: Storm and Hadoop Integration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have seen how Storm can be used for developing real-time stream processing
    applications. In general, these real-time applications are seldom used in isolation;
    they are more often than not used in combination with other batch processing operations.
  prefs: []
  type: TYPE_NORMAL
- en: The most common platform for developing batch jobs is Apache Hadoop. In this
    chapter, we will see how applications built with Apache Storm can be deployed
    over existing Hadoop clusters with the help of a Storm-YARN framework for optimized
    use and management of resources. We will also cover how we can write the process
    data into HDFS by creating an HDFS bolt in Storm.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Overview of Apache Hadoop and its various components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a Hadoop cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write Storm topology to persist data into HDFS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of Storm-YARN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying Storm-YARN on Hadoop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running a storm application on Storm-YARN.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to Hadoop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Hadoop is an open source platform for developing and deploying big data
    applications. It was initially developed at Yahoo! based on the MapReduce and
    Google File System papers published by Google. Over the past few years, Hadoop
    has become the flagship big data platform.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will discuss the key components of a Hadoop cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop Common
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is the base library on which other Hadoop modules are based. It provides
    an abstraction for OS and filesystem operations so that Hadoop can be deployed
    on a variety of platforms.
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop Distributed File System
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Commonly known as **HDFS**, the **Hadoop Distributed File System** is a scalable,
    distributed, fault-tolerant filesystem. HDFS acts as the storage layer of the
    Hadoop ecosystem. It allows the sharing and storage of data and application code
    among the various nodes in a Hadoop cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the key assumptions taken while designing HDFS:'
  prefs: []
  type: TYPE_NORMAL
- en: It should be deployable on a cluster of commodity hardware.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hardware failures are expected, and it should be tolerant to these.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It should be scalable to thousands of nodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It should be optimized for high throughput, even at the cost of latency.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most of the files will be large in size, so it should be optimized for big files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storage is cheap, so use replication for reliability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It should be locality aware so that the computations requested of the data can
    be performed on the physical node where it actually resides. This will result
    in less data movement, hence lower network congestion.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An HDFS cluster has the following components.
  prefs: []
  type: TYPE_NORMAL
- en: Namenode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The namenode is the master node in an HDFS cluster. It is responsible for managing
    the filesystem metadata and operations. It does not store any user data--but only
    the filesystem tree of all files in the cluster. It also keeps track of the physical
    locations of the blocks that are part of the files.
  prefs: []
  type: TYPE_NORMAL
- en: Since, the namenode keeps all the data in RAM, it should be deployed on a machine
    with a large amount of RAM. Also, no other processes should be hosted on the machine
    that is hosting the namenode so that all the resources are dedicated to it.
  prefs: []
  type: TYPE_NORMAL
- en: The namenode is the single point of failure in an HDFS cluster. If the namenode
    dies, no operations can take place on an HDFS cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00054.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: HDFS Cluster'
  prefs: []
  type: TYPE_NORMAL
- en: Datanode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The datanode is responsible for storing user data in HDFS clusters. There can
    be multiple datanodes in an HDFS cluster. A datanode stores data on the physical
    disks attached to the system hosting the datanode. It is not recommended to store
    datanode data on disks in a RAID configuration as HDFS achieves data protection
    by replicating data across datanodes.
  prefs: []
  type: TYPE_NORMAL
- en: HDFS client
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An HDFS client is a client library that can be used to interact with HDFS clusters.
    It usually talks to the namenode to perform meta operations, such as creating
    new files and so on, while the datanodes serve the actual data read and write
    requests.
  prefs: []
  type: TYPE_NORMAL
- en: Secondary namenode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The secondary namenode is one of the poorly named components of HDFS. Despite
    its name, it is not a standby for the namenode. To understand its function, we
    need to delve deep into how the namenode works.
  prefs: []
  type: TYPE_NORMAL
- en: A namenode keeps the filesystem metadata in the main memory. For durability,
    it also writes this metadata to the local disk in the form of the image file.
    When a namenode starts, it reads this fs image snapshot file to recreate the in-memory
    data structure for holding the filesystem data. Any updates on the filesystem
    are applied to the in-memory data structure, but not to the image. These changes
    are written to disk in separate files called edit logs. When a namenode starts,
    it merges these edit logs into the image so that the next restart will be quick.
    In production, the edit logs can grow very large as the namenode is not restarted
    frequently. This could result in a very long boot time for the namenode whenever
    it is restarted.
  prefs: []
  type: TYPE_NORMAL
- en: The secondary namenode is responsible for merging the edit logs of the namenode
    with the image so that the namenode starts faster the next time. It takes the
    image snapshot and the edit logs from the namenode and merges them and then puts
    the updated image snapshot on the namenode machines. This reduces the amount of
    merging that is required from the namenode on the restarts, thus reducing the
    time to boot for the namenode.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot illustrates the working of the secondary namenode:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00055.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Secondary Namenode functioning'
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have seen the storage side of Hadoop. Next we will look into the
    processing components.
  prefs: []
  type: TYPE_NORMAL
- en: YARN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: YARN is a cluster resource management framework that enables users to submit
    a variety of jobs to a Hadoop cluster, and manages scalability, fault tolerance,
    scheduling of jobs, and so on. As HDFS provides a storage layer for large amounts
    of data, the YARN framework gives you the plumbing required for writing big data
    processing applications.
  prefs: []
  type: TYPE_NORMAL
- en: The following are the major components of a YARN cluster.
  prefs: []
  type: TYPE_NORMAL
- en: ResourceManager (RM)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ResourceManager is the entry point for applications in the YARN cluster.
    It is the master process in the cluster that is responsible for managing all the
    resources in the cluster. It is also responsible for the scheduling of various
    jobs submitted to the cluster. This scheduling policy is pluggable, and can be
    customized by a user in case they want to support new kinds of application.
  prefs: []
  type: TYPE_NORMAL
- en: NodeManager (NM)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A NodeManager agent is deployed on each of the processing nodes in the cluster.
    It is the counterpart to the ResourceManager at the node level. It communicates
    with the ResourceManager to update the node state and receive any job requests
    from it. It is also responsible for the life cycle management and the reporting
    of various node metrics to the ResourceManager.
  prefs: []
  type: TYPE_NORMAL
- en: ApplicationMaster (AM)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once a job is scheduled by the ResourceManager, it no longer keeps track of
    its status and progress. This results in the ResourceManager being able to support
    completely different kinds of application in the cluster without worrying about
    the internal communication and logic of the application.
  prefs: []
  type: TYPE_NORMAL
- en: Whenever an application is submitted, the ResourceManager creates a new ApplicationMaster
    for that application, which is then responsible for negotiating resources from
    ResourceManager and communicating with the NodeMangers for the resources. NodeManager
    provides resources in the form of resource containers, which are abstractions
    for resource allocation, where you can tell how much CPU, memory, and so on is
    required.
  prefs: []
  type: TYPE_NORMAL
- en: Once the application starts running on various nodes in the cluster, the ApplicationMaster
    keeps track of the status of the various jobs and in case of failures, reruns
    those jobs. On completion of the job, it releases the resources to the ResourceManager.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot illustrates the various components in a YARN cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00056.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: YARN components'
  prefs: []
  type: TYPE_NORMAL
- en: Installation of Hadoop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have seen both the storage and processing parts of a Hadoop cluster,
    let's get started with the installation of Hadoop. We will be using Hadoop 2.2.0
    in this chapter. Please note that this version is not compatible with Hadoop 1.X
    versions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be setting up a cluster on a single node. Before starting, please make
    sure that you have the following installed on your system:'
  prefs: []
  type: TYPE_NORMAL
- en: JDK 1.7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ssh-keygen`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In case you don''t have `wget` or `ssh-keygen`, install it with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will need to set up a passwordless SSH on this machine as it is required
    for Hadoop.
  prefs: []
  type: TYPE_NORMAL
- en: Setting passwordless SSH
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the steps for setting up a passwordless SSH:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate your SSH key pair by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we need to copy the generated public key to the list of authorized keys
    in the current users. To do this, execute the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can check whether the passwordless SSH is working by connecting to
    localhost with the SSH by the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Since we are able to use SSH into localhost without a password, our setup is
    working now and we will now proceed with the Hadoop setup.
  prefs: []
  type: TYPE_NORMAL
- en: Getting the Hadoop bundle and setting up environment variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the steps for setting up Hadoop:'
  prefs: []
  type: TYPE_NORMAL
- en: Download Hadoop 2.2.0 from the Apache site at [http://hadoop.apache.org/releases.html#Download](http://hadoop.apache.org/releases.html#Download).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Untar the archive at a location where we want to install Hadoop. We will call
    this location `$HADOOP_HOME`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to set up the environment variables and the path for Hadoop,
    Add the following entries to your `~/.bashrc` file. Make sure that you are providing
    the paths for Java and Hadoop as per your system:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Refresh your `~/.bashrc` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s check whether the paths are properly configured with the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding snippet, we can see that the paths are properly set. Now we
    will set up HDFS on our system.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up HDFS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Follow these steps for setting up HDFS:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Make directories for holding the namenode and datanode data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify the namenode port in the `$HADOOP_CONF_DIR/core-site.xml` file by adding
    the following property inside the `<configuration>` tag:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify the namenode and datanode directory in the `$HADOOP_CONF_DIR/hdfs-site.xml`
    file by adding the following property inside the `<configuration>` tag:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will format the namenode. This is a one-time process, and it needs to
    be done only while setting up the HDFS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we are done with the configuration, and we will start HDFS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, execute the `jps` command to see if all the processes are running fine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see that all the expected processes are running.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you can check the status of HDFS using the namenode UI by opening `http://localhost:50070`
    in your browser. You should see something similar to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00057.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Namenode web UI'
  prefs: []
  type: TYPE_NORMAL
- en: You can interact with HDFS using the `hdfs dfs` command. Get all the options
    by running `hdfs dfs` on a console or refer to the documentation at [http://hadoop.apache.org/docs/r2.2.0/hadoop-project-dist/hadoop-common/FileSystemShell.html](http://hadoop.apache.org/docs/r2.2.0/hadoop-project-dist/hadoop-common/FileSystemShell.html).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that HDFS is deployed, we will set up YARN next.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up YARN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the steps for setting up YARN:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a `mapred-site.xml` file from the template `mapred-site.xml.template`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify that we are using a YARN framework by adding the following property
    in the `$HADOOP_CONF_DIR/mapred-site.xml` file in the `<configuration>` tag:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Configure the following properties in the `$HADOOP_CONF_DIR/yarn-site.xml`
    file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Start the YARN processes with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, execute the `jps` command to see if all the processes are running fine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see that all the expected processes are running.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you can check the status of YARN using the ResourceManager web UI by opening
    `http://localhost:8088/cluster` in your browser. You should see something similar
    to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00058.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: ResourceManager web UI'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can interact with YARN using the `yarn` command. Get all the options by
    running `yarn` on a console or refer to the documentation at [http://hadoop.apache.org/docs/r2.2.0/hadoop-yarn/hadoop-yarn-site/YarnCommands.html](http://hadoop.apache.org/docs/r2.2.0/hadoop-yarn/hadoop-yarn-site/YarnCommands.html).
    To get all the applications currently running on YARN, run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: With this, we have completed the deployment of the Hadoop cluster on a single
    node. Next we will see how to run Storm topologies on this cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Write Storm topology to persist data into HDFS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we are going to cover how we can write the HDFS bolt to persist
    data into HDFS. In this section, we are focusing on the following points:'
  prefs: []
  type: TYPE_NORMAL
- en: Consuming data from Kafka
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The logic to store the data into HDFS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rotating file into HDFS after a predefined time or size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Perform the following steps to create the topology to store the data into the
    HDFS:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a new maven project with groupId `com.stormadvance` and artifactId `storm-hadoop`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add the following dependencies in the `pom.xml` file. We are adding the Kafka
    Maven dependency in `pom.xml` to support Kafka Consumer. Please refer the previous
    chapter to produce data in Kafka as here we are going to consume data from Kafka
    and store in HDFS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Write a Storm Hadoop topology to consume data from HDFS and store it in HDFS.
    The following is a line-by-line description of the `com.stormadvance.storm_hadoop.topology.StormHDFSTopology`
    class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use the following lines to consume the data from Kafka:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the following lines of code to define the HDFS Namenode details and the
    name of the HDFS data directory to store the data into HDFS, create a new file
    after every 5 MB chunk of data stored into HDFS, and sync the latest data into
    the file after every 1,000 records:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the following code to connect Spout with the HDFS bolt:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Integration of Storm with Hadoop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The probability that the organizations developing and operating big data applications
    already have a Hadoop cluster deployed is very high. Also, there is a high possibility
    that they also have real-time stream processing applications deployed to go along
    with the batch applications running on Hadoop.
  prefs: []
  type: TYPE_NORMAL
- en: It would be great if we can leverage the already deployed YARN cluster to also
    run the Storm topologies. This will reduce the operational cost of maintenance
    by giving you only one cluster to manage instead of two.
  prefs: []
  type: TYPE_NORMAL
- en: Storm-YARN is a project developed by Yahoo! that enables the deployment of Storm
    topologies over YARN clusters. It enables the deployment of Storm processes on
    nodes managed by YARN.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates how the Storm processes are deployed on YARN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00059.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Storm processes on YARN'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will see how to set up Storm-YARN.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Storm-YARN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since Storm-YARN is still in alpha, we will be proceeding with the base master
    branch of the `git` repository. Make sure you have `git` installed on your system.
    If not, then run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Also make sure that you have Apache Zookeeper and Apache Maven installed on
    your system. Refer to the previous chapters for their setup instructions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the steps for deploying Storm-YARN:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Clone the `storm-yarn` repo with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Build `storm-yarn` by running the following `mvn` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Copy the `storm.zip` file from `storm-yarn/lib` to HDFS by using the following
    commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The exact version might be different from `1.0.2-wip21` in your case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a directory to hold our Storm configuration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the following configuration in the `~/storm-data/storm-1.0.2-wip21/conf/storm.yaml`
    file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: If required, change the values as per your setup.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add a `storm-yarn/bin` folder to your path by adding the following to the `~/.bashrc`
    file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Refresh `~/.bashrc`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Make sure Zookeeper is running on your system. If not, then start ZooKeeper
    by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Launch `storm-yarn` using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The Storm-YARN application has been submitted with the application ID `application_1397537047058_0001`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can retrieve the status of our application by using the following `yarn`
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also see `storm-yarn` running on the ResourceManager web UI at `http://localhost:8088/cluster/`.
    You should be able to see something similar to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00060.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Storm-YARN on the ResourceManager web UI'
  prefs: []
  type: TYPE_NORMAL
- en: You can explore the various metrics exposed by clicking through various links
    on the UI.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nimbus should also be running now, and you should be able to see it through
    the Nimbus web UI at `http://localhost:7070/`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00061.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Nimbus web UI running on YARN'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we need to get the Storm configuration that will be used when deploying
    topologies on this Storm cluster on YARN. To do so, execute the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Make sure that you are passing the correct application ID (as retrieved in step
    9) to the `-appId` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have successfully deployed Storm-YARN, we will see how to run our
    topologies on this storm cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Storm-Starter topologies on Storm-YARN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will see how to deploy the Storm-Starter topologies on `storm-yarn`.
    Storm-Starter is a set of example topologies that comes with Storm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to run the topologies on Storm-YARN:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Clone the `storm-starter` project:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Package the topologies with the following `mvn` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Deploy the topologies on `storm-yarn` with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can see the deployed topology on the Nimbus web UI at `http://localhost:7070/`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00062.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Nimbus web UI showing the word-count topology on YARN'
  prefs: []
  type: TYPE_NORMAL
- en: 'To see how you can interact with the topologies running on `storm-yarn`, run
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: It will list all the options interacting with the various Storm processes and
    starting new supervisors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So in this section, we built a Storm-started topology and ran it on `storm-yarn`.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced Apache Hadoop and the various components, such
    as HDFS, YARN, and so on, that are part of a Hadoop cluster. We also saw the subcomponents
    of an HDFS and YARN cluster and how they interact with each other. Then we walked
    through how to set up a single node Hadoop cluster.
  prefs: []
  type: TYPE_NORMAL
- en: We also introduced Storm-YARN, which was the main point of this chapter. Storm-YARN
    enables you to run Storm topologies on a Hadoop cluster. This helps from a manageability
    and operations point of view. Finally, we saw how to deploy a topology on Storm
    running on YARN.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will see how Storm can integrate with other big data
    technologies, such as HBase, Redis, and so on.
  prefs: []
  type: TYPE_NORMAL
