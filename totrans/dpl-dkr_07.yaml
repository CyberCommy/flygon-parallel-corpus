- en: The Limits of Scaling and the Workarounds
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When you scale up your systems, every tool or framework you are using will
    reach a point where it will break or just not function as expected. For some things
    that point will be high and for some it will be low, and the intent of this chapter
    is to cover strategies and workarounds for the most likely scalability issues
    you will encounter when working with microservice clusters. In this chapter we
    will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Increasing service density and stability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoiding and mitigating common issues with large-scale deployments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-service containers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practices for zero-downtime deployments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limiting service resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have not really spent any time talking about service isolation with
    regard to the resources available to the services, but it is a very important
    topic to cover. Without limiting resources, a malicious or misbehaving service
    could be liable to bring the whole cluster down, depending on the severity, so
    great care needs to be taken to specify exactly what allowance individual service
    tasks should use.
  prefs: []
  type: TYPE_NORMAL
- en: 'The generally accepted strategy for handling cluster resources is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Any resource that may cause errors or failures to other services if used beyond
    intended values is highly recommended to be limited on the service level. This
    is usually the RAM allocation, but may include CPU or others.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any resources, specifically the hardware ones, for which you have an external
    limit should also be limited for Docker containers too (e.g. you are only allowed
    to use a specific portion of a 1-Gbps NAS connection).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anything that needs to run on a specific device, machine, or host should be
    locked to those resources in the same fashion. This kind of setup is very common
    when only a certain number of machines have the right hardware for a service,
    such as in GPU computing clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any resource that you would like specifically rationed within the cluster generally
    should have a limit applied. This includes things such as lowering the CPU time
    percentage for low-priority services.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In most cases, the rest of the resources should be fine using normal allocations
    of the available resources of the host.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By applying these rules, we will ensure that our cluster is more stable and
    secure, with the exact division of resources that we want among the services.
    Also, if the exact resources required for a service are specified, the orchestration
    tool usually can make better decisions about where to schedule newly created tasks
    so that the service density per Engine is maximized.
  prefs: []
  type: TYPE_NORMAL
- en: RAM limits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Strangely enough, even though CPU might be considered the most important computing
    resource, RAM allocation for clustered services is even more important due to
    the fact that RAM overuse can (and will) cause **Out of Memory** (**OOM**) process
    and task failures for anything running on the same host. With the prevalence of
    memory leaks in software, this usually is not a matter of "if" but "when", so
    setting limits for RAM allocation is generally very desirable, and in some orchestration
    configurations it is even mandatory. Suffering from this issue is usually indicated
    by seeing `SIGKILL`, `"Process killed"`, or `exit code -9` on your service.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind, though, that these signals could very well be caused by other
    things but the most common cause is OOM failures.
  prefs: []
  type: TYPE_NORMAL
- en: By limiting the available RAM, instead of a random process on the host being
    killed by OOM manager, only the offending task's processes will be targeted for
    killing, so the identification of faulty code is much easier and faster because
    you can see the large number of failures from that service and your other services
    will stay operational, increasing the stability of the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: OOM management is a huge topic and is much more broad than it would be wise
    to include in this section, but it is a very important thing to know if you spend
    a lot of time in the Linux kernel. If you are interested in this topic, I highly
    recommend that you visit [https://www.kernel.org/doc/gorman/html/understand/understand016.html](https://www.kernel.org/doc/gorman/html/understand/understand016.html)
    and read up on it.WARNING! On some of the most popular kernels, memory and/or
    swap cgroups are disabled due to their overhead. To enable memory and swap limiting
    on these kernels, your hosts kernel must be started with `cgroup_enable=memory`
    and `swapaccount=1` flags. If you are using GRUB for your bootloader, you can
    enable them by editing `/etc/default/grub` (or, on the latest systems, `/etc/default/grub.d/<name>`),
    setting `GRUB_CMDLINE_LINUX="cgroup_enable=memory swapaccount=1"`, running `sudo
    update-grub`, and then restarting your machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use the RAM-limiting `cgroup` configuration, run the container with a combination
    of the following flags:'
  prefs: []
  type: TYPE_NORMAL
- en: '`-m` / `--memory`: A hard limit on the maximum amount of memory that a container
    can use. Allocations of new memory over this limit will fail, and the kernel will
    terminate a process in your container that will usually be the main one running
    the service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--memory-swap`: The total amount of memory including swap that the container
    can use. This must be used with the previous option and be larger than it. By
    default, a container can use up to twice the amount of allowed memory maximum
    for a container. Setting this to `-1` allows the container to use as much swap
    as the host has.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--memory-swappiness`: How eager the system will be to move pages from physical
    memory to on-disk swap space. The value is between `0` and `100`, where `0` means
    that pages will try to stay in resident RAM as much as possible, and vice versa.
    On most machines this value is `80` and will be used as the default, but since
    swap space access is very slow compared to RAM, my recommendation is to set this
    number as close to `0` as you can afford.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--memory-reservation`: A soft limit for the RAM usage of a service, which
    is generally used only for the detection of resource contentions with the generally
    expected RAM usage so that the orchestration engine can schedule tasks for maximum
    usage density. This flag does not have any guarantees that it will keep the service''s
    RAM usage below this level.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are a few more flags that can be used for memory limiting, but even the
    preceding list is a bit more verbose than you will probably ever need to worry
    about. For most deployments, big and small, you will probably only need to use
    `-m` and set a low value of `--memory-swappiness`, the latter usually being done
    on the host itself through the `sysctl.d` boot setting so that all services will
    utilize it.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can check what your `swappiness` setting is by running `sysctl vm.swappiness`.
    If you would like to change this, and in most cluster deployments you will, you
    can set this value by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '`$ echo "vm.swappiness = 10" | sudo tee -a /etc/sysctl.d/60-swappiness.conf`'
  prefs: []
  type: TYPE_NORMAL
- en: 'To see this in action, we will first run one of the most resource-intensive
    frameworks (JBoss) with a limit of 30 MB of RAM and see what happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As expected, the container used up too much RAM and was promptly killed by the
    kernel. Now, what if we try the same thing but give it 400 MB of RAM?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Our container can now start without any issues!
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have worked a lot with applications in bare metal environments, you
    might be asking yourselves why exactly the JBoss JVM didn''t know ahead of time
    that it wouldn''t be able to run within such a constrained environment and fail
    even sooner. The answer here lies in a really unfortunate quirk (though I think
    it might be considered a feature depending on your point of view) of `cgroups`
    that presents the host''s resources unaltered to the container even though the
    container itself is constrained. You can see this pretty easily if you run a memory-limited
    container and print out the available RAM limits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As you can imagine, this causes all kinds of cascade issues with applications
    launched in a `cgroup` limited container such as this, the primary one being that
    the application does not know that there is a limit at all so it will just go
    and try to do its job assuming that it has full access to the available RAM. Once
    the application reaches the predefined limits, the app process will usually be
    killed and the container will die. This is a huge problem with apps and runtimes
    that can react to high memory pressures as they might be able to use less RAM
    in the container but because they cannot identify that they are running constrained,
    they tend to gobble up memory at a much higher rate than they should.
  prefs: []
  type: TYPE_NORMAL
- en: Sadly, things are even worse on this front for containers. You must not only
    give the service a big enough RAM limit to start it, but also enough that it can
    handle any dynamically allocated memory during the full duration of the service.
    If you do not, the same situation will occur but at a much less predictable time.
    For example, if you ran an NGINX container with only a 4 MB of RAM limit, it will
    start just fine but after a few connections to it, the memory allocation will
    cross the threshold and the container will die. The service may then restart the
    task and unless you have a logging mechanism or your orchestration provides good
    tooling for it, you will just end up with a service that has a `running` state
    but, in actuality, it is unable to process any requests.
  prefs: []
  type: TYPE_NORMAL
- en: If that wasn't enough, you also really should not arbitrarily assign high limits
    either. This is due to the fact that one of the purposes of containers is to maximize
    service density for a given hardware configuration. By setting limits that are
    statistically nearly impossible to be reached by the running service, you are
    effectively wasting those resources because they can't be used by other services.
    In the long run, this increases both the cost of your infrastructure and the resources
    needed to maintain it, so there is a high incentive to keep the service limited
    by the minimum amount that can run it safely instead of using really high limits.
  prefs: []
  type: TYPE_NORMAL
- en: Orchestration tooling generally prevents overcommiting resources, although there
    has been some progress to support this feature in both Docker Swarm and Kubernetes,
    where you can specify a soft limit (memory request) versus the true limit (memory
    limit). However, even with those parameters, tweaking the RAM setting is a really
    challenging task because you may get either under-utilization or constant rescheduling,
    so all the topics covered here are still very relevant. For more information on
    orchestration-specific handling of overcommiting, I suggest you read the latest
    documentation for your specific orchestration tool.
  prefs: []
  type: TYPE_NORMAL
- en: So, when looking at all the things we must keep in mind, tweaking the limits
    is closer to an art form than anything else because it is almost like a variation
    of the famous bin-packing problem ([https://en.wikipedia.org/wiki/Bin_packing_problem](https://en.wikipedia.org/wiki/Bin_packing_problem)),
    but also adds the statistical component of the service on top of it, because you
    might need to figure out the optimum service availability compared to wasted resources
    due to loose limits.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say we have a service with the following distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: Three physical hosts with 2 GB RAM each (yes, this is really low, but it is
    to demonstrate the issues on smaller scales)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Service 1** (database) that has a memory limit of 1.5 GB, two tasks, and
    has a 1 percent chance of running over the hard limit'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Service 2** (application) that has a memory limit of 0.5 GB, three tasks,
    and has a 5 percent chance of running over the hard limit'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Service 3** (data processing service) that has a memory limit of 0.5 GB,
    three tasks, and has a 5 percent chance of running over the hard limit'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A scheduler may allocate the services in this manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/39e05686-46b7-42f2-bb8b-446d34e67613.png)WARNING! You should always
    have spare capacity on your clusters for rolling service updates, so having the
    configuration similar to the one shown in the diagram would not work well in the
    real world. Generally, this extra capacity is also a fuzzy value, just like RAM
    limits. Generally, my formula for it is the following, but feel free to tweak
    it as needed:'
  prefs: []
  type: TYPE_NORMAL
- en: '`overcapacity = avg(service_sizes) * avg(service_counts) * avg(max_rolling_service_restarts)`'
  prefs: []
  type: TYPE_NORMAL
- en: We will discuss this a bit more further in the text.
  prefs: []
  type: TYPE_NORMAL
- en: What if we take our last example and now say that we should just run with 1
    percent OOM failure rates across the board, increasing our **Service 2** and **Service
    3** memory limit from 0.5 GB to 0.75 GB, without taking into account that maybe
    having higher failure rates on the data processing service and application tasks
    might be acceptable (or even not noticeable if you are using messaging queues)
    to the end users?
  prefs: []
  type: TYPE_NORMAL
- en: 'The new service spread would now look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/c864d401-c786-4c00-9c1d-b6b70a6b8160.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Our new configuration has a massive amount of pretty obvious issues:'
  prefs: []
  type: TYPE_NORMAL
- en: 25 percent reduction in service density. This number should be as high as possible
    to get all the benefits of using microservices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 25 percent reduction in hardware utilization. Effectively, 1/4 of the available
    hardware resources are being wasted in this setup.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Node count has increased by 66 percent. Most cloud providers charge by the number
    of machines you have running assuming they are the same type. By making this change
    you have effectively raised your cloud costs by 66 percent and may need that much
    extra ops support to keep your cluster working.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even though this example has been intentionally rigged to cause the biggest
    impact when tweaked, it should be obvious that slight changes to these limits
    can have massive repercussions on your whole infrastructure. While in real-world
    scenarios this impact will be reduced because there will be larger host machines
    than in the example which will make them better able to stack smaller (relative
    to total capacity) services in the available space, *do not* underestimate the
    cascading effects of increasing service resource allocations.
  prefs: []
  type: TYPE_NORMAL
- en: CPU limits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Just like in our previous section about memory limits for services, `docker
    run` also supports a variety of CPU settings and parameters to tweak the computational
    needs of your services:'
  prefs: []
  type: TYPE_NORMAL
- en: '`-c`/`--cpu-shares`: On a high-load host, all tasks are weighted equally by
    default. Setting this on a task or service (from the default of `1024`) will increase
    or decrease the percentage of CPU utilization that the task can be scheduled for.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--cpu-quota`: This flag sets the number of microseconds that a task or service
    can use the CPU within a default block of time of 100 milliseconds (100,000 microseconds).
    For example, to only allow a maximum of 50% of a single CPU''s core usage to a
    task, you would set this flag to `50000`. For multiple cores, you would need to
    increase this value accordingly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--cpu-period`: This changes the previous quota flag default interval in microseconds
    over which the `cpu-quota` is being evaluated (100 milliseconds/100,000 microseconds)
    and either reduces it or increases it to inversely affect the CPU resource allocation
    to a service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--cpus`: A float value that combines parts of both `cpu-quota` and `cpu-period`
    to limit the number of CPU core allocations to the task. For example, if you only
    want a task to use up to a quarter of a single CPU resource, you would set this
    to `0.25` and it would have the same effect as `--cpu-quota 25000 --cpu-period
    100000`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--cpuset-cpus`: This array flag allows the service to only run on specified
    CPUs indexed from 0\. If you wanted a service to use only CPUs 0 and 3, you could
    use `--cpuset-cpus "0,3"`. This flag also supports entering values as a range
    (that is `1-3`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While it might seem like a lot of options to consider, in most cases you will
    only need to tweak the `--cpu-shares` and `--cpus` flags, but it is possible that
    you will need much more granular control over the resources that they provide.
  prefs: []
  type: TYPE_NORMAL
- en: How about we see what the `--cpu-shares` value can do for us? For this, we need
    to simulate resource contention and in the next example, we will try to do this
    by incrementing an integer variable as many times as we can within a period of
    60 seconds in as many containers as there are CPUs on the machine. The code is
    a bit gnarly, but most of it is to get the CPU to reach resource contention levels
    on all cores.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following to a file called `cpu_shares.sh` (also available on [https://github.com/sgnn7/deploying_with_docker](https://github.com/sgnn7/deploying_with_docker)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will run this code and see the effects of our flag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: While the container with the high `--cpu-share` value didn't get the full increase
    in count that might have been expected, if we ran the benchmark over a longer
    period of time with a tighter CPU-bound loop, the difference would be much more
    drastic. But even in our small example you can see that the last container had
    a distinct advantage over all the other running containers on the machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see how the `--cpus` flag compares, let''s take a look at what it can do
    on an uncontended system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the `--cpus` flag is really good for ensuring that a task will
    not use any more CPU than the specified value even if there is no contention for
    resources on the machine.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that there are a few more options for limiting resource usage for
    containers that are a bit outside of the scope of the general ones that we have
    covered already, but they are mainly for device-specific limitations (such as
    device IOPS). If you are interested in seeing all of the available ways to limit
    resources to a task or a service, you should be able to find them all at [https://docs.docker.com/engine/reference/run/#runtime-constraints-on-resources](https://docs.docker.com/engine/reference/run/#runtime-constraints-on-resources).
  prefs: []
  type: TYPE_NORMAL
- en: Pitfall avoidance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In most small and medium deployments, you will never see the same problems that
    you will start seeing when you scale up beyond them, so this section is to show
    you the most common issues that you will encounter and how to work around them
    in the cleanest way possible. While this list should cover most of the glaring
    issues you will encounter, some of your own will need custom fixes. You shouldn't
    be scared to make those changes because almost all host OS installations are just
    not geared towards the configuration that a high-load multi-container would need.
  prefs: []
  type: TYPE_NORMAL
- en: WARNING! Many of the values and tweaks in this section have been based on personal
    experiences with deploying Docker clusters in the cloud. Depending on your combination
    of cloud provider, OS distribution, and infrastructure-specific configurations,
    the values may not need changing from the defaults, and some may even be detrimental
    to your system if used verbatim without spending some time learning what they
    mean and how to modify them. If you continue reading this section, please use
    the examples only as examples on how to change the values and not as something
    to copy/paste directly into configuration management tooling.
  prefs: []
  type: TYPE_NORMAL
- en: ulimits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`ulimit` settings are little-known settings to most Linux desktop users, but
    they are a really painful and often-encountered issue when working with servers.
    In a nutshell, `ulimit` settings control many aspects around a process'' resource
    usage just like our Docker resource tweaks we covered earlier and they are applied
    to every process and shell that has been started. These limits are almost always
    set on distributions to prevent a stray process from taking down your machine,
    but the numbers have usually been chosen with regular desktop usage in mind, so
    trying to run server-type code on unchanged systems is bound to hit at least the
    open file limit, and possibly some other limits.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use `ulimit -a` to see what our current (also called **soft**) settings
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, there are only a few things set here, but there is one that
    stands out: our "open files" limit (`1024`) is fine for general applications,
    but if we run many services that handle a large number of open files (such as
    a decent amount of Docker containers), this value must be changed or you will
    hit errors and your services will effectively die.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can change this value for your current shell with `ulimit -S <flag> <value>`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: But what if we try to set this to something really high?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we have now encountered the hard limit imposed by the system. This limit
    is something that will need to be changed at the system level if we want to modify
    it beyond those values. We can check what these hard limits are with `ulimit -H
    -a`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: So, if we want to increase our open files number beyond `4096`, we really need
    to change the system-level settings. Also, even if the soft limit of `4086` is
    fine with us, the setting is only for our own shell and its child processes, so
    it won't affect any other service or process on the system.
  prefs: []
  type: TYPE_NORMAL
- en: If you really wanted to, you actually can change the `ulimit` settings of an
    already-running process with `prlimit` from the `util-linux` package, but this
    method of adjusting the values is discouraged because the settings do not persist
    during process restarts and are thus pretty useless for that purpose. With that
    said, if you want to find out whether your `ulimit` settings have been applied
    to a service that is already running, this CLI tool is invaluable, so don't be
    afraid to use it in those cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'To change this setting, you need to do a combination of options that is dependent
    on your distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a security limits configuration file. You can do this rather simply
    by adding a few lines to something like `/etc/security/limits.d/90-ulimit-open-files-increase.conf`.
    The following example sets the open file soft limit on `root` and then on all
    other accounts (`*` does not apply to the `root` account) to `65536`. You should
    find out what the appropriate value is for your system ahead of time:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the `pam_limits` module to **Pluggable Authentication Module** (**PAM**).
    This will, in turn, affect all user sessions with the previous ulimit change setting
    because some distributions do not have it included otherwise your changes might
    not persist. Add the following to `/etc/pam.d/common-session`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, on some distributions, you can directly add the setting to the
    affected service definition in `systemd` in an override file:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Overriding `systemd` services is a somewhat lengthy and distracting topic for
    this section, but it is a very common strategy for tweaking third-party services
    running on cluster deployments with that init system, so it is a very valuable
    skill to have. If you would like to know more about this topic, you can find a
    condensed version of the process at [https://askubuntu.com/a/659268](https://askubuntu.com/a/659268),
    and if you want the detailed version the upstream documentation can be found at
    [https://www.freedesktop.org/software/systemd/man/systemd.service.html](https://www.freedesktop.org/software/systemd/man/systemd.service.html).CAUTION!
    In the first example, we used the `*` wildcard, which affects all accounts on
    the machine. Generally, you want to isolate this setting to only the affected
    service accounts, if possible, for security reasons. We also used `root` because
    root values are specifically set by name in some distributions, which overrides
    the `*` wildcard setting due to the higher specificity. If you want to learn more
    about limits, you can find more information on these settings at [https://linux.die.net/man/5/limits.conf](https://linux.die.net/man/5/limits.conf).
  prefs: []
  type: TYPE_NORMAL
- en: Max file descriptors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the same way that we have a maximum open file limit for sessions and processes,
    the kernel itself has a limit for the maximum open file descriptors across the
    whole system. If this limit is reached, no other files will be able to be opened,
    and thus this needs tweaking on machines that may have a large number of files
    open at any one time.
  prefs: []
  type: TYPE_NORMAL
- en: 'This value is part of the kernel parameters and as such can be seen with `sysctl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: While on this machine the value seems reasonable, I have seen a few older distributions
    with a surprisingly low value that will get easily hit with errors if you are
    running a number of containers on the system.
  prefs: []
  type: TYPE_NORMAL
- en: Most kernel configuration settings we discuss here and later in this chapter
    can be temporarily changed with `sysctl -w <key>="<value>"`. However, since those
    values are reset back to defaults on each reboot, they usually are of no long-term
    use for us and are not going to be covered here, but keep in mind that you can
    use such techniques if you need to debug a live system or apply a temporary time-sensitive
    fix.
  prefs: []
  type: TYPE_NORMAL
- en: 'To change this to a value that will persist across reboots, we will need to
    add the following to the `/etc/sysctl.d` folder (that is,  `/etc/sysctl.d/10-file-descriptors-increase.conf`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: After this change, reboot, and you should now be able to open up to 1 million
    file handles on the machine!
  prefs: []
  type: TYPE_NORMAL
- en: Socket buffers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To increase performance, it is usually highly advantageous to increase the size
    of the socket buffers because they are no longer doing the work of a single machine
    but the work of as many Docker containers as you have running on top of regular
    machine connectivity. For this, there are a few settings that you should probably
    set to make sure that the socket buffers are not struggling to keep up with all
    the traffic passing through them. At the time of writing this book, most of these
    default buffer settings are generally pretty tiny when the machine starts (200
    KB in a few machines that I've checked) and they are supposed to be dynamically
    scaled, but you can force them to be much larger from the start.
  prefs: []
  type: TYPE_NORMAL
- en: 'On an Ubuntu LTS 16.04 installation, the following are the default ones for
    the buffer settings (though yours may vary):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We will dial these values up to some sensible defaults by adding the following
    to `/etc/sysctl.d/10-socket-buffers.conf`, but be sure to use values that make
    sense in your environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: By increasing these values, our buffers start large and should be able to handle
    quite a bit of traffic with much better throughput, which is what we want in a
    clustering environment.
  prefs: []
  type: TYPE_NORMAL
- en: Ephemeral ports
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you aren't familiar with ephemeral ports, they are the port numbers that
    all outbound connections get assigned if the originating port is not explicitly
    specified on the connection, which is the vast majority of them. For example,
    if you do any kind of outbound HTTP request with almost every client library,
    you will most likely have one of these ephemeral ports assigned as the return
    communication port for your connection.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see some sample ephemeral port usage on your machine, you can use `netstat`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: As you develop systems with multiple services with numerous outbound connections
    (which is practically mandatory when working with Docker services), you may notice
    that there are limits on the number of ports you are allowed to use and are likely
    to find that these ports may overlap with the ranges that some of your internal
    Docker services are using, causing intermittent and often annoying connectivity
    issues. In order to fix these issues, changes need to be made to the ephemeral
    port range.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since these are also kernel settings, we can see what our current ranges are
    with `sysctl`, just like we did in a couple of earlier examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: You can see that our range is in the upper half of the port allocations, but
    any service that may start listening within that range could be in trouble. It
    is also possible that we may need more than 28,000 ports.
  prefs: []
  type: TYPE_NORMAL
- en: You may be curious how you get or set the `ipv6` settings for this parameter,
    but luckily (at least for now) this same setting key is used for both `ipv4` and
    `ipv6` ephemeral port ranges. At some point, this setting name may change, but
    I think we are at least a couple of years away from that.
  prefs: []
  type: TYPE_NORMAL
- en: 'To change this value, we can either use `sysctl -w` for a temporary change
    or `sysctl.d` for a permanent change:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: With this change, we have effectively increased the number of outbound connections
    we can support by over 30%, but we could have just as easily used the same setting
    to ensure that ephemeral ports do not collide with other running services.
  prefs: []
  type: TYPE_NORMAL
- en: Netfilter tweaks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sadly, the settings we have seen so far are not the only things that need tweaking
    with increased network connections to your server. As you increase the load on
    your server, you may also begin to see `nf_conntrack: table full` errors in your
    `dmesg` and/or kernel logs. For those unfamiliar with `netfilter`, it is a kernel
    module that tracks all **Network Address Translation** (**NAT**) sessions in a
    hashed table that adds any new connections to it and clears them after they are
    closed and a predefined timeout is reached, so as you increase the connection
    volume from and to a single machine, you will most likely find that the majority
    of these related settings are defaulted rather conservatively and are in need
    of tweaking (though your distribution may vary--make sure to verify yours!):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Quite a few of these can be changed, but the usual suspects for errors that
    need tweaking are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`net.netfilter.nf_conntrack_buckets`: Controls the size of the hash table for
    the connections. Increasing this is advisable, although it can be substituted
    with a more aggressive timeout. Note that this cannot be set with regular `sysctl.d`
    settings, but instead needs to be set with a kernel module parameter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`net.netfilter.nf_conntrack_max`: The number of entries to hold. By default,
    this is four times the value of the previous entry.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`net.netfilter.nf_conntrack_tcp_timeout_established`: This keeps the mapping
    for an open connection for up to five days (!). This is generally almost mandatory
    to reduce in order to not overflow your connection tracking table, but don''t
    forget that it needs to be above the TCP `keepalive` timeout or you will get unexpected
    connection breaks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To apply the last two settings, you need to add the following to `/etc/sysctl.d/10-conntrack.conf`
    and adjust the values for your own infrastructure configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: netfilter is a massively complex topic to cover in a small section, so reading
    up on its impacts and configuration settings is highly recommended before changing
    these numbers. To get an idea of each of the settings, you can visit [https://www.kernel.org/doc/Documentation/networking/nf_conntrack-sysctl.txt](https://www.kernel.org/doc/Documentation/networking/nf_conntrack-sysctl.txt)
    and read up about it.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a bucket count, you need to directly change the `nf_conntrack` `hashsize`
    kernel module parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, to ensure that the right order is followed when loading the netfilter
    module so these values persist correctly, you will probably also need to add the
    following to the end of `/etc/modules`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: If everything was done correctly, your next restart should have all of the netfilter
    settings we talked about set.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-service containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Multi-service containers are a particularly tricky topic to broach, as the whole
    notion and recommended use of Docker is that you are only running single-process
    services within the container. Because of that, there is quite a bit of implicit
    pressure not to cover this topic because it can easily be misused and abused by
    developers who do not understand the reasons why this practice is strongly discouraged.
  prefs: []
  type: TYPE_NORMAL
- en: However, with that said and out of the way, there will be times where you will
    need to run multiple processes in a tight logical grouping where a multi-container
    solution would not make sense or it would be overly kludgey, which is why this
    topic is still important to cover. Having said all that, I cannot stress enough
    that you should only use this type of service collocation as a last resort.
  prefs: []
  type: TYPE_NORMAL
- en: Before we even write a single line of code, we must discuss an architectural
    issue with multiple processes running within the same container, which is called
    the `PID 1` problem. The crux of this issue is that Docker containers run in an
    isolated environment in which they do not get help from the host's `init` process
    in reaping orphaned child processes. Consider an example process `Parent Process`,
    that is a basic executable that starts another process called `Child Process`,
    but as some point after that, if the associated `Parent Process` exits or is killed
    you will be left with the zombie `Child Process` loitering around in your container
    since `Parent Process` is gone and there is no other orphan reaping process running
    within the container sandbox. If the container exits, then the zombie processes
    will get cleaned up because they are all wrapped in a namespace, but for long-running
    tasks this can present a serious problem for running multiple processes inside
    a single image.
  prefs: []
  type: TYPE_NORMAL
- en: Terminology here might be confusing, but what was meant in simple terms is that
    every process is supposed be removed (also known as `reaped`) from the process
    table after it exits, either by the parent process or some other designated process
    (usually `init`) in the hierarchy that will take ownership of of it in order to
    finalize it. A process that does not have a running parent process in this context
    is called an orphan process.
  prefs: []
  type: TYPE_NORMAL
- en: Some tools have the ability to reap these zombie processes (such as Bash and
    a few other shells), but even they aren't good enough init processes for our containers
    because they do not pass signals such as `SIGKILL`, `SIGINT`, and others to child
    processes, so stopping the container or pressing things such as *Ctrl* + *C* in
    the Terminal are of no use and will not terminate the container. If you really
    want to run multiple processes inside the container, your launching process must
    do orphan reaping and signal passing to children. Since we don't want to use the
    full init system like `systemd` from the container, there are a couple of alternatives
    here, but in the recent versions of Docker we now have the `--init` flag, which
    can run our containers with a real init runner process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see this in action and try to exit a program where the starting process
    is `bash`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This time, we''ll run our container with the `--init` flag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, `--init` was able to take our signal and pass it to all the
    listening children processes, and it works well as an orphan process reaper, though
    the latter is really hard to show in a basic container. With this flag and its
    functionality, you should now be able to run multiple processes with either a
    shell such as Bash or upgrade to a full process management tool such as `supervisord`
    ([http://supervisord.org/](http://supervisord.org/)) without any issues.
  prefs: []
  type: TYPE_NORMAL
- en: Zero-downtime deployments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With every cluster deployment, you will at some point need to think about code
    redeployment while minimizing the impact on your users. With small deployments,
    it is feasible that you might have a maintenance period in which you turn off
    everything, rebuild the new images, and restart the services, but this style of
    deployment is really not the way that medium and large clusters should be managed
    because you want to minimize any and all direct work needed to maintain the cluster.
    In fact, even for small clusters, handling code and configuration upgrades in
    a seamless manner can be invaluable for increased productivity.
  prefs: []
  type: TYPE_NORMAL
- en: Rolling service restarts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If the new service code does not change the fundamental way that it interacts
    with other services (inputs and outputs), often the only thing that is needed
    is a rebuild (or replacement) of the container image that is then placed into
    the Docker registry, and then the service is restarted in an orderly and staggered
    way. By staggering the restarts, there is always at least one task that can handle
    the service request available, and from an external point of view, this changeover
    should be completely seamless. Most orchestration tooling will do this automatically
    for you if you change or update any settings for a service, but since they are
    very implementation-specific we will focus on Docker Swarm for our examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, it should be simple enough to do the same thing with your own
    code changes without any downtime!
  prefs: []
  type: TYPE_NORMAL
- en: If you want to be able to restart multiple tasks instead of one at a time, Docker
    Swarm has an `--update-parallelism <count>` flag as well that can be set on a
    service. When using this flag, `--update-delay` is still observed but instead
    of a single task being restarted, they are done in batches of `<count>` size.
  prefs: []
  type: TYPE_NORMAL
- en: Blue-green deployments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Rolling restarts are nice, but sometimes the changes that you need to apply
    are on the hosts themselves and will need to be done to every Docker Engine node
    in the cluster, for example, if you need to upgrade to a newer orchestration version
    or to upgrade the OS release version. In these cases, the generally accepted way
    of doing this without a large team for support is usually by something called
    **blue-green deployments**. It starts by deploying a secondary cluster in parallel
    to the currently running one, possibly tied to the same data store backend, and
    then at the most opportune time switching the entry routing to point to the new
    cluster. Once all the processing on the original cluster has died down it is deleted,
    and the new cluster becomes the main processing group. If done properly, the impact
    on the users should be imperceptible and the whole underlying infrastructure has
    been changed in the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process starts with the creation of the secondary cluster. At that point
    there is no effective change other than testing that the new cluster behaves as
    expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/d809216d-0230-418c-8278-8645a5c6188d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After the secondary cluster is operational, the router swaps the endpoints
    and the processing continues on the new cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/39ff5e17-667b-4762-acd3-0b732bfffa5d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'With the swap made, after all the processing is done, the original cluster
    is decommissioned (or left as an emergency backup):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/4da05ec7-9bda-4c89-b27e-13b53586ab68.png)'
  prefs: []
  type: TYPE_IMG
- en: But the application of this deployment pattern on full clusters is not the only
    use for it--in some cases, it is possible to do this at the service level within
    the same cluster, using the same pattern to swap in a newer component, but there
    is a better system for that, which we will cover next.
  prefs: []
  type: TYPE_NORMAL
- en: Blue-turquoise-green deployments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With deployments of code, things get a bit trickier because changing APIs on
    either the input or output sides or the database schema can wreak havoc on a cluster
    with interspersed versions of code. To get around this problem, there is a modified
    blue-green deployment pattern called **blue-turquoise-green deployment** where
    the code is attempted to be kept compatible with all running versions until the
    new code is deployed, after which the service is again updated by removing the
    compat code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process here is pretty simple:'
  prefs: []
  type: TYPE_NORMAL
- en: The service that uses API version `x` is replaced with a new version of the
    service that supports both API version `x` and API version `(x+1)` in a rolling
    fashion. This provides zero downtime from the user's perspective, but creates
    a new service that has the newer API support.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After everything is updated, the service that has the old API version `x` is
    removed from the codebase.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Another rolling restart is done on the service to remove traces of the deprecated
    API so only API version `(x+1)` support is left.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This approach is extremely valuable when the services you are using need to
    be persistently available, and in many cases you could easily replace the API
    version with the messaging queue format, if your cluster is based on queues. The
    transitions are smooth, but there is overhead in needing to twice modify the service
    compared to a single time with a hard-swap, but it is a decent trade-off. This
    approach is also extremely valuable when the services in use deal with a database
    that might need a migration, so you should probably use this approach when others
    are not good enough.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered various tools and techniques that you will need
    as you increase your infrastructure scale beyond the simple prototypes. By now
    we should have learned how to limit service access to host's resources, handle
    the most common pitfalls with ease, run multiple services in a single container,
    and handle zero-downtime deployments and configuration changes.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will spend time working on deploying our own mini version
    of **Platform-as-a-Service **(**PAAS**)  using many of the things we have learned
    so far.
  prefs: []
  type: TYPE_NORMAL
