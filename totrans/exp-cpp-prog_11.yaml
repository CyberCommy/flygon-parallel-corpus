- en: Multithreading Implementation on the Processor and OS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The foundation of any multithreaded application is formed by the implementation
    of the required features by the hardware of the processor, as well as by the way
    these features are translated into an API for use by applications by the operating
    system. An understanding of this foundation is crucial for developing an intuitive
    understanding of how to best implement a multithreaded application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Topics covered in this chapter include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: How operating systems changed to use these hardware features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concepts behind memory safety and memory models in various architectures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Differences between various process and threading models by OSes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concurrency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to POSIX pthreads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unix, Linux, and macOS are largely compliant with the POSIX standard. **Portable
    Operating System Interface for Unix** (**POSIX**) is an IEEE standard that helps
    all Unix and Unix-like operating systems, that is Linux and macOS, communicate
    with a single interface.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, POSIX is also supported by POSIX-compliant tools--Cygwin, MinGW,
    and Windows subsystem for Linux--that provide a pseudo-Unix-like runtime and development
    environment on Windows platforms.
  prefs: []
  type: TYPE_NORMAL
- en: Note that pthread is a POSIX-compliant C library used in Unix, Linux, and macOS.
    Starting from C++11, C++ natively supports threads via the C++ thread support
    library and concurrent library. In this chapter, we will understand how to use
    pthreads, thread support, and concurrency library in an object-oriented fashion.
    Also, we will discuss the merits of using native C++ thread support and concurrency
    library as opposed to using POSIX pthreads or other third-party threading frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: Creating threads with the pthreads library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s get straight to business. You need to understand the pthread APIs we''ll
    discuss to get your hands dirty. To start with, this function is used to create
    a new thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The following table briefly explains the arguments used in the preceding function:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **API arguments** | **Comments** |'
  prefs: []
  type: TYPE_TB
- en: '| `pthread_t *thread` | Thread handle pointer |'
  prefs: []
  type: TYPE_TB
- en: '| `pthread_attr_t *attr` | Thread attribute |'
  prefs: []
  type: TYPE_TB
- en: '| `void *(*start_routine)(void*)` | Thread function pointer |'
  prefs: []
  type: TYPE_TB
- en: '| `void * arg` | Thread argument |'
  prefs: []
  type: TYPE_TB
- en: 'This function blocks the caller thread until the thread passed in the first
    argument exits, as shown in the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The following table briefly describes the arguments in the preceding function:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **API arguments** | **Comments** |'
  prefs: []
  type: TYPE_TB
- en: '| `pthread_t thread` | Thread handle |'
  prefs: []
  type: TYPE_TB
- en: '| `void **retval` | Output parameter that indicates the exit code of the thread
    procedure |'
  prefs: []
  type: TYPE_TB
- en: 'The ensuing function should be used within the thread context. Here, `retval` is
    the exit code of the thread that indicates the exit code of the thread that invoked
    this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s the argument used in this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **API argument** | **Comment** |'
  prefs: []
  type: TYPE_TB
- en: '| `void *retval` | The exit code of the thread procedure |'
  prefs: []
  type: TYPE_TB
- en: 'The following function returns the thread ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s write our first multithreaded application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: How to compile and run
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The program can be compiled with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we need to link the POSIX `pthread` library dynamically.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following screenshot and visualize the output of the multithreaded
    program:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ffc7c770-a884-446e-bb63-8f5ef4b1485e.png)'
  prefs: []
  type: TYPE_IMG
- en: The code that is written in ThreadProc runs within the thread context. The preceding
    program has a total of four threads, including the main thread. I had blocked
    the main thread with `pthread_join` to force it to wait for the other three threads
    to complete their tasks first, failing which the main thread would have exited before
    them. When the main thread exits, the application exits too, which ends up prematurely
    destroying newly created threads.
  prefs: []
  type: TYPE_NORMAL
- en: Though we created `thread1`, `thread2`, and `thread3` in the respective sequence,
    there is no guarantee that they will be started in the exact same sequence they
    were created in.
  prefs: []
  type: TYPE_NORMAL
- en: The operating system scheduler decides the sequence in which the threads must
    be started, based on the algorithm used by the operating system scheduler. Interestingly,
    the sequence in which the threads get started might vary at different runs in
    the same system.
  prefs: []
  type: TYPE_NORMAL
- en: Does C++ support threads natively?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Starting from C++11, C++ does support threads natively, and it is generally
    referred to as the C++ thread support library. The C++ thread support library
    provides an abstraction over the POSIX pthreads C library. Over time, C++ native
    thread support has improved to a greater extent.
  prefs: []
  type: TYPE_NORMAL
- en: I highly recommend you make use of the C++ native thread over pthreads. The
    C++ thread support library is supported on all platforms as it is officially part
    of standard C++ as opposed to the POSIX `pthread` library, which is only supported
    on Unix, Linux, and macOS but not directly on Windows.
  prefs: []
  type: TYPE_NORMAL
- en: The best part is thread support has matured to a new level in C++17, and it
    is poised to reach the next level in C++20\. Hence, it is a good idea to consider
    using the C++ thread support library in your projects.
  prefs: []
  type: TYPE_NORMAL
- en: Defining processes and threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Essentially, to the **operating system** (**OS**), a process consists of one
    or more threads, each thread processing its own state and variables. One would
    regard this as a hierarchical configuration, with the OS as the foundation, providing
    support for the running of (user) processes. Each of these processes then consists
    of one or more threads. Communication between processes is handled by **inter-process
    communication** (**IPC**), which is provided by the operating system.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a graphical view, this looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e2a11c2f-0b17-424f-ab3e-4db6ff9bdc62.png)'
  prefs: []
  type: TYPE_IMG
- en: Each process within the OS has its own state, with each thread in a process
    having its own state as well as the relative to the other threads within that
    same process. While IPC allows processes to communicate with each other, threads
    can communicate with other threads within the process in a variety of ways, which
    we'll explore in more depth in upcoming chapters. This generally involves some
    kind of shared memory between threads.
  prefs: []
  type: TYPE_NORMAL
- en: 'An application is loaded from binary data in a specific executable format such
    as, for example, **Executable and Linkable Format** (**ELF**) which is generally
    used on Linux and many other operating systems. With ELF binaries, the following
    number of sections should always be present:'
  prefs: []
  type: TYPE_NORMAL
- en: '`.bss`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.data`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.rodata`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.text`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `.bss` section is, essentially, allocated with uninitialized memory including
    empty arrays which thus do not take up any space in the binary, as it makes no
    sense to store rows of pure zeroes in the executable. Similarly, there is the
    `.data` section with initialized data. This contains global tables, variables,
    and the like. Finally, the `.rodata` section is like `.data`, but it is, as the
    name suggests, read-only. It contains things such as hardcoded strings.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `.text` section, we find the actual application instructions (code)
    which will be executed by the processor. The whole of this will get loaded by
    the operating system, thus creating a process. The layout of such a process looks
    like the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/078ec7eb-400d-41cb-85af-c612e8612b9b.png)'
  prefs: []
  type: TYPE_IMG
- en: This is what a process looks like when launched from an ELF-format binary, though
    the final format in memory is roughly the same in basically any OS, including
    for a Windows process launched from a PE-format binary. Each of the sections in
    the binary are loaded into their respective sections, with the BSS section allocated
    to the specified size. The `.text` section is loaded along with the other sections,
    and its initial instruction is executed once this is done, which starts the process.
  prefs: []
  type: TYPE_NORMAL
- en: In system languages such as C++, one can see how variables and other program
    state information within such a process are stored both on the stack (variables
    exist within the scope) and heap (using the new operator). The stack is a section
    of memory (one allocated per thread), the size of which depends on the operating
    system and its configuration. One can generally also set the stack size programmatically
    when creating a new thread.
  prefs: []
  type: TYPE_NORMAL
- en: In an operating system, a process consists of a block of memory addresses, the
    size of which is constant and limited by the size of its memory pointers. For
    a 32-bit OS, this would limit this block to 4 GB. Within this virtual memory space,
    the OS allocates a basic stack and heap, both of which can grow until all memory
    addresses have been exhausted, and further attempts by the process to allocate
    more memory will be denied.
  prefs: []
  type: TYPE_NORMAL
- en: The stack is a concept both for the operating system and for the hardware. In
    essence, it's a collection (stack) of so-called stack frames, each of which is
    composed of variables, instructions, and other data relevant to the execution
    frame of a task.
  prefs: []
  type: TYPE_NORMAL
- en: In hardware terms, the stack is part of the task (x86) or process state (ARM),
    which is how the processor defines an execution instance (program or thread).
    This hardware-defined entity contains the entire state of a singular thread of
    execution. See the following sections for further details on this.
  prefs: []
  type: TYPE_NORMAL
- en: Tasks in x86 (32-bit and 64-bit)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A task is defined as follows in the Intel IA-32 System Programming guide, Volume
    3A:'
  prefs: []
  type: TYPE_NORMAL
- en: '*"A task is a unit of work that a processor can dispatch, execute, and suspend.
    It can be used to execute a program, a task or process, an operating-system service
    utility, an interrupt or exception handler, or a kernel or executive utility."*'
  prefs: []
  type: TYPE_NORMAL
- en: '*"The IA-32 architecture provides a mechanism for saving the state of a task,
    for dispatching tasks for execution, and for switching from one task to another.
    When operating in protected mode, all processor execution takes place from within
    a task. Even simple systems must define at least one task. More complex systems
    can use the processor''s task management facilities to support multitasking applications."*'
  prefs: []
  type: TYPE_NORMAL
- en: This excerpt from the IA-32 (Intel x86) manual summarizes how the hardware supports
    and implements support for operating systems, processes, and the switching between
    these processes.
  prefs: []
  type: TYPE_NORMAL
- en: It's important to realize here that, to the processor, there's no such thing
    as a process or thread. All it knows of are threads of execution, defined as a
    series of instructions. These instructions are loaded into memory somewhere, and
    the current position in these instructions is kept track of along with the variable
    data (variables) being created, as the application is executed within the data
    section of the process.
  prefs: []
  type: TYPE_NORMAL
- en: Each task also runs within a hardware-defined protection ring, with the OS's
    tasks generally running on ring 0, and user tasks on ring 3\. Rings 1 and 2 are
    rarely used except for specific use cases with modern OSes on the x86 architecture.
    These rings are privilege-levels enforced by the hardware and allow for example
    for the strict separation of kernel and user-level tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The task structure for both 32-bit and 64-bit tasks are quite similar in concept.
    The official name for it is the **Task State Structure** (**TSS**). It has the
    following layout for 32-bit x86 CPUs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fdb56c2a-af43-4d41-b70a-c98b2b018900.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Following are the firlds:'
  prefs: []
  type: TYPE_NORMAL
- en: '**SS0**: The first stack segment selector field'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ESP0**: The first SP field'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For 64-bit x86_64 CPUs, the TSS layout looks somewhat different, since hardware-based
    task switching is not supported in this mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/94cc164c-5fd6-4eda-b974-1f7ba35c245c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we have similar relevant fields, just with different names:'
  prefs: []
  type: TYPE_NORMAL
- en: '**RSPn**: SP for privilege levels 0 through 2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ISTn**: Interrupt stack table pointers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even though on x86 in 32-bit mode, the CPU supports hardware-based switching
    between tasks, most operating systems will use just a single TSS structure per
    CPU regardless of the mode, and do the actual switching between tasks in software.
    This is partially due to efficiency reasons (swapping out only pointers which
    change), partially due to features which are only possible this way, such as measuring
    CPU time used by a process/thread, and to adjust the priority of a thread or process.
    Doing it in software also simplifies the portability of code between 64-bit and
    32-bit systems, since the former do not support hardware-based task switching.
  prefs: []
  type: TYPE_NORMAL
- en: During a software-based task switch (usually via an interrupt), the ESP/RSP,
    and so on are stored in memory and replaced with the values for the next scheduled
    task. This means that once execution resumes, the TSS structure will now have
    the **Stack Pointer** (**SP**), segment pointer(s), register contents, and all
    other details of the new task.
  prefs: []
  type: TYPE_NORMAL
- en: The source of the interrupt can be based in hardware or software. A hardware
    interrupt is usually used by devices to signal to the CPU that they require attention
    by the OS. The act of calling a hardware interrupt is called an Interrupt Request,
    or IRQ.
  prefs: []
  type: TYPE_NORMAL
- en: A software interrupt can be due to an exceptional condition in the CPU itself,
    or as a feature of the CPU's instruction set. The action of switching tasks by
    the OS's kernel is also performed by triggering a software interrupt.
  prefs: []
  type: TYPE_NORMAL
- en: Process state in ARM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In ARM architectures, applications usually run in the unprivileged **Exception
    Level 0** (**EL0**) level, which is comparable to ring 3 on x86 architectures,
    and the OS kernel in EL1\. The ARMv7 (AArch32, 32-bit) architecture has the SP
    in the general purpose register 13\. For ARMv8 (AArch64, 64-bit), a dedicated
    SP register is implemented for each exception level: `SP_EL0`, `SP_EL1`, and so
    on.'
  prefs: []
  type: TYPE_NORMAL
- en: For task state, the ARM architecture uses **Program State Register** (**PSR**)
    instances for the **Current Program State Register** (**CPSR**) or the **Saved
    Program State Register** (**SPSR**) program state's registers. The PSR is part
    of the **Process State** (**PSTATE**), which is an abstraction of the process
    state information.
  prefs: []
  type: TYPE_NORMAL
- en: 'While the ARM architecture is significantly different from the x86 architecture,
    when using software-based task switching, the basic principle does not change:
    save the current task''s SP, register state, and put the next task''s detail in
    there instead before resuming processing.'
  prefs: []
  type: TYPE_NORMAL
- en: The stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we saw in the preceding sections, the stack together with the CPU registers
    define a task. As mentioned earlier, this stack consists of stack frames, each
    of which defines the (local) variables, parameters, data, and instructions for
    that particular instance of task execution. Of note is that although the stack
    and stack frames are primarily a software concept, it is an essential feature
    of any modern OS, with hardware support in many CPU instruction sets. Graphically,
    it can be be visualized like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/88a7ab24-ba2b-42c1-97b3-78275eccd380.png)'
  prefs: []
  type: TYPE_IMG
- en: The SP (ESP on x86) points to the top of the stack, with another pointer (**Extended
    Base Pointer** (**EBP**) for x86). Each frame contains a reference to the preceding
    frame (caller return address), as set by the OS.
  prefs: []
  type: TYPE_NORMAL
- en: When using a debugger with one's C++ application, this is basically what one
    sees when requesting the backtrack--the individual frames of the stack showing
    the initial stack frame leading up until the current frame. Here, one can examine
    each individual frame's details.
  prefs: []
  type: TYPE_NORMAL
- en: Defining multithreading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Over the past decades, a lot of different terms related to the way tasks are
    processed by a computer have been coined and come into common use. Many of these
    are also used interchangeably, correctly or not. An example of this is multithreading
    in comparison with multiprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the latter means running one task per processor in a system with multiple
    physical processors, while the former means running multiple tasks on a singular
    processor simultaneously, thus giving the illusion that they are all being executed
    simultaneously:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d8f34726-99a5-498c-a2fe-e7c50f6de467.png)'
  prefs: []
  type: TYPE_IMG
- en: Another interesting distinction between multiprocessing and multitasking is
    that the latter uses time-slices in order to run multiple threads on a single
    processor core. This is different from multithreading in the sense that in a multitasking
    system, no tasks will ever run in a concurrent fashion on the same CPU core, though
    tasks can still be interrupted.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of a process and a shared memory space between the threads contained
    within the said process is at the very core of multithreaded systems from a software
    perspective. Though the hardware is often not aware of this--seeing just a single
    task to the OS. However, such a multithreaded process contains two or many more
    threads. Each of these threads then perform its own series of tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In other implementations, such as Intel's **Hyper-Threading** (**HT**) on x86
    processors, this multithreading is implemented in the hardware itself, where it's
    commonly referred to as SMT (see the section *Simultaneous multithreading (SMT)*
    for details). When HT is enabled, each physical CPU core is presented to the OS
    as being two cores. The hardware itself will then attempt to execute the tasks
    assigned to these so-called virtual cores concurrently, scheduling operations
    which can use different elements of a processing core at the same time. In practice,
    this can give a noticeable boost in performance without the operating system or
    application requiring any type of optimization.
  prefs: []
  type: TYPE_NORMAL
- en: The OS can of course still do its own scheduling to further optimize the execution
    of task, since the hardware is not aware of many details about the instructions
    it is executing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having HT enabled looks like this in the visual format:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6f9d96fd-abee-4e3b-9ab3-08c46e683f2f.png)'
  prefs: []
  type: TYPE_IMG
- en: In this preceding graphic, we see the instructions of four different tasks in
    memory (RAM). Out of these, two tasks (threads) are being executed simultaneously,
    with the CPU's scheduler (in the frontend) attempting to schedule the instructions
    so that as many instructions as possible can be executed in parallel. Where this
    is not possible, so-called pipeline bubbles (in white) appear where the execution
    hardware is idle.
  prefs: []
  type: TYPE_NORMAL
- en: Together with internal CPU optimizations, this leads to a very high throughput
    of instructions, also called **Instructions Per Second** (**IPC**). Instead of
    the GHz rating of a CPU, this IPC number is generally far more significant for
    determining the sheer performance of a CPU.
  prefs: []
  type: TYPE_NORMAL
- en: Flynn's taxonomy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Different types of computer architecture are classified using a system which
    was first proposed by Michael J. Flynn, back in 1966\. This classification system
    knows four categories, defining the capabilities of the processing hardware in
    terms of the number of input and output streams:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Single Instruction, Single Data** (**SISD**): A single instruction is fetched
    to operate on a single data stream. This is the traditional model for CPUs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Single Instruction, Multiple Data** (**SIMD**): With this model, a single
    instruction operates on multiple data streams in parallel. This is what vector
    processors such as **graphics processing units** (**GPUs**) use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multiple Instruction, Single Data** (**MISD**): This model is most commonly
    used for redundant systems, whereby the same operation is performed on the same
    data by different processing units, validating the results at the end to detect
    hardware failure. This is commonly used by avionics systems and similar.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multiple Instruction, Multiple Data** (**MIMD**): For this model, a multiprocessing
    system lends itself very well. Multiple threads across multiple processors process
    multiple streams of data. These threads are not identical, as is the case with
    SIMD.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An important thing to note with these categories is that they are all defined
    in terms of multiprocessing, meaning that they refer to the intrinsic capabilities
    of the hardware. Using software techniques, virtually any method can be approximated
    on even a regular SISD-style architecture. This is, however, part of multithreading.
  prefs: []
  type: TYPE_NORMAL
- en: Symmetric versus asymmetric multiprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Over the past decades, many systems were created which contained multiple processing
    units. These can be broadly divided into **Symmetric Multiprocessing** (**SMP**)
    and **Asymmetric Multiprocessing** (**AMP**) systems.
  prefs: []
  type: TYPE_NORMAL
- en: AMP's main defining feature is that a second processor is attached as a peripheral
    to the primary CPU. This means that it cannot run control software, but only user
    applications. This approach has also been used to connect CPUs using a different
    architecture to allow one to, for example, run x86 applications on an Amiga, 68k-based
    system.
  prefs: []
  type: TYPE_NORMAL
- en: 'With an SMP system, each of the CPUs are peers having access to the same hardware
    resources, and set up in a cooperative fashion. Initially, SMP systems involved
    multiple physical CPUs, but later, multiple processor cores got integrated on
    a single CPU die:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/58c33dca-c958-4806-beb3-15969b592057.png)'
  prefs: []
  type: TYPE_IMG
- en: With the proliferation of multi-core CPUs, SMP is the most common type of processing
    outside of embedded development, where uniprocessing (single core, single processor)
    is still very common.
  prefs: []
  type: TYPE_NORMAL
- en: Technically, the sound, network, and graphic processors in a system can be considered
    to be asymmetric processors related to the CPU. With an increase in **General
    Purpose GPU** (**GPGPU**) processing, AMP is becoming more relevant.
  prefs: []
  type: TYPE_NORMAL
- en: Loosely and tightly coupled multiprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A multiprocessing system does not necessarily have to be implemented within
    a single system, but can also consist of multiple systems which are connected
    in a network. Such a cluster is then called a loosely coupled multiprocessing
    system. We cover distributing computing in Chapter 9, *Multithreading with Distributed
    Computing*.
  prefs: []
  type: TYPE_NORMAL
- en: This is in contrast with a tightly coupled multiprocessing system, whereby the
    system is integrated on a single **printed circuit board** (**PCB**), using the
    same low-level, high-speed bus or similar.
  prefs: []
  type: TYPE_NORMAL
- en: Combining multiprocessing with multithreading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Virtually any modern system combines multiprocessing with multithreading, courtesy
    of multi-core CPUs, which combine two or more processing cores on a single processor
    die. What this means for an operating system is that it has to schedule tasks
    both across multiple processing cores while also scheduling them on specific cores
    in order to extract maximum performance.
  prefs: []
  type: TYPE_NORMAL
- en: This is the area of task schedulers, which we will look at in a moment. Suffice
    it to say that this is a topic worthy of its own book.
  prefs: []
  type: TYPE_NORMAL
- en: Multithreading types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like multiprocessing, there is not a single implementation, but two main ones.
    The main distinction between these is the maximum number of threads the processor
    can execute concurrently during a single cycle. The main goal of a multithreading
    implementation is to get as close to 100% utilization of the processor hardware
    as reasonably possible. Multithreading utilizes both thread-level and process-level
    parallelism to accomplish this goal.
  prefs: []
  type: TYPE_NORMAL
- en: The are two types of multithreading, which we will cover in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Temporal multithreading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Also known as super-threading, the main subtypes for **temporal multithreading**
    (**TMT**) are coarse-grained and fine-grained (or interleaved). The former switches
    rapidly between different tasks, saving the context of each before switching to
    another task's context. The latter type switches tasks with each cycle, resulting
    in a CPU pipeline containing instructions from various tasks from which the term
    *interleaved* is derived.
  prefs: []
  type: TYPE_NORMAL
- en: The fine-grained type is implemented in barrel processors. They have an advantage
    over x86 and other architectures that they can guarantee specific timing (useful
    for hard real-time embedded systems) in addition to being less complex to implement
    due to assumptions that one can make.
  prefs: []
  type: TYPE_NORMAL
- en: Simultaneous multithreading (SMT)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SMT is implemented on superscalar CPUs (implementing instruction-level parallelism),
    which include the x86 and ARM architectures. The defining characteristic of SMT
    is also indicated by its name, specifically, its ability to execute multiple threads
    in parallel, per core.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, two threads per core is common, but some designs support up to eight
    concurrent threads per core. The main advantage of this is being able to share
    resources among threads, with an obvious disadvantage of conflicting needs by
    multiple threads, which has to be managed. Another advantage is that it makes
    the resulting CPU more energy efficient due to a lack of hardware resource duplication.
  prefs: []
  type: TYPE_NORMAL
- en: Intel's HT technology is essentially Intel's SMT implementation, providing a
    basic two thread SMT engine starting with some Pentium 4 CPUs in 2002.
  prefs: []
  type: TYPE_NORMAL
- en: Schedulers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A number of task-scheduling algorithms exist, each focusing on a different goal.
    Some may seek to maximize throughput, others minimize latency, while others may
    seek to maximize response time. Which scheduler is the optimal choice solely depends
    on the application the system is being used for.
  prefs: []
  type: TYPE_NORMAL
- en: For desktop systems, the scheduler is generally kept as general-purpose as possible,
    usually prioritizing foreground applications over background applications in order
    to give the user the best possible desktop experience.
  prefs: []
  type: TYPE_NORMAL
- en: For embedded systems, especially in real-time, industrial applications would
    instead seek to guarantee timing. This allows processes to be executed at exactly
    the right time, which is crucial in, for example, driving machinery, robotics,
    or chemical processes where a delay of even a few milliseconds could be costly
    or even fatal.
  prefs: []
  type: TYPE_NORMAL
- en: The scheduler type is also dependent on the multitasking state of the OS--a
    cooperative multitasking system would not be able to provide many guarantees about
    when it can switch out a running process for another one, as this depends on when
    the active process yields.
  prefs: []
  type: TYPE_NORMAL
- en: With a preemptive scheduler, processes are switched without them being aware
    of it, allowing the scheduler more control over when processes run at which time
    points.
  prefs: []
  type: TYPE_NORMAL
- en: Windows NT-based OSes (Windows NT, 2000, XP, and so on) use what is called a
    multilevel feedback queue, featuring 32 priority levels. This type of priority
    scheduler allows one to prioritize tasks over other tasks, allowing one to fine-tune
    the resulting experience.
  prefs: []
  type: TYPE_NORMAL
- en: Linux originally (kernel 2.4) also used a multilevel feedback queue-based priority
    scheduler like Windows NT with an O(n) scheduler. With version 2.6, this was replaced
    with an O(1) scheduler, allowing processes to be scheduled within a constant amount
    of time. Starting with Linux kernel 2.6.23, the default scheduler is the **Completely
    Fair Scheduler** (**CFS**), which ensures that all tasks get a comparable share
    of CPU time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The type of scheduling algorithm used for a number of commonly used or well-known
    OSes is listed in this table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Operating System** | **Preemption** | **Algorithm** |'
  prefs: []
  type: TYPE_TB
- en: '| Amiga OS | Yes | Prioritized round-robin scheduling |'
  prefs: []
  type: TYPE_TB
- en: '| FreeBSD | Yes | Multilevel feedback queue |'
  prefs: []
  type: TYPE_TB
- en: '| Linux kernel before 2.6.0 | Yes | Multilevel feedback queue |'
  prefs: []
  type: TYPE_TB
- en: '| Linux kernel 2.6.0-2.6.23 | Yes | O(1) scheduler |'
  prefs: []
  type: TYPE_TB
- en: '| Linux kernel after 2.6.23 | Yes | Completely Fair Scheduler |'
  prefs: []
  type: TYPE_TB
- en: '| classic Mac OS pre-9 | None | Cooperative scheduler |'
  prefs: []
  type: TYPE_TB
- en: '| Mac OS 9 | Some | Preemptive scheduler for MP tasks, and cooperative for
    processes and threads |'
  prefs: []
  type: TYPE_TB
- en: '| OS X/macOS | Yes | Multilevel feedback queue |'
  prefs: []
  type: TYPE_TB
- en: '| NetBSD | Yes | Multilevel feedback queue |'
  prefs: []
  type: TYPE_TB
- en: '| Solaris | Yes | Multilevel feedback queue |'
  prefs: []
  type: TYPE_TB
- en: '| Windows 3.1x | None | Cooperative scheduler |'
  prefs: []
  type: TYPE_TB
- en: '| Windows 95, 98, Me | Half | Preemptive scheduler for 32-bit processes, and
    cooperative for 16-bit processes |'
  prefs: []
  type: TYPE_TB
- en: '| Windows NT (including 2000, XP, Vista, 7, and Server) | Yes | Multilevel
    feedback queue |'
  prefs: []
  type: TYPE_TB
- en: '(Source: [https://en.wikipedia.org/wiki/Scheduling_(computing)](https://en.wikipedia.org/wiki/Scheduling_(computing)))'
  prefs: []
  type: TYPE_NORMAL
- en: The preemptive column indicates whether the scheduler is preemptive or not,
    with the next column providing further details. As one can see, preemptive schedulers
    are very common, and used by all modern desktop operating systems.
  prefs: []
  type: TYPE_NORMAL
- en: Tracing the demo application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the demonstration code of Chapter 1, *Revisiting Multithreading*, we looked
    at a simple `c++11` application which used four threads to perform some processing.
    In this section, we will look at the same application, but from a hardware and
    OS perspective.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we look at the start of the code in the `main` function, we see that we
    create a data structure containing a single (integer) value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: After the OS creates a new task and associated stack structure, an instance
    of a vector data structure (customized for integer types) is allocated on the
    stack. The size of this was specified in the binary file's global data section
    (BSS for ELF).
  prefs: []
  type: TYPE_NORMAL
- en: When the application's execution is started using its entry function (`main()`
    by default), the data structure is modified to contain the new integer value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we create four threads, providing each with some initial data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: For the OS, this means creating new data structures, and allocating a stack
    for each new thread. For the hardware, this initially does not change anything
    if no hardware-based task switching is used.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, the OS's scheduler and the CPU can combine to execute this set
    of tasks (threads) as efficiently and quickly as possible, employing features
    of the hardware including SMP, SMT, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'After this, the main thread waits until the other threads stop executing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: These are blocking calls, which mark the main thread as being blocked until
    these four threads (tasks) finish executing. At this point, the OS's scheduler
    will resume execution of the main thread.
  prefs: []
  type: TYPE_NORMAL
- en: 'In each newly created thread, we first output a string on the standard output,
    making sure that we lock the mutex to ensure synchronous access:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: A mutex, in essence, is a singular value being stored on the stack of heap,
    which then is accessed using an atomic operation. This means that some form of
    hardware support is required. Using this, a task can check whether it is allowed
    to proceed yet, or has to wait and try again.
  prefs: []
  type: TYPE_NORMAL
- en: In this last particular piece of code, this mutex lock allows us to output on
    the standard C++ output stream without other threads interfering.
  prefs: []
  type: TYPE_NORMAL
- en: 'After this, we copy the initial value in the vector to a local variable, again
    ensuring that it''s done synchronously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The same thing happens here, except now the mutex lock allows us to read the
    first value in the vector without risking another thread accessing or even changing
    it while we use it.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is followed by the generating of a random number as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This uses the `randGen()` method, which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This method is interesting due to its use of a thread-local variable. Thread-local
    storage is a section of a thread's memory which is specific to it, and used for
    global variables, which, nevertheless, have to remain limited to that specific
    thread.
  prefs: []
  type: TYPE_NORMAL
- en: This is very useful for a static variable like the one used here. That the `generator`
    instance is static is because we do not want to reinitialize it every single time
    we use this method, yet we do not want to share this instance across all threads.
    By using a thread-local, static instance, we can accomplish both goals. A static
    instance is created and used, but separately for each thread.
  prefs: []
  type: TYPE_NORMAL
- en: The `Thread` function then ends with the same series of mutexes being locked,
    and the new value being copied to the array.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Here we see the same synchronous access to the standard output stream, followed
    by synchronous access to the values data structure.
  prefs: []
  type: TYPE_NORMAL
- en: Mutual exclusion implementations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mutual exclusion is the principle which underlies thread-safe access of data
    within a multithreaded application. One can implement this both in hardware and
    software. The **mutual exclusion** (**mutex**) is the most elementary form of
    this functionality in most implementations.
  prefs: []
  type: TYPE_NORMAL
- en: Hardware
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The simplest hardware-based implementation on a uniprocessor (single processor
    core), non-SMT system is to disable interrupts, and thus, prevent the task from
    being changed. More commonly, a so-called busy-wait principle is employed. This
    is the basic principle behind a mutex--due to how the processor fetches data,
    only one task can obtain and read/write an atomic value in the shared memory,
    meaning, a variable sized the same (or smaller) as the CPU's registers. This is
    further detailed in [Chapter 15](fa81398a-9b4d-43c6-8ee5-cec7d6a6b6c1.xhtml),
    *Atomic Operations - Working with the Hardware*.
  prefs: []
  type: TYPE_NORMAL
- en: 'When our code tries to lock a mutex, what this does is read the value of such
    an atomic section of memory, and try to set it to its locked value. Since this
    is a single operation, only one task can change the value at any given time. Other
    tasks will have to wait until they can gain access in this busy-wait cycle, as
    shown in this diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9abfe9d9-ee51-4508-b46d-cbb6ac2c97c1.png)'
  prefs: []
  type: TYPE_IMG
- en: Software
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Software-defined mutual exclusion implementations are all based on busy-waiting.
    An example is **Dekker's** algorithm, which defines a system in which two processes
    can synchronize, employing busy-wait to wait for the other process to leave the
    critical section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pseudocode for this algorithm is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '(Referenced from: [https://en.wikipedia.org/wiki/Dekker''s_algorithm](https://en.wikipedia.org/wiki/Dekker''s_algorithm))'
  prefs: []
  type: TYPE_NORMAL
- en: In this preceding algorithm, processes indicate the intent to enter a critical
    section, checking whether it's their turn (using the process ID), then setting
    their intent to enter the section to false after they have entered it. Only once
    a process has set its intent to enter to true again will it enter the critical
    section again. If it wishes to enter, but `turn` does not match its process ID,
    it'll busy-wait until the condition becomes true.
  prefs: []
  type: TYPE_NORMAL
- en: A major disadvantage of software-based mutual exclusion algorithms is that they
    only work if **out-of-order** (**OoO**) execution of code is disabled. OoO means
    that the hardware actively reorders incoming instructions in order to optimize
    their execution, thus changing their order. Since these algorithms require that
    various steps are executed in order, they no longer work on OoO processors.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Every modern programming language supports concurrency, offering high-level
    APIs that allow the execution of many tasks simultaneously. C++ supports concurrency
    starting from C++11 and more sophisticated APIs got added further in C++14 and
    C++17\. Though the C++ thread support library allows multithreading, it requires
    writing lengthy code using complex synchronizations; however, concurrency lets
    us execute independent tasks--even loop iterations can run concurrently without
    writing complex code. The bottom line is parallelization is made more easy with
    concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: The concurrency support library complements the C++ thread support library.
    The combined use of these two powerful libraries makes concurrent programming
    more easy in C++.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s write a simple `Hello World` program using C++ concurrency in the following
    file named `main.cpp`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Let's try to understand the `main()` function. Future is an object of the concurrency
    module that helps the caller function retrieve the message passed by the thread
    in an asynchronous fashion. The void in `future<void>` represents the `sayHello()` thread
    function that is not expected to pass any message to the caller, that is, the `main` thread
    function. The `async` class lets us execute a function in two modes, namely `launch::async` or `launch::deferred` mode.
  prefs: []
  type: TYPE_NORMAL
- en: The `launch::async` mode lets the `async` object launch the `sayHello()` method
    in a separate thread, whereas the `launch::deferred` mode lets the `async` object
    invoke the `sayHello()` function without creating a separate thread. In `launch::deferred` mode,
    the `sayHello()` method invocation will be different until the caller thread invokes
    the `future::get()` method.
  prefs: []
  type: TYPE_NORMAL
- en: The `futureObj.wait()` voice is used to block the main thread to let the `sayHello()` function
    complete its task. The `future::wait()` function is similar to `thread::join()` in
    the thread support library.
  prefs: []
  type: TYPE_NORMAL
- en: How to compile and run
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s go ahead and compile the program with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s launch `concurrency.exe`, as shown ahead, and understand how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1d316a09-3388-4996-b3b0-22c8fe6f15ea.png)'
  prefs: []
  type: TYPE_IMG
- en: Asynchronous message passing using the concurrency support library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s slightly modify `main.cpp`, the Hello World program we wrote in the previous
    section. Let''s understand how we could pass a message from a `Thread` function
    to the caller function asynchronously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In the previous program, `promiseObj` is used by the `sayHello()` thread function
    to pass the message to the main thread asynchronously. Note that `promise<string>` implies
    that the `sayHello()` function is expected to pass a string message, hence the
    main thread retrieves `future<string>`. The `future::get()` function call will
    be blocked until the `sayHello()` thread function calls the `promise::set_value()` method.
  prefs: []
  type: TYPE_NORMAL
- en: However, it is important to understand that `future::get()` must only be called
    once as the corresponding `promise` object will be destructed after the call to
    the `future::get()` method invocation.
  prefs: []
  type: TYPE_NORMAL
- en: Did you notice the use of the `std::move()` function? The `std::move()` function
    basically transfers the ownership of `promiseObj` to the `sayHello()` thread function,
    hence `promiseObj` must not be accessed from the `main` thread after `std::move()` is
    invoked.
  prefs: []
  type: TYPE_NORMAL
- en: How to compile and run
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s go ahead and compile the program with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Observe how the `concurrency.exe` application works by launching `concurrency.exe` as
    shown ahead:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7276bc6e-0b48-4e8c-878b-987bff2ea20d.png)'
  prefs: []
  type: TYPE_IMG
- en: As you may have guessed, the output of this program is exactly the same as our
    previous version. But this version of our program makes use of promise and future
    objects, unlike the previous version that doesn't support message passing.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The concurrency support module supports a concept called **task**. A task is work that
    happens concurrently across threads. A concurrent task can be created using the `packaged_task` class.
    The `packaged_task` class conveniently connects the `thread` function, the corresponding
    promise, and feature objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s understand the use of `packaged_task` with a simple example. The following
    program gives us an opportunity to taste a bit of functional programming with
    lambda expressions and functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previously shown program, I created a `packaged_task` instance called `addTask`.
    The `packaged_task< int (int,int)>` instance implies that the add task will return
    an integer and take two integer arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code snippet indicates it is a lambda function that is defined
    anonymously.
  prefs: []
  type: TYPE_NORMAL
- en: The interesting part is that the `addTask( )` call in `main.cpp` appears like
    a regular function call. The `future<int>` object is extracted from the `packaged_task` instance `addTask`,
    which is then used to retrieve the output of the `addTask` via the future object
    instance, that is, the `get()` method.
  prefs: []
  type: TYPE_NORMAL
- en: How to compile and run
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s go ahead and compile the program with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s quickly launch `concurrency.exe` and observe the output shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f2649f2e-71d5-4bec-9889-a120e26d7844.png)'
  prefs: []
  type: TYPE_IMG
- en: Cool! You learned how to use lambda functions with the concurrency support library.
  prefs: []
  type: TYPE_NORMAL
- en: Using tasks with a thread support library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, you learned how `packaged_task` can be used in an
    elegant way. I love lambda functions a lot. They look a lot like mathematics.
    But not everyone likes lambda functions as they degrade readability to some extent.
    Hence, it isn''t mandatory to use lambda functions with a concurrent task if you
    don''t prefer lambdas. In this section, you''ll understand how to use a concurrent
    task with the thread support library, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: How to compile and run
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s go ahead and compile the program with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s launch `concurrency.exe`, as shown in the following screenshot, and
    understand the difference between the previous program and the current version:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2c5aae5a-3b84-4b4e-b194-8ee35667ad7a.png)'
  prefs: []
  type: TYPE_IMG
- en: Yes, the output is the same as the previous section because we just refactored
    the code.
  prefs: []
  type: TYPE_NORMAL
- en: Wonderful! You just learned how to integrate the C++ thread support library
    with concurrent components.
  prefs: []
  type: TYPE_NORMAL
- en: Binding the thread procedure and its input to packaged_task
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, you will learn how you can bind the `thread` function and its
    respective arguments with `packaged_task`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take the code from the previous section and modify it to understand
    the bind feature, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The `std::bind( )` function binds the `thread` function and its arguments with
    the respective task. Since the arguments are bound upfront, there is no need to
    supply the input arguments 15 or 10 once again. These are some of the convenient
    ways in which `packaged_task` can be used in C++.
  prefs: []
  type: TYPE_NORMAL
- en: How to compile and run
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s go ahead and compile the program with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s launch `concurrency.exe`, as shown in the following screenshot, and
    understand the difference between the previous program and the current version:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e2d19c8b-835e-49ee-992e-d5c11b2d7b13.png)'
  prefs: []
  type: TYPE_IMG
- en: Congrats! You have learned a lot about concurrency in C++ so far.
  prefs: []
  type: TYPE_NORMAL
- en: Exception handling with the concurrency library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The concurrency support library also supports passing exceptions via a future
    object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s understand the exception concurrency handling mechanism with a simple
    example, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Just like the way we passed the output messages to the caller function/thread,
    the concurrency support library also allows you to set the exception that occurred
    within the task or asynchronous function. When the caller thread invokes the `future::get()` method,
    the same exception will be thrown, hence communicating exceptions is made easy.
  prefs: []
  type: TYPE_NORMAL
- en: How to compile and run
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s go ahead and compile the program with the following command. Uncle fruits
    and yodas malte:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/98871bec-2f2e-40dc-bd84-088519ea8196.png)'
  prefs: []
  type: TYPE_IMG
- en: What did you learn?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let me summarize the takeaway points:'
  prefs: []
  type: TYPE_NORMAL
- en: The concurrency support library offers high-level components that enable the
    execution of several tasks concurrently
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Future objects let the caller thread retrieve the output of the asynchronous
    function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The promise object is used by the asynchronous function to set the output or
    exception
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The type of `FUTURE` and `PROMISE` object must be the same as the type of the
    value set by the asynchronous function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concurrent components can be used in combination with the C++ thread support
    library seamlessly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The lambda function and expression can be used with the concurrency support
    library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw how processes and threads are implemented both in operating
    systems and in hardware. We also looked at various configurations of processor
    hardware and elements of operating systems involved in scheduling to see how they
    provide various types of task processing.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we took the multithreaded program example of the previous chapter,
    and ran through it again, this time considering what happens in the OS and processor
    while it is being executed.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will take a look at the various multithreading APIs
    being offered via OS and library-based implementations, along with examples comparing
    these APIs.
  prefs: []
  type: TYPE_NORMAL
