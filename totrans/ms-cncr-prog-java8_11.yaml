- en: Chapter 10. Integration of Fragments and Implementation of Alternatives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From [Chapter 2](part0022_split_000.html#KVCC1-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 2. Managing Lots of Threads – Executors"), *Managing Lots of Threads
    – Executors*, to [Chapter 8](part0051_split_000.html#1GKCM1-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 8. Processing Massive Datasets with Parallel Streams – The Map and Collect
    Model"), *Processing Massive Datasets with Parallel Streams – The Map and Collect
    Model*, you implemented different examples using the most important parts of the
    Java concurrency API. Usually, these examples are real, but most of the times,
    these examples can be parts of a bigger system. For example, in [Chapter 4](part0033_split_000.html#VF2I1-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 4. Getting Data from the Tasks – The Callable and Future Interfaces"),
    *Getting Data from the Tasks – The Callable and Future Interfaces*, you implemented
    an application to construct an inverted index to be used in an information retrieval
    system. In [Chapter 6](part0041_split_000.html#173722-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 6. Optimizing Divide and Conquer Solutions – The Fork/Join Framework"),
    *Optimizing Divide and Conquer Solutions – The Fork/Join Framework*, you implemented
    the k-means clustering algorithm to cluster a set of documents. However, you can
    implement a full information retrieval application that reads a set of documents,
    represents them using the vector space model, and clusters them using the K-NN
    algorithm. In these cases, you have different parts that may use different concurrency
    technologies (executors, streams, and so on), but they have to synchronize and
    communicate between them to get the desired results.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, all the examples presented in this book can be implemented using other
    components of the Java concurrency API. We will discuss some of those alternatives
    too.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Big-block synchronization mechanisms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An example of a document clustering application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation alternatives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Big-block synchronization mechanisms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Big computer applications are formed by different components that work together
    to get the desired functionality. Those components have to synchronize and communicate
    between them. In [Chapter 9](part0056_split_000.html#1LCVG2-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 9. Diving into Concurrent Data Structures and Synchronization Utilities"),
    *Diving into Concurrent Data Structures and Synchronization Utilities*, you learned
    that you can use different Java classes to synchronize tasks and communicate between
    them. But this task organization is more complicated when the components you want
    to synchronize are concurrent systems too that can use different mechanisms to
    implement their concurrency. For example, you have a component in an application
    that uses the Fork/Join framework to generate their results that are used by other
    tasks synchronized using the `Phaser` class.
  prefs: []
  type: TYPE_NORMAL
- en: 'In these cases, you can use the following two mechanisms to synchronize and
    communicate those components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Shared memory**: The systems share a data structure to pass information between
    them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Message passing**: One of the systems sends a message to one or more systems.
    There are different ways to implement this. In object-oriented programming languages
    such as Java, the most basic message passing mechanism is when an object calls
    a method of an other object. You can also use **Java Message Service** (**JMS**),
    buffers, or other data structures. You can have the following two kinds of message
    passing techniques:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Synchronous**: In this case, the class that sends the message waits until
    the receiver has processed its message'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Asynchronous**: In this case, the class that sends the message doesn''t wait
    for a receiver that processes its message'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, you're going to implement an application to cluster documents
    formed by four subsystems that communicate and synchronize between them to cluster
    the documents.
  prefs: []
  type: TYPE_NORMAL
- en: An example of a document clustering application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This application will read a set of documents and will organize them using
    the k-means clustering algorithms. To achieve this, we will use four components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The Reader system**: This system will read all the documents and convert
    every document into a list of `String` objects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The Indexer system**: This system will process the documents and convert
    them into a list of words. At the same time, it will generate the global vocabulary
    of the set of documents with all the words that appear on them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The Mapper system**: This system will convert each list of words into a mathematical
    representation using the vector space model. The value of each item will be the
    **Tf-Idf** (short for **term frequency–inverse document frequency**) metric.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The Clustering system**: This system will use the k-means clustering algorithm
    to cluster the documents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All these systems are concurrent and use their own tasks to implement their
    functionality. Let's see how you can implement this example.
  prefs: []
  type: TYPE_NORMAL
- en: The four systems of k-means clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's see how to implement the Reader, Indexer, Mapper, and Clustering systems.
  prefs: []
  type: TYPE_NORMAL
- en: The Reader system
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have implemented this system in the `DocumentReader` class. This class implements
    the `Runnable` interface and internally uses three attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: A `ConcurrentLinkedDeque` class of `String` objects with all the names of the
    files you have to process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `ConcurrentLinkedQueue` class of `TextFile` objects to store the documents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `CountDownLatch` object to control the end of the execution of the tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The constructor of the class initializes these attributes (the three are received
    as parameters by the constructor) and the `run()` method given here implements
    all the functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: First, we read the content of all the files. For every file, we create an object
    of the `TextFile` class. This class contains the name and the content of the text
    file. It has a constructor that receives a `Path` object with the route of the
    file. Finally, we write a message in the console and use the `countDown()` method
    of the `CountDownLatch` object to indicate the finalization of this task.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the code of the `TextFile` class. Internally, it has two attributes
    to store the file name and its contents. It uses the `readAllLines()` method of
    the `Files` class to convert the content of the file into a `List<String>` data
    structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The Indexer system
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This system is implemented in the `Indexer` class that also implements the
    `Runnable` interface. In this case, we use five internal attributes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A `ConcurrentLinkedQueue` of `TextFile` with the content of all the documents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `ConcurrentLinkedDeque` of `Document` objects to store the list of words that
    forms each document
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `CountDownLatch` object to control the finalization of the `Reader` system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `CountDownLatch` object to indicate the finalization of the tasks of this
    system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `Vocabulary` object to store all the words that form the document collection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The constructor of the class initializes this attributes (receives all of them
    as parameters):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The `run()` method implements all the functionality, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'First, it gets `TextFile` from the queue and if it''s not `null`, uses the
    `parseDoc()` method to convert it into a `Document` object. Then, it processes
    all the words of the document to store them in the global vocabulary object and
    stores the document in the list of documents, as you can see in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The `parseDoc()` method receives `List<String>` with the contents of the document
    and return a `Document` object. It creates a `Document` object to process all
    the lines using the `forEach()` method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The `parseLine()` method will split the line into its words and store them
    into the `doc` object as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'You can include an optimization in the code presented before precompiling the
    regular expression used in the `replaceAll()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The Mapper system
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This system is implemented in the `Mapper` class that also implements the `Runnable`
    interfaces. Internally, it use the following two attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: A `ConcurrentLinkedDeque` of `Document` objects with the information of all
    the documents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `Vocabulary` object with all the words in the entire collection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code for this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The constructor of the class initializes those attributes, and the `run()`
    method implements the functionality of this system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: First, it gets a document from the `Deque` object of documents using the `pollFirst()`
    method. Then, it processes all the words in the document calculating the `tfxidf`
    measure and creating a new `Attribute` object to store those values. Those attributes
    are stored in a list.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we convert the list into an array of `Attribute` objects and store
    that array in the `Document` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The Clustering system
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This system implements the k-means clustering algorithm. You can use the elements
    presented in [Chapter 5](part0037_split_000.html#1394Q1-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 5. Running Tasks Divided into Phases – The Phaser Class"), *Running Tasks
    Divided into Phases – The Phaser Class*, to implement this system. That implementation
    has the following elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The DistanceMeasurer class**: This class calculates the Euclidean distance
    between an array of `Attribute` objects with the information of a document and
    the centroid of a cluster'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The DocumentCluster class**: This stores the information about a cluster:
    the centroid and the documents of that cluster'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The AssigmentTask class**: This extends the `RecursiveAction` class (of the
    Fork/Join framework) and executes the assignment task of the algorithm where we
    calculate the distance between each document and all the clusters to decide the
    cluster of every document'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The UpdateTask class**: This extends the `RecursiveAction` class (of the
    Fork/Join framework) and executes the updated task of the algorithm that recalculates
    the centroid of every cluster as the average of the documents stored on it'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The ConcurrentKMeans class**: This class has the static method `calculate()`
    that executes the clustering algorithm and returna an array of `DocumentCluster`
    objects with all the clusters generated'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We have only added a new class, the `ClusterTask` class that implements the
    `Runnable` interface and will call the `calculate()` method of the `ConcurrentKMeans`
    class. Internally, it uses two attributes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: An array of `Document` objects with the information of all the documents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Vocabulary` object with all the words of the collection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The constructor initializes those attributes, and the `run()` method implements
    the logic of the task. We call the `calculate()` method of the `ConcurrentKMeans`
    class passing to the five parameters as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The array of `Document` objects with the information of all the documents.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Vocabulary` object with all the words of the collection.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of clusters we want to generate. In this case, we use `10` as the
    number of clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The seed used to initialize the centroids of the clusters. In this case, we
    use `991` as a seed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The reference size to split the tasks in subtasks used in the Fork/Join framework.
    In this case, we use `10` as that minimum size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is the code of that class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The main class of the document clustering application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once we have implemented all the elements we use in our application, we have
    to implement the `main()` method of our system. In this case, this method is critical
    because it's responsible for the launching of the systems and the creation of
    the elements needed to synchronize them. The `Reader` and `Indexer` systems will
    be executing at the same time. They will be using a buffer to share information
    between them. When the reader reads a document, it will write the list of `String`
    objects in the buffer and then proceed to process the next document. It doesn't
    wait for a task that processes that `List`. This is an example of **asynchronous
    message passing**. The `Indexer` system will take the documents from the buffer,
    process them, and generate the `Vocabulary` object with all the words of the documents.
    All the tasks executed by the `Indexer` system share the same instance of the
    `Vocabulary` class. This is an example of **shared memory**.
  prefs: []
  type: TYPE_NORMAL
- en: The main class will wait for the finalization of the `Reader` and `Indexer`
    systems in a synchronous way using the `await()` method of a `CountDownLatch`
    object. This method blocks the execution of the calling thread until its internal
    counter arrives at 0.
  prefs: []
  type: TYPE_NORMAL
- en: Once both systems have finished their execution, the `Mapper` system will use
    the `Vocabulary` object and the `Document` information to obtain the vector space
    model representation of each document. When the `Mapper` has finished its execution,
    the `Clustering` system clusters all the documents. We have used the `CompletableFuture`
    class to synchronize the end of the `Mapper` system with the start of the `Clustering`
    system. This is another example of asynchronous communication between two systems.
  prefs: []
  type: TYPE_NORMAL
- en: We have implemented the main class in the `ClusteringDocs` class.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create a `ThreadPoolExecutor` object and obtain `ConcurrentLinkedDeque`
    with the files that contain the documents using the `readFileNames()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we create the buffer of documents, `ConcurrentLinkedDeque`, to store
    the `Document` objects, the `Vocabulary` object, and two `CountDownLatch` objects—one
    to control the end of the tasks of the `Reader` system and other to control the
    end of the tasks of the `Indexer` system. We have the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we launch the two tasks to execute the `Reader` system of the `DocumentReader`
    class and another four tasks to execute the `Indexer` system of the `Indexer`
    class. All these tasks are executed in the `Executor` object we have created earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, the `main()` method waits for the finalization of this tasks; first,
    for the `DocumentReader` tasks and then for the `Indexer` tasks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we convert the `ConcurrentLinkedDeque` class of `Document` objects in
    an array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We launch the `Indexer` system executing four tasks of the `Mapper` class using
    the `runAsync()` method of the `CompletableFuture` class as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we launch the `Clustering` system to launch one tasks of the `ClusterTask`
    class (remember that these tasks will launch other tasks to execute the algorithm).
    The `main()` method uses the `allOf()` method of the `CompletableFuture` class
    to wait for the finalization of the `Mapper` tasks and then uses the `thenRunAsync()`
    method to launch the clustering algorithm when the `Mapper` system has finished:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we wait for the finalization of the `Clustering` system using the
    `get()` method and finish the execution of the program as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The `readFileNames()` method receives a string as a parameter that must be the
    path to the directory where the collection of documents is stored and generates
    a `ConcurrentLinkedDeque` class of `String` objects with the name of the files
    contained in that directory.
  prefs: []
  type: TYPE_NORMAL
- en: Testing our document clustering application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To test this application, we have used a subset of 10,052 documents of the
    100,673 documents with information about movies taken from Wikipedia as the collection
    of documents. In the following image, you can see the results of the first part
    of the execution—from the start of the execution until the indexer execution ends:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing our document clustering application](img/00033.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following image shows the rest of the execution of the examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing our document clustering application](img/00034.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: You can see how the tasks are synchronized as seen earlier in this chapter.
    First, the `Reader` and `Indexer` tasks are executed in a concurrent way. When
    they finish, the mapper makes the transformation of the data, and finally, the
    clustering algorithm organizes the examples.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation of alternatives with concurrent programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of the examples we have implemented through the chapters of this book can
    be implemented using other components of the Java concurrency API. In this section,
    we will describe how to implement some of these alternatives.
  prefs: []
  type: TYPE_NORMAL
- en: The k-nearest neighbors' algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You have implemented the k-nearest neighbors'' algorithm in [Chapter 2](part0022_split_000.html#KVCC1-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 2. Managing Lots of Threads – Executors"), *Managing Lots of Threads
    – Executors*, using an executor. This is a simple machine-learning algorithm used
    for supervised classification. You have a training set of previous classified
    examples. To obtain the class of a new example, you calculate the distance from
    this example to the training set of examples. The majority of classes in the nearest
    examples are the classes selected for the example. You can also implement this
    algorithm with one of the following components of the concurrency API:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Threads**: You can implement this example using `Thread` objects. You have
    to execute the tasks executed in the executor using normal threads. Each thread
    will calculate the distance between the example and a subset of the training set,
    and it will save that distance in a data structure shared between all the threads.
    When all the threads have finished, you can sort the data structure using the
    distance and calculate the class of the example.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fork/Join framework**: As in the previous solution, each task will calculate
    the distance between the example and the subset of the training set. In this case,
    you define the maximum number of examples in those subsets. If a task has to process
    more examples, you divide that task into two child tasks. After you have joined
    the two tasks, you have to generate a unique data structure with the results of
    the two subtasks. At the end, you will have a data structure with all the distances
    that you can sort to obtain the class of the example.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Streams**: You create a stream from the training data and map each training
    example in a structure that contains the distance between the example you want
    to classify and that example. Then, you sort that structure, get the closest ones
    using `limit()` and calculate the final resultant class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building an inverted index of a collection of documents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have implemented this example in [Chapter 4](part0033_split_000.html#VF2I1-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 4. Getting Data from the Tasks – The Callable and Future Interfaces"),
    *Getting Data from the Tasks – The Callable and Future Interfaces*, using an executor.
    An inverted index is a data structure used in the information retrieval field
    to speed up the searches of information. It stores the words presented in a document
    collection and for each word, the documents where they appear. When you make a
    search of information, you don''t need to process the documents. You look at the
    inverted index to extract the documents where the words you have inserted appear
    and construct the result list. You can also implement this algorithm with one
    of the following components of the concurrency API:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Threads**: Each thread will process a subset of documents. This process includes
    obtaining the vocabulary of the document and updating a common data structure
    with the global index. When all the threads have finished their execution, you
    can create the file in a sequential way.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fork/Join framework**: You define the maximum number of documents a task
    can process. If a task has to process more documents, you split that task into
    two subtasks. The result of each task will be a data structure with the inverted
    index of the documents processed by those tasks or its subtasks. After joining
    the two subtasks, you construct a unique inverted index from the inverted indexes
    of its subtasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Streams**: You create a stream to process all the files. You map each file
    in the object with its vocabulary and then you reduce that vocabulary stream to
    get the inverted index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A best-matching algorithm for words
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You have implemented this example in [Chapter 4](part0033_split_000.html#VF2I1-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 4. Getting Data from the Tasks – The Callable and Future Interfaces"),
    *Getting Data from the Tasks – The Callable and Future Interfaces*. The main objective
    of this algorithm is to find the words most similar to a string passed as a parameter.
    You can also implement this algorithm using one of the following components of
    the concurrency API:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Threads**: Each thread will calculate the distance between the searched word
    and a sublist of the whole list of words. Each thread will generate a partial
    result that will be merged into the final result for a shared class between all
    the threads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fork/Join framework**: Each task will calculate the distance between the
    searched word and a sublist of the whole list of words. If the list is too big,
    you have to split the task into two subtasks. Each task will return a partial
    result. After joining the two subtasks, the task will integrate the two sublists
    into one. The original task will return the final result.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Streams**: You create a stream for the whole list of word, map each word
    with a data structure that includes the distance between the searched word and
    that word, sort that list, and get the results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A genetic algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have implemented this example in [Chapter 5](part0037_split_000.html#1394Q1-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 5. Running Tasks Divided into Phases – The Phaser Class"), *Running Tasks
    Divided into Phases – The Phaser Class*. A **genetic algorithm** is an adaptive
    heuristic search algorithm based on the natural selection principles used to generate
    good solutions to **optimization** and **search problems**. There are different
    approaches to use the multiple threads for a genetic algorithm. The most classical
    one is to create *islands*. Each thread represents an island where a part of the
    population evolves. Sometimes, migrations between islands occur while transferring
    some individuals from one island to another. After the algorithm finishes, the
    best specie across all the islands is selected. Such an approach reduces the contention
    considerably as threads rarely talk to each other.
  prefs: []
  type: TYPE_NORMAL
- en: There are also other approaches well-described in many publications and websites.
    For example, this handouts set summarizes the approaches pretty well at [https://cw.fel.cvut.cz/wiki/_media/courses/a0m33eoa/prednasky/08pgas-handouts.pdf](https://cw.fel.cvut.cz/wiki/_media/courses/a0m33eoa/prednasky/08pgas-handouts.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also implement this algorithm using one of the following components
    of the concurrency API:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Threads**: The population with all the individuals must be a shared data
    structure. You can implement the three phases in the following way: the selection
    phase in a sequential way; the crossover phase with threads, where each thread
    will generate a predefined number of individuals; and the evaluation phase, with
    threads too. Each thread will evaluate a predefined number of individuals.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Executor**: You can implement something similar to the previous one executing
    the tasks in an executor instead of independent threads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fork/Join framework**: The main idea is the same, but in this case, your
    tasks will be divided until they process a predefined number of individuals. The
    join part in this case doesn''t do anything because the results of the tasks will
    be stored in the common data structure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A keyword extraction algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You have implemented this example in [Chapter 5](part0037_split_000.html#1394Q1-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 5. Running Tasks Divided into Phases – The Phaser Class"), *Running Tasks
    Divided into Phases – The Phaser Class*. We use this kind of algorithm to extract
    a small set of words that describes a document. We try to find the most informative
    words using measures such as the Tf-Idf. You can also implement this example using
    the following components of the concurrency API:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Threads**: You need two kinds of threads. The threads of the first group
    will process the document set to obtain the document frequency of every word.
    You need a shared data structure that stores the vocabulary of the collection.
    The threads of the second group will process the documents again to obtain the
    keywords of each document and update a structure that maintains the whole list
    of keywords.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fork/Join framework**: The main idea is similar to the previous version.
    You need two kinds of tasks. The first one to obtain the global vocabulary of
    the collection of documents. Each task will calculate the vocabulary of a subset
    of documents. If that subset is too big, the task will execute two subtasks. After
    joining the subtasks, it will group the two vocabularies obtained into one. The
    second group of tasks will calculate the list of keywords. Each task will calculate
    the list of keywords of a subset of documents. If that subset is too big, it will
    execute two subtasks. When those tasks finish, the parent task will generate a
    list of keywords with the lists returned by the child tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Streams**: You create a stream to process all the documents. You map each
    document with an object that contains the vocabulary of the document and reduce
    it to get the global vocabulary. You generate another stream to process all the
    documents again, map each document with an object that contains its keywords,
    and reduce it to generate the final list of keywords.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A k-means clustering algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You have implemented this algorithm in [Chapter 6](part0041_split_000.html#173722-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 6. Optimizing Divide and Conquer Solutions – The Fork/Join Framework"),
    *Optimizing Divide and Conquer Solutions – The Fork/Join Framework*. This algorithm
    classifies a set of elements into a previous defined number of clusters. You don''t
    have any information about the class of the elements, so this is an unsupervised
    learning algorithm that tries to find similar items. You can also implement this
    example using the following components of the concurrency API:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Threads**: You will have two kinds of threads. The first one will assign
    a cluster to the examples. Each thread will process a subset of the examples set.
    The second kind of thread will update the centroid of the clusters. The clusters
    and the examples must be data structures shared by all the threads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Executor**: You can implement the idea presented before but executing the
    tasks in an executor instead of using independent threads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A filtering data algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You have implemented this algorithm in [Chapter 6](part0041_split_000.html#173722-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 6. Optimizing Divide and Conquer Solutions – The Fork/Join Framework"),
    *Optimizing Divide and Conquer Solutions – The Fork/Join Framework*. The main
    objective of this algorithm is to select the objects that satisfy certain conditions
    from a very big set of objects. You can also implement this example using the
    following components of the concurrency API:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Threads**: Each thread will process a subset of objects. If you''re looking
    for one result, when one thread is found, it must suspend the execution of the
    rest. If you are looking for a list of elements, that list must be a shared data
    structure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Executor**: The same as earlier, but executing the tasks in an executor instead
    of using independent threads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Streams**: You can use the `filter()` method of the `Stream` class to make
    the search over the objects. Then, you can reduce those results to get them in
    the format you need.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Searching an inverted index
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You have implemented this algorithm in [Chapter 7](part0047_split_000.html#1CQAE2-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 7. Processing Massive Datasets with Parallel Streams – The Map and Reduce
    Model"), *Processing Massive Datasets with Parallel Streams – The Map and Reduce
    Model*. In a previous example, we discuss about how to implement the algorithm
    that creates the inverted index to speed up the searches of information. This
    is the algorithm that makes that search of information. You can also implement
    this example using the following components of the concurrency API:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Threads**: This is the list of results in a common data structure. Each thread
    processes a part of the inverted index. Each result is inserted in order to generate
    a sorted data structure. If you get a good enough list of results, you can return
    that list and cancel the execution of the tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Executor**: This is similar to the previous one, but executes the concurrent
    tasks in an executor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fork/Join framework**: This is similar to the previous one, but each task
    divides the part of the inverted index to process it into smaller chunks until
    they are small enough.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A numeric summarization algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You have implemented this example in [Chapter 7](part0047_split_000.html#1CQAE2-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 7. Processing Massive Datasets with Parallel Streams – The Map and Reduce
    Model"), *Processing Massive Datasets with Parallel Streams – The Map and Reduce
    Model*. This kind of algorithm wants to obtain statistical information about a
    very big set of data. You can also implement this example using the following
    components of the concurrency API:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Threads**: We will have an object to store the data generated by the threads.
    Each thread will process a subset of the data and store the results of that data
    in the common object. Maybe, we will have to postprocess that object to generate
    the final results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Executor**. This is similar to the previous one, but executes the concurrent
    tasks in an executor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fork/Join framework**: This is similar to the previous one, but each task
    divides the part of the inverted index to process into smaller chunks until they
    are small enough.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A search algorithm without indexing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You have implemented this example in [Chapter 8](part0051_split_000.html#1GKCM1-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 8. Processing Massive Datasets with Parallel Streams – The Map and Collect
    Model"), *Processing Massive Datasets with Parallel Streams – The Map and Collect
    Model*. This algorithm obtains the objects that meet certain conditions when you
    don''t have an inverted index to speed up the search. In these cases, you have
    to process all the elements when you make the search. You can also implement this
    example using the following components of the concurrency API:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Threads**: Each thread will process a subset of objects (files in our case)
    to get a list of results. The list of results will be a shared data structure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Executor**: This is similar to the previous one, but the concurrent tasks
    will be executed in an executor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fork/Join framework**: This is similar to the previous one, but the tasks
    divide the part of the inverted index to process into smaller chunks until they
    are small enough.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A recommendation system using the Map and Collect model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You have implemented this example in [Chapter 8](part0051_split_000.html#1GKCM1-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 8. Processing Massive Datasets with Parallel Streams – The Map and Collect
    Model"), *Processing Massive Datasets with Parallel Streams – The Map and Collect
    Model*. A **recommendation system** recommends a product or service to a customer
    based on the products/services he has bought/used and in the products/services
    bought/used by the users that has bought/used the same services as him. You can
    also implement this example using the Phaser component of the concurrency API.
    This algorithm has three phases:'
  prefs: []
  type: TYPE_NORMAL
- en: '**First phase**: We need to convert the list of products with their reviews
    into a list of buyers with the products they have bought. Each task will process
    a subset of products, and the list of buyers will be a shared data structure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Second phase**: We have to obtain a list of the users that bought the same
    products than the reference user. Each task will process an item of the products
    bought by the user and will add the users who bought that product to a common
    set of users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Third phase**: We obtain the recommended products. Each task will process
    a user of the previous list and add the products he has bought to a common data
    structure that will generate the final list of recommended products.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this book, you implemented a lot of real-world examples. Some of these examples
    can be used as a part of a bigger system. These bigger systems normally have different
    concurrent parts that must share information and be synchronized between them.
    To make that synchronization, we can use three mechanisms: the shared memory,
    when two or more tasks share an object or data structure, asynchronous message
    passing, when a task sends a message to another task and doesn''t wait for its
    processing, and synchronous message passing, when a task sends a message to another
    task and waits for its processing.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we implemented an application to cluster documents formed by
    four subsystems. We used the mechanisms presented earlier to synchronize and share
    information between those four subsystems.
  prefs: []
  type: TYPE_NORMAL
- en: We also revised some of the examples presented in the book to discuss other
    alternatives to their implementation.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn how to obtain debug information of the components
    of the concurrency API and how to monitor and test a concurrent application.
  prefs: []
  type: TYPE_NORMAL
