- en: Dimensionality Reduction with Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Over the course of this chapter, we will continue our exploration of unsupervised
    learning models in the form of **dimensionality reduction**.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the models we have covered so far, such as regression, classification,
    and clustering, dimensionality reduction does not focus on making predictions.
    Instead, it tries to take a set of input data with a feature dimension *D* (that
    is, the length of our feature vector), and extracts a representation of the data
    of dimension *k*, where *k* is usually significantly smaller than *D*. It is,
    therefore, a form of preprocessing or feature transformation rather than a predictive
    model in its own right.
  prefs: []
  type: TYPE_NORMAL
- en: It is important that the representation that is extracted should still be able
    to capture a large proportion of the variability or structure of the original
    data. The idea behind this is that most data sources will contain some form of
    underlying structure. This structure is typically unknown (often called latent
    features or latent factors), but if we can uncover some of this structure, our
    models could learn this structure, and make predictions from it rather than from
    the data in its raw form, which might be noisy or contain many irrelevant features.
    In other words, dimensionality reduction throws away some of the noise in the
    data, and keeps the hidden structure that is present.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, the dimensionality of the raw data is far higher than the number
    of data points we have, so, without dimensionality reduction, it would be difficult
    for other machine learning models, such as classification and regression, to learn
    anything, as they need to fit a number of parameters that is far larger than the
    number of training examples (in this sense, these methods bear some similarity
    to the regularization approaches that we have seen being used in classification
    and regression).
  prefs: []
  type: TYPE_NORMAL
- en: 'A few use cases of dimensionality reduction techniques include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploratory data analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting features to train other machine learning models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducing storage and computation requirements for very large models in the prediction
    phase (for example, a production system that makes predictions)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducing a large group of text documents down to a set of hidden topics or concepts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making learning and generalization of models easier when our data has a very
    large number of features (for example, when working with text, sound, images,
    or video data, which tends to be very high-dimensional)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this chapter, we will do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduce the types of dimensionality reduction models available in MLlib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Work with images of faces to extract features suitable for dimensionality reduction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train a dimensionality reduction model using MLlib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualize and evaluate the results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform parameter selection for our dimensionality reduction model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types of dimensionality reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MLlib provides two models for dimensionality reduction; these models are closely
    related to each other. These models are **Principal Components Analysis** (**PCA**)
    and **Singular Value Decomposition** (**SVD**).
  prefs: []
  type: TYPE_NORMAL
- en: Principal components analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PCA operates on a data matrix *X*, and seeks to extract a set of *k* principal
    components from *X*. The principal components are each uncorrelated to each other,
    and are computed such that the first principal component accounts for the largest
    variation in the input data. Each subsequent principal component is, in turn,
    computed such that it accounts for the largest variation, provided that it is
    independent of the principal components computed so far.
  prefs: []
  type: TYPE_NORMAL
- en: In this way, the *k* principal components returned are guaranteed to account
    for the highest amount of variation in the input data possible. Each principal
    component, in fact, has the same feature dimensionality as the original data matrix.
    Hence, a projection step is required in order to actually perform dimensionality
    reduction, where the original data is projected into the *k*-dimensional space
    represented by the principal components.
  prefs: []
  type: TYPE_NORMAL
- en: Singular value decomposition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'SVD seeks to decompose a matrix *X* of dimension *m x n* into these three component
    matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '*U* of dimension *m x m*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*S*, a diagonal matrix of size *m x n*; the entries of *S* are referred to
    as the **singular values**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*VT* of dimension *n x n*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*X = U * S * V ^T*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at the preceding formula, it appears that we have not reduced the dimensionality
    of the problem at all, as by multiplying *U*, *S*, and *V*, we reconstruct the
    original matrix. In practice, the truncated SVD is usually computed. That is,
    only the top k singular values, which represent the most variation in the data,
    are kept, while the rest are discarded. The formula to reconstruct *X* based on
    the component matrices is then approximate, and is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*X ~ U[k] * S[k] * V[k T]*'
  prefs: []
  type: TYPE_NORMAL
- en: 'An illustration of the truncated SVD is shown in this diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_09_001.png)'
  prefs: []
  type: TYPE_IMG
- en: The truncated singular value decomposition
  prefs: []
  type: TYPE_NORMAL
- en: Keeping the top *k* singular values is similar to keeping the top *k* principal
    components in PCA. In fact, SVD and PCA are directly related, as we will see a
    little later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: A detailed mathematical treatment of both PCA and SVD is beyond the scope of
    this book.
  prefs: []
  type: TYPE_NORMAL
- en: An overview of dimensionality reduction can be found in the Spark documentation
    at [http://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html](http://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following links contain a more in-depth mathematical overview of PCA and
    SVD respectively: [http://en.wikipedia.org/wiki/Principal_component_analysis](http://en.wikipedia.org/wiki/Principal_component_analysis) and [http://en.wikipedia.org/wiki/Singular_value_decomposition](http://en.wikipedia.org/wiki/Singular_value_decomposition).'
  prefs: []
  type: TYPE_NORMAL
- en: Relationship with matrix factorization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PCA and SVD are both matrix factorization techniques, in the sense that they
    decompose a data matrix into subcomponent matrices, each of which has a lower
    dimension (or rank) than the original matrix. Many other dimensionality reduction
    techniques are based on matrix factorization.
  prefs: []
  type: TYPE_NORMAL
- en: 'You might remember another example of matrix factorization, that is, collaborative
    filtering, which we have already seen in [Chapter 6](d3bf76a8-26be-4db7-8310-b936d220407e.xhtml),
    *Building a Classification Model with Spark*. Matrix factorization approaches
    to collaborative filtering work by factorizing the ratings matrix into two components:
    the user factor matrix and the item factor matrix. Each of these has a lower dimension
    than the original data, so these methods also act as dimensionality reduction
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: Many of the best performing approaches to collaborative filtering include models
    based on SVD. Simon Funk's approach to the Netflix prize is a famous example.
    You can look it up at [http://sifter.org/~simon/journal/20061211.html](http://sifter.org/~simon/journal/20061211.html).
  prefs: []
  type: TYPE_NORMAL
- en: Clustering as dimensionality reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The clustering models we covered in the previous chapter can also be used for
    a form of dimensionality reduction. This works in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: Assume that we cluster our high-dimensional feature vectors using a K-means
    clustering model, with k clusters. The result is a set of *k* cluster centers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can represent each of our original data points in terms of how far it is
    from each of these cluster centers. That is, we can compute the distance of a
    data point to each cluster center. The result is a set of *k* distances for each
    data point.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These *k* distances can form a new vector of dimension *k*. We can now represent
    our original data as a new vector of lower dimension relative to the original
    feature dimension.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Depending on the distance metric used, this can result in both dimensionality
    reduction and a form of nonlinear transformation of the data, allowing us to learn
    a more complex model, while still benefiting from the speed and scalability of
    a linear model. For example, using a Gaussian or exponential distance function
    can approximate a very complex nonlinear feature transformation.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting the right features from your data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As with all machine learning models we have explored so far, dimensionality
    reduction models also operate on a feature vector representation of our data.
  prefs: []
  type: TYPE_NORMAL
- en: For this chapter, we will dive into the world of image processing, using the
    **Labeled Faces in the Wild** (**LFW**) dataset of facial images. This dataset
    contains over 13,000 images of faces generally taken from the Internet, and belonging
    to well-known public figures. The faces are labeled with the person's name.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting features from the LFW dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to avoid having to download and process a very large dataset, we will
    work with a subset of the images, using people who have names that start with
    an A. This dataset can be downloaded from [http://vis-www.cs.umass.edu/lfw/lfw-a.tgz](http://vis-www.cs.umass.edu/lfw/lfw-a.tgz).
  prefs: []
  type: TYPE_NORMAL
- en: For more details and other variants of the data, visit [http://vis-www.cs.umass.edu/lfw/](http://vis-www.cs.umass.edu/lfw/).
  prefs: []
  type: TYPE_NORMAL
- en: 'The original research paper reference is:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Gary B. Huang*, *Manu Ramesh*, *Tamara Berg*, and *Erik Learned-Miller*. *Labeled
    Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments*.
    University of Massachusetts, Amherst, Technical Report 07-49, October, 2007.'
  prefs: []
  type: TYPE_NORMAL
- en: It can be downloaded from [http://vis-www.cs.umass.edu/lfw/lfw.pdf](http://vis-www.cs.umass.edu/lfw/lfw.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: 'Unzip the data using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This will create a folder called `lfw`, which contains a number of subfolders,
    one for each person.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the face data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use the Spark application to analyze the data. Make sure the data is
    unzipped into the `data` folder as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The actual code is in the `scala` folder, except a few graphs, which are in
    the `python` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now that we've unzipped the data, we face a small challenge. Spark provides
    us with a way to read text files and custom Hadoop input data sources. However,
    there is no built-in functionality to allow us to read images.
  prefs: []
  type: TYPE_NORMAL
- en: Spark provides a method called `wholeTextFiles`, which allows us to operate
    on entire files at once, compared to the `textFile` method that we have been using
    so far, which operates on the individual lines within a text file (or multiple
    files).
  prefs: []
  type: TYPE_NORMAL
- en: We will use the `wholeTextFiles` method to access the location of each file.
    Using these file paths, we will write custom code to load and process the images.
    In the following example code, we will use PATH to refer to the directory in which
    you extracted the `lfw` subdirectory.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use a wildcard path specification (using the * character highlighted
    in the following code snippet) to tell Spark to look for files in each directory
    under the `lfw` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the `first` command might take a little time, as Spark first scans
    the specified directory structure for all available files. Once completed, you
    should see an output similar to the one shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: You will see that `wholeTextFiles` returns an RDD that contains key-value pairs,
    where the key is the file location, while the value is the content of the entire
    text file. For our purposes, we only care about the file path, as we cannot work
    directly with the image data as a string (notice that it is displayed as "binary
    nonsense" in the shell output).
  prefs: []
  type: TYPE_NORMAL
- en: Let's extract the file paths from the RDD. Note that earlier, the file path
    starts with the `file:` text. This is used by Spark when reading files in order
    to differentiate between different filesystems (for example, `file://` for the
    local filesystem, `hdfs://` for HDFS, `s3n://` for Amazon S3, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, we will use custom code to read the images, so we don''t need
    this part of the path. Thus, we will remove it with the following `map` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding function will display the file location with the `file:` prefix
    removed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will see how many files we are dealing with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Running these commands creates a lot of noisy output in Spark, as it outputs
    all the file paths that are read to the console. Ignore this part, but after the
    command has completed, the output should look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: So, we can see that we have `1055` images to work with.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the face data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although there are a few tools available in Scala or Java to display images,
    this is one area where Python and the `matplotlib` library shine. We will use
    Scala to process and extract the images and run our models, and IPython to display
    the actual images.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can run a separate IPython Notebook by opening a new terminal window and
    launching a new notebook as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'If using Python Notebook, you should first execute the following code snippet
    to ensure that the images are displayed inline after each notebook cell (including
    the `%` character): `%pylab inline`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, you can launch a plain IPython console without the web notebook,
    enabling the `pylab` plotting functionality using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The dimensionality reduction techniques in MLlib are only available in Scala
    or Java at the time of writing this book, so we will continue to use the Scala
    Spark shell to run the models. Therefore, you won't need to run a PySpark console.
  prefs: []
  type: TYPE_NORMAL
- en: We have provided the full Python code with this chapter as a Python script as
    well as in the IPython Notebook format. For instructions on installing IPython,
    see the code bundle.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s display the image given by the first path, which we extracted earlier
    using PIL''s image library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the screenshot displayed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_09_002.png)'
  prefs: []
  type: TYPE_IMG
- en: Extracting facial images as vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While a full treatment of image processing is beyond the scope of this book,
    you will need to know a few basics to proceed. Each color image can be represented
    as a three-dimensional array, or matrix, of pixels. The first two dimensions,
    that is the *x* and *y* axes, represent the position of each pixel, while the
    third dimension represents the **Red**, **Blue**, and **Green** (**RGB**) color
    values for each pixel.
  prefs: []
  type: TYPE_NORMAL
- en: A grayscale image only requires one value per pixel (there are no RGB values),
    so it can be represented as a plain two-dimensional matrix. For many image processing
    and machine learning tasks related to images, it is common to operate on grayscale
    images. We will do this here by converting the color images to grayscale first.
  prefs: []
  type: TYPE_NORMAL
- en: It is also a common practice in machine learning tasks to represent an image
    as a vector instead of a matrix. We do this by concatenating each row (or, alternatively,
    each column) of the matrix to form a long vector (this is known as `reshaping`).
    In this way, each raw, grayscale image matrix is transformed into a feature vector,
    which is usable as input to a machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, for us, the built-in Java **Abstract Window Toolkit** (**AWT**)
    contains various basic image-processing functions. We will define a few utility
    functions to perform this processing using the `java.awt` classes.
  prefs: []
  type: TYPE_NORMAL
- en: Loading images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first of these is a function to read an image from a file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This preceding code is available in `Util.scala`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This returns an instance of a `java.awt.image.BufferedImage` class, which stores
    the image data, and provides a number of useful methods. Let''s test it out by
    loading the first image into our Spark shell, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: You should see the image details displayed in the shell.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: There is quite a lot of information here. Of particular interest to us is that
    the image width and height are `250` pixels, and as we can see, there are three
    components (that is, the RGB values) that are highlighted in the preceding output.
  prefs: []
  type: TYPE_NORMAL
- en: Converting to grayscale and resizing the images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The next function we will define will take the image that we have loaded with
    our preceding function, convert the image from color to grayscale, and resize
    the image's width and height.
  prefs: []
  type: TYPE_NORMAL
- en: These steps are not strictly necessary, but both steps are done in many cases
    for efficiency purposes. Using RGB color images instead of grayscale increases
    the amount of data to be processed by a factor of three. Similarly, larger images
    increase the processing and storage overhead significantly. Our raw 250 x 250
    images represent 187,500 data points per image using three color components. For
    a set of 1055 images, this is 197,812,500 data points. Even if stored as integer
    values, each value stored takes 4 bytes of memory, so just 1055 images represent
    around 800 MB of memory! As you can see, image-processing tasks can quickly become
    extremely memory intensive.
  prefs: []
  type: TYPE_NORMAL
- en: If we convert to grayscale and resize the images to, say, 50 x 50 pixels, we
    only require 2500 data points per image. For our 1055 images, this equates to
    10 MB of memory, which is far more manageable for illustrative purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s define our processing function. We will do the grayscale conversion
    and resizing in one step, using the `java.awt.image` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The first line of the function creates a new image of the desired width and
    height, and specifies a grayscale color model. The third line draws the original
    image onto this newly created image. The `drawImage` method takes care of the
    color conversion and resizing for us! Finally, we return the new, processed image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s test this out on our sample image. We will convert it to grayscale,
    and resize it to 100 x 100 pixels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following output on the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from the highlighted output, the image's width and height are
    indeed `100`, and the number of color components is `1`.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will save the processed image to a temporary location so that we can
    read it back and display it using the Python application.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: You should see a result of `true` displayed on your console, indicating that
    you've successfully saved the image to the `aeGray.jpg` file in your `/tmp` directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we will read the image in Python, and use matplotlib to display the
    image. Type the following code into your IPython Notebook or shell (remember that
    this should be open in a new terminal window):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This should display the image (note again, we haven't shown the image here).
    You will see that it is grayscale, and of slightly worse quality as compared to
    the original image. Furthermore, you will notice that the scale of the axes is
    different, representing the new 100 x 100 dimension instead of the original 250
    x 250 size.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_09_003.png)'
  prefs: []
  type: TYPE_IMG
- en: Extracting feature vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The final step in the processing pipeline is to extract the actual feature
    vectors that will be the input to our dimensionality reduction model. As we mentioned
    earlier, the raw grayscale pixel data will be our features. We will form the vectors
    by flattening out the two-dimensional pixel matrix. The `BufferedImage` class
    provides a utility method to do just this, which we will use in our function as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We can then combine these three functions into one utility function, which takes
    a file location together with the desired image's width and height, and returns
    the raw `Array[Double]` value that contains the pixel data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Applying this preceding function to each element of the RDD that contains all
    the image file paths will give us a new RDD that contains the pixel data for each
    image. Let''s do this and inspect the first few elements as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see output similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The final step is to create an MLlib `vector` instance for each image. We will
    cache the RDD to speed up our later computations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: We used the `setName` function earlier to assign an RDD a name. In this case,
    we called it `image-vectors`. This is so that we can later identify it more easily
    when looking at the Spark web interface.
  prefs: []
  type: TYPE_NORMAL
- en: Normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is a common practice to standardize input data prior to running dimensionality
    reduction models, particularly, for PCA. As we did in [Chapter 6](d3bf76a8-26be-4db7-8310-b936d220407e.xhtml),
    *Building a Classification Model with Spark*, we will do this using the built-in
    `StandardScaler` provided by MLlib's `feature` package. We will only subtract
    the mean from the data in this case.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '**Standard Scalar**: It standardizes features by removing the mean, and scaling
    to unit standard using column summary statistics on the samples in the training
    set.'
  prefs: []
  type: TYPE_NORMAL
- en: '`@param``withMean`: `False` by default. This centers the data with the mean
    before scaling. It builds a dense output, so this does not work on sparse input,
    and raises an exception.'
  prefs: []
  type: TYPE_NORMAL
- en: '`@param withStd`: `True` by default. This scales the data to unit standard
    deviation.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Calling `fit` triggers a computation on our `RDD[Vector]`. You should see an
    output similar to the one shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Note that subtracting the mean works for dense input data. In Image processing,
    we always have dense input data, because each pixel has a value. However, for
    sparse vectors, subtracting the mean vector from each input will transform the
    sparse data into dense data. For very high-dimensional input, this will likely
    exhaust the available memory resources, so it is not advisable.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will use the returned `scaler` to transform the raw image vectors
    to vectors with the column means subtracted.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: We mentioned earlier that the resized grayscale images would take up around
    10 MB of memory. Indeed, you can take a look at the memory usage in the Spark
    application monitor storage page by going to `http://localhost:4040/storage/`
    in your web browser.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we gave our RDD of image vectors a friendly name of `image-vectors`,
    you should see something like the following screenshot (note that, as we are using
    `Vector[Double]`, each element takes up 8 bytes instead of 4 bytes; hence, we
    actually use 20 MB of memory):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_09_004.png)'
  prefs: []
  type: TYPE_IMG
- en: Size of image vectors in memory
  prefs: []
  type: TYPE_NORMAL
- en: Training a dimensionality reduction model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dimensionality reduction models in MLlib require vectors as inputs. However,
    unlike clustering that operated on an `RDD[Vector]`, PCA and SVD computations
    are provided as methods on a distributed `RowMatrix` (this difference is largely
    down to syntax, as a `RowMatrix` is simply a wrapper around an `RDD[Vector]`).
  prefs: []
  type: TYPE_NORMAL
- en: Running PCA on the LFW dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have extracted our image pixel data into vectors, we can instantiate
    a new `RowMatrix`.
  prefs: []
  type: TYPE_NORMAL
- en: '`def computePrincipalComponents(k: Int)`: Matrix'
  prefs: []
  type: TYPE_NORMAL
- en: Computes the top `k` principal components. Rows correspond to observations,
    and columns correspond to variables. The principal components are stored as a
    local matrix of size n-by-`k`. Each column corresponds for one principal component,
    and the columns are in descending order of component variance. The row data do
    not need to be "centered" first; it is not necessary for the mean of each column
    to be `0`.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this cannot be computed on matrices with more than `65535` columns.
  prefs: []
  type: TYPE_NORMAL
- en: '`K` is the number of top principal components.'
  prefs: []
  type: TYPE_NORMAL
- en: It returns a matrix of size n-by-k, whose columns are principal components
  prefs: []
  type: TYPE_NORMAL
- en: Annotations
  prefs: []
  type: TYPE_NORMAL
- en: '@Since( "1.0.0" )'
  prefs: []
  type: TYPE_NORMAL
- en: 'Call the `computePrincipalComponents` method to compute the top `K` principal
    components of our distributed matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: You will likely see quite a lot of output on your console while the model runs.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you see warnings such as WARN LAPACK: Failed to load implementation from:
    com.github.fommil.netlib.NativeSystemLAPACK or WARN LAPACK: Failed to load implementation
    from: com.github.fommil.netlib.NativeRefLAPACK, you can safely ignore these.'
  prefs: []
  type: TYPE_NORMAL
- en: This means that the underlying linear algebra libraries used by MLlib could
    not load the native routines. In this case, a Java-based fallback will be used,
    which is slower, but there is nothing to worry about for the purposes of this
    example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the model training is complete, you should see a result that looks similar
    to the following one displayed on the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Visualizing the Eigenfaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have trained our PCA model, what is the result? Let''s inspect
    the dimensions of the resulting matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: As you should see from your console output, the matrix of the principal components
    has `2500` rows and `10` columns.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Recall that the dimension of each image is 50 x 50, so here, we have the top
    10 principal components, each with a dimension identical to that of the input
    images. These principal components can be thought of as the set of latent (or
    hidden) features that capture the greatest variation in the original data.
  prefs: []
  type: TYPE_NORMAL
- en: In facial recognition and image processing, these principal components are often
    referred to as **Eigenfaces**, as PCA is closely related to the eigenvalue decomposition
    of the covariance matrix of the original data.
  prefs: []
  type: TYPE_NORMAL
- en: See [http://en.wikipedia.org/wiki/Eigenface](http://en.wikipedia.org/wiki/Eigenface)
    for more details.
  prefs: []
  type: TYPE_NORMAL
- en: Since each principal component is of the same dimension as the original images,
    each component can itself be thought of and represented as an image, making it
    possible to visualize the Eigenfaces as we would the input images.
  prefs: []
  type: TYPE_NORMAL
- en: As we have often done in this book, we will use functionality from the Breeze
    linear algebra library as well as Python's numpy and matplotlib to visualize the
    Eigenfaces.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will extract the pc variable (an MLlib matrix) into a Breeze `DenseMatrix`
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Breeze provides a useful function within the `linalg` package to write the matrix
    out as a CSV file. We will use this to save the principal components to a temporary
    CSV file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will load the matrix in IPython, and visualize the principal components
    as images. Fortunately, numpy provides a utility function to read the matrix from
    the CSV file we created.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following output, confirming that the matrix we read has
    the same dimensions as the one we saved:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We will need a utility function to display the images, which we define here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: This preceding function is adapted from the LFW dataset example code in the
    **scikit-learn** documentation available at [http://scikit-learn.org/stable/auto_examples/applications/face_recognition.html](http://scikit-learn.org/stable/auto_examples/applications/face_recognition.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now use this function to plot the top 10 Eigenfaces as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'This last command should display the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_09_005.png)'
  prefs: []
  type: TYPE_IMG
- en: Top 10 Eigenfaces
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting the Eigenfaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Looking at the preceding images, we can see that the PCA model has effectively
    extracted recurring patterns of variation, which represent various features of
    the facial images. Each principal component can, as with clustering models, be
    interpreted. Again, like clustering, it is not always straightforward to interpret
    precisely what each principal component represents.
  prefs: []
  type: TYPE_NORMAL
- en: We can see from these images that there appear to be some images that pick up
    directional factors (for example, images 6 and 9), some hone in on hair patterns
    (such as images 4, 5, 7, and 10), while others seem to be somewhat more related
    to facial features such as eyes, nose, and mouth (images 1, 7, and 9).
  prefs: []
  type: TYPE_NORMAL
- en: Using a dimensionality reduction model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is interesting to be able to visualize the outcome of a model in this way;
    however, the overall purpose of using dimensionality reduction is to create a
    more compact representation of the data that still captures the important features
    and variability in the raw dataset. To do this, we need to use a trained model
    to transform our raw data by projecting it into the new, lower-dimensional space
    represented by the principal components.
  prefs: []
  type: TYPE_NORMAL
- en: Projecting data using PCA on the LFW dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will illustrate this concept by projecting each LFW image into a ten-dimensional
    vector. This is done through a matrix multiplication of the image matrix with
    the matrix of principal components. As the image matrix is a distributed MLlib
    `RowMatrix`, Spark takes care of distributing this computation for us through
    the `multiply` function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'This preceding function will give you the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Observe that each image that had a dimension of 2500, has been transformed
    into a vector of size 10\. Let''s take a look at the first few vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: As the projected data is in the form of vectors, we can use the projection as
    an input to another machine learning model. For example, we could use these projected
    inputs together with a set of input data generated from various images without
    faces to train a facial recognition model. Alternatively, we could train a multiclass
    classifier, where each person is a class, thus creating a model that learns to
    identify the particular person that a face belongs to.
  prefs: []
  type: TYPE_NORMAL
- en: The relationship between PCA and SVD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We mentioned earlier that there is a close relationship between PCA and SVD.
    In fact, we can recover the same principal components, and also apply the same
    projection into the space of principal components using SVD.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, the right singular vectors derived from computing the SVD will
    be equivalent to the principal components we have calculated. We can see that
    this is the case by first computing the SVD on our image matrix and comparing
    the right singular vectors to the result of PCA. As was the case with PCA, SVD
    computation is provided as a function on a distributed `RowMatrix`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: We can see that SVD returns a matrix `U` of dimension 1055 x 10, a vector `S`
    of the singular values of length `10`, and a matrix `V` of the right singular
    vectors of dimension 2500 x 10.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The matrix V is exactly equivalent to the result of PCA (ignoring the sign
    of the values and floating point tolerance). We can verify this with this next
    utility function to compare the two by approximately comparing the data arrays
    of each matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'We will test the function on some test data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give you the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s try another test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give you the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can apply our equality function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Both SVD and PCA can be used to calculate the principal components and corresponding
    Eigen/Singular values; the extra step of calculating the covariance matrix can
    lead to numerical rounding off errors while calculating Eigen vectors. SVD summarizes
    the ways in which the data deviates from zero, and PCA summarizes the ways in
    which the data deviates from the mean data sample.
  prefs: []
  type: TYPE_NORMAL
- en: The other relationship that holds is that the multiplication of the matrix `U`
    and vector `S` (or, strictly speaking, the diagonal matrix `S`) is equivalent
    to the PCA projection of our original image data into the space of the top 10
    principal components.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now show that this is indeed the case. We will first use Breeze to
    multiply each vector in `U` by `S`, element-wise. We will then compare each vector
    in our PCA-projected vectors with the equivalent vector in our SVD projection,
    and sum up the number of equal cases, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: This preceding code should display a result of 1055, as we would expect, confirming
    that each row of projected PCA is equal to each row of `projectedSVD`.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the **:*** operator, highlighted in the preceding code, represents
    element-wise multiplication of the vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating dimensionality reduction models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Both PCA and SVD are deterministic models. That is, given a certain input dataset,
    they will always produce the same result. This is in contrast to many of the models
    we have seen so far, which depend on some random element (most often for the initialization
    of model weight vectors, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: Both models are also guaranteed to return the top principal components or singular
    values, and hence, the only parameter is *k*. Like clustering models, increasing
    *k* always improves the model performance (for clustering, the relevant error
    function, while for PCA and SVD, the total amount of variability explained by
    the *k* components). Therefore, selecting a value for *k* is a trade-off between
    capturing as much structure of the data as possible while keeping the dimensionality
    of projected data low.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating k for SVD on the LFW dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will examine the singular values obtained from computing the SVD on our
    image data. We can verify that the singular values are the same for each run,
    and that they are returned in a decreasing order, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'This last code should generate an output similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Singular values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Singular values lets us understand the trade-off between space and time for
    fidelity of the reduction.
  prefs: []
  type: TYPE_NORMAL
- en: As with evaluating values of *k* for clustering, in the case of SVD (and PCA),
    it is often useful to plot the singular values for a larger range of *k*, and
    see where the point on the graph is where the amount of additional variance accounted
    for by each additional singular value starts to flatten out considerably.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will do this by first computing the top 300 singular values, as shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: We will write out the vector S of singular values to a temporary CSV file (as
    we did for our matrix of Eigenfaces previously), and then read it back in our
    IPython console, plotting the singular values for each *k*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see an image displayed similar to the one shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_09_006.png)'
  prefs: []
  type: TYPE_IMG
- en: Top 300 singular values
  prefs: []
  type: TYPE_NORMAL
- en: A similar pattern is seen in the cumulative variation accounted for by the top
    300 singular values (which we will plot on a log scale for the *y* axis).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Full Source code for Python plots can be found at the following link: [https://github.com/ml-resources/spark-ml/tree/branch-ed2/Chapter_09/data/python](https://github.com/ml-resources/spark-ml/tree/branch-ed2/Chapter_09/data/python)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_09_007.png)'
  prefs: []
  type: TYPE_IMG
- en: Cumulative sum of top 300 singular values
  prefs: []
  type: TYPE_NORMAL
- en: We can see that after a certain value range for *k* (around 100 in this case),
    the graph flattens considerably. This indicates that a number of singular values
    (or principal components) equivalent to this value of *k* probably explains enough
    of the variation of the original data.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, if we are using dimensionality reduction to help improve the performance
    of another model, we could use the same evaluation methods used for that model
    to help us choose a value for *k*.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we could use the AUC metric, together with cross-validation, to
    choose both the model parameters for a classification model as well as the value
    of *k* for our dimensionality reduction model. This does come at the expense of
    higher computation cost, however, as we would have to recompute the full model
    training and testing pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored two new unsupervised learning methods, PCA and
    SVD, for dimensionality reduction. We saw how to extract features for, and train,
    these models using facial image data. We visualized the results of the model in
    the form of Eigenfaces, saw how to apply the models to transform our original
    data into a reduced dimensionality representation, and investigated the close
    link between PCA and SVD.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will delve more deeply into techniques for text processing
    and analysis with Spark.
  prefs: []
  type: TYPE_NORMAL
