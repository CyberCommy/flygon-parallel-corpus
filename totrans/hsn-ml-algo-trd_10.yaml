- en: Decision Trees and Random Forests
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树和随机森林
- en: 'In this chapter, we will learn about two new classes of machine learning models:
    decision trees and random forests. We will see how decision trees learn rules
    from data that encodes non-linear relationships between the input and the output
    variables. We will illustrate how to train a decision tree and use it for prediction for
    regression and classification problems, visualize and interpret the rules learned
    by the model, and tune the model''s hyperparameters to optimize the bias-variance
    tradeoff and prevent overfitting. Decision trees are not only important standalone
    models but are also frequently used as components in other models.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习两种新的机器学习模型：决策树和随机森林。我们将看到决策树如何从数据中学习规则，这些规则编码了输入和输出变量之间的非线性关系。我们将说明如何训练决策树并将其用于回归和分类问题的预测，可视化和解释模型学到的规则，并调整模型的超参数以优化偏差-方差权衡并防止过拟合。决策树不仅是重要的独立模型，而且经常被用作其他模型的组成部分。
- en: In the second part of this chapter, we will introduce ensemble models that combine
    multiple individual models to produce a single aggregate prediction with lower
    prediction-error variance. We will illustrate bootstrap aggregation, often called
    bagging, as one of several methods to randomize the construction of individual
    models and reduce the correlation of the prediction errors made by an ensemble's
    components.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第二部分，我们将介绍集成模型，它将多个单独的模型组合起来，产生一个具有较低预测误差方差的单一聚合预测。我们将说明自举聚合，通常称为装袋，作为多种方法之一，用于随机化单个模型的构建，并减少集成组件所产生的预测误差的相关性。
- en: Boosting is a very powerful alternative method that merits its own chapter to
    address a range of recent developments. We will illustrate how bagging effectively
    reduces the variance, and learn how to configure, train, and tune random forests. We
    will see how random forests as an ensemble of a large number of decision trees,
    can dramatically reduce prediction errors, at the expense of some loss in interpretation.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 增强是一个非常强大的替代方法，值得拥有自己的章节来解决一系列最新的发展。我们将说明如何通过装袋有效地减少方差，并学习如何配置、训练和调整随机森林。我们将看到随机森林作为大量决策树的集成，可以显著减少预测误差，但会牺牲一些解释性。
- en: 'In short, in this chapter, we will cover the following:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，在本章中，我们将涵盖以下内容：
- en: How to use decision trees for regression and classification
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用决策树进行回归和分类
- en: How to gain insights from decision trees and visualize the decision rules learned
    from the data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何从决策树中获得见解，并可视化从数据中学到的决策规则
- en: Why ensemble models tend to deliver superior results
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么集成模型往往能够产生更优秀的结果
- en: How bootstrap aggregation addresses the overfitting challenges of decision trees
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何通过自举聚合解决决策树的过拟合挑战
- en: How to train, tune, and interpret random forests
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何训练、调整和解释随机森林
- en: Decision trees
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树
- en: Decision trees are a machine learning algorithm that predicts the value of a
    target variable based on decision rules learned from training data. The algorithm
    can be applied to both regression and classification problems by changing the
    objective function that governs how the tree learns the decision rules.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是一种机器学习算法，它根据从训练数据中学到的决策规则来预测目标变量的值。该算法可以通过改变控制树学习决策规则的目标函数，应用于回归和分类问题。
- en: We will discuss how decision trees use rules to make predictions, how to train
    them to predict (continuous) returns as well as (categorical) directions of price
    movements, and how to interpret, visualize, and tune them effectively.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论决策树如何使用规则进行预测，如何训练它们以预测（连续的）回报以及（分类的）价格走势方向，以及如何有效地解释、可视化和调整它们。
- en: How trees learn and apply decision rules
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 树是如何学习和应用决策规则的
- en: The linear models we studied in [Chapters 7](0cf85bb4-8b3f-4f83-b004-f980f348028b.xhtml),
    *Linear Models* and [Chapter 8](4ea03bd9-a996-462b-803f-2a27365ff636.xhtml), *Time
    Series Models*, learn a set of parameters to predict the outcome using a linear
    combination of the input variables, possibly after transformation by an S-shaped
    link function in the case of logistic regression.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第7章](0cf85bb4-8b3f-4f83-b004-f980f348028b.xhtml)和[第8章](4ea03bd9-a996-462b-803f-2a27365ff636.xhtml)中学习的线性模型，学习一组参数来预测结果，使用输入变量的线性组合，可能在逻辑回归的情况下通过S形链接函数进行转换。
- en: 'Decision trees take a different approach: they learn and sequentially apply
    a set of rules that split data points into subsets and then make one prediction
    for each subset. The predictions are based on the outcome values for the subset
    of training samples that result from the application of a given sequence of rules.
    As we will see in more detail further, classification trees predict a probability
    estimated from the relative class frequencies or the value of the majority class
    directly, whereas regression models compute prediction from the mean of the outcome
    values for the available data points.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树采用了不同的方法：它们学习并顺序地应用一组规则，将数据点分成子集，然后为每个子集进行一次预测。这些预测是基于对训练样本子集的结果值的预测，这些结果值是由给定规则序列的应用所产生的。正如我们将在后面更详细地看到的，分类树预测的是从相对类频率或者直接从多数类的值估计出的概率，而回归模型则从可用数据点的结果值的平均值计算预测。
- en: 'Each of these rules relies on one particular feature and uses a threshold to
    split the samples into two groups with values either below or above the threshold
    with respect to this feature. A binary tree naturally represents the logic of
    the model: the root is the starting point for all samples, nodes represent the
    application of the decision rules, and the data moves along the edges as it is
    split into smaller subsets until arriving at a leaf node where the model makes
    a prediction.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 每个规则都依赖于一个特定的特征，并使用阈值将样本分成两组，其值要么低于要么高于该特征的阈值。二叉树自然地表示了模型的逻辑：根是所有样本的起点，节点代表决策规则的应用，数据沿着边移动，直到到达叶节点，模型才会进行预测。
- en: For a linear model, the parameter values allow for an interpretation of the
    impact of the input variables on the output and the model's prediction. In contrast,
    for a decision tree, the path from the root to the leaves creates transparency
    about how the features and their values lead to specific decisions by the model.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure highlights how the model learns a rule. During training,
    the algorithm scans the features and, for each feature, seeks to find a cutoff
    that splits the data to minimize the loss that results from predictions made using
    the subsets that would result from the split, weighted by the number of samples
    in each subset:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/10fc5ab5-63f9-4998-8b42-c9bb256d839a.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
- en: To build an entire tree during training, the learning algorithm repeats this
    process of dividing the feature space, that is, the set of possible values for
    the *p* input variables, *X[1], X[2], ..., X[p]*, into mutually-exclusive and
    collectively-exhaustive regions, each represented by a leaf node. Unfortunately,
    the algorithm will not be able to evaluate every possible partition of the feature
    space given the explosive number of possible combinations of sequences of features
    and thresholds. Tree-based learning takes a top-down, greedy approach, known as
    recursive binary splitting to overcome this computational limitation.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: This process is recursive because it uses subsets of data resulting from prior
    splits. It is top-down because it begins at the root node of the tree, where all
    observations still belong to a single region and then successively creates two
    new branches of the tree by adding one more split to the predictor space. It is
    greedy because the algorithm picks the best rule in the form of a feature-threshold
    combination based on the immediate impact on the objective function rather than
    looking ahead and evaluating the loss several steps ahead. We will return to the
    splitting logic in the more specific context of regression and classification
    trees because this represents the major difference.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: The number of training samples continues to shrink as recursive splits add new
    nodes to the tree. If rules split the samples evenly, resulting in a perfectly
    balanced tree with an equal number of children for every node, then there would
    be 2^(n )nodes at level *n*, each containing a corresponding fraction of the total
    number of observations. In practice, this is unlikely, so the number of samples
    along some branches may diminish rapidly, and trees tend to grow to different
    levels of depth along different paths.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: To arrive at a prediction for a new observation, the model uses the rules that
    it inferred during training to decide which leaf node the data point should be
    assigned to, and then uses the mean (for regression) or the mode (for classification)
    of the training observations in the corresponding region of the feature space.
    A smaller number of training samples in a given region of the feature space, that
    is, in a given leaf node, reduces the confidence in the prediction and may reflect
    overfitting.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Recursive splitting would continue until each leaf node contains only a single
    sample and the training error has been reduced to zero. We will introduce several
    criteria to limit splits and prevent this natural tendency of decision trees to produce
    extreme overfitting.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: How to use decision trees in practice
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we illustrate how to use tree-based models to gain insight
    and make predictions. To demonstrate regression trees we predict returns, and
    for the classification case, we return to the example of positive and negative
    asset price moves. The code examples for this section are in the notebook `decision_trees`
    unless stated otherwise.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: How to prepare the data
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We use a simplified version of the data set constructed in [Cha](31520630-da72-4cf6-8d84-6a74b7f4f259.xhtml)[pter](31520630-da72-4cf6-8d84-6a74b7f4f259.xhtml)
    *[4](31520630-da72-4cf6-8d84-6a74b7f4f259.xhtml),* *Alpha Factor Research*. It
    consists of daily stock prices provided by Quandl for the 2010-2017 period and
    various engineered features. The details can be found in the `data_prep` notebook
    in the GitHub repo for this chapter. The decision tree models in this chapter
    are not equipped to handle missing or categorical variables, so we will apply
    dummy encoding to the latter after dropping any of the former.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了[Cha](31520630-da72-4cf6-8d84-6a74b7f4f259.xhtml)[pter](31520630-da72-4cf6-8d84-6a74b7f4f259.xhtml)
    *[4](31520630-da72-4cf6-8d84-6a74b7f4f259.xhtml)* *Alpha Factor Research*中构建的数据集的简化版本。它包括Quandl提供的2010-2017年间的每日股票价格和各种工程特征。详情可以在本章的GitHub存储库中的`data_prep`笔记本中找到。本章的决策树模型无法处理缺失或分类变量，因此我们将在删除任何缺失变量后对后者应用虚拟编码。
- en: How to code a custom cross-validation class
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何编写自定义交叉验证类
- en: 'We also construct a custom cross-validation class tailored to the format of
    the data just created, which has pandas MultiIndex with two levels, one for the
    ticker and one for the data:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还构建了一个定制的交叉验证类，以适应刚刚创建的数据格式，该数据具有两个级别的pandas MultiIndex，一个用于股票代码，一个用于数据：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`OneStepTimeSeriesSplit` ensures a split of training and validation sets that
    avoids a lookahead bias by training models using only data up to period *T-1* for
    each stock when validating using data for month *T*. We will only use one-step-ahead
    forecasts.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`OneStepTimeSeriesSplit`确保了训练和验证集的分割，避免了向前偏差，即在验证时使用月份*T*的数据时，对每支股票只使用直到期间*T-1*的数据进行训练。我们只会使用一步预测。'
- en: How to build a regression tree
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何构建回归树
- en: Regression trees make predictions based on the mean outcome value for the training
    samples assigned to a given node and typically rely on the mean-squared error to
    select optimal rules during recursive binary splitting.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 回归树是基于分配给给定节点的训练样本的平均结果值进行预测的，并且通常依赖于均方误差来在递归二元分割过程中选择最佳规则。
- en: Given a training set, the algorithm iterates over the predictors, *X[1], X[2], ..., X[p]*,
    and possible cutpoints, *s[1], s[1], ..., s[N]*, to find an optimal combination.
    The optimal rule splits the feature space into two regions, *{X|X[i] < s[j]}*
    and *{X|X[i] > s[j]**}*, with values for the *X[i]* feature either below or above
    the *s[j]* threshold so that predictions based on the training subsets maximize
    the reduction of the squared residuals relative to the current node.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个训练集，算法会迭代预测变量*X[1], X[2], ..., X[p]*和可能的切点*s[1], s[1], ..., s[N]*，以找到最佳组合。最佳规则将特征空间分割为两个区域，*{X|X[i]
    < s[j]}*和 *{X|X[i] > s[j]**}*，使得基于训练子集的预测最大化相对于当前节点的平方残差的减少。
- en: 'Let''s start with a simplified example to facilitate visualization and only
    use two months of lagged returns to predict the following month, in the vein of
    an AR(2) model from the last chapter:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个简化的例子开始，以便可视化，并且只使用滞后两个月的收益来预测接下来的一个月，就像上一章中的AR(2)模型一样：
- en: '![](img/c36793b6-ed96-4e4d-9baa-aec4d13bbe97.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c36793b6-ed96-4e4d-9baa-aec4d13bbe97.png)'
- en: 'Using `sklearn`, configuring and training a regression tree is very straightforward:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`sklearn`，配置和训练回归树非常简单：
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The OLS summary and a visualization of the first two levels of the decision
    tree reveal the striking differences between the model. The OLS model provides
    three parameters for the intercepts and the two features in line with the linear
    assumption this model makes about the *f* function.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: OLS摘要和决策树的前两个级别的可视化揭示了模型之间的显著差异。OLS模型提供了截距和两个特征的三个参数，符合该模型对*f*函数的线性假设。
- en: 'In contrast, the regression tree chart displays, for each node of the first
    two levels, the feature and threshold used to split the data (note that features
    can be used repeatedly), as well as the current value of the **mean-squared error**
    (**MSE**), the number of samples, and predicted value based on these training
    samples:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，回归树图显示了前两个级别的每个节点使用的特征和阈值来分割数据（注意特征可以重复使用），以及当前的**均方误差**（**MSE**）、样本数量和基于这些训练样本的预测值：
- en: '![](img/ac90c600-e937-4ae5-9a6d-28f6e1cd50df.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ac90c600-e937-4ae5-9a6d-28f6e1cd50df.png)'
- en: The regression tree chart
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 回归树图
- en: The tree chart also highlights the uneven distribution of samples across the
    nodes as the numbers vary between 28,000 and 49,000 samples after only two splits.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 树状图还突出显示了节点之间样本的不均匀分布，因为在仅进行两次分割后，样本数量在28,000和49,000之间变化。
- en: 'To further illustrate the different assumptions about the functional form of
    the relationships between the input variables and the output, we can visualize
    current return predictions as a function of the feature space, that is, as a function
    of the range of values for the lagged returns. The following figure shows the
    current period return as a function of returns one and two periods ago for linear
    regression and the regression tree:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步说明关于输入变量和输出之间关系的不同假设形式，我们可以将当前收益的预测可视化为特征空间的函数，即作为滞后收益值范围的函数。下图显示了线性回归和回归树的当前期收益作为一期和两期前收益的函数：
- en: '![](img/73257312-4779-4370-944d-660c324c9d99.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/73257312-4779-4370-944d-660c324c9d99.png)'
- en: The linear-regression model result on the right side underlines the linearity
    of the relationship between lagged and current returns, whereas the regression
    tree chart on the left illustrates the non-linear relationship encoded in the
    recursive partitioning of the feature space.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧的线性回归模型结果强调了滞后和当前收益之间的线性关系，而左侧的回归树图则说明了特征空间的递归分区中编码的非线性关系。
- en: How to build a classification tree
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何构建分类树
- en: A classification tree works just like the regression version, except that categorical
    nature of the outcome requires a different approach to making predictions and
    measuring the loss. While a regression tree predicts the response for an observation
    assigned to a leaf node using the mean outcome of the associated training samples,
    a classification tree instead uses the mode, that is, the most common class among
    the training samples in the relevant region. A classification tree can also generate
    probabilistic predictions based on relative class frequencies.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 分类树的工作方式与回归版本类似，只是结果的分类性质需要不同的方法来进行预测和测量损失。而回归树会使用相关训练样本的平均结果来预测分配到叶节点的观测的响应，分类树则使用众数，也就是相关区域内训练样本中最常见的类别。分类树还可以基于相对类别频率生成概率预测。
- en: How to optimize for node purity
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何优化节点纯度
- en: When growing a classification tree, we also use recursive binary splitting but,
    instead of evaluating the quality of a decision rule using the reduction of the
    mean-squared error, we can use the classification error rate, which is simply
    the fraction of the training samples in a given (leave) node that do not belong
    to the most common class.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建分类树时，我们也使用递归二元分裂，但是，与使用均方误差减少来评估决策规则的质量不同，我们可以使用分类错误率，它只是给定（叶）节点中不属于最常见类别的训练样本的比例。
- en: 'However, the alternative measures, Gini Index or Cross-Entropy, are preferred
    because they are more sensitive to node purity than the classification error rate.
    Node purity refers to the extent of the preponderance of a single class in a node.
    A node that only contains samples with outcomes belonging to a single class is
    pure and imply successful classification for this particular region of the feature
    space. They are calculated as follows for a classification outcome taking on *K*
    values, *0,1,…,K-1*, for a given node, *m*, that represents a region, *R*[*m*, ]of
    the feature space and where *p[mk]* is the proportion of outcomes of the *k* class
    in the *m* node:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Gini指数或交叉熵等替代测量方法更受欢迎，因为它们对节点纯度的敏感性更高，而不是分类错误率。节点纯度是指节点中单个类别的优势程度。只包含属于单个类别结果的样本的节点是纯净的，并且意味着对特征空间的这个特定区域的成功分类。它们的计算如下，对于取值为*K*的分类结果，*0,1,…,K-1*，对于给定节点*m*，表示特征空间的区域*R*[*m*,]，其中*p[mk]*是节点*m*中类别*k*的结果比例：
- en: '![](img/5bd6d3fd-2e19-4cf1-ad78-a87aaf5f445c.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5bd6d3fd-2e19-4cf1-ad78-a87aaf5f445c.png)'
- en: '![](img/f207630b-8ef5-460d-a93c-a9324ca59ba5.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f207630b-8ef5-460d-a93c-a9324ca59ba5.png)'
- en: Both the Gini Impurity and the Cross-Entropy measure take on smaller values
    when the class proportions approach zero or one, that is, when the child nodes
    become pure as a result of the split and are highest when the class proportions
    are even or 0.5 in the binary case. The chart at the end of this section visualizes
    the values assumed by these two measures and the misclassification error rates
    across the [0, 1] interval of proportions.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 当类别比例接近零或一时，基尼不纯度和交叉熵测量都会取得较小的值，也就是说，当子节点由于分裂而变得纯净时，它们的值最高，而在二元情况下，当类别比例均匀或为0.5时，它们的值最高。本节末尾的图表可视化了这两个测量值以及在[0,
    1]比例区间内的误分类错误率。
- en: How to train a classification tree
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何训练分类树
- en: 'We will now train, visualize, and evaluate a classification tree with up to
    5 consecutive splits using 80% of the samples for training to predict the remaining
    20%. We are taking a shortcut here to simplify the illustration and use the built-in
    `train_test_split`, which does not protect against lookahead bias, as our custom
    iterator. The tree configuration implies up to 2⁵=32 leaf nodes that, on average
    in the balanced case, would contain over 4,300 of the training samples. Take a
    look at the following code:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将训练、可视化和评估一个分类树，使用80%的样本进行训练，以预测剩余的20%。我们在这里采取了一种简化说明的捷径，并使用内置的`train_test_split`，它不会防止前瞻性偏差，作为我们自定义的迭代器。树的配置意味着最多有2⁵=32个叶节点，在平衡的情况下，平均会包含超过4,300个训练样本。看一下以下代码：
- en: '[PRE2]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The output after training the model displays all the `DecisionTreeClassifier`
    parameters that we will address in more detail in the next section when we discuss
    parameter-tuning.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型后的输出显示了我们将在下一节详细讨论的所有`DecisionTreeClassifier`参数，当我们讨论参数调整时，我们将更详细地讨论这些参数。
- en: How to visualize a decision tree
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何可视化决策树
- en: 'You can visualize the tree using the `graphviz` library (see GitHub for installation
    instructions) because `sklearn` can output a description of the tree using the
    `.dot` language used by that library. You can configure the output to include
    feature and class labels and limit the number of levels to keep the chart readable,
    as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`graphviz`库来可视化树（请参阅GitHub获取安装说明），因为`sklearn`可以输出使用该库的`.dot`语言描述树。您可以配置输出以包括特征和类标签，并限制级别的数量以保持图表的可读性，如下所示：
- en: '[PRE3]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The result shows that the model uses a variety of different features and indicates
    the split rules for both continuous and categorical (dummy) variables. The chart
    displays, under the label value, the number of samples from each class and, under
    the label class, the most common class (there were more up months during the sample
    period):'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，模型使用了各种不同的特征，并指示了连续和分类（虚拟）变量的分裂规则。图表显示了每个类别的样本数量，并在类别标签下显示了最常见的类别（在样本期间有更多的上升月份）。
- en: '![](img/4f29972b-e427-42ee-8046-af32e27007f2.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4f29972b-e427-42ee-8046-af32e27007f2.png)'
- en: How to evaluate decision tree predictions
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何评估决策树预测
- en: 'To evaluate the predictive accuracy of our first classification tree, we will
    use our test set to generate predicted class probabilities, as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们第一个分类树的预测准确性，我们将使用测试集生成预测的类别概率，如下所示：
- en: '[PRE4]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The `.predict_proba()` method produces one probability for each class. In the
    binary class, these probabilities are complementary and sum to 1, so we only need
    the value for the positive class. To evaluate the generalization error, we will
    use the area under the curve based on the receiver-operating characteristic that
    we introduced in [Chapter 6](3efbd9df-a459-406a-a86e-1cb5512a9122.xhtml), *The
    Machine Learning Process*. The result indicates a significant improvement above
    and beyond the baseline value of 0.5 for a random prediction:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Feature importance
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision trees can not only be visualized to inspect the decision path for a
    given feature, but also provide a summary measure of the contribution of each
    feature to the model fit to the training data.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: The feature importance captures how much the splits produced by the feature
    helped to optimize the model's metric used to evaluate the split quality, which
    in our case is the Gini Impurity index. A feature's importance is computed as
    the (normalized) total reduction of this metric and takes into account the number
    of samples affected by a split. Hence, features used earlier in the tree where
    the nodes tend to contain more samples typically are considered of higher importance.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 'The following chart shows the feature importance for the top 15 features:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4184425b-a288-43ee-ae49-602e4c77c664.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
- en: Overfitting and regularization
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision trees have a strong tendency to overfit, especially when a dataset
    has a large number of features relative to the number of samples. As discussed
    in previous chapters, overfitting increases the prediction error because the model
    does not only learn the signal contained in the training data, but also the noise.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several ways to address the risk of overfitting:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '**Dimensionality reduction** ([Chapter 12](c187906e-9fde-4f85-b709-df88dd0f7e88.xhtml),
    *Unsupervised Learning*) improves the feature-to-sample ratio by representing
    the existing features with fewer, more informative, and less noisy features.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ensemble models**, such as random forests, combine multiple trees while randomizing
    the tree construction, as we will see in the second part of this chapter.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees provide several **regularization** hyperparameters to limit the
    growth of a tree and the associated complexity. While every split increases the
    number of nodes, it also reduces the number of samples available per node to support
    a prediction. For each additional level, twice the number of samples is needed
    to populate the new nodes with the same sample density.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tree-pruning** is an additional tool to reduce the complexity of a tree by
    eliminating nodes or entire parts of a tree that add little value but increase
    the model''s variance. Cost-complexity-pruning, for instance, starts with a large
    tree and recursively reduces its size by replacing nodes with leaves, essentially
    running the tree construction in reverse. The various steps produce a sequence
    of trees that can then be compared using cross-validation to select the ideal
    size.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to regularize a decision tree
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following table lists key parameters available for this purpose in the
    sklearn decision tree implementation. After introducing the most important parameters,
    we will illustrate how to use cross-validation to optimize the hyperparameter
    settings with respect to the bias-variance tradeoff and lower prediction errors:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: <tdDefault
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '| **Parameter** | **Options** | **Description** |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
- en: '| `**max_depth**` | None | int | Maximum number of levels: split nodes until
    reaching `max_depth` or all leaves are pure or contain fewer than `min_samples_split`
    samples. |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
- en: '| `**max_features**` | None | None: all features; int float: fraction'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: 'auto, sqrt: sqrt(n_features)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'log2: log2(n_features) | Number of features to consider for a split. |'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '| `**max_leaf_nodes**` | None | None: unlimited number of leaf nodes int |
    Split nodes until creating this many leaves. |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
- en: '| `**min_impurity_decrease**` | 0 | float | Split node if impurity decreases
    by at least this value. |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '`**min_impurity_decrease**` | 0 | float | 如果不纯度至少减少了这个值，就分裂节点。'
- en: '| `**min_samples_leaf**` | 1 | int;float (as a percentage of N) | Minimum number
    of samples to be at a leaf node. A split will only be considered if there are
    at least `min_samples_leaf` training samples in each of the left and right branches.
    May smoothen the model, especially for regression. |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '`**min_samples_leaf**` | 1 | int;float (as a percentage of N) | 叶子节点上的最小样本数。只有在左右分支中的每个分支中至少有`min_samples_leaf`训练样本时，才会考虑分裂。可能会使模型平滑，特别是对于回归问题。'
- en: '| `**min_samples_split**` | 2 | int; float (percent of N) | The minimum number
    of samples required to split an internal node: |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '`**min_samples_split**` | 2 | int; float (percent of N) | 分裂内部节点所需的最小样本数： |'
- en: '| `**min_weight_fraction_leaf**` | 0 |   | The minimum weighted fraction of
    the sum total of all sample weights needed at a leaf node. Samples have equal
    weight unless `sample_weight` provided in fit method. |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '`**min_weight_fraction_leaf**` | 0 |   | 叶子节点所需的所有样本权重总和的最小加权分数。除非在拟合方法中提供了`sample_weight`，否则样本权重相等。'
- en: The `max_depth` parameter imposes a hard limit on the number of consecutive
    splits and represents the most straightforward way to cap the growth of a tree.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_depth`参数对连续分裂的次数施加了硬性限制，是限制树生长的最直接方式。'
- en: The `min_samples_split` and `min_samples_leaf` parameters are alternative, data-driven
    ways to limit the growth of a tree. Rather than imposing a hard limit on the number
    of consecutive splits, these parameters control the minimum number of samples
    required to further split the data. The latter guarantees a certain number of
    samples per leaf, while the former can create very small leaves if a split results
    in a very uneven distribution. Small parameter values facilitate overfitting,
    while a high number may prevent the tree from learning the signal in the data.
    The default values are often quite low, and you should use cross-validation to
    explore a range of potential values. You can also use a float to indicate a percentage
    as opposed to an absolute number.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`min_samples_split` 和 `min_samples_leaf` 参数是限制树生长的替代、数据驱动的方式。这些参数控制了进一步分裂数据所需的最小样本数，而不是对连续分裂次数施加硬性限制。后者保证了每个叶子节点的一定数量样本，而前者则可能导致非常小的叶子节点，如果分裂导致了非常不均匀的分布。小的参数值有利于过拟合，而较高的数值可能阻止树学习数据中的信号。默认值通常相当低，您应该使用交叉验证来探索一系列潜在数值。您还可以使用浮点数来表示百分比，而不是绝对数值。'
- en: The sklearn documentation contains additional details about how to use the various
    parameters for different use cases; see GitHub references.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: sklearn文档中包含了如何在不同用例中使用各种参数的其他详细信息；请参阅GitHub参考资料。
- en: Decision tree pruning
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树修剪
- en: Recursive binary-splitting will likely produce good predictions on the training
    set but tends to overfit the data and produce poor generalization performance
    because it leads to overly complex trees, reflected in a large number of leaf
    nodes or partitioning of the feature space. Fewer splits and leaf nodes imply
    an overall smaller tree and often lead to better predictive performance as well
    as interpretability.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 递归二分法分裂可能会在训练集上产生良好的预测，但往往会导致过拟合数据和产生较差的泛化性能，因为它会导致过于复杂的树，反映在大量叶子节点或特征空间的分区中。较少的分裂和叶子节点意味着整体较小的树，通常会导致更好的预测性能和可解释性。
- en: One approach to limit the number of leaf nodes is to avoid further splits unless
    they yield significant improvements of the objective metric. The downside of this strategy,
    however, is that sometimes splits that result in small improvements enable more
    valuable splits later on as the composition of the samples keeps changing.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 限制叶子节点数量的一种方法是除非它们对目标指标的改进显著，否则避免进一步分裂。然而，这种策略的缺点是，有时导致小幅改进的分裂会使后来的样本组合发生更有价值的分裂。
- en: Tree-pruning, in contrast, starts by growing a very large tree before removing
    or pruning nodes to reduce the large tree to a less complex and overfit subtree.
    Cost-complexity-pruning generates a sequence of subtrees by adding a penalty for
    adding leaf nodes to the tree model and a regularization parameter, similar to
    the lasso and ridge linear-regression models, that modulates the impact of the
    penalty. Applied to the large tree, an increasing penalty will automatically produce
    a sequence of subtrees. Cross-validation of the regularization parameter can be
    used to identify the optimal, pruned subtree.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，树修剪是通过先生长一个非常大的树，然后删除或修剪节点，将大树减少为一个不太复杂和过拟合的子树。成本复杂度修剪通过对树模型添加叶子节点的惩罚和一个正则化参数生成一系列子树，类似于套索和岭线性回归模型，调节惩罚的影响。应用于大树，增加的惩罚将自动生成一系列子树。可以使用正则化参数的交叉验证来识别最佳的修剪子树。
- en: This method is not yet available in sklearn; see references on GitHub for further
    details and ways to manually implement pruning.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法目前在sklearn中还不可用；有关详细信息和手动实现修剪的方法，请参阅GitHub上的参考资料。
- en: How to tune the hyperparameters
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何调整超参数
- en: Decision trees offer an array of hyperparameters to control and tune the training
    result. Cross-validation is the most important tool to obtain an unbiased estimate
    of the generalization error, which in turn permits an informed choice among the
    various configuration options. sklearn offers several tools to facilitate the
    process of cross-validating numerous parameter settings, namely the `GridSearchCV`
    convenience class that we will illustrate in the next section. Learning curves
    also allow for diagnostics that evaluate potential benefits of collecting additional
    data to reduce the generalization error.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树提供了一系列超参数来控制和调整训练结果。交叉验证是获得对泛化误差的无偏估计的最重要工具，从而允许在各种配置选项中做出明智的选择。sklearn提供了几种工具来简化交叉验证多个参数设置的过程，即我们将在下一节中介绍的`GridSearchCV`便利类。学习曲线还允许进行诊断，评估收集额外数据以减少泛化误差的潜在好处。
- en: GridsearchCV for decision trees
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树的GridsearchCV
- en: '`sklearn` provides a method to define ranges of values for multiple hyperparameters.
    It automates the process of cross-validating the various combinations of these
    parameter values to identify the optimal configuration. Let''s walk through the
    process of automatically tuning your model.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to instantiate a model object and define a dictionary where
    the keywords name the hyperparameters, and the values list the parameter settings
    to be tested:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Then, instantiate the `GridSearchCV` object, providing the estimator object
    and parameter grid, as well as a scoring method and cross-validation choice to
    the initialization method. We'll use an object of our custom `OneStepTimeSeriesSplit`
    class, initialized to use ten folds for the `cv` parameter, and set the scoring
    to the `roc_auc` metric. We can parallelize the search using the `n_jobs` parameter
    and automatically obtain a trained model that uses the optimal hyperparameters
    by setting `refit=True`.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'With all settings in place, we can fit `GridSearchCV` just like any other model:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The training process produces some new attributes for our `GridSearchCV` object,
    most importantly the information about the optimal settings and the best cross-validation
    score (now using the proper setup that avoids lookahead bias).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'Setting `max_depth` to `13`, `min_samples_leaf` to `500`, and randomly selecting
    only a number corresponding to the square root of the total number of features
    when deciding on a split, produces the best results, with an AUC of `0.5855`:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The automation is quite convenient, but we also would like to inspect how the
    performance evolves for different parameter values. Upon completion of this process,
    the `GridSearchCV` object makes available detailed cross-validation results to
    gain more insights.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: How to inspect the tree structure
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The notebook also illustrates how to run cross-validation more manually to
    obtain custom tree attributes, such as the total number of nodes or leaf nodes
    associated with certain hyperparameter settings. The following function accesses
    the internal `.tree_` attribute to retrieve information about the total node count,
    and how many of these nodes are leaf nodes:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can combine this information with the train and test scores to gain detailed
    knowledge about the model behavior throughout the cross-validation process, as
    follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The result is shown on the left panel of the following chart. It highlights
    the in- and out-of-sample performance across the range of `max_depth` settings,
    alongside a confidence interval around the error metrics. It also shows the number
    of leaf nodes on the right-hand log scale and indicates the best-performing setting
    at 13 consecutive splits, as indicated by the vertical black line.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Learning curves
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A learning curve is a useful tool that displays how the validation and training
    score evolve as the number of training samples evolves.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of the learning curve is to find out whether and how much the model
    would benefit from using more data during training. It is also useful to diagnose
    whether the model's generalization error is more likely driven by bias or variance.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: If, for example, both the validation score and the training score converge to
    a similarly low value despite an increasing training set size, the error is more
    likely due to bias, and additional training data is unlikely to help.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the following visualization:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d5ae417a-2ab6-47c8-91d0-c9b787354234.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
- en: Strengths and weaknesses of decision trees
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Regression and classification trees take a very different approach to prediction
    when compared to the linear models we have explored so far. How do you decide
    which model is more suitable to the problem at hand? Consider the following:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: If the relationship between the outcome and the features is approximately linear
    (or can be transformed accordingly), then linear regression will likely outperform
    a more complex method, such as a decision tree that does not exploit this linear
    structure.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the relationship appears highly non-linear and more complex, decision trees
    will likely outperform the classical models.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果关系看起来非常非线性和更复杂，决策树可能会胜过经典模型。
- en: 'Several advantages have made decision trees very popular:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树非常受欢迎的几个优点：
- en: They are fairly straightforward to understand and to interpret, not least because
    they can be easily visualized and are thus more accessible to a non-technical
    audience. Decision trees are also referred to as white-box models given the high
    degree of transparency about how they arrive at a prediction. Black-box models,
    such as ensembles and neural networks may deliver better prediction accuracy but
    the decision logic is often much more challenging to understand and interpret.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们相当容易理解和解释，部分是因为它们可以很容易地可视化，因此更容易被非技术人员理解。决策树也被称为白盒模型，因为它们在预测到达的方式上具有很高的透明度。黑盒模型，如集成和神经网络，可能提供更好的预测准确性，但决策逻辑往往更难理解和解释。
- en: Decision trees require less data preparation than models that make stronger
    assumptions about the data or are more sensitive to outliers and require data
    standardization (such as regularized regression).
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树需要比对数据做出更强的假设或对数据更敏感的模型少得多的数据准备，并且需要数据标准化（例如正则化回归）。
- en: Some decision tree implementations handle categorical input, do not require
    the creation of dummy variables (improving memory efficiency), and can work with
    missing values, as we will see in [Chapter 11](2fbfa6b5-87f3-49c3-b13a-5ead63471370.xhtml),
    *Gradient Boosting Machines*, but this is not the case for sklearn.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些决策树实现处理分类输入，不需要创建虚拟变量（提高内存效率），并且可以处理缺失值，正如我们将在[第11章](2fbfa6b5-87f3-49c3-b13a-5ead63471370.xhtml)中看到的*梯度提升机*，但这并不适用于sklearn。
- en: Prediction is fast because it is logarithmic in the number of leaf nodes (unless
    the tree becomes extremely unbalanced).
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测速度快，因为它对叶节点数量是对数级的（除非树变得极度不平衡）。
- en: It is possible to validate the model using statistical tests and account for
    its reliability (see GitHub references).
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用统计测试验证模型并考虑其可靠性（请参阅GitHub参考资料）。
- en: 'Decision trees also have several key disadvantages:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树也有一些关键的缺点：
- en: Decision trees have a built-in tendency to overfit to the training set and produce
    a high generalization error. Key steps to address this weakness are pruning (not
    yet supported by sklearn) as well as regularization using the various early-stopping
    criteria outlined in the previous section.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树有一个内置的倾向，即过度拟合训练集并产生高泛化误差。解决这一弱点的关键步骤是修剪（目前sklearn尚不支持），以及使用前一节中概述的各种早停止标准进行正则化。
- en: Closely related is the high variance of decision trees that results from their
    ability to closely adapt to a training set so that minor variations in the data
    can produce wide swings in the structure of the decision trees and, consequently,
    the predictions the model generates. The key mechanism to address the high variance
    of decision trees is the use of an ensemble of randomized decision trees that
    have low bias and produce uncorrelated prediction errors.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与此密切相关的是决策树的高方差，这是由于它们能够紧密适应训练集，以至于数据的轻微变化可能会导致决策树结构和因此模型生成的预测产生巨大波动。解决决策树高方差的关键机制是使用低偏差并产生不相关预测误差的随机决策树的集成。
- en: The greedy approach to decision-tree learning optimizes based on local criteria,
    that is, to reduce the prediction error at the current node and does not guarantee
    a globally optimal outcome. Again, ensembles consisting of randomized trees help
    to mitigate this problem.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树学习的贪婪方法基于局部标准进行优化，即减少当前节点的预测误差，并不能保证全局最优结果。同样，由随机树组成的集成有助于缓解这个问题。
- en: Decision trees are also sensitive to unbalanced class weights and may produce
    biased trees. One option is to oversample the underrepresented or under-sample
    the more frequent class. It is typically better, though, to use class weights
    and directly adjust the objective function.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树也对不平衡的类权重敏感，并可能产生有偏的树。一种选择是过采样代表性不足的类别或者对更频繁的类别进行欠采样。然而，直接使用类权重并直接调整目标函数通常更好。
- en: Random forests
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林
- en: Decision trees are not only useful for their transparency and interpretability
    but are also fundamental building blocks for much more powerful ensemble models
    that combine many individual trees with strategies to randomly vary their design
    to address the overfitting and high variance problems discussed in the preceding
    section.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树不仅因其透明度和可解释性而有用，而且还是更强大的集成模型的基本构建模块，这些模型将许多个体树与策略结合起来，以随机变化其设计，以解决前一节讨论的过度拟合和高方差问题。
- en: Ensemble models
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成模型
- en: Ensemble learning involves combining several machine learning models into a
    single new model that aims to make better predictions than any individual model. More
    specifically, an ensemble integrates the predictions of several base estimators
    trained using one or more given learning algorithms to reduce the generalization
    error that these models may produce on their own.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习涉及将几个机器学习模型组合成一个新的模型，旨在比任何单个模型做出更好的预测。更具体地说，集成将使用一个或多个给定的学习算法训练的几个基本估计器的预测集成在一起，以减少这些模型可能产生的泛化误差。
- en: 'For ensemble learning to achieve this goal, the individual models must be:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一目标，个体模型必须是：
- en: '**Accurate:** They outperform a naive baseline (such as the sample mean or
    class proportions)'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确:** 它们胜过一个天真的基线（如样本均值或类比例）'
- en: '**Independent:** Their predictions are generated differently to produce different
    errors'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**独立:** 它们的预测是以不同方式生成的，以产生不同的错误'
- en: Ensemble methods are among the most successful machine learning algorithms,
    in particular for standard numerical data. Large ensembles are very successful
    in machine learning competitions and may consist of many distinct individual models
    that have been combined by hand or using another machine learning algorithm.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: There are several disadvantages to combining predictions made by different models.
    These include reduced interpretability, and higher complexity and cost of training,
    prediction, and model maintenance. As a result, in practice (outside of competitions),
    the small gains in accuracy from large-scale ensembling may not be worth the added
    costs.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two groups of ensemble methods that are typically distinguished depending
    on how they optimize the constituent models and then integrate the results for
    a single ensemble prediction:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '**Averaging methods **train several base estimators independently and then
    average their predictions. If the base models are not biased and make different
    prediction errors that are not highly correlated, then the combined prediction
    may have lower variance and can be more reliable. This resembles the construction
    of a portfolio from assets with uncorrelated returns to reduce the volatility
    without sacrificing the return.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Boosting methods**, in contrast, train base estimators sequentially with
    the specific goal to reduce the bias of the combined estimator. The motivation
    is to combine several weak models into a powerful ensemble.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will focus on automatic averaging methods in the remainder of this chapter,
    and boosting methods in [Chapter 11](2fbfa6b5-87f3-49c3-b13a-5ead63471370.xhtml),
    *Gradient Boosting Machines*.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: How bagging lowers model variance
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We saw that decision trees are likely to make poor predictions due to high variance,
    which implies that the tree structure is quite sensitive to the composition of
    the training sample. We have also seen that a model with low variance, such as
    linear regression, produces similar estimates despite different training samples
    as long as there are sufficient samples given the number of features.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: For a given a set of independent observations, each with a variance of *σ²*,
    the standard error of the sample mean is given by *σ/n*. In other words, averaging
    over a larger set of observations reduces the variance. A natural way to reduce
    the variance of a model and its generalization error would thus be to collect
    many training sets from the population, train a different model on each dataset,
    and average the resulting predictions.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: In practice, we do not typically have the luxury of many different training
    sets. This is where bagging, short for bootstrap aggregation, comes in. Bagging is
    a general-purpose method to reduce the variance of a machine learning model, which
    is particularly useful and popular when applied to decision trees.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Bagging refers to the aggregation of bootstrap samples, which are random samples
    with replacement. Such a random sample has the same number of observations as
    the original dataset but may contain duplicates due to replacement.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Bagging increases predictive accuracy but decreases model interpretability because
    it's no longer possible to visualize the tree to understand the importance of
    each feature. As an ensemble algorithm, bagging methods train a given number of
    base estimators on these bootstrapped samples and then aggregate their predictions
    into a final ensemble prediction.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Bagging reduces the variance of the base estimators by randomizing how, for
    example, each tree is grown and then averages the predictions to reduce their
    generalization error. It is often a straightforward approach to improve on a given
    model without the need to change the underlying algorithm. It works best with
    complex models that have low bias and high variance, such as deep decision trees,
    because its goal is to limit overfitting. Boosting methods, in contrast, work
    best with weak models, such as shallow decision trees.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several bagging methods that differ by the random sampling process
    they apply to the training set:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Pasting draws random samples from the training data without replacement, whereas
    bagging samples with replacement
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random subspaces randomly sample from the features (that is, the columns) without
    replacement
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random patches train base estimators by randomly sampling both observations
    and features
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bagged decision trees
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To apply bagging to decision trees, we create bootstrap samples from our training
    data by repeatedly sampling with replacement, then train one decision tree on
    each of these samples, and create an ensemble prediction by averaging over the
    predictions of the different trees.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Bagged decision trees are usually grown large, that is, have many levels and
    leaf nodes and are not pruned so that each tree has low bias but high variance.
    The effect of averaging their predictions then aims to reduce their variance.
    Bagging has been shown to substantially improve predictive performance by constructing
    ensembles that combine hundreds or even thousands of trees trained on bootstrap
    samples.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate the effect of bagging on the variance of a regression tree, we
    can use the `BaggingRegressor` meta-estimator provided by `sklearn`. It trains a
    user-defined base estimator based on parameters that specify the sampling strategy:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '`max_samples` and `max_features` control the size of the subsets drawn from
    the rows and the columns, respectively'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bootstrap` and `bootstrap_features` determine whether each of these samples
    is drawn with or without replacement'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following example uses an exponential function to generate training samples
    for a single `DecisionTreeRegressor` and a `BaggingRegressor` ensemble that consists
    of ten trees, each grown ten levels deep. Both models are trained on the random
    samples and predict outcomes for the actual function with added noise.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we know the true function, we can decompose the mean-squared error into
    bias, variance, and noise, and compare the relative size of these components for
    both models according to the following breakdown:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ef67ad78-69ec-4c98-93d2-4e16e807c697.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
- en: 'For 100 repeated random training and test samples of 250 and 500 observations
    each, we find that the variance of the predictions of the individual decision
    tree is almost twice as high as that for the small ensemble of `10` bagged trees
    based on bootstrapped samples:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'For each model, the following plot shows the mean prediction and a band of
    two standard deviations around the mean for both models in the upper panel, and
    the bias-variance-noise breakdown based on the values for the true function in
    the bottom panel:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bcadab8d-75ec-463c-a2e9-dabf9e995e71.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
- en: See the notebook `random_forest`  for implementation details.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: How to build a random forest
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The random forest algorithm expands on the randomization introduced by the bootstrap
    samples generated by bagging to reduce variance further and improve predictive
    performance.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: In addition to training each ensemble member on bootstrapped training data,
    random forests also randomly sample from the features used in the model (without
    replacement). Depending on the implementation, the random samples can be drawn
    for each tree or each split. As a result, the algorithm faces different options
    when learning new rules, either at the level of a tree or for each split.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'The sizes of the feature samples differ for regression and classification trees:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: For **classification**, the sample size is typically the square root of the
    number of features.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For **regression**, it can be anywhere from one-third to all features and should
    be selected based on cross-validation.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram illustrates how random forests randomize the training
    of individual trees and then aggregate their predictions into an ensemble prediction:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/18ee1d20-03cb-45eb-adf8-65b5059a5952.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
- en: The goal of randomizing the features in addition to the training observations
    is to further de-correlate the prediction errors of the individual trees. All
    features are not created equal, and a small number of highly relevant features
    will be selected much more frequently and earlier in the tree-construction process,
    making decision trees more alike across the ensemble. However, the less the generalization
    errors of individual trees correlate, the more the overall variance will be reduced.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 随机化特征除了训练观察值外的目标是进一步去相关化单独树的预测误差。所有特征并非都是平等的，一小部分高度相关的特征将更频繁地被选择，并且在树构建过程中更早地被选择，使得决策树在整个集成中更加相似。然而，单独树的泛化误差相关性越小，整体方差就会减少得越多。
- en: How to train and tune a random forest
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何训练和调整随机森林
- en: 'The key configuration parameters include the various hyperparameters for the
    individual decision trees introduced in the section *How to tune the hyperparameters*. The
    following tables lists additional options for the two `RandomForest` classes:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 关键配置参数包括在*如何调整超参数*部分介绍的各个决策树的超参数。以下表格列出了两个`RandomForest`类的其他选项：
- en: '| **Keyword** | **Default** | **Description** |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| **关键词** | **默认值** | **描述** |'
- en: '| `bootstrap` | `True` | Bootstrap samples during training. |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| `bootstrap` | `True` | 训练过程中使用自助采样。 |'
- en: '| `n_estimators` | `10` | Number of trees in the forest. |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| `n_estimators` | `10` | 森林中树的数量。 |'
- en: '| `oob_score` | `False` | Uses out-of-bag samples to estimate the R² on unseen
    data. |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| `oob_score` | `False` | 使用袋外样本来估计未见数据的R²。 |'
- en: The `bootstrap` parameter activates in the preceding bagging algorithm outline,
    which in turn enables the computation of the out-of-bag score (`oob_score`) that
    estimates the generalization accuracy using samples not included in the bootstrap
    sample used to train a given tree (see next section for detail).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`bootstrap`参数在前述装袋算法概述中激活，从而启用了计算未包含在用于训练给定树的自助样本中的样本的袋外得分(`oob_score`)，用于估计使用未包含在用于训练给定树的自助样本中的样本的泛化准确性（有关详细信息，请参见下一节）。'
- en: The `n_estimators` parameter defines the number of trees to be grown as part
    of the forest. Larger forests perform better, but also take more time to build.
    It is important to monitor the cross-validation error as a function of the number
    of base learners to identify when the marginal reduction of the prediction error
    declines and the cost of additional training begins to outweigh the benefits.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '`n_estimators`参数定义了要作为森林一部分生长的树的数量。较大的森林表现更好，但也需要更多时间来构建。重要的是要监控交叉验证误差作为基本学习者数量的函数，以确定预测误差的边际减少何时下降，额外训练的成本开始超过收益。'
- en: The `max_features` parameter controls the size of the randomly selected feature
    subsets available when learning a new decision rule and split a node. A lower
    value reduces the correlation of the trees and, thus, the ensemble's variance,
    but may also increase the bias. Good starting values are `n_features` (the number
    of training features) for regression problems and `sqrt(n_features)` for classification
    problems, but will depend on the relationships among features and should be optimized
    using cross-validation.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_features`参数控制了在学习新的决策规则和分裂节点时可用的随机选择特征子集的大小。较低的值会减少树之间的相关性，从而减少集成的方差，但也可能增加偏差。对于回归问题，良好的起始值是`n_features`（训练特征的数量），对于分类问题是`sqrt(n_features)`，但这将取决于特征之间的关系，并应使用交叉验证进行优化。'
- en: Random forests are designed to contain deep fully-grown trees, which can be
    created using `max_depth=None` and `min_samples_split=2`. However, these values
    are not necessarily optimal, especially for high-dimensional data with many samples
    and, consequently, potentially very deep trees that can become very computationally-,
    and memory-, intensive.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林旨在包含深度完全生长的树，可以使用`max_depth=None`和`min_samples_split=2`来创建。然而，这些值不一定是最佳的，特别是对于具有许多样本和因此可能非常深的树的高维数据而言，这可能会变得非常计算密集和内存密集。
- en: The `RandomForest` class provided by `sklearn` support parallel training and
    prediction by setting the `n_jobs` parameter to the `k` number of jobs to run
    on different cores. The `-1` value uses all available cores. The overhead of interprocess
    communication may limit the speedup from being linear so that *k* jobs may take
    more than *1/k* the time of a single job. Nonetheless, the speedup is often quite
    significant for large forests or deep individual trees that may take a meaningful
    amount of time to train when the data is large, and split evaluation becomes costly.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn`提供的`RandomForest`类通过将`n_jobs`参数设置为`k`来支持并行训练和预测，其中`k`是要在不同核心上运行的作业数量。值为`-1`使用所有可用核心。进程间通信的开销可能会限制速度提升的线性，因此*k*个作业可能需要比单个作业的*1/k*时间更长。尽管如此，对于大型森林或需要大量时间来训练的深度单独树的数据，以及分裂评估变得昂贵的情况下，速度提升通常是相当显著的。'
- en: 'As always, the best parameter configuration should be identified using cross-validation.
    The following steps illustrate the process:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 与往常一样，应该使用交叉验证来确定最佳参数配置。以下步骤说明了该过程：
- en: 'We will use `GridSearchCV` to identify an optimal set of parameters for an
    ensemble of classification trees:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用`GridSearchCV`来确定一组最佳参数，用于分类树的集成：
- en: '[PRE12]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We will use 10-fold custom cross-validation and populate the parameter grid
    with values for the key configuration settings:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用10折自定义交叉验证，并使用关键配置设置的数值填充参数网格：
- en: '[PRE13]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Configure `GridSearchCV` using the preceding input:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用前述输入配置`GridSearchCV`：
- en: '[PRE14]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Train the multiple ensemble models defined by the parameter grid:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练由参数网格定义的多个集成模型：
- en: '[PRE15]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Obtain the best parameters as follows:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取最佳参数如下：
- en: '[PRE16]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The best score is a small but significant improvement over the single-tree
    baseline:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最佳得分相比单棵树基准有了一些小但显著的改进：
- en: '[PRE17]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Feature importance for random forests
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林的特征重要性
- en: A random forest ensemble may contain hundreds of individual trees, but it is
    still possible to obtain an overall summary measure of feature importance from
    bagged models.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林集成可能包含数百棵单独的树，但仍然可以从装袋模型中获得特征重要性的总体摘要度量。
- en: For a given feature, the importance score is the total reduction in the objective
    function's value, which results from splits based on this feature, averaged over
    all trees. Since the objective function takes into account how many features are
    affected by a split, this measure is implicitly a weighted average so that features
    used near the top of a tree will get higher scores due to the larger number of
    observations contained in the much smaller number of available nodes. By averaging
    over many trees grown in a randomized fashion, the feature importance estimate
    loses some variance and becomes more accurate.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: The computation differs for classification and regression trees based on the
    different objectives used to learn the decision rules and is measured in terms
    of the mean square error for regression trees and the Gini index or entropy for
    classification trees.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '`sklearn` further normalizes the feature-importance measure so that it sums
    up to `1`. Feature importance thus computed is also used for feature selection
    as an alternative to the mutual information measures we saw in [Chapter 6](3efbd9df-a459-406a-a86e-1cb5512a9122.xhtml),
    *T**he Machine Learning Process* (see `SelectFromModel` in the `sklearn.feature_selection` module).'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, the importance values for the top-20 features are as shown
    here:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/22539933-6f9f-4069-8529-7cefdf1ba137.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
- en: Feature-importance values
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Out-of-bag testing
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Random forests offer the benefit of built-in cross-validation because individual
    trees are trained on bootstrapped versions of the training data. As a result,
    each tree uses on average only two-thirds of the available observations. To see
    why, consider that a bootstrap sample has the same size, *n*, as the original
    sample, and each observation has the same probability, *1/n*, to be drawn. Hence,
    the probability of not entering a bootstrap sample at all is *(1-1/n)**^n*, which
    converges (quickly) to *1/e*, or roughly one-third.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: This remaining one-third of the observations that are not included in the training
    set used to grow a bagged tree is called **out-of-bag** (**OOB**) observations
    and can serve as a validation set. Just as with cross-validation, we predict the
    response for an OOB sample for each tree built without this observation, and then
    average the predicted responses (if regression is the goal) or take a majority
    vote or predicted probability (if classification is the goal) for a single ensemble
    prediction for each OOB sample. These predictions produce an unbiased estimate
    of the generalization error, conveniently computed during training.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: The resulting OOB error is a valid estimate of the generalization error for
    this observation because the prediction is produced using decision rules learned
    in the absence of this observation. Once the random forest is sufficiently large,
    the OOB error closely approximates the leave-one-out cross-validation error. The
    OOB approach to estimate the test error is very efficient for large datasets where
    cross-validation can be computationally costly.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Pros and cons of random forests
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Bagged ensemble models have both advantages and disadvantages. The advantages
    of random forests include:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: The predictive performance can compete with the best supervised learning algorithms
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They provide a reliable feature importance estimate
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They offer efficient estimates of the test error without incurring the cost
    of repeated model training associated with cross-validation
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On the other hand, random forests also have a few disadvantages:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: An ensemble model is inherently less interpretable than an individual decision
    tree
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a large number of deep trees can have high computational costs (but
    can be parallelized) and use a lot of memory
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predictions are slower, which may create challenges for applications that require low
    latency
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about a new class of models capable of capturing
    a non-linear relationship, in contrast to the classical linear models we had explored
    so far. We saw how decision trees learn rules to partition the feature space into
    regions that yield predictions and thus segment the input data into specific regions.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了一类能够捕捉非线性关系的新模型，与我们迄今为止探索过的经典线性模型形成对比。我们看到决策树如何学习规则，将特征空间划分为产生预测的区域，从而将输入数据分割成特定的区域。
- en: Decision trees are very useful because they provide unique insights into the
    relationships between features and target variables, and we saw how to visualize
    the sequence of decision rules encoded in the tree structure.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树非常有用，因为它们提供了关于特征和目标变量之间关系的独特见解，我们看到了如何可视化树结构中编码的决策规则序列。
- en: Unfortunately, a decision tree is prone to overfitting. We learned that ensemble
    models and the bootstrap aggregation method manages to overcome some of the shortcomings
    of decision trees and render them useful, as components of much more powerful
    composite models.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，决策树容易过拟合。我们了解到集成模型和自举聚合方法成功地克服了决策树的一些缺点，并使它们成为更强大的复合模型的组成部分。
- en: In the next chapter, we will explore another ensemble model, which has come
    to be considered one of the most important machine learning algorithms.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨另一个集成模型，这个模型被认为是最重要的机器学习算法之一。
