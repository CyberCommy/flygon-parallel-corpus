- en: Chapter 7. Managing Containers on Your Cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover one of the currently most discussed technology
    approaches, containers. The popularity around containers continues to build and
    rightfully so, who does not want an easier method of deploying applications and
    a consolidated approach to consuming computing resources? The best analogy I like
    to use, outside of the obvious shipping container on a ship analogy, when talking
    about containers is imagine putting all your code into a car or SUV. Then a vehicle
    carrier shows up at your door to pick up your vehicle. The maintenance for your
    vehicle will be minimal to none since the vehicle carrier is doing all the work.
    You only need to worry about making sure the vehicle carrier is working. This
    is the principle behind containers, we will go deeper into the container concept
    and also learn how you can leverage OpenStack and/or Ansible to build and deploy
    them. As in our usual fashion, as we go through each section we will be creating
    a few Ansible examples of how you can possibly manage various different container
    formats. We will cover the following topic in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Container concept explained
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build and deploy containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building containers with Ansible Container
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying Kubernetes on OpenStack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing CoreOS and Docker with Ansible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying Nova LXD on OpenStack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reviewing playbooks and roles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The container concept explained
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I have to believe that most folks that are into technology already know what
    containerization (aka containers) is, but in the freakish chance that my assumption
    is wrong, it felt like a good idea to start off by explaining what exactly it
    is. I will try to do my best not to just give you the Wikipedia definition and
    try to put some solid meaning around why the container model is a very useful
    addition to resource virtualization.
  prefs: []
  type: TYPE_NORMAL
- en: With the start of traditional virtualization, folks realized that I can sort
    of slice up my server into consumable chunks. No more is the need to dedicate
    a whole server to just being a web or application server. Quickly came on the
    adoption of cloud as many started to realize they were not using those virtualized
    resources correctly. Virtual machines sitting out there with no use or with too
    many resources that were not needed. One of the clouds major selling points is
    the fact that you can consume only what you need and that those resources are
    meant to be disposable, meaning use it and then dispose of it. All along while
    these technologies made consuming compute resources easier, none of them really
    helped in improving how you deploy applications.
  prefs: []
  type: TYPE_NORMAL
- en: Remember why you need those VM and instances, it is to run applications. What
    is the point of getting your hands on resources faster if it still takes days
    to still deploy a new application? In my opinion, this is the basis of why the
    containerization approach was crafted. Developers and system administrators (mainly
    system admins) wanted a more efficient method of deploying applications. I can
    personally remember the extremely painful process of deploying a new application
    or API. It consisted of attempting to step through a deployment document written
    by a developer, who most likely had never even logged into a server before or
    managed web/application server software before. Let's just say it was riddled
    with missed steps, wrong commands, and could never take into account any environment
    modifications that could be needed (such as dependent software versions).
  prefs: []
  type: TYPE_NORMAL
- en: Fast forwarding to the current time where you now have more options. There are
    quite a few different container technologies that now allow for developers to
    package up an application into a container and literally *ship it* to your container
    platform of choice. No more deployment documents, no more 2 AM deployment parties,
    and most importantly no more deployment mistakes. Since the container consists
    of a complete runtime environment for your applications all you need to manage
    is the container technology itself and the operating system it runs on. Containers
    are then also very easily moved between environments or across systems, as the
    only dependency is a server running the same container technology.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have some facts on containers you have to choose the platform that
    best suits your needs. Some of the most popular container technologies are Docker
    ([https://www.docker.com](https://www.docker.com)), Kubernetes ([http://kubernetes.io](http://kubernetes.io)),
    CoreOS ([https://coreos.com](https://coreos.com)) and LXC/LXD ([https://linuxcontainers.org](https://linuxcontainers.org)).
  prefs: []
  type: TYPE_NORMAL
- en: So before you ask, you are probably thinking that since containers are relatively
    new can it be trusted, has the containerization concept been proven to work? Well
    the answer is yes, due to the simple fact that containers are not a new concept.
    Containers, or the concept of containerization, has been around for 10 years.
    The first container technology was LXC and it has been part of the Linux kernel
    for years now. With that said I would certainly say it has been tested true and
    certainly a technology to add to your organization's portfolio.
  prefs: []
  type: TYPE_NORMAL
- en: We can now embark on our journey of exploring containers further and strategize
    on how you can automate building and deploying them on your OpenStack cloud. The
    first path we need to go down in our journey is building our first container.
  prefs: []
  type: TYPE_NORMAL
- en: Building and deploying containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will learn how to design, build, and deploy containers
    to various container technologies. The breakdown of topics we will cover here
    are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Building Containers with Ansible Container
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying Kubernetes on OpenStack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing CoreOS and Docker with Ansible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying Nova LXD on OpenStack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As mentioned previously, we will first start with learning how to build our
    first container using what I consider to be the easiest container tool there is,
    Ansible Container. Hope you are excited because I surely am, let's go!
  prefs: []
  type: TYPE_NORMAL
- en: Building containers with Ansible Container
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What is Ansible Container?
  prefs: []
  type: TYPE_NORMAL
- en: '*Ansible describes Ansible Container as "the ultimate workflow for container
    development, testing, and deployment"                                        
             –  ([https://docs.ansible.com/ansible-container](https://docs.ansible.com/ansible-container))*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Consider it a workflow tool that enables you to not only build Docker images
    but also orchestrate the deployment of them and applications using Ansible playbooks.
    I will give you a second to pull yourself together now. Yes, our friends at Ansible
    have done it again and delivered another fantastic tool to put in our toolbox.
    No more having to be solely dependent on Dockerfile. All the power Ansible bring
    to the table can now be directly combined with building, running, deploying, and
    even pushing container images to the registry of choice. Everything you would
    like to know about Ansible Container can be found here: [http://docs.ansible.com/ansible-container](http://docs.ansible.com/ansible-container).'
  prefs: []
  type: TYPE_NORMAL
- en: Just like with every other tool Ansible enables the center focus around Ansible
    Container, to be, simplicity and ease of use. Personally, I was able to install
    it and deploy my first container in just a few hours. One of the key features
    to Ansible Container is the ability to leverage shared container builds from Ansible
    Galaxy ([https://galaxy.ansible.com/intro](https://galaxy.ansible.com/intro))
    to get a jump start on designing your container images. Remember Open Source is
    all about sharing with the community.
  prefs: []
  type: TYPE_NORMAL
- en: Automation considerations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first step is to get it installed, and since Ansible documentation is like
    none other there would be no need for me to reinvent the wheel. The installation
    options and details can be found at the following location: [http://docs.ansible.com/ansible-container/installation.html](http://docs.ansible.com/ansible-container/installation.html).
    After you get it running the very next step I would suggest is to review the Getting
    Started guide found here: [http://docs.ansible.com/ansible-container/getting_started.html](http://docs.ansible.com/ansible-container/getting_started.html).'
  prefs: []
  type: TYPE_NORMAL
- en: We will now step through an example Ansible Container project that I have created
    to get started. This to me is the best way of learning a new technology. Spend
    some time with it, get your hands dirty, and come out more knowledgeable.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Getting started with an Ansible Container project starts with something as
    simple as creating a new directory. Once the new directory has been created you
    then need to move into that directory and execute the Ansible Container initialization
    command. A working example of those commands is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the command would resemble this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Step 1](graphics/image_07_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the example shown, our project will be named `elk-containers` and will be
    initialized in a directory named the same. Now that you have initialized your
    project, you will find that the Ansible Container files were created in a directory
    named `ansible`. The directory structure for your project will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The files created here are skeleton files providing a shell to get you started.
    If you examine the two most important files, `container.yml` and `main.yml`, they
    will look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '**container.yml**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**main.yml**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Step 2
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now we are ready to either manually configure our container and/or leverage
    any of the many pre-packaged Ansible Container configs hosted on Ansible Galaxy.
    For our example here, we will pull down and use three different configs from Ansible
    Galaxy. Our example project will deploy three containers that collectively will
    be running the ELK stack (Elasticsearch, Logstash, and Kibana).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before executing the commands below, please make sure you have installed Ansible
    Container and all the prerequisite software.  Refer to the Ansible Container installation
    instructions for details: [https://docs.ansible.com/ansible-container/installation.html](https://docs.ansible.com/ansible-container/installation.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The commands to handle this are mentioned here; make sure you are in the `root`
    directory of your project directory when executed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the command would resemble this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Step 2](graphics/image_07_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Once the base image is downloaded, Ansible Container will load it into a virtual
    container with all possible image dependencies to ready it to be built.
  prefs: []
  type: TYPE_NORMAL
- en: '![Step 2](graphics/image_07_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Step 3
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Next we will now review what the previous `ansible-container` install commands
    did to our project. If we take a look at our `container.yml` and `main.yml` files
    now, we will notice that all the automation code we need to deploy the ELK stack
    to the containers is now there. Let''s take a look at the changes to those files:'
  prefs: []
  type: TYPE_NORMAL
- en: '**container.yml**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '**main.yml**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The other file that we need to examine as well now is the `requirements.yml`
    file. Since we are using pre-packaged configs, a link to those configs will be
    added in this file:'
  prefs: []
  type: TYPE_NORMAL
- en: '**requirements.yml**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: At this point you have the option to make changes to the files if you have the
    need to adjust variables, specific application changes, or add additional orchestration
    steps. The best thing out of all of this is you can also choose not to make any
    changes at all. You can build and run this container project just as it is.
  prefs: []
  type: TYPE_NORMAL
- en: Step 4
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In our final step, here we will take what we have designed, execute the Ansible
    Container build process, and finally deploy those containers locally. Again, for
    our example, we did not need to make any changes to container design files.
  prefs: []
  type: TYPE_NORMAL
- en: 'The build process is very powerful as all the container dependencies and orchestration
    will be implemented in order to create the container images. Those images will
    be used when you wish to deploy the containers. The following is the command to
    be used to build our containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'A snippet of the output of the command would resemble this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Step 4](graphics/B06086_07_04_resized-769x1024.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Last, and certainly not least, we are ready to test out our brand new containers.
    As they say in the containerization world, *just ship it!.* Using Ansible Container
    to deploy the container images locally in order to test them out is yet another
    feature that just makes perfect sense. The `ansible-container run` command is
    what you will use to deploy the containers to your locally configured **Docker
    Engine** installation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Once run, the output of the command would resemble this, and we can confirm
    our container deployment by executing the `docker ps command`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Step 4](graphics/image_07_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, we have done well as we now have three containers running locally.
    *We did it*! Our very first container has been designed, configured, built, and
    deployed (all in less than an hour nevertheless). Before we move on we should
    probably stop or removed our containers for now.  Please use the following command
    to either stop or remove your containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Deploying Kubernetes on OpenStack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the time of writing, Kubernetes has been the market choice for container
    orchestration with being one of the top GitHub projects and emerging as the leading
    enterprise choice to manage containers. Some of the high-level features of Kubernetes
    are being able to perform rolling upgrades, zero downtime deployments, manage
    large-scale complex workloads, and highly available/fault tolerant out of the
    box. If you are looking to manage clusters of containers in a production environment,
    you should definitely give Kubernetes a go.
  prefs: []
  type: TYPE_NORMAL
- en: In that light, the question often arises, why would I want to run Kubernetes
    on a platform such as OpenStack? Many folks often forget that OpenStack is a hypervisor
    manager and not a hypervisor itself. OpenStack enables an operator to manage many
    different kinds of hypervisors, with container orchestration software being just
    another kind of hypervisor so to speak. Within OpenStack, you have a few ways
    you can choose to manage and deploy a Kubernetes cluster. It can be done via Magnum,
    the container management project within OpenStack. Another way is using Heat templates
    to manage the Kubernetes cluster as a stack. Lastly, you can use a GitHub project
    named **kargo**, which allows you to deploy Kubernetes using Ansible on many different
    systems and cloud platforms.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our example here, we will cover the last option and use kargo to deploy
    Kubernetes on our OpenStack cloud. It did not feel like a good use of time to
    try and reinvent the wheel by creating our own Ansible playbooks/roles to deploy
    Kubernetes. The kargo project can be found at: [https://github.com/kubernetes-incubator/kargo](https://github.com/kubernetes-incubator/kargo).
    There are instructions in the repository that will walk you through how to set
    up in order to run the setup playbook.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Please keep in mind that kargo is an open source project, and just like every
    other open source project it is subject to change. Change could include reorganizing
    the repository layout, changes in deployment instructions and even depreciation.
    At the time of writing, the project is alive and working.
  prefs: []
  type: TYPE_NORMAL
- en: 'The OpenStack specific instructions can be found here: [https://github.com/kubernetes-incubator/kargo/blob/master/docs/openstack.md](https://github.com/kubernetes-incubator/kargo/blob/master/docs/openstack.md).
    To get started, you would clone the kargo repository to your Utility container
    on your OpenStack cloud:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Automation considerations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For the most part, the installation will go smoothly. I did have to tweak two
    small things to get the playbooks to finish successfully. The first tweak was
    within my OpenRC file. As you will note in the instructions, the second step is
    to source your OpenRC file before running the setup playbook. My file was missing
    two parameters that the playbook checked for; it was the `OS_TENANT_ID` and `OS_REGION_NAME`
    parameters. A working example of my OpenRC file is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The other tweak that I had to make was adjusting how a particular Kubernetes
    dependent software container was pulled. The container repository tag was changed
    and the kargo project had not updated it yet. The update performed to the `roles/download/defaults/main.yml`
    file within the project. A snippet of the original file looked like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The file needed to be changed to look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: With those two changes in place, all you need to do is spin up instances to
    serve as the Kubernetes master, etcd, and node. The instances can be any Linux-based
    operating system you wish. The way you layout your Kubernetes cluster varies on
    the type of environment and ultimately the use case. Reference architecture for
    a stable Kubernetes cluster would be to have two instances as masters, three instances
    as etcds, and leverage Ironic to deploy at least three bare metal servers to be
    the nodes. Of course, for testing purposes you can deploy the whole cluster as
    instances on your OpenStack cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step would be to configure your inventory file to include the instances
    you spun up to act as your Kubernetes cluster. My inventory file was named `os-inventory`.
    A working example of an inventory file is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Believe it or not, you are now ready to run the setup playbook to deploy your
    Kubernetes cluster. The command to do so is as follows, please make sure you are
    within the `root` directory of the kargo repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The installation will run for a bit, but at the end of it all you will have
    a working Kubernetes cluster to experiment with. We will now transition into another
    container orchestration technology and experiment with how we can use Ansible
    to manage containers while also leveraging OpenStack.
  prefs: []
  type: TYPE_NORMAL
- en: Managing CoreOS and Docker with Ansible
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'CoreOS seemed like another great fit to run on top of OpenStack because it
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '*A lightweight Linux operating system designed for clustered deployments providing
    automation, security, and scalability for your most critical applications    
                                                                         –  ([https://coreos.com/why/#cluster](https://coreos.com/why/#cluster))*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: CoreOS's focus is to provide an operating system that is by default cluster
    aware, making it a perfect fit for platforms such as container technologies. Docker
    was also an obvious choice for experimenting with containers since it was what
    made containers popular again. As well as, Docker has a vast variety of images
    ready to be pulled down and deployed as is. For our example, here we will review
    a very simple playbook that will deploy the ELK stack in containers on CoreOS.
  prefs: []
  type: TYPE_NORMAL
- en: Automation considerations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first step in this process is to spin up a minimum of three instances with
    a flavor that has at least 2 GB of memory and a stable CoreOS image. Since I enjoy
    using Heat for things such as this, I spun up my instances using a Heat template.
    The template I created to accomplish this can be found here: [https://github.com/wbentley15/openstack-heat-templates/tree/master/coreos](https://github.com/wbentley15/openstack-heat-templates/tree/master/coreos).
    The command to then deploy the stack with Heat looked like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![Automation considerations](graphics/image_07_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Coding the playbooks and roles
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once your CoreOS stack is online, you can execute the playbook we will now
    create. For this example, all the tasks will be in the playbook named `base.yml`,
    of which is located within the `root` directory of the `playbook` directory. The
    beginning contents of this file will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The very first task in the playbook is key to running packages such as Ansible
    against the target CoreOS instances. Since CoreOS is a minimal operating system
    it ships without any versions of Python. As we know, one of the major prerequisites
    to running Ansible against a host is to have Python installed. To circumvent this
    limitation we will use a role named `defunctzombie.coreos-bootstrap`, which will
    install `pypy` on our CoreOS instances. We will learn later how to tell Ansible
    where to find our Python interpreter on these nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can pull down this role from Galaxy by executing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The next two tasks will now set up the environment on the CoreOS instances
    to run Docker images as containers. Please make note that we will pin the `docker-py`
    and `docker-compose` packages to a particular version; this was due to a known
    bug with the `docker_image` and `docker_container` modules. This dependency can
    be removed once the bug is addressed, or those versions may need to be adjusted
    as time goes on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The final remaining tasks will handle pulling down the Docker images for the
    ELK stack and then launching those containers on your CoreOS cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The Docker images are pulled down from repositories on [https://hub.docker.com](https://hub.docker.com)
    and then deployed on the CoreOS instances hosting Docker.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our `hosts` file for this example was a bit unique again because of the custom
    Python interpreter we had to install for CoreOS. We were required to configure
    Ansible to use that alternative Python interpreter. In the following working example,
    you will find that we configured Ansible to use the Python interpreter located
    at `/home/core/bin/python` and the pip package at `/home/core/bin/pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Later in this chapter, we will wrap up by reviewing these playbooks and roles
    again, then also conclude with a test in order to see the finished results.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Nova LXD on OpenStack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Last, but certainly not least, we will conclude the chapter with the container
    option that really started it all, LXC, or rather its newer bigger brother, LXD.
    LXD is described as:'
  prefs: []
  type: TYPE_NORMAL
- en: '*an container "hypervisor" and a new user experience for LXC              
                                                           –   ([https://www.ubuntu.com/cloud/lxd](https://www.ubuntu.com/cloud/lxd))*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The three major components of LXD are its system-wide daemon (**lxd**), command
    line client (**lxc**), and the OpenStack Nova plugin. It is that very plugin which
    will enable us to run LXD as a hypervisor under OpenStack control and use traditional
    OpenStack commands to spin up containers. With something like this you can literally
    run instances and containers on separate compute nodes under the same control
    plane. LXD is further described as being secure by design, scalable, intuitive,
    image, based, and the ability to perform live migrations.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately for us the Ansible gods have heard and answered our prayers. Within
    the **openstack-ansible project **(**OSA**) under the Newton release (and going
    forward) you can now deploy LXD as an alternative hypervisor to KVM. It is now
    as simple as editing two configuration files before deploying your OSA cloud.
    We will outline those changes and demonstrate how you can spin up your first LXD
    container using OpenStack.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we get started you should know that the detailed instructions for enabling
    LXD on OSA could be found here: [http://docs.openstack.org/developer/openstack-ansible-os_nova/](http://docs.openstack.org/developer/openstack-ansible-os_nova/).'
  prefs: []
  type: TYPE_NORMAL
- en: Automation considerations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An OSA deployment revolves around three main configuration files located within
    the `/etc/openstack_deploy` directory on your deployment node. You will need to
    edit the `user_variables.yml` and `user_secrets.yml` files. Starting with the
    `user_variables.yml` file, you will need to set the `nova_virt_type` variable
    to use LXD. A working example is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The second file that needs to be edited is the `user_secrets.yml` file. You
    will just need to supply a password for the LXD trust. An example of the line
    that needs to be edited is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the event you are planning to set up a mixed compute node farm and wish to
    have both KVM and LXD hosts. You will need to edit the `openstack_user_config.yml`
    file and set the `nova_virt_type` for each host. A working example of how to configure
    this can be found in the preceding documentation link.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you can begin your OSA installation knowing that you will be able to spin
    up LXD containers as well as instances running on KVM. After the installation
    finishes you have one last step to complete. We now have to create an LXD compatible
    image that will be used when you spin up your containers. LXD requires the use
    of raw images, so we will pull down an image that meets those requirements. From
    within the Utility container on your OSA cloud, execute the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '![Automation considerations](graphics/image_07_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'With your new image in place you are now ready to spin up your first LXD container.
    The LXD containers are managed in a similar manner as instances running on KVM.
    You can create a container via the Horizon dashboard or via the OpenStack Client
    CLI. For this example, we will use the OpenStack Client to create the containers.
    The following command will create your containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'You can count it as a success if your output looks similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Automation considerations](graphics/image_07_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: You can then execute the `openstack server list` command to verify that your
    new containers are running.
  prefs: []
  type: TYPE_NORMAL
- en: '![Automation considerations](graphics/image_07_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Very cool my friend, you did great yet again! I know we covered a lot, but you
    are an old pro by now so no worries. Keeping with our tradition, we will finish
    up the chapter with a quick review of what we covered and what to expect in the
    next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing playbooks and roles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s jump right into examining the master playbook that we created earlier
    to deploy Docker containers on CoreOS called **ansible-coreos**. The completed
    playbook and file, named `base.yml`, located in the root of the `ansible-coreos`
    directory, looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The corresponding role we pulled down from Galaxy is located in the `ansible-coreos/roles/defunctzombie.coreos-bootstrap/tasks`
    directory, and looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we created the `hosts` file, which also is located in the `root` directory
    of the `playbook` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The complete set of code can again be found in the following GitHub repository:
    [https://github.com/os-admin-with-ansible/os-admin-with-ansible-v2](https://github.com/os-admin-with-ansible/os-admin-with-ansible-v2).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We are finally ready to give this playbook a try. Assuming you have cloned
    the previous GitHub repository, the command to test out the playbook from the
    deployment node would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Assuming all goes well, the output should resemble the snippet in the following
    screenshot:![Reviewing playbooks and roles](graphics/image_07_010.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: Normally, I also like to take the extra step of verifying that the containers
    are running by executing the `docker ps` command on the CoreOS instances.
  prefs: []
  type: TYPE_NORMAL
- en: '![Reviewing playbooks and roles](graphics/image_07_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Crossing the finish line does feel nice for sure. I hope that the raw power
    of what containers can offer you will inspire you to start deploying them on your
    OpenStack clouds. It always feels good to have options outside of the traditional
    VM's and instances.
  prefs: []
  type: TYPE_NORMAL
- en: Before concluding this chapter, let's take a moment to recap this chapter. We
    started the chapter with exploring the concept of Containerization and why it
    has become so very popular. You learned how to use Ansible Container to create
    our very first container image and build. We reviewed the kargo project that enables
    you to deploy Kubernetes using Ansible on multiple cloud platforms, including
    OpenStack. Next we demonstrated how to use Ansible to manage CoreOS running a
    Docker cluster on OpenStack. Lastly, we reviewed the configuration changes needed
    to deploy LXD with the **openstack-ansible** project.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will also be a very interesting one because as a cloud operator
    you will eventually have to think about scaling up/out your cloud footprint. OpenStack
    has useful built-in features, which makes the process of scaling fairly easy and
    simple. In the next chapter, we will cover the concept of setting up active-active
    cloud regions and then take it up a notch with automating this task to alleviate
    the stress of scaling when the time arises. If you are ready for some unchartered
    waters, set a course for [Chapter 8](ch08.html "Chapter 8. Setting Up Active-Active
    Regions"), *Setting Up Active-Active Regions*!
  prefs: []
  type: TYPE_NORMAL
