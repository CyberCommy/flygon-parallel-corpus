- en: Chapter 4. Creating a Production-Grade Swarm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn how to create real Swarm clusters with thousands
    of nodes; specifically we''ll cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Tools to deploy large Swarms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Swarm2k: One of the largest Swarm mode cluster ever built, made of 2,300 nodes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Swarm3k: The second experiment, a cluster with 4,700 nodes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to plan hardware resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HA cluster topologies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Swarm infrastructures management, networking, and security
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring dashboards
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What you learned from the Swarm2k and Swarm3k experiments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With Swarm Mode, we can easily design a production-grade cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The principles and architecture we're illustrating here are important in general
    and give a foundation on how to architect production installations, regardless
    of the tools. However, from a practical point of view, the tools to use are also
    important.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the time of writing this book, Docker Machine was not the ideal single tool
    to use for large swarms setups, so we''re demonstrating our production-scale deployments
    with a tool born alongside with this book that we already introduced in [Chapter
    1](ch01.html "Chapter 1. Welcome to Docker Swarm"), *Welcome to Docker Swarm*:
    belt ([https://github.com/chanwit/belt](https://github.com/chanwit/belt)). We''ll
    use it in conjunction with Docker Machine, Docker Networking, and the DigitalOcean''s
    `doctl` command.'
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 5](ch05.html "Chapter 5. Administer a Swarm Cluster"), *Administer
    a Swarm Cluster* you'll learn how it's possible to automate the creation of Swarms;
    especially, how to quickly join a massive number of workers with scripts and other
    mechanisms, such as Ansible.
  prefs: []
  type: TYPE_NORMAL
- en: '![Tools](images/image_04_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: An HA Topology for Swarm2k
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Swarm2k and Swarm3k were collaborative experiments. We raised funds in terms
    of Docker Hosts, instead of money, with a call to participate. The result was
    astonishing-Swarm2k and Swarm3k were joined by dozens of individuals and corporate
    geographically distributed contributors. In total, for Swarm2k, we collected around
    2,300 nodes, while for Swarm3k, around 4,700.
  prefs: []
  type: TYPE_NORMAL
- en: Let's discuss the architecture of *Swarm2k*. In the preceding figure, there
    are three managers, denoted as **mg0**, **mg1**, and **mg2**. We will use the
    three managers because it is the optimum number of managers, suggested by the
    Docker core team. Managers formed a quorum on a high-speed network link and raft
    nodes employ employed a significant amount of resources to synchronize their activities.
    So, we decided to deploy our managers on a 40GB Ethernet link into the same data
    center.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the beginning of the experiment, we had the following configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: mg0 was the cluster's manager leader
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: mg1 hosted the stat collector
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: mg2 was a ready (reserve) manager
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead, the **W** nodes were Swarm workers.
  prefs: []
  type: TYPE_NORMAL
- en: The stat collector installed on mg1 queries the information out of the local
    Docker Engine and sends them to store in a remote time-series database, *InfluxDB*.
    We chose InfluxDB because it's natively supported by *Telegraf*, our monitoring
    agent. To display the cluster's statistics, we used *Grafana* as a dashboard,
    which we'll see later.
  prefs: []
  type: TYPE_NORMAL
- en: Managers specifications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Managers are CPU bound rather than memory bound. For a 500-1,000 nodes Swarm
    cluster, we observed empirically that three managers with 8 vCPUs each are enough
    to keep the load. However, if it's beyond 2,000 nodes, we recommend, for each
    manager, at least 16-20 vCPUs to meet eventual Raft recoveries.
  prefs: []
  type: TYPE_NORMAL
- en: In case of Raft recovery
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The following diagram shows the CPU usage during a hardware upgrade and under
    a massive number of workers join process. During the hardware upgrade to 8 vCPUs
    (the machines downtime is represented by lines disconnections), we can see that
    the CPU usage of mg0, the leader, spiked to 75-90% when mg**1** and mg**2** rejoined
    the cluster. The event that triggered this spike is the Raft log synchronization
    and recovery.
  prefs: []
  type: TYPE_NORMAL
- en: Under normal conditions, with no required recoveries, CPU usage of each manager
    stays low, as shown in the following image.
  prefs: []
  type: TYPE_NORMAL
- en: '![In case of Raft recovery](images/image_04_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Raft files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In manager hosts, Swarm data is saved in `/var/lib/docker/swarm`, called the
    *swarm directory*. Specifically, Raft data is saved in `/var/lib/docker/swarm/raft`
    and consists of the Write Ahead Log (WAL) and snapshot files.
  prefs: []
  type: TYPE_NORMAL
- en: In these files, there are entries for nodes, services, and tasks, as defined
    by the Protobuf format.
  prefs: []
  type: TYPE_NORMAL
- en: WAL and snapshot files are frequently written to disk. In SwarmKit and Docker
    Swarm Mode, they are written to disk every 10,000 entries. Per this behavior,
    we mapped the swarm directory to a fast and dedicated disk with increased throughput,
    specifically an SSD drive.
  prefs: []
  type: TYPE_NORMAL
- en: We will explain backup and restore procedures in case of the swarm directory
    corruption in [Chapter 5](ch05.html "Chapter 5. Administer a Swarm Cluster"),
    *Administer a Swarm Cluster*.
  prefs: []
  type: TYPE_NORMAL
- en: Running tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal of Swarm clusters is to run services, for example, large-scale Web
    applications made from a big number of containers. We will call this deployment
    type the *Mono* model. In this model, network ports are considered resources that
    must be published globally. With *namespaces* in the future versions of Docker
    Swarm Mode, the deployment can be in the *Multi* model, where we are allowed to
    have multiple subclusters, which expose the same port for different services.
  prefs: []
  type: TYPE_NORMAL
- en: In small-size clusters, we can decide to allow managers to host worker tasks
    with some prudence. For larger setups, instead, managers use more resources. Moreover,
    in case a manager load saturates its resources, the cluster will become unstable
    and unresponsive and will not take any commands. We call this state the *Berserk*
    *state*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make a large cluster, such as Swarm2k or Swarm3k stable, all managers''
    availability must be set to the "Drained" state so that all the tasks will not
    be scheduled on them, only on workers, with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Manager topologies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ll discuss this HA property in [Chapter 5](ch05.html "Chapter 5. Administer
    a Swarm Cluster"), *Administer a Swarm Cluster* again, but here, we will introduce
    it to illustrate some Swarm topologies theory. The HA theory makes it mandatory
    to form a HA cluster with an odd number of nodes. The following table shows the
    fault tolerance factors for a single data center. In this chapter, we''ll call
    the 5(1)-3-2 formula for a cluster size of 5 over 1 data center with 3-node quorum
    that allows 2 nodes to fail:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Cluster Size** | **Quorum** | **Node Failure allowed** |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 2 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 3 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 4 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 5 | 4 |'
  prefs: []
  type: TYPE_TB
- en: 'However, there are several manager topologies that can be designed to use in
    production environments with multiple data centers. For example, the 3(3) manager
    topology can be distributed as 1 + 1 + 1, while the 5(3) manager topology can
    be distributed as 2 + 2 + 1\. The following image shows the optimum 5(3) manager
    topology:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Manager topologies](images/image_04_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: With the same level of tolerance, the next image shows an alternative 5(4) topology
    containing 5 managers across 4 data centers. There are 2 managers, mg0, and mg1,
    running in Data Center 1, while each of the remaining managers, mg2, mg3, and
    mg4, run in Data Center 2, 3, and 4 respectively. The mg0 and mg1 managers are
    connected on high-speed network, while mg2, mg3, and mg4 can use a slower link.
    Therefore, 2 + 2 + 1 across 3 data centers will be rearranged as 2 + 1 + 1 + 1
    over 4 data centers.
  prefs: []
  type: TYPE_NORMAL
- en: '![Manager topologies](images/image_04_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Finally, there is another distributed topology, 6(4), which is much more performant
    because, at its heart, there are 3 nodes forming a central quorum on high-speed
    links. The 6-manager cluster needs a quorum size of 4\. If Data Center 1 fails,
    the control plane of the cluster will stop working. In normal case, 2 nodes or
    2 data centers, except the main one, can be down.
  prefs: []
  type: TYPE_NORMAL
- en: '![Manager topologies](images/image_04_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: To sum it up, stick with odd numbers of managers whenever possible. If you want
    stability of manager quorum, form it over high-speed links. If you want to avoid
    single point of failures, distribute them as much as possible.
  prefs: []
  type: TYPE_NORMAL
- en: To confirm which topology will work for you, try forming it and test the manager
    latency by intentionally putting some managers down and then measure how fast
    they recover.
  prefs: []
  type: TYPE_NORMAL
- en: For Swarm2k and Swarm3k, we chose to form topology with all three managers being
    on a single data center because we wanted to achieve the best performances.
  prefs: []
  type: TYPE_NORMAL
- en: Provisioning the infrastructure with belt
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we created a cluster template named `swarm2k` for DigitalOcean with
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command creates a configuration template file in the current directory
    called `.belt/swarm2k/config.yml`. This was our starting point to define other
    attributes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We checked if our cluster was defined by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'With the use command, we can switch and used the available `swarm2k` clusters,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: At this point, we refined the `swarm2k` template's attributes.
  prefs: []
  type: TYPE_NORMAL
- en: 'By setting  the DigitalOcean''s instance region to be `sgp1` by issuing the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Belt required to define all necessary values with this command. Here''s a list
    of the required template keys for the DigitalOcean driver that we specified in
    `config.yml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`image`: This is to specify the DigitalOcean image ID or snapshot ID'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`region`: This is to specify the DigitalOcean region, for example, sgp1 or
    nyc3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ssh_key_fingerprint`: This is to specify the DigitalOcean SSH Key ID or fingerprint'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ssh_user`: This is to specify the username used by the image, for example,
    root'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`access_token`: This is to specify DigitalOcean''s access token; it is recommended
    to not put any of the tokens here'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Every template attribute has its environment variable counterpart. For example,
    the `access_token` attribute can be set via `DIGITALOCEAN_ACCESS_TOKEN`. So, in
    practice, we can also export `DIGITALOCEAN_ACCESS_TOKEN` as a shell variable before
    proceeding.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the configuration in place, we verified the current template attributes
    by running the following piece of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we created a set of 3 512MB manger nodes called mg0, mg1, and mg2 with
    the following syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: All new nodes are initialized and go to a new status.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the following command to wait until all 3 nodes become active:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we set node1 to be the active manger host, and our Swarm will be ready
    to be formed. Setting the active host can be done by running the active command,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we formed a swarm. We initialized mg0 as the manager leader,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command outputs the strings to copy and paste to join other managers
    and workers, for example, take a look at the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Belt provides a convenient shortcut to join nodes with the following syntax,
    that was what we used to join mg1 and mg2 to the swarm.:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Now, we have the mg0, mg1, and mg2 managers configured and ready to get the
    swarm of workers.
  prefs: []
  type: TYPE_NORMAL
- en: Securing Managers with Docker Machine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker Machine won't scale well for massive Docker Engine deployments, but it
    turns out to be very useful for automatically securing small number of nodes.
    In the following section, we'll use Docker Machine to secure our Swarm manager
    using the generic driver, a driver that allows us to control existing hosts.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, we already did set up a Docker Swarm manager on mg0\. Furthermore,
    we want to secure Docker Engine by enabling the TLS connection for its remote
    endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: How can Docker Machine do the work for us? First, Docker Machine connects to
    the host via SSH; detects the operating system of mg0, in our case, Ubuntu; and
    the provisioner, in our case, systemd.
  prefs: []
  type: TYPE_NORMAL
- en: After that, it installs the Docker Engine; however, in case one is already in
    place, like here, it will skip this step.
  prefs: []
  type: TYPE_NORMAL
- en: Then, as the most important part, it generates a Root CA certificate, as well
    as all certificates, and stores them on the host. It also automatically configures
    Docker to use those certificates. Finally, it restarts Docker.
  prefs: []
  type: TYPE_NORMAL
- en: If everything goes well, Docker Engine will be started again with TLS enabled.
  prefs: []
  type: TYPE_NORMAL
- en: We then used Docker Machine to generate a Root CA for the Engine on mg0, mg1
    and mg2, and configure a TLS connection. Then, we later used the Docker client
    to further control Swarm without the need of SSH, which is slower.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, `docker node ls` will work normally with this setup We verified now that
    the 3 managers formed the initial swarm, and were able to accept a bunch of workers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**How secure is this cluster?**'
  prefs: []
  type: TYPE_NORMAL
- en: We will use the Docker client to connect to the Docker Engine equipped TLS;
    and, there's another TLS connection among the swarm's node with CA expiring in
    three months, and it will be auto-rotated. Advanced security setup will be discussed
    in [Chapter 9](ch09.html "Chapter 9. Securing a Swarm Cluster and the Docker Software
    Supply Chain"), *Securing a Swarm Cluster and the Docker Software Supply Chain*.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding some Swarm internals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At this point, we checked that the Swarm was operative, by creating a service
    nginx with 3 replicas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Afteer that, we found where the net namespace ID of the running Nginx was. We
    connected to mg0 via SSH to mg0 via SSH. The network namespace for Swarm's routing
    mesh was the one having the same timestamp as the special network namespace, `1-5t4znibozx`.
    In this example, the namespace we're looking for is `fe3714ca42d0`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We can figure out our IPVS entries with ipvsadm, and run it inside the net
    namespace using the nsenter tool ([https://github.com/jpetazzo/nsenter](https://github.com/jpetazzo/nsenter)),
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can notice that there is an active round-robin IPVS entry. IPVS is
    the kernel-level load balancer, and it's used by Swarm to balance traffic in conjunction
    with iptables, which is used to forward and filter packets.
  prefs: []
  type: TYPE_NORMAL
- en: 'After cleaning the nginx test service (`docker service rm nginx`), we will
    set the managers in Drain mode, so to avoid them to take tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Now, we are ready to announce the availability of our managers on Twitter and
    Github and start the experiment!
  prefs: []
  type: TYPE_NORMAL
- en: Joining workers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our contributors started joining their nodes to manager **mg0** as workers.
    Anyone used their own favorite method, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Looping `docker-machine ssh sudo docker swarm join` commands
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ansible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Custom scripts and programs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll cover some of these methods in [Chapter 5](ch05.html "Chapter 5. Administer
    a Swarm Cluster"), *Administer a Swarm Cluster* .
  prefs: []
  type: TYPE_NORMAL
- en: 'After some time, we reached the quota of 2,300 workers and launched an **alpine**
    service with a replica factor of 100,000:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Joining workers](images/image_04_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Upgrading Managers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After some time, we reached the maximum of capacity for our managers, and we
    had to increase their physical resources. Live upgrading and maintenance of managers
    may be an expected operation in production. Here is how we did this operation.
  prefs: []
  type: TYPE_NORMAL
- en: Live Upgrading the Managers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With an odd number for the quorum, it is safe to demote a manager for maintenance.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we had mg1 as a reachable manager, and we demoted it to worker with the
    following syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the `Reachable` status of `mg1` disappears from node ls output
    when it becomes a worker.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'When the node is not a manager anymore, it''s safe to shut it down, for example,
    with the DigitalOcean CLI, as we did:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Listing the nodes, we noticed that mg1 was already down.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We upgraded its resources to have 16G of memory, and then we powered the machine
    on again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: When listing this time, we can expect some delay as mg1 is being back and reentered
    the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can promote it back to the manager, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Once this is done, the cluster operated normally. So, we repeated the operation
    for mg0 and mg2.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring Swarm2k
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the production-grade cluster, we usually want to set up some kind of monitoring.
    At the date, there is not a specific way to monitor Docker service and tasks in
    Swarm mode. We did this for Swarm2k with Telegraf, InfluxDB, and Grafana.
  prefs: []
  type: TYPE_NORMAL
- en: InfluxDB Time-Series Database
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: InfluxDB is a time-series database, which is easy to install because of no dependency.
    InfluxDB is useful to store metrics, information about events, and use them for
    later analysis. For Swarm2k, we used InfluxDB to store information of cluster,
    nodes, events, and for tasks with Telegraf.
  prefs: []
  type: TYPE_NORMAL
- en: Telegraf is pluggable and has a certain number of input plugins useful to observe
    the system environment.
  prefs: []
  type: TYPE_NORMAL
- en: Telegraf Swarm plugin
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We developed a new plugin for Telegraf to store stats into InfluxDB. This plugin
    can be found at [http://github.com/chanwit/telegraf](http://github.com/chanwit/telegraf).
    Data may contain *values*, *tags*, and *timestamp*. Values will be computed or
    aggregated based on timestamp. Additionally, tags will allow you to group these
    values together based on timestamp.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Telegraf Swarm plugin collects data and creates the following series containing
    values, which we identified as the most interesting for Swarmk2, tags, and timestamp
    into InfluxDB:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Series `swarm_node`: This series contains `cpu_shares` and `memory` as values
    and allow you to be grouped by `node_id` and `node_hostname` tags.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Series `swarm`: This series contains `n_nodes` for number of nodes, `n_services`
    for number of services, and `n_tasks` for number of tasks. This series does not
    contain tags.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Series `swarm_task_status`: This series contains number of tasks grouped by
    status at a time. Tags of this series are tasks status names, for example, Started,
    Running, and Failed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To enable the Telegraf Swarm plugin, we will need to tweak `telegraf.conf` by
    adding the following configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'First, set up an instance of InfluxDB as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, set up an instance of Grafana, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'After we setup an instance of Grafana, we can create the dashboard from the
    following JSON configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://objects-us-west-1.dream.io/swarm2k/swarm2k_final_grafana_dashboard.json](https://objects-us-west-1.dream.io/swarm2k/swarm2k_final_grafana_dashboard.json)'
  prefs: []
  type: TYPE_NORMAL
- en: To connect the dashboard to InfluxDB, we will have to define the default data
    source and point it to the InfluxDB host port `8086`. Here's the JSON configuration
    to define the data source. Replace `$INFLUX_DB_IP` with your InfluxDB instance.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'After linking everything together, we''ll see a dashboard like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Telegraf Swarm plugin](images/image_04_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Swarm3k
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Swarm3k was the second collaborative project trying to form a very large Docker
    cluster with the Swarm mode. It was fired up on 28th October 2016 with more than
    50 individuals and companies joining this project.
  prefs: []
  type: TYPE_NORMAL
- en: Sematext was one of the very first companies that offered to help us by offering
    their Docker monitoring and logging solution. They became the official monitoring
    system for Swarm3k. Stefan, Otis, and their team provided wonderful support for
    us from the very beginning.
  prefs: []
  type: TYPE_NORMAL
- en: '![Swarm3k](images/image_04_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Sematext Dashboard*'
  prefs: []
  type: TYPE_NORMAL
- en: Sematext is the one and only Docker monitoring company that allows us to deploy
    the monitoring agents as the global Docker service at the moment. This deployment
    model provides for a greatly simplified monitoring process.
  prefs: []
  type: TYPE_NORMAL
- en: Swarm3k Setup and Workload
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We aimed at 3000 nodes, but in the end, we successfully formed a working, geographically
    distributed 4,700-node Docker Swarm cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The managers' specifications were high-mem 128GB DigitalOcean nodes in the same
    Data Center, each with 16 vCores.
  prefs: []
  type: TYPE_NORMAL
- en: The cluster init configuration included an undocumented "KeepOldSnapshots",
    which tells the Swarm mode not to delete but preserve all data snapshots for later
    analysis. The Docker daemon of each manager started in the DEBUG mode so to have
    more information on the go..
  prefs: []
  type: TYPE_NORMAL
- en: We used belt to set up the managers, as we showed in the previous section, and
    waited for contributors to join their workers.
  prefs: []
  type: TYPE_NORMAL
- en: Docker 1.12.3 was used on managers, while workers were a mix of 1.12.2 and 1.12.3\.
    We organized the services on the *ingress* and *overlay* networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'We planned the following two workloads:'
  prefs: []
  type: TYPE_NORMAL
- en: MySQL with Wordpress cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: C1M (Container-1-Million)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '25 nodes were intended to form a MySQL cluster. First, we created an overlay
    network, `mydb`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we prepared the following `entrypoint.sh` script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will prepare a new Dockerfile for our special version of Etcd, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Don't forget to build it with `$ docker build -t chanwit/etcd.` before you start
    using it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Third, we started an Etcd node as a central discovery service for MySQL cluster
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'By inspecting the Virtual IP of Etcd, we will get the service VIP as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'With this information, we created our `mysql` service, which can scale at any
    degree. Take a look at the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: We experienced some IP addresses issues from both mynet and ingress networks
    because of a Libnetwork bug; check out [https://github.com/docker/docker/issues/24637](https://github.com/docker/docker/issues/24637)
    for more information. We work around this bug by binding the cluster only to a
    *single* overlay network, `mydb.`
  prefs: []
  type: TYPE_NORMAL
- en: Now, we attempted a `docker service create` with the replica factor 1 for a
    WordPress container. We intentionally didn't control where the Wordpress container
    would be scheduled. When we were trying to wire this Wordpress service to the
    MySQL service, however, the connection repeatedly timed out. We concluded that
    for a Wordpress + MySQL combo at this scale, it's much better to put a few constraints
    on the cluster to make all the services run together in the same data center.
  prefs: []
  type: TYPE_NORMAL
- en: Swarm performance at a scale
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'What you also learned from this issue was that the performance of the overlay
    network greatly depends on the correct tuning of network configuration on each
    host. As suggested by a Docker engineer, we may experience the "Neighbour Table
    Overflow" error when there are too many ARP requests (when the network is very
    big) and each host is not able to reply. These were the tunables we increased
    on the Docker hosts to fix the following behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Here, `gc_thresh1` is the expected number of hosts, where `gc_thresh2` is the
    soft limit and `gc_thresh3` is the hard limit.
  prefs: []
  type: TYPE_NORMAL
- en: So, when the MySQL + Wordpress test failed, we changed our plan to experiment
    NGINX on a Routing Mesh.
  prefs: []
  type: TYPE_NORMAL
- en: The ingress network was set up with a /16 pool so, it could accommodate a maximum
    of 64,000 IP addresses. From a suggestion by Alex Ellis, we started 4,000 (four
    thousands!) NGINX containers on the cluster. During this test, nodes were still
    coming in and out. Eventually, a few minutes later, the NGINX service started
    and the Routing Mesh was formed. It could correctly serve, even as some nodes
    kept failing, so this test verified that the Routing Mesh in 1.12.3 is rock solid
    and production ready.We then stopped the NGINX service and started to test the
    scheduling of as many containers as possible, aiming at 1,000,000, one million.
  prefs: []
  type: TYPE_NORMAL
- en: So, we created an "alpine top" service, as we did for Swarm2k. However, the
    scheduling rate was a bit slower this time. We reached 47,000 containers in approximately
    30 minutes. Therefore, we foresaw it was going to take approximately ~10.6 hours
    to fill the cluster with our 1,000,000 containers.
  prefs: []
  type: TYPE_NORMAL
- en: As that was expected to take too much time, we decided to change plans again
    and go for 70,000 containers.
  prefs: []
  type: TYPE_NORMAL
- en: '![Swarm performance at a scale](images/image_04_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Scheduling a huge number of containers (**docker scale alpine=70000**) stressed
    out the cluster. This created a huge scheduling queue that would not commit until
    all 70,000 containers finished their scheduling. Therefore, when we decided to
    shut down the managers, all scheduling tasks disappeared and the cluster became
    unstable, for the Raft log got corrupted.
  prefs: []
  type: TYPE_NORMAL
- en: On the way, one of the most interesting things we wanted to check by collecting
    CPU profile information was to see what Swarm primitives were loading the cluster
    more.
  prefs: []
  type: TYPE_NORMAL
- en: '![Swarm performance at a scale](images/image_04_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, we can see that only 0.42% of the CPU was spent on the scheduling algorithm.
    We concluded with some approximations that the Docker Swarm scheduling algorithm
    in version 1.12 is quite fast. This means that there is an opportunity to introduce
    a more sophisticated scheduling algorithm that could result in an even better
    resource utilization in future versions of Swarm, by adding just some acceptable
    overhead.
  prefs: []
  type: TYPE_NORMAL
- en: '![Swarm performance at a scale](images/image_04_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Also, we found that a lot of CPU cycles were spent on nodes communication. Here,
    we can see the Libnetwork member list layer. It used ~12% of the overall CPU.
  prefs: []
  type: TYPE_NORMAL
- en: '![Swarm performance at a scale](images/image_04_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: However, it seemed that the major CPU consumer was Raft, which also invoked
    the Go garbage collector significantly here. This used ~30% of the overall CPU.
  prefs: []
  type: TYPE_NORMAL
- en: Swarm2k and Swarm3k lessons learned
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here''s a summary of what you learned from these experiments:'
  prefs: []
  type: TYPE_NORMAL
- en: For a large set of workers, managers require a lot of CPUs. CPUs will spike
    whenever the Raft recovery process kicks in.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the leading manager dies, it's better to stop Docker on that node and wait
    until the cluster becomes stable again with n-1 managers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep snapshot reservation as small as possible. The default Docker Swarm configuration
    will do. Persisting Raft snapshots uses extra CPU.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thousands of nodes require a huge set of resources to manage, both in terms
    of CPU and network bandwidth. Try to keep services and the managers' topology
    geographically compact.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hundreds of thousand tasks require high memory nodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, a maximum of 500-1000 nodes are recommended for stable production setups.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If managers seem to be stuck, wait; they'll recover eventually.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `advertise-addr` parameter is mandatory for Routing Mesh to work.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Put your compute nodes as close to your data nodes as possible. The overlay
    network is great and will require tweaking Linux net configuration for all hosts
    to make it work best.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker Swarm Mode is robust. There were no task failures, even with unpredictable
    network connecting this huge cluster together.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For Swarm3k, we would like to thank all the heroes: `@FlorianHeigl`; `@jmaitrehenry`
    from PetalMD; `@everett_toews` from Rackspace, Internet Thailand; `@squeaky_pl`,
    `@neverlock`, `@tomwillfixit` from Demonware; `@sujaypillai` from Jabil; `@pilgrimstack`
    from OVH; `@ajeetsraina` from Collabnix; `@AorJoa` and `@PNgoenthai` from Aiyara
    Cluster; `@GroupSprint3r`, `@toughIQ`, `@mrnonaki`, `@zinuzoid` from HotelQuickly;
    `@_EthanHunt_`; `@packethost` from Packet.io; `@ContainerizeT-ContainerizeThis`,
    The Conference; `@_pascalandy` from FirePress; @lucjuggery from TRAXxs; @alexellisuk;
    @svega from Huli; @BretFisher; `@voodootikigod` from Emerging Technology Advisors;
    `@AlexPostID`; `@gianarb` from ThumpFlow; `@Rucknar`, `@lherrerabenitez`; `@abhisak`
    from Nipa Technology; and `@djalal` from NexwayGroup.'
  prefs: []
  type: TYPE_NORMAL
- en: We would also like to thank Sematext again for the best-of-class Docker monitoring
    system; and DigitalOcean for providing us with all resources.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we showed you how we deployed two huge Swarm clusters over
    Digital Ocean by using belt. These stories gave you much to learn from. We summarized
    the lessons and outlined some tips for running huge production swarms. On the
    go, we also introduced some Swarm features, such as services and security, and
    we discussed managers' topologies. In the next chapter, we'll discuss in detail
    how to administer Swarm. Topics included will be deploying workers with belt,
    scripts and Ansible, managing nodes, monitoring, and graphical interfaces.
  prefs: []
  type: TYPE_NORMAL
