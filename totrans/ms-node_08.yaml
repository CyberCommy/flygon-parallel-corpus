- en: Scaling Your Application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"Evolution is a process of constant branching and expansion."'
  prefs: []
  type: TYPE_NORMAL
- en: '- Stephen Jay Gould'
  prefs: []
  type: TYPE_NORMAL
- en: 'Scalability and performance are not the same things:'
  prefs: []
  type: TYPE_NORMAL
- en: '"The terms "performance" and "scalability" are commonly used interchangeably,'
  prefs: []
  type: TYPE_NORMAL
- en: 'but the two are distinct: performance measures the speed with which a single
    request can be executed, while scalability measures the ability of a request to
    maintain its performance under increasing load. For example, the performance of
    a request may be reported as generating a valid response within three seconds,
    but the scalability of the request measures the request''s ability to maintain
    that three-second response time as the user load increases."'
  prefs: []
  type: TYPE_NORMAL
- en: '- Steven Haines, "Pro Java EE 5"'
  prefs: []
  type: TYPE_NORMAL
- en: In the last chapter, we looked at how Node clusters might be used to increase
    the performance of an application. Through the use of clusters of processes and
    workers, we learned how to efficiently deliver results in the face of many simultaneous
    requests. We learned to scale Node *vertically*, keeping the same footprint (a
    single server) and increasing throughput by piling on the power of the available
    CPUs.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will focus on *horizontal* scalability; the idea is that
    an application composed of self-sufficient and independent units (servers) can
    be scaled by adding more units without altering the application's code.
  prefs: []
  type: TYPE_NORMAL
- en: We want to create an architecture within which any number of optimized and encapsulated
    Node-powered servers can be added or subtracted in response to changing demands,
    dynamically scaling without ever requiring a system rewrite. We want to share
    work across different systems, pushing requests to the OS, to another server,
    to a third-party service, while coordinating those I/O operations intelligently
    using Node's evented approach to concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: Through architectural parallelism, our systems can manage increased data volume
    more efficiently. Specialized systems can be isolated when necessary, even independently
    scaled or otherwise clustered.
  prefs: []
  type: TYPE_NORMAL
- en: Node is particularly well-suited to handle two key aspects of horizontally-scaled
    architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, Node enforces non-blocking I/O, such that the seizing up of any one
    unit will not cause a cascade of locking that brings down an entire application.
    As no single I/O operation will block the entire system, integrating third-party
    services can be done with confidence, encouraging a decoupled architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, Node places great importance on supporting as many fast network communication
    protocols as possible. Whether through a shared database, a shared filesystem,
    or a message queue, Node's efficient network and `Stream` layers allow many servers
    to synchronize their efforts in balancing load. Being able to efficiently manage
    shared socket connections, for instance, helps when scaling out a cluster of servers
    as much as it does a cluster of processes.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will look at how to balance traffic between many servers
    running Node, how these distinct servers can communicate, and how these clusters
    can bind to and benefit from specialized cloud services.
  prefs: []
  type: TYPE_NORMAL
- en: When to scale?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The theory around application scaling is a complex and interesting topic that
    continues to be refined and expanded. A comprehensive discussion of the topic
    will require several books, curated for different environments and needs. For
    our purposes, we will simply learn how to recognize when scaling up (or even scaling
    down) is necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Having a flexible architecture that can add and subtract resources as needed
    is essential to a resilient scaling strategy. A vertical scaling solution does
    not always suffice (simply adding memory or CPUs will not deliver the necessary
    improvements). When should horizontal scaling be considered?
  prefs: []
  type: TYPE_NORMAL
- en: It is essential that you are able to monitor your servers. One simple but useful
    way to check the CPU and memory usage commanded by Node processes running on a
    server is to use the Unix `ps` (*process status*) command, for example, `ps aux
    | grep node`. A more robust solution is to install an interactive process manager,
    such as HTOP ([http://hisham.hm/htop/](http://hisham.hm/htop/)) for Unix systems,
    or Process Explorer for Windows-based systems ([https://docs.microsoft.com/en-us/sysinternals/downloads/process-explorer](https://docs.microsoft.com/en-us/sysinternals/downloads/process-explorer)).
  prefs: []
  type: TYPE_NORMAL
- en: Network latency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When network response times are exceeding some threshold, such as each request
    taking several seconds, it is likely that the system has gone well past a stable
    state.
  prefs: []
  type: TYPE_NORMAL
- en: While the easiest way to discover this problem is to wait for customer complaints
    about slow websites, it is better to create controlled stress tests against an
    equivalent application environment or server.
  prefs: []
  type: TYPE_NORMAL
- en: '**AB** (**Apache Bench**) is a simple and straightforward way to do blunt stress
    tests against a server. This tool can be configured in many ways, but the kind
    of test you would do for measuring the network response times for your server
    is generally straightforward.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s test the response times for this simple Node server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s how one might test running 10,000 requests against that server, with
    a concurrency of 100 (the number of simultaneous requests):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'If all goes well, you will receive a report similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: There is a lot of useful information contained in this report. In particular,
    one should be looking for failed requests and the percentage of long-running requests.
  prefs: []
  type: TYPE_NORMAL
- en: Much more sophisticated testing systems exist, but `ab` is a good quick-and-dirty
    snapshot of performance. Get in the habit of creating testing environments that
    mirror your production systems and test them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Running `ab` on the same server running the Node process you are testing will,
    of course, impact the test speeds. The test runner itself uses a lot of server
    resources, so your results will be misleading. Full documentation for ab can be
    found at: [https://httpd.apache.org/docs/2.4/programs/ab.html](https://httpd.apache.org/docs/2.4/programs/ab.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Hot CPUs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When CPU usage begins to nudge maximums, start to think about increasing the
    number of units processing client requests. Remember that while adding one new
    CPU to a single-CPU machine will bring immediate and enormous improvements, adding
    another CPU to a 32-core machine will not necessarily bring an equal improvement.
    Slowdowns are not always about slow calculations.
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned earlier, `htop` is a great way to get a quick overview of your
    server''s performance. As it visualizes the load being put on each core in real
    time, it is a great way to get an idea of what is happening. Additionally, the
    load average of your server is nicely summarized with three values. This is a
    happy server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: What do these values mean? What is a "good" or a "bad" load average?
  prefs: []
  type: TYPE_NORMAL
- en: All three numbers are measuring CPU usage, presenting measurements taken at
    one, five, and fifteen-minute intervals. Generally, it can be expected that short-term
    load will be higher than long-term load. If, on an average, your server is not
    overly stressed over time, it is likely that clients are having a good experience.
  prefs: []
  type: TYPE_NORMAL
- en: On a single-core machine, load average should remain between 0.00 and 1.00\.
    Any request will take *some* time—the question is whether the request is taking
    *more time than necessary*—and whether there are delays due to excessive load.
  prefs: []
  type: TYPE_NORMAL
- en: If a CPU can be thought of as a pipe, a measurement of 0.00 means that there
    is no excessive friction, or delay, in pushing through a drop of water. A measurement
    of 1.00 indicates that our pipe is at its capacity; water is flowing smoothly,
    but any additional attempts to push water through will be faced with delays, or
    backpressure. This translates into latency on the network, with new requests joining
    an ever-growing queue.
  prefs: []
  type: TYPE_NORMAL
- en: A multicore machine simply multiplies the measurement boundary. A machine with
    four cores is at its capacity when load average reaches 4.00.
  prefs: []
  type: TYPE_NORMAL
- en: How you choose to react to load averages depends on the specifics of an application.
    It is not unusual for servers running mathematical models to see their CPU averages
    hit maximum capacity; in such cases, you want *all* available resources dedicated
    to performing calculations. A file server running at capacity, on the other hand,
    is likely worth investigating.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, a load average above 0.60 should be investigated. Things are not
    urgent, but there may be a problem around the corner. A server that regularly
    reaches 1.00 after all known optimizations have been made is a clear candidate
    for scaling, as of course is any server exceeding that average.
  prefs: []
  type: TYPE_NORMAL
- en: 'Node also offers native process information via the `os` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Socket usage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When the number of persistent socket connections begins to grow past the capacity
    of any single Node server, however optimized, it will be necessary to think about
    spreading out the servers handling user sockets. Using `socket.io`, it is possible
    to check the number of connected clients at any time using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In general, it is best to track web socket connection counts within the application,
    via some sort of tracking/logging system.
  prefs: []
  type: TYPE_NORMAL
- en: Many file descriptors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When the number of file descriptors opened in an OS hovers close to its limit,
    it is likely that an excessive number of Node processes are active, files are
    open, or other file descriptors (such as sockets or named pipes) are in play.
    If these high numbers are not due to bugs or a bad design, it is time to add a
    new server.
  prefs: []
  type: TYPE_NORMAL
- en: 'Checking the number of open file descriptors of any kind can be accomplished
    using `lsof`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Data creep
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When the amount of data being managed by a single database server begins to
    exceed many millions of rows or many gigabytes of memory, it is time to think
    about scaling. Here, you might choose to simply dedicate a single server to your
    database, begin to share databases, or even move into a managed cloud storage
    solution earlier rather than later. Recovering from a data layer failure is rarely
    a quick fix, and in general, it is dangerous to have a single point of failure
    for something as important as *all of your data*.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you''re using Redis, the `info` command will provide most of the data you
    will need, to make these decisions. Consider the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'More information on `INFO` can be found at: [https://redis.io/commands/INFO](https://redis.io/commands/INFO).'
  prefs: []
  type: TYPE_NORMAL
- en: 'For MongoDB, you might use the `db.stats()` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Passing the argument `1024` flags `stats` to display all values in kilobytes.
  prefs: []
  type: TYPE_NORMAL
- en: 'More information can be found at: [https://docs.mongodb.com/v3.4/reference/method/db.stats/](https://docs.mongodb.com/v3.4/reference/method/db.stats/)'
  prefs: []
  type: TYPE_NORMAL
- en: Tools for monitoring servers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several tools available for monitoring servers, but few designed specifically
    for Node. One strong candidate is **N|Solid** ([https://nodesource.com/products/nsolid](https://nodesource.com/products/nsolid)),
    a company staffed by many key contributors to Node's core. This cloud service
    is easily integrated with a Node app, offering a useful dashboard visualizing
    CPU usage, average response times, and more.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other good monitoring tools to consider are listed in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Nagios**: [https://www.nagios.org](https://www.nagios.org)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Munin**: [http://munin-monitoring.org/](http://munin-monitoring.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monit**: [https://mmonit.com/](https://mmonit.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NewRelic**: [https://newrelic.com/nodejs](https://newrelic.com/nodejs)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Keymetrics**: [https://keymetrics.io/](https://keymetrics.io/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running multiple Node servers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is easy to purchase several servers and then to run some Node processes on
    them. However, how can those distinct servers be coordinated such that they form
    part of a single application? One aspect of this problem concerns clustering multiple
    identical servers around a single entry point. How can client connections be shared
    across a pool of servers?
  prefs: []
  type: TYPE_NORMAL
- en: '**Horizontal scaling** is the process of splitting up your architecture into
    network-distinct nodes and coordinating them. *Cloud computing* relates here,
    and simply means locating some of the functionality an application running on
    a server somewhere on a remote server, running somewhere else. Without a single
    point of failure (so the theory goes) the general system is more robust. The *parking
    lot problem* is another consideration that Walmart likely faces—during shopping
    holidays, you will need many thousands of parking spots, but during the rest of
    the year, this investment in empty space is hard to justify. In terms of servers,
    the ability to dynamically scale both up and down argues against building fixed
    vertical silos. Adding hardware to a running server is also a more complicated
    process than spinning up and seamlessly linking another virtual machine to your
    application.'
  prefs: []
  type: TYPE_NORMAL
- en: File serving speeds are, of course, not the only reason you might use a proxy
    server like NGINX. It is often true that network topology characteristics make
    a reverse proxy the better choice, especially when the centralization of common
    services, such as compression, makes sense. The point is simply that Node should
    not be excluded solely due to outdated biases about its ability to efficiently
    serve files.
  prefs: []
  type: TYPE_NORMAL
- en: Forward and reverse proxies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **proxy** is someone or something acting on behalf of another.
  prefs: []
  type: TYPE_NORMAL
- en: A **forward proxy** normally works on behalf of clients in a private network,
    brokering requests to an outside network, such as retrieving data from the internet.
    Earlier in this book, we looked at how one might set up a proxy server using Node,
    where the Node server functioned as an intermediary, forwarding requests from
    clients to other network servers, usually via the internet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Early web providers such as AOL functioned in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fc316c7a-de5c-4f91-a7ea-6f9c909a5543.png)'
  prefs: []
  type: TYPE_IMG
- en: Network administrators use forward proxies when they must restrict access to
    the outside world, that is, the internet. If network users are downloading malware
    from `somebadwebsite.com` via an email attachment, the administrator can block
    access to that location. Restrictions on access to social networking sites might
    be imposed on an office network. Some countries even restrict access to public
    websites in this way.
  prefs: []
  type: TYPE_NORMAL
- en: 'A **reverse proxy**, not surprisingly, works in the opposite way, accepting
    requests from a public network and servicing those requests within a private network
    the client has little much visibility into. Direct access to servers by clients
    is first delegated to a reverse proxy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e27712db-31b9-4716-ad25-1e40a6ae8e44.png)'
  prefs: []
  type: TYPE_IMG
- en: This is the type of proxy we might use to balance requests from clients across
    many Node servers. Client *X* does not communicate with any given server directly.
    A broker *Y* is the point of first contact, able to direct *X* to a server that
    is under less load, or that is located closer to *X*, or is in some other way
    the best server for *X* to access at this time.
  prefs: []
  type: TYPE_NORMAL
- en: We will now take a look at how to implement reverse proxies when scaling Node,
    discussing implementations that use **NGINX** (pronounced as **Engine X**), a
    popular choice when load balancing Node servers, and those using native Node modules.
  prefs: []
  type: TYPE_NORMAL
- en: Using the http-proxy module
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For many years, it was recommended that a web server (such as NGINX) be placed
    in front of Node servers. The claim was that mature web servers handle static
    file transfers more efficiently. While this may have been true for earlier Node
    versions (which also suffered from the bugs that new technologies face), it is
    no longer necessarily true in terms of pure speed. More importantly, using **Content
    Delivery Networks** (**CDN**) and other *edge* services the static files your
    application might need will already be cached—your server won't be serving these
    files, to begin with.
  prefs: []
  type: TYPE_NORMAL
- en: Node is designed to facilitate the creation of network software, so it comes
    as no surprise that several proxying modules have been developed. A popular production-grade
    Node proxy is **http-proxy**. Let's take a look at how we would use it to balance
    requests to different Node servers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The entirety of our routing stack will be provided by Node. One Node server
    will be running our proxy, listening on port `80`. We''ll cover the following
    three scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: Running multiple Node servers on separate ports on the same machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using one box as a pure router, proxying to external URLs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a basic round-robin load balancer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As an initial example, let''s look at how to use this module to redirect requests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: By starting this server on port `80` of our local machine, we are able redirect the
    user to another URL.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run several distinct Node servers, each responding to a different URL, on
    a single machine, one simply has to define a router:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: For each of your distinct websites, you can now point your DNS name servers
  prefs: []
  type: TYPE_NORMAL
- en: '(via ANAME or CNAME records) to the same endpoint (wherever this Node program
    is running), and they will resolve to different Node servers. This is handy when
    you want to run several websites but don''t want to create a new physical server
    for each one. Another strategy is to handle different paths within the same website
    on different Node servers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This allows specialized functionality in your application to be handled by uniquely
    configured servers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Setting up a load balancer is also straightforward. As we''ll see later with
    NGINX''s **upstream** directive, we simply provide a list of servers to be balanced:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we treat servers equally, cycling through them in order. After
    the selected server is proxied, it is returned to the *rear* of the list.
  prefs: []
  type: TYPE_NORMAL
- en: It should be clear that this example can be easily extended to accommodate other
    directives, such as NGINX's **weight**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `redbird` module is an extremely advanced reverse proxy built on top of **http-proxy**.
    Among other things, it has built-in support for automatic SSL certificate generation
    and HTTP/2 support. Learn more at: [https://github.com/OptimalBits/redbird](https://github.com/OptimalBits/redbird).'
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a NGINX load balancer on Digital Ocean
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As Node is so efficient, most websites or applications can accommodate all of
    their scaling needs in the vertical dimension. Node can handle enormous levels
    of traffic using only a few CPUs and an unexceptional volume of memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'NGINX is a very popular high-performance web server that is often used as a
    proxy server. There is some serendipity in the fact that NGINX is a popular choice with Node
    developers, given its design:'
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned on [http://www.linuxjournal.com/magazine/nginx-high-performance-web-server-and-reverse-proxy](http://www.linuxjournal.com/magazine/nginx-high-performance-web-server-and-reverse-proxy),
    "NGINX is able to serve more requests per second with [fewer] resources because
    of its architecture. It consists of a master process, which delegates work to
    one or more worker processes. Each worker handles multiple requests in an event-driven
    or asynchronous manner using special functionality from the Linux kernel (epoll/select/poll).
    This allows NGINX to handle a large number of concurrent requests quickly with
    very little overhead."
  prefs: []
  type: TYPE_NORMAL
- en: NGINX also makes load balancing very easy. In the following examples, we will
    see how proxying through NGINX comes with load balancing *out of the box.*
  prefs: []
  type: TYPE_NORMAL
- en: Digital Ocean is a cloud hosting provider that is inexpensive and easy to set
    up. We will build an NGINX load balancer on this service.
  prefs: []
  type: TYPE_NORMAL
- en: 'To sign up, visit: [https://www.digitalocean.com](https://www.digitalocean.com).
    The basic package (at the time of this writing) incurs a five dollar fee, but
    promotion codes are regularly made available; a simple web search should result
    in a usable code. Create and verify an account to get started.'
  prefs: []
  type: TYPE_NORMAL
- en: Digital Ocean packages are described as droplets, with certain characteristics—amount
    of storage space, transfer limits, and so on. A basic package is sufficient for
    our needs. Also, you will indicate a hosting region as well as the OS to install
    in your droplet (in this example, we’ll use the latest version of Ubuntu). Create
    a droplet, and check your email for login instructions. You’re done!
  prefs: []
  type: TYPE_NORMAL
- en: You will receive full login information for your instance. You can now open
    a Terminal and SSH into your box using those login credentials.
  prefs: []
  type: TYPE_NORMAL
- en: On your initial login, you might want to update your packages. For Ubuntu, you
    would run `apt-get update` and `apt-get upgrade`. Other package managers have
    similar commands (such as `yum update` for RHEL/CentOs).
  prefs: []
  type: TYPE_NORMAL
- en: Before we begin to install, let’s change our root password and create a non-root
    user (it is unsafe to expose root to external logins and software installs). To
    change your root password, type `passwd` and follow the instructions in your Terminal.
    To create a new user, enter `adduser <new user name>` (for example, `adduser john`).
    Follow the instructions.
  prefs: []
  type: TYPE_NORMAL
- en: 'One more step: we want to give some administrative privileges to our new user,
    as we’ll be installing software as that user. In Unix parlance, you want to give
    `sudo` access to this new user. Instructions on how to do this are easy to find
    for whichever OS you’ve chosen. Essentially, you will want to change the `/etc/sudoers`
    file. Remember to do this using a command such as `lvisudo`; do not edit the sudoers
    file by hand! You may also want to restrict root logins and do other SSH access
    management at this point.'
  prefs: []
  type: TYPE_NORMAL
- en: After successfully executing ` sudo -i` in your Terminal, you will be able to
    enter commands without prefixing each one with `sudo`. The following examples
    assume that you’ve done this.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll now create a NGINX load balancer frontend for two Node servers. This
    means we will create three droplets: one for the balancer, and an additional two
    droplets to serve as Node servers. At the end, we will end up with an architecture
    that looks something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a7b32bd5-9ec9-49f2-92b2-3ebc005f8a39.png)'
  prefs: []
  type: TYPE_IMG
- en: Installing and configuring NGINX
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s install NGINX and Node/npm. If you''re still logged in as root, log out
    and reauthenticate as the new user you’ve just created. To install NGINX (on Ubuntu),
    simply type this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Most other Unix package managers will have NGINX installers.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start NGINX, use this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Full documentation for NGINX can be found at: [https://www.nginx.com/resources/wiki/start/](https://www.nginx.com/resources/wiki/start/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'You should now be able to point your browser to the IP you were assigned (check
    your inbox if you''ve forgotten) and see something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0c104d17-f129-4b82-8901-142c313ab105.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, let's set up the two servers that NGINX will balance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create an additional two droplets in Digital Ocean. You will *not* install
    NGINX on these severs. Configure permissions on these servers as we did earlier,
    and install Node in both droplets. An easy way to manage your Node installation
    is using *Tim Caswell''s* **NVM** (Node Version Manager). NVM is essentially a
    bash script that provides a set of command-line tools facilitating Node version
    management, allowing you to easily switch between versions. To install it, run
    the following command in your Terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, install your preferred Node version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'You might want to add a command to your `.bashrc` or `.profile` file to ensure
    that a certain node version is used each time you start a shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'To test our system, we need to set up Node servers on both of these machines.
    Create the following program file on each server, changing `**` to something unique
    on each (such as *one* and *two*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Start this file on each server (`node serverfile.js`). Each server will now
    answer on port `8080`. You should now be able to reach this server by pointing
    a browser to each droplet’s IP:8080\. Once you have two servers responding with
    distinct messages, we can set up the NGINX load balancer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Load balancing across servers is straightforward with NGINX. You need simply
    indicate, in the NGINX configuration script, which upstream servers should be
    balanced. The two Node servers we''ve just created will be the upstream servers.
    NGINX will be configured to balance requests to each:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/59ceb3dd-08f4-42e3-9c88-19ae3e47824c.png)'
  prefs: []
  type: TYPE_IMG
- en: Each request will be handled first by NGINX, which will check its *upstream*
    configuration, and based on how it is configured, will (reverse) proxy requests
    to upstream servers that will actually handle the request.
  prefs: []
  type: TYPE_NORMAL
- en: You will find the default NGINX server configuration file on your balancer droplet
    at `/etc/nginx/sites-available/default`.
  prefs: []
  type: TYPE_NORMAL
- en: In production, you'll likely want to create a custom directory and configuration
    file, but for our purposes, we'll simply modify the default configuration file
    (you might want to make a backup before you start modifying it).
  prefs: []
  type: TYPE_NORMAL
- en: 'At the top of the NGINX configuration file, we want to define some *upstream*
    servers that will be candidates for redirection. This is simply a map with the
    `lb-servers` arbitrary key, to be referenced in the server definition that follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we''ve established the candidate map, we need to configure NGINX such
    that it will forward requests in a balanced way to each of the members of lb-servers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The key line is this one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Note how the name `lb-servers` matches the name of our upstream definition.
    This should make what is happening clear: an NGINX server listening on port `80`
    will pass the request on to a server definition as contained in lb-servers. If
    the upstream definition has only one server in it, that server gets all the traffic.
    If several servers are defined, NGINX attempts to distribute traffic evenly among
    them.'
  prefs: []
  type: TYPE_NORMAL
- en: It is also possible to balance load across several *local servers* using the
    same technique. One would simple run different Node servers on different ports,
    such as `server 127.0.0.1:8001` and `server 127.0.0.1:8002`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Go ahead and change the NGINX configuration (consulting the `nginx.config`
    file in the code bundle for this book if you get stuck). Once you''ve changed
    it, restart NGINX with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, if you prefer, use this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Assuming that the other two droplets running Node servers are active, you should
    now be able to point your browser to your NGINX-enabled droplet and see messages
    from those servers!
  prefs: []
  type: TYPE_NORMAL
- en: As we will likely want more precise control over how traffic is distributed
    across our upstream servers, there are further directives that can be applied
    to upstream server definitions.
  prefs: []
  type: TYPE_NORMAL
- en: 'NGINX balances load using a weighted round-robin algorithm. In order to control
    the relative weighting of traffic distribution, we use the weight directive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This definition tells NGINX to distribute twice as much load to the second server
    as to the first. Servers with more memory or CPUs might be favored, for example.
    Another way to use this system is to create an A/B testing scenario, where one
    server containing a proposed new design receives some small fraction of total
    traffic, such that metrics on the testing server (sales, downloads, engagement
    length, and so forth) can be compared against the wider average.
  prefs: []
  type: TYPE_NORMAL
- en: 'Three other useful directives are available, which work together to manage
    connection failures:'
  prefs: []
  type: TYPE_NORMAL
- en: '`max_fails`: The number of times communication with a server fails prior to
    marking that server as inoperative. The period of time within which these failures
    must occur is defined by `fail_timeout`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fail_timeout`: The time slice during which `max_fails` must occur, indicating
    that a server is inoperative. This number also indicates the amount of time after
    a server is marked inoperative that NGINX will again attempt to reach the flagged
    server.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Consider this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '`backup`: A server marked with this directive will only be called when and
    if *all* the other listed servers are unavailable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Additionally, there are some directives for the upstream definition that add
    some control over how clients are directed to upstream servers:'
  prefs: []
  type: TYPE_NORMAL
- en: '`least_conn`: Pass a request to the server with the least connections. This
    provides a slightly smarter balancing, taking into consideration server load as
    well as weighting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ip_hash`: The idea here is to create a hash of each connecting IP, and to
    ensure that requests from a given client are always passed to the same server.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Another commonly used tool for balancing Node servers is the dedicated load
    balancer HAProxy, available at: [http://www.haproxy.org](http://www.haproxy.org).'
  prefs: []
  type: TYPE_NORMAL
- en: Message queues – RabbitMQ
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the best ways to ensure that distributed servers maintain a dependable
    communication channel is to bundle the complexity of remote procedure calls into
    a distinct unit-a messaging queue. When one process wishes to send a message to
    another process, the message can simply be placed on this queue-like a to-do list
    for your application, with the queue service doing the work of ensuring that messages
    get delivered as well as delivering any important replies back to the original
    sender.
  prefs: []
  type: TYPE_NORMAL
- en: There are a few enterprise-grade message queues available, many of them deploying
    **AMQP** **(Advanced Message Queueing Protocol)**. We will focus on a very stable
    and well-known implementation—RabbitMQ.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install RabbitMQ in your environment, follow the instructions found at:
    [https://www.rabbitmq.com/download.html](https://www.rabbitmq.com/download.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once installed, you will start the RabbitMQ server with this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'To interact with RabbitMQ using Node, we will use the `node-amqp` module created
    by *Theo Schlossnagle*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'To use a message queue, one must first create a consumer—a binding to RabbitMQ
    that will listen for messages published to the queue. The most basic consumer
    will listen for all messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: We are now listening for messages from the RabbitMQ server bound to port `5672`.
  prefs: []
  type: TYPE_NORMAL
- en: Once this consumer establishes a connection, it will establish the name of the
    queue it will listen to, and should `bind` to an `exchange`. In this example,
    we create a topic `exchange` (the default), giving it a unique name. We also indicate
    that we would like to listen for *all* messages via `#`. All that is left to do
    is subscribe to the queue, receiving a message object. We will learn more about
    the message object as we progress. For now, note the important `data` property,
    containing sent messages.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have established a consumer, let''s publish a message to the exchange.
    If all goes well, we will see the sent message appear in our console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: We have already learned enough to implement useful scaling tools. If we have
    a number of distributed Node processes, even on different physical servers, each
    can reliably send messages to one another via RabbitMQ. Each process simply needs
    to implement an **exchange queue subscriber** to receive messages, and an **exchange
    publisher** to send messages.
  prefs: []
  type: TYPE_NORMAL
- en: Types of exchanges
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'RabbitMQ provides three types of exchanges: **direct**, **fanout**, and **topic**.
    The differences appear in the way each type of exchange processes **routing keys**—the
    first argument sent to `exchange.publish`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A direct exchange matches routing keys directly. A queue binding like the following
    one matches *only* messages sent to `''room-1''`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: As no parsing is necessary, direct exchanges are able to process more messages
    than topic exchanges in a set period of time.
  prefs: []
  type: TYPE_NORMAL
- en: A fanout exchange is indiscriminate; it routes messages to all the queues bound
    to it, ignoring routing keys. This type of exchange is used for wide broadcasts.
  prefs: []
  type: TYPE_NORMAL
- en: A topic exchange matches routing keys based on the wildcards `#` and `*`. Unlike
    other types, routing keys for topic exchanges *must* be composed of words separated
    by dots, `"animals.dogs.poodle"`, for example. A `#` matches zero or more words;
    it will match every message (as we saw in the previous example), just like a fanout
    exchange. The other wildcard is *, and this matches *exactly* one word.
  prefs: []
  type: TYPE_NORMAL
- en: Direct and fanout exchanges can be implemented using nearly the same code as
    the given topic exchange example, requiring only that the exchange type be changed,
    and bind operations be aware of how they will be associated with routing keys
    (fanout subscribers receive all messages, regardless of the key; for direct, the
    routing key must match directly).
  prefs: []
  type: TYPE_NORMAL
- en: 'This last example should drive home how topic exchanges work. We will create
    three queues with different matching rules, filtering the messages each queue
    receives from the exchange:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The `node-amqp` module contains further methods for controlling connections,
    queues, and exchanges; in particular, it contains methods for removing queues
    from exchanges, and subscribers from queues. Generally, changing the makeup of
    a running queue on the fly can lead to unexpected errors, so use these with caution.
  prefs: []
  type: TYPE_NORMAL
- en: 'To learn more about the AMQP (and the options available when setting up with
    `node-amqp`), visit: [http://www.rabbitmq.com/tutorials/amqp-concepts.html](http://www.rabbitmq.com/tutorials/amqp-concepts.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Using Node's UDP module
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**UDP** **(User Datagram Protocol)** is a lightweight core internet messaging
    protocol, enabling servers to pass around concise **datagrams**. UDP was designed
    with a minimum of protocol overhead, forgoing delivery, ordering, and duplication
    prevention mechanisms in favor of ensuring high performance. UDP is a good choice
    when perfect reliability is not required and high-speed transmission is, such
    as what is found in networked video games and videoconferencing applications.'
  prefs: []
  type: TYPE_NORMAL
- en: This is not to say that UDP is normally unreliable. In most applications, it
    delivers messages with high probability. It is simply not suitable when *perfect*
    reliability is needed, such as in a banking application. It is an excellent candidate
    for monitoring and logging applications, and for non-critical messaging services.
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating a UDP server with Node is straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The `bind` command takes three arguments, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**port**: The `Integer` port number.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**address**: This is an optional address. If this is not specified, the OS
    will try to listen on all addresses (which is often what you want). You might
    also try using `0.0.0.0` explicitly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**callback**: This is an optional callback, which receives no arguments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This socket will now emit a `message` event whenever it receives a datagram
    via the `41234` port. The event callback receives the message itself as a first
    parameter, and a map of packet information as the second:'
  prefs: []
  type: TYPE_NORMAL
- en: '**address**: The originating IP'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**family**: One of IPv4 or IPv6'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**port**: The originating port'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**size**: The size of the message in bytes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This map is similar to the map returned when calling `socket.address()`.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the message and listening events, a UDP socket also emits a **close**
    and **error** event, the latter receiving an `Error` object whenever an error
    occurs. To close a UDP socket (and trigger the close event), use `server.close()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sending a message is even easier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The `send` method takes the `client.send(buffer, offset, length, port, host,
    callback)` form:'
  prefs: []
  type: TYPE_NORMAL
- en: '`buffer`: A Buffer containing the datagram to be sent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`offset`: An Integer indicating the position in buffer where the datagram begins.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`length`: The number of bytes in a datagram. In combination with **offset**,
    this value identifies the full datagram within buffer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`port`: An Integer identifying the destination port.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`address`: A String indicating the destination IP for the datagram.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`callback`: An optional callback function, called after the send has taken
    place.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The size of a datagram cannot exceed 65507 bytes, which is equal to *2^16-1*
    (65535) bytes, minus the 8 bytes used by the UDP header minus the 20 bytes used
    by the IP header.
  prefs: []
  type: TYPE_NORMAL
- en: We now have another candidate for inter-process messaging. It will be rather
    easy to set up a monitoring server for our node application, listening on a UDP
    socket for program updates and stats sent from other processes. The protocol speed
    is fast enough for real-time systems, and any packet loss or other UDP hiccups
    will be insignificant taken as a percentage of total volume over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Taking the idea of broadcasting further, we can also use the `dgram` module
    to create a multicast server. A **multicast** is simply a one-to-many server broadcast.
    We can broadcast to a range of IPs that have been permanently reserved as multicast
    addresses:'
  prefs: []
  type: TYPE_NORMAL
- en: As can be found on [http://www.iana.org/assignments/multicast-addresses/multicast-addresses.xhtml](http://www.iana.org/assignments/multicast-addresses/multicast-addresses.xhtml),
    "Host Extensions for IP Multicasting [RFC1112] specifies the extensions required
    of a host implementation of the Internet Protocol (IP) to support multicasting.
    The multicast addresses are in the range 224.0.0.0 through 239.255.255.255."
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, the range between `224.0.0.0` and `224.0.0.255` is further reserved
    for special routing protocols. Also, certain port numbers are allocated for use
    by UDP (and TCP), a list of which can be found at: [https://en.wikipedia.org/wiki/List_of_TCP_and_UDP_port_numbers](https://en.wikipedia.org/wiki/List_of_TCP_and_UDP_port_numbers).'
  prefs: []
  type: TYPE_NORMAL
- en: The upshot of all this fascinating information is the knowledge that there is
    a block of IPs and ports reserved for UDP and/or multicasting, and we will use
    some of them to implement multicasting over UDP with Node.
  prefs: []
  type: TYPE_NORMAL
- en: UDP multicasting with Node
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The only difference between setting up a multicasting UDP server and a *standard*
    one is binding to a special UDP port for sending, and indicating that we''d like
    to listen to *all* available network adapters. Our multicasting server initialization
    looks like the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Once we've decided on a multicast port and an address and have bound, we catch
    the `listenng` event and configure our server. The most important command is `socket.addMembership`,
    which tells the kernel to join the multicast group at `multicastAddress`. Other
    UDP sockets can now subscribe to the multicast group at this address.
  prefs: []
  type: TYPE_NORMAL
- en: Datagrams hop through networks just like any network packet. The `setMulticastTTL`
    method is used to set the maximum number of hops (Time To Live) a datagram is
    allowed to make before it is abandoned, and not delivered. The acceptable range
    is 0-255, with the default being one (1) on most systems. This is not normally
    a setting one needs to worry about, but it is available when precise limits make
    sense, such as when packets are cheap and hops are costly.
  prefs: []
  type: TYPE_NORMAL
- en: If you'd like to also allow listening on the local interface, use `socket.setBroadcast(true)`
    and `socket.setMulticastLoopback(true)`. This is normally not necessary.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will eventually use this server to broadcast messages to all UDP listeners
    on `multicastAddress`. For now, let''s create two clients that will listen for
    multicasts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have two clients listening to the same multicast port. All that is left
    to do is the multicasting. In this example, we will use `setTimeout` to send a
    counter value every second:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The counter values will produce something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We have two clients listening to broadcasts from a specific group. Let''s add
    another client, listening on a different group, let''s say at multicast address
    `230.3.2.1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'As our server currently broadcasts messages to a different address, we will
    need to change our server configuration and add this new address with another
    `addMembership` call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now send to *both* addresses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Nothing stops the client from broadcasting to others in the group, or even
    members of another group:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Any node process that has an address on our network interface can now listen
    on a UDP multicast address for messages, providing a fast and elegant inter-process
    communication system.
  prefs: []
  type: TYPE_NORMAL
- en: Using Amazon Web Services in your application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As a few thousand users become a few million users, as databases scale to terabytes
    of data, the cost and complexity of maintaining an application begins to overwhelm
    teams with insufficient experience, funding, and/or time. When faced with rapid
    growth, it is sometimes useful to delegate responsibilities for one or more aspects
    of your application to cloud-based service providers. **AWS****(Amazon Web Services)**
    is just such a suite of cloud-computing services, offered by [amazon.com](http://amazon.com).
  prefs: []
  type: TYPE_NORMAL
- en: You will need an AWS account in order to use these examples. All the services
    we will explore are free or nearly free for low-volume development uses. To create
    an account on AWS, visit the following link: [https://aws.amazon.com/](https://aws.amazon.com/).
    Once you have created an account, you will be able to manage all of your services
    via the AWS console: [https://aws.amazon.com/console/](https://aws.amazon.com/console/)
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section we will learn how to use three popular AWS services:'
  prefs: []
  type: TYPE_NORMAL
- en: For storing documents and files we will connect with Amazon **S3**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**(Simple Storage Service)**'
  prefs: []
  type: TYPE_NORMAL
- en: Amazon's Key/Value database, **DynamoDB**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To manage a large volume of e-mail, we will leverage Amazon's **SES**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**(Simple Email Service)**'
  prefs: []
  type: TYPE_NORMAL
- en: To access these services we will use the AWS SDK for Node, which can be found
  prefs: []
  type: TYPE_NORMAL
- en: at the following link: [https://github.com/aws/aws-sdk-js](https://github.com/aws/aws-sdk-js)
  prefs: []
  type: TYPE_NORMAL
- en: 'To install the module run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Full documentation for the `aws-sdk` module can be found at: [https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/index.html](https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/index.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Authenticating
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Developers registered with AWS are assigned two identifiers:'
  prefs: []
  type: TYPE_NORMAL
- en: A public **Access Key ID** (a 20-character, alphanumeric sequence).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **Secret Access Key** (a 40-character sequence). It is very important to keep
    your Secret Key private.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon also provides developers with the ability to identify the region with
    which to communicate, such as `"us-east-1"`. This allows developers to target
    the closest servers (regional endpoint) for their requests.
  prefs: []
  type: TYPE_NORMAL
- en: The regional endpoint and both authentication keys are necessary to make requests.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a breakdown of regional endpoints, visit: [https://docs.aws.amazon.com/general/latest/gr/rande.html](https://docs.aws.amazon.com/general/latest/gr/rande.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we will be using the same credentials in each of the following examples,
    let''s create a single `config.json` file that is reused:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: We also configure the specific API versions we will use for services. Should
    Amazon's services API change, this will ensure that our code will continue to
    work.
  prefs: []
  type: TYPE_NORMAL
- en: 'An AWS session can now be initialized with just two lines of code. Assume that
    these two lines exist prior to any of the example code that follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Errors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When experimenting with these services, it is likely that error codes will
    appear on occasion. Due to their complexity, and the nature of cloud computing,
    these services can sometimes emit surprising or unexpected errors. For example,
    because S3 can only promise eventual consistency in some regions and situations,
    attempting to read a key that has just been written to may not always succeed.
    We will be exploring the complete list of error codes for each service, and they
    can be found at the following locations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**S3:**[ https://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html](https://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DynamoDB: **[http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SES: **[https://docs.aws.amazon.com/ses/latest/DeveloperGuide/api-error-codes.html](https://docs.aws.amazon.com/ses/latest/DeveloperGuide/api-error-codes.html)
    and [https://docs.aws.amazon.com/ses/latest/DeveloperGuide/smtp-response-codes.html](https://docs.aws.amazon.com/ses/latest/DeveloperGuide/smtp-response-codes.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As it will be difficult in the beginning to predict where errors might arise,
    it is important to employ the `domain` module or other error-checking code as
    you proceed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, a subtle but fundamental aspect of Amazon''s security and consistency
    model is the strict synchronization of its web server time and time as understood
    by a server making requests. A discrepancy of 15 minutes is the maximum allowed.
    While this seems like a long time, in fact time drift is very common. When developing
    watch out for 403: Forbidden errors that resemble one of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`SignatureDoesNotMatch`: This error means that the signature has expired'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RequestTimeTooSkewed`: The difference between the request time and the current
    time is too large'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If such errors are encountered, the internal time of the server making requests
    may have drifted. If so, that server''s time will need to be synchronized. On
    Unix, one can use the **NTP****(Network Time Protocol)** to achieve synchrony.
    One solution is to use the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'For more information on NTP and time synchronization, visit: [http://www.pool.ntp.org/en/use.html](http://www.pool.ntp.org/en/use.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Let's start using AWS services, beginning with the distributed file service,
    S3.
  prefs: []
  type: TYPE_NORMAL
- en: Using S3 to store files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: S3 can be used to store any file one expects to be able to store on a filesystem.
    Most commonly, it is used to store media files such as images and videos. S3 is
    an excellent document storage system as well, especially well-suited for storing
    small JSON objects or similar data objects.
  prefs: []
  type: TYPE_NORMAL
- en: Also, S3 objects are accessible via HTTP, which makes retrieval very natural,
    and REST methods such as PUT/DELETE/UPDATE are supported. S3 works very much like
    one would expect a typical file server to work, is spread across servers that
    span the globe, and offers storage capacity that is, for all practical purposes,
    limitless.
  prefs: []
  type: TYPE_NORMAL
- en: S3 uses the concept of a **bucket** as a sort of corollary to *hard drive*.
    Each S3 account can contain 100 buckets (this is a hard limit), with no limits
    on the number of files contained in each bucket.
  prefs: []
  type: TYPE_NORMAL
- en: Working with buckets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Creating a bucket is easy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'We will receive a data map containing the `Location` bucket, and a `RequestId`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'It is likely that many different operations will be made against a bucket.
    As a convenience, the `aws-sdk` allows a bucket name to be automatically defined
    in the parameter list for all further operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Use `listBuckets` to fetch an array of the existing buckets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Bucket names are global to all S3 users. No single user of S3 can use a bucket
    name that another user has claimed. If I have a bucket named `foo`, no other S3
    user can ever use that bucket name. This is a gotcha that many miss.
  prefs: []
  type: TYPE_NORMAL
- en: Working with objects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s add a document to the `nodejs-book` bucket on S3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'If the PUT is successful, its callback will receive an object similar to the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'You are encouraged to consult the SDK documentation and experiment with all
    the parameters that `putObject` accepts. Here, we focus on the only two required
    fields, and a few useful and common ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Key`: A name to uniquely identify your file within this bucket.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Body`: A Buffer, String, or Stream comprising the file body.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ServerSideEncryption`: Whether to encrypt the file within S3\. The only current
    option is AES256 (which is a good one!).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ContentType`: Standard MIME type.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ContentLength`: A String indicating the destination IP for the datagram.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ACL`: Canned access permissions, such as `private` or `public-read-write`.
    Consult the S3 documentation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It is a good idea to have the `Key` object resemble a filesystem path, helping
    with sorting and retrieval later on. In fact, Amazon''s S3 console reflects this
    pattern in its UI:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1d16565b-7ef5-48cb-b02b-7e7548a362b8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s stream an image up to S3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'As we gave this image `public-read` permissions, it will be accessible at:
    [https://s3.amazonaws.com/nodejs-book/demos/putObject/testimage.jpg](https://s3.amazonaws.com/nodejs-book/demos/putObject/testimage.jpg).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fetching an object from S3 and streaming it onto a local filesystem is even
    easier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, we can catch data events on the HTTP chunked transfer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'To delete an object, do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'To delete multiple objects, pass an Array (to a maximum of 1,000 objects):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Using AWS with a Node server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Putting together what we know about Node servers, streaming file data through
    pipes, and HTTP, it should be clear how to mount S3 as a filesystem in just a
    few lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'A standard Node HTTP server receives a request URL. We first attempt a HEAD
    operation using the `aws-sdk` method `headObject`, accomplishing two things:'
  prefs: []
  type: TYPE_NORMAL
- en: We'll determine whther the file is available
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will have the header information necessary to build a response
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After handling any non-200 status code errors, we only need to set our response
    headers and stream the file back to the requester, as previously demonstrated.
  prefs: []
  type: TYPE_NORMAL
- en: Such a system can also operate as a **fail-safe**, in both directions; should
    S3, or the file, be unavailable, we might bind to another filesystem, streaming
    from there. Conversely, if our preferred local filesystem fails, we might fall
    through to our backup S3 filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the `amazon/s3-redirect.js` file in the code bundle available at the
    Packt website for an example of using 302 redirects to similarly mount an AWS
    filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: S3 is a powerful data storage system with even more advanced features than those
    we've covered, such as object versioning, download payment management, and setting
    up objects as torrent files. With its support for streams, the `aws-sdk` module
    makes it easy for Node developers to work with S3 as if it was a local filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: Getting and setting data with DynamoDB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**DynamoDB** (**DDB**) is a NoSQL database providing very high throughput and
    predictability that can be easily scaled. DDB is designed for **data-intensive**
    applications, performing massive map/reduce and other analytical queries with
    low latency and reliably. That being said, it is also an excellent database solution
    for general web applications.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The whitepaper announcing DynamoDB was highly influential, sparking a real
    interest in NoSQL databases, and inspiring many, including **Apache** **Cassandra**.
    The paper deals with advanced concepts, but rewards careful study; it is available
    at: [http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf](http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: A Dynamo database is a collection of tables, which is a collection of items,
    which are a collection of attributes. Each item in a table (or row, if you prefer)
    must have a primary key, functioning as an index for the table. Each item can
    have any number of attributes (up to a limit of 65 KB) in addition to the primary
    key.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is an item with five attributes, one attribute serving as the primary
    key (`Id`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create a table with both a primary and a secondary key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'The callback will receive an object similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Table creation/deletion is not immediate; you are essentially queueing up the
    creation of a table (note `TableStatus`). At some point in the (near) future,
    the table will exist. As DDB table definitions cannot be changed without deleting
    the table and rebuilding it, in practice, this delay is not something that should
    impact your application—build once, and then work with items.
  prefs: []
  type: TYPE_NORMAL
- en: DDB tables must be given a schema indicating the item attributes that will function
    as keys, defined by `KeySchema`. Each attribute in `KeySchema` can be either a
    `RANGE` or a `HASH`. There must be one such index; there can be at most two. Each
    added item must contain any defined keys, with as many additional attributes as
    desired.
  prefs: []
  type: TYPE_NORMAL
- en: Each item in `KeySchema` must be matched in count by the items in `AttributeDefinitions`.
    In `AttributeDefinitions`, each attribute can be either a number (`"N"`) or a
    string (`"S"`). When adding or modifying attributes, it is always necessary to
    identify attributes by its type as well as by the name.
  prefs: []
  type: TYPE_NORMAL
- en: 'To add an item, use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition to our primary and (optional) secondary keys, we want to add other
    attributes to our item. Each must be given one of the following types:'
  prefs: []
  type: TYPE_NORMAL
- en: '`S`: A String'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`N`: A Number'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`B`: A Base64-encoded string'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SS`: An Array of Strings (String set)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NS`: An Array of Numbers (Number set)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BS`: An Array of Base64-encoded strings (Base64 set)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All items will need to have the same number of columns; again, dynamic schemas
    are *not* a feature of DDB.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assume that we''ve created a table that looks like the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Id** | **Date** | **Action** | **Cart** | **UserId** |'
  prefs: []
  type: TYPE_TB
- en: '| 123 | 1375314738466 | buy | { "song1", "song2" } | DD9DDD8892 |'
  prefs: []
  type: TYPE_TB
- en: '| 124 | 1375314738467 | buy | { "song2", "song4" } | DD9EDD8892 |'
  prefs: []
  type: TYPE_TB
- en: '| 125 | 1375314738468 | buy | { "song12", "song6" } | DD9EDD8890 |'
  prefs: []
  type: TYPE_TB
- en: Now, let's perform some search operations.
  prefs: []
  type: TYPE_NORMAL
- en: Searching the database
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two types of search operations available: **query** and **scan**.
    A scan on a table with a single primary key will, without exception, search every
    item in a table, returning those matching your search criteria. This can be very
    slow on anything but small databases. A query is a direct key lookup. We''ll look
    at queries first. Note that in this example, we will assume that this table has
    only one primary key.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To fetch the `Action` and `Cart` attributes for item `124`, we use the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'This will return the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: To select all attributes, simply omit the `AttributesToGet` definition.
  prefs: []
  type: TYPE_NORMAL
- en: A scan is more expensive, but allows more involved searches. The usefulness
    of secondary keys is particularly pronounced when doing scans, allowing us to
    avoid the overhead of scanning the entire table. In our first example of scan,
    we will work as if there is only a primary key. Then, we will show how to filter
    the scan using the secondary key.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get all the records whose `Cart` attribute contains `song2`, use the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: This will return all attribute values for items with `Id` 123 and 124.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now use our secondary key to filter this further:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: This new filter limits results to item 124.
  prefs: []
  type: TYPE_NORMAL
- en: Sending mail via SES
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Amazon describes the problems SES is designed to solve in this way:'
  prefs: []
  type: TYPE_NORMAL
- en: '"Building large-scale email solutions to send marketing and transactional messages
    is often a complex and costly challenge for businesses. To optimize the percentage
    of emails that are successfully delivered, businesses must deal with hassles such
    as email server management, network configuration, and meeting rigorous Internet
    Service Provider (ISP) standards for email content."'
  prefs: []
  type: TYPE_NORMAL
- en: Apart from the typical network scaling problems inherent in growing any system,
    providing email services is made particularly difficult due to the prevalence
    of spam. It is very hard to send a large number of unsolicited emails without
    ending up blacklisted, even when the recipients are amenable to receiving them.
    Spam control systems are automated; your service must be listed in the *whitelists*,
    which is used by various email providers and spam trackers in order to avoid having
    a low percentage of your emails end up somewhere other than your customer's inbox.
    A mail service must have a good reputation with the right people or it becomes
    nearly useless.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon's SES service has the necessary reputation, providing application developers
    with cloud-based e-mail service which is reliable and able to handle a nearly
    infinite volume of e-mail. In this section we will learn how SES can be used by
    a Node application as a reliable mail delivery service.
  prefs: []
  type: TYPE_NORMAL
- en: Ensure that you have SES access by visiting your developer console. When you
    first sign up with SES, you will be given *Sandbox* access. When in this mode,
    you are limited to using only Amazon's mailbox simulator, or sending email to
    address you have verified (such as one's own). You may request production access,
    but for our purposes, you will only need to verify an email address to test with.
  prefs: []
  type: TYPE_NORMAL
- en: 'As the cost of using a service such as SES will increase as your mail volume
    increases, you might want to periodically check your quotas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'To send a message, do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'The callback will receive something like the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: Multiple recipients, HTML body contents, and all the other features one would
    expect from a mail service are available.
  prefs: []
  type: TYPE_NORMAL
- en: Using Twilio to create an SMS bot on Heroku
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are going to be building an application that works as a customer service
    application, whereby customer service agents can field SMS requests from customers
    and respond to them. There will be two parts to the system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 1 : A client application, running on your local machine, which spins up
    a React-powered web interface that displays incoming SMS messages, indicates the
    sentiment of the message (is the customer angry? Happy?) and allows you to respond
    to the message. Note that even though this server is running on a local machine,
    it could just as well be deployed to Heroku, or somewhere else -- the goal is
    to demonstrate how many servers in different locations can intelligently communicate
    with each other.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Part 2 : A *switchboard* that fields messages arriving via the Twilio SMS gateway,
    processes them, and distributes messages across any number of client servers --
    if you have 10 customer service representatives connected to the switchboard using
    the client application, messages the switchboard receives will be spread across
    these clients evenly. This second application will be deployed on Heroku.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You will first need to get an account on **Heroku**, a cloud server provider
    similar to Digital Ocean: [http://www.heroku.com](http://www.heroku.com)'
  prefs: []
  type: TYPE_NORMAL
- en: Heroku provides free accounts, so you will be able to build out the following
    application without any cost to you.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have your account, log in and download the Heroku CLI for your system: [https://devcenter.heroku.com/articles/getting-started-with-nodejs#set-up](https://devcenter.heroku.com/articles/getting-started-with-nodejs#set-up).
    Follow the steps on that page to log in to the command line toolbelt for Heroku.
  prefs: []
  type: TYPE_NORMAL
- en: Make sure you have Git installed ([https://git-scm.com/book/en/v2/Getting-Started-Installing-Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a directory on your local file system, and clone the following two repositories
    into that folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/sandro-pasquali/thankyou](https://github.com/sandro-pasquali/thankyou)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/sandro-pasquali/switchboard](https://github.com/sandro-pasquali/switchboard)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *thankyou *repository is the client application. You will now deploy the *switchboard*
    repository to Heroku.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using your Terminal navigate to the *switchboard* repository and deploy a copy
    to Heroku:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see something like the following displayed in your Terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3742c243-35e8-497b-8b4f-cc12e582c705.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Heroku has established a Git endpoint on your server. Now, run the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see a list of two elements returned: *heroku* and *origin*. These
    are the two remote branches that your local *switchboard* repository is tracking,
    the one on Heroku and the one you originally cloned from.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to push your local repository into the Heroku repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see a lot of installation instructions. When everything completes
    successfully navigate to your application URL. You should see that there is an
    *Application Error*. Heroku provides complete logs for your application. Let''s
    access them now to discover what went wrong:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'To keep a running tail of log activity on your Heroku server, use:'
  prefs: []
  type: TYPE_NORMAL
- en: '**` > heroku logs --tail`**'
  prefs: []
  type: TYPE_NORMAL
- en: You should see several errors around the absence of environment variables, especially
    for Twilio. The application expects these to be set in the application environment,
    and they haven't been. Let's do that now.
  prefs: []
  type: TYPE_NORMAL
- en: Using Twilio webhooks to receive and send SMS messages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The switchboard application ultimately provides a single service—to set up a
    REST-enabled endpoint that Twilio can call with SMS messages received on the number
    you've registered. It stores a log of those messages on a per-phone-number basis
    in LevelDB (a very fast key/value storage library), broadcasting new messages
    to clients connected to the switchboard.
  prefs: []
  type: TYPE_NORMAL
- en: 'The logical flow of the entire application will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/41dc79a2-620b-464c-911c-cf54e2030a86.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that the logic of our application begins with an SMS from Twilio,
    and supports responses from clients. This is the basic pattern for constructing
    a *NoUI*, or pure SMS application. This pattern is growing in popularity, often
    seen in the form of chat bots, AI-enabled assistants, and so on. We'll dig deeper
    into the application soon.
  prefs: []
  type: TYPE_NORMAL
- en: Right now, we need to enable the Twilio bridge.
  prefs: []
  type: TYPE_NORMAL
- en: To start, you will need to create a test account on Twilio to get some API variables.
    Go to [https://www.twilio.com](https://www.twilio.com) and sign up for a test
    account. Ensure that you set up a test phone number; we'll be sending SMS messages
    to that number.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you''ve done that, grab your Twilio API keys from the account Dashboard,
    your phone number, and your phone number SID. Now you''ll need to add that information
    to the environment variables for Heroku, along with some other keys. Go to your
    Heroku dashboard, find your instance, click on it, and navigate to Settings |
    Reveal Config Vars:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/04197e4a-5fa5-470a-9412-e5172e94c99f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is where you add key/value pairs to `process.env` in your running Node
    process. Add the following key/value pairs:'
  prefs: []
  type: TYPE_NORMAL
- en: '`TWILIO_AUTH_TOKEN` / <your auth token>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TWILIO_SID` / <your sid>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TWILIO_PHONE_NUMBER_SID` / <your phone # sid>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TWILIO_DEFAULT_FROM` / <your assigned phone number>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SOCK_PORT` / `8080`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`URL` / <your server URL (with no trailing slash)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once you've saved these new environment variables, your application will automatically
    restart on Heroku. Try your application URL again. If everything is working, you
    should see a message like Switchboard is active.
  prefs: []
  type: TYPE_NORMAL
- en: You can quickly load your application into a browser from the command line with **`> heroku open`**.
  prefs: []
  type: TYPE_NORMAL
- en: While we will not be using the shell, you can log in to your Heroku box via
    your Terminal with `> heroku run bash`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To communicate with Twilio, we''ll be using the official Node library at: [https://github.com/twilio/twilio-node](https://github.com/twilio/twilio-node).
    The important code can be found in the `router/sms/` folder of the switchboard
    repository. This module will allow us to register a webhook to receive messages, and
    to respond to those messages.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's build the switchboard.
  prefs: []
  type: TYPE_NORMAL
- en: The switchboard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The switchboard will have a single responsibility—to communicate with Twilio.
    Since we're using webhooks, we'll need to create a server that can catch POST
    data from Twilio. For the web server, we'll use the `restify` package ([http://www.restify.com](http://www.restify.com)).
    This is a very fast Node server implementation that is designed specifically for
    fast, high-load REST-based APIs. Since the switchboard is solely focused on handling
    incoming messages from Twilio, and its outbound traffic is through WebSockets,
    there is no need for a *higher-level* server like Express, which is designed to
    facilitate the presentation of views through templates, sessions, and so forth.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the code instantiating a restify server that accepts POST messages
    from Twilio containing SMS message data, and the socket server to bind clients
    to the switchboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'With very little code, we''ve set up a web server and extended it with a socket
    server (using the excellent `ws` module, which you can grab at: [https://github.com/websockets/ws](https://github.com/websockets/ws)).
    You are encouraged to look at the code in the switchboard repository, where the
    details of how client connections are managed should be easy to follow. In particular,
    investigate the `router/Db/index.js` file, where a `levelDB` connection is established
    and an API for storing message histories (`api.addToNumberHistory`) is defined.
    For our purposes, note the `server.get` method, which establishes a handler for
    GET requests used simply as a *ping* service should we need to check whether the
    switchboard is available. We''ll add the important webhook route next.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Twilio webhook code is presented with the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'The code in that file looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Given that we''re connecting to and using a global SMS gateway, the code is
    surprisingly simple. After instantiating an instance of the Twilio API using the
    environment variables we set earlier on Heroku, we can conveniently use this API
    to programmatically establish a webhook, avoiding the manual process of logging
    into a Twilio dashboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: More importantly, this technique allows us to dynamically reconfigure the Twilio
    endpoint Twilio; it is always nice to be able to *hot swap* handlers should naming,
    or something else, change.
  prefs: []
  type: TYPE_NORMAL
- en: 'The body that Twilio POSTs and we will be receiving looks something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: In the hook handler, we grab the key info—the number of the sender and the message—storing
    them in `levelDB` via the `api.addToNumberHistory` method (which returns a Promise).
    Now we are ready to inform a client of the message. How do we do that?
  prefs: []
  type: TYPE_NORMAL
- en: Clients are connected via a websocket. We can, after writing to the DB, simply
    send the message to the client in the same function body. However, now our code
    is starting to become complex, taking on two responsibilities (receiving and sending)
    rather than just one (receiving). It may seem like a small matter, but this is
    the sort of place where feature creep appears—maybe next we add logging in this
    function, and so on. Also, if we are responsible for notifying clients of new
    messages, we'll also be required to confirm that a database write was successful;
    this is often not clear cut, false positives not being out of the ordinary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of doing that, let''s create a notification system that broadcasts
    changesets. Whenever a new message is written to the DB—a confirmed write—announce
    that update event, and register an event handler. In our intial server code, this
    functionality is bound using the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'The code to catch changesets and broadcast them uses *Domenic Tarr''s* `level-live-stream`
    package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: Using `level-live-stream`, we are able to concentrate our logic on the right
    event—a confirmed write to the database—leaving this *microservice* responsible
    solely for finding an available client and sending them the updated message history.
    Note that the way clients are stored and referenced in this example is not at
    all definitive. You might want to continue with the *do one thing well* philosophy
    and create another small service solely responsible for brokering connections.
    For example, we might remove all the client code from this example and create
    another service exposing a `getNextAvailableClient` method. This type of compositional
    strategy orchestrating microservices will be discussed further in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now receive, store, and broadcast incoming SMS messages. There is only
    one bit of functionality left—sending client responses back to Twilio, continuing
    the SMS conversation. The composition of these responses is performed by the *thankyou*
    application we''ll be discussing next. Ultimately, however, those responses are
    directed by the switchboard (recall the preceding sequence diagram), and the following
    is the very short code that you can use to send SMS messages through the Twilio
    gateway:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'You should recall an *on message* listener for the websocket registered in
    our base server code, which uses the preceding functionality to respond to callers.
    We can now expand that listener:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: We see `addToNumberHistory` here again since responses are of course part of
    the conversation history. Once the outgoing message is added to the database record,
    send it along via Twilio. Did you note something? That's only one of the things
    we have to do. The other is send the updated message history *back to the client*
    so that this response can appear in their view of the history. Typically, this
    is done using some client JavaScript that, when the client types a response and
    clicks on *send*, it optimistically updates the client state in the hope the message
    makes it to the switchboard. What does it not, though?
  prefs: []
  type: TYPE_NORMAL
- en: We see how the changeset approach helps here. If the client message fails to
    reach the switchboard or otherwise fails, the `levelDB` will never be updated
    and the client's history state will not be out of sync with the canonical history
    represented by the data layer. While this may not matter so much in a trivial
    application like this one, it will matter if you're building transactional software.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's walk through the other half of the application—the *thankyou* client.
  prefs: []
  type: TYPE_NORMAL
- en: The ThankYou interface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To recap, we want to create a system whereby a switchboard receives SMS messages
    and passes them along to *service representatives* running a conversational interface
    on their local laptop or similar. This client is defined in the `thankyou` repository,
    and will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a0f0409d-094e-4e6d-8361-8d3ea78df642.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we see the message history the switchboard manages, and the interface
    for sending back responses. Importantly, there are icons indicating the sentiment
    (the winking happiness, the sad anger), of the messages as well as their timestamp
    in a human readable format (*a few seconds ago*). The goal for `thankyou` will
    be to catch incoming (and outgoing) messages, run sentiment analysis on the message
    stream, and display the results. We'll use React to build the UI.
  prefs: []
  type: TYPE_NORMAL
- en: React requires a build system, and we're using **Browserify**, **Gulp**, and
    **BrowserSync**. How those technologies work is beyond the scope of this chapter.
    Go over the contents of `gulpfile.js` and the `/source` directory of the `thankyou`
    repository. There are many online tutorials for these popular technologies.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we are serving a real UI, for this project, we will use Express to build
    our Node server. Still, the server is very simple. It is only responsible for
    serving the single view just pictured, which is contained in a single `index.html`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: React components are bundled by the build system into `components.js`; JavaScript
    files are similarly bundled into `app.js`, as stylesheets are into `app.css`.
    The jQuery DOM manipulation library is used for simple element effects and for
    managing the message composer. As mentioned, we won't be going deep into client
    JavaScript. It will be useful to take a brief look at the React component used
    to construct the timeline, as this component is ultimately what will be receiving
    new messages from the switchboard.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the main UI component powering `thankyou`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: Even if you don't know React, you should be able to see that the `MessageHistory`
    component extends the `Timeline` component. The `Timeline` component is responsible
    for maintaining the application state, or in our case, the current message history.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the key UI code in `MessageHistory`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'You may recall the message history that switchboard works with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the section in the `MessageHistory` where that data is rendered into
    UI views. We won''t go too much farther into the UI code, but you should note
    a property that switchboard did not generate: `it.sentiment`. Keep that in mind
    that as we go over how thankyou communicates data with switchboard.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the switchboard receives and sends messages over WebSockets, `Timeline`
    has such a reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'The socket code was included in our `index.html` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, it looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'This code puts the `ws` reference in the browser global scope on the client
    (`window.ws`). While generally not the best practice, for our simple UI, this
    makes it easy for React components to grab the same socket reference. This reference
    is also used in the `MessageComposer` component, which accepts responses from
    the client:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: This component renders a text area, into which responses can be composed and
    sent, via socket, to the switchboard.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's look at how the client server communicates with the client UI, brokering
    communication with the switchboard.
  prefs: []
  type: TYPE_NORMAL
- en: 'The client server is defined in `router/index.js`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a standard Express setup. Note the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the main broker logic. Recalling the application sequence diagram,
    here is where switchboard messages are received and passed along, through another
    socket, to the UI and ultimately, the React renderer. Similarly, the server listens
    for messages from the UI and passes those along to the switchboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: This should make sense now, given the *switchboard* design. Starting at the
    bottom, we see that when the client socket server `localClientSS` receives a message,
    it validates and passes the message along to the *switchboard*, where it will
    be added to the message history for the phone number this client is handling.
    More interesting is the code to receive messages from the switchboard, which performs
    sentiment analysis and converts timestamps into human readable sentences.
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform these transformations, the payload received from the *switchboard*
    (an Array in JSON format) is converted into an object stream using the `arrayToStream.js`
    module. Streams are covered in [Chapter 3](c7665bc9-3f44-4d7c-8318-61f9dfe962b3.xhtml), *Streaming
    Data Across Nodes and Clients*; we''re simply creating a `Readable` Stream that
    pipes each element in the array as a distinct object. The real fun begins when
    we apply transformations. Let''s look at the code to do sentiment analysis (which
    processes the `''message''` property of the history object), using the `through2`
    module ([https://github.com/rvagg/through2](https://github.com/rvagg/through2))
    to simplify the creation and design of a transform stream, and of course, the
    sentiment module ([https://github.com/thisandagain/sentiment](https://github.com/thisandagain/sentiment))
    to gauge the mood of the message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: The functionality is straightforward; for each object in the history array sent
    from the switchboard, determine the sentiment score for the message. Negative
    scores are *bad*, along the range from very bad (*devil*) to *unhappy*. We similarly
    score positive sentiments along a range from very good (*wink*) to *happy*. A
    new `sentiment` property is added to the message object, and as we saw earlier
    when considering the `MessageHistory` component, this will set which icon the
    message receives in the UI.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to continue development of this application on your own, you should
    fork the repositories onto your own GitHub account, and repeat the process with
    these personal repositories. This will allow you to push changes and otherwise
    modify the application to suit your own needs.
  prefs: []
  type: TYPE_NORMAL
- en: The coordination of *switchboard* and *thankyou* should give you some ideas
    on how to use services, sockets, REST endpoints, and third-party APIs to distribute
    functionality, helping you scale through adding (or removing) components, across
    the stack. By using transform streams, you can apply "on the fly" stream data
    transformations without blocking, managing data models on your servers, and leaving
    layout to the UI itself.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Big data applications have placed significant responsibility on developers of
    network applications to prepare for scale. Node has offered help in creating a
    network-friendly application development environment that can easily connect to
    other devices on a network, such as cloud services and, in particular, other Node
    servers.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we learned some good strategies for scaling Node servers, from
    analyzing CPU usage to communicating across processes. With our new knowledge
    of message queues and UDP, we can build networks of Node servers scaling horizontally,
    letting us handle more and more traffic by simply replicating the existing nodes.
    Having investigated load balancing and proxying with both Node and NGINX, we can
    confidently add capacity to our applications. When matched with the cloud services
    provided by Digital Ocean, AWS, and Twilio, we can attempt enterprise-scale development,
    data storage, and broadcast at a low cost and without adding much complexity to
    our application.
  prefs: []
  type: TYPE_NORMAL
- en: As our applications grow, we will need to maintain continuous awareness of how
    each part as well as the whole is behaving. As we keep adding new components and
    functionality, some local, some through the cloud, some maybe even written in
    another language, how do we, as developers, intelligently track and plan additions
    and other changes? In the next chapter, we will learn about microservices, a way
    of developing one application out of many, small, cooperating horizontally-distributed
    network services.
  prefs: []
  type: TYPE_NORMAL
