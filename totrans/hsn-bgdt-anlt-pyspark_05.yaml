- en: Powerful Exploratory Data Analysis with MLlib
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will explore Spark's capability to perform regression tasks
    with models such as linear regression and **support-vector machines** (**SVMs**).
    We will learn how to compute summary statistics with MLlib, and discover correlations
    in datasets using Pearson and Spearman correlations. We will also test our hypothesis
    on large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Computing summary statistics with MLlib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the Pearson and Spearman methods to discover correlations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing our hypotheses on large datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing summary statistics with MLlib
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will be answering the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What are summary statistics?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we use MLlib to create summary statistics?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLlib is the machine learning library that comes with Spark. There has been
    a recent new development that allows us to use Spark's data-processing capabilities
    to pipe into machine learning capabilities native to Spark. This means that we
    can use Spark not only to ingest, collect, and transform data, but we can also
    analyze and use it to build machine learning models on the PySpark platform, which
    allows us to have a more seamless deployable solution.
  prefs: []
  type: TYPE_NORMAL
- en: Summary statistics are a very simple concept. We are familiar with average,
    or standard deviation, or the variance of a particular variable. These are summary
    statistics of a dataset. The reason why it's called a summary statistic is that it
    gives you a summary of something via a certain statistic. For example, when we
    talk about the average of a dataset, we're summarizing one characteristic of that
    dataset, and that characteristic is the average.
  prefs: []
  type: TYPE_NORMAL
- en: Let's check how to compute summary statistics in Spark. The key factor here
    is the `colStats` function. The `colStats` function computes the column-wise summary
    statistics for an `rdd` input. The `colStats` function accepts one parameter,
    that is `rdd`, and it allows us to compute different summary statistics using
    Spark.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the code in the Jupyter Notebook (available at [https://github.com/PacktPublishing/Hands-On-Big-Data-Analytics-with-PySpark/tree/master/Chapter05](https://github.com/PacktPublishing/Hands-On-Big-Data-Analytics-with-PySpark/tree/master/Chapter05))
    for this chapter in `Chapter5.ipynb`. We will first collect the data from the `kddcup.data.gz`
    text file and pipe this into the `raw_data` variable as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The `kddcup.data` file is a **comma-separated value** (**CSV**) file. We have
    to split this data by the `,` character and put it in the `csv` variable as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take the first feature `x[0]` of the data file; this feature represents
    the `duration`, that is, aspects of the data. We will transform it into an integer
    here, and also wrap it in a list as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This helps us do summary statistics over multiple variables, and not just one
    of them. To activate the `colStats` function, we need to import the `Statistics`
    package, as shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This `Statistics` package is a sub package of `pyspark.mllib.stat`. Now, we
    need to call the `colStats` function in the `Statistics` package and feed it some
    data. Here, we are talking about the `duration` data from the dataset and we''re
    feeding the summary statistics into the `summary` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'To access different summary statistics, such as the mean, standard deviation,
    and so on, we can call the functions of the `summary` objects, and access different
    summary statistics. For example, we can access the `mean`, and since we have only
    one feature in our `duration` dataset, we can index it by the `00` index, and
    we''ll get the mean of the dataset as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, if we import the `sqrt` function from the Python standard library,
    we can create the standard deviation of the durations seen in the datasets, as
    demonstrated in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'If we don''t index the summary statistics with `[0]`, we can see that `summary.max()`
    and `summary.min()` gives us back an array, of which the first element is the
    summary statistic that we desire, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Using Pearson and Spearman correlations to discover correlations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will look at two different ways of computing correlations
    in your datasets, and these two methods are called Pearson and Spearman correlations.
  prefs: []
  type: TYPE_NORMAL
- en: The Pearson correlation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Pearson correlation coefficient shows us how two different variables vary
    at the same time, and then adjusts it for how much they vary. This is probably
    one of the most popular ways to compute a correlation if you have a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The Spearman correlation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spearman's rank correlation is not the default correlation calculation that
    is built into PySpark, but it is very useful. The Spearman correlation coefficient
    is the Pearson correlation coefficient between the ranked variables. Using different
    ways of looking at correlation gives us more dimensions of understanding on how
    correlation works. Let's look at how we calculate this in PySpark.
  prefs: []
  type: TYPE_NORMAL
- en: Computing Pearson and Spearman correlations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To understand this, let''s assume that we are taking the first three numeric
    variables from our dataset. For this, we want to access the `csv` variable that
    we defined previously, where we simply split `raw_data` using a comma (`,`). We
    will consider only the first three columns that are numeric. We will not take
    anything that contains words; we''re only interested in features that are purely
    based on numbers. In our case, in `kddcup.data`, the first feature is indexed
    at `0`; feature 5 and feature 6 are indexed at `4` and `5`, respectively which
    are the numeric variables that we have. We use a `lambda` function to take all
    three of these into a list and put it into the `metrics` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In the *Computing summary statistics with MLlib* section, we simply took the
    first feature into a list and created a list with a length of one. Here, we're
    taking three quantities of three variables into the same list. Now, each list
    has a length of three.
  prefs: []
  type: TYPE_NORMAL
- en: To compute the correlations, we call the `corr` method on the `metrics` variable and
    specify the `method` as `"spearman"`. PySpark would give us a very simple matrix
    telling us the correlation between the variables. In our example, the third variable
    in our `metrics` variable is more correlated than the second variable.
  prefs: []
  type: TYPE_NORMAL
- en: If we run `corr` on `metrics` again, but specify that the method is `pearson`,
    then it gives us Pearson correlations. So, let's examine why we need to be qualified
    as the data scientist or machine learning researcher to call these two simple
    functions and simply change the value of the second parameter. A lot of machine
    learning and data science revolves around our understanding of statistics, understanding
    how data behaves, an understanding of how machine learning models are grounded,
    and what gives them their predictive power.
  prefs: []
  type: TYPE_NORMAL
- en: So, as a machine learning practitioner or a data scientist, we simply use PySpark
    as a big calculator. When we use a calculator, we never complain that the calculator
    is simple to use—in fact, it helps us to complete the goal in a more straightforward
    way. It is the same case with PySpark; once we move from the data engineering
    side to the MLlib side, we will notice that the code gets incrementally easier.
    It tries to hide the complexity of the mathematics underneath it, but we need
    to understand the difference between different correlations, and we also need
    to know how and when to use them.
  prefs: []
  type: TYPE_NORMAL
- en: Testing our hypotheses on large datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will look at hypothesis testing, and also learn how to test
    the hypotheses using PySpark. Let's look at one particular type of hypothesis
    testing that is implemented in PySpark. This form of hypothesis testing is called
    Pearson's chi-square test. Chi-square tests how likely it is that the differences
    in the two datasets are there by chance.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we had a retail store without any footfall, and suddenly you
    get footfall, how likely is it that this is random, or is there even any statistically
    significant difference in the level of visitors that we are getting now in comparison
    to before? The reason why this is called the chi-square test is that the test
    itself references the chi-square distributions. You can refer to online documentation
    to understand more about chi-square distributions.
  prefs: []
  type: TYPE_NORMAL
- en: There are three variations within Pearson's chi-square test. We will check whether
    the observed datasets are distributed differently than in a theoretical dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how we can implement this. Let's start by importing the `Vectors`
    package from `pyspark.mllib.linalg`. Using this vector, we're going to create
    a dense vector of the visitor frequencies by day in our store.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s imagine that the frequencies go from `0.13` an hour to `0.61`, `0.8`,
    and `0.5`, finally ending on Friday at `0.3`. So, we are putting these visitor
    frequencies into the `visitors_freq` variable. Since we''re using PySpark, it
    is very simple for us to run a chi-square test from the `Statistics` package,
    which we have already imported as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'By running the chi-square test, the `visitors_freq` variable gives us a bunch
    of useful information, as demonstrated in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ec0a248d-d599-476c-bbd7-665a504a76bc.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding output shows the chi-square test summary. We've used the `pearson`
    method, where there are `4` degrees of freedom in our Pearson chi-square test,
    and the statistics are `0.585`, which means that the `pValue` is `0.964`. This
    results in no presumption against the null hypothesis. In this way, the observed
    data follows the same distribution as expected, which means our visitors are not
    actually different. This gives us a good understanding of hypothesis testing.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned summary statistics and computing the summary statistics
    with MLlib. We also learned about Pearson and Spearman correlations, and how we
    can discover these correlations in our datasets using PySpark. Finally, we learned
    one particular way of performing hypothesis testing, which is called the Pearson
    chi-square test. We then used PySpark's hypothesis-testing functions to test our
    hypotheses on large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we're going to look at putting the structure on our big
    data with Spark SQL.
  prefs: []
  type: TYPE_NORMAL
