- en: Chapter 10. Story De-duplication and Mutation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How large is the World Wide Web? Although it is impossible to know the exact
    size - not to mention the Deep and Dark Web - it was estimated to hold more than
    a trillion pages in 2008, that, in the data era, was somehow the middle age. Almost
    a decade later, it is safe to assume that the Internet's collective brain has
    more neurons than our actual gray matter that's stuffed between our *ears*. But
    out of these trillion plus URLs, how many web pages are truly identical, similar,
    or covering the same topic?
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will de-duplicate and index the GDELT database into stories.
    Then, we will track stories over time and understand the links between them, how
    they may mutate and if they could lead to any subsequent event in the near future.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understand the concept of *Simhash* to detect near duplicates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build an online de-duplication API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build vectors using TF-IDF and reduce dimensionality using *Random Indexing*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build stories connection in pseudo real-time using Streaming KMeans
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting near duplicates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While this chapter is about grouping articles into stories, this first section
    is all about detecting near duplicates. Before delving into the de-duplication
    algorithm itself, it is worth introducing the notion of story and de-duplication
    in the context of news articles. Given two distinct articles - by distinct we
    mean two different URLs - we may observe the following scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: The URL of article 1 actually redirects to article 2 or is an extension of the
    URL provided in article 2 (some additional URL parameters, for instance, or a
    shortened URL). Both articles with the same content are considered as *true duplicates*
    although their URLs are different.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both article 1 and article 2 are covering the exact same event, but could have
    been written by two different publishers. They share lots of content in common,
    but are not truly similar. Based on certain rules explained hereafter, they might
    be considered as *near-duplicates*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both article 1 and article 2 are covering the same type of event. We observe
    major differences in style or different *flavors* of the same topic. They could
    be grouped into a common *story*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both article 1 and article 2 are covering two different events. Both contents
    are *different* and should not be grouped within the same story, nor should they
    be considered as near duplicates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Facebook users must have noticed the *related articles* feature. When you like
    a news article-click on an article's link or play an article's video, Facebook
    thinks this link is interesting and updates its timeline (or whatever it is called)
    to display more content that looks similar. In *Figure 1*, I was really amused
    to see the Samsung Galaxy Note 7 smartphone emitting smoke or catching fire, therefore
    banned from most of US flights. Facebook automatically suggested me similar articles
    around this Samsung fiasco. What probably happened is that by opening this link
    I may have queried the Facebook internal APIs and asked for similar content. Here
    comes the notion of finding near duplicates in real-time, and here is what we
    will try to build in the first section.
  prefs: []
  type: TYPE_NORMAL
- en: '![Detecting near duplicates](img/image_10_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Facebook suggesting related articles'
  prefs: []
  type: TYPE_NORMAL
- en: First steps with hashing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Finding true duplicates is easy. Two articles will be considered as identical
    if their content is the same. But instead of comparing strings (that may be large
    and therefore not efficient), we can compare their hash values just like one would
    compare hand signatures; two articles having the same signature should be considered
    as identical. A simple `groupBy` function would detect the true duplicatess out
    of a string array as shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'But even the most complex hash function leads to some collisions. Java''s built-in
    `hashCode` function is encoding a string into a 32-bit integer, which means that,
    in theory, we *only* have 2^(32) possibilities of getting different words sharing
    the same hash value. In practice, collisions should always be handled carefully,
    as, according to the *birthday paradox*, they will appear much more often than
    once every 2^(32) values. To prove our point, the following example considers
    the four different strings to be identical:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Also, some articles may sometimes only differ by a very small portion of text,
    for example, a piece of advertisement, an additional footer, or an extra bit in
    the HTML code that make a hash signature different from almost identical content.
    In fact, even a minor typo on one single word would result in a total different
    hash value, making two near-duplicate articles to be considered as totally different.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Although the strings `Hello Spark` and `Hello, Spark` are really close (they
    only differ by 1 character), their hash values differ by 16-bits (out of 32).
    Luckily, the elders of the Internet may have found a solution to detect near duplicates
    using hash values.
  prefs: []
  type: TYPE_NORMAL
- en: Standing on the shoulders of the Internet giants
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Needless to say, Google is fairly good at indexing webpages. With more than
    a trillion distinct URLs, detecting duplicates is the key when it comes to indexing
    web content. Surely the Internet giants must have developed techniques over the
    years to solve this problem of scale, hence limiting the amount of computing resources
    needed to index the whole Internet. One of these techniques described here is
    called *Simhash* and is so simple and neat, albeit so efficient, that it is worth
    knowing if you truly want to *Master Spark for data science*.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: More information about *Simhash* could be found at [http://www.wwwconference.org/www2007/papers/paper215.pdf](http://www.wwwconference.org/www2007/papers/paper215.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Simhashing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The main idea behind **Simhash** is not to compute one single hash value at
    once, but rather to look at the article''s content and compute multiple individual
    hashes. For each word, each pair of word, or even each two-character shingle,
    we can easily compute hash values using the simple Java built-in `hashCode` function
    described earlier. In the following *Figure 2*, we report all the 32-bit hash
    values (the first 20 zero values omitted) of the two characters set included in
    the string **hello simhash**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Simhashing](img/B05261_10_02-1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Building hello simhash shingles'
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple Scala implementation is reported next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: With all these hash values computed, we initialize a `Simhash` object as a zero
    integer. For each bit in that 32-bit integer, we count the number of hash values
    in our list with that particular bit set to 1 and subtract the number of values
    within that same list with that particular bit that is not set. This gives us
    the array reported in *Figure 3*. Finally, any value greater than 0 will be set
    to 1, any value lower or equal to 0 will be left as 0\. The only tricky part here
    is to work on bit shifting operations, but the algorithm itself is fairly trivial.
    Note that we use recursion here to avoid the use of mutable variables (using `var`)
    or lists.
  prefs: []
  type: TYPE_NORMAL
- en: '![Simhashing](img/B05261_10_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Building hello simhash'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The hamming weight
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is easy to understand that the more words two articles have in common, the
    more both of them will share a same bit *b* set to 1 in their Simhash. But the
    beauty of Simhash comes with this aggregation step. Many other words in our corpus
    (hence other hashes) may not have this particular bit *b* set, hence making this
    value to also decrease when some different hashes are observed. Sharing a common
    set of words is not enough, similar articles must also share the same word frequency.
    The following example shows three Simhash values computed for the strings **hello
    simhash**, **hello minhash**, and **hello world**.
  prefs: []
  type: TYPE_NORMAL
- en: '![The hamming weight](img/B05261_10_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Comparing hello simhash'
  prefs: []
  type: TYPE_NORMAL
- en: When both **hello simhash** and **hello world** differ by 3-bits, **hello simhash** and
    **hello minhash** only differ by **1**. In fact, we can express the distance between
    them as the hamming weight of their EXCLUSIVE OR (**XOR**) product. **Hamming
    weight** is the number of bits we need to change in order to turn a given number
    into the zero element. The hamming weight of the **XOR** operation of two numbers
    is therefore the number of bits that differ between these two elements, **1**
    in that case.
  prefs: []
  type: TYPE_NORMAL
- en: '![The hamming weight](img/B05261_10_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Hamming weight of hello simhash'
  prefs: []
  type: TYPE_NORMAL
- en: We simply use Java's `bitCount` function that returns the number of one-bits
    in the two's complement binary representation of the specified integer value.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We have been able to successfully build Simhash and perform some simple pairwise
    comparison. The next step is to scale this up and start detecting actual duplicates
    out of the GDELT database.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting near duplicates in GDELT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We covered the data acquisition process in depth in [Chapter 2](ch02.xhtml "Chapter 2. Data
    Acquisition"),* Data Acquisition*. For this use case, we will use a NiFi flow
    in *Figure 6* that listens to the GDELT master URL, fetches and unzips the latest
    GKG archive, and stores this file on HDFS in a compressed format.
  prefs: []
  type: TYPE_NORMAL
- en: '![Detecting near duplicates in GDELT](img/image_10_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Downloading GKG data'
  prefs: []
  type: TYPE_NORMAL
- en: We first parse our GKG records using the set of parsers we created earlier (available
    in our GitHub repo), extract all the distinct URLs and fetch the HTML content
    using the Goose extractor introduced in [Chapter 6](ch06.xhtml "Chapter 6. Scraping
    Link-Based External Data") *,Scraping Link-Based External Data*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Because the `hashcode` function is case sensitive *(Spark* and *spark* result
    in total different hash values), it is strongly recommended to clean our text
    prior to a `simhash` function. Similar to what was described in [Chapter 9](ch09.xhtml
    "Chapter 9.  News Dictionary and Real-Time Tagging System") *, News Dictionary
    and Real-Time Tagging System*, we first use the following Lucene analyzer to stem
    words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: As you may have noticed earlier, we wrote our Simhash algorithm inside of an
    implicit class; we can apply our `simhash` function directly on a string using
    the following import statement. A bit of extra effort taken at an early stage
    of development always pays off.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We now have an RDD of content (`Content` being a case class wrapping the article
    URL, title and body) together with its Simhash value and a unique identifier we
    may be using later. Let's first try to validate our algorithm and find our first
    duplicates. From now on, we only consider as duplicates the articles that have
    no more than 2-bits difference in their 32-bit Simhash values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'But here comes a scalability challenge: we certainly do not want to execute
    a Cartesian product to compare pairwise articles from our Simhash RDD. Instead,
    we want to leverage the MapReduce paradigm (using a `groupByKey` function) and
    only group articles that are duplicates. Our approach is following an *expand-and-conquer* pattern
    where we first expand our initial dataset, leverage the Spark shuffle and then
    solve our problem locally at the executor level. As we only need to deal with
    1-bit difference (we will then apply the same logic for 2-bits), our strategy
    is to expand our RDD so that for each Simhash `s`, we output all the 31 other
    1-bit combinations of **s** using the same 1-bit mask.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Taking a Simhash value `s`, we output the possible 1-bit combinations of **s**
    using a XOR between each preceding mask and the Simhash value `s`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Dealing with 2-bits is not that different, although a bit more aggressive in
    terms of scalability (we now have 496 possible combinations to output, meaning
    any combination of 2-bits out of 32).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we build our set of masks to apply (note that we also want to output
    the original Simhash by applying a 0-bit difference mask) in order to detect duplicates
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This also helps us expand our initial RDD accordingly. This surely is an expensive
    operation as it increases the size of our RDD by a constant factor (496 + 32 +
    1 possible combinations), but stays linear in terms of time complexity while Cartesian
    join is a quadratic operation - *O(n²).*
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We find that article A is a duplicate of article B, which is a duplicate of
    article C. This is a simple graph problem that can easily be solved through *GraphX*
    using a connected components algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Out of the 15,000 articles used for that test, we extracted around 3,000 different
    stories. We report an example in *Figure 7*, which includes two near-duplicate
    articles we were able to detect, both of them being highly similar but not truly
    identical.
  prefs: []
  type: TYPE_NORMAL
- en: '![Detecting near duplicates in GDELT](img/B05261_10_07_2.jpg)![Detecting near
    duplicates in GDELT](img/B05261_10_07_1-1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Galaxy Note 7 fiasco from the GDELT database'
  prefs: []
  type: TYPE_NORMAL
- en: Indexing the GDELT database
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next step is to start building our online API so that any user can detect
    near-duplicate events in real time just like Facebook does on a user's timeline.
    We use the *Play Framework* here, but we will keep the description short as this
    has already been covered in [Chapter 8](ch08.xhtml "Chapter 8. Building a Recommendation
    System"), *Building a Recommendation System*.
  prefs: []
  type: TYPE_NORMAL
- en: Persisting our RDDs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Firstly, we need to extract data out of our RDD and persist it somewhere that
    is reliable, scalable, and highly efficient for search by key. As the main purpose
    of that database is to retrieve an article given a specific key (key being the
    Simhash), **Cassandra** (maven dependency as follows) sounds like a good fit for
    that job.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Our data model is fairly simple and consists of one simple table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The easiest way to store our RDD into Cassandra is to wrap our result in a
    case class object that matches our earlier table definition and calls the `saveToCassandra`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Building a REST API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The next step is to work on the API itself. We create a new maven module (packaged
    as `play2`) and import the following dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We first create a new **data access layer**, that, given an input Simhash,
    builds the list of all the possible 1-bit and 2-bit masks discussed earlier and
    pulls all the matching records from Cassandra:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: In our **controller**, given an input URL, we extract the HTML content, tokenize
    the text, build a Simhash value, and call our service layer to finally return
    our matching records in a JSON format.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The following `play2` route will redirect any GET request to the `detect` method
    we saw earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, our API can be started and exposed to the end users as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Congratulations! You have now built an online API that can be used to detect
    near-duplicates such as the ones around Galaxy Note 7 fiasco; but how accurate
    our API is compared to the one from Facebook? This surely is accurate enough to
    start de-noising GDELT data by grouping highly similar events into stories.
  prefs: []
  type: TYPE_NORMAL
- en: Area of improvement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although we are already satisfied with the overall quality of the results returned
    by our API, here we discuss a major improvement in the context of news articles.
    In fact, articles are not only made of different bag of words, but follow a clear
    structure where the order truly matters. In fact, the title is always a punch
    line, and the main content is well covered within the first few lines only. The
    rest of the article does matter, but may not be as important as the introduction.
    Given that assumption, we can slightly modify our Simhash algorithm to take the
    order into account by attributing a different weight to each word.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Instead of adding 1 or -1 any time the same bit value is set or not, we add
    the corresponding weight of that word according to its position in the article.
    Similar articles will have to share same words, same word frequency, but also
    a similar structure. In other words, we are much less permissive in any difference
    occurring in the first few lines of text than we are at the really bottom line
    of each article.
  prefs: []
  type: TYPE_NORMAL
- en: Building stories
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Simhash* should be used to detect near-duplicate articles only. Extending
    our search to a 3-bit or 4-bit difference becomes terribly inefficient (3-bit
    difference requires 5,488 distinct queries to Cassandra while 41,448 queries will
    be needed to detect up to 4-bit differences) and seems to bring much more noise
    than related articles. Should the user want to build larger stories, a typical
    clustering technique must be applied then.'
  prefs: []
  type: TYPE_NORMAL
- en: Building term frequency vectors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will start grouping events into stories using a KMeans algorithm, taking
    the articles' word frequencies as input vectors. TF-IDF is simple, efficient,
    and a proven technique to build vectors out of text content. The basic idea is
    to compute a word frequency that we normalize using the inverse document frequency
    across the dataset, hence decreasing the weight on common words (such as stop
    words) while increasing the weight of words specific to the definition of a document.
    Its implementation is part of the basics of MapReduce processing, the *Wordcount*
    algorithm. We first compute our RDD of term frequency for each word in each document.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The IDF is the logarithmic value of the total number of documents divided by
    the number of documents containing the letter *w*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building term frequency vectors](img/image_10_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: As our output vectors are made of words, we need to assign a sequence ID to
    each word in our corpus. We may have two solutions here. Either we build our dictionary
    and assign an ID for each word, or group different words in same buckets using
    a hash function. The former is ideal but results in vectors about a million features
    long (as many features as we do have unique words) while the latter is much smaller
    (as many features as the user specifies) but may lead to undesired effects due
    to hash collisions (the least features the more collisions).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Although we describe the TF-IDF technique in detail, this hashing TF can be
    done within only a couple of lines thanks to the MLlib utilities, which we'll
    see next. We built our RDD of 256 large vectors that can (technically) be fed
    in a KMeans clustering, but, due to the hashing properties we just explained earlier,
    we would be subject to dramatic hash collisions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The curse of dimensionality, the data science plague
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Increasing our feature size from 256 to say 2^(20) will strongly limit the number
    of collisions, but will come at a price, our data points are now embedded on a
    highly dimensional space.
  prefs: []
  type: TYPE_NORMAL
- en: Here we describe a clever approach to overcome the *curse of dimensionality*
    ([http://www.stat.ucla.edu/~sabatti/statarray/textr/node5.html](http://www.stat.ucla.edu/~sabatti/statarray/textr/node5.html))
    without having to deep dive into fuzzy mathematical theories around matrix calculation
    (such as singular value decomposition) and without the need of compute-intensive
    operation. This approach is called *Random Indexing* and is similar to the *Simhash*
    concept described earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: More information about Random Indexing can be found at [http://eprints.sics.se/221/1/RI_intro.pdf](http://eprints.sics.se/221/1/RI_intro.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea is to generate a sparse, randomly generated and unique representation
    of each distinct feature (a word here), composed of +1s, -1s, and mainly 0s. Then,
    each time we come across a word in a context (a document), we add this word''s
    signature to a context vector. A document vector is then the sum of each of its
    words'' vectors as per the following *Figure 8* (or the sum of each of its TF-IDF
    vectors, in our case):'
  prefs: []
  type: TYPE_NORMAL
- en: '![The curse of dimensionality, the data science plague](img/B05261_10_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Building a Random Indexing vector'
  prefs: []
  type: TYPE_NORMAL
- en: We invite our purist math geek readers to dive into the *Johnson-Lindenstrauss* ([http://ttic.uchicago.edu/~gregory/courses/LargeScaleLearning/lectures/jl.pdf](http://ttic.uchicago.edu/~gregory/courses/LargeScaleLearning/lectures/jl.pdf))
    lemma that basically states that *"if we project points in a vector space into
    a randomly selected subspace of sufficiently high dimensionality, the distances
    between the points are approximately preserved"*. Although the *Random Indexing*
    technique itself can be implemented (with its fair amount of effort), the *Johnson-Lindenstrauss*
    lemma is quite useful but by far more difficult to grasp. Luckily, an implementation
    is part of the excellent spark-package *generalized-kmeans-clustering* ([https://github.com/derrickburns/generalized-kmeans-clustering](https://github.com/derrickburns/generalized-kmeans-clustering))
    from *Derrick Burns*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: We were finally able to project our 2^(20) large vectors in *only* 256 dimensions.
    This technique offers huge benefits to say the least.
  prefs: []
  type: TYPE_NORMAL
- en: We have a fixed number of features. Our vectors will never grow in size should
    we encounter a new word in the future that was not part of our initial dictionary.
    This will be particularly useful in a streaming context.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our input feature set is extremely large (2^(20)). Although the collisions will
    still occur, the risk is mitigated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The distances are preserved thanks to the *Johnson-Lindenstrauss* lemma.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our output vectors are relatively small (256). We overcame the curse of dimensionality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we cached our vector RDD into memory, we can now look at the KMeans clustering
    itself.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing KMeans
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We assume our readers are familiar with KMeans clustering already as this algorithm
    is probably the most famous and widely used algorithm for unsupervised clustering.
    Any attempt at doing yet another explanation here will not be as good as the many
    resources you will be able to find out there after more than half a century of
    active research.
  prefs: []
  type: TYPE_NORMAL
- en: 'We previously created our vectors based on the articles'' contents (TF-IDF).
    The next step is to start grouping articles into stories based on their similarity.
    In Spark implementation of KMeans, only the *Euclidean distance* measure is supported.
    One would argue the *Cosine distance* would be more suitable for text analysis,
    but we assume that the former is accurate enough as we do not want to repackage
    the MLlib distribution for that exercise.For more explanation regarding the use
    of cosine distance for text analysis, please refer to [http://www.cse.msu.edu/~pramanik/research/papers/2003Papers/sac04.pdf](http://www.cse.msu.edu/~pramanik/research/papers/2003Papers/sac04.pdf).
    We report in the following code both the Euclidean and Cosine functions that can
    be applied on any array of double (the logical data structure behind dense vectors):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Training a new KMeans clustering is fairly simple using the MLlib package. We
    specify a threshold of 0.01 after which we consider our cluster centers to converge
    and set the maximum iterations to 1,000.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: But what is the right number of clusters in our particular use case? With between
    500 to 1,000 different articles per 15mn batch, how many stories can we build?
    The right question is, *How many true events do we think happened over a 15mn
    batch window?* In fact, optimizing KMeans for news articles is not different from
    any other use case; this is done by optimizing its associated cost, cost being
    the **sum of the squared distances** (**SSE**) from the points to their respective
    centroids.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: With *k* equal to the number of articles, the associated cost will be 0 (each
    article is the center of its own cluster). Similarly, with *k* equal to 1, the
    cost will be maximum. The best value of *k* is therefore the minimum possible
    value after which adding a new cluster would not bring any gain in the associated
    cost, usually represented as an elbow in the SSE curve shown in the next figure.
  prefs: []
  type: TYPE_NORMAL
- en: Using all the 15,000 articles we collected so far, the optimal number of clusters
    is not obvious here but would probably be around 300.
  prefs: []
  type: TYPE_NORMAL
- en: '![Optimizing KMeans](img/image_10_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Elbow method using the cost function'
  prefs: []
  type: TYPE_NORMAL
- en: A rule of thumb is to use *k* as a function of *n* (number of articles). With
    over 15,000 articles, following this rule would return *k* ![Optimizing KMeans](img/image_10_011.jpg)
    100.
  prefs: []
  type: TYPE_NORMAL
- en: '![Optimizing KMeans](img/image_10_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We use a value of 100 and start predicting our clusters for each of our data
    points.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Although this could be greatly improved, we confirm many articles that look
    similar are grouped within same stories. We report some Samsung-related articles
    belonging to the same cluster here:'
  prefs: []
  type: TYPE_NORMAL
- en: '*What Samsung can learn from Tylenol, Mattel, and JetBlue...*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Huawei Mate 9 Appears To Be A Samsung Galaxy Note 7 Clone...*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*In light of the Note 7 debacle, Samsung may be poised to...*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Samsung''s spiralling stock draws investors betting...*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Note 7 fiasco leaves Samsung''s smartphone brand...*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Samsung''s smartphone brand takes beating from Note 7 fiasco...*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Note 7 fiasco leaves Samsung''s smartphone brand in question...*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Note 7 fiasco leaves Samsung''s smartphone brand in question...*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Samsung''s smartphone brand takes beating from Note 7 fiasco...*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Fiasco leaves Samsung''s smartphone brand in question...*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Surely these similar articles were not eligible for a Simhash lookup as they
    differ by more than 1-bits or 2-bits. A clustering technique can be used to group
    similar (but not duplicate) articles into broader stories. It is worth mentioning
    that optimizing KMeans is a tedious task that requires many iterations and thorough
    analysis. This, however, is not part of the scope here as we will be focusing
    on much larger clusters and much smaller datasets in real time.
  prefs: []
  type: TYPE_NORMAL
- en: Story mutation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We now have enough material to enter the heart of the subject. We were able
    to detect near-duplicate events and group similar articles within a story. In
    this section, we will be working in real time (on a Spark Streaming context),
    listening for news articles, grouping them into stories, but also looking at how
    these stories may change over time. We appreciate that the number of stories is
    undefined as we do not know in advance what events may arise in the coming days.
    As optimizing KMeans for each batch interval (15 mn in GDELT) would not be ideal,
    neither would it be efficient, we decided to take this constraint not as a limiting
    factor but really as an advantage in the detection of breaking news articles.
  prefs: []
  type: TYPE_NORMAL
- en: The Equilibrium state
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If we were to divide the world''s news articles into say 10 or 15 clusters,
    and fix that number to never change over time, then training a KMeans clustering
    should probably group similar (but not necessarily duplicate) articles into generic
    stories. For convenience, we give the following definitions:'
  prefs: []
  type: TYPE_NORMAL
- en: An **article** is the news article covering a particular event at a time T
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **story** is a group of similar articles covering an event at a time T
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **topic** is a group of similar stories covering different events over a period
    P
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An **epic** is a group of similar stories covering the same event over a period
    P
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We assume that after some time without any major news events, any story will
    be grouped into distinct *topics* (each covering one or several themes). As an
    example, any article about politics - regardless the nature of the political event
    - may be grouped into the politics bucket. This is what we call the *Equilibrium
    state*, where the world is equally divided into 15 distinct and clear categories
    (war, politics, finance, technology, education, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: But what happens if a major event just breaks through? An event may become so
    important that, over time, (and because of the fixed number of clusters) it could
    shadow the least important *topic* and become part of its own *topic*. Similar
    to the BBC broadcast that is limited to a 30 mn window, some minor events like
    the *Oyster festival in Whitstable* may be skipped in favor of a major international
    event (to the very dismay of oysters' fans). This topic is not generic anymore
    but is now associated to a particular event. We called this topic an *epic*. As
    an example, the generic *topic* [terrorism, war, and violence] became an epic
    [**Paris Attacks**] in November last year when a major terrorist attack broke
    through; what was deemed to be a broad discussion about violence and terrorism
    in general became a branch dedicated to the articles covering the events held
    in Paris.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now imagine an *epic* keeps growing in size; while the first articles about
    **Paris Attacks** were covering facts, few hours later, the entire world was paying
    tribute and condemning terrorism. At the same time, investigations were led by
    both French and Belgium police to track and dismantle the terrorism network. Both
    of these stories were massively covered, hence became two different versions of
    the same *epic*. This concept of branching is reported in the following *Figure
    10*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Equilibrium state](img/image_10_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Concept of a story mutation branching'
  prefs: []
  type: TYPE_NORMAL
- en: Surely some epics will last longer than others, but when they vanish - if they
    do - their branches may be recycled to cover new breaking articles (remember the
    fixed number of clusters) or be re-used to group generic stories back to their
    generic topics. At some point in time, we eventually reach a new Equilibrium state
    where the world nicely fits again within 15 different topics. We assume, though,
    that a new Equilibrium may not be a perfect clone of the previous one as this
    disturbance may have carved and re-shaped the world somehow. As a concrete example,
    we still mention 9/11-related articles nowadays; world trade center attacks that
    happened in NYC in 2001 are still contributing to the definition of [violence,
    war, and terrorism] *topic*.
  prefs: []
  type: TYPE_NORMAL
- en: Tracking stories over time
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although the preceding description is more conceptual than anything, and would
    probably deserve a subject for a PhD in data science applied to geo-politics,
    we would like to dig that idea further and see how Streaming KMeans could be a
    fantastic tool for that use case.
  prefs: []
  type: TYPE_NORMAL
- en: Building a streaming application
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first thing is to acquire our data in real time, hence modifying our existing
    NiFi flow to fork our downloaded archive to a Spark Streaming context. One could
    simply **netcat** the content of a file to an open socket, but we want this process
    to be resilient and fault tolerant. NiFi comes, by default, with the concept of
    output ports that provide a mechanism to transfer data to remote instances using
    *Site-To-Site*. In that case, the port works like a queue, and no data should
    be lost in transit, hopefully. We enable this functionality in the `nifi.properties`
    file by allocating a port number.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: We create a port called [`Send_To_Spark`] on our canvas, and every record (hence
    the `SplitText` processor) will be sent to it just like we would be doing on a
    Kafka topic.
  prefs: []
  type: TYPE_NORMAL
- en: '![Building a streaming application](img/image_10_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Sending GKG records to Spark Streaming'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although we are designing a streaming application, it is recommended to always
    keep an immutable copy of your data in a resilient data store (HDFS here). In
    our NiFi flow earlier, we did not modify our existing process, but forked it to
    also send records to our Spark Streaming. This will be particularly useful when/if
    we need to replay part of our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the Spark side, we need to build a Nifi receiver. This can be achieved using
    the following maven dependency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: We define the NiFi endpoint together with the port name [`Send_To_Spark`] we
    assigned earlier. Our stream of data will be received as packet stream that can
    easily be converted into a String using the `getContent` method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: We start our streaming context and listen to new GDELT data coming every 15
    mn.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to download the HTML content for each article. The tricky
    part here is to download articles for distinct URLs only. As there is no built-in
    `distinct` operation on `DStream`, we need to access the underlying RDDs using
    a `transform` operation on top of which we pass an `extractUrlsFromRDD` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Similarly, building vectors requires access to the underlying RDDs as we need
    to count the document frequency (used for TF-IDF) across the entire batch. This
    is also done within the `transform` function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Streaming KMeans
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our use case perfectly fits in a **Streaming KMeans** algorithm. The concept
    of Streaming KMeans does not differ from the classic KMeans except that it applies
    on dynamic data and therefore needs to be constantly re-trained and updated.
  prefs: []
  type: TYPE_NORMAL
- en: At each batch, we find the closest center for each new data point, average the
    new cluster centers and update our model. As we track the true clusters and adapt
    to the changes in pseudo real-time, it will be particularly easy to track the
    same topics across different batches.
  prefs: []
  type: TYPE_NORMAL
- en: The second important feature of a Streaming KMeans is the forgetfulness. This
    ensures new data points received at time t will be contributing more to the definition
    of our clusters than any other point in the past history, hence allowing our cluster
    centers to smoothly drift over time (stories will mutate). This is controlled
    by the decay factor and its half-life parameter (expressed in the number of batches
    or number of points) that specifies the time after which a given point will only
    contribute half of its original weight.
  prefs: []
  type: TYPE_NORMAL
- en: With an infinite decay factor, all the history will be taken into account, our
    cluster centers will be drifting slowly and will not be reactive if a major news
    event just breaks through
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With a small decay factor, our clusters will be too reactive towards any point
    and may drastically change any time a new event is observed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third and most important feature of a Streaming KMeans is the ability to
    detect and recycle dying clusters. When we observe a drastic change in our input
    data, one cluster may become far from any known data point. Streaming KMeans will
    eliminate this dying cluster and split the largest one in two. This is totally
    in-line with our concept of story branching, where multiple stories may share
    a common ancestor.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use a half-life parameter of two batches here. As we get new data every
    15 mn, any new data point will stay *active* for 1 hour only. The process for
    training Streaming KMeans is reported in *Figure 12*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Streaming KMeans](img/image_10_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Training a Streaming KMeans'
  prefs: []
  type: TYPE_NORMAL
- en: 'We create a new Streaming KMeans as follows. Because we did not observe any
    data point yet, we initialize it with 15 random centers of 256 large vectors (size
    of our TF-IDF vectors) and train it in real time using the `trainOn` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we predict our clusters for any new data point:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: We then save our results to our Elasticsearch cluster using the following attributes
    (accessed through a series of join operations). We do not report here how to persist
    RDD to Elasticsearch as we believe this has been covered in depth in the previous
    chapters already. Note that we also save the vector itself as we may re-use it
    later in the process.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Visualization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we stored our articles with their respective stories and *topics* on Elasticsearch,
    we can browse any events using a keyword search (as the articles are fully analyzed
    and indexed) or for a particular person, theme, organization, and so on. We build
    visualizations on top of our stories and try to detect their potential drifts
    on a Kibana dashboard. The different cluster IDs (our different *topics*) over
    time are reported in the following *Figure 13* for the 13^(th) of November (35,000
    articles indexed):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualization](img/image_10_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Kibana visualization of the Paris attacks'
  prefs: []
  type: TYPE_NORMAL
- en: The results are quite promising. We were able to detect the **Paris Attacks**
    at around 9:30 p.m. on November 13th, only a few minutes after the first attacks
    started. We also confirm a relative good consistency of our clustering algorithm
    as a particular cluster was made of events related to the **Paris Attacks** only
    (5,000 articles) from 9:30 p.m. to 3:00 a.m.
  prefs: []
  type: TYPE_NORMAL
- en: But we may wonder what this particular cluster was about before the first attack
    took place. Since we indexed all the articles together with their cluster ID and
    their GKG attributes, we can easily track a story backwards in time and detect
    its mutation. It turns out this particular *topic* was mainly covering events
    related to [MAN_MADE_DISASTER] theme (among others) until 9 p.m. to 10 p.m. when
    it turned into the **Paris Attacks** *epic* with themes around [TERROR], [STATE_OF_EMERGENCY],
    [TAX_ETHNICITY_FRENCH], [KILL], and [EVACUATION].
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualization](img/image_10_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: Kibana streamgraph of the Paris attacks cluster'
  prefs: []
  type: TYPE_NORMAL
- en: 'Needless to say the 15 mn average tone we get from GDELT dropped drastically
    after 9 p.m. for that particular *topic*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualization](img/image_10_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: Kibana average tone-Paris attacks cluster'
  prefs: []
  type: TYPE_NORMAL
- en: Using these three simple visualizations, we prove that we can track a story
    over time and study its potential mutation in genre, keywords, persons, or organizations
    (basically any entity we can extract from GDELT). But we could also look at the
    geolocation from the GKG records; with enough articles, we could possibly track
    the terrorist hunt held between Paris and Brussels on a map, in pseudo real time!
  prefs: []
  type: TYPE_NORMAL
- en: 'Although we found one main cluster that was specific to the Paris Attacks,
    and that this particular cluster was the first one to cover this series of events,
    this may not be the only one. According to the Streaming KMeans definition earlier,
    this *topic* became so big that it surely had triggered one or several subsequent
    *epics*. We report in the following *Figure 16* the same results as per *Figure
    13*, but this time filtered out for any article matching the keyword *Paris*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualization](img/image_10_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: Kibana Paris Attacks multiple epics'
  prefs: []
  type: TYPE_NORMAL
- en: It seems that around midnight, this *epic* gave rise to multiple versions of
    the same event (at least three major ones). After an hour following the attacks
    (1 hour is our decay factor), Streaming KMeans started to recycle dying clusters,
    hence creating new branches out of the most important event (our *Paris attack* cluster).
  prefs: []
  type: TYPE_NORMAL
- en: While the main *epic* was still covering the event itself (the facts), the second
    most important one was more about social network related articles. A simple word
    frequency analysis tells us this *epic* was about the **#portesOuvertes** (open
    doors) and **#prayForParis** hashtags where Parisians responded to terror with
    solidarity. We also detected another cluster focusing more on all the politicians
    paying tributes to France and condemning terrorism. All these new stories share
    the *Paris attack* *epic* as a common ancestor, but cover a different flavor of
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Building story connections
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How can we link these branches together? How can we track an *epic* over time
    and see when, if, how, or why it may split? Surely visualization helps, but we
    are looking at a graph problem to solve here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because our KMeans model keeps getting updated at each batch, our approach
    is to retrieve the articles that we predicted using an outdated version of our
    model, pull them from Elasticsearch, and predict them against our updated KMeans.
    Our assumption is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*If we observe many articles at a time* **t** *that belonged to a story* **s**,
    *and now belong to a story* **s''** *at a time* ![Building story connections](img/B05261_10_22.jpg),
    *then* **s** *most likely migrated to* **s''** *in* ![Building story connections](img/B05261_10_23.jpg)*time.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a concrete example, the first **#prayForParis** article was surely belonging
    to the *Paris Attacks* *epic*. Few batches later, that same article belonged to
    the *Paris Attacks/Social Network* cluster. Therefore, the *Paris Attack* *epic*
    may have spawn the *Paris Attacks/Social Network* *epic*. This process is reported
    in the following *Figure 17*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building story connections](img/image_10_020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: Detecting story connections'
  prefs: []
  type: TYPE_NORMAL
- en: 'We read a JSON RDD from Elasticsearch and applied a range query using the batch
    ID. In the following example, we want to access all the vectors built over the
    past hour (four last batches) together with their original cluster ID and re-predict
    them against our updated model (accessed through the `latestModel` function):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Finally, a simple `reduceByKey` function will count the number of different
    edges over the past hour. In most of the cases, articles in story *s* will stay
    in story *s*, but in case of the Paris attacks, we may observe some stories to
    drift over time towards different *epics*. Most importantly, the more connections
    two branches have in common, the more similar they are (as many of their articles
    are interconnected) and therefore the closest they seem to be in a force directed
    layout. Similarly, branches that are not sharing many connections will seem to
    be far from another in the same graph visualization. A force atlas representation
    of our story connections is done using Gephi software and reported in the following *Figure
    18*. Each node is a story at a batch *b*, and each edge is the number of connections
    we found between two stories. The 15 lines are our 15 *topics* that all share
    a common ancestor (the initial cluster spawn when first starting the streaming
    context).
  prefs: []
  type: TYPE_NORMAL
- en: '![Building story connections](img/B05261_10_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18: Force-directed layout of story mutation'
  prefs: []
  type: TYPE_NORMAL
- en: The first observation we can make is this line shape. This observation surprisingly
    confirms our theory of an Equilibrium state where the world nicely fitted in 15
    distinct *topics* until the Paris attacks happened. Before the event, most of
    the *topics* were isolated and intraconnected (hence this line shape). After the
    event, we see our main *Paris Attack* *epic* to be dense, interconnected, and
    drifting over time. It also seems to drag few branches down with it due to the
    growing number on interconnections. These two similar branches are the two other
    clusters mentioned earlier (social network and tributes). This *epic* is being
    more and more specific over time, it naturally becomes more different from others,
    hence pushing all these different stories upwards and resulting in this scatter
    shape.
  prefs: []
  type: TYPE_NORMAL
- en: We also want to know what these different branches were about, and whether or
    not we could explain why a story may have split into two. For that purpose, we
    find the main article of each story as being the closest point to its centroid.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'In *Figure 19*, we report the same graph enriched with the story titles. Although
    it is difficult to find a clear pattern, we found an interesting case. A *topic*
    was covering (among others) an event related to *Prince Harry* joking around about
    his hair style, that slightly migrated to *Obama* offering a statement on the
    attack in Paris, and finally turned into the Paris attack and the tributes paid
    by politicians. This branch did not come out of nowhere but seemed to follow a
    logical flow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[ROYAL, PRINCE HARRY, JOKES]'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[ROYAL, PRINCE HARRY]'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRINCE HARRY, OBAMA]'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRINCE HARRY, OBAMA, POLITICS]'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[OBAMA, POLITICS]'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[OBAMA, POLITICS, PARIS]'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[POLITICS, PARIS]'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Building story connections](img/B05261_10_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19: Force-directed layout of story mutation - title'
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, it seems that a breaking news event acts as a sudden perturbation
    of an Equilibrium state. Now we may wonder how long would that disturbance last,
    whether or not a new Equilibrium will be reached in the future and what would
    be the shape of the world resulting from that *wound*. Most importantly, what
    effect a different decay factor would have on that world shape.
  prefs: []
  type: TYPE_NORMAL
- en: With enough time and motivation, we would potentially be interested in applying
    some concepts of physics around *perturbation theory* ([http://www.tcm.phy.cam.ac.uk/~bds10/aqp/handout_dep.pdf](http://www.tcm.phy.cam.ac.uk/~bds10/aqp/handout_dep.pdf)).
    I personally would be interested in finding harmonics around this Equilibrium.
    The reason of the Paris attacks events being so memorable is because of its violent
    nature for sure, but also because it happened only a few months after the *Charlie
    Hebdo* attacks in Paris.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter was really complex and the story mutation problem could not be
    easily solved in the time frame allowed for delivering this chapter. However,
    what we discovered is truly amazing as it opens up a lot of questions. We did
    not want to draw any conclusion though, so we stopped our process right after
    the observation of the Paris attack disturbance and left that discussion open
    for our readers. Feel free to download our code base and study any breaking news
    and their potential impacts in what we define as an Equilibrium state. We are
    very much looking forward to hearing back from you and learning about your findings
    and different interpretations.
  prefs: []
  type: TYPE_NORMAL
- en: Surprisingly, we did not know anything about the *Galaxy Note 7 fiasco* before
    writing this chapter, and without the API created in the first section, the related
    articles would surely have been indistinguishable from the mass. De-duplicating
    content using  **Simhash** really helped us get a better overview of the world
    news events.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will try to detect abnormal tweets related to the US
    elections and the new president elect (*Donald Trump*). We will cover both *Word2Vec*
    algorithm and Stanford NLP for sentiment analysis.
  prefs: []
  type: TYPE_NORMAL
