- en: InterPlanetary - A Brave New File System
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are going to learn about the **InterPlanetary File System**
    (**IPFS**). The IPFS is not actually part of the blockchain technology; instead,
    it complements it. IPFS with blockchain is a match made in heaven. As you have
    learned in previous chapters, storage in a blockchain is expensive. Usually, people
    save links to files in a blockchain and save the actual files in normal storage,
    such as cloud storage. But this strategy suffers the fate of centralization. IPFS
    offers blockchain developers a way to avoid this.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you are going to learn about the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The motivation behind IPFS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Merkle DAG
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peer-to-peer networking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The motivation behind IPFS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: IPFS is not a normal filesystem, such as `fat32`, `ntfs`, or `ext3`. It is more
    similar to Dropbox. It is a cross-device filesystem. You can save a file in this
    filesystem and people around the world can access it as easily as if the file
    were on their own computer. If Ethereum can be thought of as the world's singleton
    operating system, IPFS can be considered as the world's singleton storage!
  prefs: []
  type: TYPE_NORMAL
- en: 'The slogan of the IPFS website is *IPFS is the Distributed Web*. IPFS tries
    to replace, or at least supplement, HTTP. The HTTP protocol has served us for
    a long time, over 20 years, but it is not considered sufficient for upcoming challenges,
    such as increasing bandwidth demands or redundancy of files. HTTP uses a client-server
    model. You can only choose one of these two roles: either to be a server or a
    client.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a couple of problems with this architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: The first problem is that to pick up the server role, we have to have sufficient
    resources. If not, if the server is flooded with a lot of requests, it could go
    down rapidly. The resources required to handle one million requests per minute
    is out of reach for many common people.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second problem is that the server-and-client architecture is not efficient
    in some situations. Imagine that you are sat beside a grandma in a park and both
    of you are watching the same video of a cute panda from the same URL (something
    like [https://example.com/cute_panda.mp4](https://example.com/cute_panda.mp4)).
    Let's say that the size of this video is 20 MB. This means the server must send
    a 20 MB file twice to two different locations, even though these two different
    locations are located closely together with a proximity of one meter. In other
    words, the server uses 40 MB of bandwidth. Imagine, however, if you could pull
    the file not from the server, but from the grandma who sits beside you (in this
    case, let's assume grandma has watched the cute panda video two minutes before
    you). Wouldn't this be more efficient?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Juan Benet was inspired to build IPFS in late 2013\. Back then, he was working
    with knowledge tools, a term that refers to software that can be used to efficiently
    gather knowledge from papers. Let's say, for example, that a scientist reads a
    lot of papers. It would be better if that scientist could get this knowledge faster. Benet
    came across the problem that datasets required too much effort to distribute.
    There was no easy way to handle the versioning of datasets. He looked at various
    tools, such as Git and BitTorrent, and wondered if they could be combined to solve
    this problem. As a result, IPFS was born. BitTorrent inspired IPFS with regard
    to distributing files and finding files among the nodes. Git inspired IPFS with
    regard to keeping the integrity of files and converting saved files into storage.
  prefs: []
  type: TYPE_NORMAL
- en: IPFS is a peer-to-peer hypermedia protocol that makes the web faster, safer,
    and more open. The goal of IPFS is pragmatic and idealistic. Besides saving bandwidth,
    another of its aims is to increase the longevity of a file. Keeping a file in
    a server for a very long time (such as a decade) requires a huge amount of resources.
    The reason why we might want a file to stay alive is usually because it has some
    kind of economic benefit for the owner of the server; for example, it could be
    monetized with ads if it is a blog post. If not, there is a possibility that the
    file will be destroyed by the owner of the storage server. This happened when
    Geocities was shut down.
  prefs: []
  type: TYPE_NORMAL
- en: Geocities was a website that allowed people to create their own personal website.
    It was similar to [wordpress.com](http://wordpress.com) and [medium.com](http://medium.com).
    Some owners of servers would keep files alive even without ads, like Wikipedia,
    which works thanks to donations. Other than that, however, the files are not so
    lucky.
  prefs: []
  type: TYPE_NORMAL
- en: The other goals of IPFS are more idealistic and involved democratizing how we
    provide content. Right now, content is heavily centralized. We usually go to just
    a few websites, such as Facebook, Instagram, Reddit, Medium, Netflix, Amazon,
    Google, Wikipedia, and so on. This oligopoly of information hinders innovation
    on the internet because information is controlled literally by a few companies.
    Apart from Wikipedia, most, if not all, companies are beholden to rich shareholders.
    This situation is in stark contrast to 10 years ago, when the internet was considered
    a great equalizer of wealth and information, similar to printing press technology.
  prefs: []
  type: TYPE_NORMAL
- en: The other disadvantage of this heavy centralization is that the information
    that's provided is susceptible to censorship. For example, Google is a company
    based in Mountain View, California, and is therefore subject to US law. Most people
    who have the power to make decisions (senior executives and C-levels) are American
    and therefore have an American bias in their perception of the world. Things that
    are fine in most countries in Europe could be censored in the name of American
    morals. This could include content that is disliked by the state because it is
    considered blasphemous or dangerous. The founder of the IPFS project likened this
    situation to the case of burning books that were considered dangerous by the state
    or powerful institutions. One of the goals of the IPFS project was to increase
    the resistance of documents to censorship. IPFS makes it easier for people to
    mirror and serve dangerous documents. We'll discuss how IPFS achieves this goal
    in a later section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The final goal of IPFS, which is more pragmatic, concerns our fragile internet
    infrastructure, which is composed of computer networks and core routers connected
    by fiber-optic cables. If the connecting cable is damaged accidentally or deliberately,
    a block or area could go offline. In 2011, a woman with a shovel damaged the cable
    that brought internet to Armenia when she was digging looking for metal to sell.
    The IPFS project does not solve this problem completely, but it can mitigate the
    damage to some extent.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the incident about the woman and her shovel here: [https://web.archive.org/web/20141225063937/http://www.wsj.com/articles/SB10001424052748704630004576249013084603344.](https://web.archive.org/web/20141225063937/http://www.wsj.com/articles/SB10001424052748704630004576249013084603344)
  prefs: []
  type: TYPE_NORMAL
- en: Merkle DAG
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have learned about the internals of Git, Merkle **Directed Acyclic Graph** (**DAG**)
    shouldn't be too foreign. As a version control system software, Git is required
    to keep many versions of a file and distribute them easily to other people. It
    also needs to be able to check the integrity of the file very quickly.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two words that make up Merkle DAG: Merkle and DAG. Let''s discuss
    Merkle first. Actually, the full word of Merkle in this context is Merkle tree.
    A Merkle tree is a fast way to check whether partial data has been tampered with
    or not.'
  prefs: []
  type: TYPE_NORMAL
- en: Merkle tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s take a look at an example of a Merkle tree in order to understand it.
    Let''s say you have eight pieces of data. In this case, we will use the names
    of animals for our data, but in Bitcoin, which uses a Merkle tree, the pieces
    of data are usually transactions. Back to Merkle trees: put the data in order,
    so in this case, cat is the first piece of data, dog is the second, ant is the
    third, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/405fb6e7-eda7-4523-b59e-6bbe3c647838.png)'
  prefs: []
  type: TYPE_IMG
- en: We take the hash of each piece of data, in this case, cat, dog, ant, and so
    on. For this demonstration, we use the hash function SHA256\. Because of limited
    space, we have truncated the full hash result in the diagram. For now, we will
    order the data from left to right, so the hash of the "cat" string is `Data 1`,
    the hash of the "dog" string is `Data 2`, the hash of the "ant" string is `Data
    3`, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Here's come the interesting part. For `Data 1` and `Data 2`, we combine the
    hash and hash the result. Combining the hash means concatenating it. Do this for
    `Data 3` and `Data 4`, `Data 5` and `Data 6`, `Data 7` and `Data 8` as well.
  prefs: []
  type: TYPE_NORMAL
- en: This might remind you of a knockout competition. We are now in the semi-final
    phase. We now have `Hash 1` (from `Data 1` and `Data 2`), `Hash 2` (from `Data
    3` and `Data 4`), `Hash 3` (from `Data 5` and `Data 6`), and `Hash 4` (from `Data
    7` and `Data 8`).
  prefs: []
  type: TYPE_NORMAL
- en: We then concatenate `Hash 1` and `Hash 2`, hash the result, and name this `Hash
    5`. We then do the same thing for `Hash 3` and `Hash 4`. Name the result `Hash
    6`.
  prefs: []
  type: TYPE_NORMAL
- en: We are now in the final phase. Combine `Hash 5` and `Hash 6`, then hash the
    result. The result is the `Root Hash`. This `Root Hash` can guarantee the integrity
    of all the pieces of data (from `Data 1` to `Data 8`). If you change any of the
    data, `Root Hash` would be different.
  prefs: []
  type: TYPE_NORMAL
- en: You may be asking why we don't just concatenate all the data (from `Data 1`
    to `Data 8`) from the beginning and then hash the result. It turns out, however,
    that Merkle trees has some benefits over just concatenating all the data together
    and then hashing it (this technique is called a **hash list**, and it is used
    in some situations). One of the benefits is that it is easier and cheaper to check
    the integrity of the partial data when we use a Merkel tree.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a Merkle tree, to check the integrity of `Data 5`, you only need to download
    `Data 5`, `Data 6`, `Hash 4`, `Hash 5`, and the `Root Hash`, as shown in the following
    diagram. You don''t need to download all the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/1dc3424f-2b27-496c-9b53-600829a14d09.png)'
  prefs: []
  type: TYPE_IMG
- en: If you use a naive approach, you need to download all the hashes of the data
    (`Data 1` to `Data 8`) and the `Root Hash`. In this example, we only have eight
    pieces of data. Imagine if we had 100 and you had to download the entire dataset.
    Merkle trees makes this process more efficient because we don't need to download
    the full set of data.
  prefs: []
  type: TYPE_NORMAL
- en: If we had an odd number of nodes, such as seven, the general rule (the one that
    Bitcoin implements) is to clone the last node, so `Data 8` is a copy of `Data
    7`. You could use another rule, however; I have seen an implementation of a Merkle
    tree in which a single piece of data (`Data 7` in our example) is simply promoted
    to the top. In this case, `Hash 4` is just `Data 7`.
  prefs: []
  type: TYPE_NORMAL
- en: This is what Bitcoin does when people use Simplified Payment Verification. With
    a mobile app, downloading the full node is difficult. In order to send Bitcoin
    transactions, the user downloads only the important parts of the node instead
    of the full node. Merkle tree enables this process.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will move on to learn about DAGs.
  prefs: []
  type: TYPE_NORMAL
- en: Directive Acrylic Graphs (DAGs)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Directive Acrylic Graphs** (**DAGs**), as its name suggests, are graphs in
    which each vertex (or node) can have edges pointing to other vertexes, as shown
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/d0bcac5f-5616-4cbb-a1c8-b580ce87596d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The direction of the arrow does not matter, as long as you make it consistent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/510e335c-e929-47e0-9d13-cd718836c316.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The rule is that these edges should not make a cycle. In the following figure,
    we can see that vertexes A, C, and D make a cycle, which is against the rules
    of a DAG:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/52c1b720-a13c-45d3-bcd0-109dfaea1cfd.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, if you combine a Merkle tree and DAG, you get a Merkle DAG. This is the
    data structure that is used by Git and IPFS.
  prefs: []
  type: TYPE_NORMAL
- en: In a Merkle tree, only the leaf nodes hold data. In a Merkle DAG, however, any
    node could hold the data. In a Merkle tree, the tree has to be balanced, but there
    is no such limitation in a Merkle DAG.
  prefs: []
  type: TYPE_NORMAL
- en: Before we jump into Merkle DAGs, let's learn about content addressing, because
    Merkle DAGs are dependent on this feature.
  prefs: []
  type: TYPE_NORMAL
- en: Content addressing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a linked list, you chain together nodes (or blocks) with a pointer. A pointer
    is a data type that points to memory. For example, let's say we have two nodes,
    node A and node B. Node A is the head and node B is the tail. The structure of
    the node has two important components. The first component is the data component
    where you store the data. In Git, this data could be the content of the file.
    The second component is a link to another node. In a linked list, this is the
    pointer to a node's address.
  prefs: []
  type: TYPE_NORMAL
- en: But with content addressing, instead of just a pointer, we also add the hash
    of the target (in this case, node B). You may recognize this concept; this is
    exactly what happens in blockchain. A Merkle DAG, however, is not a linked list
    that spans linearly in one straight line. A Merkle DAG is a tree that can have
    branches.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a linked list. It is used in a blockchain''s data structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dc737437-77bb-4f46-b437-6a5f01b6993b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, consider this case. We have three nodes: nodes A1 and A2 are both heads
    that point to node B. Instead of putting the pointers on node A1 and node A2,
    we put the pointers on node B. Node B now has two pointers. Node B hashes nodes
    A1 and A2, then concatenates both hashes before hashing the result again. In this
    way, node B can keep the integrity of the content of node A1 and node A2\. If
    somebody changes the content of node A1 or the content of node A2, the hash kept
    by node B would be invalid:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/afdb0bee-6c71-4ad7-bf53-7259f5221866.png)'
  prefs: []
  type: TYPE_IMG
- en: 'IPFS is different to HTTP in terms of how it fetches a document. HTTP uses
    links, which work like pointers. For example, let''s say we have the following
    link: [https://example.com/cute_panda.png](https://example.com/cute_panda.png).
    This uses a location to fetch a document called `cute_panda.png`. Only one provider
    could serve this document, which is `example.com`. IPFS, however, does not use
    a URL link. Instead, it uses a hash link, such as `ipfs://QmYeAiiK1UfB8MGLRefok1N7vBTyX8hGPuMXZ4Xq1DPyt7`.
    When you access this hash link, the IPFS sofware will find the document that,
    when hashed, will give you the same hash output. Because hashing is a one-way
    function, IPFS must have some other information to locate the document. Basically,
    it broadcasts the request to nodes that are nearby the document that has this
    hash output. If the nearby nodes don''t have these files, they forward the requests
    to their nearby nodes. This peer-finding request is quite complex. IPFS uses S/Kademlia
    Distributed Hash Tables, which we will discuss in a later section of this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: The interesting thing is that when you use content addressing, there may be
    multiple providers that can serve this document. In the case of the `cute_panda.png`
    document, there could be more than four nodes that can serve this document. We
    can pick the nearest node to make the download process more efficient. This property
    also makes censorship much more difficult. In the case of  HTTP, an actor could
    ban the server [https://example.com](https://example.com). In the case of IPFS,
    however, anyone could launch a new node and serve the document. Right now, IPFS
    is transparent, perhaps too much so. The node that requests the document can see
    the IP address of the node serving the document, and vice versa. The actor could
    ban the IP address to forbid this document being spread. The development to make
    IPFS work with Tor, software that allows users to browse websites anonymously, however,
    is still in its early days.
  prefs: []
  type: TYPE_NORMAL
- en: If you download a document from [https://example.com/cute_panda.png](https://example.com/cute_panda.png),
    the document that you get at that moment may be different to the document that
    your friend downloaded from the same URL yesterday. It could be that the admin
    of the server changed the document before you downloaded it today.
  prefs: []
  type: TYPE_NORMAL
- en: With the content addressing system, however, the document that you get from
    the IPFS hash link, `ipfs://QmYeAiiK1UfB8MGLRefok1N7vBTyX8hGPuMXZ4Xq1DPyt7`, will
    always be the same, no matter when or where you download it. This hash link guarantees
    that nobody can tamper with the document. If you change the document and upload
    it to IPFS, the IPFS URL or hash would be different.
  prefs: []
  type: TYPE_NORMAL
- en: We can create a simple Python script to illustrate this case. Create a directory
    called `ipfs_tutorial`. Create three sample files in this directory. The first
    sample file is `hello.txt`, which has the content `I am a good boy.\n`. The second
    sample file is `hello2.txt`, which has the content `I am a good girl.\n`. The
    third sample file is `hello3.txt`, which has the content `I am a good horse.\n`.
    The fourth sample file is `hello4.txt`, which has the content `I am a good girl.\n`.
    The fact that the second and fourth files have the same content is deliberate.
    You can create different files, if you wish, but make sure that at least two of
    them have the same content.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a Python script as shown in the following code block and name it `create_hash_from_content.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This script lists all files in the same directory that have a name that starts
    with `hello`. You can modify this part if your sample files don't start with `hello`.
    The long hash is the hash of the content of `hello2.txt`.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you run the script, you will get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, there are four files, but the final output is three, not four.
    This is because there are three files with unique content, not four. This is how
    content addressing works. It does not care about the filename, it only cares about
    the content. It doesn't matter whether the file is called `hello1.txt` or `hello2.txt`
    or `hello4.txt`, it only matters that the content, `I am a good girl.\n`, is the
    same. Technically speaking, this is a **white lie**; there is a situation when
    IPFS must consider the filename and cannot ignore it. I'll explain the truth of
    this matter later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: What we have seen in the preceding example is normal hashing. There is no Markle
    DAG or even Merkle tree. Let's now create a more complicated scene with a big
    file. Hashing a big file is not efficient. Usually, we split the file into multiple
    smaller pieces of the same size. For example, a 900 KB file would turn into four
    files. The first, second, and third files would have a size of 250 KB. The fourth
    file would have a size of 150 KB. Then, we hash each smaller file and combine
    it with a Merkle tree.
  prefs: []
  type: TYPE_NORMAL
- en: For illustration purposes, we won't use a large file, but we will make some
    imaginary limitations. We don't want to hash content that spans more than one
    line. If the text file has four lines, we would split them into four smaller files.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inside your project directory, create a file called `hello_big.txt` and enter
    the following lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Before we create a script to hash this big file, let's create a very simple
    Merkle tree library and name it `merkle_tree.py`. Refer to the GitLab link for
    the complete code file: [https://gitlab.com/arjunaskykok/hands-on-blockchain-for-python-developers/tree/master/chapter_10](https://gitlab.com/arjunaskykok/hands-on-blockchain-for-python-developers/tree/master/chapter_10).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s discuss this Merkle tree library, starting from its initialization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We make sure there are at least four nodes. If not, we might as well use the
    hash list technique. The `leaf_nodes` are original data nodes. They are string
    lists, such as `['cat', 'dog', 'unicorn', 'elephant']`. The `hash_nodes` are the
    hash list of the data nodes, such as `[hash of 'cat', hash of 'dog', hash of 'unicorn',
    hash of 'elephant']` or `['77af778...', 'cd6357e...', 'c6cb50e...', 'cd08c4c...']`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the `_hash_list()` method to hash list the data if there are less than
    four nodes. We concatenate all the pieces of data before hashing them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `_turn_leaf_nodes_to_hash_nodes()` method, we fill the `hash_nodes` based
    on the `leaf_nodes`. This is one-to-one mapping:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `_hash()` method, we wrap the `sha256` hashing function. This is to
    make the customization of the class easier, since we may want to use a different
    hashing function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code block shows how we can get the root nodes from the hash
    nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we are carrying out multiple iterations on hash nodes. It jumps two steps
    on each iteration. For each iteration, it works on two nodes. It concatenates
    the hash of these two nodes, then hashes the result. The resulting hash is the
    parent of these two nodes. This parent becomes part of the hash nodes that will
    be iterated over again. This parent, along with its neighbor, will be concatenated
    again before being hashed, and so on. If there is an odd number of hash nodes,
    the last node will be concatenated with itself before being hashed. If there is
    only one parent, we return the hash of that, which is the **root hash**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `_convert_parent_from_two_nodes()` method allows us to get the parent hash
    from the two child nodes. We concatenate the two nodes and hash them. If the second
    node is `None`, meaning there is an odd number of nodes or we are processing the
    last node, we just concatenate the node with itself before hashing it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the Merkle tree library is ready, we will create a Python script to
    hash the `hello_big.txt` file and name it `hash_big_file.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'If you execute this Python script, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: If the file is big, you would not hash it directly, because this could cause
    you to run out of memory. Instead, you split the file. Here, we split the text
    file based on the new lines. If you handle the binary file, you read the file
    chunk by chunk and save that chunk into a smaller file. Of course, before feeding
    them into a Merkle tree, you need to serialize the binary data into the text data.
    Once you have done that, you can feed the pieces of data into a Merkle tree. You
    get the root hash, which will protect the integrity of the original file. If you
    alter a single bit in a piece of data, the root hash would be different.
  prefs: []
  type: TYPE_NORMAL
- en: The Merkle DAG data structure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have used content addressing to handle a file. If the file is big, we can
    split it and get the root hash with a Merkle tree. In this case, we only care
    about the content of the file; we don't even save its name.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a situation, however, where the name of the file does matter. For
    example, let''s say that you want to save a file directory that contains 100 images
    of cute pandas. The names of the files in this case don''t matter; what we care
    about is the content, the pictures of the cute pandas! If this is a directory
    of a programming project, however, the names of the files do matter. If one Python
    file tries to import another Python library that is contained in a different file,
    we have to keep the name of the file. Let''s say that we have a Python file called `main.py` that
    has the following content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The `main.py` file is dependent on another file in the same directory called `secret_algorithm.py`.
    It is not just the content of the `secret_algorithm.py` file that matters, but
    also its name. If the filename changes, `main.py` will not be able to import the
    library.
  prefs: []
  type: TYPE_NORMAL
- en: In order to save the content and the filename, we need to use a Merkle DAG data
    structure. As mentioned before, one of the differences between a Merkle DAG and
    a Merkle tree is that any node in a Merkle DAG can hold data, not just a leaf
    node, as is the case in a Merkle tree.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a sample directory that contains sample files and a nested directory
    that also contains files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, create a Python script to explain this new data structure. Create a file
    called `merkle_dag.py` in your project directory. Refer to the GitLab link for
    the complete code file: [https://gitlab.com/arjunaskykok/hands-on-blockchain-for-python-developers/tree/master/chapter_10](https://gitlab.com/arjunaskykok/hands-on-blockchain-for-python-developers/tree/master/chapter_10).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s discuss the `MerkleDAGNode` class, starting from its initialization
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The `_init_()` method accepts a file path as an argument. This could be a path
    to a file or a directory. We make an assumption that this is a valid path and
    not a symbolic link.  `self.pointers` will be explained later on in the section
    with the `_iterate_directory_contents()` method. `self.dirtype` is used to differentiate
    between the directory or the file. `self.filename` is used to hold the name of
    the file or the name of the directory.
  prefs: []
  type: TYPE_NORMAL
- en: If the argument is the path to the file (not the directory), we read the content
    into `self.content`. For demonstration purposes, we assume the content of the
    file is small and we don't try to split the files like we did before. Then, we
    calculate the hash based on the filename and the content.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the argument is the path to the directory, the content would be an array
    of `MerkleDAGNode` objects of the inner files inside that directory. To calculate
    the hash, we use a Merkle tree to get the root hash of its children. However,
    we need to concatenate this with the name of the directory before hashing it again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '`_hash()` is a wrapper method of the `sha256` hashing function.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `_iterate_directory_contents()` method is used to iterate over the inner
    children of the directory. We convert every file or directory inside this directory
    to a `MerkleDAGNode` object. The `self.pointers` object is used to make it easier
    to access the `MerkleDAGNode` based on the filename. Basically, it is like a recursive
    function, especially when we hit a directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The `_repr_()` method is used to make it easier to print objects for debugging:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The `_eq_()` method is needed so that we can compare the `MerkleDAGNode` object
    with other `MerkleDAGNode` objects. This is useful during the testing process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create a `hash_directory.py` file to demonstrate the power of this data
    structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'You would get the following result if you execute the script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The output is the schema of the Merkle DAG node.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is how Git keeps the files. Our implementation is just for education purposes
    and would not be fit for production purposes. In the real world, you should have
    many optimizations. One of the optimizations that you could implement is using
    a reference for the data, just like Git. If there are two different files that
    have the same content (but different filenames), the content would be saved just
    once. The other optimization is that Git uses compression. The following diagram
    illustrates the concept of Git, where we have two files, **file B** and **file
    D***.* These both have the same content, **content xxx**.**File B **is saved just
    once in **directory A***.* **File D** is saved at **directory C** with **file
    E***,* which has a different content, **content yyy**. **Directory C** is also saved
    in **directory A**. But the content of **File B** and **File D**, which is **content
    xxx**, is saved only once:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/46c4396a-6399-4c6b-be3c-9cc48bf2509e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that we know how to save a directory of files with Merkle DAG, what if
    we want to change the content of the file? Should we abandon this Merkle DAG node
    and create a totally new node? A more efficient way to solve this problem would
    be to use a versioning system. A file could have version 1, version 2, version
    3, and so on. The easiest way to implement versioning is to use a linked list,
    as illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/d99ffb30-fb8b-497a-8b8d-1e4415225d2f.png)'
  prefs: []
  type: TYPE_IMG
- en: Peer-to-peer networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We understand how to save files in IPFS. The key is the **hash**. The value
    is the name of the file or directory and the content of the file or directory.
    If we were building a centralized system, our story would be finished. We would
    just need to add a few other things to create a piece of software to save files
    and search them based on the hash. This software would be similar to a database,
    such as SQLite or LevelDB. IPFS is neither of those; it is a peer-to-peer filesystem
    that is like a database but spread all over the place. In other words, it is a
    distributed hash table.
  prefs: []
  type: TYPE_NORMAL
- en: IPFS uses S/Kademlia, an extended version of Kademlia, as a distributed hash
    table. Before we discuss Kademlia, let's discuss its predecessor.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, imagine a hash table, which is like a dictionary in Python, as shown
    in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Key** | **Value** |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Cat |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | Unicorn |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | Elephant |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | Horse |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Rhino |'
  prefs: []
  type: TYPE_TB
- en: '| 101 | Blue Parrot |'
  prefs: []
  type: TYPE_TB
- en: '| 33 | Dragon |'
  prefs: []
  type: TYPE_TB
- en: In IPFS, the key is the hash, not a number. But for demonstration purposes,
    let's make it a simple integer. The value is just a simple name of animal, not
    the content of the file or the content of the files inside a directory.
  prefs: []
  type: TYPE_NORMAL
- en: Now, imagine you have four nodes. A node could be a computer that is located
    in a different continent to the rest of the nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s define which node holds which keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Node** | **Keys** |'
  prefs: []
  type: TYPE_TB
- en: '| A | 2, 9, 11 |'
  prefs: []
  type: TYPE_TB
- en: '| B | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| C | 4, 33 |'
  prefs: []
  type: TYPE_TB
- en: '| D | 101 |'
  prefs: []
  type: TYPE_TB
- en: You keep this table in a central server. One of the nodes will be the central
    node. This means that if someone wants to access key five, they have to ask the
    central server before receiving the answer, node B. After that, the request can
    be directed to node B. Node B would return "Unicorn" to the data requester.
  prefs: []
  type: TYPE_NORMAL
- en: This method is very efficient; no time is wasted. Napster, the peer-to-peer
    music sharing system, uses this approach. The drawback is that the central server
    is a single point of failure. An adversary (someone who does not like this information
    being spread; in the case of Napster, this could be a big music labels) could
    attack the central server.
  prefs: []
  type: TYPE_NORMAL
- en: One solution would be to ask all nodes about which node holds the key instead
    of keeping this information in the central node. This is what Gnutella does. This
    setup is resilient to censorship and attacks from adversaries but it makes life
    hard for nodes and people who request the data. The node must work hard when receiving
    many requests. This setup is called **flooding**. It is suitable for Bitcoin,
    but not for IPFS.
  prefs: []
  type: TYPE_NORMAL
- en: This is why the distributed hash table technology was created. There are a couple
    of distributed hash table algorithms, one of which is Kademlia. This algorithm
    was created by Petar Maymounkov and David Mazières in 2002\. It was later used
    by the eDonkey file sharing platform.
  prefs: []
  type: TYPE_NORMAL
- en: The notion of closeness of data and nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a distributed hash table, we don't put the data in every node. We put the
    data in certain nodes according to the notion of closeness. We want to put the
    data in nearby nodes. This means that we have the concept of distance not just
    between nodes, but also between the data and the nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine that every node launched or created in this distributed hash table
    is given an ID between 1 and 1000\. Every node ID is unique, so there can be a
    maximum of 1,000 nodes. There are likely to be more than 1,000 nodes in a real-world
    setting, but this will work as an example. Let''s say that we have 10 nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Node ID** |'
  prefs: []
  type: TYPE_TB
- en: '| 5 |'
  prefs: []
  type: TYPE_TB
- en: '| 13 |'
  prefs: []
  type: TYPE_TB
- en: '| 45 |'
  prefs: []
  type: TYPE_TB
- en: '| 48 |'
  prefs: []
  type: TYPE_TB
- en: '| 53 |'
  prefs: []
  type: TYPE_TB
- en: '| 60 |'
  prefs: []
  type: TYPE_TB
- en: '| 102 |'
  prefs: []
  type: TYPE_TB
- en: '| 120 |'
  prefs: []
  type: TYPE_TB
- en: '| 160 |'
  prefs: []
  type: TYPE_TB
- en: '| 220 |'
  prefs: []
  type: TYPE_TB
- en: 'We also have some data. To make it simple, the data in this case is just some
    strings:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Data** |'
  prefs: []
  type: TYPE_TB
- en: '| Unicorn |'
  prefs: []
  type: TYPE_TB
- en: '| Pegasus |'
  prefs: []
  type: TYPE_TB
- en: '| Cat |'
  prefs: []
  type: TYPE_TB
- en: '| Donkey |'
  prefs: []
  type: TYPE_TB
- en: '| Horse |'
  prefs: []
  type: TYPE_TB
- en: 'To be able to say whether this data is close to or far from certain nodes,
    we need to convert this data into a number between 1 and 1000\. In the real world,
    you could hash the data. But for our practical demonstration, we will just allocate
    a random number:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Key** | **Data** |'
  prefs: []
  type: TYPE_TB
- en: '| 54 | Unicorn |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Pegasus |'
  prefs: []
  type: TYPE_TB
- en: '| 100 | Cat |'
  prefs: []
  type: TYPE_TB
- en: '| 900 | Donkey |'
  prefs: []
  type: TYPE_TB
- en: '| 255 | Horse |'
  prefs: []
  type: TYPE_TB
- en: If we want to store the Unicorn data in the four nearest nodes (four is just
    a configuration number), this can be done as follows. First, you check the key,
    which is 54\. Then, we want to get the nearest four nodes to 54\. If you check
    the node ID list, the nearest four nodes are 45, 48, 53, and 60\. So, we store
    the Unicorn data in these four nodes. If we want to store the Cat data, the nearest
    neighbors from its key, 100, are 53, 60, 102, and 120, so we store the Cat data
    in these four nodes.
  prefs: []
  type: TYPE_NORMAL
- en: We treat data as a node when calculating the distance. This is how we look up
    data in a distributed hash table. The data and the nodes share the same space.
  prefs: []
  type: TYPE_NORMAL
- en: XOR distance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: However, in Kademlia, we don't measure distance by decimal subtraction. To make
    it clear, decimal subtraction is just normal subtraction. The distance between
    45 and 50 is 5\. The distance between 53 and 63 is 10.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Kademlia, measuring distance is done by XOR distance. The XOR distance between
    3 and 6 is 5, not 3\. Here''s how to count it:'
  prefs: []
  type: TYPE_NORMAL
- en: The binary version of 3 is 011\. The binary version of 6 is 110\. What I mean
    by binary version is the number in base 2\. XOR means *exclusive or*. Using the
    XOR operation, 1 XOR 0 is 1, 1 XOR 1 is 0, 0 XOR 0 is 0, and 0 XOR 1 is 1\. If
    two operands are same, the result is 0\. If two operands are different, the result
    is 1.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 101 is the binary version of 5.
  prefs: []
  type: TYPE_NORMAL
- en: The XOR distance has a few useful properties that prompted the author of the
    Kademlia paper to choose the XOR distance to measure the distance between the
    nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first property is that the XOR distance of a node to itself is 0\. The
    closest node to a node with an ID of 5 is another node with an ID 5, or itself.
    The binary version of 5 is 0101:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The 0 distance is only possible if we measure the distance between a node and
    itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second property is that the distance between different nodes is symmetrical.
    The XOR distance between 4 and 8 is same as the XOR distance between 8 and 4\.
    The binary version of 4 is 0100 and the binary version of 8 is 1000\. So, if we
    calculate the distance between them using their binary value, we get the same
    value. The XOR distance between 4 and 8 is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The XOR distance between 8 and 4 is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: If you are used to working with decimal subtraction distances, this will be
    intuitive to you.
  prefs: []
  type: TYPE_NORMAL
- en: The last useful property is that the distance between node X and node Z is less
    than or equal to the distance between node X and node Y plus the distance between
    node Y and Z. This last property is important because a node in a Kademlia distributed
    hash table does not save all the other nodes' addresses. It only saves some nodes'
    addresses. But a node can reach another node through intermediate nodes. Node
    X knows the address of node Y, but does not know the address of node Z. Node Y
    does know the address of node Z. Node X can query the neighbor nodes of node Y
    from node Y. Then, node X can reach node Z knowing that the distance to node Z
    is less than or equal to the distance of node X and node Y added to the distance
    of node Y and node Z.
  prefs: []
  type: TYPE_NORMAL
- en: If this property were not true, the longer node X searches for a node, the further
    the distance a particular node will be, which is not what we wanted. But with
    this property, the addresses of neighbor nodes from other nodes may be smaller
    than, if not the same as, the combined distances.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you think about using XOR distance, you should think that the more prefixes
    shared by two numbers, the shorter the distance between those two numbers. For
    example, if the numbers share three common prefixes, such as five and four, the
    distance is one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Likewise, for numbers 14 and 15, the distance is also 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'But, if the bit differences are on the left side, such as is the case for 5
    and 13, the distance might be large, in this case eight:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The XOR distance between 4 and 5 is 1 but the XOR distance between 5 and 6
    is 3\. This is counter-intuitive if you are accustomed to decimal subtraction
    distances. To make this concept easier to explain, let''s create a binary tree
    that is composed of numbers from 1 to 15:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/8c80866f-e3cb-4e74-848d-350b3decd707.png)'
  prefs: []
  type: TYPE_IMG
- en: Look at this tree carefully. The XOR distance between 4 and 5 is 1, but the
    XOR distance between 5 and 6 is 3\. If you look at the picture, 4 and 5 are under
    an immediate branch, whereas 5 and 6 is under a larger branch, which implies a
    larger distance. The immediate branch corresponds to the bit on the right. The
    parent branch of that immediate branch corresponds to the second-most right bit.
    The top branch corresponds to the bit on the left. So, if the number is separated
    by a top branch, the distance is at least 8\. The binary version of 8 is 1000.
  prefs: []
  type: TYPE_NORMAL
- en: This is just for understanding purposes; it is not a rigorous mathematical definition.
    If you look at the journey from 5 to 11 and 5 to 13, you should get roughly the
    same distance, but this is not the case. The XOR distance of 5 and 13 is 8 but
    the XOR distance of 5 and 11 is 14.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Python, you can XOR two numbers with the `^` operator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'You can turn any decimal number to its binary version using the `bin` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, if you want to convert the binary number back to a decimal number, use
    the `int` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The second argument of the `int` function indicates which base the first argument
    is. Binary is base 2.
  prefs: []
  type: TYPE_NORMAL
- en: Buckets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have gone through XOR distances, we will take a look at how a node
    saves other nodes' addresses. A node does not save all other nodes in a distributed
    hash table. The number of nodes a node can save depends on the number of bits
    in a node and the *k* configuration number. Let's discuss these one by one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember the tree picture we saw previously? It has 16 leaves. Now imagine
    the smallest tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/6b875b7f-a32b-4794-9b1a-00f29338e452.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It has two leaves. Let''s double the tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/2db92237-ac9d-4f48-bda5-a2b63d59e967.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The tree now has four leaves. Let''s double it again:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/b73a1d5c-48ab-43bb-a493-6076e78744e1.png)'
  prefs: []
  type: TYPE_IMG
- en: The tree now has eight leaves. If you double it again, you would have a tree
    like our previous tree, which has 16 leaves.
  prefs: []
  type: TYPE_NORMAL
- en: The progression we can see is 2, 4, 8, 16\. If we continue the journey, the
    numbers would be 32, 64, 128, and so on. This can be written as 2^(1,) 2^(2,)
    2^(3,) 2^(4,) 2^(5 )... 2^n.
  prefs: []
  type: TYPE_NORMAL
- en: Let's focus on a tree with 16 leaves. When we represent the leaf number, we
    must use a 4-bit binary number, such as 0001 or 0101, because the biggest number
    is 15, or 1111\. If we use a tree with 64 leaves, we must use a 6-bit number,
    such as 000001, 010101 because the biggest possible number is 63 or 111111\. The
    bigger the bit number, the larger the amount of nodes a node must save in its
    address book.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we have the *k* configuration number. *k* decides the maximum amount of
    nodes a node can save in a bucket. The number of buckets is the same as the number
    of bits used in a distributed hash table. In a tree with 16 leaves, the number
    of buckets is 4\. In a tree with 64 leaves, the number of buckets is 6\. Each
    bucket corresponds to a bit. Let's say we have a tree with 16 leaves, so each
    number has 4 bits, such as 0101 or 1100\. This means the node has four buckets.
  prefs: []
  type: TYPE_NORMAL
- en: The first bucket corresponds to the first bit from the left. The second bucket
    corresponds to the second bit from the left. The third bucket corresponds to the
    third bit from the left. The fourth bucket corresponds to the fourth bit from
    the left.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at example of a node with ID 3 in a tree with 16 leaves. For
    now, we assume we have 16 nodes in a tree that has 16 leaves. In the real world,
    the tree would be sparse and a lot of branches would be empty.
  prefs: []
  type: TYPE_NORMAL
- en: In the paper that describes Kademlia, the authors used 160 buckets or a 160-bit
    address. The number of leaves in this tree is vast. For comparison, 2^(78) is
    the number of atoms in visible universe. The *k* configuration number is chosen
    as 20 in this paper, so a node can have a maximum of 3,200 nodes in its address
    book.
  prefs: []
  type: TYPE_NORMAL
- en: For this example, let's say that the *k* number is 2\. This means for every
    bucket, the node saves two other nodes. The first bucket, which corresponds to
    the first bit, corresponds to the other half of the tree, where the node does
    not reside. We have eight nodes in this half of the tree but we can only save
    two of them because the *k* number is 2\. Let's choose nodes 11 and 14 for this
    bucket. How we choose which nodes go in which buckets will be described later.
  prefs: []
  type: TYPE_NORMAL
- en: Then, let's divide the half of the tree where the node resides, so we have two
    branches. The first branch consists of a node with ID 0, a node with ID 1, a node
    with ID 2, and a node with ID 3\. The second branch consists of a node with ID
    4, a node with ID 5, a node with ID 6, and a node with ID 7\. This second branch
    is the second bucket. There are four nodes in this branch, but we can only save
    two nodes. Let's choose the node with ID 4 and the node with ID 5.
  prefs: []
  type: TYPE_NORMAL
- en: Then, let's divide the branch where our node (the node with ID 3) resides so
    we have two small branches. The first small branch consists of a node with ID
    0 and a node with ID 1\. The second small branch consists of a node with ID 2
    and a node with ID 3\. So the third bucket is the first small branch. There are
    only two nodes, a node with ID 0 and node with ID 1, so we save both.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, let's divide the the small branch where our node (the node with ID
    3) resides so we have two tiny branches. The first branch consists of a node with
    ID 2 and the second branch consists of a node with ID 3\. The fourth bucket, or
    the last bucket, would be the branch that consists of node 3.
  prefs: []
  type: TYPE_NORMAL
- en: 'We save this one node because it is less than the *k* configuration number:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/7d329036-5aeb-4c97-a021-9e0881104884.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following diagram shows the full four buckets. Each bucket is half of the
    branch in which the source node does not reside. The bucket configuration of different
    nodes are different. The node with ID 11 could have a bucket configuration that
    looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/2b9a9034-ab6e-4ea9-9c39-581024b84b36.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's take a look at an example of how a certain node could find another node
    that does not reside in its address book. Imagine the *k* configuration number
    is 1\. The source node is the node with the ID 3 in a tree with 16 leaves. For
    the first bucket (the largest branch that consists of the nodes from ID 8 to ID
    15), the node with ID 3 saves the node with ID 10\. But the node with ID 3 wants
    to find the node with ID 13\. The node with ID 3 contacts the node with ID 10
    with a request, "Can you help me find the node with ID 13?". The node with ID
    10 has saved the node with ID 14 in its corresponding bucket (the branch that
    consists of nodes with IDs 12, 13, 14, and 15). The node with ID 10 gives the
    node with ID 14 to the node with ID 3\. The node with ID 3 asks the same question
    to the node with ID 14, "Can you help me find the node with ID 13?". The node
    with ID 14 does not have it, but it has the node with ID 12 in its bucket (the
    branch that consists of the node with ID 12 and the node with ID 13). The node
    with ID 14 gives the node with ID 12 to the node with ID 3\. The node with ID
    3 asks the same question again to the node with ID 12\. This time, the node with
    ID 12 can give the destination node or the node with ID 13 to the node with ID
    3\. A happy ending!
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/cfbe47f1-27c2-428f-811e-d7b9c40d0bc6.png)'
  prefs: []
  type: TYPE_IMG
- en: Did you notice how many times the node ID 3 must repeat the request? Four times.
    If this number sounds familiar, that is because this tree has 16 leaves, which
    is 2^(4.) In computer science, the worst case scenario of the amount of hopping
    required before getting to the destination is 2 log *n* + *c*. *n* is how many
    leaves the tree has and *c* is constant number.
  prefs: []
  type: TYPE_NORMAL
- en: 'The tree you have just seen has full nodes; there are no empty leaves or empty
    branches. In the real world, however, there are empty branches and empty leaves.
    Imagine that you have a tree with 1,024 (2^(10)) leaves and the *k* number is
    3\. You launch the first node with the ID 0\. This node will be the source node.
    We will see the tree from the lens of the node with ID 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/0f92b192-ba20-438a-938f-75da307a0aab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, you launch the node with ID 800:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/32cc83c0-7e33-4d6f-b7e2-53896d14288a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The tree will be split into two buckets. Then, you launch the node with ID
    900 and the node with ID 754:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/481a5c67-8515-47b9-bb3c-6723620403ce.png)'
  prefs: []
  type: TYPE_IMG
- en: What if we add another node to the bucket? Let's launch the node with ID 1011\.
    The node with ID 0 will ping the least recently used node, which is the node with
    ID 800, to see if it is still alive. If it is, it will check the other nodes.
    If the node with ID 754 is not alive, then this node will be replaced with the
    node with ID 1011\. If all the nodes are still alive, then the node with ID 1011
    will be rejected from the bucket. The reason for this is to avoid new nodes swamping
    the system. We assume that the nodes with longer uptimes are trustworthy and we
    prefer these nodes to new nodes. Let's say we reject the node with ID 1011.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we launch the node with ID 490\. Then, we split the branch where the
    node with ID 0 resides:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/f415e136-998e-4aec-a5b3-5028bc47442a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s add the node with ID 230:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/e8f5df06-7ab4-4ea4-a2f7-bf275c745490.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s add the node with ID 60:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/9179cbd8-eff2-4dc0-b73f-ca7f4a29ade6.png)'
  prefs: []
  type: TYPE_IMG
- en: '...and so on. Every time we add a node in a branch where the source node resides,
    it will split the bucket into two until it reaches the lowest level. If we add
    a node in other branch on which the source node does not live, we add nodes until
    we reach the *k* number.'
  prefs: []
  type: TYPE_NORMAL
- en: You now have a basic understanding of how Kademlia works. This is not, however,
    the whole story. If a node is inserted, a node needs to tell the older nodes of
    its existence. That node also needs to get the contacts from the old node. I mentioned
    that the branch is split when a node is inserted to a branch on which the source
    node resides, but there is a case where the branch is split even when the source
    node does not reside there. This happens because a node is required to keep all
    valid nodes in a branch that has at least *k* nodes if that means the branch in
    which the source node does not reside has to be split.
  prefs: []
  type: TYPE_NORMAL
- en: There are other important aspects of Kademlia other than routing algorithms.
    A node is required to republish the key and the value (the data) every hour. This
    is to anticipate the old nodes leaving and the new nodes joining the system. These
    nodes are nearer, so they are more suited to keep the data. There is also an accelerated
    lookup algorithm so that we can use fewer steps when a node is looking for another
    node.
  prefs: []
  type: TYPE_NORMAL
- en: You can refer to the Kademlia paper for the full specification. [https://pdos.csail.mit.edu/~petar/papers/maymounkov-kademlia-lncs.pdf](https://pdos.csail.mit.edu/~petar/papers/maymounkov-kademlia-lncs.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: IPFS uses S/Kademlia, an extended version of Kademlia. It differs from the original
    Kademlia algorithm in that S/Kademlia has some security requirements. Not all
    nodes join the Kademlia distributed hash table with a noble purpose. So, in S/Kademlia,
    to generate the ID of a node, it requires the node to generate a cryptography
    key pair, so it is very difficult to tamper with the communication between the
    nodes. Other requirements include the fact that proof-of-work (like in Bitcoin
    and Ethereum) is used before a node is able to generate its ID. There is also
    some adjustment in the routing algorithm to make sure a node can communicate with
    other nodes in the midst of adversaries, such as nodes that spam the network.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have studied IPFS. We started by looking at the motivations
    of the IPFS project and its history. Although IPFS is not a part of the blockchain
    technology, it is similar to blockchain because it complements blockchain technology.
    We then learned about the data structure of the content that we saved in the IPFS
    filesystem. This data structure is Merkle **Directed Acyclic Graph** (**DAG**),
    which is based on the Merkle tree. We created simple Merkle tree and Merkle DAG libraries
    to understand the uniqueness of these data structures. Merkle trees provide an
    easy way to check the integrity of partial data, while Merkle DAGs are used when
    we want to save a directory with files and we want to keep the filenames. Then,
    we learned about the peer-to-peer networking aspect of a Kademlia distributed
    hash table. The distance between nodes is based on the XOR distance. The nodes
    also are kept in buckets, which corresponds to bit addressing. Finally, we showed
    how a node can find others nodes by hopping through the buckets.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to use the IPFS software and interact with
    it programmatically.
  prefs: []
  type: TYPE_NORMAL
