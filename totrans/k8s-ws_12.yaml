- en: 12\. Your Application and HA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will explore Kubernetes cluster life cycle management through
    the use of Terraform and Amazon **Elastic Kubernetes Service** (**EKS**). We will
    also deploy an application and learn some principles to make applications better
    suited to the Kubernetes environment.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will walk you through using Terraform to create a fully functioning,
    highly available Kubernetes environment. You will deploy an application to the
    cluster and modify its functionality to make it suitable for a highly available
    environment. We will also learn how to get traffic from the internet to an application
    running in a cluster by using a Kubernetes ingress resource.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we set up our first multi-node Kubernetes cluster in
    a cloud environment. In this section, we're going to talk about how we operationalize
    a Kubernetes cluster for our application—that is, we will use the cluster to run
    a containerized application other than the dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: Since Kubernetes has as many uses as can be imagined by a cluster operator,
    no two use cases for Kubernetes are alike. So, we're going to make some assumptions
    about the type of application that we're operationalizing our cluster for. We're
    going to optimize a workflow for deploying a stateless web application with a
    stateful backend that has high-availability requirements in a cloud-based environment.
    In doing so, we're hopefully going to cover a large percentage of what people
    generally use Kubernetes clusters for.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes can be used for just about anything. Even if what we cover does not
    exactly match your use case for Kubernetes, it's worth studying since this point
    is important. What we're going to be doing in this chapter is merely running through
    an example workflow for running a web application on Kubernetes in the cloud.
    Once you have studied the principles that we will use for running the example
    workflow in this chapter, you can look up many other resources on the internet
    that can help you discover other ways of optimizing your workflow with Kubernetes
    if this doesn't fit your use case.
  prefs: []
  type: TYPE_NORMAL
- en: But before we move on to ensure the high availability of the application that
    we will be running on the cluster, let's take a step back and consider the high-availability
    requirements for your cloud infrastructure. In order to maintain high availability
    at an application level, it is also imperative that we manage our infrastructure
    with the same goal in mind. This brings us to a discussion about infrastructure
    life cycle management.
  prefs: []
  type: TYPE_NORMAL
- en: An Overview of Infrastructure Life Cycle Management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In simple words, infrastructure life cycle management refers to how we manage
    our servers through each phase of its useful life. This involves provisioning,
    maintaining, and decommissioning physical hardware or cloud resources. Since we
    are leveraging cloud infrastructure, we should leverage infrastructure life cycle
    management tools to provision and de-provision resources programmatically. To
    understand why this is important, let's consider the following example.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine for a moment that you work as a system administrator, DevOps engineer,
    site reliability engineer, or any other role that requires you to deal with server
    infrastructure for a company that is in the digital news industry. What that means
    is that the primary output of the people who are working for this company is the
    information that they publish on their website. Now, imagine that the entirety
    of the website runs on one server in your company's server room. The application
    running on the server is a PHP blog site with a MySQL backend. One day, an article
    goes viral and suddenly you are handling an exponentially higher amount of traffic
    than you were handling the day before. What do you do? The website keeps crashing
    (if it loads at all) and your company is losing money while you try to figure
    out a solution.
  prefs: []
  type: TYPE_NORMAL
- en: Your solution is to start separating concerns and isolating single points of
    failure. The first thing you do is buy a lot more hardware and start configuring
    it to hopefully scale the website horizontally. After doing this, you're running
    five servers, with one running HAProxy, which is load-balancing connections to
    your PHP application running on three servers and a database server. OK, now you
    think that you have it under control. However, not all of the server hardware
    is the same—they run different distributions of Linux, the resource requirements
    are different for each machine, and patching, upgrading, and maintaining each
    server individually becomes difficult. Well, as luck would have it, another article
    goes viral and suddenly you're experiencing five times more requests than the
    current hardware can handle. What do you do now? Keep scaling it out horizontally?
    You're only one person, though, so you're bound to make a mistake in configuring
    the next set of servers. Due to that mistake, you've crashed the website in new
    and exciting ways that no one in management is happy about. Are you feeling as
    stressed reading this as I was writing it?
  prefs: []
  type: TYPE_NORMAL
- en: It's because of misconfigurations that engineers began to leverage tools and
    configuration written in source code to define their topologies. That way, if
    a mutation in the infrastructure state is required, it can be tracked, controlled,
    and rolled out in a way that makes the code responsible for resolving differences
    between your declared infrastructure state and what it observes in reality.
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure is only as good as the life cycle management tools that surround
    it and the application that runs atop it. What this means is that if your cluster
    is well-built but there is no tool that exists to successfully update your application
    on that cluster, then it won't serve you well. In this chapter, we're going to
    take a look at an application-level view of how we can leverage a continuous integration
    build pipeline to be able to roll out new updates to our application in a zero-downtime,
    cloud-native manner.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will provide a test application for you to manage. We will
    also be using an infrastructure life cycle management tool called **Terraform**
    in order to manage the Deployment of Kubernetes cloud infrastructure more efficiently.
    This chapter should help you develop an effective skill set that will allow you
    to begin creating your own application delivery pipeline in your own environment
    in Kubernetes very quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Terraform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last chapter, we used **kops** to create a Kubernetes cluster from scratch.
    However, this process can be viewed as tedious and difficult to replicate, which
    creates a high probability of misconfiguration, resulting in unexpected events
    at application runtime. Luckily, there is a very powerful community-supported
    tool that solves this issue very well for Kubernetes clusters running on **Amazon
    Web Services** (**AWS**), as well as several other cloud platforms, such as Azure,
    **Google Cloud Platform** (**GCP**), and many more.
  prefs: []
  type: TYPE_NORMAL
- en: Terraform is a general-purpose infrastructure life cycle management tool; that
    is, Terraform can manage the state of your infrastructure as defined through code.
    The goal of Terraform, when it was initially created, was to create both a language
    (**HashiCorp Configuration Language** (**HCL**)) and runtime that can create infrastructure
    in a repeatable manner and control changes to that infrastructure in the same
    way that we control changes to application source code—through pull requests,
    reviews, and version control. Terraform has since grown considerably, and it is
    now a general-purpose configuration management tool. In this chapter, we will
    be using its original functionality of infrastructure life cycle management in
    its most classical sense.
  prefs: []
  type: TYPE_NORMAL
- en: Terraform files are written in a language called HCL. HCL looks a lot like YAML
    and JSON, but with a few differences. For example, HCL supports the interpolation
    of references to other resources in its files and is capable of determining the
    order in which resources need to be created so as to ensure that resources that
    depend on the creation of other resources won't be created in the wrong order.
    Terraform files have the `.tf` file extension.
  prefs: []
  type: TYPE_NORMAL
- en: You can think of a Terraform file as specifying the desired state of your entire
    infrastructure in a similar way as, for example, a Kubernetes YAML file would
    specify the desired state of a Deployment. This allows the declarative management
    of your entire infrastructure. So, we arrive at the idea of managing **Infrastructure
    as Code** (**IaC**).
  prefs: []
  type: TYPE_NORMAL
- en: Terraform works in two stages—**plan** and **apply**. This is to ensure that
    you have the chance to review infrastructure changes before making them. Terraform
    assumes that it alone is responsible for all state changes to your infrastructure.
    So, if you are using Terraform to manage your infrastructure, it would be inadvisable
    to make infrastructure changes by any other means (for example, by adding a resource
    via the AWS console). This is because if you make a change and don't make sure
    that it is updated in the Terraform file, then the next time the Terraform file
    is applied, it will remove your one-time change. It isn't a bug, it's a feature,
    for real this time. The reason for this is that when you track infrastructure
    as code, every change can be tracked, reviewed, and managed with automated tooling,
    such as a CI/CD pipeline. So, if the state of your system drifts away from what
    is written down, then Terraform will be responsible for reconciling your observed
    infrastructure to what you have written down.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will introduce you to Terraform as it is very commonly used
    in the industry as a convenient way to manage infrastructure as code. However,
    we will not dive deep into creating every single AWS resource with Terraform to
    keep our discussion focused on Kubernetes. We will just carry out a quick demo
    to ensure that you understand some basic principles.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'You can learn more about using Terraform for AWS in this book: [https://www.packtpub.com/networking-and-servers/getting-started-terraform-second-edition](https://www.packtpub.com/networking-and-servers/getting-started-terraform-second-edition
    )'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 12.01: Creating an S3 Bucket with Terraform'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, we will implement some common commands that you will use when
    working with Terraform and introduce you to a Terraform file that will be the
    definition of our infrastructure as code
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Terraform will create resources on our behalf in AWS, which will cost you money.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s make a directory where we''re going to make our Terraform changes,
    and then we will navigate to that directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we''re going to make our first Terraform file. Terraform files have a
    `.tf` file extension. Create a file named `main.tf` (there is no significance
    to the word `main`, unlike some other languages) with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This block has a definition called `aws_s3_bucket`, which means that it will
    create an Amazon S3 bucket with the name specified in the `bucket` field. The
    `acl="private"` line indicates that we are not allowing public access to this
    bucket. Be sure to replace `<<NAME>>` with a unique name of your own.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started with Terraform, we need to initialize it. So, let''s do that
    with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.1: Initializing Terraform'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_12_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 12.1: Initializing Terraform'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command to have Terraform determine a plan to create resources
    defined by the `main.tf` file that we created earlier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'You will be prompted to enter an AWS region. Use the one that''s closest to
    you. In the following screenshot, we are using `us-west-2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.2: Calculating the required changes to the cluster resources'
  prefs: []
  type: TYPE_NORMAL
- en: for creating an S3 bucket
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_12_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 12.2: Calculating the required changes to the cluster resources for
    creating an S3 bucket'
  prefs: []
  type: TYPE_NORMAL
- en: So, we can see that Terraform has accessed our AWS account using the access
    keys that we set up in *Exercise 11.01, Setting Up Our Kubernetes Cluster* of
    the previous chapter and calculated what it will need to do in order to make our
    AWS environment look like what we have defined in our Terraform file. As we can
    see in the screenshot, it's planning to add an S3 bucket for us, which is what
    we want.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Terraform will try to apply all the files with a `.tf` extension in your current
    working directory.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous screenshot, we can see that the `terraform` command is indicating
    that we haven't specified an `-out` parameter, so it won't guarantee that the
    exact calculated plan will be applied. This is because something in your AWS infrastructure
    could have changed from the time of planning to the time of applying. Let's say
    that you calculate a plan today. Then, later, you add or remove a few resources.
    So, the required modifications to achieve the given state would be different.
    So, unless you specify the `-out` parameter, Terraform will recalculate its plan
    before applying it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command to apply the configuration and create the resources
    specified in our Terraform file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Terraform will give us one more chance to review the plan and decide what we
    want to do before making the changes to the AWS resources for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.3: Calculation of the changes and confirmation prompt for creating
    an S3 bucket'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_12_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 12.3: Calculation of the changes and confirmation prompt for creating
    an S3 bucket'
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned earlier, Terraform calculated the required changes even when we
    used the `apply` command. Confirm the actions displayed by Terraform, and then
    enter `yes` to proceed with the plan displayed. Now, Terraform has made an S3
    bucket for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.4: Creating an S3 bucket after confirmation'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_12_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 12.4: Creating an S3 bucket after confirmation'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we''re going to destroy all the resources that we created to clean up
    before we move on to the next exercise. To destroy them, run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, to confirm this action, you must explicitly allow Terraform to destroy
    your resources by entering `yes` when prompted, as in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.5: Destroying resources created using Terraform'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_12_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 12.5: Destroying resources created using Terraform'
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, we demonstrated how to create a single resource (an S3 bucket)
    using Terraform, and also how to destroy a bucket. This should have familiarized
    you with the simple tooling of Terraform, and we will now expand on these concepts
    further.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's make a Kubernetes cluster with Terraform. Last time, we built and
    managed our own cluster control plane. Since almost every cloud provider provides
    this service to their customers, we will be leveraging Amazon **Elastic Kubernetes
    Service** (**EKS**), a **managed service** for Kubernetes provided by AWS.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we use a managed Kubernetes service, the following is taken care of by
    the cloud service vendor:'
  prefs: []
  type: TYPE_NORMAL
- en: Managing and securing etcd
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing and securing user authentication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing the control plane components, such as the controller manager(s), the
    scheduler, and the API server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provisioning the CNI running between Pods in your network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The control plane is exposed to your nodes through elastic network interfaces
    bound to your VPC. You still need to manage the worker nodes and they run as EC2
    instances in your account. So, using a managed service allows you to focus on
    the work that you want to get done using Kubernetes, but the drawback is not having
    very granular control of the control plane.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Since AWS handles user authentication for the cluster, we will have to use AWS
    IAM credentials to access our Kubernetes clusters. We can leverage the AWS IAM
    Authenticator binary on our machines to do that. More on this in the upcoming
    sections.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 12.02: Creating a Cluster with EKS Using Terraform'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this exercise, we will use the `main.tf` file that we have already provided
    to create a production-ready, highly available Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This Terraform file is adapted from the examples available at [https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/examples](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/examples).
  prefs: []
  type: TYPE_NORMAL
- en: 'This will enable Terraform to create the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A VPC with IP address space `10.0.0.0/16`. It will have three public subnets
    with `/24`s (`255`) worth of IP addresses each.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Route tables and an internet gateway for the VPC to work properly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security groups for the control plane to communicate with the nodes, as well
    as to receive traffic from the outside world on the allowed and required ports.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IAM roles for both the EKS control plane (to perform tasks such as creating
    **ELB** (**Elastic Load Balancer**) for services on your behalf) and the nodes
    (to handle EC2 API-related concerns).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The EKS control plane and a setup of all the necessary connections to your VPC
    and nodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An **ASG** (**Autoscaling Group**) for nodes to join the cluster (it will provision
    two **m4.large** instances).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate both a kubeconfig file and a ConfigMap, which are necessary for the
    nodes to join the cluster and for you to communicate with the cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is a relatively secure and stable way for you to create a Kubernetes cluster
    that is capable of reliably handling production workloads. Let''s begin with the
    exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following command to fetch the `main.tf` file that we have provided:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This will replace the existing `main.tf` file, if you still have it from the
    previous exercise. Note that you should not have any other Terraform files in
    the directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need Terraform to apply the state defined in the `main.tf` file to
    your cloud infrastructure. To do that, use the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You should not use the AWS IAM user we generated for kops in the previous chapter
    to execute these commands, but rather a user with Administrative access to your
    AWS account so there is no chance of accidental permissions issues.
  prefs: []
  type: TYPE_NORMAL
- en: 'This may take around 10 minutes to complete. You should see a very long output
    similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.6: Creating resources for our EKS cluster'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_12_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 12.6: Creating resources for our EKS cluster'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once this is done, there will be two terminal outputs—a ConfigMap for nodes
    and a kubeconfig file for accessing the cluster, as demonstrated in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.7: Getting the information required to access our cluster'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_12_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 12.7: Getting the information required to access our cluster'
  prefs: []
  type: TYPE_NORMAL
- en: Copy the ConfigMap to a file and name it `configmap.yaml`, and then copy the
    kubeconfig file and write it to the `~/.kube/config` file on your computer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need to apply the changes to allow our worker nodes to communicate
    with the control plane. This is a YAML-formatted file for joining the worker nodes
    to your EKS cluster; we already saved this as `configmap.yaml`. Run the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'To run this command, you need the `aws-iam-authenticator` binary installed
    on your computer. To do that, follow the instructions here: [https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html](https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'This applies the ConfigMap that allows the Kubernetes cluster to communicate
    with the nodes. You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s verify that everything is running OK. Run the following command
    in the terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.8: Checking whether our nodes are accessible'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_12_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 12.8: Checking whether our nodes are accessible'
  prefs: []
  type: TYPE_NORMAL
- en: At this stage, we have a running Kubernetes cluster using EKS as the control
    plane and two worker nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Remember that your cluster resources will stay online until you delete them.
    If you plan to come back to the following exercises later, you may want to delete
    your cluster to minimize your bill. To do that, run `terraform destroy`. To get
    your cluster back online, run this exercise again.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have our cluster set up, in the next section, let's take a look
    at an efficient and flexible way to bring traffic to any application to be run
    on our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes Ingress
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the early days of the Kubernetes project, the Service object was used to
    get traffic from outside the cluster to the running Pods. You had only two options
    to get that traffic from outside in—using either a NodePort service or a LoadBalancer
    service. The latter option was preferred in public cloud provider environments
    because the cluster would automatically manage setting up security groups/firewall
    rules and to point the LoadBalancer to the correct ports on your worker nodes.
    However, there is one slight problem with that approach, especially for those
    who are just getting started with Kubernetes or those who have tight cloud budgets.
    The problem is that one LoadBalancer can only point toward a single Kubernetes
    service object.
  prefs: []
  type: TYPE_NORMAL
- en: Now, imagine that you have 100 microservices running in Kubernetes, all of which
    need to be exposed publicly. In AWS, the average cost of an ELB (a load balancer
    provided by AWS) is roughly $20 per month. So, in this scenario, you're paying
    $2,000 per month just to have the option of getting traffic into your cluster,
    and we still have not factored in the additional costs for networking.
  prefs: []
  type: TYPE_NORMAL
- en: Let's also understand another limitation of the one-to-one relationship between
    Kubernetes Service objects and AWS load balancers. Let's say that for your project,
    you need to have a path-based mapping to internal Kubernetes services from the
    same load-balancing endpoint. Let's suppose that you have a web service running
    at `api.example.io` and you want `api.example.io/users` to go to one microservice
    and `api.examples.io/weather` to go to another completely separate microservice.
    Before the arrival of Ingress, you would need to set up your own Kubernetes Service
    and do the internal path resolution to your app.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is now no longer a problem due to the advent of the Kubernetes Ingress
    resource. The Kubernetes Ingress resource is meant to operate in conjunction with
    an Ingress controller (which is an application running in your cluster watching
    the Kubernetes API server for changes to the Ingress resource). Together, these
    two components allow you to define multiple Kubernetes services, which do not
    have to be exposed externally themselves to be routed through a single load-balancing
    endpoint. Let''s examine the following diagram to understand this a bit better:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.9: Using Ingress to route traffic to our services'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_12_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 12.9: Using Ingress to route traffic to our services'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, all requests are being routed to `api.example.io` from the
    internet. One request is going to `api.example.io/a`, another is going to `api.example.io/b`,
    and the last to `api.example.io/c`. The requests are going to a single load balancer
    and a Kubernetes Service, which is controlled through a Kubernetes Ingress resource.
    This Ingress resource forwards the traffic from the single Ingress endpoint to
    the services it was configured to forward traffic to. In the following sections,
    we will set up the `ingress-nginx` Ingress controller, which is a commonly used
    open-source tool used in the Kubernetes community for ingress. Then, we will configure
    the Ingress to allow traffic into our cluster to access our highly available application.
  prefs: []
  type: TYPE_NORMAL
- en: Highly Available Applications Running on Top of Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you've had a chance to spin up an EKS cluster and learn about Ingress,
    let's introduce you to our application. We have provided an example application
    that has a flaw that prevents it from being cloud-native and really being able
    to be horizontally scaled in Kubernetes. We will deploy this application in the
    following exercise and observe its behavior. Then, in the next section, we will
    deploy a modified version of this application and observe how it is more suited
    to achieve our stated objective of being highly available.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 12.03: Deploying a Multi-Replica Non-HA Application in Kubernetes'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will deploy a version of the application that''s not horizontally
    scalable. We will try to scale it and observe the problem that prevents it from
    being scaled horizontally:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We have provided the source code for this application in the GitHub repository
    for reference. However, since our focus is on Kubernetes, we will use commands
    to fetch it directly from the repository in this exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following command to get the manifest for all of the objects required
    to run the application:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This should download the manifest to your current directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.10: Downloading the application manifest'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_12_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 12.10: Downloading the application manifest'
  prefs: []
  type: TYPE_NORMAL
- en: If you take a look at the manifest, it has a Deployment running a single replica
    of a Pod and a Service of the ClusterIP type to route traffic to it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, create a Kubernetes Deployment and Service object so that we can run
    our application:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.11: Creating the resources for our application'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_12_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 12.11: Creating the resources for our application'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need to add a Kubernetes Ingress resource to be able to access this
    website. To get started with Kubernetes Ingress, we need to run the following
    commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'These three commands will deploy the Nginx Ingress controller implementation
    for EKS. You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.12: Implementing the Ingress controllers'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_12_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 12.12: Implementing the Ingress controllers'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This command is to be run for the AWS cloud provider only. If you are running
    your cluster on another platform, you will need to find the appropriate link from
    [https://kubernetes.github.io/ingress-nginx/deploy/#aws](https://kubernetes.github.io/ingress-nginx/deploy/#aws).
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we need to create an Ingress for ourselves. In the same folder we are
    in, let''s create a file named `ingress.yaml` with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, run the Ingress using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Now, we will configure the Ingress controller such that when a request arrives
    at the load balancer that has a `Host:` header of `counter.com`, it should be
    forwarded to the `kubernetes-test-ha-application-without-redis` service on port
    `80`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, let''s find the URL that we need to access:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see an output similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.13: Checking the URL to access the Ingress load balancer endpoint'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_12_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 12.13: Checking the URL to access the Ingress load balancer endpoint'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the preceding screenshot, note that the Ingress load balancer endpoint
    that Kubernetes created for us in AWS is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Your value will likely be different from the preceding one and you should use
    the one that you get for your setup.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s access the endpoint using `curl`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'You should get a response similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run it multiple times, you''ll see that the number increases by 1 each
    time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.14: Repeatedly accessing our application'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_12_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 12.14: Repeatedly accessing our application'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s discover the problem with the application. In order to make the
    application highly available, we need to have multiple replicas of it running
    simultaneously so that we can allow at least one replica to be unavailable. This,
    in turn, enables the app to tolerate failure. To scale the app, we''re going to
    run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.15: Scaling the application Deployment'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_12_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 12.15: Scaling the application Deployment'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, try accessing the application again multiple times, as we did in *step
    7*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see a response similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.16: Repeatedly accessing the scaled application to observe the
    behavior'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_12_16.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 12.16: Repeatedly accessing the scaled application to observe the behavior'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This output may not be exactly the same for you, but if you see the number increasing
    with the first few attempts, keep accessing the application again. You will be
    able to observe the problem behavior after a few attempts.
  prefs: []
  type: TYPE_NORMAL
- en: This output highlights the problem with our application—the number isn't always
    increasing. Why is that? That is because the load balancer may pass the request
    to any one of the replicas, and the replica that receives the request returns
    a response based on its local state.
  prefs: []
  type: TYPE_NORMAL
- en: Working with Stateful Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous exercise demonstrates the challenge of working with stateful applications
    in a distributed context. As a brief overview, a stateless app is an application
    program that does not save client data generated in one session for use in the
    next session with that client. This means that in general, a stateless application
    depends entirely on the input to derive its output. Imagine a server displaying
    a static web page that does not need to change for any reason. In the real world,
    stateless applications typically need to be combined with stateful applications
    in order to create a useful experience for clients or consumers of the application.
    There are, of course, exceptions to this.
  prefs: []
  type: TYPE_NORMAL
- en: A stateful application is one whose output depends on multiple factors, such
    as user input, input from other applications, and past saved events. These factors
    are called the "state" of the application, which determines its behavior. One
    of the most important parts of creating distributed applications with multiple
    replicas is that any state that is used to generate output needs to be shared
    among all the replicas. If the different replicas of your application are working
    with different states, then your application is going to exhibit random behavior
    based on which replica your request is routed to. This effectively defeats the
    purpose of horizontally scaling an application using replicas.
  prefs: []
  type: TYPE_NORMAL
- en: In the use case from the previous exercise, for each replica to respond with
    the correct number, we need to move the storage of that number outside each replica.
    To do this, we need to modify the application. Let's think for a second about
    how this can be done. Could we communicate the numbers between the replicas using
    another request? Could we assign each replica to only respond with multiples of
    the number it is assigned? (If we had three replicas, one would only respond with
    `1`, `4`, `7`…, while another would respond with `2`, `5`, `8`…, and the last
    one would respond with `3`, `6`, `9`….) Or, might we share the number in an external
    state store, such as a database? Regardless of what we choose, the path forward
    will involve updating our running application in Kubernetes. So, we will need
    to talk briefly about a strategy to do this.
  prefs: []
  type: TYPE_NORMAL
- en: The CI/CD Pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the help of containerization technology and a container image tag revision
    policy, we can push an incremental update to our application in a fairly easy
    manner. Just as with source code and infrastructure as code, we can keep the scripts
    and Kubernetes manifests that execute steps of our build and deploy a pipeline
    versioned in a tool such as **git**. This allows us to have tremendous visibility
    into, and flexibility to control, how software updates happen in our cluster using
    approaches such as CI and CD.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the uninitiated, **CI/CD** stands for **Continuous Integration and Continuous
    Deployment/Delivery**. The CI aspect uses tooling, such as Jenkins or Concourse
    CI, to integrate new changes to our source code in a repeatable process for testing
    and assembling our code into a final artifact for deployment. The goal of CI is
    manifold, but here are a few benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: Defects in the software are found earlier in the process (if testing is adequate).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Repeatable steps create reproducible results when we are deploying to an environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visibility exists to communicate the status of a feature with stakeholders.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It encourages frequent software updates to give developers confidence that their
    new code is not breaking existing functionality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The other part, CD, is the incorporation of automated mechanisms to constantly
    deliver small updates to end-users, such as updating Deployment objects in Kubernetes
    and tracking rollout statuses. The CI/CD pipeline is the prevalent DevOps model
    today.
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, a CI/CD pipeline should be able to reliably and predictably take code
    from a developer's machine and bring it all the way to a production environment
    with as few manual interventions as possible. A CI pipeline should ideally have
    components for compilation (where necessary), testing, and final application assembly
    (in the case of a Kubernetes cluster, this is a container).
  prefs: []
  type: TYPE_NORMAL
- en: A CD pipeline should have some way of automating its interactions with an infrastructure
    to take the application revision and deploy it, along with any dependent configurations
    and one-off deployment tasks, in such a way that the desired version of the software
    becomes the running version of the software via some kind of strategy (such as
    using a Deployment object in Kubernetes). It should also include telemetry tooling
    to observe the immediate impact of the Deployment on the surrounding environment.
  prefs: []
  type: TYPE_NORMAL
- en: The problem that we observed in the previous section with our application is
    that each replica is working off of its local state to return a number via HTTP.
    To solve this problem, we propose that we should use an external state store (database)
    to manage the information (the number) shared between each replica of our application.
    We have several options of state stores to choose from. We chose Redis simply
    because it's easy to get started with and it's simple to understand. Redis is
    a high-performance key-value database, much like etcd. In our example refactor,
    we will be sharing the state between the replicas by setting a key with the `num`
    name and the value is the increasing integer value that we want to return. During
    each request, this value will be incremented and stored back into the database
    so that each replica can work off the most up-to-date information.
  prefs: []
  type: TYPE_NORMAL
- en: Every company and individual has a different process that they use to manage
    new versions of code being deployed. Therefore, we are going to use simple commands
    to perform our steps, which can be automated via Bash with the tool of your choice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 12.04: Deploying an Application with State Management'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, we will deploy a modified version of the application that
    we deployed in the previous exercise. As a reminder, this application counts how
    many times it has been accessed and returns that value in JSON format to the requestor.
    However, at the end of the previous exercise, we observed in *Figure 12.16* that
    when we scale this application horizontally with multiple replicas, we get numbers
    that are not always increasing.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We have provided the source code for this application in the GitHub repository
    for your reference. However, since our focus is on Kubernetes, we will use commands
    to directly fetch it from the repository in this exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this modified version of the application, we have refactored our code to
    add the capability of storing this increasing count in a Redis database. This
    allows us to have multiple replicas of our application, but always have the count
    increase each time we make a request to the endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In our implementation of Redis, we are not using a transaction to set the count
    after getting it. So, there is a very small chance that we are getting and acting
    on old information when we update the value set in the database, which may lead
    to unexpected results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following command to get the manifest of all the objects required for
    this application:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see a response similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.17: Downloading the manifest for the modified application'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_12_17.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 12.17: Downloading the manifest for the modified application'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you open this manifest, you will see that we have a Deployment for our app
    running three replicas: a ClusterIP Service to expose it, a Deployment for Redis
    running one replica, and another ClusterIP Service to expose Redis. We are also
    modifying the Ingress object created earlier to point to the new Service.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, it is time to deploy it on Kubernetes. We can run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see a response similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.18: Creating the resources required for our cluster'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_12_18.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 12.18: Creating the resources required for our cluster'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s see what this application gives us by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Run this command repeatedly. You should be able to see an increasing number,
    as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.19: Predictable output with consistently increasing numbers'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_12_19.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 12.19: Predictable output with consistently increasing numbers'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the preceding output, the program now outputs numbers in sequence
    because all of the replicas of our Deployment now share a single datastore responsible
    for managing the application state (Redis).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a lot of other paradigms that need to be shifted if you want to create
    a truly highly available, fault-tolerant software system, and it is beyond the
    scope of this book to explore them in detail. However, for more information, you
    can check out Packt''s book on distributed systems at this link: [https://www.packtpub.com/virtualization-and-cloud/hands-microservices-kubernetes.](https://www.packtpub.com/virtualization-and-cloud/hands-microservices-kubernetes
    )'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Again, remember that your cluster resources are still running at this point.
    Don't forget to tear down your cluster using `terraform destroy` if you expect
    to continue with the activity later.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have built our application with the ability to persist and share
    its state among different replicas, we will expand it further in the following
    activity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 12.01: Expanding the State Management of Our Application'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Right now, our application can leverage a shared Redis database running inside
    our Kubernetes cluster to manage the variable counter that we return to the user
    when it is fetched.
  prefs: []
  type: TYPE_NORMAL
- en: But let's suppose for a moment that we don't trust Kubernetes to reliably manage
    the Redis container (since it's a volatile in-memory datastore) and instead we
    want to use AWS ElastiCache to do so. Your goal in this activity is to use the
    tools we have learned in this chapter to modify our application to work with AWS
    ElastiCache.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use the following guidelines to complete this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Use Terraform to provision ElastiCache.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can find the required parameter values for provisioning ElastiCache at
    this link: [https://www.terraform.io/docs/providers/aws/r/elasticache_cluster.html#redis-instance](https://www.terraform.io/docs/providers/aws/r/elasticache_cluster.html#redis-instance).'
  prefs: []
  type: TYPE_NORMAL
- en: Change the application to connect to Redis. You will need to use an environment
    variable in your Kubernetes Deployment for that. You can find the required information
    in the `redis_address` field when you run the `terraform apply` command.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add the ElastiCache endpoint to the appropriate Kubernetes manifest environment
    variable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Roll out the new version of code onto the Kubernetes cluster using any tool
    you want.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'By the end, you should be able to observe the application responding similarly
    to what we saw in the previous exercise, but this time, it will use ElastiCache
    for its state management:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.20: Expected output of the Activity 12.01'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B14870_12_20.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 12.20: Expected output of the Activity 12.01'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution to this activity can be found at the following address: [https://packt.live/304PEoD](https://packt.live/304PEoD).
    Remember that your cluster resources will stay online until you delete them. To
    delete the cluster, you need to run `terraform destroy`.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In an earlier chapter of this book, we explored how Kubernetes works favorably
    with a declarative approach to application management; that is, you define your
    desired state and let Kubernetes take care of the rest. Throughout this chapter,
    we took a look at some tools that help us manage our cloud infrastructure in a
    similar way. We introduced Terraform as a tool that can help us manage the state
    of our infrastructure and introduced the idea of treating your infrastructure
    as code.
  prefs: []
  type: TYPE_NORMAL
- en: We then created a mostly secure, production-ready Kubernetes cluster using Terraform
    in Amazon EKS. We took a look at the Ingress object and learned about the major
    motivations for using it, as well as the various advantages that it provides.
    Then, we deployed two versions of an application on a highly available Kubernetes
    cluster and explored some concepts that allow us to improve at horizontally scaling
    stateful applications. This gave us a glimpse of the challenges that come with
    running stateful applications, and we will explore some more ways of dealing with
    them in *Chapter 14*, *Running Stateful Components in Kubernetes*.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we're going to take a look at continuing our production
    readiness by further securing our cluster.
  prefs: []
  type: TYPE_NORMAL
