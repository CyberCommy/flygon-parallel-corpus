- en: Monitoring with the ELK Stack
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Monitoring is an essential part of any environment, whether it is production,
    QA, or development; the **Elastic Stack** (**ELK Stack**) helps simplify this
    task by allowing logs, metrics, and events from different sources to be aggregated
    in a single indexable location: Elasticsearch.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'The ELK Stack is a collection of three different pieces of software:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logstash
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kibana
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will explain the role of each component.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will cover the following topics:Defining the main functionality
    of Elasticsearch
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the concept of centralized logs
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How Kibana helps bring together the other components
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here''s the list of technical requirements for this chapter:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch product page: [https://www.elastic.co/products/elasticsearch](https://www.elastic.co/products/elasticsearch)
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of Logstash: [https://www.elastic.co/products/logstash](https://www.elastic.co/products/logstash)
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Available input plugins for Logstash: [https://www.elastic.co/guide/en/logstash/current/input-plugins.html](https://www.elastic.co/guide/en/logstash/current/input-plugins.html)
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grok pattern matching: [https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html](https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kibana user guide: [https://www.elastic.co/guide/en/kibana/current/index.html](https://www.elastic.co/guide/en/kibana/current/index.html)
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the need for monitoring
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine that you're asked to provide historical data to the CIO, as an ongoing
    project requires information on how much CPU the entire ecosystem is using on
    average, but the business never invested the time to implement a good monitoring
    system. Therefore, your only option is to log into each system and run local commands,
    record results into a spreadsheet, do some math to obtain the average results,
    and, after all this, you realize that the data is no longer valid and you have
    to go through all of this again. This is precisely why we have monitoring systems
    such as Elasticsearch. The same process could've taken you a couple of minutes.
    Not just that, you would be getting accurate data and real-time reports. Let's
    find out more about what monitoring is, and why you, as an architect, should consider
    it to be the best thing ever to exist.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring refers to the process of taking raw data from any given environment,
    aggregating it, storing it, and analyzing it in a way that is understandable.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: All environments should have some form of monitoring in place, from a simple
    log file for keeping track of failed logins, to a more robust system that is in
    charge of analyzing data from thousands of hosts. Monitoring data allows system
    administrators to detect problems before they occur, and allows architects to
    make decisions for future or ongoing projects based on data.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: You may recall from the [Chapter 1](ef6464b0-95db-45b2-95ce-4f9067e2c6c8.xhtml), *Introduction
    to Design Methodology*, that we spoke about how asking the right questions can
    help design a better solution, and, at the same time, give the right answers;
    for example, it could help make sizing decisions based on historical usage data.
    Providing usage data to architects helps size the solution correctly. They not
    only leverage future usage statistics, but also past instances, where usage spikes
    have been recorded during peak times, such as during weekends.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try to condense why we need monitoring into four main areas:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Make decisions through historical data
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Proactively detect problems
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand environment performance
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plan for budget
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decisions made through historical data
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Monitoring gives the ability to go back in time and analyze usage trends to
    help identify areas of opportunity. For example, in the scenario presented in
    the [Chapter 1](ef6464b0-95db-45b2-95ce-4f9067e2c6c8.xhtml), *Introduction to
    Design Methodology*, where the customer needed a web server solution able to sustain
    10,000 hits per second. You, as an architect, requested access to usage data from
    their existing solution, and, after looking at their usage trends, you determined
    that usage increased tenfold during the first week of each month.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 监控能够让我们回到过去并分析使用趋势，以帮助识别机会领域。例如，在[第1章](ef6464b0-95db-45b2-95ce-4f9067e2c6c8.xhtml)中提到的情景，客户需要一个能够每秒维持10,000次点击量的Web服务器解决方案。作为架构师，你请求访问他们现有解决方案的使用数据，并在查看他们的使用趋势后，确定每个月的第一周使用量增加了十倍。
- en: While users might not complain about problems during these days, you should
    take into account that this high usage tends to leverage resources during this
    times. The data taken from the monitoring system might lead to a decision where
    either more resources need to be destined to the server (for example, more CPU
    and RAM ) than previously calculated, or more servers need to be added to the
    cluster (if possible).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然用户在这些日子里可能不会抱怨问题，但你应该考虑到在这些时间内高使用量往往会消耗资源。从监控系统中获取的数据可能会导致一个决定，即服务器需要分配更多资源（例如更多的CPU和RAM）比之前计算的，或者需要向集群中添加更多的服务器（如果可能的话）。
- en: Without this data, no one would ever know that more resources are needed due
    to spiking. The ability to discern normal usage from spikes helps make the right
    choices when designing and sizing a solution.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 没有这些数据，没有人会知道由于激增而需要更多资源。能够区分正常使用和激增有助于在设计和规模化解决方案时做出正确选择。
- en: From the same scenario, we could conclude from the historical data usage that
    the current solution had been sustaining 10,000 hits per second for the last several
    months. This might mean that the customers were able to achieve the desired performance
    all along, but in reality what they needed was a solution that could handle the
    usage spikes, as mentioned earlier.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 从同样的情景中，我们可以从历史数据使用中得出结论，即当前解决方案在过去几个月一直能够维持每秒10,000次的点击量。这可能意味着客户一直能够实现期望的性能，但实际上他们需要的是一个能够处理使用激增的解决方案，正如前面提到的。
- en: Proactively detect problems
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主动检测问题
- en: Imagine that you're almost ready to go home for the day when suddenly someone
    reports that a database server is unable to receive connections. You log into
    the server and notice that the problem is a lot worse than initially reported.
    The disks where the data from the database resides are now all reported as failed.
    You look closer at the logs on the system and notice that disk errors had been
    reported for the last four months; however, as a robust monitoring system was
    not in place, no one ever knew that the errors were there. Now, the data is lost,
    and you have to retrieve an old backup that takes several hours to go back to
    production.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，当你准备下班时，突然有人报告数据库服务器无法接收连接。你登录服务器后发现问题比最初报告的严重得多。数据库所在的磁盘现在都被报告为损坏。你仔细查看系统日志，发现过去四个月一直报告磁盘错误；然而，由于没有健壮的监控系统，没有人知道这些错误。现在数据丢失了，你不得不恢复一个旧的备份，需要数小时才能恢复到生产状态。
- en: Unfortunately, this situation is not uncommon, and most of the time, IT works
    reactively, meaning that if something breaks, someone reports the broken something,
    and someone then goes and fix the broken something. This could've been avoided
    altogether if a monitoring system had been implemented and configured to report
    errors. The disks could have been replaced before they catastrophically failed.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这种情况并不罕见，大多数时候，IT工作是被动的，这意味着如果出现问题，有人会报告问题，然后有人会去修复问题。如果实施了监控系统并配置为报告错误，这种情况本来可以完全避免。在磁盘彻底损坏之前就可以更换磁盘。
- en: Being able to proactively detect problems before they occur is, in our opinion,
    one of the most critical aspects of a monitoring system. Predicting where a problem
    might occur before it happens helps decrease downtime by allowing actions to be
    taken. For example, in the previous scenario, replacing the drives could have
    prevented data loss. Predicting changes also helps to decrease operational costs
    by preventing business losses due to downtime or failures, and by increasing production
    (or uptime).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 能够在问题发生之前主动检测问题，在我们看来，是监控系统中最关键的方面之一。在问题发生之前预测问题可能发生的地方有助于通过采取行动来减少停机时间。例如，在前面的情景中，更换磁盘可以防止数据丢失。预测变化还有助于通过防止因停机或故障而导致的业务损失，以及增加生产（或正常运行时间）来降低运营成本。
- en: Understand environment performance
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解环境性能
- en: In [Chapter 5](b140a44b-3594-4c0d-ad7c-03de29a31815.xhtml), *Analyzing Performance
    in a Gluster Syste*m, we went through performance testing of a GlusterFS implementation.
    With monitoring systems, the process of obtaining a baseline for performance can
    be streamlined by aggregating historical data and averaging statistics.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](b140a44b-3594-4c0d-ad7c-03de29a31815.xhtml)中，*在Gluster系统中分析性能*，我们对GlusterFS实施进行了性能测试。通过监控系统，可以通过汇总历史数据和平均统计数据来简化性能基线的获取过程。
- en: By looking at historical data, we can see the average performance of any given
    system through a certain amount of time, allowing an architect to define what
    is normal and what is not. By obtaining a baseline, we can understand on a deeper
    level how the environment behaves throughout the day, week, or even month. For
    example, we can identify that storage servers have a constant throughput of about
    200 MB/s through the day, and when users log in during the first hours of the
    day, throughput spikes to 300 MB/s. A spike of 100 MB/s might seem like a problem
    at first, but, looking at the data, this appears to be a trend, and is standard
    behavior.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: With this information, we know that the baseline is around 200 MB/s with spikes
    of 300 MB/s. When the solution is benchmarked, it is expected to perform to this
    spec. If we obtain results below this number, we know that there is a problem,
    and an investigation is required to determine the cause of the poor performance.
    This might be either a redesign of the solution, or an actual problem with the
    configuration. On the other hand, a high number indicates that the solution can
    perform to spec even under load spikes.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Without this data, we wouldn't know what erratic behavior looks like, be able
    to confirm whether or not this is an actual problem, or see what is normal for
    the environment. Knowing the performance and usage of a solution can help spot
    problems where there might not seem to be one. For example, consider a situation
    with the previous numbers, where users interact normally with the storage server
    and have average response times; however, from the monitoring data, we observe
    that even with the regular user load we get a throughput of only 50 MB/s. From
    the user's perspective, everything seems fine, but when asked, they do report
    that even when response times are good, transfers are taking longer than usual,
    and upon further investigation a problem is found, with one of the nodes requiring
    maintenance.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding example, by merely looking at the performance data, an instance
    where the solution was under-performing was identified, and actions were taken
    that avoided downtime and loss to the business. This is the power of understanding
    the environment through the use of data.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Plan for budget
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data usage trends allow for more granular control of budget planning, as knowing
    how much storage space is required can help avoid situations where not enough
    space has been provisioned.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: In the [Chapter 1](ef6464b0-95db-45b2-95ce-4f9067e2c6c8.xhtml), *Introduction
    to Design Methodology*, we spoke about the procurement process of businesses,
    and how trying to stick to the timelines is essential as this varies from company
    to company. Understanding space requirements and usage is crucial for this process,
    since it can help predict, for example, when the solution will run out of space
    and can help make decisions around acquiring new storage space.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Knowing if the business consumes X amount of storage per day (also known as
    the daily change rate) through the use of a monitoring system allows system administrators
    and architects to predict how long the business can run with the space that is
    currently available. This will also allow them to predict when the solution will
    run out of space so that they can act before it runs out of storage—which is a
    situation that every IT department should avoid.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Understanding resource utilization is crucial to any business, as it prevents
    unnecessary equipment acquisition. Using data to decide whether more resources
    should be added to the existing environment or not reduces costs by choosing the
    right amount of equipment to be added in the case of upgrades. It's not the same
    when the application is under-performing due to a lack of resources (or outdated
    hardware) rather than having data that confirms that the current environment is
    working as expected and still has some room for growth.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Today, the need for monitoring is more crucial than ever. With the almost exponential
    growth of data within IT environments, being able to predict behaviors and act
    proactively can be achieved through data-driven decisions, which is only possible
    through monitoring systems, such as the ELK Stack.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Centralized logs
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before jumping deeper into what makes the ELK Stack, let's explore the concept
    of centralized logs.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Imagine the following scenario; there seems to be a security breach in the environment,
    and some strange looking files have been spotted in some servers. Looking at the
    `/var/log/secure` file, you find root logins from several addresses, and you want
    to know which systems have been affected. There's just one problem—the environment
    has 5,000+ Linux servers, and you have to log into each of the systems and look
    at the logs. It might take about a minute to grep each host; that's 83+ hours
    straight looking at system logs.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: This problem of having to go to each node can be solved by aggregating and having
    the logs in a centralized location. While the rest of the industry seems to be
    going the route of de-centralizing services, having all of the environment's log
    in a single location can help simplify tasks, such as investigating events that
    might have affected multiple systems. Having a single location to look for decreases
    the amount of time required to troubleshoot, and at the same time allows administrators
    to look for problems within the environment more effectively.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 'A centralized logging architecture looks like this:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/56d1586c-58b4-46c8-be8a-551323da1d85.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
- en: Logs from multiple applications are sent to a log parser (such as Logstash)
    and later moved to an indexer (such as Elasticsearch). Each host has an agent
    that is in charge of shipping the logs to the parser.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: The parser's job is to transform the data for easy indexing, later shipping
    the data to the indexer.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: In the next segment, we will look at the components that make up the ELK Stack.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch overview
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we will dive deep into the components of the ELK Stack, and we will start
    with the most important component: Elasticsearch.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch is based on an Apache project named Lucene. Its role is to index
    data and store it for later retrieval. Elasticsearch receives data from different
    sources and stores it in a centralized location, or multiple nodes if they are
    set up as a cluster. For this setup, we'll be using Logstash as a data source;
    however, Elasticsearch can receive data directly from Beats, which we will discuss
    later on. At its core, Elasticsearch is an analytics and search engine capable
    of retrieving data very quickly; since data is indexed once it is stored, Elasticsearch
    stores the data as a JSON document.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 'A couple of things that define Elasticsearch are as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Fast
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scalable
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Highly available
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fast
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Searches are almost real-time; what this means is, when you input a search term,
    Elasticsearch returns results almost immediately. This is thanks to the indexes
    and data being stored as JSON.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Scalable
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scaling an Elasticsearch cluster can be done quickly by simply adding more nodes
    to the cluster.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Highly available
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When configured as a cluster, Elasticsearch allocates shards between multiple
    nodes, creating replicas of the shards in case one or more nodes fail.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: A shard is a fragment of the JSON document. Elasticsearch creates replicas of
    the shards and allocates them on the cluster nodes. This allows the cluster to
    sustain a catastrophic failure, as data is still present as a replica.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Logstash
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of the time, data, such as log files, is designed so that humans can easily
    understand what the events mean. This type of data is unstructured, as machines
    can't easily index the events since they don't follow the same structure or format.
    Take system logs and Apache, for example. While each log provides different types
    of events, none follow the same format or structure, and, for an indexing system,
    this becomes a problem. That's where Logstash comes in.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Logstash data processing parser is capable of receiving data from several sources
    simultaneously, and then transforming the data by parsing it into a structured
    form, and later shipping it to Elasticsearch as indexed, easily-searchable data.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: One of the main features of Logstash is the vast amount of plugins available
    for filters such as Grok, allowing greater flexibility on what type of data can
    be parsed and indexed.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Grok
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Grok is a plugin available in Logstash; it takes unstructured data from sources
    such as system logs, MySQL, Apache, and other webserver logs and transforms them
    into structured and queryable data for easy ingestion into Elasticsearch.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 'Grok combines text patterns into something that matches the logs, for example,
    numbers or IP address. The pattern for this is as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, `SYNTAX` is the name of the pattern that matches the text and SEMANTIC
    is the identifier given to the segment of text.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of an event for HTTP would be as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'One pattern match for this could be the following:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'So, by putting it all together in an actual filter configuration, it looks
    like this:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Custom patterns
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When running a custom application, Logstash won't have the correct pattern to
    match the syntaxes and semantics. Logstash allows the creation of custom patterns
    that can match custom data. The same logic from the previous example can be used
    to match data.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Kibana brings everything together
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While Elasticsearch is the heavy lifting part of the ELK Stack, and Logstash
    is the parsing and processing bit, Kibana is what aggregates everything else together.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: The ability to visualize the data allows users to give meaning to their data.
    By just looking at the raw data, it is difficult to make any sense of it. Kibana
    visualizes the data that is stored within Elasticsearch through graphs, maps,
    and other methods of shaping data.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a quick glance at Kibana''s interface taken from the live
    demo:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f1cac6cf-9ffd-4ac3-a0e0-7c0dfbc8cc70.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
- en: Kibana Dashboard
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: We can see how easy is to interpret data with multiple modules showing different
    metrics.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Kibana enables easy understanding of large datasets. Being a browser-based application,
    it can be accessed from anywhere. This also allows dashboards and reports to be
    easily shared with others. It can be installed alongside Elasticsearch; however,
    for larger deployments, it is a good practice to allocate a host to Kibana. Also,
    Kibana runs on Node.js, so it can be installed on pretty much every system that
    can run Node.js, from all of the flavors of Linux to Windows and MacOS.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the need for monitoring, and learned the process
    of acquiring data from an environment, aggregating it, and storing it so that
    it can be retrieved later for further analysis. Being able to shape data and understand
    how the environment behaves by just glancing at the data helps to enhance operational
    efficiency.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring allows us to proactively detect problems before they happen or become
    a bigger problem. This is done by looking at trends, and is by far one of the
    most crucial reasons to implement and design a monitoring solution. We also spoke
    about being able to act proactively, and how that can help decrease downtime and
    wasting money on problems; something that can be achieved by giving shape to data.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Performance is also an area that benefits from data analysis. You may recall
    from previous chapters that being able to baseline and measure performance enables
    granular control while designing a solution. Having historical data to refer back
    to can help make decisions that would affect a design performance-wise, while
    at the same time allowing us to plan for the budget based on real data taken from
    a running environment.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 性能也是受益于数据分析的领域。您可能还记得之前章节提到的，能够基线和测量性能使得在设计解决方案时能够进行细粒度控制。拥有历史数据可以帮助做出影响设计性能的决策，同时还可以根据来自运行环境的真实数据来规划预算。
- en: We went through the main reasons why having a centralized logging system can
    help simplify administration tasks; instead of connecting to each system in the
    environment, looking at all of the logs from a single location saves time and
    allows quicker, more efficient investigations.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了拥有集中式日志系统的主要原因，它可以帮助简化管理任务；而不是连接到环境中的每个系统，从单个位置查看所有日志可以节省时间，并且可以进行更快，更高效的调查。
- en: We also went through an overview of each of the components that comprise the
    ELK Stack. Elasticsearch is the main component, where the storing and analysis
    of data happens. We noted that it is very fast, as data is stored as JSON documents;
    that the solution is scalable, as nodes can be easily added; and that it is highly
    available, as data is spread across the nodes.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还概述了ELK Stack的每个组件。 Elasticsearch是主要组件，用于存储和分析数据。我们注意到它非常快，因为数据存储为JSON文档；解决方案可扩展，因为可以轻松添加节点；并且高度可用，因为数据分布在节点之间。
- en: Logstash provides data transformation and filtering through plugins such as
    GROK, where it matches a `SYNTAX` with a `SEMANTIC`, for example, an IP with a
    client.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Logstash通过插件提供数据转换和过滤，例如GROK，它可以将`SYNTAX`与`SEMANTIC`进行匹配，例如将IP与客户端进行匹配。
- en: Finally, we looked at how Kibana connects all of the other components by allowing
    the data to be visualized and analyzed through comprehensive graphics.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们看了Kibana如何通过允许数据可视化和通过全面的图形进行分析来连接所有其他组件。
- en: In the next chapter, we will jump into the requirements for each of the components.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍每个组件的要求。
- en: Questions
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What is monitoring?
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 监控是什么？
- en: How can monitoring help make business decisions?
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 监控如何帮助做出业务决策？
- en: How can problems be proactively detected?
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何主动检测问题？
- en: How can monitoring allow for performance baselining?
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 监控如何允许性能基线？
- en: How can monitoring help identify erratic behaviors?
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 监控如何帮助识别异常行为？
- en: What is the main need for centralized logs?
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 集中日志的主要需求是什么？
- en: What is Elasticsearch?
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Elasticsearch是什么？
- en: In what format does Elasticsearch store data?
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Elasticsearch以什么格式存储数据？
- en: What is Logstash?
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Logstash是什么？
- en: What is Kibana?
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kibana是什么？
- en: Further reading
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '*Hands-on Big Data Modeling* by *James Lee, Tao Wei: *[https://www.packtpub.com/big-data-and-business-intelligence/hands-big-data-modeling](https://www.packtpub.com/big-data-and-business-intelligence/hands-big-data-modeling)'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*James Lee, Tao Wei的《实战大数据建模》：*[https://www.packtpub.com/big-data-and-business-intelligence/hands-big-data-modeling](https://www.packtpub.com/big-data-and-business-intelligence/hands-big-data-modeling)'
- en: '*Practical Data Analysis – Second Edition* by *Hector Cuesta, Dr. Sampath Kumar: *[https://www.packtpub.com/big-data-and-business-intelligence/practical-data-analysis-second-edition](https://www.packtpub.com/big-data-and-business-intelligence/practical-data-analysis-second-edition)'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Hector Cuesta, Dr. Sampath Kumar的《实用数据分析-第二版》：*[https://www.packtpub.com/big-data-and-business-intelligence/practical-data-analysis-second-edition](https://www.packtpub.com/big-data-and-business-intelligence/practical-data-analysis-second-edition)'
