- en: Algorithms for Natural Language Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduces algorithms for **natural language processing** (**NLP**).
    This chapter proceeds from the theoretical to the practical in a progressive manner.
    It will first introduce the fundamentals of NLP, followed by the basic algorithms.
    Then, it will look at one of the most popular neural networks that is widely used
    to design and implement solutions for important use cases for textual data. We
    will then look at the limitations of NLP before finally learning how we can use
    NLP to train a machine learning model that can predict the polarity of movie reviews.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will consist of the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing NLP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bag-of-words-based (BoW-based) NLP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to word embedding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use of recurrent neural networks for NLP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using NLP for sentiment analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Case study: movie review sentiment analysis'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will understand the basic techniques that are
    used for NLP. You should be able to understand how NLP can be used to solve some
    interesting real-world problems.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start with the basic concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'NLP is used to investigate methodologies to formalize and formulate the interactions
    between computers and human (natural) languages. NLP is a comprehensive subject,
    and involves using computer linguistics algorithms and human–computer interaction
    technologies and methodologies to process complex unstructured data. NLP can be
    used for a variety of cases, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Topic identification**:To discover topics in a text repository and classify
    the documents in the repository according to the discovered topics'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentiment analysis**: To classify the text according to the positive or negative
    sentiments that it contains'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Machine translation**: To translate the text from one spoken human language
    to another'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text to speech**: To convert spoken words into text'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Subjective interpretation**: To intelligently interpret a question and answer
    it using the information available'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Entity recognition**:To identify entities (such as a person, place, or thing)
    from text'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fake news detection**: To flag fake news based on the content'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start by looking at some of the terminology that is used when discussing
    NLP.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding NLP terminology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: NLP is a comprehensive subject. In the literature surrounding a certain field,
    we will observe that, sometimes, different terms are  used to specify the same
    thing. We will start by looking at some of the basic terminology related to NLP.
    Let's start with normalization, which is one of the basic kinds of NLP processing,
    usually performed on the input data.
  prefs: []
  type: TYPE_NORMAL
- en: Normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Normalization is performed on the input text data to improve its quality in
    the context of training a machine learning model. Normalization usually involves
    the following processing steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Converting all text to uppercase or lowercase
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removing punctuation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removing numbers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that although the preceding processing steps are typically needed, the
    actual processing steps depend on the problem that we want to solve. They will
    vary from use case to use case—for example, if the numbers in the text represent
    something that may have some value in the context of the problem that we are trying
    to solve, then we may not need to remove the numbers from the text in the normalization
    phase.
  prefs: []
  type: TYPE_NORMAL
- en: Corpus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The group of input documents that we are using to solve the problem in question
    is called the **corpus**. The corpus acts as the input data for the NLP problem.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we are working with NLP, the first job is to divide the text into a list
    of tokens. This process is called **tokenization**. The granularity of the resulting
    tokens will vary based on the objective—for example, each token can consist of
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A word
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A combination of words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A sentence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A paragraph
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Named entity recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In NLP, there are many use cases where we need to identify certain words and
    numbers from unstructured data as belonging to predefined categories, such as
    phone numbers, postal codes, names, places, or countries. This is used to provide
    a structure to the unstructured data. This process is called **named entity recognition**
    (**NER**).
  prefs: []
  type: TYPE_NORMAL
- en: Stopwords
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After word-level tokenization, we have a list of words that are used in the
    text. Some of these words are common words that are expected to appear in almost
    every document. These words do not provide any additional insight into the documents
    that they appear in. These words are called **stopwords**. They are usually removed
    in the data-processing phase. Some examples of stopwords are *was*, *we,* and
    *the*.
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sentimental analysis, or opinion mining, is the process of extracting positive
    or negative sentiments from the text.
  prefs: []
  type: TYPE_NORMAL
- en: Stemming and lemmatization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In textual data, most words are likely to be present in slightly different forms.
    Reducing each word to its origin or stem in a family of words is called **stemming**.
    It is used to group words based on their similar meanings to reduce the total
    number of words that need to be analyzed. Essentially, stemming reduces the overall
    conditionality of the problem.
  prefs: []
  type: TYPE_NORMAL
- en: For example, {use, used, using, uses} => use.
  prefs: []
  type: TYPE_NORMAL
- en: The most common algorithm for stemming English is the Porter algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Stemming is a crude process that can result in chopping off the ends of words.
    This may result in words that are misspelled. For many use cases, each word is
    just an identifier of a level in our problem space, and misspelled words do not
    matter. If correctly spelled words are required, then lemmatization should be
    used instead of stemming.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms lack common sense. For the human brain, treating similar words the
    same is straightforward. For an algorithm, we have to guide it and provide the
    grouping criteria.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fundamentally, there are three different ways of implementing NLP. These three
    techniques, which are different in terms of sophistication, are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bag-of-words**-based (Bo**W**-based) NLP'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Traditional NLP classifiers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using deep learning for NLP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NLTK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Natural Language Toolkit** (**NLTK**) is the most widely utilized package
    for handling NLP tasks in Python. NLTK is one of the oldest and most popular Python
    libraries used for NLP. NLTK is great because it basically provides a jumpstart
    to building any NLP process by giving you the basic tools that you can then chain
    together to accomplish your goal rather than building all those tools from scratch.
    A lot of tools are packaged into NLTK, and in the next section, we will download
    the package and explore some of those tools.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at BoW-based NLP.
  prefs: []
  type: TYPE_NORMAL
- en: BoW-based NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The representation of input text as a bag of tokens is called **BoW-based processing**.
    The drawback of using BoW is that we discard most of the grammar and tokenization,
    which sometimes results in losing the context of the words. In the BoW approach,
    we first quantify the importance of each word in the context of each document
    that we want to analyze.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fundamentally, there are three different ways of quantifying the importance
    of the words in the context of each document:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Binary**: A feature will have a value of 1 if the word appears in the text
    or 0 otherwise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Count**: A feature will have the number of times the word appears in the
    text as its value or 0 otherwise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Term frequency/Inverse document frequency**: The value of the feature will
    be a ratio of how unique a word is in a single document to how unique it is in
    the entire corpus of documents. Obviously, for common words such as the, in, and
    so on (known as stop words), the **term frequency–inverse document frequency**
    (**TF-IDF**) score will be low. For more unique words—for instance, domain-specific
    terms—the score will be higher.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that by using BoW, we are throwing away information—namely, the order of
    the words in our text. This often works, but may lead to reduced accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at a specific example. We will train a model that can classify the
    reviews of a restaurant as negative or positive. The input file is a strutted
    file where the reviews will be classified as positive or negative.
  prefs: []
  type: TYPE_NORMAL
- en: For this, let's first process the input data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The processing steps are defined in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/70cf5585-8206-45a8-9b86-8aa31f914b37.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s implement this processing pipeline by going through the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the packages that we need:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we import the dataset from a `CSV` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/99fd1e80-976c-43b6-bf61-3e878651d157.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we clean the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s define the features (represented by `y`) and the label (represented
    by `X`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s divide the data into testing and training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'For training the model, we are using the naive Bayes algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s predict the test set results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The confusion matrix looks like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/d3c55c12-09ac-460b-a3c2-915340b89103.png)'
  prefs: []
  type: TYPE_IMG
- en: Looking at the confusion matrix, we can estimate the misclassification.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to word embedding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the preceding section, we studied how we can perform NLP by using BoW as
    the abstraction for the input text data. One of the major advancements in NLP
    is our ability to create a meaningful numeric representation of words in the form
    of dense vectors. This technique is called word embedding. Yoshua Bengio first
    introduced the term in his paper *A Neural Probabilistic Language Model*. Each
    word in an NLP problem can be thought of as a categorical object. Mapping each
    of the words to a list of numbers represented as a vector is called word embedding.
    In other words, the methodologies that are used to convert words into real numbers
    are called word embedding. A differentiating feature of embedding is that it uses
    a dense vector, instead of using traditional approaches that use sparse matrix
    vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are basically two problems with using BoW for NLP:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Loss of semantic context**: When we tokenize the data, its context is lost.
    A word may have different meanings based on where it is used in the sentence;
    this becomes even more important when interpreting complex human expressions,
    such as humor or satire.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sparse input**: When we tokenize, each word becomes a feature. As we saw
    in the preceding example, each word is a feature. It results in sparse data structures.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The neighborhood of a word
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A key insight into how to present textual data (specifically, individual words,
    or lexemes) to an algorithm comes from linguistics. In word embedding, we pay
    attention to the neighborhood of each word and use it to establish its meaning
    and importance. Neighbourhood of a word is the set of words surrounds a particular
    word. The context of a word is determined by its neighborhood.
  prefs: []
  type: TYPE_NORMAL
- en: Note that in BoW, a word loses its context, as its context is from the neighborhood
    that it is in.
  prefs: []
  type: TYPE_NORMAL
- en: Properties of word embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Good word embeddings exhibit the following four properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '**They are dense**: In fact, embeddings are essentially factor models. As such,
    each component of the embedding vector represents a quantity of a (latent) feature.
    We typically do not know what that feature represents; however, we will have very
    few—if any—zeros that will cause a sparse input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**They are low dimensional**: An embedding has a predefined dimensionality
    (chosen as a hyperparameter). We saw earlier that in the BoW representation we
    needed |*V*| inputs for each word, so that the total size of the input was |*V*|
    * *n* where *n* is the number of words we use as input. With word embeddings,
    our input size will be *d* * *n*, where *d* is typically between 50 and 300\.
    Considering the fact that large text corpora are often much larger than 300 words,
    this means that we have large savings in input size, which we saw can lead to
    better accuracy for a smaller total number of data instances.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**They embed domain semantics**: This property is probably the most surprising,
    but also the most useful. When properly trained, embeddings learn about the meaning
    of their domain.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generalize easily**: Finally, web embedding is capable of picking up generalized
    abstract patterns—for example, we can train on (the embeddings of) cat, deer,
    dog, and so on, and the model will understand that we mean animals. Note that
    the model was never trained for sheep, and yet the model will still correctly
    classify it. By using embedding, we can expect to receive the correct answer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let us explore, how we can use RNNs for Natural Language Processing.
  prefs: []
  type: TYPE_NORMAL
- en: Using RNNs for NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An RNN is a traditional feed-forward network with feedback. A simple way of
    thinking about an RNN is that it is a neural network with states. RNNs are used
    with any type of data for generating and predicting various sequences of data.
    Training an RNN model is about formulating these sequences of data. RNNs can be
    used for text data as sentences are just sequences of words. When we use RNNs
    for NLP, we can use them for the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Predicting the next word when typing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Generating new text, following the style already used in the text:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](assets/8aba997b-e588-4c5d-8307-d33b6ad5c15b.png)'
  prefs: []
  type: TYPE_IMG
- en: Remember the combination of words that resulted in their correct prediction?
    The learning process of RNNs is based on the text that is found in the corpus.
    They are trained by reducing the error between the predicted next word and the
    actual next word.
  prefs: []
  type: TYPE_NORMAL
- en: Using NLP for sentiment analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The approach presented in this section is based on the use case of classifying
    a high rate of incoming stream tweets. The task at hand is to extract the embedded
    sentiments within the tweets about a chosen topic. The sentiment classification
    quantifies the polarity in each tweet in real time and then aggregate the total
    sentiments from all tweets to capture the overall sentiments about the chosen
    topic. To face the challenges posed by the content and behavior of Twitter stream
    data and perform the real-time analytics efficiently, we use NLP by using a trained
    classifier. The trained classifier is then plugged into the Twitter stream to
    determine the polarity of each tweet (positive, negative, or neutral), followed
    by the aggregation and determination of the overall polarity of all tweets about
    a certain topic. Let's see how this is done step by step.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we have to train the classifier. In order to train the classifier, we
    needed an already-prepared dataset that has historical Twitter data and follows
    the patterns and trends of the real-time data. Therefore, we used a dataset from
    the website [www.sentiment140.com](http://www.sentiment140.com/), which comes
    with a human-labeled corpus (a large collection of texts upon which the analysis
    is based) with over 1.6 million tweets. The tweets within this dataset have been
    labeled with one of three polarities: zero for negative, two for neutral, and
    four for positive. In addition to the tweet text, the corpus provides the tweet
    ID, date, flag, and the user who tweeted. Now let''s see each of the operations
    that are performed on the live tweet before it reaches the *trained* classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: Tweets are first split into individual words called tokens (tokenization).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The output from tokenization creates a BoW, which is a collection of individual
    words in the text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These tweets are further filtered by removing numbers, punctuations, and stop
    words (stop word removal). Stop words are words that are extremely common, such
    as *is*, *am*, *are*, and *the*. As they hold no additional information, these
    words are removed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Additionally, nonalphabetical characters, such as *#**@* and numbers, are removed
    using pattern matching, as they hold no relevance in the case of sentiment analysis.
    Regular expressions are used to match alphabetical characters only and the rest
    are ignored. This helps to reduce the clutter from the Twitter stream.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The outcomes of the prior phase are taken to the stemming phase. In this phase,
    the derived words are reduced to their roots—for example, a word like *fish* has
    the same roots as *fishing* and *fishes*. For this, we use the library of standard
    NLP, which provides various algorithms, such as Porter stemming.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the data is processed, it is converted into a structure called a **term
    document matrix** (**TDM**). The TDM represents the term and frequency of each
    work in the filtered corpus.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the TDM, the tweet reaches the trained classifier (as it is trained, it
    can process the tweet), which calculates the **sentimental polarity importance**
    (**SPI**) of each word which is a number from -5 to +5\. The positive or negative
    sign specifies the type of emotions represented by that particular word, and its
    magnitude represents the strength of sentiment. This means that the tweet can
    be classified as either positive or negative (refer to the following image). Once
    we calculate the polarity of the individual tweets, we sum their overall SPI to
    find the aggregated sentiment of the source—for example, an overall polarity greater
    than one indicates that the aggregated sentiment of the tweets in our observed
    period of time is positive.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To retrieve the real-time raw tweets, we use the Scala library *Twitter4J*,
    a Java library that provides a package for a real-time Twitter streaming API.
    The API requires the user to register a developer account with Twitter and fill
    in some authentication parameters. This API allows you to either get random tweets
    or filter tweets using chosen keywords. We used filters to retrieve tweets related
    to our chosen keywords.
  prefs: []
  type: TYPE_NORMAL
- en: 'The overall architecture is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/3dc16321-97c8-4e5e-b815-3584e3e61f97.png)'
  prefs: []
  type: TYPE_IMG
- en: The sentiment analysis have various applications. It can be applied to classify
    the feedback from customers. Social media polarity analysis can be used by governments
    to find the effectiveness of their policies. It can also quantify the success
    of various advertisement campaigns.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn how we can actually apply sentiment analysis
    to predict the sentiments of movie reviews.
  prefs: []
  type: TYPE_NORMAL
- en: 'Case study: movie review sentiment analysis'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s use NLP to conduct a movie review sentiment analysis. For this, we will
    use some open source movie review data available at [http://www.cs.cornell.edu/people/pabo/movie-review-data/](http://www.cs.cornell.edu/people/pabo/movie-review-data/):'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will import the dataset that contains the movie reviews:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Now, let us load the movies' data and print the first few rows to observe its
    structure.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/d613c48f-67c9-44c0-97b0-db32f368deae.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the dataset has `2000` movie reviews. Out of these, half are negative
    and half are positive.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's start preparing the dataset for training the model. First, let us
    drop any missing values that are in the data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we need to remove the whitespaces as well. Whitespaces are not null but
    need to be removed. For this, we need to iterate over each row in the input `DataFrame`.
    We will use `.itertuples()` to access every field:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Note that we have used `i`, `lb`, and `rv` to the index, label, and review columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s split the data into test and train datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to specify the features and the label and then split the
    data into the train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now we have the testing and training datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s divide the sets into train and test:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Note that we are using `tfidf` to quantify the importance of a datapoint in
    a collection.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's train the model using the naive Bayes algorithm, and then test the
    trained model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s follow these steps to train the model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s train the model using the testing and training datasets that we
    created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us run the predictions and analyze the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Let us now look into the performance of our model by printing the confusion
    matrix. We will also look at *precision*, *recall*, *f1-score*, and *accuracy*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/774aa10e-3479-403d-a3a5-949c7179ab32.png)'
  prefs: []
  type: TYPE_IMG
- en: These performance metrics give us a measure of the quality of the predictions.
    With a 0.78 accuracy, now we have successfully trained a model that can predict
    what type of review we can predict for that particular movie.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed the algorithms related to NLP. First, we looked
    at the terminology related to NLP. Next, we looked at the BoW methodology of implementing
    an NLP strategy. Then we looked at the concept of word embedding and the use of
    neural networks in NLP. Finally, we looked at an actual example where we used
    the concepts developed in this chapter to predict the sentiments of movie reviews
    based on their text. Having gone through this chapter, the user should be able
    to use NLP for text classification and sentiment analysis.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at recommendation engines. We will study different
    types of recommendation engine and how they can be used to solve some real-world
    problems.
  prefs: []
  type: TYPE_NORMAL
