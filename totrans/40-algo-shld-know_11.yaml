- en: Algorithms for Natural Language Processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduces algorithms for **natural language processing** (**NLP**).
    This chapter proceeds from the theoretical to the practical in a progressive manner.
    It will first introduce the fundamentals of NLP, followed by the basic algorithms.
    Then, it will look at one of the most popular neural networks that is widely used
    to design and implement solutions for important use cases for textual data. We
    will then look at the limitations of NLP before finally learning how we can use
    NLP to train a machine learning model that can predict the polarity of movie reviews.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will consist of the following sections:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Introducing NLP
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bag-of-words-based (BoW-based) NLP
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to word embedding
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use of recurrent neural networks for NLP
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using NLP for sentiment analysis
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Case study: movie review sentiment analysis'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will understand the basic techniques that are
    used for NLP. You should be able to understand how NLP can be used to solve some
    interesting real-world problems.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Let's start with the basic concepts.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Introducing NLP
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'NLP is used to investigate methodologies to formalize and formulate the interactions
    between computers and human (natural) languages. NLP is a comprehensive subject,
    and involves using computer linguistics algorithms and human–computer interaction
    technologies and methodologies to process complex unstructured data. NLP can be
    used for a variety of cases, including the following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '**Topic identification**:To discover topics in a text repository and classify
    the documents in the repository according to the discovered topics'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentiment analysis**: To classify the text according to the positive or negative
    sentiments that it contains'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Machine translation**: To translate the text from one spoken human language
    to another'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text to speech**: To convert spoken words into text'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Subjective interpretation**: To intelligently interpret a question and answer
    it using the information available'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Entity recognition**:To identify entities (such as a person, place, or thing)
    from text'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fake news detection**: To flag fake news based on the content'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start by looking at some of the terminology that is used when discussing
    NLP.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Understanding NLP terminology
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: NLP is a comprehensive subject. In the literature surrounding a certain field,
    we will observe that, sometimes, different terms are  used to specify the same
    thing. We will start by looking at some of the basic terminology related to NLP.
    Let's start with normalization, which is one of the basic kinds of NLP processing,
    usually performed on the input data.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Normalization
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Normalization is performed on the input text data to improve its quality in
    the context of training a machine learning model. Normalization usually involves
    the following processing steps:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Converting all text to uppercase or lowercase
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removing punctuation
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removing numbers
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that although the preceding processing steps are typically needed, the
    actual processing steps depend on the problem that we want to solve. They will
    vary from use case to use case—for example, if the numbers in the text represent
    something that may have some value in the context of the problem that we are trying
    to solve, then we may not need to remove the numbers from the text in the normalization
    phase.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Corpus
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The group of input documents that we are using to solve the problem in question
    is called the **corpus**. The corpus acts as the input data for the NLP problem.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we are working with NLP, the first job is to divide the text into a list
    of tokens. This process is called **tokenization**. The granularity of the resulting
    tokens will vary based on the objective—for example, each token can consist of
    the following:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: A word
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A combination of words
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A sentence
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A paragraph
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Named entity recognition
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In NLP, there are many use cases where we need to identify certain words and
    numbers from unstructured data as belonging to predefined categories, such as
    phone numbers, postal codes, names, places, or countries. This is used to provide
    a structure to the unstructured data. This process is called **named entity recognition**
    (**NER**).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Stopwords
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After word-level tokenization, we have a list of words that are used in the
    text. Some of these words are common words that are expected to appear in almost
    every document. These words do not provide any additional insight into the documents
    that they appear in. These words are called **stopwords**. They are usually removed
    in the data-processing phase. Some examples of stopwords are *was*, *we,* and
    *the*.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment analysis
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sentimental analysis, or opinion mining, is the process of extracting positive
    or negative sentiments from the text.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Stemming and lemmatization
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In textual data, most words are likely to be present in slightly different forms.
    Reducing each word to its origin or stem in a family of words is called **stemming**.
    It is used to group words based on their similar meanings to reduce the total
    number of words that need to be analyzed. Essentially, stemming reduces the overall
    conditionality of the problem.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: For example, {use, used, using, uses} => use.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: The most common algorithm for stemming English is the Porter algorithm.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Stemming is a crude process that can result in chopping off the ends of words.
    This may result in words that are misspelled. For many use cases, each word is
    just an identifier of a level in our problem space, and misspelled words do not
    matter. If correctly spelled words are required, then lemmatization should be
    used instead of stemming.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms lack common sense. For the human brain, treating similar words the
    same is straightforward. For an algorithm, we have to guide it and provide the
    grouping criteria.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'Fundamentally, there are three different ways of implementing NLP. These three
    techniques, which are different in terms of sophistication, are as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '**Bag-of-words**-based (Bo**W**-based) NLP'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Traditional NLP classifiers
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using deep learning for NLP
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NLTK
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Natural Language Toolkit** (**NLTK**) is the most widely utilized package
    for handling NLP tasks in Python. NLTK is one of the oldest and most popular Python
    libraries used for NLP. NLTK is great because it basically provides a jumpstart
    to building any NLP process by giving you the basic tools that you can then chain
    together to accomplish your goal rather than building all those tools from scratch.
    A lot of tools are packaged into NLTK, and in the next section, we will download
    the package and explore some of those tools.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at BoW-based NLP.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: BoW-based NLP
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The representation of input text as a bag of tokens is called **BoW-based processing**.
    The drawback of using BoW is that we discard most of the grammar and tokenization,
    which sometimes results in losing the context of the words. In the BoW approach,
    we first quantify the importance of each word in the context of each document
    that we want to analyze.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: 'Fundamentally, there are three different ways of quantifying the importance
    of the words in the context of each document:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '**Binary**: A feature will have a value of 1 if the word appears in the text
    or 0 otherwise.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Count**: A feature will have the number of times the word appears in the
    text as its value or 0 otherwise.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Term frequency/Inverse document frequency**: The value of the feature will
    be a ratio of how unique a word is in a single document to how unique it is in
    the entire corpus of documents. Obviously, for common words such as the, in, and
    so on (known as stop words), the **term frequency–inverse document frequency**
    (**TF-IDF**) score will be low. For more unique words—for instance, domain-specific
    terms—the score will be higher.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that by using BoW, we are throwing away information—namely, the order of
    the words in our text. This often works, but may lead to reduced accuracy.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at a specific example. We will train a model that can classify the
    reviews of a restaurant as negative or positive. The input file is a strutted
    file where the reviews will be classified as positive or negative.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: For this, let's first process the input data.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 'The processing steps are defined in the following diagram:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/70cf5585-8206-45a8-9b86-8aa31f914b37.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
- en: 'Let''s implement this processing pipeline by going through the following steps:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the packages that we need:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then we import the dataset from a `CSV` file:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/99fd1e80-976c-43b6-bf61-3e878651d157.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
- en: 'Next, we clean the data:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now let''s define the features (represented by `y`) and the label (represented
    by `X`):'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let''s divide the data into testing and training data:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'For training the model, we are using the naive Bayes algorithm:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let''s predict the test set results:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The confusion matrix looks like this:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/d3c55c12-09ac-460b-a3c2-915340b89103.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
- en: Looking at the confusion matrix, we can estimate the misclassification.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to word embedding
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the preceding section, we studied how we can perform NLP by using BoW as
    the abstraction for the input text data. One of the major advancements in NLP
    is our ability to create a meaningful numeric representation of words in the form
    of dense vectors. This technique is called word embedding. Yoshua Bengio first
    introduced the term in his paper *A Neural Probabilistic Language Model*. Each
    word in an NLP problem can be thought of as a categorical object. Mapping each
    of the words to a list of numbers represented as a vector is called word embedding.
    In other words, the methodologies that are used to convert words into real numbers
    are called word embedding. A differentiating feature of embedding is that it uses
    a dense vector, instead of using traditional approaches that use sparse matrix
    vectors.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 'There are basically two problems with using BoW for NLP:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '**Loss of semantic context**: When we tokenize the data, its context is lost.
    A word may have different meanings based on where it is used in the sentence;
    this becomes even more important when interpreting complex human expressions,
    such as humor or satire.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sparse input**: When we tokenize, each word becomes a feature. As we saw
    in the preceding example, each word is a feature. It results in sparse data structures.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The neighborhood of a word
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A key insight into how to present textual data (specifically, individual words,
    or lexemes) to an algorithm comes from linguistics. In word embedding, we pay
    attention to the neighborhood of each word and use it to establish its meaning
    and importance. Neighbourhood of a word is the set of words surrounds a particular
    word. The context of a word is determined by its neighborhood.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Note that in BoW, a word loses its context, as its context is from the neighborhood
    that it is in.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Properties of word embeddings
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Good word embeddings exhibit the following four properties:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '**They are dense**: In fact, embeddings are essentially factor models. As such,
    each component of the embedding vector represents a quantity of a (latent) feature.
    We typically do not know what that feature represents; however, we will have very
    few—if any—zeros that will cause a sparse input.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**They are low dimensional**: An embedding has a predefined dimensionality
    (chosen as a hyperparameter). We saw earlier that in the BoW representation we
    needed |*V*| inputs for each word, so that the total size of the input was |*V*|
    * *n* where *n* is the number of words we use as input. With word embeddings,
    our input size will be *d* * *n*, where *d* is typically between 50 and 300\.
    Considering the fact that large text corpora are often much larger than 300 words,
    this means that we have large savings in input size, which we saw can lead to
    better accuracy for a smaller total number of data instances.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**They embed domain semantics**: This property is probably the most surprising,
    but also the most useful. When properly trained, embeddings learn about the meaning
    of their domain.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generalize easily**: Finally, web embedding is capable of picking up generalized
    abstract patterns—for example, we can train on (the embeddings of) cat, deer,
    dog, and so on, and the model will understand that we mean animals. Note that
    the model was never trained for sheep, and yet the model will still correctly
    classify it. By using embedding, we can expect to receive the correct answer.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let us explore, how we can use RNNs for Natural Language Processing.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Using RNNs for NLP
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An RNN is a traditional feed-forward network with feedback. A simple way of
    thinking about an RNN is that it is a neural network with states. RNNs are used
    with any type of data for generating and predicting various sequences of data.
    Training an RNN model is about formulating these sequences of data. RNNs can be
    used for text data as sentences are just sequences of words. When we use RNNs
    for NLP, we can use them for the following:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Predicting the next word when typing
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Generating new text, following the style already used in the text:'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](assets/8aba997b-e588-4c5d-8307-d33b6ad5c15b.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
- en: Remember the combination of words that resulted in their correct prediction?
    The learning process of RNNs is based on the text that is found in the corpus.
    They are trained by reducing the error between the predicted next word and the
    actual next word.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Using NLP for sentiment analysis
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The approach presented in this section is based on the use case of classifying
    a high rate of incoming stream tweets. The task at hand is to extract the embedded
    sentiments within the tweets about a chosen topic. The sentiment classification
    quantifies the polarity in each tweet in real time and then aggregate the total
    sentiments from all tweets to capture the overall sentiments about the chosen
    topic. To face the challenges posed by the content and behavior of Twitter stream
    data and perform the real-time analytics efficiently, we use NLP by using a trained
    classifier. The trained classifier is then plugged into the Twitter stream to
    determine the polarity of each tweet (positive, negative, or neutral), followed
    by the aggregation and determination of the overall polarity of all tweets about
    a certain topic. Let's see how this is done step by step.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we have to train the classifier. In order to train the classifier, we
    needed an already-prepared dataset that has historical Twitter data and follows
    the patterns and trends of the real-time data. Therefore, we used a dataset from
    the website [www.sentiment140.com](http://www.sentiment140.com/), which comes
    with a human-labeled corpus (a large collection of texts upon which the analysis
    is based) with over 1.6 million tweets. The tweets within this dataset have been
    labeled with one of three polarities: zero for negative, two for neutral, and
    four for positive. In addition to the tweet text, the corpus provides the tweet
    ID, date, flag, and the user who tweeted. Now let''s see each of the operations
    that are performed on the live tweet before it reaches the *trained* classifier:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Tweets are first split into individual words called tokens (tokenization).
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The output from tokenization creates a BoW, which is a collection of individual
    words in the text.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These tweets are further filtered by removing numbers, punctuations, and stop
    words (stop word removal). Stop words are words that are extremely common, such
    as *is*, *am*, *are*, and *the*. As they hold no additional information, these
    words are removed.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Additionally, nonalphabetical characters, such as *#**@* and numbers, are removed
    using pattern matching, as they hold no relevance in the case of sentiment analysis.
    Regular expressions are used to match alphabetical characters only and the rest
    are ignored. This helps to reduce the clutter from the Twitter stream.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The outcomes of the prior phase are taken to the stemming phase. In this phase,
    the derived words are reduced to their roots—for example, a word like *fish* has
    the same roots as *fishing* and *fishes*. For this, we use the library of standard
    NLP, which provides various algorithms, such as Porter stemming.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the data is processed, it is converted into a structure called a **term
    document matrix** (**TDM**). The TDM represents the term and frequency of each
    work in the filtered corpus.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the TDM, the tweet reaches the trained classifier (as it is trained, it
    can process the tweet), which calculates the **sentimental polarity importance**
    (**SPI**) of each word which is a number from -5 to +5\. The positive or negative
    sign specifies the type of emotions represented by that particular word, and its
    magnitude represents the strength of sentiment. This means that the tweet can
    be classified as either positive or negative (refer to the following image). Once
    we calculate the polarity of the individual tweets, we sum their overall SPI to
    find the aggregated sentiment of the source—for example, an overall polarity greater
    than one indicates that the aggregated sentiment of the tweets in our observed
    period of time is positive.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To retrieve the real-time raw tweets, we use the Scala library *Twitter4J*,
    a Java library that provides a package for a real-time Twitter streaming API.
    The API requires the user to register a developer account with Twitter and fill
    in some authentication parameters. This API allows you to either get random tweets
    or filter tweets using chosen keywords. We used filters to retrieve tweets related
    to our chosen keywords.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: 'The overall architecture is shown in the following figure:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/3dc16321-97c8-4e5e-b815-3584e3e61f97.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
- en: The sentiment analysis have various applications. It can be applied to classify
    the feedback from customers. Social media polarity analysis can be used by governments
    to find the effectiveness of their policies. It can also quantify the success
    of various advertisement campaigns.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn how we can actually apply sentiment analysis
    to predict the sentiments of movie reviews.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'Case study: movie review sentiment analysis'
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s use NLP to conduct a movie review sentiment analysis. For this, we will
    use some open source movie review data available at [http://www.cs.cornell.edu/people/pabo/movie-review-data/](http://www.cs.cornell.edu/people/pabo/movie-review-data/):'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will import the dataset that contains the movie reviews:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now, let us load the movies' data and print the first few rows to observe its
    structure.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](assets/d613c48f-67c9-44c0-97b0-db32f368deae.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
- en: Note that the dataset has `2000` movie reviews. Out of these, half are negative
    and half are positive.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's start preparing the dataset for training the model. First, let us
    drop any missing values that are in the data
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now we need to remove the whitespaces as well. Whitespaces are not null but
    need to be removed. For this, we need to iterate over each row in the input `DataFrame`.
    We will use `.itertuples()` to access every field:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note that we have used `i`, `lb`, and `rv` to the index, label, and review columns.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s split the data into test and train datasets:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to specify the features and the label and then split the
    data into the train and test sets:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now we have the testing and training datasets.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s divide the sets into train and test:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们将数据集分成训练集和测试集：
- en: '[PRE11]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note that we are using `tfidf` to quantify the importance of a datapoint in
    a collection.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们正在使用`tfidf`来量化集合中数据点的重要性。
- en: Next, let's train the model using the naive Bayes algorithm, and then test the
    trained model.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们使用朴素贝叶斯算法来训练模型，然后测试训练好的模型。
- en: 'Let''s follow these steps to train the model:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们按照以下步骤来训练模型：
- en: 'Now let''s train the model using the testing and training datasets that we
    created:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们使用我们创建的测试和训练数据集来训练模型：
- en: '[PRE12]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Let us run the predictions and analyze the results:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们运行预测并分析结果：
- en: '[PRE13]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Let us now look into the performance of our model by printing the confusion
    matrix. We will also look at *precision*, *recall*, *f1-score*, and *accuracy*.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过打印混淆矩阵来查看模型的性能。我们还将查看*精确度*、*召回率*、*F1分数*和*准确度*。
- en: '![](assets/774aa10e-3479-403d-a3a5-949c7179ab32.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/774aa10e-3479-403d-a3a5-949c7179ab32.png)'
- en: These performance metrics give us a measure of the quality of the predictions.
    With a 0.78 accuracy, now we have successfully trained a model that can predict
    what type of review we can predict for that particular movie.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这些性能指标为我们提供了预测质量的度量。准确率为0.78，现在我们已经成功训练了一个可以预测特定电影评论类型的模型。
- en: Summary
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed the algorithms related to NLP. First, we looked
    at the terminology related to NLP. Next, we looked at the BoW methodology of implementing
    an NLP strategy. Then we looked at the concept of word embedding and the use of
    neural networks in NLP. Finally, we looked at an actual example where we used
    the concepts developed in this chapter to predict the sentiments of movie reviews
    based on their text. Having gone through this chapter, the user should be able
    to use NLP for text classification and sentiment analysis.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了与自然语言处理相关的算法。首先，我们研究了与自然语言处理相关的术语。接下来，我们研究了实施自然语言处理策略的BoW方法。然后，我们研究了词嵌入的概念以及在自然语言处理中使用神经网络。最后，我们看了一个实际的例子，我们在这一章中使用了开发的概念来根据电影评论的文本来预测情感。通过学习本章内容，用户应该能够将自然语言处理用于文本分类和情感分析。
- en: In the next chapter, we will look at recommendation engines. We will study different
    types of recommendation engine and how they can be used to solve some real-world
    problems.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将研究推荐引擎。我们将研究不同类型的推荐引擎以及它们如何用于解决一些现实世界的问题。
