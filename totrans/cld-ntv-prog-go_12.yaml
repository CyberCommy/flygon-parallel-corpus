- en: Where to Go from Here?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Welcome to the last chapter in our journey to learning cloud native programming
    in the Go language. By now, you should have enough knowledge to build production
    grade microservices, design complex distributed architectures, utilize the power
    of key Amazon web services, empower your software with containers, among other
    things.
  prefs: []
  type: TYPE_NORMAL
- en: However, the subject of cloud native programming is a very deep and massive
    one. This means that there are topics that you can still learn to enrich your
    knowledge and skills in that area. The purpose of this chapter is to light a path
    for you from where this book leaves off, by providing some practical overviews
    of topics not covered in this book that are strong avenues to pursue after you
    absorb the knowledge contained in this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Additional microservice communication patterns and protocols, such as Protocol
    Buffers and GRPC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More useful features offered by cloud providers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other cloud providers (Azure, GCP, and OpenStack)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serverless Computing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microservices communications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this book, we covered two approaches for microservices to communicate with
    each other:'
  prefs: []
  type: TYPE_NORMAL
- en: The first approach was via RESTful APIs, where a web HTTP layer would be built
    into a microservice, effectively allowing the microservice to communicate with
    any web client, whether the web client is another microservice or a web browser.
    One advantage to this approach is that it empowers microservices to communicate
    with the outside world when needed, since HTTP is now a universal protocol that
    is supported by all software stacks out there. The disadvantage of this approach,
    however, is the fact that HTTP can be a heavy protocol with multiple layers, which
    may not be the best choice when the requirement is fast efficient communications
    between internal microservices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second approach is via message queues, where a message broker software such
    as RabbitMQ or Kafka will facilitate the exchange of messages between microservices.
    Message brokers receive messages from sending microservices, queue the messages,
    and then deliver them to microservices that previously indicated their interests
    in those messages. One major advantage of this approach is the fact that it can
    solidify data consistency in large-scale distributed microservices architectures,
    as explained in [Chapter 11](1b9e8bd9-c7a6-46ce-87f7-d45631865e07.xhtml), *Migration*.
    This approach enables event-driven distributed architectures, such as event sourcing
    and CQRS. However, if our scaling requirements are relatively simple in scope,
    this approach may be too complex for our needs. This is because it requires us
    to maintain a message broker software with all its configurations and backends.
    In those cases, direct microservice to microservice communication may be all what
    we need.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you haven''t noted already, one obvious disadvantage for either of those
    approaches is the fact that they don''t offer direct efficient microservice to
    microservice communications. There are two popular choices for technologies that
    we can employ for direct microservices communications: Protocol buffers and GRPC.'
  prefs: []
  type: TYPE_NORMAL
- en: Protocol buffers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In their official documentations, protocol buffers are defined as a language-neutral,
    platform-neutral mechanism for serializing structured data. Let's take a look
    at an example to help build a clear picture of what protocol buffers are.
  prefs: []
  type: TYPE_NORMAL
- en: Assume that you have two microservices in your application; the first microservice
    (service 1) has collected information about a new customer and would like to send
    it to the second microservice (service 2). This data is considered structured
    data because it contains structured information such as the customer name, age,
    job, and phone numbers. One way to send this data is to send it as a JSON document
    (our data format) over HTTP from service 1 to service 2\. However, what if we
    want to send this data faster and in a smaller form? This is where protocol buffers
    come into the picture. Inside service 1, protocol buffers will take the customer
    object, then serialize it into a compact form. From there, we can take this encoded
    compact piece of data and send it to service 2 via an efficient communications
    protocol such as TCP or UDP.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we described protocol buffers as inside the service in the preceding
    example. This is true because protocol buffers come as software libraries that
    we can import and include in our code. There are protocol buffer packages for
    a wide selection of programming languages (Go, Java, C#, C++, Ruby, Python, and
    more).
  prefs: []
  type: TYPE_NORMAL
- en: 'The way how protocol buffers work is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: You define your data in a special file, known as the `proto` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You use a piece of software known as the protocol buffer compiler to compile
    the proto file into code files written in the programming language of your choice.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You use the generated code files combined with the protocol buffer software
    package in your language of choice to build your software.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is protocol buffers in a nutshell. In order to obtain more deep understanding
    of protocol buffers, go to [https://developers.google.com/protocol-buffers/](https://developers.google.com/protocol-buffers/),
    where you will find good documentation to get you started with the technology.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are currently two commonly used versions for protocol buffers: protocol
    buffers 2 and protocol buffers 3\. A lot of the current training resources available
    online cover the newest version, Protocol Buffers 3\. If you are looking for a
    resource to help with protocol buffers version 2, you can check this article in
    my website at [http://www.minaandrawos.com/2014/05/27/practical-guide-protocol-buffers-protobuf-go-golang/](http://www.minaandrawos.com/2014/05/27/practical-guide-protocol-buffers-protobuf-go-golang/).'
  prefs: []
  type: TYPE_NORMAL
- en: GRPC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One key feature missing from the protocol buffers technology is the communications
    part. Protocol buffers excel at encoding and serializing data into compact forms
    that we can share with other microservices. However, when the concept of Protocol
    buffers was initially conceived, only serialization was considered, but not the
    part where we actually send the data elsewhere. For that, developers used to roll
    their sleeves and implement their own TCP or UDP application layer to exchange
    the encoded data between services. However, what if we can't spare the time and
    effort to worry about an efficient communication layer? This is where GRPC comes
    into picture.
  prefs: []
  type: TYPE_NORMAL
- en: GRPC can simply be described as protocol buffers combined with an RPC layer
    on top. A **Remote Procedure Call** (**RPC**) layer is a software layer that allows
    different piece of software such as microservices to interact via an efficient
    communications protocol such as TCP. With GRPC, your microservice can serialize
    your structured data via protocol buffers version 3 and then will be able to communicate
    this data with other microservices without you having to worry about implementing
    a communications layer.
  prefs: []
  type: TYPE_NORMAL
- en: If your application architecture needs efficient and fast interactions between
    your microservices, and at the same time you can't use message queues or Web APIs,
    then consider GRPC for your next application.
  prefs: []
  type: TYPE_NORMAL
- en: To get started with GRPC, visit [https://grpc.io/](https://grpc.io/). Similar
    to protocol buffers, GRPC is supported by a wide range of programming languages.
  prefs: []
  type: TYPE_NORMAL
- en: More on AWS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this book, we dedicated two chapters to providing a practical dive into AWS
    fundamentals, with a focus on how to write Go microservices that would site comfortably
    on Amazon's cloud. However, AWS is a very deep topic that deserves an entire book
    to cover it as opposed to just a few chapters. In this section, we will provide
    brief overviews on some useful AWS technologies that we didn't get a chance to
    cover in this book. You can use the following section as an introduction for your
    next steps in learning AWS.
  prefs: []
  type: TYPE_NORMAL
- en: DynamoDB streams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 8](25f18fd2-4d08-41fb-a8b2-acc927bd0876.xhtml), *AWS II - S3, SQS,
    API Gateway, and DynamoDB,* we covered the popular AWS DynamoDB service. We learned
    what DynamoDB is, how it models data, and how to write Go applications that can
    harness DynamoDB's power.
  prefs: []
  type: TYPE_NORMAL
- en: There is one powerful feature of DynamoDB that we didn't get a chance to cover
    in this book, which is known as DynamoDB streams. DynamoDB streams allow us to
    capture changes that happen to items in a DynamoDB table, at the same time the
    change occurs. In practice, this means that we can react to data changes that
    happen to the database in real time. As usual, let's take an example to solidify
    the meaning.
  prefs: []
  type: TYPE_NORMAL
- en: Assume that we are building the cloud native distributed microservices application
    that powers a large multiplayer game. Let's say that we use DynamoDB as the database
    backend for our application and that one of our microservices added a new player
    to the database. If we are utilizing DynamoDB streams in our application, other
    interested microservices will be able to capture the new player information shortly
    after it gets added. This allows the other microservices to act accordingly with
    this new information. For instance, if one of the other microservices is responsible
    for locating players in the game map, it will then attach the new player to a
    start location on the game map.
  prefs: []
  type: TYPE_NORMAL
- en: The way DynamoDB streams work is simple. They capture changes that happen to
    a DynamoDB table item in order. The information gets stored in a log that goes
    up to 24 hours. Other applications written by us can then access this log and
    capture the data changes.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, if an item gets created, deleted, or updated, DynamoDB streams
    would store the item primary key and the data modification that occurred.
  prefs: []
  type: TYPE_NORMAL
- en: DynamoDB streams need to be enabled on tables that need monitoring. We can also
    disable DynamoDB streams on existing tables, if, for any reason, the tables don't
    need any more monitoring. DynamoDB streams operate in parallel to the DynamoDB
    tables, which basically means that there are no performance impact to using them.
  prefs: []
  type: TYPE_NORMAL
- en: To get started with DynamoDB streams, check out [http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html](http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html).
  prefs: []
  type: TYPE_NORMAL
- en: To get started with DynamoDB streams support in the Go programming language,
    check out [https://docs.aws.amazon.com/sdk-for-go/api/service/dynamodbstreams/](https://docs.aws.amazon.com/sdk-for-go/api/service/dynamodbstreams/).
  prefs: []
  type: TYPE_NORMAL
- en: Autoscaling on AWS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Due to the fact that AWS is designed from the grounds up to be utilized with
    massively distributed microservices applications, AWS comes with built-in features
    to allow developers of these huge applications to autoscale their applications
    in the cloud with the least manual intervention possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the world of AWS, the word autoscaling means three main things:'
  prefs: []
  type: TYPE_NORMAL
- en: The ability to automatically replace unhealthy applications or bad EC2 instances
    without your intervention.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to automatically create new EC2 instances to handle increased loads
    on your microservices application without your intervention. Then, the ability
    to shut down EC2 instances when the application loads decrease.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to automatically increase cloud service resources available for
    your application, when the application loads increase. AWS cloud resources here
    go beyond just EC2\. An example of a cloud service resource that can go automatically
    up or down according to your need is DynamoDB read and write throughput.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To serve this broad definition of autoscaling, the AWS autoscaling service
    offers three main features:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fleet management for EC2 instances**: This feature allows you to monitor
    the health of running EC2 instances, automatically replaces bad instances without
    manual intervention, and balances Ec2 instances across multiple zones when multiple
    zones are configured.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dynamic Scaling**: This feature allows you to first configure tracking policies
    to engage the amount of load on your applications. For example, monitor CPU utilization
    or capture the number of incoming requests. Then, the dynamic scaling feature
    can automatically add or remove EC2 instances based on your configured target
    limits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Application Auto Scaling**: This feature allows you to dynamically scale
    resources on AWS services that go beyond EC2, based on your application''s needs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To get started with the AWS autoscaling services, visit [https://aws.amazon.com/autoscaling/](https://aws.amazon.com/autoscaling/.).
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Relational Database Service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 8](25f18fd2-4d08-41fb-a8b2-acc927bd0876.xhtml), *AWS II - S3, SQS,
    API Gateway, and DynamoDB*, when we covered the database service in the AWS world,
    we covered DynamoDB exclusively. DynamoDB is a managed NoSQL database service
    that is offered by Amazon on AWS. If you have enough technical expertise with
    database engines, you would probably ask the obvious question: what about relational
    databases? Shouldn''t there be a managed AWS service for that as well?'
  prefs: []
  type: TYPE_NORMAL
- en: The answer for the previous two questions is yes, there is, and it's called
    **Amazon Relational Database Service** (**RDS**). AWS RDS allows developers to
    easily configure, operate, scale, and deploy a relational database engine on the
    cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon RDS supports a collection of well-known relational database engines that
    a lot of developers use and love. This includes PostgreSQL, MySQL, MariaDB, Oracle,
    and Microsoft SQL server. In addition to RDS, Amazon offers a service known as
    Database Migration service that allows you to easily migrate or replicate your
    existing database to Amazon RDS.
  prefs: []
  type: TYPE_NORMAL
- en: To get started on AWS RDS, visit [https://aws.amazon.com/rds/](https://aws.amazon.com/rds/).
    To build Go applications capable of interacting with RDS, visit [https://docs.aws.amazon.com/sdk-for-go/api/service/rds/](https://docs.aws.amazon.com/sdk-for-go/api/service/rds/).
  prefs: []
  type: TYPE_NORMAL
- en: Other cloud providers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up until now, we have focused on AWS as a cloud provider. Of course, there are
    other providers that offer similar services, the two biggest being the Microsoft
    Azure Cloud and the Google Cloud Platform. Besides these, there are many other
    providers that also offer IaaS solutions, more often than not based on the open
    source platform OpenStack.
  prefs: []
  type: TYPE_NORMAL
- en: All cloud providers employ similar concepts, so if you have experience with
    one of them, you will probably find your way around others. For this reason, we
    decided not to cover each of them in depth within this book, but instead focus
    on AWS and give you a short outlook on other providers and how they are different.
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft Azure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can sign up for the Azure cloud on [https://azure.microsoft.com/en-us/free/](https://azure.microsoft.com/en-us/free/).
    Like AWS, Azure offers multiple regions and availability zones in which you can
    run your services. Also, most of the Azure core services work similar to AWS,
    although they often are named differently:'
  prefs: []
  type: TYPE_NORMAL
- en: The service managing virtual machines (EC2 in AWS terms) is called just that, **virtual
    machines**. When creating a virtual machine, you will need to select an image
    (both Linux and Windows images are supported), provide an SSH public key, and
    choose a machine size. Other core concepts are named similarly. You can configure
    network access rules using **Network Security Groups**, loadbalance traffic using **Azure
    Load Balancers** (named Elastic Load Balancer in AWS), and manage automatic scaling
    using **VM Scale Sets**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relational databases (managed by the Relational Database Service in AWS) are
    managed by **Azure SQL Databases**. However, at the time of writing this book,
    only Microsoft SQL databases are supported. Support for MySQL and PostgreSQL databases
    is available as a preview service only.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NoSQL databases, similar to DynamoDB, are available in the form of the **Azure
    Cosmos DB**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Message Queues similar to the Simple Queue Service are provided by the **Queue
    Storage** service.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to APIs provided by your services is possible using the **Application
    Gateway**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To consume Azure services from within your Go application, you can use the **Azure
    SDK for Go**, which is available at [https://github.com/Azure/azure-sdk-for-go](https://github.com/Azure/azure-sdk-for-go).
    You can install it using the usual `go get` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The Azure SDK for Go is currently still under heavy development and should be
    used with caution. To not be surprised by any breaking changes in the SDK, ensure
    that you use a dependency management tool such as *Glide* to put a version of
    this library into your vendor/directory (as you learned in [Chapter 9](465ef76f-a2b2-42f7-b9c4-3c60ac552f77.xhtml),
    *Continuous Delivery*).
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud Platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **Google Cloud Platform** (**GCP**) is the IaaS offering by Google. You
    can sign up at [https://console.cloud.google.com/freetrial](https://console.cloud.google.com/freetrial).
    Just as with the Azure cloud, you will recognize many core features, although
    differently named:'
  prefs: []
  type: TYPE_NORMAL
- en: You can manage virtual instances using the **Google Compute Engine**. As usual,
    each instance is created from an image, a selected machine type, and an SSH public
    key. Instead of Security Groups, you have **Firewall Rules**, and autoscaling
    groups are called **Managed Instance Groups**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relational databases are provided by the **Cloud SQL** service. GCP supports
    both MySQL and PostgreSQL instances.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For NoSQL databases, you can use the **Cloud Datastore** service.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Cloud Pub/Sub** service offers the possibility to implement complex publish/subscribe
    architectures (in fact, superceding the possibilities that AWS offers with SQS).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since both come from Google, it goes without saying that GCP and Go go hand
    in hand (pun intended). You can install the Go SDK via the usual `go get` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: OpenStack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are also many cloud providers that build their products on the open source
    cloud management software, OpenStack ([https://www.openstack.org](https://www.openstack.org)).
    OpenStack is a highly modular software, and clouds built on it may vary significantly
    in their setup, so it''s difficult to make any universally valid statements about
    them. Typical OpenStack installations might consist of the following services:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Nova** manages virtual machine instances and Neutron to manage networking.
    In the management console, you will find this under the Instances and Networks labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Zun** and **Kuryr** manage containers. Since these are relatively young components,
    it will probably be more common to find managed Kubernetes clusters in OpenStack
    clouds, though.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trove** provides database services for both relational and nonrelational
    databases, such as MySQL or MongoDB.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Zaqar** provides messaging services similar to SQS.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to access OpenStack features from a Go application, there are multiple
    libraries that you can choose from. First of all, there is the official client
    library—[github.com/openstack/golang-client](http://github.com/openstack/golang-client)—which,
    however, is not yet recommended for production use. At the time of writing this
    book, the most mature Go client library for OpenStack is the [github.com/gophercloud/gophercloud](http://github.com/openstack/golang-client) library.
  prefs: []
  type: TYPE_NORMAL
- en: Running containers in the cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 6](af835c34-a409-4898-9a55-74fc9d05b46b.xhtml), *Deploying Your
    Application in Containers*, we got a thorough look at how to deploy a Go application
    using modern container technologies. When it comes to deploying these containers
    into a cloud environment, you have a variety of different ways to do that.
  prefs: []
  type: TYPE_NORMAL
- en: One possibility to deploy containerized applications is using an orchestration
    engine such as **Kubernetes**. This is especially easy when you are using the
    Microsoft Azure cloud or the Google Cloud Platform. Both providers offer Kubernetes
    as a managed service, although not under that name; look for the **Azure Container
    Service** (**AKS**) or **Google Container Engine** (**GKE**).
  prefs: []
  type: TYPE_NORMAL
- en: Although AWS does not offer a managed Kubernetes service, they have a similar
    offering called **EC2 Container Service** (**ECS**). Since ECS is a service available
    exclusively on AWS, it is very tightly integrated with other AWS core services,
    which can be both an advantage and disadvantage. Of course, you can set up your
    own Kubernetes cluster on AWS using the building blocks provided in the form of
    VMs, networking, and storage. Doing this is incredibly complex work, but do not
    despair. You can use third-party tools to set up a Kubernetes cluster on AWS automatically.
    One of these tools is **kops**.
  prefs: []
  type: TYPE_NORMAL
- en: You can download kops at [https://github.com/kubernetes/kops](https://github.com/kubernetes/kops).
    After that, follow the setup instructions for AWS that you can find in the project
    documentation at [https://github.com/kubernetes/kops/blob/master/docs/aws.md](https://github.com/kubernetes/kops/blob/master/docs/aws.md).
  prefs: []
  type: TYPE_NORMAL
- en: Kops itself is also written in Go and uses the very same AWS SDK that you have
    already come across in [Chapters 7](f11f12b7-32c8-4ee8-ad1a-b9e1882b57f6.xhtml), *AWS
    I–Fundamentals, AWS SDK for Go, and EC2*, and [Chapter 8](25f18fd2-4d08-41fb-a8b2-acc927bd0876.xhtml),
    *AWS II - S3, SQS, API Gateway, and DynamoDB*. Take a look at the source code
    to see a real-life example of some very sophisticated usage of the AWS client
    library.
  prefs: []
  type: TYPE_NORMAL
- en: Serverless architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When consuming a traditional Infrastructure-as-a-Service offering, you are provided
    a number of virtual machines along with the respective infrastructure (such as
    storage and networking). You typically need to operate everything running within
    these virtual machines yourself. This usually means not only your compiled application,
    but also the entire operating system, including the kernel of each and every system
    service of a full-blown Linux (or Windows) system. You are also responsible for
    the capacity planning of your infrastructure (which means estimating your application's
    resource requirements and defining sensible boundaries for your autoscaling groups).
  prefs: []
  type: TYPE_NORMAL
- en: All of this means **Operational Overhead** that keeps you from your actual job,
    that is, building and deploying software that drives your business. To reduce
    this overhead, you can instead use a Platform-as-a-Service offering instead of
    an IaaS one. One common form of PaaS hosting is using container technologies,
    where the developer simply provides a container image, and the provider takes
    care of running (and optionally, scaling) the application and managing the underlying
    infrastructure. Typical container-based PaaS offerings include the EC2 Container
    Service by AWS or any Kubernetes cluster, such as the Azure Container Service
    or the Google Container Engine. Noncontainer-based PaaS offerings might include
    AWS Elastic Beanstalk or Google App Engine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recently, another approach has arisen that strives to eliminate even more operational
    overhead than PaaS offerings: **Serverless Computing**. Of course, that name is
    wildly misleading, as applications being run on a serverless architecture obviously
    still require servers. The key difference is that the existence of these servers
    is completely hidden from the developer. The developer only provides the application
    to be executed, and the provider takes care of provisioning infrastructure for
    this application and deploying and running it. This approach works well with Microservice
    Architectures, as it becomes incredibly easy to deploy small pieces of code that
    communicate with each other using web services, message queues, or other means.
    Taken to the extreme, this often results in single functions being deployed as
    services, resulting in the alternate term for serverless computing: **Functions-as-a-Service**
    (**FaaS**).'
  prefs: []
  type: TYPE_NORMAL
- en: Many cloud providers offer FaaS functionalities as part of their services, the
    most prominent example being **AWS Lambda**. At the time of writing this book,
    AWS Lambda does not officially support Go as a programming language (supported
    languages are JavaScript, Python, Java, and C#), and running Go functions is only
    possible using third-party wrappers such as [https://github.com/eawsy/aws-lambda-go](https://github.com/eawsy/aws-lambda-go).
  prefs: []
  type: TYPE_NORMAL
- en: Other cloud providers offer similar services. Azure offers **Azure Functions** (supporting
    JavaScript, C#, F#, PHP, Bash, Batch, and PowerShell) and GCP offers **Cloud Functions** as
    a Beta product (supporting only JavaScript). If you are running a Kubernetes cluster,
    you can use the Fission framework ([https://github.com/fission/fission](https://github.com/fission/fission))
    to run your own FaaS platform (which even supports Go). However, Fission is a
    product in an early alpha development stage and not yet recommended for production
    use.
  prefs: []
  type: TYPE_NORMAL
- en: As you may have noticed, support for the Go language is not yet far spread among
    the popular FaaS offerings. However, given the popularity of both Go as a programming
    language and Serverless Architecture, not all hope is lost.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With this, we reach the end of our book. By now, you should have enough knowledge
    to build sophisticated microservices cloud native applications that are resilient,
    distributed, and scalable. With this chapter, you should also develop ideas of
    where to go next to take your newly acquired knowledge to the next level. We thank
    you for giving us the chance to guide you through this learning journey and look
    forward to being part of your future journeys.
  prefs: []
  type: TYPE_NORMAL
