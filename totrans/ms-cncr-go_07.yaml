- en: Chapter 7. Performance and Scalability
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To build a high-powered web server in Go with just a few hundred lines of code,
    you should be quite aware of how concurrent Go provides us with exceptional tools
    for performance and stability out of the box.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Our example in [Chapter 6](part0054_split_000.html#page "Chapter 6. C10K – A
    Non-blocking Web Server in Go"), *C10K – A Non-blocking Web Server in Go*, also
    showed how imposing blocking code arbitrarily or inadvertently into our code can
    introduce some serious bottlenecks and quickly torpedo any plans to extend or
    scale your application.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: What we'll look at in this chapter are a few ways that can better prepare us
    to take our concurrent application and ensure that it's able to continuously scale
    in the future and that it is capable of being expanded in scope, design, and/or
    capacity.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: We'll expand a bit on **pprof**, the CPU profiling tool we looked at briefly
    in previous chapters, as a way to elucidate the way our Go code is compiled and
    to locate possible unintended bottlenecks.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Then we'll expand into distributed Go and into ways to offer some performance-enhancing
    parallel-computing concepts to our applications. We'll also look at the Google
    App Engine, and at how you can utilize it for your Go-based applications to ensure
    scalability is placed in the hands of one of the most reliable hosting infrastructures
    in the world.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we'll look at memory utilization, preservation, and how Google's garbage
    collector works (and sometimes doesn't). We'll finally delve a bit deeper into
    using memory caching to keep data consistent as well as less ephemeral, and we
    will also see how that dovetails with distributed computing in general.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: High performance in Go
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up to this point, we've talked about some of the tools we can use to help discover
    slowdowns, leaks, and inefficient looping.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Go's compiler and its built-in deadlock detector keep us from making the kind
    of mistake that's common and difficult to detect in other languages.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: We've run time-based benchmarks based on specific changes to our concurrency
    patterns, which can help us design our application using different methodologies
    to improve overall execution speed and performance.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Getting deeper into pprof
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The pprof tool was first encountered in [Chapter 5](part0048_split_000.html#page
    "Chapter 5. Locks, Blocks, and Better Channels"), *Locks, Blocks, and Better Channels*,
    and if it still feels a bit cryptic, that's totally understandable. What pprof
    shows you in export is a **call graph**, and we can use this to help identify
    issues with loops or expensive calls on the heap. These include memory leaks and
    processor-intensive methods that can be optimized.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: One of the best ways to demonstrate how something like this works is to build
    something that doesn't. Or at least something that doesn't work the way it should.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: You might be thinking that a language with garbage collection might be immune
    to these kinds of memory issues, but there are always ways to hide mistakes that
    can lead to memory leakage. If the GC can't find it, it can sometimes be a real
    pain to do so yourself, leading to a lot of—often feckless—debugging.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: To be fair, what constitutes a memory leak is sometimes debated among computer
    science members and experts. A program that continuously consumes RAM may not
    be leaking memory by technical definition if the application itself could re-access
    any given pointers. But that's largely irrelevant when you have a program that
    crashes and burns after consuming memory like an elephant at a buffet.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: The basic premise of creating a memory leak in a garbage-collected language
    relies on hiding the allocation from the compiler—indeed, any language in which
    you can access and utilize memory directly provides a mechanism for introducing
    leaks.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: We'll review a bit more about garbage collection and Go's implementation later
    in this chapter.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: So how does a tool like pprof help? Very simply put, by showing you **where**
    your memory and CPU utilization goes.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first design a very obvious CPU hog as follows to see how pprof highlights
    this for us:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The output of the preceding code is shown in the following diagram:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting deeper into pprof](img/00039.jpeg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
- en: In this case, we know where our stack resource allocation is going, because
    we willfully introduced the loop (and the loop within that loop).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that we didn't intentionally do that and had to locate resource hogs.
    In this case, pprof makes this pretty easy, showing us the creation and memory
    allocation of simple strings comprising the majority of our samples.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: We can modify this slightly to see the changes in the pprof output. In an effort
    to allocate more and more memory to see whether we can vary the pprof output,
    we might consider heavier types and more memory.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: 'The easiest way to accomplish that is to create a slice of a new type that
    includes a significant amount of these heavier types such as int64\. We''re blessed
    with Go: in that, we aren''t prone to common C issues such as buffer overflows
    and memory protection and management, but this makes debugging a little trickier
    when we cannot intentionally break the memory management system.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**The unsafe package**'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite the built-in memory protection provided, there is still another interesting
    tool provided by Go: the **unsafe** package. As per Go''s documentation:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '*Package unsafe contains operations that step around the type safety of Go
    programs.*'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: This might seem like a curious library to include—indeed, while many low-level
    languages allow you to shoot your foot off, it's fairly unusual to provide a segregated
    language.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Later in this chapter, we'll examine `unsafe.Pointer`, which allows you to read
    and write to arbitrary bits of memory allocation. This is obviously extraordinarily
    dangerous (or useful and nefarious, depending on your goal) functionality that
    you would generally try to avoid in any development language, but it does allow
    us to debug and understand our programs and the Go garbage collector a bit better.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'So to increase our memory usage, let''s switch our string allocation as follows,
    for random type allocation, specifically for our new struct `MemoryHog`:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: There's obviously nothing preventing us from extending this into some ludicrously
    large set of slices, huge arrays of int64s, and so on. But our primary goal is
    solely to change the output of pprof so that we can identify movement in the call
    graph's samples and its effect on our stack/heap profiles.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: 'Our arbitrarily expensive code looks as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: With this in place, our CPU consumption remains about the same (due to the looping
    mechanism remaining largely unchanged), but our memory allocation has increased—unsurprisingly—by
    about 900 percent. It's unlikely that you will precisely duplicate these results,
    but the general trend of a small change leading to a major difference in resource
    allocation is reproducible. Note that memory utilization reporting is possible
    with pprof, but it's not what we're doing here; the memory utilization observations
    here happened outside of pprof.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: 'If we took the extreme approach suggested previously—to create absurdly large
    properties for our struct—we could carry that out even further, but let''s see
    what the aggregate impact is on our CPU profile on execution. The impact is shown
    in the following diagram:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting deeper into pprof](img/00040.jpeg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
- en: On the left-hand side, we have our new allocation approach, which invokes our
    larger struct instead of an array of strings. On the right-hand side, we have
    our initial application.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: A pretty dramatic flux, don't you think? While neither of these programs is
    wrong in design, we can easily toggle our methodologies to see where resources
    are going and discern how we can reduce their consumption.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Parallelism's and concurrency's impact on I/O pprof
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One issue you'll likely run into pretty quickly when using pprof is when you've
    written a script or application that is especially bound to efficient runtime
    performance. This happens most frequently when your program executes too quickly
    to properly profile.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: A related issue involves network applications that require connections to profile;
    in this case, you can simulate traffic either in-program or externally to allow
    proper profiling.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 'We can demonstrate this easily by replicating something like the preceding
    example with goroutines as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The following diagram shows the pprof output of the preceding code:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '![Parallelism''s and concurrency''s impact on I/O pprof](img/00041.jpeg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
- en: It's not nearly as informative, is it?
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: If we want to get something more valuable about the stack trace of our goroutines,
    Go—as usual—provides some additional functionality.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 'In the runtime package, there is a function and a method that allow us to access
    and utilize the stack traces of our goroutines:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '`runtime.Lookup`: This function returns a profile based on name'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`runtime.WriteTo`: This method sends the snapshot to the I/O writer'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we add the following line to our program, we won't see the output in the
    `pprof` Go tool, but we can get a detailed analysis of our goroutines in the console.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The previous code line gives us some more of the abstract goroutine memory
    location information and package detail, which will look something like the following
    screenshot:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '![Parallelism''s and concurrency''s impact on I/O pprof](img/00042.jpeg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
- en: 'But an even faster way to get this output is by utilizing the `http`/`pprof`
    tool, which keeps the results of our application active via a separate server.
    We''ve gone with port 6000 here as shown in the following code, though you can
    modify this as necessary:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: While you cannot get an SVG output of the goroutine stack call, you can see
    it live in your browser by going to `http://localhost:6060/debug/pprof/goroutine?debug=1`.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Using the App Engine
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While not right for every project, Google's App Engine can open up a world of
    scalability when it comes to concurrent applications, without the hassle of VM
    provisioning, reboots, monitoring, and so on.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: The App Engine is not entirely dissimilar to Amazon Web Services, DigitalOcean,
    and the ilk, except for the fact that you do not need to necessarily involve yourself
    in the minute details of direct server setup and maintenance. All of them provide
    a single spot to acquire and utilize virtual computing resources for your applications.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Rather, it can be a more abstract environment within Google's architecture with
    which to house and run your code in a number of languages, including—no surprise
    here—the Go language itself.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: While large-scale apps will cost you, Google provides a free tier with reasonable
    quotas for experimentation and small applications.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: 'The benefits as they relate to scalability here are two-fold: you''re not responsible
    for ensuring uptime on the instances as you would be in an AWS or DigitalOcean
    scenario. Who else but Google will have not only the architecture to support anything
    you can throw at it, but also have the fastest updates to the Go core itself?'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: There are some obvious limitations here that coincide with the advantages, of
    course, including the fact that your core application will be available exclusively
    via `http` (although it will have access to plenty of other services).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To deploy apps to the App Engine, you'll need the SDK for Go, available for
    Mac OS X, Linux, and Windows, at [https://developers.google.com/appengine/downloads#Google_App_Engine_SDK_for_Go](https://developers.google.com/appengine/downloads#Google_App_Engine_SDK_for_Go).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Once you've installed the SDK, the changes you'll need to make to your code
    are minor—the most noteworthy point is that for most cases, your Go tool command
    will be supplanted by `goapp`, which handles serving your application locally
    and then deploying it.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Distributed Go
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've certainly covered a lot about concurrent and parallel Go, but one of the
    biggest infrastructure challenges for developers and system architects today has
    to do with cooperative computing.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Some of the applications and designs that we've mentioned previously scale from
    parallelism to distributed computing.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Memcache(d) is a form of in-memory caching, which can be used as a queue among
    several systems.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Our master-slave and producer-consumer models we presented in [Chapter 4](part0040_split_000.html#page
    "Chapter 4. Data Integrity in an Application"), *Data Integrity in an Application*,
    have more to do with distributed computing than single-machine programming in
    Go, which manages concurrency idiomatically. These models are typical concurrency
    models in many languages, but can be scaled to help us design distributed systems
    as well, utilizing not just many cores and vast resources but also redundancy.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: The basic premise of distributed computing is to share, spread, and best absorb
    the various burdens of any given application across many systems. This not only
    improves performance on aggregate, but provides some sense of redundancy for the
    system itself.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 'This all comes at some cost though, which are as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Potential for network latency
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating slowdowns in communication and in application execution
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall increase in complexity both in design and in maintenance
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Potential for security issues at various nodes along the distributed route(s)
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Possible added cost due to bandwidth considerations
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is all to say, simply, that while building a distributed system can provide
    great benefits to a large-scale application that utilizes concurrency and ensures
    data consistency, it's by no means right for every example.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Types of topologies
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Distributed computing recognizes a slew of logical topologies for distributed
    design. Topology is an apt metaphor, because the positioning and logic of the
    systems involved can often represent physical topology.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Out of the box, not all of the accepted topologies apply to Go. When we design
    concurrent, distributed applications using Go, we'll generally rely on a few of
    the simpler designs, which are as follows.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Type 1 – star
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The star topology (or at least this particular form of it), resembles our master-slave
    or producer-consumer models as outlined previously.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: 'The primary method of data passing involves using the master as a message-passing
    conduit; in other words, all requests and commands are coordinated by a single
    instance, which uses some routing method to pass messages. The following diagram
    shows the star topology:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '![Type 1 – star](img/00043.jpeg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
- en: 'We can actually very quickly design a goroutine-based system for this. The
    following code is solely the master''s (or distributed destination''s) code and
    lacks any sort of security considerations, but shows how we can parlay network
    calls to goroutines:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Our standard, basic libraries are defined as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'These are the two custom types we''ll use here. A `Subscriber` type is any
    distributed helper that comes into the fray, and a `Task` type represents any
    given distributable task. We''ve left that undefined here because it''s not the
    primary goal of demonstration, but you could ostensibly have `Task` do anything
    by communicating standardized commands across the TCP connection. The `Subscriber`
    type is defined as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This essentially treats every connection as a new `Subscriber`, which gets
    its own channel based on its index. This master server then iterates through existing
    `Subscriber` connections using the following very basic round-robin approach:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As mentioned previously, this lacks any sort of security model, which means
    that any connection to port 9000 would become a `Subscriber` and could get network
    messages assigned to it (and ostensibly could invoke new messages too). But you
    may have noticed an even bigger omission: this distributed application doesn''t
    do anything. Indeed, this is just a model for assignment and management of subscribers.
    Right now, it doesn''t have any path of action, but we''ll change that later in
    this chapter.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Type 2 – mesh
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The mesh is very similar to the star with one major difference: each node is
    able to communicate not just through the master, but also directly with other
    nodes as well. This is also known as a **complete graph**. The following diagram
    shows a mesh topology:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '![Type 2 – mesh](img/00044.jpeg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
- en: For practical purposes, the master must still handle assignments and pass connections
    back to the various nodes.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 'This is actually not particularly difficult to add through the following simple
    modification of our previous server code:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then, we add the following corresponding `broadcast` function to share all
    available connections to all other connections:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The Publish and Subscribe model
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In both the previous topologies, we've replicated a Publish and Subscribe model
    with a central/master handling delivery. Unlike in a single-system, concurrent
    pattern, we lack the ability to use channels directly across separate machines
    (unless we use something like Go's Circuit as described in [Chapter 4](part0040_split_000.html#page
    "Chapter 4. Data Integrity in an Application"), *Data Integrity in an Application*).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Without direct programmatic access to send and receive actual commands, we rely
    on some form of API. In the previous examples, there is no actual task being sent
    or executed, but how could we do this?
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'Obviously, to create tasks that can be formalized into non-code transmission,
    we''ll need a form of API. We can do this one of two ways: serialization of commands,
    ideally via JSONDirect transmission, and execution of code.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: As we'll always be dealing with compiled code, the serialization of commands
    option might seem like you couldn't include Go code itself. This isn't exactly
    true, but passing full code in any language is fairly high on lists of security
    concerns.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: 'But let''s look at two ways of sending data via API in a task by removing a
    URL from a slice of URLs for retrieval. We''ll first need to initialize that array
    in our `main` function as shown in the following code:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Every URL in our array will include the URI, its status, and the subscriber
    address to which it's been assigned. We'll formalize the status points as 0 for
    unassigned, 1 for assigned and waiting, and 2 for assigned and complete.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Remember our `CurrentSubscriber` iterator? That represents the next-in-line
    round robin assignment which will fulfill the `SubscriberID` value for our `URL`
    struct.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we''ll create an arbitrary array of URLs that will represent our overall
    job here. Some suspension of incredulity may be necessary to assume that the retrieval
    of four URLs should require any distributed system; in reality, this would introduce
    significant slowdown by virtue of network transmission. We''ve handled this in
    a purely single-system, concurrent application before:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Serialized data
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In our first option in the API, we''ll send and receive serialized data in
    JSON. Our master will be responsible for formalizing its command and associated
    data. In this case, we''ll want to transmit a few things: what to do (in this
    case, retrieve) with the relevant data, what the response should be when it is
    complete, and how to address errors.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: 'We can represent this in a custom struct as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Remote code execution
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The remote code execution option is not necessarily separate from serialization
    of commands, but instead of structured and interpreted formatted responses, the
    payload could be code that will be run via a system command.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, code from any language could be passed through the network and
    executed from a shell or from a syscall library in another language, like the
    following Python example:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The disadvantages to this approach are many: it introduces serious security
    issues and makes error detection within your client nearly impossible.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: The advantages are you do not need to come up with a specific format and interpreter
    for responses as well as potential speed improvements. You can also offload the
    response code to another external process in any number of languages.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: In most cases, serialization of commands is far preferable over the remote code
    execution option.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Other topologies
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There exist quite a few topology types that are more complicated to manage as
    part of a messaging queue.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the bus topology:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '![Other topologies](img/00045.jpeg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
- en: The bus topology network is a unidirectional transmission system. For our purposes,
    it's neither particularly useful nor easily managed, as each added node needs
    to announce its availability, accept listener responsibility, and be ready to
    cede that responsibility when a new node joins.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 'The advantage of a bus is quick scalability. This comes with serious disadvantages
    though: lack of redundancy and single point of failure.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Even with a more complex topology, there will always be some issue with potentially
    losing a valuable cog in the system; at this level of modular redundancy, some
    additional steps will be necessary to have an always-available system, including
    automatic double or triple node replication and failovers. That's a bit more than
    we'll get into here, but it's important to note that the risk will be there in
    any event, although it would be a little more vulnerable with a topology like
    the bus.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the ring topology:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '![Other topologies](img/00046.jpeg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
- en: 'The ring topology looks similar to our mesh topology, but lacks a master. It
    essentially requires the same communication process (announce and listen) as does
    a bus. Note one significant difference: instead of a single listener, communication
    can happen between any node without the master.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: This simply means that all nodes must both listen and announce their presence
    to other nodes.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Message Passing Interface
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There exists a slightly more formalized version of what we built previously,
    called Message Passing Interface. MPI was borne from early 1990s academia as a
    standard for distributed communication.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Originally written with FORTRAN and C in mind, it is still a protocol, so it's
    largely language agnostic.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: MPI allows the management of topology above and beyond the basic topologies
    we were able to build for a resource management system, including not only the
    line and ring but also the common bus topology.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: For the most part, MPI is used by the scientific community; it is a highly concurrent
    and analogous method for building large-scale distributed systems. Point-to-point
    operations are more rigorously defined with error handling, retries, and dynamic
    spawning of processes all built in.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Our previous basic examples lend no prioritization to processors, for example,
    and this is a core effect of MPI.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: There is no official implementation of MPI for Go, but as there exists one for
    both C and C++, it's entirely possible to interface with it through that.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There is also a simple and incomplete binding written in Go by Marcus Thierfelder
    that you can experiment with. It is available at [https://github.com/marcusthierfelder/mpi](https://github.com/marcusthierfelder/mpi).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: You can read more about and install OpenMPI from [http://www.open-mpi.org/](http://www.open-mpi.org/).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Also you can read more about MPI and MPICH implementations at [http://www.mpich.org/](http://www.mpich.org/).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Some helpful libraries
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There's little doubt that Go provides some of the best ancillary tools available
    to any compiled language out there. Compiling to native code on a myriad of systems,
    deadlock detection, pprof, fmt, and more allow you to not just build high-performance
    applications, but also test them and format them.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: This hasn't stopped the community from developing other tools that can be used
    for debugging or aiding your concurrent and/or distributed code. We'll take a
    look at a few great tools that may prove worthy of inclusion in your app, particularly
    if it's highly visible or performance critical.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Nitro profiler
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you are probably now well aware, Go's pprof is extremely powerful and useful,
    if not exactly user-friendly.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: If you love pprof already, or even if you find it arduous and confusing, you
    may love Nitro profiler twice as much. Coming from Steve Francia of spf13, Nitro
    profiler allows you to produce even cleaner analyses of your application and its
    functions and steps, as well as providing more usable a/b tests of alternate functions.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Read more about Nitro profiler at [http://spf13.com/project/nitro](http://spf13.com/project/nitro).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: You can get it via [github.com/spf13/nitro](http://github.com/spf13/nitro).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: As with pprof, Nitro automatically injects flags into your application, and
    you'll see them in the results themselves.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Unlike pprof, your application does not need to be compiled to get profile analysis
    from it. Instead, you can simply append `-stepAnalysis` to the `go run` command.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Heka
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Heka is a data pipeline tool that can be used to gather, analyze, and distribute
    raw data. Available from Mozilla, Heka is more a standalone application rather
    than a library, but when it comes to acquiring, analyzing, and distributing data
    such as server logfiles across multiple servers, Heka can prove itself worthy.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Heka is also written in Go, so make sure to check out the source to see how
    Mozilla utilizes concurrency and Go in real-time data analysis.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can visit the Heka home page at [http://heka-docs.readthedocs.org/en/latest/](http://heka-docs.readthedocs.org/en/latest/)
    and the Heka source page at [https://github.com/mozilla-services/heka](https://github.com/mozilla-services/heka).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: GoFlow
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, there's GoFlow, a flow-based programming paradigm tool that lets you
    segment your application into distinct components, each capable of being bound
    to ports, channels, the network, or processes.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: While not itself a performance tool, GoFlow might be an appropriate approach
    to extending concurrency for some applications.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Visit GoFlow at [https://github.com/trustmaster/goflow](https://github.com/trustmaster/goflow).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Memory preservation
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the time of this writing, Go 1.2.2's compiler utilizes a naive mark/sweep
    garbage collector, which assigns a reference rank to objects and clears them when
    they are no longer in use. This is noteworthy only to point out that it is widely
    considered a relatively poor garbage collection system.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'So why does Go use it? As Go has evolved; language features and compiler speed
    have largely taken precedence over garbage collection. While it''s a long-term
    development timeline for Go, for the time being, this is where we are. The tradeoff
    is a good one, though: as you well know by now, compiling Go code is light years
    faster than, say, compiling C or C++ code. Good enough for now is a fair description
    for the GC. But there are some things you can do to augment and experiment within
    the garbage collection system.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Garbage collection in Go
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To get an idea of how the garbage collector is managing the stack at any time,
    take a look at the `runtime.MemProfileRecord` object, which keeps track of presently
    living objects in the active stack trace.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'You can call the profile record when necessary and then utilize it against
    the following methods to get a few interesting pieces of data:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '`InUseBytes()`: This method has the bytes used presently as per the memory
    profile'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`InUseObjects()`:This method has the number of live objects in use'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`InUseObjects()`:该方法返回正在使用的活动对象的数量'
- en: '`Stack()`: This method has the full stack trace'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Stack()`:该方法返回完整的堆栈跟踪'
- en: 'You can place the following code in a heavy loop in your application to get
    a peek at all of these:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将以下代码放入应用程序的重循环中，以查看所有这些内容：
- en: '[PRE16]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Summary
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We can now build some pretty high-performance applications and then utilize
    some of Go's built-in tools and third-party packages to seek out the most performance
    in a single instance application as well as across multiple, distributed systems.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以构建一些非常高性能的应用程序，然后利用一些Go内置工具和第三方包，以在单个实例应用程序以及跨多个分布式系统中寻求最佳性能。
- en: In the next chapter, we're going to wrap everything together to design and build
    a concurrent server application that can work quickly and independently, and easily
    scale in performance and scope.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将把所有内容整合起来，设计并构建一个并发服务器应用程序，它可以快速独立地工作，并且可以轻松地在性能和范围上进行扩展。
