- en: Chapter 7. Performance and Scalability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To build a high-powered web server in Go with just a few hundred lines of code,
    you should be quite aware of how concurrent Go provides us with exceptional tools
    for performance and stability out of the box.
  prefs: []
  type: TYPE_NORMAL
- en: Our example in [Chapter 6](part0054_split_000.html#page "Chapter 6. C10K – A
    Non-blocking Web Server in Go"), *C10K – A Non-blocking Web Server in Go*, also
    showed how imposing blocking code arbitrarily or inadvertently into our code can
    introduce some serious bottlenecks and quickly torpedo any plans to extend or
    scale your application.
  prefs: []
  type: TYPE_NORMAL
- en: What we'll look at in this chapter are a few ways that can better prepare us
    to take our concurrent application and ensure that it's able to continuously scale
    in the future and that it is capable of being expanded in scope, design, and/or
    capacity.
  prefs: []
  type: TYPE_NORMAL
- en: We'll expand a bit on **pprof**, the CPU profiling tool we looked at briefly
    in previous chapters, as a way to elucidate the way our Go code is compiled and
    to locate possible unintended bottlenecks.
  prefs: []
  type: TYPE_NORMAL
- en: Then we'll expand into distributed Go and into ways to offer some performance-enhancing
    parallel-computing concepts to our applications. We'll also look at the Google
    App Engine, and at how you can utilize it for your Go-based applications to ensure
    scalability is placed in the hands of one of the most reliable hosting infrastructures
    in the world.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we'll look at memory utilization, preservation, and how Google's garbage
    collector works (and sometimes doesn't). We'll finally delve a bit deeper into
    using memory caching to keep data consistent as well as less ephemeral, and we
    will also see how that dovetails with distributed computing in general.
  prefs: []
  type: TYPE_NORMAL
- en: High performance in Go
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up to this point, we've talked about some of the tools we can use to help discover
    slowdowns, leaks, and inefficient looping.
  prefs: []
  type: TYPE_NORMAL
- en: Go's compiler and its built-in deadlock detector keep us from making the kind
    of mistake that's common and difficult to detect in other languages.
  prefs: []
  type: TYPE_NORMAL
- en: We've run time-based benchmarks based on specific changes to our concurrency
    patterns, which can help us design our application using different methodologies
    to improve overall execution speed and performance.
  prefs: []
  type: TYPE_NORMAL
- en: Getting deeper into pprof
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The pprof tool was first encountered in [Chapter 5](part0048_split_000.html#page
    "Chapter 5. Locks, Blocks, and Better Channels"), *Locks, Blocks, and Better Channels*,
    and if it still feels a bit cryptic, that's totally understandable. What pprof
    shows you in export is a **call graph**, and we can use this to help identify
    issues with loops or expensive calls on the heap. These include memory leaks and
    processor-intensive methods that can be optimized.
  prefs: []
  type: TYPE_NORMAL
- en: One of the best ways to demonstrate how something like this works is to build
    something that doesn't. Or at least something that doesn't work the way it should.
  prefs: []
  type: TYPE_NORMAL
- en: You might be thinking that a language with garbage collection might be immune
    to these kinds of memory issues, but there are always ways to hide mistakes that
    can lead to memory leakage. If the GC can't find it, it can sometimes be a real
    pain to do so yourself, leading to a lot of—often feckless—debugging.
  prefs: []
  type: TYPE_NORMAL
- en: To be fair, what constitutes a memory leak is sometimes debated among computer
    science members and experts. A program that continuously consumes RAM may not
    be leaking memory by technical definition if the application itself could re-access
    any given pointers. But that's largely irrelevant when you have a program that
    crashes and burns after consuming memory like an elephant at a buffet.
  prefs: []
  type: TYPE_NORMAL
- en: The basic premise of creating a memory leak in a garbage-collected language
    relies on hiding the allocation from the compiler—indeed, any language in which
    you can access and utilize memory directly provides a mechanism for introducing
    leaks.
  prefs: []
  type: TYPE_NORMAL
- en: We'll review a bit more about garbage collection and Go's implementation later
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: So how does a tool like pprof help? Very simply put, by showing you **where**
    your memory and CPU utilization goes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first design a very obvious CPU hog as follows to see how pprof highlights
    this for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting deeper into pprof](img/00039.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In this case, we know where our stack resource allocation is going, because
    we willfully introduced the loop (and the loop within that loop).
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that we didn't intentionally do that and had to locate resource hogs.
    In this case, pprof makes this pretty easy, showing us the creation and memory
    allocation of simple strings comprising the majority of our samples.
  prefs: []
  type: TYPE_NORMAL
- en: We can modify this slightly to see the changes in the pprof output. In an effort
    to allocate more and more memory to see whether we can vary the pprof output,
    we might consider heavier types and more memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'The easiest way to accomplish that is to create a slice of a new type that
    includes a significant amount of these heavier types such as int64\. We''re blessed
    with Go: in that, we aren''t prone to common C issues such as buffer overflows
    and memory protection and management, but this makes debugging a little trickier
    when we cannot intentionally break the memory management system.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**The unsafe package**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite the built-in memory protection provided, there is still another interesting
    tool provided by Go: the **unsafe** package. As per Go''s documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Package unsafe contains operations that step around the type safety of Go
    programs.*'
  prefs: []
  type: TYPE_NORMAL
- en: This might seem like a curious library to include—indeed, while many low-level
    languages allow you to shoot your foot off, it's fairly unusual to provide a segregated
    language.
  prefs: []
  type: TYPE_NORMAL
- en: Later in this chapter, we'll examine `unsafe.Pointer`, which allows you to read
    and write to arbitrary bits of memory allocation. This is obviously extraordinarily
    dangerous (or useful and nefarious, depending on your goal) functionality that
    you would generally try to avoid in any development language, but it does allow
    us to debug and understand our programs and the Go garbage collector a bit better.
  prefs: []
  type: TYPE_NORMAL
- en: 'So to increase our memory usage, let''s switch our string allocation as follows,
    for random type allocation, specifically for our new struct `MemoryHog`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: There's obviously nothing preventing us from extending this into some ludicrously
    large set of slices, huge arrays of int64s, and so on. But our primary goal is
    solely to change the output of pprof so that we can identify movement in the call
    graph's samples and its effect on our stack/heap profiles.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our arbitrarily expensive code looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: With this in place, our CPU consumption remains about the same (due to the looping
    mechanism remaining largely unchanged), but our memory allocation has increased—unsurprisingly—by
    about 900 percent. It's unlikely that you will precisely duplicate these results,
    but the general trend of a small change leading to a major difference in resource
    allocation is reproducible. Note that memory utilization reporting is possible
    with pprof, but it's not what we're doing here; the memory utilization observations
    here happened outside of pprof.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we took the extreme approach suggested previously—to create absurdly large
    properties for our struct—we could carry that out even further, but let''s see
    what the aggregate impact is on our CPU profile on execution. The impact is shown
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting deeper into pprof](img/00040.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: On the left-hand side, we have our new allocation approach, which invokes our
    larger struct instead of an array of strings. On the right-hand side, we have
    our initial application.
  prefs: []
  type: TYPE_NORMAL
- en: A pretty dramatic flux, don't you think? While neither of these programs is
    wrong in design, we can easily toggle our methodologies to see where resources
    are going and discern how we can reduce their consumption.
  prefs: []
  type: TYPE_NORMAL
- en: Parallelism's and concurrency's impact on I/O pprof
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One issue you'll likely run into pretty quickly when using pprof is when you've
    written a script or application that is especially bound to efficient runtime
    performance. This happens most frequently when your program executes too quickly
    to properly profile.
  prefs: []
  type: TYPE_NORMAL
- en: A related issue involves network applications that require connections to profile;
    in this case, you can simulate traffic either in-program or externally to allow
    proper profiling.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can demonstrate this easily by replicating something like the preceding
    example with goroutines as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram shows the pprof output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Parallelism''s and concurrency''s impact on I/O pprof](img/00041.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: It's not nearly as informative, is it?
  prefs: []
  type: TYPE_NORMAL
- en: If we want to get something more valuable about the stack trace of our goroutines,
    Go—as usual—provides some additional functionality.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the runtime package, there is a function and a method that allow us to access
    and utilize the stack traces of our goroutines:'
  prefs: []
  type: TYPE_NORMAL
- en: '`runtime.Lookup`: This function returns a profile based on name'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`runtime.WriteTo`: This method sends the snapshot to the I/O writer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we add the following line to our program, we won't see the output in the
    `pprof` Go tool, but we can get a detailed analysis of our goroutines in the console.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous code line gives us some more of the abstract goroutine memory
    location information and package detail, which will look something like the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Parallelism''s and concurrency''s impact on I/O pprof](img/00042.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'But an even faster way to get this output is by utilizing the `http`/`pprof`
    tool, which keeps the results of our application active via a separate server.
    We''ve gone with port 6000 here as shown in the following code, though you can
    modify this as necessary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: While you cannot get an SVG output of the goroutine stack call, you can see
    it live in your browser by going to `http://localhost:6060/debug/pprof/goroutine?debug=1`.
  prefs: []
  type: TYPE_NORMAL
- en: Using the App Engine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While not right for every project, Google's App Engine can open up a world of
    scalability when it comes to concurrent applications, without the hassle of VM
    provisioning, reboots, monitoring, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: The App Engine is not entirely dissimilar to Amazon Web Services, DigitalOcean,
    and the ilk, except for the fact that you do not need to necessarily involve yourself
    in the minute details of direct server setup and maintenance. All of them provide
    a single spot to acquire and utilize virtual computing resources for your applications.
  prefs: []
  type: TYPE_NORMAL
- en: Rather, it can be a more abstract environment within Google's architecture with
    which to house and run your code in a number of languages, including—no surprise
    here—the Go language itself.
  prefs: []
  type: TYPE_NORMAL
- en: While large-scale apps will cost you, Google provides a free tier with reasonable
    quotas for experimentation and small applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'The benefits as they relate to scalability here are two-fold: you''re not responsible
    for ensuring uptime on the instances as you would be in an AWS or DigitalOcean
    scenario. Who else but Google will have not only the architecture to support anything
    you can throw at it, but also have the fastest updates to the Go core itself?'
  prefs: []
  type: TYPE_NORMAL
- en: There are some obvious limitations here that coincide with the advantages, of
    course, including the fact that your core application will be available exclusively
    via `http` (although it will have access to plenty of other services).
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To deploy apps to the App Engine, you'll need the SDK for Go, available for
    Mac OS X, Linux, and Windows, at [https://developers.google.com/appengine/downloads#Google_App_Engine_SDK_for_Go](https://developers.google.com/appengine/downloads#Google_App_Engine_SDK_for_Go).
  prefs: []
  type: TYPE_NORMAL
- en: Once you've installed the SDK, the changes you'll need to make to your code
    are minor—the most noteworthy point is that for most cases, your Go tool command
    will be supplanted by `goapp`, which handles serving your application locally
    and then deploying it.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed Go
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've certainly covered a lot about concurrent and parallel Go, but one of the
    biggest infrastructure challenges for developers and system architects today has
    to do with cooperative computing.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the applications and designs that we've mentioned previously scale from
    parallelism to distributed computing.
  prefs: []
  type: TYPE_NORMAL
- en: Memcache(d) is a form of in-memory caching, which can be used as a queue among
    several systems.
  prefs: []
  type: TYPE_NORMAL
- en: Our master-slave and producer-consumer models we presented in [Chapter 4](part0040_split_000.html#page
    "Chapter 4. Data Integrity in an Application"), *Data Integrity in an Application*,
    have more to do with distributed computing than single-machine programming in
    Go, which manages concurrency idiomatically. These models are typical concurrency
    models in many languages, but can be scaled to help us design distributed systems
    as well, utilizing not just many cores and vast resources but also redundancy.
  prefs: []
  type: TYPE_NORMAL
- en: The basic premise of distributed computing is to share, spread, and best absorb
    the various burdens of any given application across many systems. This not only
    improves performance on aggregate, but provides some sense of redundancy for the
    system itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'This all comes at some cost though, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Potential for network latency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating slowdowns in communication and in application execution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall increase in complexity both in design and in maintenance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Potential for security issues at various nodes along the distributed route(s)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Possible added cost due to bandwidth considerations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is all to say, simply, that while building a distributed system can provide
    great benefits to a large-scale application that utilizes concurrency and ensures
    data consistency, it's by no means right for every example.
  prefs: []
  type: TYPE_NORMAL
- en: Types of topologies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Distributed computing recognizes a slew of logical topologies for distributed
    design. Topology is an apt metaphor, because the positioning and logic of the
    systems involved can often represent physical topology.
  prefs: []
  type: TYPE_NORMAL
- en: Out of the box, not all of the accepted topologies apply to Go. When we design
    concurrent, distributed applications using Go, we'll generally rely on a few of
    the simpler designs, which are as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Type 1 – star
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The star topology (or at least this particular form of it), resembles our master-slave
    or producer-consumer models as outlined previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'The primary method of data passing involves using the master as a message-passing
    conduit; in other words, all requests and commands are coordinated by a single
    instance, which uses some routing method to pass messages. The following diagram
    shows the star topology:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Type 1 – star](img/00043.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can actually very quickly design a goroutine-based system for this. The
    following code is solely the master''s (or distributed destination''s) code and
    lacks any sort of security considerations, but shows how we can parlay network
    calls to goroutines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Our standard, basic libraries are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'These are the two custom types we''ll use here. A `Subscriber` type is any
    distributed helper that comes into the fray, and a `Task` type represents any
    given distributable task. We''ve left that undefined here because it''s not the
    primary goal of demonstration, but you could ostensibly have `Task` do anything
    by communicating standardized commands across the TCP connection. The `Subscriber`
    type is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This essentially treats every connection as a new `Subscriber`, which gets
    its own channel based on its index. This master server then iterates through existing
    `Subscriber` connections using the following very basic round-robin approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'As mentioned previously, this lacks any sort of security model, which means
    that any connection to port 9000 would become a `Subscriber` and could get network
    messages assigned to it (and ostensibly could invoke new messages too). But you
    may have noticed an even bigger omission: this distributed application doesn''t
    do anything. Indeed, this is just a model for assignment and management of subscribers.
    Right now, it doesn''t have any path of action, but we''ll change that later in
    this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: Type 2 – mesh
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The mesh is very similar to the star with one major difference: each node is
    able to communicate not just through the master, but also directly with other
    nodes as well. This is also known as a **complete graph**. The following diagram
    shows a mesh topology:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Type 2 – mesh](img/00044.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: For practical purposes, the master must still handle assignments and pass connections
    back to the various nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is actually not particularly difficult to add through the following simple
    modification of our previous server code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we add the following corresponding `broadcast` function to share all
    available connections to all other connections:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The Publish and Subscribe model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In both the previous topologies, we've replicated a Publish and Subscribe model
    with a central/master handling delivery. Unlike in a single-system, concurrent
    pattern, we lack the ability to use channels directly across separate machines
    (unless we use something like Go's Circuit as described in [Chapter 4](part0040_split_000.html#page
    "Chapter 4. Data Integrity in an Application"), *Data Integrity in an Application*).
  prefs: []
  type: TYPE_NORMAL
- en: Without direct programmatic access to send and receive actual commands, we rely
    on some form of API. In the previous examples, there is no actual task being sent
    or executed, but how could we do this?
  prefs: []
  type: TYPE_NORMAL
- en: 'Obviously, to create tasks that can be formalized into non-code transmission,
    we''ll need a form of API. We can do this one of two ways: serialization of commands,
    ideally via JSONDirect transmission, and execution of code.'
  prefs: []
  type: TYPE_NORMAL
- en: As we'll always be dealing with compiled code, the serialization of commands
    option might seem like you couldn't include Go code itself. This isn't exactly
    true, but passing full code in any language is fairly high on lists of security
    concerns.
  prefs: []
  type: TYPE_NORMAL
- en: 'But let''s look at two ways of sending data via API in a task by removing a
    URL from a slice of URLs for retrieval. We''ll first need to initialize that array
    in our `main` function as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Every URL in our array will include the URI, its status, and the subscriber
    address to which it's been assigned. We'll formalize the status points as 0 for
    unassigned, 1 for assigned and waiting, and 2 for assigned and complete.
  prefs: []
  type: TYPE_NORMAL
- en: Remember our `CurrentSubscriber` iterator? That represents the next-in-line
    round robin assignment which will fulfill the `SubscriberID` value for our `URL`
    struct.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we''ll create an arbitrary array of URLs that will represent our overall
    job here. Some suspension of incredulity may be necessary to assume that the retrieval
    of four URLs should require any distributed system; in reality, this would introduce
    significant slowdown by virtue of network transmission. We''ve handled this in
    a purely single-system, concurrent application before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Serialized data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In our first option in the API, we''ll send and receive serialized data in
    JSON. Our master will be responsible for formalizing its command and associated
    data. In this case, we''ll want to transmit a few things: what to do (in this
    case, retrieve) with the relevant data, what the response should be when it is
    complete, and how to address errors.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can represent this in a custom struct as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Remote code execution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The remote code execution option is not necessarily separate from serialization
    of commands, but instead of structured and interpreted formatted responses, the
    payload could be code that will be run via a system command.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, code from any language could be passed through the network and
    executed from a shell or from a syscall library in another language, like the
    following Python example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The disadvantages to this approach are many: it introduces serious security
    issues and makes error detection within your client nearly impossible.'
  prefs: []
  type: TYPE_NORMAL
- en: The advantages are you do not need to come up with a specific format and interpreter
    for responses as well as potential speed improvements. You can also offload the
    response code to another external process in any number of languages.
  prefs: []
  type: TYPE_NORMAL
- en: In most cases, serialization of commands is far preferable over the remote code
    execution option.
  prefs: []
  type: TYPE_NORMAL
- en: Other topologies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There exist quite a few topology types that are more complicated to manage as
    part of a messaging queue.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the bus topology:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Other topologies](img/00045.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The bus topology network is a unidirectional transmission system. For our purposes,
    it's neither particularly useful nor easily managed, as each added node needs
    to announce its availability, accept listener responsibility, and be ready to
    cede that responsibility when a new node joins.
  prefs: []
  type: TYPE_NORMAL
- en: 'The advantage of a bus is quick scalability. This comes with serious disadvantages
    though: lack of redundancy and single point of failure.'
  prefs: []
  type: TYPE_NORMAL
- en: Even with a more complex topology, there will always be some issue with potentially
    losing a valuable cog in the system; at this level of modular redundancy, some
    additional steps will be necessary to have an always-available system, including
    automatic double or triple node replication and failovers. That's a bit more than
    we'll get into here, but it's important to note that the risk will be there in
    any event, although it would be a little more vulnerable with a topology like
    the bus.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the ring topology:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Other topologies](img/00046.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The ring topology looks similar to our mesh topology, but lacks a master. It
    essentially requires the same communication process (announce and listen) as does
    a bus. Note one significant difference: instead of a single listener, communication
    can happen between any node without the master.'
  prefs: []
  type: TYPE_NORMAL
- en: This simply means that all nodes must both listen and announce their presence
    to other nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Message Passing Interface
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There exists a slightly more formalized version of what we built previously,
    called Message Passing Interface. MPI was borne from early 1990s academia as a
    standard for distributed communication.
  prefs: []
  type: TYPE_NORMAL
- en: Originally written with FORTRAN and C in mind, it is still a protocol, so it's
    largely language agnostic.
  prefs: []
  type: TYPE_NORMAL
- en: MPI allows the management of topology above and beyond the basic topologies
    we were able to build for a resource management system, including not only the
    line and ring but also the common bus topology.
  prefs: []
  type: TYPE_NORMAL
- en: For the most part, MPI is used by the scientific community; it is a highly concurrent
    and analogous method for building large-scale distributed systems. Point-to-point
    operations are more rigorously defined with error handling, retries, and dynamic
    spawning of processes all built in.
  prefs: []
  type: TYPE_NORMAL
- en: Our previous basic examples lend no prioritization to processors, for example,
    and this is a core effect of MPI.
  prefs: []
  type: TYPE_NORMAL
- en: There is no official implementation of MPI for Go, but as there exists one for
    both C and C++, it's entirely possible to interface with it through that.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There is also a simple and incomplete binding written in Go by Marcus Thierfelder
    that you can experiment with. It is available at [https://github.com/marcusthierfelder/mpi](https://github.com/marcusthierfelder/mpi).
  prefs: []
  type: TYPE_NORMAL
- en: You can read more about and install OpenMPI from [http://www.open-mpi.org/](http://www.open-mpi.org/).
  prefs: []
  type: TYPE_NORMAL
- en: Also you can read more about MPI and MPICH implementations at [http://www.mpich.org/](http://www.mpich.org/).
  prefs: []
  type: TYPE_NORMAL
- en: Some helpful libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There's little doubt that Go provides some of the best ancillary tools available
    to any compiled language out there. Compiling to native code on a myriad of systems,
    deadlock detection, pprof, fmt, and more allow you to not just build high-performance
    applications, but also test them and format them.
  prefs: []
  type: TYPE_NORMAL
- en: This hasn't stopped the community from developing other tools that can be used
    for debugging or aiding your concurrent and/or distributed code. We'll take a
    look at a few great tools that may prove worthy of inclusion in your app, particularly
    if it's highly visible or performance critical.
  prefs: []
  type: TYPE_NORMAL
- en: Nitro profiler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you are probably now well aware, Go's pprof is extremely powerful and useful,
    if not exactly user-friendly.
  prefs: []
  type: TYPE_NORMAL
- en: If you love pprof already, or even if you find it arduous and confusing, you
    may love Nitro profiler twice as much. Coming from Steve Francia of spf13, Nitro
    profiler allows you to produce even cleaner analyses of your application and its
    functions and steps, as well as providing more usable a/b tests of alternate functions.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Read more about Nitro profiler at [http://spf13.com/project/nitro](http://spf13.com/project/nitro).
  prefs: []
  type: TYPE_NORMAL
- en: You can get it via [github.com/spf13/nitro](http://github.com/spf13/nitro).
  prefs: []
  type: TYPE_NORMAL
- en: As with pprof, Nitro automatically injects flags into your application, and
    you'll see them in the results themselves.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike pprof, your application does not need to be compiled to get profile analysis
    from it. Instead, you can simply append `-stepAnalysis` to the `go run` command.
  prefs: []
  type: TYPE_NORMAL
- en: Heka
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Heka is a data pipeline tool that can be used to gather, analyze, and distribute
    raw data. Available from Mozilla, Heka is more a standalone application rather
    than a library, but when it comes to acquiring, analyzing, and distributing data
    such as server logfiles across multiple servers, Heka can prove itself worthy.
  prefs: []
  type: TYPE_NORMAL
- en: Heka is also written in Go, so make sure to check out the source to see how
    Mozilla utilizes concurrency and Go in real-time data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can visit the Heka home page at [http://heka-docs.readthedocs.org/en/latest/](http://heka-docs.readthedocs.org/en/latest/)
    and the Heka source page at [https://github.com/mozilla-services/heka](https://github.com/mozilla-services/heka).
  prefs: []
  type: TYPE_NORMAL
- en: GoFlow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, there's GoFlow, a flow-based programming paradigm tool that lets you
    segment your application into distinct components, each capable of being bound
    to ports, channels, the network, or processes.
  prefs: []
  type: TYPE_NORMAL
- en: While not itself a performance tool, GoFlow might be an appropriate approach
    to extending concurrency for some applications.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Visit GoFlow at [https://github.com/trustmaster/goflow](https://github.com/trustmaster/goflow).
  prefs: []
  type: TYPE_NORMAL
- en: Memory preservation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the time of this writing, Go 1.2.2's compiler utilizes a naive mark/sweep
    garbage collector, which assigns a reference rank to objects and clears them when
    they are no longer in use. This is noteworthy only to point out that it is widely
    considered a relatively poor garbage collection system.
  prefs: []
  type: TYPE_NORMAL
- en: 'So why does Go use it? As Go has evolved; language features and compiler speed
    have largely taken precedence over garbage collection. While it''s a long-term
    development timeline for Go, for the time being, this is where we are. The tradeoff
    is a good one, though: as you well know by now, compiling Go code is light years
    faster than, say, compiling C or C++ code. Good enough for now is a fair description
    for the GC. But there are some things you can do to augment and experiment within
    the garbage collection system.'
  prefs: []
  type: TYPE_NORMAL
- en: Garbage collection in Go
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To get an idea of how the garbage collector is managing the stack at any time,
    take a look at the `runtime.MemProfileRecord` object, which keeps track of presently
    living objects in the active stack trace.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can call the profile record when necessary and then utilize it against
    the following methods to get a few interesting pieces of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '`InUseBytes()`: This method has the bytes used presently as per the memory
    profile'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`InUseObjects()`:This method has the number of live objects in use'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Stack()`: This method has the full stack trace'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can place the following code in a heavy loop in your application to get
    a peek at all of these:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can now build some pretty high-performance applications and then utilize
    some of Go's built-in tools and third-party packages to seek out the most performance
    in a single instance application as well as across multiple, distributed systems.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we're going to wrap everything together to design and build
    a concurrent server application that can work quickly and independently, and easily
    scale in performance and scope.
  prefs: []
  type: TYPE_NORMAL
