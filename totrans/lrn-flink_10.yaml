- en: Chapter 10. Best Practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far in this book, we have learned various things about Flink. We started
    with Flink's architecture and the various APIs it supports. We also learned how
    we use graph and machine learning APIs provided by Flink. Now in this concluding
    chapter, we are going to talk about some best practices you should follow in order
    to create production quality maintainable Flink applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be discussing about the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Logging best practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using custom serializers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using and monitoring the REST API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Back pressure monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So let's get started.
  prefs: []
  type: TYPE_NORMAL
- en: Logging best practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is very important to have logs configured in any software application. Logs
    help in debugging the issues.  We don’t follow these logging practices, it would
    be very diffuclt to understand the progress of the job or if any issues with it.
    There are couple of libraries we can use for better logging experience.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Log4j
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Log4j, as we know, is one the most widely used logging libraries. We can configure
    it in any Flink application with very little effort. We have only to include a
    `log4j.properties` file. We can pass the `log4j.properties` file by passing it
    as an `Dlog4j.configuration=/path/to/log4j.properties` argument.
  prefs: []
  type: TYPE_NORMAL
- en: 'Flink supports the following default property files:'
  prefs: []
  type: TYPE_NORMAL
- en: '`log4j-cli.properties`: This file is used by the Flink command line tool. Here
    is the exact file at [https://github.com/apache/flink/blob/master/flink-dist/src/main/flink-bin/conf/log4j-cli.properties](https://github.com/apache/flink/blob/master/flink-dist/src/main/flink-bin/conf/log4j-cli.properties).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`log4j-yarn-session.properties`: This file is used by the Flink YARN session.
    Here is the exact file at [https://github.com/apache/flink/blob/master/flink-dist/src/main/flink-bin/conf/log4j-yarn-session.properties](https://github.com/apache/flink/blob/master/flink-dist/src/main/flink-bin/conf/log4j-yarn-session.properties).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`log4j.properties`: This file is used by the Flink Job Manager and Task Manager.
    Here is the exact file at [https://github.com/apache/flink/blob/master/flink-dist/src/main/flink-bin/conf/log4j.properties](https://github.com/apache/flink/blob/master/flink-dist/src/main/flink-bin/conf/log4j.properties).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring Logback
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: These days a lot of people prefer using Logback over Log4j because of it features.
    Logback provides faster I/O, thoroughly tested libraries, extensive documentation
    etc.  Flink also supports configuring Logback for an application.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to use the same property to configure `logback.xml`. `Dlogback.configurationFile=<file>`,
    or we can also put the `logback.xml` file in the class path. A sample `logback.xml`
    would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We can always change the `logback.xml` file and set the logging level according
    to our preferences.
  prefs: []
  type: TYPE_NORMAL
- en: Logging in applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While using SLF4J in any Flink application, we need to import the following
    package and classes, and initiate the logger with the class name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'It is also a best practice to use a placeholder mechanism for logging instead
    of using a string formatter. The placeholder mechanism helps to avoid unnecessary
    string formations instead it only does string concatenation. The following code
    snippet shows how to use a placeholder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also use placeholder logging in exception handling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Using ParameterTool
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since Flink 0.9, we have a built-in `ParameterTool` in Flink, which helps to
    get parameters from external sources such as arguments, system properties, or
    from property files. Internally, it is a map of strings which keeps the key as
    the parameter name and the value as the parameter value.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we can think of using ParameterTool in our DataStream API example,
    where we need to set Kafka properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: From system properties
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can read properties defined in system variables. We need to pass the system
    properties file before initializing them by setting `Dinput=hdfs://myfile`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can read all those properties in `ParameterTool` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: From command line arguments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can also read the parameters from command line arguments. We have to set
    `--elements` before invoking the application.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows how to read parameters from command line arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: From .properties file
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can also read the parameters from the `.properties` file. The following
    is the code for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We can read the parameters in the Flink program. The following shows how we
    get the parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Naming large TupleX types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we know, a tuple is a complex data type used to represent complex data structures.
    It is a combination of various primitive data types. Generally, it is recommended
    not to use large tuples; instead it is recommended to use Java POJOs. If you want
    to use a tuple, it is recommended to name it with some custom POJO type.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is very easy to create a custom type for a large tuple. For example, if
    we want to use `Tuple8` then we can define it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Registering a custom serializer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the distributed computing world, it is very important to take care of each
    and every small thing. Serialization is one of them. By default, Flink uses the
    Kryo serializer. Flink also allows us to write custom serializers in case you
    think the default one is not good enough. We need to register the custom serializer
    in order for Flink to understand it. Registering the custom serializer is very
    simple; we just need to register its class type in the Flink execution environment.
    The following code snippet shows how we do that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Here is a complete sample class for Custom Serializer at [https://github.com/deshpandetanmay/mastering-flink/blob/master/chapter10/flink-batch-adv/src/main/java/com/demo/flink/batch/RecordSerializer.java](https://github.com/deshpandetanmay/mastering-flink/blob/master/chapter10/flink-batch-adv/src/main/java/com/demo/flink/batch/RecordSerializer.java).
  prefs: []
  type: TYPE_NORMAL
- en: And the custom type at [https://github.com/deshpandetanmay/mastering-flink/blob/master/chapter10/flink-batch-adv/src/main/java/com/demo/flink/batch/Record.java](https://github.com/deshpandetanmay/mastering-flink/blob/master/chapter10/flink-batch-adv/src/main/java/com/demo/flink/batch/Record.java).
  prefs: []
  type: TYPE_NORMAL
- en: We need to make sure that the custom serializer has to extend the Kryo's serializer
    class. With Google Protobuf and Apache Thrift, this has been done already.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can read more about Google Protobuf at [https://github.com/google/protobuf](https://github.com/google/protobuf).
    Details on Apache Thrift can be read at [https://thrift.apache.org/](https://thrift.apache.org/).
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to use Google Protobuf, you can add the following Maven dependency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Flink supports a metrics system which allows users to know more about the Flink
    setup and the applications running on it. This would be very useful if you are
    using Flink in a very big production system where a huge number of jobs are running
    and we need to get details of each. We can also use these to feed external monitoring
    systems. So let's try to understand what is available and how to use them.
  prefs: []
  type: TYPE_NORMAL
- en: Registering metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Metric functions are available for use from any user function which extends
    `RichFunction` by calling `getRuntimeContext().getMetricGroup()`. These methods
    return a `MetricGroup` object, which can be used to create and register a new
    metric.
  prefs: []
  type: TYPE_NORMAL
- en: 'Flink supports various metrics types, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Counters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gauges
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Histograms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Meters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Counters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A counter can be used to count certain things while processing. A simple use
    of a counter can be to count invalid records in the data. You can choose to either
    increment or decrement the counter, based on the conditions. The following code
    snippet shows this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Gauges
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A gauge can provide any value whenever required. In order to use a gauge, first
    we need to create a class that implements `org.apache.flink.metrics.Gauge`. Later,
    you can register that with `MetricGroup`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows the use of a gauge in the Flink application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Histograms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A histogram provides for the distribution of long values over a metric. This
    can be used to monitor certain metrics over time. The following code snippet shows
    how to use this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Meters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A meter is used for monitoring a specific parameter''s average throughput.
    The occurrence of an event is registered using the `markEvent()` method. We can
    register a meter using the `meter(String name, Meter meter)` method on `MeterGroup`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Reporters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Metrics can be displayed to the external system by configuring one or more
    reporters in the `conf/flink-conf.yaml` file. Most of you might be aware of systems
    such as JMX, which help in monitoring many systems. We can consider configuring
    JMX reporting in Flink. A reporter should have certain properties, as listed in
    the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Configuration** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `metrics.reporters` | The list of named reporters |'
  prefs: []
  type: TYPE_TB
- en: '| `metrics.reporter.<name>.<config>` | Configuration for reporter with `<name>`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `metrics.reporter.<name>.class` | Reporter class used for reporter named
    `<name>` |'
  prefs: []
  type: TYPE_TB
- en: '| `metrics.reporter.<name>.interval` | Interval time for reporter with name
    `<name>` |'
  prefs: []
  type: TYPE_TB
- en: '| `metrics.reporter.<name>.scope.delimiter` | Scope of reporter with name `<name>`
    |'
  prefs: []
  type: TYPE_TB
- en: 'The following is an example of a reported configuration for the JMX reporter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Once we add the preceding given configuration in `config/flink-conf.yaml`, we
    need to start Flink Job Manager process. Now Flink will start exposing these variables
    to JMX port `8789.` We can use JConsole to monitor the reports published by Flink.
    JConsole comes by default with JDK installation. We just need to go to JDK installation
    directory and start `JConsole.exe`. Once the JConsole is running, we need to select
    the Flink Job Manager process to monitor and we can see various values that can
    be monitored. Following is a sample screenshot of a JConsole screen monitoring
    Flink.
  prefs: []
  type: TYPE_NORMAL
- en: '![Reporters](img/B05653_10_01-1024x318.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Apart from JMX, Flink supports reporters such as Ganglia, Graphite and StasD.
    More information on those can be found at [https://ci.apache.org/projects/flink/flink-docs-release-1.2/monitoring/metrics.html#reporter](https://ci.apache.org/projects/flink/flink-docs-release-1.2/monitoring/metrics.html#reporter).
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring REST API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Flink supports the monitoring of the status of running and completed apps. These
    APIs are also used by Flink's own job dashboard. The status APIs support the `get`
    method which returns JSON objects giving information of the job. Currently, monitoring
    APIs is by default started within the Flink Job Manager dashboard. This information
    can also be accessed with Job Manager Dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: There are many APIs available in Flink. Let's start understanding some of them.
  prefs: []
  type: TYPE_NORMAL
- en: Config API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This gives configuration details of the API:  `http://localhost:8081/config`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is  the response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Overview API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This gives an overview of the Flink cluster: `http://localhost:8081/overview`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Overview of the jobs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This gives an overview of the jobs which have run recently and are currently
    running: ` http://localhost:8081/jobs`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '`http://localhost:8081/joboverview` API gives the complete overview of a Flink
    job. It contains job ID, start and end times, duration of run, no. of tasks and
    their states. A state could be started, running, killed or finished.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Details of a specific job
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This gives details of the specific job. We need to provide the job ID returned
    by the previous API. When a job is submitted, Flink creates a Directed Acyclic
    Job (DAG) for that job. This graph contains vertices as the tasks of the job and
    the execution plan. Following output shows the same details.   `http://localhost:8081/jobs/<jobid>`
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: User defined job configuration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This gives the user defined job configuration used by a specific job:'
  prefs: []
  type: TYPE_NORMAL
- en: '`http://localhost:8081/jobs/<jobid>/config`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, you can explore all the following listed APIs on your own setup:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Back pressure monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Back pressure is a special situation in Flink applications where the downstream
    operators are not able to consume data with the same speed of the upstream operator
    that is pushing the data. This starts building pressure on the pipeline and the
    data flow starts in the opposite direction. Generally, if this happens, Flink
    gives us warnings in the logs.
  prefs: []
  type: TYPE_NORMAL
- en: In a source sink scenario, if we see a warning to the source, then it means
    sink is consuming data slower than the source is producing it.
  prefs: []
  type: TYPE_NORMAL
- en: It is very important to monitor back pressure in all streaming jobs, as a high
    back pressuring job may fail or give the wrong results. The backpressure can be
    monitored from the Flink dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: Flink handles back pressure monitoring continuously, taking sample stack traces
    of the running tasks. If the sample shows that the task is stuck in an internal
    method, this indicates that there is a back pressure.
  prefs: []
  type: TYPE_NORMAL
- en: 'On an average, the Job Manager triggers 100 stack traces every 50 milliseconds.
    Based on the number of tasks stuck in the internal process, the back pressure
    warning level is decided, as shown in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Ratio** | **Back pressure level** |'
  prefs: []
  type: TYPE_TB
- en: '| 0 to 0.10 | ok |'
  prefs: []
  type: TYPE_TB
- en: '| 0.10 to 0.5 | low |'
  prefs: []
  type: TYPE_TB
- en: '| 0.5 to 1 | high |'
  prefs: []
  type: TYPE_TB
- en: 'You can also configure the number of samples and their intervals by setting
    the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Parameter** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `jobmanager.web.backpressure.refresh-interval` | Refresh interval to reset
    available stats. Default is `60,000`, 1 min. |'
  prefs: []
  type: TYPE_TB
- en: '| `jobmanager.web.backpressure.delay-between-samples` | Interval for delay
    between the samples. Default is `50` ms. |'
  prefs: []
  type: TYPE_TB
- en: '| `jobmanager.web.backpressure.num-samples` | Number of samples to determine
    the back pressure. Default is `100`. |'
  prefs: []
  type: TYPE_TB
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this final chapter, we looked at some best practices you should follow in
    order to achieve the best of Flink's performance. We also looked at various monitoring
    APIs and metrics which can be used for the detailed monitoring of Flink applications.
  prefs: []
  type: TYPE_NORMAL
- en: For Flink, I would say the journey has just started and I am sure over the years,
    the community and support is going to get stronger and better. After all, Flink
    is called the **fourth generation** (**4G**) of big data!
  prefs: []
  type: TYPE_NORMAL
