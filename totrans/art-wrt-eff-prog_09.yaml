- en: '*Chapter 7*: Data Structures for Concurrency'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the last chapter, we explored, in detail, the synchronization primitives
    that can be used to ensure the correctness of concurrent programs. We also studied
    the simplest but useful building blocks for these programs: **thread-safe counters**
    and **pointers**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to continue the study of data structures for
    concurrent programs. The aim of this chapter is two-fold: on the one hand, you
    will learn how to design thread-safe variants of several fundamental data structures.
    On the other hand, we will point out several general principles and observations
    that are important for designing your own data structures to be used in concurrent
    programs, as well as for evaluating the best approaches to organize and store
    your data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding thread-safe data structures, including sequential containers,
    stack and queue, node-based containers, and lists
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving concurrency, performance, and order guarantees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recommendations for designing thread-safe data structures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Again, you will need a C++ compiler and a micro-benchmarking tool, such as the
    Google Benchmark library we used in the previous chapter (found at [https://github.com/google/benchmark](https://github.com/google/benchmark)).
    The code accompanying this chapter can be found at [https://github.com/PacktPublishing/The-Art-of-Writing-Efficient-Programs/tree/master/Chapter07](https://github.com/PacktPublishing/The-Art-of-Writing-Efficient-Programs/tree/master/Chapter07).
  prefs: []
  type: TYPE_NORMAL
- en: What is a thread-safe data structure?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we begin learning about thread-safe data structures, we have to know
    what they are. If this seems like a simple question – *data structures that can
    be used by multiple threads at once* – you have not given the question enough
    thought. I cannot overstate how important it is to ask this question every time
    you start designing a new data structure or an algorithm to be used in a concurrent
    program. If this sentence puts you on guard and gives you pause, there is a good
    reason for it: I have just implied that the *thread-safe data structure* has no
    single definition that suits every need and every application. This is indeed
    the case, and is a very important point to understand.'
  prefs: []
  type: TYPE_NORMAL
- en: The best kind of thread safety
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s start with something that should be obvious but is often forgotten in
    practice: a very general principle of designing for high performance is that *doing
    zero work is always faster than doing some work*. For the subject at hand, this
    general principle can be narrowed down to *do you need any kind of thread-safety
    for this data structure?* Ensuring thread safety, whatever form it takes, implies
    some amount of work that will need to be done by the computer. Ask yourself, *do
    I really need it? Can I arrange the computation so that each thread has its own
    set of data to operate on?*'
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple example is the thread-safe counter we used in the previous chapter.
    If you need all threads to see the current value of the counter at all times,
    then it was the right solution. However, let''s say that all we need is to count
    some event that happens on multiple threads, such as searching for something in
    a large set of data that has been divided between the threads. A thread does not
    need to know the current value of the count to do the search. Of course, it would
    need to know the latest value of the count to increment it, but that is true only
    if we try to increment the single shared count on all threads, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The performance of the counting itself is dismal, as can be seen in a benchmark
    where we do nothing but count (no *search*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – Counting on multiple threads does not scale if the count is
    shared'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.1_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.1 – Counting on multiple threads does not scale if the count is shared
  prefs: []
  type: TYPE_NORMAL
- en: 'The scaling of the counting is actually negative: it takes longer to get to
    the same value of the count on two threads than on one, despite our best efforts
    to use a wait-free count with the minimal memory order requirements. Of course,
    if the search is very long compared to the counting, then the performance of the
    count is irrelevant (but the search code itself may present the same choice of
    doing some work on a global data or a per-thread copy, so consider this an instructive
    example).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assuming we only care about the value of the count at the very end of the computation,
    a much better solution is, of course, to maintain local counts on each thread
    and increment the shared count only once:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: To highlight just how unimportant the shared count increment is now, we are
    going to use the basic mutex; usually, a lock is a safer choice as it is easier
    to understand (so, harder to make bugs), although, in the case of a count, an
    atomic integer actually yields simpler code.
  prefs: []
  type: TYPE_NORMAL
- en: 'If each thread increments the local count many times before it reaches the
    end and has to increment the shared count, the scaling is near-perfect:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Counting on multiple threads scales perfectly with per-thread
    counts'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.2_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.2 – Counting on multiple threads scales perfectly with per-thread counts
  prefs: []
  type: TYPE_NORMAL
- en: 'So the best kind of thread safety is the one that is guaranteed by the fact
    that you don''t access the data structure from multiple threads. Often, this arrangement
    comes at the cost of some overhead: for example, each thread maintains a container
    or a memory allocator whose size grows and shrinks repeatedly. You can avoid any
    locking whatsoever if you don''t release the memory to the main allocator until
    the end of the program. The price will be that the unused memory on one thread
    is not made available to other threads, so the total memory use will be the sum
    of the peak uses of all threads, even if these moments of peak use occur at different
    times. Whether or not this is acceptable depends on the details of the problem
    and the implementation: it is something you have to consider for every program.'
  prefs: []
  type: TYPE_NORMAL
- en: You could say that this entire section is a cop-out when it comes to thread
    safety. It is, from a certain point of view, but it happens so often in practice
    that a shared data structure is used where it is not necessary, and the performance
    gain can be so significant that this point needs to be made. Now it is time to
    move on to the *real* thread safety, where a data structure must be shared between
    threads.
  prefs: []
  type: TYPE_NORMAL
- en: The real thread safety
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s assume that we really need to access a particular data structure from
    multiple threads at the same time. Now we have to talk about thread safety. But
    there is still not enough information to determine what this *thread safety* means.
    We have already discussed in the previous chapter the strong and weak thread-safety
    guarantees. We will see in this chapter that even that partitioning is not enough,
    but it puts us on the right track: instead of talking about general *thread safety*,
    we should be describing the set of guarantees provided by the data structure with
    regard to concurrent access.'
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen, the weak (but usually easy to provide) guarantee is that multiple
    threads can read the same data structure as long as it remains unchanged. The
    strongest guarantee is, obviously, that any operation can be done by any number
    of threads at any time, and the data structure remains in a well-defined state.
    This guarantee is often both expensive and unnecessary. Your program may require
    such a guarantee from some but not all operations supported by the data structure.
    There may be other simplifications, such as the number of threads accessing the
    data structure at once may be limited.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a rule, you want to provide as few guarantees as necessary to make your
    program correct and no more: additional thread-safety features are often very
    expensive and create overhead even when they are not used.'
  prefs: []
  type: TYPE_NORMAL
- en: With this in mind, let's start exploring concrete data structures and see what
    it takes to provide different levels of thread-safety guarantees.
  prefs: []
  type: TYPE_NORMAL
- en: The thread-safe stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the simplest data structures from the point of view of concurrency is
    the **stack**. All operations on the stack deal with the top element, so there
    is (conceptually, at least) a single location that needs to be guarded against
    races.
  prefs: []
  type: TYPE_NORMAL
- en: 'The C++ standard library offers us the `std::stack` container, so it makes
    a good starting point. All C++ containers, including the stack, offer the weak
    thread-safety guarantee: a read-only container can be safely accessed by many
    threads. In other words, any number of threads can call any `const` methods at
    the same time as long as no thread calls any non-`const` methods. While this sounds
    easy, almost simplistic, there is a subtle point here: there must be some kind
    of synchronization event accompanied by a memory barrier between the last modification
    of the object and the portion of the program where it is considered read-only.
    In other words, write access is not really *done* until all threads execute a
    memory barrier: the writer must, as a minimum, do a release, while all readers
    must acquire. Any stronger barrier will work as well, and so will a lock, but
    every thread must take this step.'
  prefs: []
  type: TYPE_NORMAL
- en: Interface design for thread safety
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, what if at least one thread is modifying the stack, and we need a stronger
    guarantee? The most straightforward way to provide one is by guarding every member
    function of the class with a mutex. This can be done at the application level,
    but such implementation does not enforce the thread safety and is, therefore,
    error-prone. It is also hard to debug and analyze because the lock is not associated
    with the container.
  prefs: []
  type: TYPE_NORMAL
- en: 'A better option is to *wrap* the stack class with our own, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we could use inheritance instead of encapsulation. Doing so would
    make it easier to write the constructors of `mt_stack`: we would need only one
    `using` statement. However, using public inheritance exposes every member function
    of the base class `std::stack`, so if we forget to wrap one of them, the code
    will compile but will call the unguarded member function directly. Private (or
    protected) inheritance avoids this problem but presents other dangers. Some of
    the constructors would need to be reimplemented: for example, the move constructor
    would need to lock the stack that is being moved from, so it needs a custom implementation
    anyway. Several other constructors would be dangerous to expose without a wrapper
    because they read or modify their arguments. Overall, it is safer if we have to
    write every constructor we want to provide. This is consistent with the very general
    rule of C++; *prefer composition over inheritance*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our thread-safe or multi-threaded stack (that''s what *mt* stands for) now
    has the *push* functionality and is ready to receive data. We just need the other
    half of the interface, the *pop*. We can certainly follow the preceding example
    and wrap the `pop()` method, but this is not enough: the STL stack uses three
    separate member functions to remove elements from the stack. `pop()` removes the
    top element but returns nothing, so if you want to know what''s on top of the
    stack, you have to call `top()` first. It is undefined behavior to call either
    of those if the stack is empty, so you have to call `empty()` first and check
    the result. OK, we can wrap all three methods, but this gives us nothing at all.
    In the following code, assume that all member functions of the stack are guarded
    by a lock:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Each member function is perfectly thread-safe and perfectly useless in a multi-threaded
    context: the stack may be non-empty one moment – the moment we happen to call
    `s.empty()` – but become empty the next, before we call `s.top()`, because another
    thread could remove the top element in the meantime.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This may very well be the most important lesson from the entire book: *in order
    to provide usable thread-safe functionality, the interface must be chosen with
    thread safety in mind*. More generally, it is not possible to *add* thread safety
    on top of an existing design. Instead, the design must be done with thread safety
    in mind. The reason is this: you may choose to provide certain guarantees and
    invariants in your design that are impossible to maintain in a concurrent program.
    For example, `std::stack` provides the guarantee that if you call `empty()` and
    it returns `false`, you can safely call `top()` as long as you don''t do anything
    else to the stack between these two calls. There is no practically useful way
    to maintain this guarantee in a multi-threaded program.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, since we are writing our own wrapper class anyway, we are not
    constrained to use the interface of the wrapped class verbatim. So, what should
    we do instead? Clearly, the entire *pop* operation should be a single member function:
    it should remove the top element from the stack and return it to the caller. One
    complication is what to do when the stack is empty. We have multiple options here.
    We could return a pair of the value and a Boolean flag that indicates whether
    the stack was empty (the value would have to be default-constructed in this case).
    We could return the Boolean alone and pass the value by reference (it remains
    unchanged if the stack is empty). In C++17, the natural solution is to return
    `std::optional`, as shown in the following code. It''s a perfect fit for the job
    of holding a value that may not exist:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the entire operation of popping the element from the stack
    is now protected by a lock. The key property of this interface is that it is transactional:
    each member function takes the object from one known state to another known state.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If the object has to transition through some intermediate states that are not
    sufficiently defined, such as the state after calling `empty()` but before calling
    `pop()`, then these states must be hidden from the caller. The caller is instead
    presented with a single atomic transaction: either the top element is returned,
    or the caller is informed that there isn''t one. This ensures the correctness
    of the program; now, we can look at the performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Performance of mutex-guarded data structures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How well does our stack perform? Given that every operation is locked from start
    to finish, we should not expect the calls to the stack member function to scale
    at all. At best, all threads will execute their stack operations serially, but,
    in reality, we should expect some overhead from the locking. We can measure this
    overhead in a benchmark if we compare the performance of the multi-threaded stack
    with that of `std::stack` on a single thread.
  prefs: []
  type: TYPE_NORMAL
- en: 'To simplify the benchmark, you may choose to implement a single-threaded non-blocking
    wrapper around `std::stack` that presents the same interface as our `mt_stack`.
    Beware that you cannot benchmark just by pushing on the stack: your benchmark
    will probably run out of memory. Similarly, you cannot reliably benchmark the
    pop operation unless you want to measure the cost of popping from an empty stack.
    If the benchmark runs long enough, you have to combine both push and pop. The
    simplest benchmark may look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'When running multi-threaded, there is a chance that some of the `pop()` operations
    will happen while the stack is empty. This may be realistic for the application
    for which you are designing the stack. Also, since the benchmark gives us only
    an approximation of the performance of the data structure in the real application,
    it may not matter. For a more accurate measurement, you would probably have to
    emulate the realistic sequence of push and pop operations produced by your application.
    Anyway, the results should look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – Performance of a mutex-guarded stack'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.3_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.3 – Performance of a mutex-guarded stack
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the "item" here is a push followed by a pop, so the value of "items
    per second" shows how many data elements we can send through the stack every second.
    For comparison, the same stack without any locks performs more than 10 times faster
    on a single thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – Performance of std::stack (compare with Figure 7.3)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.4_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.4 – Performance of std::stack (compare with Figure 7.3)
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the simplest implementation of the stack using a mutex has rather
    poor performance. However, you should not be in a rush to find or design some
    clever thread-safe stack, at least not yet. The first question you should ask
    is, *does it matter?* What does the application do with the data on the stack?
    If, say, each data element is a parameter for a simulation that takes several
    seconds, it probably doesn't matter how fast the stack is. On the other hand,
    if the stack is at the heart of some real-time transaction processing system,
    its speed is likely the key to the performance of the entire system.
  prefs: []
  type: TYPE_NORMAL
- en: By the way, the results will likely be similar for any other data structure
    such as list, deque, queue, and tree, where the individual operations are much
    faster than the operations on the mutex. But before we can try to improve the
    performance, we have to consider exactly what kind of performance our application
    requires.
  prefs: []
  type: TYPE_NORMAL
- en: Performance requirements for different uses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the rest of this chapter, let's assume that the performance of the data
    structures matters in your application. Now, can we see the fastest stack implementation
    already? Again, not yet. We also need to consider the use model; in other words,
    what do we do with the stack and what exactly needs to be fast.
  prefs: []
  type: TYPE_NORMAL
- en: For example, as we have just seen, the key reason for the poor performance of
    the mutex-guarded stack is that its speed is essentially limited by the mutex
    itself. Benchmarking the stack operations is almost the same as benchmarking locking
    and unlocking the mutex. One way to improve the performance would be to improve
    the implementation of the mutex or use another synchronization scheme. Another
    way is to use the mutex less often; this way requires that we redesign the client
    code.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, very often, the caller has multiple items that must be pushed
    onto the stack. Similarly, the caller may be able to pop several elements at once
    from the stack and process them. In this case, we can implement a batch push or
    a batch pop using an array or another container to copy multiple elements to and
    from the stack at once. Since the overhead of locking is large, we can expect
    that pushing, say, 1,024 elements on the stack with one lock/unlock operation
    is faster than pushing each one under a separate lock. Indeed, the benchmark shows
    this to be the case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5 – Performance of the batch stack operations (1,024 elements per
    lock)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.5_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.5 – Performance of the batch stack operations (1,024 elements per lock)
  prefs: []
  type: TYPE_NORMAL
- en: 'We should be very clear about what this technique does and does not accomplish:
    it reduces the overhead of the locking if the critical section is much faster
    than the lock operations themselves. It does not make the locked operations scale.
    Furthermore, by making the critical section longer, we force the threads to wait
    longer on the lock. This is fine if all threads are mostly trying to access the
    stack (this is why the benchmark is getting faster). But if, in our application,
    the threads are mostly doing other computations and only occasionally access the
    stack, the longer wait will likely degrade the overall performance. To answer
    definitively whether batch push and batch pop are beneficial, we would have to
    profile them in a more realistic context.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are other scenarios where the search for a more limited, application-specific
    solution can yield performance gains far above what any improved implementation
    of a general solution can do. For example, this scenario is common in some applications:
    a single thread pushed a lot of data on the stack upfront, and then multiple threads
    remove the data from the stack and process it, and maybe push more data onto the
    stack. In this case, we can implement an unlocked push to be used only in the
    single-threaded context for the upfront push. While the responsibility is on the
    caller to never use this method in a multi-threaded context, the unlocked stack
    is so much faster than the locked one that it may be worth the complexity.'
  prefs: []
  type: TYPE_NORMAL
- en: 'More complex data structures offer a variety of use models, but even the stack
    can be used by more than simple push and pop. We can also look at the top element
    without deleting it. The `std::stack` provides the `top()` member function, but,
    once again, it is not transactional, so we have to create our own. It is very
    similar to the transactional `pop()` function, only without removing the top element:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that, to allow the lookup-only function, `top()`, to be declared `const`,
    we had to declare the mutex as `mutable`. This should be done with caution: the
    convention for multi-threaded programs is that, following the STL, all `const`
    member functions are safe to call on multiple threads as long as no non-`const`
    member functions are called. This generally implies that `const` methods do not
    modify the object, that they are truly read-only. The mutable data members violate
    this assumption. As a minimum, they should not represent the logical state of
    the object: they are only implementation details. Then, care should be taken to
    avoid any race conditions when modifying them. The mutex satisfies both of these
    requirements.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can consider different use patterns. In some applications, the data
    is pushed on the stack and popped from it. In others, the top stack element may
    need to be examined many times between each push and pop. Let''s focus on the
    latter case first. Examine the code for the `top()` method again. There is an
    obvious inefficiency here: because of the lock, only one thread can read the top
    element of the stack at any moment. But reading the top element is a non-modifying
    (read-only) operation. If all threads did that and no thread tried to modify the
    stack at the same time, we would not need the lock at all, and the `top()` operation
    would scale perfectly. Instead, it has a performance similar to that of the `pop()`
    method.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The reason we cannot omit lock in `top()` is that we cannot be sure that another
    thread is not calling `push()` or `pop()` at the same time. But even then, we
    do not need to lock two calls to `top()` against each other; they can proceed
    simultaneously. Only the operations that modify the stack need to be locked. There
    is a type of lock that provides such functionality; it is most commonly called
    a `top()` method uses the shared lock, so any number of threads can execute it
    simultaneously, but the `push()` and `pop()` methods require the unique lock:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Unfortunately, our benchmark shows that the performance of the call to `top()`
    by itself does not scale even with the read-write lock:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6 – Performance of the stack with std::shared_mutex; read-only operations'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.6_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.6 – Performance of the stack with std::shared_mutex; read-only operations
  prefs: []
  type: TYPE_NORMAL
- en: 'Even worse, the performance of the operations that need the unique lock is
    degraded even more compared to the regular mutex:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7 – Performance of the stack with std::shared_mutex; write operations'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.7_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.7 – Performance of the stack with std::shared_mutex; write operations
  prefs: []
  type: TYPE_NORMAL
- en: 'Comparing *Figures 7.6* and *7.7* with the earlier measurements in *Figure
    7.4*, we can see that the read-write lock did not give us any improvement at all.
    This conclusion is far from universal: the performance of different mutexes depends
    on the implementation and the hardware. However, in general, the more complex
    locks, such as the shared mutex, will have more overhead than the simple locks.
    Their target application is different: if the critical section itself took much
    longer (say, milliseconds instead of microseconds) and most threads executed read-only
    code, there would be great value in not locking the read-only threads against
    each other, and the overhead of a few microseconds would be much less noticeable.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The longer critical section observation is of great importance: if our stack
    elements were much larger and very expensive to copy, the performance of the locks
    would matter less compared to the cost of copying the large objects, and we would
    start to see scaling. However, assuming our overall goal is to make the program
    fast, rather than showing off a scalable stack implementation, we would optimize
    the entire application by eliminating the expensive copying altogether and using
    a stack of pointers instead.'
  prefs: []
  type: TYPE_NORMAL
- en: Despite the setback we have suffered with the read-write lock, we are on the
    right track with the idea of a more efficient implementation. But before we can
    design one, we have to understand in more detail what exactly each of the stack
    operations does and what are the possible data races at each step that we must
    guard against.
  prefs: []
  type: TYPE_NORMAL
- en: Stack performance in detail
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we try to improve the performance of the thread-safe stack (or any other
    data structure) beyond that of the simple lock-guarded implementation, we have
    to first understand in detail the steps involved in each operation and how they
    may interact with other operations executed on different threads. The main value
    of this section is not the faster stack but this analysis: it turns out that these
    low-level steps are common to many data structures. Let''s start with the push
    operation. Most stack implementations are built on top of some array-like container,
    so let''s view the top of the stack as a contiguous block of memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8 – Top of the stack for push operation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.8_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.8 – Top of the stack for push operation
  prefs: []
  type: TYPE_NORMAL
- en: 'There are `N` elements on the stack, so the element count is also the index
    of the first free slot where the next element would go. The push operation must
    increment the top index (which is also the element count) from `N` to `N+1` to
    reserve its slot and then construct the new element in the slot `N`. Note that
    this top index is the only part of the data structure where the threads doing
    push can interact with each other: as long as the index increment operation is
    thread-safe, only one thread can see each value of the index. The first thread
    to execute the push advances the top index to `N+1` and reserves the `N`th slot,
    the next thread increments the index to `N+2` and reserves the `N+1`st slot, and
    so on. The key point here is that there is no race for the slots themselves: only
    one thread can get a particular slot, so it can construct the object there without
    any danger of another thread interfering with it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This suggests a very simple synchronization scheme for the push operations:
    all we need is a single atomic value for the top index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'A push operation atomically increments this index and then constructs the new
    element in the array slot indexed by the old value of the index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, there is no need to protect the construction step from other threads.
    The atomic index is all we need to make the push operations thread-safe. By the
    way, this is true if we use an array as the stack memory. If we use a container
    such as `std::deque`, we cannot simply construct a new element over its memory:
    we have to call `push_back` to update the size of the container, and that call
    is not thread-safe even if the deque does not need to allocate more memory. For
    this reason, data structure implementations that go beyond basic locks usually
    also have to manage their own memory. Speaking of memory, we have assumed so far
    that the array has space to add more elements, and we do not run out of memory.
    Let''s stick with this assumption for now.'
  prefs: []
  type: TYPE_NORMAL
- en: 'What we have so far is a very efficient way to implement a thread-safe push
    operation in a particular case: multiple threads may be pushing data onto the
    stack, but nobody is reading it until all push operations are done.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The same idea works if we have a stack with elements already pushed onto it,
    and we need to pop them (and no more new elements are added). *Figure 7.8* works
    for this scenario as well: a thread atomically decrements the top count and then
    returns the top element to the caller:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The atomic decrement guarantees that only one thread can access each array slot
    as the top element. Of course, this works only as long as the stack is not empty.
    We could change the top element index from an unsigned to a signed integer; then,
    we would know that the stack is empty when the index becomes negative.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is, again, a very efficient way to implement thread-safe pop operation
    under very special conditions: the stack is already populated, and no new elements
    are added. In this case, we also know how many elements are on the stack, so it
    is fairly easy to avoid an attempt to pop the empty stack.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In some specific applications, this may be of some value: if the stack is first
    populated by multiple threads without any pops and there is a clearly defined
    point in the program where it switches from adding data to removing it, then we
    have a great solution for each half of the problem. But let''s continue to a more
    general case.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our very efficient push operation is, unfortunately, of no help when it comes
    to reading from the stack. Let''s consider again how we would implement the operation
    that pops the top element. We have the top index, but all it tells us is how many
    elements are currently being constructed; it says nothing about the location of
    the last element whose construction is completed (element `N-3` in *Figure 7.9*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.9 – Top of the stack for push and pop operations'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.9_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.9 – Top of the stack for push and pop operations
  prefs: []
  type: TYPE_NORMAL
- en: Of course, the thread that does the push and, therefore, the construction, knows
    when it's done. Perhaps what we need is another count that shows how many elements
    are fully constructed. Alas, if only it was that simple. In *Figure 7.9*, let's
    assume that thread A is constructing the element `N-2` and that thread B is constructing
    the element `N-1`. Obviously, thread A was the first to increment the top index.
    But it doesn't mean it will also be the first to complete the push. Thread B may
    finish the construction first. Now, the last constructed element on the stack
    has the index `N-1`, so we could advance the *constructed count* to `N-1` (note
    that we *jumped* over element `N-2`, which is still in the middle of the construction).
    Now we want to pop the top element; no problem, the element `N-1` is ready, and
    we can return it to the caller and remove it from the stack; the *constructed
    count* is now decremented to `N-2`. Which element should we pop next? The element
    `N-2` is still not ready, but nothing in our stack warns us about it. We have
    only one count for *completed* elements, and its value is `N-1`. Now we have a
    data race between the thread that constructs a new element on the stack and the
    thread that tried to pop it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Even without this race, there is another problem: we just popped the element
    `N-1`, which was the right thing to do at the time. But while that was happening,
    a push was requested on thread C. Which slot should be used? If we use slot `N-1`,
    we risk overwriting the same element that is currently being accessed by thread
    A. If we use slot `N`, then, once all the operations are completed, we have a
    *hole* in the array: the top element is `N`, but the next one is not `N-1`: it
    was already popped, and we have to jump over it. Nothing in this data structure
    tells us that we must do so.'
  prefs: []
  type: TYPE_NORMAL
- en: We could keep track of which elements are *real* and which ones are *holes*,
    but this is becoming more and more complex (and doing it in a thread-safe manner
    will require additional synchronization that will reduce performance). Also, leaving
    many array slots unused wastes memory. We could attempt to reuse the *holes* for
    new elements pushed on the stack, but at this point, the elements are no longer
    stored consecutively, the atomic top count no longer works, and the whole structure
    begins to resemble a list. By the way, if you think that a list would be a great
    way to implement a thread-safe stack, wait until you see what it takes to implement
    a thread-safe list later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point in our design, we must pause the deep dive into the implementation
    details and again review the more general approach to the problem. There are two
    steps that we must do: generalize the conclusions from our deeper understanding
    of the details of the stack implementations and do some performance estimates
    to get a general idea about what solutions are likely to yield performance improvements.
    We will start with the latter.'
  prefs: []
  type: TYPE_NORMAL
- en: Performance estimates for synchronization schemes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our first attempt at a very simple stack implementation without a lock yielded
    some interesting solutions for special cases but no general solution. Before we
    spend much more time building a complex design, we should try to estimate how
    likely is it that it is going to be more efficient than the simple lock-based
    one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, this may seem like circular reasoning: in order to estimate the
    performance, we must first have something to estimate. But we don''t want to do
    the complex design without at least some assurances that the effort will pay off,
    the assurances that require a performance estimate.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, we can fall back on the general observations we learned earlier:
    the performance of concurrent data structures depends largely on how many shared
    variables are accessed concurrently. Let''s assume that we can come up with a
    clever way to implement the stack with a single atomic counter. It is reasonable
    to assume that every push and pop will have to do at least one atomic increment
    or decrement of this counter (unless we are doing batch operations, but we already
    know that they are faster). We can get a reasonable performance estimate if we
    make a benchmark that combines push and pop on the single-threaded stack with
    an atomic operation on a shared atomic counter. There is no synchronization going
    on, so we have to use a separate stack for every thread to avoid race conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `st_stack` is a stack wrapper that presents the same interface as our
    lock-based `mt_stack` but without any locks. The real implementation is going
    to be somewhat slower because the stack top is also shared between threads, but
    this will give us an estimate from above: it is highly unlikely that any implementation
    that is actually thread-safe will outperform this artificial benchmark. What do
    we compare the results to? The benchmark of the lock-based stack in *Figure 7.3*
    shows the performance of the lock-based stack to be between 30M push/pop operations
    per second on one thread and 3.1M on 8 threads. We also know the baseline performance
    of the stack without any locks to be about 485M operations per second (*Figure
    7.4*). On the same machine, our performance estimate with a single atomic counter
    yields these results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.10 – Performance estimate of a hypothetical stack with a single
    atomic counter'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.10_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.10 – Performance estimate of a hypothetical stack with a single atomic
    counter
  prefs: []
  type: TYPE_NORMAL
- en: 'The result seems like a mixed bag: even under optimal conditions, our stack
    is not going to scale. Again, this is primarily because we are testing a stack
    of small elements; if the elements were large and expensive to copy, we would
    see scaling because multiple threads can copy data at the same time. But the earlier
    observation stands: if copying data becomes so expensive that we need many threads
    to do it, we are better off using a stack of pointers and not copying any data
    at all.'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, the atomic counter is much faster than the mutex-based stack.
    Of course, this is an estimate from above, but it suggests that a lock-free stack
    has some possibilities. However, so does the lock-based stack: there are more
    efficient locks than `std::mutex` when we need to lock very short critical sections.
    We had already seen one such lock in [*Chapter 6*](B16229_06_Epub_AM.xhtml#_idTextAnchor103),
    *Concurrency and Performance*, when we implemented a spinlock. If we use this
    spinlock in our lock-based stack, then, instead of *Figure 7.2*, we get these
    results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.11 – Performance of the spinlock-based stack'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.11_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.11 – Performance of the spinlock-based stack
  prefs: []
  type: TYPE_NORMAL
- en: 'Comparing this result with *Figure 7.10* paints a very depressing picture:
    we are not going to come up with a lock-free design that can outperform a simple
    spinlock. The reason that the spinlock can outperform an atomic increment in some
    cases has to do with the relative performance of different atomic instructions
    on this particular hardware; we should not read too much into it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We could try to do the same estimate with an atomic exchange or compare-and-swap
    instead of the atomic increment. As you learn more about designing thread-safe
    data structures, you will get a sense of which synchronization protocol is likely
    to be useful and what operations should go into the estimate. Also, if you work
    with particular hardware, you should run simple benchmarks to determine which
    operations are more efficient on it. All results so far were obtained on X86-based
    hardware. If we run the same estimates on a large ARM-based server designed specifically
    for HPC applications, we get a very different outcome. The benchmark of a lock-based
    stack yields these results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.12 – Performance of the lock-based stack on an ARM HPC system'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.12_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.12 – Performance of the lock-based stack on an ARM HPC system
  prefs: []
  type: TYPE_NORMAL
- en: The ARM systems typically have a much larger number of cores than X86 systems,
    while the performance of a single core is lower. This particular system has 160
    cores on two physical processors, and the performance of the lock drops significantly
    when the program runs on both CPUs. The estimate for the upper limit of the lock-free
    stack performance should be done with a compare-and-swap instruction instead of
    the atomic increment (the latter is particularly inefficient on these processors).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.13 – Performance estimate for a hypothetical stack with a single
    CAS operation (ARM processors)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.13_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.13 – Performance estimate for a hypothetical stack with a single CAS
    operation (ARM processors)
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the estimates in *Figure 7.13*, there is a chance that, for a large
    number of threads, we can come up with something better than a simple lock-based
    stack. We are going to continue with our efforts to develop a lock-free stack.
    There are two reasons for it: first of all, this effort is ultimately going to
    pay off on some hardware. Second, the basic elements of this design will be seen
    later in many other data structures, and the stack offers us a simple test case
    for learning about them.'
  prefs: []
  type: TYPE_NORMAL
- en: Lock-free stack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have decided to try and outperform a simple lock-based implementation,
    we need to consider the lessons we have learned from our exploration of the push
    and pop operations by themselves. Each operation is very simple by itself, but
    the interaction of the two is what creates complexity. This is a very common situation:
    it is much harder to correctly synchronize producer and consumer operations running
    on multiple threads than it is to handle only producers or only consumers. Remember
    this when designing your own data structures: if your application allows for any
    kind of limitation on the operations you need to support, such as producers and
    consumers are separate in time, or there is a single producer (or consumer) thread,
    you can almost certainly design a faster data structure for these limited operations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assuming that we need a fully generic stack, the essence of the problem of
    the producer-consumer interaction can be understood on a very simple example.
    Again, we assume that the stack is implemented on top of an array or an array-like
    container, and the elements are stored consecutively. Let''s say that we have
    `N` elements currently on the stack. The producer thread P is executing the push
    operation, and the consumer thread C is executing the pop operation at the same
    time. What should be the outcome? While it is tempting to try to come up with
    a wait-free design (like we did for only consumers or only producers), any design
    that allows both threads to proceed without waiting is going to break our fundamental
    assumption about how the elements are stored: the thread C has to either wait
    for the thread P to complete the push or return the current top element, `N`.
    Similarly, the thread P has to either wait for the thread C to complete or construct
    a new element in the slot `N+1`. If neither thread waits, the result is a *hole*
    in the array: the last element has the index `N+1`, but there is nothing stored
    in the slot `N`, so we must somehow skip it when we pop data from the stack.'
  prefs: []
  type: TYPE_NORMAL
- en: It looks like we have to give up the idea of the wait-free stack implementation
    and make one of the threads wait for the other one to complete its operation.
    We also have to deal with the possibility of the empty stack when the top index
    is zero and a consumer thread attempts to further decrement it. A similar problem
    occurs at the upper bound of the array when the top index points to the last element
    and a producer thread needs another slot.
  prefs: []
  type: TYPE_NORMAL
- en: 'Both of these problems require a bounded atomic increment operation: perform
    the increment (or decrement) unless the value equals the specified bound. There
    is no ready-made atomic operation for this in C++ (or on any mainstream hardware
    available today), but we can implement it using **compare-and-swap** (**CAS**)
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a typical example of how CAS operation is used to implement a complex
    lock-free atomic operation:'
  prefs: []
  type: TYPE_NORMAL
- en: Read the current value of the variable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check the necessary conditions. In our case, we verify that the increment would
    not give us the value outside of the specified bounds `[0, maxn)`. If the bounded
    increment fails, we signal it to the caller by returning `-1` (this is an arbitrary
    choice; usually, there is a specific action to be performed for the out-of-bounds
    case).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Atomically replace the value with the desired result if the current value is
    still equal to what we read earlier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If *step 3* failed, the current value has been updated, check it again, and
    repeat *steps 3* and *4* until we succeed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'While this may seem to be a kind of lock, there is a fundamental difference:
    the only way the CAS comparison can fail on one thread is if it succeeded (and
    the atomic variable was incremented) on another thread, so any time there is a
    contention for the shared resource, at least one thread is guaranteed to make
    forward progress.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There is one more important observation that often makes all the difference
    between a scalable implementation and a very inefficient one. The CAS loop, as
    written, is very hostile to the scheduling algorithms of most modern operating
    systems: the thread that loops unsuccessfully also consumes more CPU time and
    will be given higher priority. This is the exact opposite of what we want: we
    want the thread that is currently doing the useful work to run faster. The solution
    is for a thread to yield the scheduler after a few unsuccessful CAS attempts.
    This is accomplished by a system call that is OS-dependent, but C++ has a system-independent
    API via the call to `std::this_thread::yield()`. On Linux, usually one can get
    better performance by calling the `nanosleep()` function to sleep for the minimum
    possible time (1 nanosecond) every few iterations of the loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The same approach can be used to implement much more complex atomic transactions,
    such as stack push and pop operations. But first, we have to figure out what atomic
    variables are needed. For the producer threads, we need the index of the first
    free slot in the array. For the consumer threads, we need the index of the last
    fully constructed element. This is all the information we need about the current
    state of the stack, assuming we do not allow "*holes*" in the array:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.14 – Lock-free stack: c_ is the index of the last fully constructed
    element, and p_ is the index of the first free slot in the array'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.14_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.14 – Lock-free stack: `c_` is the index of the last fully constructed
    element, and `p_` is the index of the first free slot in the array'
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, neither push nor pop can proceed if the two indices are currently
    not equal: different counts imply that either a new element is being constructed
    or the current top element is being copied out. Any stack modification in this
    state may lead to the creation of *holes* in the array.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If the two indices are equal, then we can proceed. To do the push, we need
    to atomically increment the producer index `p_` (bounded by the current capacity
    of the array). Then we can construct the new element in the slot we just reserved
    (indexed by the old value of `p_`). Then we increment the consumer index `c_`
    to indicate that the new element is available to the consumer threads. Note that
    another producer thread could grab the next slot even before the construction
    is completed, but we would have to wait until all new elements are constructed
    before we allow any consumer thread to pop an element. Such an implementation
    is possible, but it is more complex, and it tends to favor the currently executed
    operation: if a push is currently in progress, a pop has to wait, but another
    push can proceed without delay. The result is likely to be a *swarm* of push operations
    executing while all consumer threads are waiting (the effect is similar if a pop
    operation is in progress; it favors another pop).'
  prefs: []
  type: TYPE_NORMAL
- en: The pop is implemented similarly, only we first decrement the consumer index
    `c_` to reserve the top slot, and then decrement `p_` after the object is copied
    or moved from the stack.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is just one more trick we have to learn, and that is how to manipulate
    both counts atomically. For example, we said earlier that a thread has to wait
    for the two indices to become equal. How can this be accomplished? If we read
    one index atomically and then the other index, also atomically, there is a chance
    that the first index has changed since we read it. We have to read both indices
    in a single atomic operation. The same is true for other operations on the indices.
    C++ allows us to declare an atomic struct of two integers; however, we must be
    careful: very few hardware platforms have a *double CAS* instruction that operates
    on two long integers atomically, and even then, it is usually very slow. The better
    solution is to pack both values into a single 64-bit word (on a 64-bit processor).
    The hardware atomic instructions such as load or compare-and-swap do not really
    care how you are going to interpret the data they read or write: they just copy
    and compare 64-bit words. You can later treat these bits as a long or a double
    or a pair of ints (the atomic increment is, of course, different, which is why
    you cannot use it on a double value).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, all that is left is to convert the preceding algorithm into code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The two indices are 32-bit integers packed into a 64-bit atomic value. The
    method `equal()` may look strange, but its purpose will become evident in a moment.
    It returns true if the two indices are equal; otherwise, it updates the stored
    index values from the specified atomic variable. This follows the CAS pattern
    we have seen earlier: if the desired condition is not met, read the atomic variable
    again.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we can no longer build our thread-safe stack on top of the STL stack:
    the container itself is shared between threads, and the `push()` and `pop()` operations
    on it are not thread-safe without locking even if the container is not growing.
    For simplicity, in our example, we used a deque that was initialized with a *large
    enough* number of default-constructed elements. As long as we don''t call any
    container member functions, we can operate on different elements of the container
    from different threads independently. Remember that this is just a shortcut to
    avoid dealing with memory management and thread safety at the same time: in any
    practical implementation, you don''t want to default-construct all the elements
    upfront (and the element type may not even have a default constructor). Often,
    high-performance concurrent software systems have their own custom memory allocators
    anyway. Otherwise, you can also use an STL container of a dummy type of the same
    size and alignment as the stack element type, but with a simple constructor and
    destructor (the implementation is simple enough and is left as an exercise to
    the reader).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The push operation implements the algorithm we discussed earlier: wait for
    the indices to become equal, advance the producer index `p_`, construct the new
    object, and advance the consumer index `c_` when done:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The last CAS operation should never fail unless there is a bug in our code:
    once the calling thread successfully advanced `p_`, no other thread can change
    either value until the same thread advanced `c_` to match (as we already discussed,
    there is an inefficiency in that, but fixing it comes at the cost of much higher
    complexity). Also, note that, for brevity, we omitted the call to `nanosleep()`
    or `yield()` inside the loop, but it is essential in any practical implementation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The pop operation is similar, only it first decrements the consumer index `c_`
    and then, when it is done removing the top element from the stack, decrements
    `p_` to match `c_`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Again, the last compare-and-swap should not fail if the program is correct.
  prefs: []
  type: TYPE_NORMAL
- en: 'The lock-free stack is one of the simplest lock-free data structures possible,
    and it is already fairly complex. The testing required to validate that our implementation
    is correct is not straightforward: in addition to all the single-threaded unit
    tests, we have to validate that there are no race conditions. This task is made
    much easier by the sanitizer tools such as the **Thread Sanitizer** (**TSAN**)
    available in recent GCC and CLANG compilers. The advantage of these sanitizers
    is that they detect potential data races, not just the data races that actually
    happen during the test (in a small test, the chances to observe two threads accessing
    the same memory incorrectly at the same time are rather slim).'
  prefs: []
  type: TYPE_NORMAL
- en: 'After all our effort, what is the performance of the lock-free stack? As expected,
    on X86 processors, it does not outperform the spinlock-based version:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.15 – Performance of the lock-free stack on X86 CPU (compare with
    Figure 7.11)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.15_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.15 – Performance of the lock-free stack on X86 CPU (compare with Figure
    7.11)
  prefs: []
  type: TYPE_NORMAL
- en: 'For comparison, the spinlock-guarded stack can execute about 70M operations
    per second on the same machine. This is consistent with the expectations we had
    after the performance estimates in the previous section. The same estimates, however,
    suggested that the lock-free stack may be superior on ARM processors. The benchmark
    confirms that our efforts were not wasted:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.16 – Performance of the lock-free stack on ARM CPU (compare with
    Figure 7.12)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.16_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.16 – Performance of the lock-free stack on ARM CPU (compare with Figure
    7.12)
  prefs: []
  type: TYPE_NORMAL
- en: While the single-threaded performance of the lock-based stack is superior, the
    lock-free stack is much faster if the number of threads is large. The advantage
    of the lock-free stack becomes even greater if the benchmark includes a large
    fraction of `top()` calls (that is, many threads read the top element before one
    thread pops it) or if the producer and consumer threads are distinct (some threads
    call only `push()`, while other threads call only `pop()`).
  prefs: []
  type: TYPE_NORMAL
- en: 'To conclude this section, we have explored the different implementations of
    a thread-safe stack data structure. To understand what is required for thread
    safety, we had to analyze each operation separately, as well as the interaction
    of multiple concurrent operations. The following are the lessons that we learned:'
  prefs: []
  type: TYPE_NORMAL
- en: With a good lock implementation, a lock-guarded stack offers reasonable performance
    and is much simpler than the alternatives.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Any application-specific knowledge about the limitations on the use of the
    data structure should be exploited to gain performance cheaply. This is not the
    place to develop generic solutions, quite the opposite: implement as few features
    as you can and try to gain performance advantages from the restrictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A generic lock-free implementation is possible but, even for a data structure,
    that is as simple as a stack, it is quite complex. Sometimes, this complexity
    may even be justified.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So far, we have skirted the issue of memory management: it is hidden behind
    the vague *allocate more memory* when the stack runs out of capacity. We will
    need to come back to that later. But first, let''s explore more different data
    structures.'
  prefs: []
  type: TYPE_NORMAL
- en: The thread-safe queue
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The next data structure we are going to consider is the queue. It is again
    a very simple data structure, conceptually an array that is accessible from both
    ends: the data is added to the end of the array and removed from the beginning
    of it. There are some very important differences between the queue and the stack
    when it comes to implementation. There are also many similarities, and we will
    refer to the previous section frequently.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Just like the stack, the STL has a queue container, `std::queue`, and it has
    the exact same problem when it comes to concurrency: the interface for removing
    elements is not transactional, it requires three separate member function calls.
    If we wanted to use `std::queue` with a lock to create a thread-safe queue, we
    would have to wrap it just like we did with the stack:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We decided to use the spinlock right away (a simple benchmark can confirm that
    it is again faster than a mutex). The `front()` method, if desired, can be implemented
    similarly to the `pop()` method, only without removing the front element. The
    basic benchmark again measured the time it takes to push an element onto the queue
    and pop it back. Using the same X86 machine we did in the last section, we can
    obtain these numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.17 – Performance of a spinlock-guarded std::queue'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.17_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.17 – Performance of a spinlock-guarded std::queue
  prefs: []
  type: TYPE_NORMAL
- en: For comparison, on the same hardware, `std::queue` without any locks delivers
    about 280M items per second (an *item* is a push and a pop, so we measure how
    many elements we can send through the queue per second). So far, the picture is
    very similar to what we have seen earlier for the stack. To do better than the
    lock-guarded version, we have to try to come up with a lock-free implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Lock-free queue
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we dive into designing a lock-free queue, it is important to do a detailed
    analysis of each transaction, just like we did for the stack. Again, we will assume
    that the queue is built on top of an array or an array-like container (and we
    will defer the questions about what happens when the array is full). Pushing elements
    onto the queue looks just like it does for the stack:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.18 – Adding elements to the back of the queue (producer''s view)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.18_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.18 – Adding elements to the back of the queue (producer's view)
  prefs: []
  type: TYPE_NORMAL
- en: 'All we need is the index of the first empty slot in the array. Removing elements
    from the queue, however, is quite different from the same operation on the stack.
    You can see this in *Figure 7.19* (compare it with *Figure 7.9*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.19 – Removing elements from the front of the queue (consumer''s
    view)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.19_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.19 – Removing elements from the front of the queue (consumer's view)
  prefs: []
  type: TYPE_NORMAL
- en: The elements are removed from the front of the queue, so we need the index of
    the first element that has not been removed yet (the current front of the queue),
    and that index is also advanced.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we come to the crucial difference between the queue and the stack: in the
    stack, both producer and consumer operate on the same location: the top of the
    stack. We have seen the consequences of this: once the producer started to construct
    a new element at the top of the stack, the consumer has to wait for it to complete.
    The pop operation cannot return the last constructed element without leaving a
    *hole* in the array, and it can''t return the element being constructed until
    the construction is done.'
  prefs: []
  type: TYPE_NORMAL
- en: The situation is very different for the queue. As long as the queue is not empty,
    the producers and the consumers do not interact at all. The push operation does
    not need to know what the front index is, and the pop operation does not care
    where the back index is as long as it's somewhere ahead of the front. The producers
    and the consumers are not competing for access to the same memory location.
  prefs: []
  type: TYPE_NORMAL
- en: Whenever we have the case that there are several different ways to access the
    data structure and they (mostly) do not interact with each other, the general
    suggestion is to first consider the scenario where these roles are assigned to
    different threads. The further simplification can be to start with the case of
    one thread of each kind; in our case, it means one producer thread and one consumer
    thread.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since only the producer needs access to the back index, and there is only one
    producer thread, we don''t even need an atomic integer for this index. Similarly,
    the front index is just a regular integer. The only time the two threads interact
    with each other is when the queue becomes empty. For that, we need an atomic variable:
    the size of the queue. The producer constructs the new element in the first empty
    slot and advances the back index (in any order, there is only one producer thread).
    Then, it increments the size of the queue to reflect the fact that the queue now
    has one more element ready to be taken from it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The consumer must operate in reverse order: first, check the size to make sure
    the queue is not empty. Then the consumer can take the first element from the
    queue and advance the front index. Of course, there is no guarantee that the size
    does not change between the time it is checked and the time the front element
    is accessed. But it does not cause any problems: there is only one consumer thread,
    and the producer thread can only increment the size.'
  prefs: []
  type: TYPE_NORMAL
- en: 'While exploring the stack, we deferred the issue of adding more memory to the
    array and assumed that we somehow know the maximum capacity of the stack and will
    not exceed it (we could also make the push operation fail if that capacity is
    exceeded). For the queue, the same assumption is not enough: as the elements are
    added and removed from the queue, both the front and the back indices advance
    and will eventually reach the end of the array. Of course, at this point, we have
    the first elements of the array unused, so the simplest solution is to treat the
    array as a circular buffer and use modulo arithmetic for array indices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This queue requires a special benchmark because of the constraints we accepted
    on its design: one producer thread and one consumer thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'For comparison, we should benchmark our lock-guarded queue under the same conditions
    (performance of the locks is generally sensitive to the exact nature of the contention
    between threads). On the same X86 machine, the two queues perform at roughly the
    same throughput of 100M integer elements per second. On the ARM processor, the
    locks are relatively more expensive, in general, and our queue is no exception:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.20 – Performance of a lock-based versus a lock-free queue of integers
    on ARM'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.20_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.20 – Performance of a lock-based versus a lock-free queue of integers
    on ARM
  prefs: []
  type: TYPE_NORMAL
- en: 'However, even on X86, our analysis is not yet complete. In the previous section,
    we mentioned that if the stack elements are large, copying them takes relatively
    longer than the thread synchronization (locking or atomic operations). We could
    not make much use of it because most of the time, one thread still had to wait
    for the other thread to complete the copy, so the alternative was suggested: a
    stack of pointers, with the actual data stored elsewhere. The downside is that
    we need another thread-safe container to store this data (although often, the
    program needs to store it somewhere anyway). This is still a viable suggestion
    for the queue, but now we have another alternative. As we have already mentioned,
    the producer and consumer threads in the queue do not wait for each other: their
    interaction ends after the size is checked. It stands to reason that, if the data
    elements are large, the lock-free queue will have an advantage because both threads
    can copy the data at the same time and the contention between the threads, or
    the time when two threads are competing for access to the same memory location
    (the lock or the atomic value), is much shorter. To do such a benchmark, we just
    need to create a queue of large objects, such as a struct with a large array in
    it. As expected, the lock-free queue now performs faster, even on the X86 hardware:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.21 – Performance of a lock-based versus a lock-free queue of large
    elements on X86'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.21_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.21 – Performance of a lock-based versus a lock-free queue of large
    elements on X86
  prefs: []
  type: TYPE_NORMAL
- en: 'Even with the restrictions we have imposed, this is a very useful data structure:
    this queue can be used for transferring data between a producer and a consumer
    thread when we know an upper bound on the number of elements we can enqueue or
    can handle the situation when the producer has to wait before pushing more data.
    The queue is very efficient; even more important for some applications is the
    fact that it has very low and predictable latency: the queue itself is not just
    lock-free but wait-free. One thread never has to wait for the other unless the
    queue is full. By the way, if the consumer has to do certain processing on each
    data element it takes from the queue and starts falling behind until the queue
    fills up, one common approach is to have the producer process the elements it
    could not enqueue. This serves to delay the producer thread until the consumer
    can catch up (this method is not suitable for every application since it can process
    data out of order, but quite often, it works).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The generalization of our queue for the case of many producer or consumer threads
    is going to make the implementation more complex. The simple wait-free algorithm
    based on atomic size no longer works even if we make the front and back indices
    atomic: if multiple consumer threads read a non-zero value of size, this is no
    longer sufficient for all of them to proceed. With multiple consumers, the size
    can decrease and become zero after it was checked by one thread and found to be
    non-zero (it just means that the other threads popped all remaining elements after
    the first thread tested the size, but before it tried to access the front of the
    queue).'
  prefs: []
  type: TYPE_NORMAL
- en: 'One general solution is to use the same technique we used for the stack: pack
    the front and back indices into a single 64-bit atomic word and access them both
    atomically using compare-and-swap. The implementation is similar to that of the
    stack; the reader who understood the code in the previous section is well-prepared
    to implement this queue. There are other lock-free queue solutions that can be
    found in the literature; this chapter should give you sufficient background to
    understand, compare, and benchmark these implementations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Implementing a complex lock-free data structure correctly is a time-consuming
    project that requires skill and attention. It is good to have some performance
    estimates before the implementation is complete, so we can know whether the effort
    is likely to pay off. We have already seen one approach to benchmarking the code
    that does not yet exist: a simulated benchmark that combines the operations on
    a non-thread-safe data structure (local to each thread) with the operations on
    shared variables (locks or atomic data). The goal is to come up with a computationally
    equivalent code fragment that can be benchmarked; it is never going to be perfect,
    but if we have an idea for a lock-free queue with three atomic variables and a
    compare-and-swap operation on each one, and we discover that the estimated benchmark
    is several times slower than the spinlock-guarded queue, the work of implementing
    the real queue is unlikely to pay off.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second way to benchmark partially implemented code is to construct benchmarks
    that avoid certain corner cases that we have not yet implemented. For example,
    if you expect the queue to not be empty most of the time, and your initial implementation
    does not handle the case of the empty queue, you should benchmark that implementation
    and restrict the benchmark so the queue never gets empty. This benchmark will
    tell you if you are on the right track: it will show what performance you can
    expect in the typical case of the non-empty queue. We had actually taken this
    approach already when we deferred handling of the case when the stack or the queue
    runs out of memory. We simply assumed that it''s not going to happen very often
    and constructed the benchmark to avoid this case.'
  prefs: []
  type: TYPE_NORMAL
- en: There is yet another type of concurrent data structure implementation that can
    often be very efficient. We are going to learn about this technique next.
  prefs: []
  type: TYPE_NORMAL
- en: Non-sequentially consistent data structures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s first revisit the simple question, *what is a queue?* Of course, we
    know what a queue is: it''s a data structure such that the element added first
    is also retrieved first. Conceptually, and in many implementations, this is guaranteed
    by the order in which the elements are added to the underlying array: we have
    an array of queued elements, new entries are added to the front, while the oldest
    ones are read from the back.'
  prefs: []
  type: TYPE_NORMAL
- en: 'But let''s examine closely if this definition still holds for a concurrent
    queue. The code that is executed when an element is read from the queue looks
    something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The return value may be wrapped in `std::optional` or passed by reference; it
    doesn't matter. The point is, the value is read from the queue, the back index
    is decremented, and the control returns to the caller. In a multi-threaded program,
    the thread can be preempted at any moment. It is entirely possible that if we
    have two threads, A and B, and thread A reads the oldest element from the queue,
    it is thread B that completes execution of `pop()` first and returns its value
    to the caller. Thus, if we enqueue two elements X and Y, in that order, and have
    multiple threads dequeue them and print their values, the program prints Y then
    X. The same kind of reordering can happen when multiple threads push elements
    onto the queue. The end result is that even if the queue itself maintains a strict
    order (if you were to pause the program and examine the array in memory, the elements
    are in the right order), the order of dequeued elements as observed by the rest
    of the program is not guaranteed to be exactly the order in which they were enqueued.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, the order is not entirely random either: even in a concurrent program,
    a stack looks very different from a queue. The order of the data retrieved from
    a queue is approximately the order in which the values were added; significant
    rearrangements are rare (they happen when one thread is, for some reason, delayed
    for a significant time).'
  prefs: []
  type: TYPE_NORMAL
- en: 'There is another very important property that is still preserved by our queue:
    **sequential consistency**. A sequentially consistent program produces the output
    that is identical to the output of a program where operations from all threads
    are executed one at a time (without any concurrency), and the order of the operations
    executed by any particular thread is not changed. In other words, the equivalent
    program takes the sequences of operations executed by all threads and interleaves
    them but does not reshuffle them.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sequential consistency is a convenient property to have: it is much easier
    to analyze the behavior of such programs. For example, in the case of the queue,
    we have the guarantee that if two elements X and Y were enqueued by thread A,
    X first, then Y, and they happen to be both dequeued by thread B, they will come
    out in the correct order. On the other hand, we can argue that, in practice, it
    doesn''t really matter: the two elements may be dequeued by two different threads,
    in which case they can appear in any order, so the program has to be able to handle
    it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we are willing to give up sequential consistency, this opens up a whole
    new approach to designing concurrent data structures. Let''s explore it on the
    example of a queue. The basic idea is this: instead of a single queue thread-safe
    queue, we can have several single-threaded sub-queues. Each thread must atomically
    acquire exclusive ownership of one of these sub-queues. The simplest way to implement
    this is with an array of atomic pointers to the sub-queues, as shown in *Figure
    7.22*. To acquire the ownership and, at the same time, prevent any other thread
    from getting access to the queue, we atomically exchange the sub-queue pointer
    with null.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.22 – Non-sequentially-consistent queue based on an array sub-queue
    accessed via atomic pointers'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.22_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.22 – Non-sequentially-consistent queue based on an array sub-queue
    accessed via atomic pointers
  prefs: []
  type: TYPE_NORMAL
- en: A thread that needs to access the queue must first acquire a sub-queue. We can
    start from any element of the pointer array; if it's null, that sub-queue is currently
    busy, and we try the next element, and so on until we reserve a sub-queue. At
    this point, there is only one thread operating on the sub-queue, so there is no
    need for thread safety (the sub-queue can even be `std::queue`). After the operation
    (push or pop) is completed, the thread returns the ownership of the sub-queue
    to the queue by atomically writing the sub-queue pointer back into the array.
  prefs: []
  type: TYPE_NORMAL
- en: The push operation must continue to try to reserve the sub-queue until it finds
    one (alternatively, we can allow the push to fail after a certain number of tries
    and signal the caller that the queue is too busy). The pop operation may reserve
    a sub-queue only to find that it's empty. In this case, it has to try to pop from
    another sub-queue (we can keep an atomic count of elements in the queue to optimize
    the fast return if the queue is empty).
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, pop may fail on one thread and report that the queue is empty when
    in fact, it isn''t because another thread has pushed new data onto the queue.
    But this could happen with any concurrent queue: one thread checks the queue size,
    finds that the queue is empty, but before the control is returned to the caller,
    the queue becomes non-empty. Again, the sequential consistency puts some limits
    on what kind of inconsistencies can be observed by multiple threads, while our
    non-sequentially consistent queue makes the order of outgoing elements much less
    certain. Still, the order is maintained *on average*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is not the right data structure for every problem, but when the *mostly
    queue-like most of the time* order is acceptable, it can lead to significant performance
    improvements, especially in systems with many threads. Observe the scaling of
    the non-sequentially consistent queue on a large X86 server running many threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.23 – Performance of the non-sequentially-consistent queue'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.23_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.23 – Performance of the non-sequentially-consistent queue
  prefs: []
  type: TYPE_NORMAL
- en: In this benchmark, all threads do both push and pop operations, and the elements
    are fairly large (copying each element requires copying 1KB of data). For comparison,
    the spinlock-guarded `std::queue` delivers the same performance (about 170k elements
    per second) on a single thread but does not scale at all (the entire operation
    is locked), and the performance drops slowly (due to the overhead of locking)
    to about 130k elements per second for the maximum number of threads.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, many other data structures can benefit from this approach if you're
    willing to embrace the chaos of the non-sequentially-consistent programs for the
    sake of performance.
  prefs: []
  type: TYPE_NORMAL
- en: The last subject we need to cover when it comes to concurrent sequential containers
    such as stack and queue is how to handle the situation when they need more memory.
  prefs: []
  type: TYPE_NORMAL
- en: Memory management for concurrent data structures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far, we persisted in pushing back on the issue of memory management and
    assumed that the initial memory allocation for the data structure would suffice,
    at least for lock-free data structures that do not make the entire operation single-threaded.
    The lock-guarded and the non-sequentially-consistent data structures we have seen
    throughout this chapter do not have this problem: under the lock or exclusive
    ownership, there is only one thread operating on the particular data structure,
    so the memory is allocated in the usual way.'
  prefs: []
  type: TYPE_NORMAL
- en: For a lock-free data structure, memory allocation is a significant challenge.
    It is usually a relatively long operation, especially if the data must be copied
    to the new location. Even though multiple threads may detect that the data structure
    ran out of memory, usually only one thread can add new memory (it is very hard
    to make that part multi-threaded as well), the remaining threads must wait. There
    is no good general solution to this problem, but we will present several recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, the best option is to avoid the problem altogether. In many situations,
    when a lock-free data structure is needed, it is possible to estimate its maximum
    capacity and preallocate the memory. For example, we may know the total number
    of data elements we are going to enqueue. Alternatively, it may be possible to
    push the problem back to the caller: instead of adding memory, we can tell the
    caller that the data structure is out of capacity; in some problems, this may
    be an acceptable trade-off for the performance of the lock-free data structure.'
  prefs: []
  type: TYPE_NORMAL
- en: If the memory needs to be added, it is highly desirable that adding memory should
    not require copying of the entire existing data structure. This implies that we
    can't simply allocate more memory and copy everything to the new location. Instead,
    we must store the data in memory blocks of a fixed size, the way `std::deque`
    does it. When more memory is required, another block is allocated, and there are
    usually a few pointers that need to be changed, but no data is copied.
  prefs: []
  type: TYPE_NORMAL
- en: In all cases where memory allocation is done, this must be an infrequent event.
    If this is not so, then we are almost certainly better off with a single-threaded
    data structure protected by a lock or temporary exclusive ownership. The performance
    of this rare event is not critical, and we can simply lock the entire data structure
    and have one thread do the memory allocation and all the necessary updates. The
    key requirement is to make the common execution path, the one where we do not
    need more memory, as fast as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea is very simple: we certainly do not want to acquire the memory lock
    on every thread every time, which would serialize the whole program. We also don''t
    need to do this: most of the time, we are not out of memory, and there is no need
    for this lock. So instead, we are going to check an atomic flag. The flag is set
    only if memory allocation is currently in progress, and all threads must wait:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The problem here is that multiple threads may detect the out-of-memory condition
    at the same time before one of them sets the wait flag; they would then all try
    to add more memory to the data structure. This usually creates a race (reallocating
    the underlying storage is rarely thread-safe). However, there is a simple solution
    known as the **double-checked locking**. It uses both a mutex (or another lock)
    and an atomic flag. If the flag is not set, all is well, and we can proceed as
    usual. If the flag is set, we must acquire the lock and check the flag again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The first time, we check the out-of-memory condition without any locking. It
    is fast and, most of the time, we are not out of memory. The second time, we check
    it under the lock, where we have the guarantee that only one thread is executing
    at a time. Multiple threads may detect that we are out of memory; however, the
    first one to get the lock is the thread that handles this case. All remaining
    threads wait for the lock; when they acquire the lock, they do the second check
    (hence, double-checked locking) and discover that we are no longer out of memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach can be generalized to handle any special case that happens very
    infrequently but is much more difficult to implement in a lock-free manner than
    the rest of the code. In some cases, it may even be useful for situations such
    as the empty queue: as we have seen, the handling of multiple producers or multiple
    consumers would require a simple atomically incremented index if the two groups
    of threads never had to interact with each other. If, in a particular application,
    we have a guarantee that the queue rarely, if ever, becomes empty, we could favor
    an implementation that is very fast (wait-free) for the non-empty queue but falls
    back on a global lock if the queue might be empty.'
  prefs: []
  type: TYPE_NORMAL
- en: We have covered the sequential data structures in enough detail now. It is time
    to study the nodal data structures next.
  prefs: []
  type: TYPE_NORMAL
- en: The thread-safe list
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the sequential data structures we have studied so far, the data is stored
    in an array (or at least a conceptual array made up of memory blocks). Now we
    will consider a very different type of data structure where the data is linked
    together by pointers. The simplest example is a list where each element is allocated
    separately, but everything we learn here applies to other nodal containers such
    as trees, graphs, or any other data structure where each element is allocated
    separately, and the data is linked together by pointers.
  prefs: []
  type: TYPE_NORMAL
- en: 'For simplicity, we will consider a singly linked list; in STL, it is available
    as `std::forward_list`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.24 – Singly-linked list with iterators'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.24_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.24 – Singly-linked list with iterators
  prefs: []
  type: TYPE_NORMAL
- en: Because each element is allocated separately, it can also be deallocated individually.
    Often, a lightweight allocator is used for these data structures, where the memory
    is allocated in large blocks that are partitioned into node-sized fragments. When
    a node is deallocated, the memory is not returned to the OS but is put on a free
    list for the next allocation request. For our purposes, it is largely irrelevant
    whether the memory is allocated directly from the OS or handled by a specialized
    allocator (although the latter can often be much more efficient).
  prefs: []
  type: TYPE_NORMAL
- en: The list iterators present an additional challenge in concurrent programs. As
    we see in *Figure 7.24*, these iterators can point anywhere in the list. If an
    element is removed from the list, we expect its memory to eventually become available
    for constructing and inserting another element (if we do not do this and hold
    all memory until the entire list is deleted, adding and removing a few elements
    repeatedly can waste a lot of memory). However, we cannot delete the list node
    if there is an iterator pointing to it. This is true in single-threaded programs
    as well, but it is often much harder to manage in concurrent programs. With multiple
    threads possibly working with iterators, we often cannot guarantee by the execution
    flow of the operations that no iterators are pointing to the element we are about
    to delete. In this case, we need the iterators to extend the lifetime of the list
    nodes that they point to. This, of course, is a job for reference-counted smart
    pointers such as `std::shared_ptr`. Let's assume from now on that all the pointers
    in the list, both the ones linking the nodes together and the ones inside the
    iterators, are smart pointers (`std::shared_ptr` or a similar pointer with stronger
    thread-safety guarantees).
  prefs: []
  type: TYPE_NORMAL
- en: 'Just like we did with the sequential data structures, our first attempt at
    a thread-safe data structure should be a lock-guarded implementation. In general,
    you should never design a lock-free data structure until you know that you need
    one: developing lock-free code may be *cool*, but trying to find bugs in it is
    most definitely not.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Just like we did earlier, we have to redesign parts of the interface, so all
    operations are transactional: `pop_front()` should work whether the list is empty
    or not, for example. We can then protect all operations with a lock. For operations
    such as `push_front()` and `pop_front()`, we can expect a performance similar
    to what we have observed for the stack or the queue earlier. But the list presents
    additional challenges we did not have to face until now.'
  prefs: []
  type: TYPE_NORMAL
- en: First, the list supports insertions at arbitrary locations; in the case of `std::forward_list`,
    it is `insert_after()` to insert a new element after the one pointed to by an
    iterator. If we insert two elements on two threads simultaneously, we would like
    the insertions to proceed concurrently unless the two locations are close to each
    other and affect the same list node. But we cannot get that with a single lock
    guarding the entire list.
  prefs: []
  type: TYPE_NORMAL
- en: 'The situation is even worse if we consider long-running operations such as
    searching the list for an element that has the desired value (or satisfies some
    other condition). We would have to lock the list for the entire search operation,
    so no adding or removing elements to the list while the list is traversed. Of
    course, if we search frequently, the list is not the right data structure, but
    trees and other nodal data structures have the same problem: if we need to traverse
    large parts of the data structure, the lock is held for the duration of the entire
    operation, preventing all other threads from accessing even the nodes unrelated
    to the ones we''re currently operating on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, these problems are not your concern if you never encounter them:
    if your list is accessed from the front and backends only, then a lock-guarded
    list may be perfectly sufficient. As we have seen many times, when it comes to
    designing concurrent data structures, unnecessary generality is your enemy. Build
    only what you need.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Most of the time, however, nodal data structures are accessed not just from
    the ends or, in the case of trees or graphs, there aren''t really any *ends*.
    Locking the entire data structure so that it can be accessed by only one thread
    at a time is not acceptable if the program spends most of the time operating on
    this data structure. The next idea you may consider is locking each node separately;
    in the case of the list, we could add a spinlock to every node and lock the node
    if we need to change it. Unfortunately, this approach runs into the problem that
    is the bane of all lock-based solutions: the deadlocks. Any thread that needs
    to operate on more than one node will have to acquire multiple locks. Let''s say
    that thread A holds the lock on node 1, and now it needs to insert a new node
    after node 2, so it tries to get that lock too. At the same time, thread B holds
    the lock on node 2, and it wants to erase the node after node 1, so it tries to
    get that lock. Both threads will now wait forever. This problem is not avoidable
    with so many locks that can be acquired in arbitrary order unless we enforce very
    strict limitations on how the threads may access the list (hold only one lock
    at any time), and then we run the risk of livelocks as many threads constantly
    release and reacquire locks.'
  prefs: []
  type: TYPE_NORMAL
- en: If we truly need a list or another nodal data structure that is accessed concurrently,
    we have to come up with a lock-free implementation. As we have seen already, lock-free
    code is not easy to write and even harder to write correctly. Quite often, the
    better option is to come up with a different algorithm that does not require a
    thread-safe nodal data structure. Often, this can be done by copying parts of
    the global data structure into a thread-specific one that is then accessed by
    a single thread; at the end of the computation, the fragments from all threads
    are put together again. Sometimes, it is easier to partition the data structure
    so no nodes are accessed concurrently (for example, it may be possible to partition
    the graph and process each subgraph on one thread and then handle the boundary
    nodes). But if you really need a thread-safe nodal data structure, the next section
    will explain the challenges and give you some options for the implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Lock-free list
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The basic idea behind a **lock-free list**, or any other nodal container, is
    quite simple and is based on using compare-and-swap to manipulate the pointers
    to the nodes. Let''s start with the simpler operation: the insertion. We are going
    to describe the insertion at the head of the list, but the insertion after any
    other node works the same way.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.25 – Insertion of a new node at the head of a singly-linked list'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.25_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.25 – Insertion of a new node at the head of a singly-linked list
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say that we want to insert a new node at the head of the list shown
    in *Figure 7.25a*. The first step is to read the current head pointer, that is,
    the pointer to the first node. Then we create the new node with the desired value;
    its next pointer is the same as the current head pointer, so this node is linked
    into the list before the current first node (*Figure 7.25b*). At this point, the
    new node is not yet accessible to any other thread, so the data structure can
    be accessed concurrently. Finally, we execute the CAS: if the current head pointer
    is still unchanged, we atomically replace it with the pointer to the new node
    (*Figure 7.25c*). If the head pointer no longer has the value it had when we first
    read it, we read the new value, write it as the next pointer of our new node,
    and try the atomic CAS again.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a simple and reliable algorithm. It is the generalization of the publishing
    protocol we saw in the previous chapter: the new data is created on a thread with
    no concern for thread safety because it is not yet accessible to other threads.
    As the final action, the thread publishes the data by atomically changing the
    root pointer from which all the data can be accessed (in our case, the head of
    the list). If we were inserting the new node after another node, we would atomically
    change that node''s next pointer instead. The only difference is that multiple
    threads may be trying to publish new data at the same time; to avoid data races,
    we have to use compare-and-swap.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s consider the opposite operation, erasing the front node of the
    list. This is also done in three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.26 – Lock-free removal at the head of a singly-linked list'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.26_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.26 – Lock-free removal at the head of a singly-linked list
  prefs: []
  type: TYPE_NORMAL
- en: First, we read the head pointer, use it to access the first node of the list,
    and read its next pointer (*Figure 7.26a*). Then we atomically write the value
    of that next pointer into the head pointer (*Figure 7.26b*), but only if the head
    pointer has not changed (CAS). At this point, the former first node is not accessible
    to any other thread, but our thread still has the original value of the head pointer
    and can use it to delete the node we had removed (*Figure 7.26c*). This is, again,
    simple and reliable. But the trouble arises when we try to combine both of these
    operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s assume that two threads operate on the list at the same time. Thread
    A is trying to remove the first node of the list. The first step is to read the
    head pointer and the pointer to the next node; this pointer is about to become
    the new head of the list, but the compare-and-swap hasn''t happened yet. For now,
    the head is unchanged, and the new head is a value head'' that exists only in
    some local variable of thread A. This moment is captured in *Figure 7.27a*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.27 – Lock-free insertion and removal at the head of a singly-linked
    list'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.27_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.27 – Lock-free insertion and removal at the head of a singly-linked
    list
  prefs: []
  type: TYPE_NORMAL
- en: Just at this moment, thread B successfully removes the first node of the list.
    Then it removes the next node also, leaving the list in the state shown in *Figure
    7.27b* (thread A has not made any more progress). Thread B then inserts a new
    node at the head of the list (*Figure 7.27c*); however, since the memory of the
    two deleted nodes was deallocated, the new allocation for the node T4 reuses the
    old allocation, so the node T4 is allocated at the same address as the original
    node T1 used to have. This can easily happen as long as the memory of the deleted
    nodes is available for new allocations; in fact, most memory allocators prefer
    to return the most recently released memory on the assumption that it is still
    *hot* in the cache of the CPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, thread A is finally running again, and the operation it is about to do
    is compare-and-swap: if the head pointer has not changed since the last time thread
    A read it, the new head becomes `head''`. Unfortunately, the value of the head
    pointer is still the same, as far as thread A can see (it could not observe the
    entire history of the changes). The CAS operation succeeds, and the new head pointer
    now points to the unused memory where the node T2 used to be, while the node T4
    is no longer accessible (*Figure 7.27d*). The entire list is corrupted.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This failure mechanism is so common in lock-free data structures that it has
    a name: the **A-B-A problem**. **A** and **B** here refer to memory locations:
    the problem is that some pointer in the data structure changes its value from
    A to B and then back to A. Another thread observes only the initial and the final
    values and sees no change at all; the compare-and-swap operation succeeds, and
    the execution takes the path where the programmer has assumed that the data structure
    is unchanged. Unfortunately, this assumption is not true: the data structure may
    have changed almost arbitrarily, except that the value of the observed pointer
    was restored to what it once was.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The root of the problem is that if the memory is deallocated and reallocated,
    pointers, or addresses in memory, do not uniquely identify the data stored at
    that address. There are multiple solutions to this problem, but they all accomplish
    the same thing by different means: you have to make sure that once you read a
    pointer that will eventually be used by compare-and-swap, the memory at that address
    cannot be deallocated until the compare-and-swap is done (successfully or otherwise).
    If the memory is not deallocated, then another allocation cannot happen at the
    same address, and you are safe from the A-B-A problem. Note that *not deallocating
    memory* is not the same as *not deleting nodes*: you can certainly make the node
    inaccessible from the rest of the data structure (remove the node), and you can
    even call the destructor for the data stored in the node; you just cannot free
    the memory occupied by the node.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many ways to solve the A-B-A problem by delaying memory deallocation.
    The application-specific options are usually the simplest if they are possible.
    If you know that the algorithm does not remove many nodes over the lifetime of
    the data structure, you may simply keep all removed nodes on a list of deferred
    deallocations, to be deleted when the entire data structure is deleted. A more
    general version of this approach can be described as application-driven garbage
    collection: all deallocated memory goes on a *garbage* list first. The garbage
    memory is periodically returned to the main memory allocator, but during this
    garbage collection, all operations on the data structure are suspended: the operations
    in progress must complete before the collection starts, and all new operations
    are blocked until the collection is done. This ensures that no compare-and-swap
    operation can span the time interval of the garbage collection and, thus, the
    recycled memory is never encountered by any operation. The popular and often very
    efficient **RCU** (**read-copy-update**) technique is a variant of this method
    as well. Another common approach is the use of hazard pointers.'
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we will present yet another approach that employs atomic shared
    pointers (`std::shared_ptr` is not atomic by itself, but the standard included
    the necessary functions for atomic operations on shared pointers, or you can write
    your own for this specific application and make it faster). Let's revisit *Figure
    7.27b*, but now let all pointers be atomic shared pointers. As long as there is
    at least one such pointer to a node, that node cannot be deallocated. In the same
    sequence of events, thread A still has the old head pointer that points to the
    original node T1, as well as the intended new head pointer, `head'`, that points
    to the node T2\.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.28 – Lock-free insertion and removal at the head of a singly-linked
    list with shared pointers'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.28_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.28 – Lock-free insertion and removal at the head of a singly-linked
    list with shared pointers
  prefs: []
  type: TYPE_NORMAL
- en: Thread B has removed both nodes from the list (*Figure 7.28*), but the memory
    has not been released. The new node T4 is allocated at some other address, different
    from the addresses of all currently allocated nodes. Thus, when thread A resumes
    execution, it will find the new list head different from the old head value; the
    compare-and-swap will fail, and thread A will attempt the operation again. At
    this point, it will re-read the head pointer (and get the address of the node
    T3). The old value of the head pointer is now gone; since it was the last shared
    pointer pointing to the node T1, this node has no more references and is deleted.
    Similarly, node T2 is deleted as soon as the shared pointer `head'` is reset to
    its new intended value (the next pointer of the node T3). Both nodes T1 and T2
    have no shared pointers pointing to them, so they are finally deleted.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, this takes care of the insertion at the front. To allow insertion
    and removal anywhere, we have to make all pointers to the nodes into shared pointers.
    This includes the *next* pointers of all nodes as well as the pointers to nodes
    that are hidden inside list iterators. Such a design has another major advantage:
    it takes care of the problems with list traversals (such as search operations)
    that happen concurrently with insertions and deletions.'
  prefs: []
  type: TYPE_NORMAL
- en: If a list node was removed while there is an iterator pointing to this list
    (*Figure 7.29*), the node remains allocated, and the iterator is valid. Even if
    we remove the next node (T3), it will not be deallocated because there is a shared
    pointer pointing to it (the *next* pointer of node T2). The iterator can traverse
    the entire list.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.29 – Thread-safe traversal of a lock-free list with atomic shared
    pointers'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_7.29_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.29 – Thread-safe traversal of a lock-free list with atomic shared pointers
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, this traversal may include nodes that are no longer in the list,
    that is, no longer reachable from the head of the list. This is the nature of
    the concurrent data structures: there is no meaningful way to talk about the *current
    content of the list*: the only way to know the content of the list is to iterate
    over it from the head to the last node, but, by the time the iterator reached
    the end of the list, the previous nodes might have changed, and the result of
    the traversal is no longer *current*. This way of thinking takes some getting
    used to.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We are not going to show any benchmarks of the lock-free list versus a lock-guarded
    list because these benchmarks must be specific to the application. If you benchmark
    only insertions and deletions at the head of the list (`push_front()` and `pop_front()`),
    the spinlock-guarded list will be faster (atomic shared pointers are not cheap).
    On the other hand, if you benchmark simultaneous insertions and searches, you
    can make the lock-free list faster by as much as you want: do a traversal of a
    list of 1M elements with the lock-guarded list locked the entire time while the
    lock-free list can do simultaneous iterations on every thread, along with insertions
    and deletions. No matter how slow the atomic pointers are, the lock-free list
    will be faster if you just make it long enough. This is not a gratuitous observation:
    your application may need to do the operations that would require locking the
    list for a very long time unless you can somehow partition the list in a way that
    avoids deadlocks. If this is what you need to do, the lock-free list is the fastest
    by far. On the other hand, if you need to iterate over just a few elements and
    never in many different locations at the same time, a lock-guarded list will do
    fine.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The A-B-A problem and the solutions we have listed apply not just to the lists
    but to all nodal data structures: doubly-linked list, tree, and graph. In data
    structures linked by multiple pointers, you may encounter additional problems.
    First of all, even if all pointers are atomic, changing two atomic pointers one
    after the other is not an atomic operation. This leads to temporary inconsistencies
    in the data structure: for example, you may expect that going from a node to the
    next node and back to the previous node will get you back to the original node.
    This is not always true in the case of concurrency: if a node is inserted or removed
    at this location, one of the pointers may be updated before the other. The second
    problem is specific to shared pointers or any other implementation that uses reference
    counting: if the data structure has pointer loops, the nodes in the loop do not
    get deleted even when there are no more external references to them. The simplest
    example is the doubly-linked list, where two adjacent nodes always have pointers
    to each other. The way we solve this problem in single-threaded programs is by
    using weak pointers (in a doubly-linked list, all *next* pointers could be shared,
    and all *previous* pointers would then be weak). This does not work as well for
    concurrent programs: the whole point is to delay the deallocation of memory until
    there are no more references to it, and the weak pointers do not do that. For
    these cases, additional garbage collection may be necessary: after the last external
    pointer to a node is deleted, we have to traverse the linked nodes and check whether
    there are any external pointers to them (we can do it by checking the reference
    counts). List fragments with no external pointers can be safely deleted. For such
    data structures, alternative approaches such as hazard pointers or explicit garbage
    collection may be preferred. The reader should refer to specialized publications
    on lock-free programming for more information on these methods.'
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our exploration of high-performance data structures for concurrent
    programming. Let's now summarize what we have learned.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most important lesson of this chapter is that *designing data structures
    for concurrency is hard, and you should take every opportunity to simplify it*.
    Application-specific restrictions on the use of the data structures can be used
    to make them both simpler and faster.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first decision you must make is which parts of your code need thread safety
    and which do not. Often, the best solution is to give each thread its own data
    to work on: any data used by a single thread needs no thread-safety concerns at
    all. When that is not an option, look for other application-specific restrictions:
    do you have multiple threads modifying a particular data structure? The implementation
    is often simpler if there is only one writer thread. Are there any application-specific
    guarantees you can exploit? Do you know the maximum size of the data structure
    upfront? Do you need to delete data from the data structure as well as add it
    at the same time, or can you separate these operations in time? Are there well-defined
    periods where some data structures are not changing? If so, you do not need any
    synchronization to read them. These and many other application-specific restrictions
    can be used to greatly improve the performance of the data structures.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second important decision is: what operations on the data structures are
    you going to support? Another way to restate the last paragraph is "implement
    the minimal necessary interface." Any interface you do implement must be transactional:
    each operation must have well-defined behavior for any state of the data structure.
    Any operation that is valid only if the data structure is in a certain state cannot
    be safely invoked in a concurrent program unless the caller uses client-side locking
    to combine multiple operations into a single transaction (in which case, these
    should probably be one operation in the first place).'
  prefs: []
  type: TYPE_NORMAL
- en: The chapter also teaches several ways to implement data structures of different
    types, as well as the ways to estimate and evaluate their performance. Ultimately,
    accurate performance measurement can be obtained only in the context of the real
    application and with the actual data. However, useful approximate benchmarks can
    save a lot of time during the development and evaluation of potential alternatives.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter concludes our exploration of concurrency. Next, we go on to learn
    how the use of the C++ language itself influences the performance of our programs.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is the most critical feature of the interface of data structures designed
    for thread safety?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why are the data structures with limited functionality often more efficient
    than their generic variants?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Are lock-free data structures always faster than lock-based ones?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the challenges of managing memory in concurrent applications?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the A-B-A problem?.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
