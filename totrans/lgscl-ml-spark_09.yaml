- en: Chapter 9.  Advanced Machine Learning with Streaming and Graph Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter guides the reader on how to apply machine learning techniques
    with the help of Spark MLlib, Spark ML and Spark Streaming to streaming and graph
    data using GraphX. For example, topic modeling from the real-time tweets data
    from Twitter. The readers will be able to use available APIs to build real-time
    and predictive applications from streaming data sources such as Twitter. Through
    the Twitter data analysis, we will show how to perform large scale social sentiment
    analysis. We will also show how to develop a large-scale movie recommendation
    using Spark MLlib, which is an implicit part of social network analysis. In a
    nutshell, the following topics will be covered throughout this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Developing real-time ML pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time series and social network analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Movie recommendation using Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing a real-time ML pipeline from streaming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML pipeline on graph data and semi-supervised graph-based learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, what it really needs in order to be an effective and emerging ML application
    is a continuous flow of labeled data. Consequently, preprocessing the large-scale
    unstructured data and accurate labeling to that data essentially introduces many
    unwanted latencies.
  prefs: []
  type: TYPE_NORMAL
- en: Nowadays, we hear and read a lot about real-time machine learning. More or less,
    people usually provide this appealing business scenario when discussing sentiment
    analysis from **Social Network Services** (**SNS**), credit card fraud detection
    systems, or mining purchase rules related to the customer from business oriented
    transactional data.
  prefs: []
  type: TYPE_NORMAL
- en: According to many ML experts, it is possible to continuously update the credit
    card fraud detection model in real time. It's fantastic, but not realistic to
    me, for several reasons. Firstly, ensuring the continuous flow of this kind of
    data is not needed for model retraining. Secondly, creating labeled data would
    probably be the slowest and the most expensive step in most of the machine learning
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: Developing real-time ML pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to develop a real-time machine learning application, we need to have
    access to a continuous flow of data. The data might include transactional data,
    simple texts, tweets from Twitter, messaging or streaming from Flume or Kafka,
    and so on, as this is mostly unstructured data.
  prefs: []
  type: TYPE_NORMAL
- en: To deploy these kinds of ML applications, we need to go through a series of
    steps. The most unreliable source of the data that would serve our purpose is
    the real-time data from several sources. Often networks are a performance bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: For example, it's not guaranteed that you will always receive a bunch of tweets
    from Twitter. Moreover, labeling this data towards building an ML model on the
    fly is not a realistic idea. Nevertheless, here we provide a real insight on how
    we could develop and deploy an ML pipeline from real-time streaming data. *Figure
    1* shows the workflow of a real-time ML application development.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming data collection as unstructured text data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We would like to stress here that real time stream data collection depends
    on:'
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of the data collection. If the purpose is to develop a credit card
    fraud detection online, then the data should be collected from your own network
    through the web API. If the purpose is to collect social media sentiment analysis
    then data could be collected from Twitter, LinkedIn, Facebook, or newspaper sites,
    and if the purpose is to network anomaly detection, data could be collected from
    the network data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data availability is an issue since not all social media platforms provide public
    APIs for collecting data. The network condition is important since stream data
    is huge and needs very fast network connectivity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storage capability is an important consideration since a collection of a few
    minutes of tweets data, for example, could contribute to several GB of data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moreover, we should wait at least a couple of days before marking the transactions
    as *Fraud* or *Not Fraud*, for example. In contrast, if somebody reported a fraud
    transaction, we can immediately label this transaction as *Fraud* for the sake
    of simplicity.
  prefs: []
  type: TYPE_NORMAL
- en: Labeling the data towards making the supervised machine learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The labeled data set plays a central role in the whole process. It ensures that
    it is very easy to change the parameters of an algorithm such as the feature normalization
    or loss function. In that case, we would have several options of choosing the
    algorithm itself from logistic regression, to **Support Vector Machine** (**SVM**),
    or random forest, for example.
  prefs: []
  type: TYPE_NORMAL
- en: However, we cannot change the labeled data set since this information is predefined
    and your model should predict the labels that you already have. In previous chapters,
    we have shown that labeling the structured data takes a considerable amount of
    time.
  prefs: []
  type: TYPE_NORMAL
- en: Now think about the fully unstructured stream data that we would be receiving
    from streaming or real-time sources. In that case, labeling the data would take
    a considerable amount of time. Nevertheless, we will also have to do the pre-processing,
    such as tokenization, cleaning, indexing, removing stop words, and removing special
    characters from the unstructured data.
  prefs: []
  type: TYPE_NORMAL
- en: Now, essentially, there would be a question of *how long does the data labeling
    process take?* The final thing about the labeled dataset is that we should understand
    that the labeled dataset might be biased sometimes if we don't do the labeling
    carefully, which might lead to a lot of issues with the model's performance.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and building the model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For training the sentiment analytics, the credit card fraud detection model,
    and the association rule mining model, we need to have a lot of examples of transaction
    data that is as accurate as possible. Once we have the labeled dataset, we are
    ready to train and build the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating and building the model](img/00142.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Real-time machine learning workflow.'
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 7](part0059_split_000.html#1O8H62-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 7. Tuning Machine Learning Models"), *Tuning Machine Learning Models*,
    we discussed how to choose appropriate models and ML algorithms in order to produce
    better predictive analytics. The model can be presented as a binary or multiclass
    classifier with several classes. Alternatively, use an LDA model for the sentiment
    analysis using the topic modeling concept. In a nutshell, *Figure 1* shows a real-time
    machine learning workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Real-time predictive analytics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When your ML model is properly trained and built, your model is ready for doing
    the real-time predictive analytics. If you get a good prediction from the model
    it would be fantastic. However, as we previously mentioned when discussing some
    accuracy issues such as true positives and false positives, if the number of false
    positives is high then that means the performance of the model is not satisfactory.
  prefs: []
  type: TYPE_NORMAL
- en: 'This essentially means three things: we have not properly labeled the stream
    dataset, in that case iterate step two (labeling the data in *Figure 1*), or have
    not selected the proper ML algorithm to train the model, and finally we have not
    tuned what would eventually help us to find the appropriate hyperparameters or
    model selection; in that case, go straight to step seven (model deployment in
    *Figure 1*).'
  prefs: []
  type: TYPE_NORMAL
- en: Tuning the ML model for improvement and model evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As mentioned in step four (model evaluation in *Figure 1*), if the performance
    of the model is not satisfactory or convincing enough, then we need to tune the
    model. As discussed in [Chapter 7](part0059_split_000.html#1O8H62-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 7. Tuning Machine Learning Models"), *Tuning Machine Learning Models*,
    we learned about how to choose the appropriate model and ML algorithms in order
    to produce better predictive analytics. There are several techniques for tuning
    the models, performance and we can go for them based on requirements and the situation.
    When we have done the tuning, finally we should do the model evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Model adaptability and deployment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When we have the tuned and found best model, the machine learning model has
    to be prepared in order to learn incrementally over the new data types when the
    model is updated each time it sees a new training instance. When we have our model
    ready for making the accurate and reliable prediction for the large-scale streaming
    data, we can deploy it in real life.
  prefs: []
  type: TYPE_NORMAL
- en: Time series and social network analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will try to provide some insights and challenges of dealing
    and developing a large-scale ML pipeline from the time series and social network
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Time series analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Time series data often arises, when, for example, monitoring industrial processes
    or tracking corporate business metrics. One of the fundamental differences between
    modeling data via time series methods is that time series analysis accounts for
    the fact that data points taken over time may have an internal structure.
  prefs: []
  type: TYPE_NORMAL
- en: This might include autocorrelation, trends, or seasonal variation that should
    be taken into account. Regression analysis in this regard is mostly used to test
    the theories. The target is to test to make sure that the current values of one
    or more independent times series parameters are correlated to the current properties
    of other time series data.
  prefs: []
  type: TYPE_NORMAL
- en: To develop large scale predictive analytics applications, time series analysis
    techniques can be applied to real-valued, categorical variables, continuous data,
    discrete numeric data, or even discrete symbolic data. A time series is a sequence
    of floating-point values, each linked to a timestamp. In particular, we try as
    hard as possible to stick with *time series* as meaning a univariate time series,
    although in other contexts, it sometimes refers to a series of multiple values
    at the same timestamp.
  prefs: []
  type: TYPE_NORMAL
- en: 'An instant in the time series data is the vector of values in a collection
    of time series corresponding to a single point in time. An observation is a tuple
    (timestamp, key, value), that is, a single value in a time series or instant.
    In a nutshell, a time series has mainly four characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Series with trends**: Since observations increase or decrease over time,
    although the trends are persistent and have long term movement.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Series data with seasonality**: Since observations stay high then drop off
    and some patterns repeat from one period to the next, and contain regular periodic
    fluctuations, say within a 12 month period.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Series data with cyclic component**: Since the business model changes periodically,
    that is, recessions in the business occur in a cyclic order sometimes. It also
    might contain the repeating swings or movements over more than one year.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Random variation**: An unpredictable component that gives time series graphs
    an irregular or zigzag appearance. It also contains erratic or residual fluctuations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because of these kinds of challenging characteristics, it really becomes difficult
    to develop practical machine learning applications for practical purposes. Hence,
    until now, there is only one package available for time series data analysis,
    developed by Cloudera; it is called the Spark-TS library. Here each time series
    is typically labeled with a key that enables identifying it among a collection
    of time series.
  prefs: []
  type: TYPE_NORMAL
- en: However, the current implementation of Spark does not provide any implemented
    algorithm for the time series data analysis. However, since it is an emerging
    and trending topic of interest, hopefully, we will have at least some algorithms
    implemented in Spark in coming releases. In [Chapter 10](part0079_split_000.html#2BASE2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 10.  Configuring and Working with External Libraries"), *Configuring
    and Working with External Libraries*, we will provide more insight into how to
    use these kinds of third-party packages with Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Social network analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A social network is made up of **nodes** (points) and associated **links**,where
    nodes, links, or edges are then identifiable categories of analysis. These nodes
    might include the information about the people, groups, and organizations. Typically,
    this information is usually the main priority and concern for any type of social
    experimentation and analysis. The links in this kind of analysis focus on the
    collective way to include social contacts and exchangeable information to expand
    social interaction, such as Facebook, LinkedIn, Twitter, and so on. Therefore,
    it is obvious that organizations that are embedded in networks of larger social
    processes, links, and nodes, influence the others.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, according to Otte E.et al. (*Social network analysis: A
    powerful strategy, also for the information sciences*, Journal of Information
    Science, 28: 441-453), **Social Network Analysis** (**SNA**) is the study of finding
    the mapping and measuring the relationships between the connected people or groups.
    It is also used to find the flows between people, groups, organizations, and information
    processing entities.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A proper SNA analysis could be used to show the distinction between the three
    most popular individual centrality measures: degree centrality, betweenness centrality,
    and closeness centrality. **Degree centrality** signifies how many links or incidents
    a node has or how many ties a node has.'
  prefs: []
  type: TYPE_NORMAL
- en: The **betweenness centrality** is a centrality measure of a vertex within a
    graph. This also considers the edge of betweenness. Moreover, the betweenness
    centrality signifies the number of times a node acts as a bridge by considering
    the shortest paths between other nodes.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the **closeness centrality** of a node is the average length
    of the shortest path between a particular node and all other nodes in a connected
    graph, such as a social network.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Interested readers are recommended to read more about the eigenvector centrality,
    Katz centrality, PageRank centrality, Percolation centrality, Cross-clique centrality,
    and alpha centrality for the proper understanding of statistical as well as social
    network centrality.
  prefs: []
  type: TYPE_NORMAL
- en: 'The social network is often represented as connected graphs (directed or undirected).
    As a result, it also involves graph data analysis, where people act as nodes and
    the connections or links act as the edges. Moreover, collecting and analyzing
    large-scale data and later on developing predictive and descriptive analytics
    applications from the social network, such as Facebook, Twitter, and LinkedIn
    also involve social network data analysis, including: link prediction such as
    predicting relations or friendship, determining communities in social networks
    such as clustering on graphs, and determining opinion leaders in networks, which
    is essentially a PageRank problem if the proper structure is done on a graph data.'
  prefs: []
  type: TYPE_NORMAL
- en: Spark has its dedicated API for the analysis of graphs, which is called GraphX.
    This API can be used, for instance, to search for spam, rank search results, determine
    communities in social networks, or search for opinion leaders, and it's not a
    complete list of applying methods for analyzing graphs. We will discuss using
    the GraphX later in this chapter in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Movie recommendation using Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Model-based collaborative filtering is commonly being used by many companies,
    such as Netflix, as a recommender system for a real-time movie recommendation.
    In this section, we will see a complete example of how it works towards recommending
    movies for new users.
  prefs: []
  type: TYPE_NORMAL
- en: Model-based movie recommendation using Spark MLlib
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The implementation in Spark MLlib supports the model-based collaborative filtering.
    In the model based collaborative filtering technique, users and products are described
    by a small set of factors, also called the **latent factors** (**LFs**).The LFs
    are then used for predicting the missing entries. Spark API provides the implementation
    of the **Alternating Least Squares** (also known as the **ALS** widely) algorithm,
    which is used to learn these latent factors by considering six parameters, including:'
  prefs: []
  type: TYPE_NORMAL
- en: '`numBlocks`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rank`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`iterations`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lambda`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`implicitPrefs`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`alpha`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To learn more about these parameters, refer to the recommendation system section
    in [Chapter 5](part0043_split_000.html#190862-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 5.  Supervised and Unsupervised Learning by Examples"), *Supervised and
    Unsupervised Learning by Examples*. Note that to construct an ALS instance with
    default parameters, you can set the value based on your requirements. The default
    values are as follows: `numBlocks`: -1, `rank`: 10, `iterations`: 10, `lambda`:
    0.01, `implicitPrefs`: false, `alpha`: 1.0.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The construction of an ALS instance, in short, is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: At first, the ALS, which is an iterative algorithm, is used to model the rating
    matrix as the multiplication of low-ranked users and product factors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After that, the learning task is done by using these factors by minimizing the
    reconstruction error of the observed ratings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, the unknown ratings can successively be calculated by multiplying these
    factors together.
  prefs: []
  type: TYPE_NORMAL
- en: The approach for a movie recommendation or any other recommendation based on
    the collaborative filtering technique used in the Spark MLlib has been proven
    to be a high performer with high prediction accuracy and scalable for the billions
    of ratings on commodity clusters used by companies such as Netflix. By following
    this approach, a company such as Netflix can recommend movies to its subscriber
    based on the predicted ratings. The ultimate target is to increase the sales,
    and of course, the customer satisfaction.
  prefs: []
  type: TYPE_NORMAL
- en: Data exploration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The movie and the corresponding rating dataset were downloaded from the MovieLens
    website ([https://movielens.org](https://movielens.org/movies/1)). According to
    the data description on the MovieLens website, all the ratings are described in
    the `ratings.csv` file. Each row of this file followed by the header represents
    one rating for one movie by one user.
  prefs: []
  type: TYPE_NORMAL
- en: 'The CSV dataset has the following columns: `userId`, `movieId`, `rating`, and
    `timestamp` as shown in *Figure 2*. The rows are ordered first by the `userId`
    within the user, by `movieId`. Ratings are made on a five-star scale; with half-star
    increments (0.5 stars up to 5.0 stars). The timestamps represent the seconds since
    midnight **Coordinated Universal Time** (**UTC**) of January 1, 1970, where we
    have 105,339 ratings from the 668 users on 10,325 movies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Data exploration](img/00123.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Sample ratings for the top 20 movies.'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, the movie information is contained in the `movies.csv` file.
    Each row apart from the header information represents one movie containing the
    columns: `movieId`, `title`, and `genres`.'
  prefs: []
  type: TYPE_NORMAL
- en: Movie titles are either created or inserted manually or imported from the website
    of the movie database at [https://www.themoviedb.org/](https://www.themoviedb.org/).
    The release year, however, is shown in the bracket.
  prefs: []
  type: TYPE_NORMAL
- en: Since movie titles are inserted manually, some errors or inconsistencies may
    exist in these titles. Readers are therefore recommended to check the IMDb database
    ([http://www.ibdb.com/](http://www.ibdb.com/)) to make sure if there are no inconsistencies
    or incorrect titles with their corresponding release year.
  prefs: []
  type: TYPE_NORMAL
- en: 'Genres are a separated list, and are selected from the following genre categories:'
  prefs: []
  type: TYPE_NORMAL
- en: Action, Adventure, Animation, Children's, Comedy, Crime
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentary, Drama, Fantasy, Film-Noir, Horror, Musical
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mystery, Romance, Sci-Fi, Thriller, Western, War
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Data exploration](img/00065.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The title and genres for the top 20 movies.'
  prefs: []
  type: TYPE_NORMAL
- en: Movie recommendation using Spark MLlib
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this subsection, we will show you how to recommend the movie for other users
    through a step-by-step example from data collection to movie recommendation. Download
    the `movies.csv` and `ratings.csv` files from Packt supplementary documents and
    place them in your project directory.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1: Configure your Spark environment**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to configure your Spark environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 2: Load, parse, and explore the movie and rating Dataset**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code illustrated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This code segment should return you the `Dataset<Row>` of the ratings same
    as in *Figure 2*. On the other hand, the following code segment shows you the
    `Dataset<Row>` of movies, same as in *Figure 3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 3: Register both Datasets as temp tables**'
  prefs: []
  type: TYPE_NORMAL
- en: 'To register both Datasets, we can use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This will help to make the in-memory querying faster by creating a temporary
    view as a table in min-memory. The lifetime of the temporary table using the `createOrReplaceTempView()`
    method is tied to the `[[SparkSession]]` that was used to create this Dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 4: Explore and query for related statistics**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s check the ratings related statistics. Just use the following code lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: You should find 105,339 ratings from 668 users on 10,325 movies. Now, let's
    get the maximum and minimum ratings along with the count of users who have rated
    a movie.
  prefs: []
  type: TYPE_NORMAL
- en: However, you need to perform a SQL query on the rating table we just created
    in-memory in the previous step. Making a query here is simple, and it is similar
    to making a query from a MySQL database or RDBMS.
  prefs: []
  type: TYPE_NORMAL
- en: However, if you are not familiar with SQL-based queries, you are suggested to
    look at the SQL query specification to find out how to perform a selection using
    `SELECT` from a particular table, how to perform the ordering using `ORDER`, and
    how to perform a joining operation using the `JOIN` keyword.
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, if you know the SQL query, you should get a new Dataset by using a complex
    SQL query as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![Movie recommendation using Spark MLlib](img/00108.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Maximum and minimum ratings along with the count of users who have
    rated a movie.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To get an insight, we need to know more about the users and their ratings.
    Now let''s find the top most active users and how many times they rated a movie:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![Movie recommendation using Spark MLlib](img/00135.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Number of ratings provided by an individual user.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s have a look at a particular user, and find the movies that, say
    user 668, rated higher than 4:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![Movie recommendation using Spark MLlib](img/00146.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: The related rating for user 668.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 5: Prepare training and test rating data and see the counts**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code illustrated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: You should find that there are 84,011 ratings in the training and 21,328 ratings
    in the test Dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 6: Prepare the data for building the recommendation model using ALS**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code illustrates for building the recommendation model using
    APIs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The `ratingsRDD` RDD will contain the `userId`, `movieId`, and corresponding
    ratings from the training dataset that we prepared in the previous step. On the
    other hand, the following `testRDD` also contains the same information coming
    from the test Dataset we prepared in the previous step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 7: Build an ALS user product matrix**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Build an ALS user matrix model based on the `ratingsRDD` by specifying the
    rank, iterations, and lambda:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Note that we have randomly selected the value of rank as `20` and have iterated
    the model for learning for 10 times and the lambda as `0.01`. With this setting,
    we got a good prediction accuracy. Readers are suggested to apply the hyper-parameter
    tuning to get to know the most optimum values for these parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, readers are suggested to change the value of these two parameters based
    on their dataset. Moreover, as mentioned earlier, they also can use and specify
    other parameters such as `numberblock`, `implicitPrefs`, and `alpha` if the prediction
    performance is not satisfactory. Furthermore, set the number of blocks for both
    user blocks and product blocks to parallelize the computation into `pass -1` for
    an auto-configured number of blocks. The value is `-1`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Step 8: Making predictions**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get the top six movie predictions for user 668:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![Movie recommendation using Spark MLlib](img/00145.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Top six movies rated by the user 668.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 9: Get the predicted ratings to compare with the test ratings**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code illustrated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s check the top 10 prediction for 10 users:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 10: Prepare predictions**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we will prepare the predictions related to RDDs in two steps. The first
    step includes preparing predictions for comparison from the `predictionsForTestRDD`
    RDD structure. It goes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The second step includes preparing the test for comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 11: Join the test with predictions and see the combined ratings**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Join `testKeyedByUserProductRDD` and `predictionsKeyedByUserProductRDD` RDDs
    to get the combined test as well as predicted ratings against each user and `movieId`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![Movie recommendation using Spark MLlib](img/00076.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Combined test as well as predicted ratings against each user and
    movieId.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 12: Evaluating the model against prediction performance**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s check the performance of the ALS model by checking the number of true
    positives and false positives:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now print the number of true positive predictions. We have considered that
    a prediction is a true prediction when the predicted rating is less than the highest
    rating (that is, `5`). Consequently, if the predicted rating is more or equal
    to `5`, consider that prediction as a false positive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: You should find the value as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'The number of true positive prediction is 798\. Now it''s time to print the
    statistics for the false positives:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'In this particular example, we have got only 14 false positive predictions,
    which is outstanding. Now let''s check the performance of the prediction in terms
    of mean absolute error calculation between the test and predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Which returns the value as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Developing a real-time ML pipeline from streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: According to the API guidelines provided by Spark at [http://spark.apache.org/docs/latest/streaming-programming-guide.html](http://spark.apache.org/docs/latest/streaming-programming-guide.html),
    technically, Spark Streaming receives live input data streams as objects (objects
    could be `Java/Python/R` objects). Later on, the streams are divided into batches,
    which are then processed by the Spark engine to generate the final input stream
    in batches. To make this process even easier, Spark Streaming provides a high-level
    abstraction, which is also called a discretized stream or DStream.
  prefs: []
  type: TYPE_NORMAL
- en: The **DStream** represents a continuous stream of data coming from real-time
    streaming sources such as Twitter, Kafka, Fume, Kinesis, Sensors, or any other
    sources. Discretized streams can be created from these sources, alternatively,
    high-level operations on other DStreams can also be applied for doing that. Internally,
    a DStream is represented as a sequence of RDDs, that means the RDD abstraction
    has been reused for processing the stream of RDDs.
  prefs: []
  type: TYPE_NORMAL
- en: As already discussed in [Chapter 6](part0049_split_000.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 6.  Building Scalable Machine Learning Pipelines"), *Building Large Scale
    Machine Learning Pipelines*, a topic modeling technique automatically infers the
    topics discussed and inherently places them in a collection of documents as hidden
    resources. This is commonly used in the **Natural Language Processing** (**NLP**)
    and text mining tasks. These topics can be used to analyze, summarize, and organize
    those documents. Alternatively, those topics can be used for featurization and
    dimensionality reduction in later stages of a **machine learning** (**ML**) pipeline
    development. The most popular topic modeling algorithms are **Latent Dirichlet
    Allocation** (**LDA**) and **Probabilistic Latent Semantic Analysis** (**pLSA**).
    Previously we discussed how to apply the LDA algorithm for the static dataset
    that is already available. However, if the topic modeling is prepared from the
    real-time streaming data, that would be great and would be more live in knowing
    the trends in social media such as Twitter, LinkedIn, or Facebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, due to the limited API facility by Facebook or LinkedIn, it would
    be difficult to collect the real-time data from those social media platforms.
    Spark also provides the API for accessing data from Twitter, Kafka, Flume, and
    Kinesis. The workflow of a near real-time ML application development from streaming
    data should follow the workflows as presented in *Figure 9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Developing a real-time ML pipeline from streaming](img/00129.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Real-time predictive ML model development from streaming data using
    Spark.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will show you how to develop a real-time ML pipeline that
    handles streaming data. More specifically, we will show a step-by-step topic modeling
    from Twitter streaming data. The topic modeling here has two steps: Twitter data
    collection and topic modeling using LDA.'
  prefs: []
  type: TYPE_NORMAL
- en: Real-time tweet data collection from Twitter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Spark provides APIs to access real-time tweets from the Twitter timeline. The
    tweets can be further made by using keywords or hashtags. Alternatively, tweets
    can also be downloaded from someone''s Twitter timeline. However, before accessing
    the tweets data, you will have to create a sample Twitter application on Twitter
    and generate four keys: `consumerKey`, `consumerSecret`, `accessToken`, and `accessTokenSecret`.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that Twitter (and some other) driver support has been removed during Spark
    upgrade from 1.6.2 to 2.0.0\.  This means streaming data collection support using
    Spark from less used streaming connectors, including Twitter, Akka, MQTT, and
    ZeroMQ has been removed in Spark 2.0.0\. Therefore, it i€™s not possible to develop
    an application for Twitter data collection using Spark 2.0.0\. Consequently, Spark
    1.6.1 will be used for the demonstration for Twitter data collection in this section.
    Readers are suggested to create a Maven project on Eclipse using the provided
    Maven friendly `pom.xml` file.
  prefs: []
  type: TYPE_NORMAL
- en: After authenticating your Spark ML application to collect data from Twitter,
    you will have to define the `JavaStreamingContext` by specifying the `SparkConf`
    and duration for collecting tweets. After that, tweets data can be downloaded
    as DStream or discrete stream through the `TwitterUtils` API of Spark once you
    revoke the `start()` method using the `JavaStreamingContext` object.
  prefs: []
  type: TYPE_NORMAL
- en: Upon starting to receive the tweets, you can save the tweets data on your local
    machine or HDFS or any other filesystem where applicable. However, the streaming
    will be continued until you terminate the streaming using `awaitTermination()`.
  prefs: []
  type: TYPE_NORMAL
- en: The received tweets, however, can be also be pre-processed or cleaned by using
    the `foreachRDD` design pattern and then can be saved to your desired location.
    Due to page limitation, we have limited our discussion here.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Moreover, interested readers should follow the API guidelines for Spark Streaming
    in the following web page of Spark:[http://spark.apache.org/docs/latest/streaming-programming-guide.html](http://spark.apache.org/docs/latest/streaming-programming-guide.html).
  prefs: []
  type: TYPE_NORMAL
- en: Tweet collection using TwitterUtils API of Spark
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this sub-section, at first, we will show you how to collect real-time tweets
    data from Twitter using the `TwitterUtils` API. Then the same tweets data will
    be used for topic modeling in the next sub-section.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1: Load required packages and APIs**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to load the required packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 2: Setting the Logger level**'
  prefs: []
  type: TYPE_NORMAL
- en: 'For setting the Logger level, we use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 3: Spark streaming environment setting**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the codes for Spark streaming illustrated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 4: Setting the authentication for accessing Twitter data**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Get the authentications values from the sample Twitter application by visiting
    the following URL: [https://apps.twitter.com/](https://apps.twitter.com/):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that here we have provided the same values for these four secret keys.
    Replace these values with your own keys accordingly. Well, now we need to set
    the system property using `twitter4j.oauth` for the previous four keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 5: Enable the check pointing**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows how to enable the check pointing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The metadata check pointing is primarily needed for recovery from driver failures,
    whereas data or RDD check pointing is necessary even for basic functioning if
    stateful transformations are used. For more details, please visit the following
    web page of Spark: [http://spark.apache.org/docs/latest/streaming-programming-guide.html#checkpointing](http://spark.apache.org/docs/latest/streaming-programming-guide.html#checkpointing).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 6: Start accepting stream of tweets as a discrete stream**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s collect only 100 tweets for simplicity and but can collect as much as
    you want:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 7: Filter tweets and save as regular text files**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for filter tweets is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Here we are pre-processing the tweets using the singleton method, `foreachRDD`,
    which accepts only filtered tweets, that is, if the status count is at least 1\.
    When the number of collected tweets is equal or more than the number of tweets
    to be collected, then we exit the collection. Finally, we save the tweets as texts
    in the output directory.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 8: Controlling the streaming switch**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for controlling the streaming switch is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Eventually, we will use these texts of tweets for the topic modeling in the
    next step. If you recall the topic modeling in [Chapter 6](part0049_split_000.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 6.  Building Scalable Machine Learning Pipelines"), *Building Scalable
    Machine Learning Pipelines*, we saw the corresponding term weight, topic name,
    and term indices. However, we also need to have the actual terms. In the next
    step, we will show the detailed technique of retrieving the terms extensively
    dependent on the vocabulary that needs to be created for that.
  prefs: []
  type: TYPE_NORMAL
- en: Topic modeling using Spark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this sub-section, we represented a semi-automated technique of topic modeling
    using Spark. The following steps show the topic modeling from data reading to
    printing the topics along with their term-weights.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1: Load necessary packages and APIs**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to load the necessary packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 2: Configure the Spark environment**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to configure the Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 3: Setting the logging level**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to set the logging level:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that setting the logging level that was just previously shown is optional.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 4: Create Java RDD and cache them in-memory**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create Java RDD and cache them for the tweets data from the previous step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 5: Tokenize the terms**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the list of stop-words provided by Spark and tokenize the terms by filtering
    them by applying three constraints: text length at least 4, not a stop word, and
    making them each lower case. Note that we discussed stop-words in [Chapter 6](part0049_split_000.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 6.  Building Scalable Machine Learning Pipelines"), *Building Scalable
    Machine Learning Pipelines*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 6: Prepare the term counts**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prepare the term counts by filtering them by applying four constraints: text
    length at least 4, not stop words, selecting only characters, and making them
    all lower case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Note that here `isStopWords()` and `isOnlyLetters()` are two user defined methods
    that will be discussed at the end of this step.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 7: Sort the term counts**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sort the terms counts by applying two transformations, `sortByKey()` and `mapToPair()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 8: Create the vocabulary**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a vocabulary RDD by mapping the sorted term counts. Finally, print the
    key value pairs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see a screenshot of the vocabulary terms and their indices shown in
    *Figure 10*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Topic modeling using Spark](img/00042.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Vocabulary terms and their indices.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 9: Create the document matrix from the tokenized words/terms**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the document matrix as `JavaPairRDD` from the tokenized terms by mapping
    the vocabulary we created in the previous step. After that, cache the RDD in-memory
    for faster processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 10: Train the LDA model**'
  prefs: []
  type: TYPE_NORMAL
- en: Train the LDA model using the documents matrix from step 9 and describe 10 topic
    terms against four topics for simplicity.
  prefs: []
  type: TYPE_NORMAL
- en: Note that here we have used **Latent Dirichilet Allocation** (**LDA**), which
    is one of the most popular topic modeling algorithms commonly used for text mining.
    We could use more robust topic modeling algorithms such as **Probabilistic Latent
    Sentiment Analysis** (**pLSA**), **Pachinko Allocation Model** (**PAM**), or **Hierarchical
    Dirichilet Process** (**HDP**) algorithms. However, pLSA has the overfitting problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, both HDP and PAM are more complex topic modeling algorithms
    used for complex text mining such as mining topics from high dimensional text
    data or documents of unstructured text. Moreover, to this date, Spark has implemented
    only one topic modeling algorithm, that is LDA. Therefore, we have to use LDA
    reasonably:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Note that to keep the topic generation simple, we have set the number of the
    topic as 4 and have iterated the LDA 10 times. Another reason is that in the next
    section we want to show how to connect these four topics through their common
    terms. Readers are recommended to change the value based on their requirements.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 11: Get the topic terms, index, term weights, and total sum across each
    topic**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Get these statistics from the vocabulary and topic description described in
    step 10 and step 8:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'If you look at the preceding code segment carefully, the `vocabKey` indicates
    the corresponding topic term, `vocabIndex` is the index, and `prob` indicates
    the term weight for each term in a topic. The print statements have been used
    to format the outputs. Now let''s see the output that describes four topics for
    simplicity in *Figure 11*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Topic modeling using Spark](img/00098.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Describing four topics.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we mentioned in *step 6*, here we will show how we develop the `isStopWord()`
    method. Just use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'And the `isOnlyLetters()` method goes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will introduce how to parse and handle large-scale graph
    data using GraphX API of Spark to find the connected components from the topics
    data that we got from *Figure 10*.
  prefs: []
  type: TYPE_NORMAL
- en: ML pipeline on graph data and semi-supervised graph-based learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Due to the big data deluge, there has been a large amount of unlabeled data,
    and very small amounts of labeled data. As already discussed, labeling and annotating
    this data is computationally expensive and an obstacle in finding real insight
    from the data. Also, the increasing growth of the social network and media producers
    graph data at scale. These data thrive to develop real-time and large-scale supervised
    learning methods that can use information in the input distribution.
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind the graph-based semi-supervised learning is to construct a graph
    connecting similar data points or components. That lets the hidden and unobserved
    labels be random variables on the nodes of this graph. In this type of learning,
    similar data points can have similar labels and the information *propagates* from
    labeled data points to other data points. A similar restriction also limits the
    ability to express many important steps in typical processing and analytical pipelines.
    However, the graph-based learning is not optimized for an iterative diffusion
    technique such as PageRank since lots of computational aspects are underlying
    and involved.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, due to API restriction and unavailability, we will not discuss this
    graph-based machine learning in detail with suitable examples.
  prefs: []
  type: TYPE_NORMAL
- en: However, in this section, we will provide a graph-based semi-supervised application
    development, which is basically a continuation of the topic modeling that we presented
    in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to GraphX
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GraphX is a comparatively new component in Spark for graphs processing, graph
    analytics, graph visualization, and graph-parallel computation. Actually, the
    original computational aspect of the Spark RDD was extended by introducing a new
    graph abstraction layer as a resilient distributed graph computation that provides
    resilient properties in the graph processing and storage.
  prefs: []
  type: TYPE_NORMAL
- en: To provide the graph related computation, a set of basic operators such as subgraph,
    `jointVertices`, and `aggregateMessages` are exposed by the GraphX. In addition
    to this, it also inherited the optimized variant of the Pregel API in the GraphX
    implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, to simplify graph analytics tasks, GraphX is being enriched and growing
    with a collection of graph algorithms and builders.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will introduce how to parse and handle large-scale graph
    data using the GraphX API of Spark to find the connected components from the topics
    data that we got from *Figure 11*.
  prefs: []
  type: TYPE_NORMAL
- en: Getting and parsing graph data using the GraphX API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this subsection, we will show you how to parse graph data using the GraphX
    API and then the connected components from the graph will be described in the
    next subsection. Due to API limitation in GraphX, we were unable to provide the
    same implementation in Java, but we did so for Scala implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run the following source code, go to your Spark distribution and start the
    Spark shell by providing the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Then the Spark shell will be available with the Spark session. We assume your
    Spark distribution is saved in the `home/spark-2.0.0-bin-hadoop2.7` path. Please
    change the path accordingly in order to run the Spark shell. Also, please save
    the topic terms shown in *Figure 11* to separate text files so that you will be
    able to use those terms for analyzing as graph data before you proceed to follow
    the next steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1: Loading required packages and APIs**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to load the required packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 2: Prepare the Spark environment**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to prepare the Spark environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 3: Parse the topic''s terms and tokenize them**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code illustrates to parse he topic''s terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 4: Create RDDs of documents**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the RDDs of documents as a RDD[(`DocumentID`, (`nodeName`, `wordCount`))]
    notation. For example, RDD[(1L, (Topic_0, 4))]:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding print method generates the output as shown in Figure 12:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting and parsing graph data using the GraphX API](img/00126.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: The nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 5: Make a word document pair**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to make a word document pair:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 6: Create the graph relationships between nodes**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows to create a graph relationships between nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Note; if you want to make the graph connected, but not undirected, then just
    enable the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Immediately after the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Increment the count by 2, that is, `count += 2` to make the changes consistent.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 7: Initialize the graph**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code illustrated here shows how to illustrate the graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Finding the connected components
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: According to the API documentation at [http://spark.apache.org/docs/latest/graphx-programming-guide.html](http://spark.apache.org/docs/latest/graphx-programming-guide.html),
    each connected components of the graph are labeled by the connected components
    algorithm using the lowest-numbered vertex IDs. For example, in a social network
    analysis, the clusters are approximated by the connected components. To make this
    even easier and faster, the GraphX API contains an implementation of the algorithm
    as the `ConnectedComponents` object.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, there''s no Java, Python, or R-based implementation for finding the
    connected components. Therefore, it allows us to compute the connected components
    in the topics we have calculated using the LDA algorithm through one or more term,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'This should produce the output as shown in *Figure 13*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Finding the connected components](img/00085.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Connection between the topics using GraphX.'
  prefs: []
  type: TYPE_NORMAL
- en: If you look the output in *Figure 13* carefully, we have printed the relationships
    with a triplet. It is to be noted that in addition to the support of the vertices
    and edges, Spark GraphX also has the notion of a triplet. More technically, a
    triplet is an object that broadens the Edge object. From the graph perspective,
    it stores the information about an edge and the related vertices adjacent to it
    in a graph.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have shown how to develop large-scale machine learning applications
    from real-time Twitter stream data and graph data. We have discussed the social
    network and time-series data analysis. In addition, we also developed an emerging
    recommendation application by using the content-based collaborative filtering
    algorithms of Spark MLlib to make movie recommendations for users. These applications,
    however, can be extended and deployed for other use cases.
  prefs: []
  type: TYPE_NORMAL
- en: It is worth noting that the current implementation of Spark contains a few implemented
    algorithms for the streaming or network data analysis. However, we can hope that,
    for example, GraphX will be improved in the future and extended for not only Scala,
    but for Java, R, and Python too. In the next chapter, we will focus on how to
    interact with external data sources to make the Spark working environment more
    diverse.
  prefs: []
  type: TYPE_NORMAL
