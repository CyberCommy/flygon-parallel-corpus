- en: Background Processing in Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes includes support for one-off (also known as batch) computation work,
    as well as supporting common use cases for asynchronous background work. In this
    chapter, we look at the Kubernetes concept of job, and its neighbor, CronJob.
    We also look at how Kubernetes handles and supports persistence, and some of the
    options that are available within Kubernetes. We then look at how Kubernetes can
    support asynchronous background tasks and the ways those can be represented, operated,
    and tracked by Kubernetes. We also go over how to set up worker codes operating
    from a message queue.
  prefs: []
  type: TYPE_NORMAL
- en: 'Topics covered in this chapter include:'
  prefs: []
  type: TYPE_NORMAL
- en: Job
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CronJob
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A worker queue example with Python and Celery
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Persistence with Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stateful Sets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Custom Resource Definitions** (**CRDs**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Job
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of what we have covered so far has been focused on continuous, long-running
    processes. Kubernetes also has support for shorter, discrete runs of software.
    A job in Kubernetes is focused on a discrete run that is expected to end within
    some reasonably-known timeframe, and report a success or failure. Jobs use and
    build upon the same construct as the long-running software, so they use the pod
    specification at their heart, and add the concept of tracking the number of successful
    completions.
  prefs: []
  type: TYPE_NORMAL
- en: The simplest use case is to run a single pod to completion, letting Kubernetes
    handle any failures due to a node failure or reboot. The two optional settings
    you can use with jobs are parallelism and completion. Without specifying parallelism,
    the default is `1` and only one job will be scheduled at a time. You can specify
    both values as integers to run a number of jobs in parallel to achieve multiple
    completions, and you can leave completions unset if the job is working from a
    work queue of some form.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to know that the settings for completions and parallelism aren't
    guarantees – so the code within your pods needs to be tolerant of multiple instances
    running. Likewise, the job needs to be tolerant of a container restarting in the
    event of a container failure (for example, when using the `restartPolicy OnFailure`),
    as well as handling any initialization or setup that it needs if on restart it
    finds itself running on a new pod (which can happen in the event of node failure).
    If your job is using temporary files, locks, or working off a local file to do
    its work, it should verify on startup what the state is and not presume the files
    will always be there, in the event of a failure during processing.
  prefs: []
  type: TYPE_NORMAL
- en: When a job runs to completion, the system does not create any more pods, but
    does not delete the pod either. This lets you interrogate the pod state for success
    or failure, and look at any logs from the containers within the pod. Pods that
    have run to completion will not show up in a simple run of `kubectl get pods`,
    but will appear if you use the `-a` option. It is up to you to delete completed
    jobs, and when you use `kubectl delete` to remove the job, the associated pod
    will be removed and cleaned up as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, let''s run an example job to look at how this works. A simple
    job that simply prints `hello world` can be specified with the following YAML:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'And then you can run this job using either `kubectl create` or `kubectl apply`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The expected command of `kubectl get jobs` will show you the jobs that exist
    and their current state. Since this job is so simple, it will likely complete
    before you can run a command to see its current state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'And like pods, you can use the `kubectl describe` command to get more detailed
    state and output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run the command `kubectl get pods`, you won''t see the pod `helloworld-2b2xt`
    in the list of pods, but running `kubectl get pods -a` will show the pods, including
    completed or failed pods that still exist:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'If you just want to see for yourself what the state of the pod was, using `kubectl
    describe` to get the details will show you the information in human, readable
    form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'An example of this is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/a7c7f878-dbeb-4b92-8ccb-9e4d10436dc8.png)'
  prefs: []
  type: TYPE_IMG
- en: If you are making a simple job with a shell script like in this example, it's
    easy to make a mistake. In those cases, the default will be for Kubernetes to
    retry running the pod repeatedly, leaving pods in a failed state in the system
    for you to see. Having a backoff limit, in this case, can limit the number of
    times the system retries your job. If you don't specify this value, it uses the
    default value of six tries.
  prefs: []
  type: TYPE_NORMAL
- en: 'A job with a simple mistake in the command might look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'And looking at the `pods`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The logs for each pod will be available, so you can diagnose what went wrong.
  prefs: []
  type: TYPE_NORMAL
- en: If you do make a mistake, then you may be tempted to make a quick modification
    to the job specification and use `kubectl apply` to fix the error. Jobs are considered
    immutable by the system, so you will get an error if you try to make a quick fix
    and apply it. When you are working with jobs, it is better to delete the job and
    create a new one.
  prefs: []
  type: TYPE_NORMAL
- en: Jobs are not tied to the life cycle of other objects in Kubernetes, so if you
    are thinking about using one to initialize data in a persistence store, remember
    that you will need to coordinate running that job. In cases where you want some
    logic checked every time before a service starts to preload data, you may be better
    off using an initialization container, as we explored in the last chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Some common cases that work well for jobs are loading a backup into a database,
    making a backup, doing some deeper system introspection or diagnostics, or running
    out-of-band cleanup logic. In all these cases, you want to know that the function
    you wrote ran to completion, and that it succeeded. And in the case of failure,
    you may want to retry, or simply know what happened via the logs.
  prefs: []
  type: TYPE_NORMAL
- en: CronJob
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CronJobs are an extension that build on jobs to allow you to specify a recurring
    schedule for when they run. The name pulls from a common Linux utility for scheduling
    recurring scripts called `cron`. CronJobs were alpha as of Kubernetes version
    1.7, and moved to beta in version 1.8, and remain in beta as of version 1.9\.
    Remember that Kubernetes specifications may change, but tend to be fairly solid
    and have expected utility with beta, so the v1 release of CronJobs may be different,
    but you can likely expect it to be pretty close to what's available as of this
    writing.
  prefs: []
  type: TYPE_NORMAL
- en: The specification is highly related to a job, with the primary difference being
    the kind is CronJob and there is a required field schedule that takes a string
    representing the timing for running this job.
  prefs: []
  type: TYPE_NORMAL
- en: 'The format for this string is five numbers, and wildcards can be used. The
    fields represent:'
  prefs: []
  type: TYPE_NORMAL
- en: Minute (0–59)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hour (0–23)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Day of Month (1–31)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Month (1–12)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Day of Week (0–6)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A `*` or? a character can be used in any of these fields to represent that
    any value is acceptable. A field can also include a `*/` and a number, which indicates
    a recurring instance at some interval, specified by the associated number. Some
    examples of this format are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`12 * * * *`: Run every hour at 12 minutes past the hour'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`*/5 * * * *`: Run every 5 minutes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`0 0 * * 0`: Run every Saturday at midnight'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are also a few special strings that can be used for common occurrences
    that are a bit more human readable:'
  prefs: []
  type: TYPE_NORMAL
- en: '`@yearly`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`@monthly`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`@weekly`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`@daily`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '``@hourly``'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A CronJob has five additional fields that you can specify, but which are not
    required. Unlike jobs, CronJobs are mutable (just like pods, deployments, and
    so on), so these values can be changed or updated after the CronJob is created.
  prefs: []
  type: TYPE_NORMAL
- en: The first is `startingDeadlineSeconds`, which, if specified, will put a limit
    on when a job can be started if Kubernetes doesn't meet its specified deadline
    of when to start the job. If the time exceeds `startingDeadlineSeconds`, that
    iteration will be marked as a failure.
  prefs: []
  type: TYPE_NORMAL
- en: The second is `concurrencyPolicy`, which controls whether Kubernetes allows
    multiple instances of the same job to run concurrently. The default for this is
    `Allow`, which will let multiple jobs run at the same time, with alternate values
    of `Forbid` and `Replace`. `Forbid` will mark the following job as a failure if
    the first is still running, and `Replace` will cancel the first job and attempt
    to run that same code again.
  prefs: []
  type: TYPE_NORMAL
- en: The third field is `suspended`, which defaults to `False`, and can be used to
    suspend any further invocations of jobs on the schedule. If a job is already running
    and `suspend` is added to the CronJob specification, that current job will run
    to completion, but any further jobs won't be scheduled.
  prefs: []
  type: TYPE_NORMAL
- en: The fourth and fifth fields are `successfulJobsHistoryLimit` and `failedJobsHistoryLimit`,
    which default to values of `3` and `1` respectively. By default, Kubernetes will
    clean up old jobs beyond these values, but retain the recent success and failures,
    including the logs, so they can be inspected as you wish.
  prefs: []
  type: TYPE_NORMAL
- en: When you create a CronJob, you will also want to choose (and define in your
    specification) a `restartPolicy`. A CronJob doesn't allow for the default value
    of `Always`, so you will want to choose between `OnFailure` and `Never`.
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple CronJob that prints `hello world` every minute might look like the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'After creating this job with `kubectl apply -f cronjob.yaml`, you can see the
    summary output with `kubectl get cronjob`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Or, see more detailed output with `kubectl describe cronjob helloworld`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'As you might guess from this output, the CronJob is actually creating jobs
    to the schedule you define, and from the template in the specification. Each job
    gets its own name based on the name of the CronJob, and can be viewed independently:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/1b610135-65a4-4cd3-8991-e02851bc1d2a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You can see the jobs that are created on the schedule defined from the preceding
    CronJob, by using the `kubectl get jobs` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'And you can view the pods that were created and run to completion from each
    of those jobs, leveraging the `-a` option with `kubectl get pods`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: A worker queue example with Python and Celery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Where the CronJob is well positioned to run repeated tasks at a specific schedule,
    another common need is to process a series of work items more or less constantly.
    A job is well oriented to running a single task until it is complete, but if the
    volume of things you need to process is large enough, it may be far more effective
    to maintain a constant process to work on those items.
  prefs: []
  type: TYPE_NORMAL
- en: 'A common pattern to accommodate this kind of work uses a message queue, as
    shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/587495c0-7d7c-4060-9796-3c2d5e40607f.png)'
  prefs: []
  type: TYPE_IMG
- en: With a message queue, you can have an API frontend that creates the work to
    be run asynchronously, move that into a queue, and then have a number of worker
    processes pull from the queue to do the relevant work. Amazon has a web-based
    service supporting exactly this pattern of processing called **Simple Queue Service**
    (**SQS**). A huge benefit of this pattern is decoupling the workers from the request,
    so you can scale each of those pieces independently, as required.
  prefs: []
  type: TYPE_NORMAL
- en: You can do exactly the same within Kubernetes, running the queue as a service
    and workers connecting to that queue as a deployment. Python has a popular framework,
    Celery, that carries out background processing from a message, supporting a number
    of queue mechanisms. We will look at how you can set up an example queue and worker
    process, and how to leverage a framework such as Celery within Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Celery worker example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Celery has been in development and use since 2009, long before Kubernetes existed.
    It was written expecting to be deployed on multiple machines. This translates
    reasonably well to containers, which our example will illustrate. You can get
    more details about Celery at: [http://docs.celeryproject.org/en/latest/](http://docs.celeryproject.org/en/latest/).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we will set up a service with a deployment of RabbitMQ and
    a separate deployment of our own container, **celery-worker**, to process jobs
    from that queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/28e529ac-48cc-40f5-9217-60753631314a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The deployments and source code for this example are available on GitHub at
    [https://github.com/kubernetes-for-developers/kfd-celery/](https://github.com/kubernetes-for-developers/kfd-celery/).
    You can get this code by using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: RabbitMQ and configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This example uses a container that includes RabbitMQ from Bitnami. The source
    for that image is available at [https://github.com/bitnami/bitnami-docker-rabbitmq](https://github.com/bitnami/bitnami-docker-rabbitmq),
    and the container images are hosted publicly on DockerHub at [https://hub.docker.com/r/bitnami/rabbitmq/](https://hub.docker.com/r/bitnami/rabbitmq/).
  prefs: []
  type: TYPE_NORMAL
- en: RabbitMQ has a large number of options that can be used with it, and a variety
    of ways to deploy it, including in clusters to support HA. For this example, we
    are using a single RabbitMQ replica within a deployment to back a service called
    `message-queue`. We also set up a `ConfigMap` with some of the variables that
    we might want to adjust for a local setting, although in this example, the values
    are the same as the defaults within the container. The deployment does use a persistent
    volume to enable persistence for the queue in the event of a failure. We will
    go into persistent volumes and how to use them later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `ConfigMap` we create will be used by both the RabbitMQ container and our
    worker deployment. The `ConfigMap` is called `queue-config.yaml` and reads as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'To deploy it, you can use this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The `ConfigMap` was created based on the documentation for the Bitnami RabbitMQ
    container, which supports setting a number of configuration items through environment
    variables. You can see all the details that the container could take at the Docker
    Hub web page, or at the source in GitHub. In our case, we set some of the most
    common values.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**: You would probably want to set username and password more correctly
    with secrets instead of including the values in a `ConfigMap`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the specification for the deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/acdf2ee8-f989-4935-8ff0-abefc68f18ee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And this is how to deploy the instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Celery worker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To create a worker, we made our own container image very similar to the Flask
    container. The Dockerfile uses Alpine Linux and explicitly loads Python 3 onto
    that image, then installs the requirements from a `requirements.txt` file and
    adds in two Python files. The first, `celery_conf.py`, is the Python definition
    for a couple of tasks taken directly from the Celery documentation. The second, `submit_tasks.py`,
    is a short example that is meant to be run interactively to create work and send
    it over the queue. The container also includes two shell scripts: `run.sh` and
    `celery_status.sh`.
  prefs: []
  type: TYPE_NORMAL
- en: In all of these cases, we used environment variables that we source from the
    preceding `ConfigMap` to set up the logging output from the worker, as well as
    the host, username, and password for communicating with RabbitMQ within Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Dockerfile uses the `run.sh` script as its command, so that we can use
    this shell script to set up any environment variables and invoke Celery. Because
    Celery was originally written as a command-line tool, using a shell script to
    set up and invoke what you want is very convenient. Here is a closer look at `run.sh`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/02e28d34-3a15-4bf3-9030-3533a0120941.png)'
  prefs: []
  type: TYPE_IMG
- en: The script sets two shell script options, `-e` and `-x`. The first, (`-e`),
    is to make sure that if we make a typo or a command in the script returned an
    error, the script itself will return an error. The second, (`-x`), echoes the
    commands invoked in the script to `STDOUT`, so we can see that in the container
    log output.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next line with `DEBUG_LEVEL` uses the shell to look for a default environment
    variable: `WORKER_DEBUG_LEVEL`. If it''s set, it will use it, and `WORKER_DEBUG_LEVEL`
    was added earlier to the `ConfigMap`. If the value isn''t set, this will use a
    default of `info` in its place, so if the value is missing from the `ConfigMap`,
    we will still have a reasonable value here.'
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier, Celery was written as a command-line utility and takes
    advantage of Python's module loading to do its work. Python module loading includes
    working from the current directory, so we explicitly change to the directory containing
    the Python code. Finally, the script invokes the command to start a Celery worker.
  prefs: []
  type: TYPE_NORMAL
- en: We use a similar structure in the script, `celery_status.sh`, which is used
    to provide an exec command used in both liveness and readiness probes for the
    worker container, with the key idea being if the command `celery status` returns
    without an error, the container is communicating effectively with RabbitMQ and
    should be fully able to process tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The code that contains the logic that will be invoked is all in `celery_conf.py:`
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/e8492df5-f86d-4f03-8d02-cb9e5a2a3834.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see that we again make use of environment variables to get the values
    needed to communicate with RabbitMQ (a hostname, username, password, and `vhost`)
    and assemble these from environment variables with defaults, if they're not provided.
    The hostname default (`message-queue`) also matches the service name in our service
    definition that fronts RabbitMQ, giving us a stable default. The remainder of
    the code comes from the Celery documentation, supplying two sample tasks that
    we can import and use separately as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can deploy the worker using this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This should report the deployment created, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'And now, you should have two deployments running together. You can verify this
    with `kubectl get pods`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'To watch the system a bit more interactively, run this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This will stream the logs from the `celery-worker`, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/17a0f4de-79ed-45b2-8d96-f703686865a8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This will display the logs from the `celery-worker` deployment, as they happen.
    Open a second Terminal window and invoke the following command to run a temporary
    pod with an interactive shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Within the shell, you can now run the script to generate some tasks for the
    worker to process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'An example of this script is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/a99ef267-0e69-4f89-b00e-69f57f3a17b0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This script will run indefinitely, invoking the two sample tasks in the worker,
    roughly every five seconds, and in the window that is showing you the logs, you
    should see the output update with logged results from the Celery worker:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/15f20ba0-d087-4589-8b67-98d7c1aec03b.png)'
  prefs: []
  type: TYPE_IMG
- en: Persistence with Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, all our examples, and even code, have been essentially stateless. In
    the last chapter, we introduced a container using Redis, but didn't specify anything
    special for it. By default, Kubernetes will assume any resources associated with
    a pod are ephemeral, and if the node fails, or a deployment is deleted, all the
    associated resources can and will be deleted with it.
  prefs: []
  type: TYPE_NORMAL
- en: That said, almost all the work we do requires storing and maintaining state
    somewhere—a database, an object store, or even a persistent, in-memory queue.
    Kubernetes includes support for persistence, and as of this writing, it's still
    changing and evolving fairly rapidly.
  prefs: []
  type: TYPE_NORMAL
- en: Volumes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The earliest support in Kubernetes was for volumes, which can be defined by
    the cluster administrator, and we've already seen some variations of this construct
    with the configuration being exposed into a container using the Downward API back
    in [Chapter 4](a210420d-4d80-43c1-9acb-531bc6b19b75.xhtml), *Declarative Infrastructure*.
  prefs: []
  type: TYPE_NORMAL
- en: Another kind of volume that can be easily used is `emptyDir`, which you can
    use in a pod specification to create an empty directory, and mount it to one or
    more of the containers in your pod. This is typically created on whatever storage
    the local node has available, but includes an option to specify a medium of *memory*,
    which you can use to make a temporary memory-backed filesystem. This takes up
    more memory on the node, but creates a very fast, ephemeral file system for use
    by your pods. If your code wants to use some scratch space on disk, maintain a
    periodic checkpoint, or load ephemeral content, this can be a very good way of
    managing that space.
  prefs: []
  type: TYPE_NORMAL
- en: As we specified in the configuration, when you use a volume, you specify it
    under volumes and make a related entry under `volumeMounts` that indicates where
    you're using it on each container.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can modify our Flask example application to have a cache space that is a
    memory-backed temporary space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'If we deploy this version of the specification and open an interactive shell
    in the container, you can see `/opt/cache` listed as a volume of type `tmpfs`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'If we had specified the volume without the medium of type `Memory`, the directory
    would show up on the local disk instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: If you are using volumes on a cloud provider, then you can use one of their
    persistent volumes. In these cases, you need to have created a persistent disk
    at the cloud provider that is accessible to the nodes where you have your Kubernetes
    cluster, but this allows you to have data exist beyond the lifetime of any pod
    or node. The volumes for each cloud provider are specific to each provider, for
    example, `awsElasticBlockStore`, `azureDisk`, or `gcePersistentDisk`.
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of other volume types available, and most of these depend
    on how your cluster was set up and what might be available in that setup. You
    can get a sense of all the supported volumes from the formal documentation for
    volumes at [https://kubernetes.io/docs/concepts/storage/volumes/.](https://kubernetes.io/docs/concepts/storage/volumes/)
  prefs: []
  type: TYPE_NORMAL
- en: PersistentVolume and PersistentVolumeClaim
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you want to use persistent volumes, independent of the specific location
    where you have built your cluster, you probably want to take advantage of two
    newer Kubernetes resources: `PersistentVolume` and `PersistentVolumeClaim`. These
    separate the specifics of how volumes are provided and allow you to specify more
    of how you expect those volumes to be used, with both falling under the idea of
    dynamic volume provisioning—meaning that when you deploy your code into Kubernetes,
    the system should make any persistent volumes available from disks that have already
    been identified. The Kubernetes administrator will need to specify at least one,
    and possibly more, storage classes, which are used to define the general behavior
    and backing stores for persistent volumes available to the cluster. If you''re
    using Kubernetes on Amazon Web Services, Google Compute Engine, or Microsoft''s
    Azure, the public offerings all have storage classes predefined and available
    for use. You can see the default storage classes and how they are defined in the
    documentation at [https://kubernetes.io/docs/concepts/storage/storage-classes/](https://kubernetes.io/docs/concepts/storage/storage-classes/).
    If you are using Minikube locally to try things out, it also comes with a default
    storage class defined, which uses the volume type of `HostPast`.'
  prefs: []
  type: TYPE_NORMAL
- en: Defining a `PersistentVolumeClaim` to use with your code in a deployment is
    very much like defining the configuration volume or cache with `EmptyDir`, with
    the exception that you will need to make a `persistentVolumeClaim` resource before
    you reference it in your pod specification.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example `persistentVolumeClaim` that we might use for our Redis storage
    might be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'This will create a 1 GB volume available for our container to use. We can add
    this onto the Redis container to give it persistent storage by referencing this
    `persistentVolumeClaim` by name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The `mountPath` of `/data` was chosen to match how the Redis container was built.
    If we look at the documentation for that container (from [https://hub.docker.com/_/redis/](https://hub.docker.com/_/redis/)),
    we can see that the built-in configuration expects all data to be used from the `/data` path,
    so we can override that path with our own `persistentVolumeClaim` in order to
    back that space with something that will live beyond the life cycle of the deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you deployed these changes to Minikube, you can see the resulting resources
    reflected within the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also open an interactive Terminal into the Redis instance to see how
    it was set up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Stateful Sets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Following dynamic provisioning, as you think about persistence systems – whether
    they are classic databases, key-value data stores, memory caches, or document-based
    datastores – it is common to want to have some manner of redundancy and failover.
    ReplicaSets and deployments go a fairly significant way to supporting some of
    that capability, especially with persistent volumes, but it would be greatly beneficial
    to these systems to have them more fully integrated with Kubernetes, so that we
    can leverage Kubernetes to handle the life cycle and coordination of these systems.
    A starting point for this effort is Stateful Sets, which act similarly to a deployment
    and ReplicaSet in that they manage a group of pods.
  prefs: []
  type: TYPE_NORMAL
- en: Stateful Sets differ from the other systems as they also support each pod having
    a stable, unique identity and specific ordered scaling, both up and down. Stateful
    Sets are relatively new in Kubernetes, first appearing in Kubernetes 1.5, and
    moving into beta in version 1.9\. Stateful Sets also work closely with a specific
    service we touched upon earlier, a Headless Service, which needs to be created
    prior to the Stateful Set, and is responsible for the network identify of the
    pods.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a reminder, a Headless Service is a service that does not have a specific
    cluster IP, and instead provides a unique service identity to all the pods associated
    with it as individual endpoints. This means that any system consuming the service
    will need to know that the service has identity-specific endpoints, and needs
    to be able to communicate with the one it wants. When a Stateful Set is created
    along with a headless service to match it, the pods will get an identity based
    on the name of the Stateful Set and then an ordinal number. For example, if we
    created a Stateful Set called datastore and requested three replicas, then the
    pods would be created as `datastore-0`, `datastore-1`, and `datastore-2`. Stateful
    Sets also have a `serviceName` field that gets included in the domain name of
    the service. To complete this example, if we set the `serviceName` to `db`, the
    associated DNS entries created for the pods would be:'
  prefs: []
  type: TYPE_NORMAL
- en: '`datastore-0.db.[namespace].svc.cluster.local`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`datastore-1.db.[namespace].svc.cluster.local`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`datastore-2.db.[namespace].svc.cluster.local`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As the number of replicas is changed, a Stateful Set will also explicitly and
    carefully add and remove pods. It terminates the pods sequentially from the highest
    number first, and will not terminate a higher-numbered pod until lower-numbered
    pods are reporting `Ready` and `Running`. As of Kubernetes 1.7, Stateful Sets
    introduced the ability to vary this with an optional field, `podManagementPolicy`.
    The default, `OrderedReady`, operates as described, with the alternative, `Parallel`,
    which does not operate sequentially nor require lower-numbered pods to be `Running`
    or `Ready` prior to terminating a pod.
  prefs: []
  type: TYPE_NORMAL
- en: Rolling updates, akin to a deployment, are also slightly different on Stateful
    Sets. It is defined by the `updateStrategy` optional field, and if not explicitly
    set, uses the `OnDelete` setting. With this setting, Kubernetes will not delete
    older pods, even after a spec update, requiring you to manually delete those pods.
    When you do, the system will automatically recreate the pods, according to the
    updated spec. The other value is `RollingUpdate`, which acts more akin to a deployment
    in terminating and recreating the pods automatically, but following the ordering
    explicitly, and verifying that the pods are *ready and running* prior to continuing
    to update the next pod. `RollingUpdate` also has an additional (optional) field, `partition`,
    which if specified with a number, will have the `RollingUpdate` operate automatically
    on only a subset of the pods. For example, if partition was set to `3` and you
    had `6` replicas, then only pods `3`, `4`, and `5` would be updated automatically
    as the spec was updated. Pods `0`, `1`, and `2` would be left alone, and even
    if they were manually deleted, they will be recreated at the previous version.
    The partition capability can be useful to stage an update or to perform a phased
    rollout.
  prefs: []
  type: TYPE_NORMAL
- en: A Node.js example using Stateful Set
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code within the application doesn't have any need for the Stateful Set mechanics,
    but let's use it as an easy-to-understand update to show how you might use a Stateful
    Set and how to watch it operate.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for this update is available on GitHub with the project at: [https://github.com/kubernetes-for-developers/kfd-nodejs](https://github.com/kubernetes-for-developers/kfd-nodejs)
    on the branch 0.4.0\. The project''s code wasn''t changed, except that the deployment
    specification was changed to a Stateful Set. You can use the following commands
    to get the code for this version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The service definition changed, removing the `Nodeport` type and setting `clusterIP`
    to `None`. The new definition for `nodejs-service` now reads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'This sets up a headless service to use with the Stateful Set. The change from
    deployment to Stateful Set is equally simple, replacing type `Deployment` with
    type `StatefulSet` and adding values for `serviceName`, replicas, and setting
    a selector. I went ahead and added a datastore mount with a persistent volume
    claim as well, to show how that can integrate with your existing specification.
    The existing `ConfigMap`, `livenessProbe`, and `readinessProbe` settings were
    all maintained. The resulting `StatefulSet` specification now reads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we updated the code in the last chapter to use Redis with its readiness
    probes, we will want to make sure we have Redis up and running for this Stateful
    Set to advance. You can deploy the updated Redis service definition set with this
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can take advantage of the watch option (`-w`) with `kubectl get` to
    watch how Kubernetes sets up the Stateful Set and carefully progresses. Open an
    additional terminal window and run this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: At first, you shouldn't see any output, but updates will appear as Kubernetes
    advances through the Stateful Set.
  prefs: []
  type: TYPE_NORMAL
- en: 'In your original Terminal window, deploy the specification we''ve updated to
    be a `StatefulSet` with this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see a response that the `service`, `configmap`, and `statefulset`
    objects were all created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'In the window where you''re watching the pods, you should see output start
    to appear as the first container comes online. A line will appear in the output,
    every time the watch triggers that there''s an update to one of the pods matching
    the descriptor we set (`-l app=nodejs`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The definition we set has five replicas, so five pods will be generated in
    all. You can see the status of that rollout with this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding command, `sts` is a shortcut for `statefulset`. You can also
    get a more detailed view on the current state in a human-readable form with this
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/c12e0f06-ca4c-40e6-af6e-b80d2b8854b0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If you edit the specification, change the replicas to two, and then apply the
    changes, you will see the pods get torn down in the reverse order to how they
    were set up—highest ordinal number first. The following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Should report back:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: And in the window where you're watching the pods, you will see `nodejs-4` start
    terminating, and it will continue until `nodejs-3` and then `nodejs-2` terminates.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you were to run a temporary `pod` to look at DNS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'You can use the `nslookup` command to verify the DNS values for the `pods`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Custom Resource Definition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Stateful Sets don't automatically match all the persistent stores that are available,
    and some of them have even more complex logic requirements for managing the life
    cycle of the application. As Kubernetes looked at how to support extending its
    controllers to support more complex logic, the project started with the idea of
    Operators, external code that could be included in the Kubernetes project, and
    has evolved as of Kubernetes 1.8 to make this more explicit with `CustomResourceDefinitions`.
    A custom resource extends the Kubernetes API, and allows for custom API objects
    to be created and matched with a custom controller that you can also load into
    Kubernetes to handle the life cycle of those objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Custom Resource Definitions go beyond the scope of what we will cover in this
    book, although you should be aware that they exist. You can get more details about
    Custom Resource Definitions and how to extend Kubernetes at the project''s documentation
    site: [https://kubernetes.io/docs/concepts/api-extension/custom-resources/.](https://kubernetes.io/docs/concepts/api-extension/custom-resources/)'
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of Operators available via open source projects that utilize
    Custom Resource Definitions to manage specific applications within Kubernetes.
    The CoreOS team supports operators and custom resources for managing Prometheus
    and `etcd`. There is also an open source storage resource and associated technology
    called Rook, which functions using Custom Resource Definitions.
  prefs: []
  type: TYPE_NORMAL
- en: The broad set of how to best run persistent stores with Kubernetes is still
    evolving, as of this writing. There are numerous examples of how you can run a
    database or NoSQL data store of your choice in Kubernetes that also supports redundancy
    and failover. Most of these systems were created with a variety of mechanisms
    to support managing them, and few of them have much support for automated scaling
    and redundancy. There are a number of techniques supporting a wide variety of
    data stores that are available as examples. Some of the more complex systems use
    Operators and these Custom Resource Definitions; others use sets of pods and containers
    with simpler Replica Sets to achieve their goals.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we reviewed jobs and CronJobs, which Kubernetes provides to
    support batch and scheduled batch processing, respectively. We also looked through
    a Python example of how to set up a Celery worker queue with RabbitMQ and configure
    the two deployments to work together. We then looked at how Kubernetes can provide
    persistence with volumes, `PersistentVolume`, and its concept of `PersistentVolumeClaims`
    for automatically creating volumes for deployments as needed. Kubernetes also
    supports Stateful Sets for a variation of deployment that requires stable identity
    and persistent volumes, and we looked at a simple Node.js example converting our
    previous example of a deployment into a Stateful Set. We finished the chapter
    with a look at Custom Resource Definitions, used to extend Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we start to look at how to leverage Kubernetes to get information
    about all these structures. We review how to capture and view metrics, leveraging
    Kubernetes and additional open source projects, as well as examples of collating
    logs from the horizontally-scaled systems that Kubernetes encourages.
  prefs: []
  type: TYPE_NORMAL
