- en: Deployment, Logging, and Monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"Tactics without strategy is the noise before defeat."'
  prefs: []
  type: TYPE_NORMAL
- en: '- Sun Tzu'
  prefs: []
  type: TYPE_NORMAL
- en: We need a very strong deployment strategy before going live to production and
    starting to earn revenue. Lack of planning always results in an unforeseen emergency,
    which leads to drastic failures. That's what we are going to do in this chapter.
    Now that we are done with our development stuff and have added double checks by
    testing and providing documentation, we are now going to target our *Go Live Phase*.
    We will see all aspects involved in deployment including current trending terms—continuous
    integration, continuous delivery, and the new serverless architecture. We will
    then see the need for logs and how to create a custom centralized logging solution.
    Moving further, we will look at **Zipkin**—an emerging tool for logging in distributed
    systems. In the end, we are going to look at monitoring challenges. We will look
    at two famous tools—**Keymetrics** and **Prometheus**.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Deployment 101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to Docker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serverless architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logging 101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customized logging using ELK
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed tracing using Zipkin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring 101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring with tools such as Keymetrics, Prometheus, and Grafana
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Releasing an application in a production environment with sufficient confidence
    that it is not going to crash or lose organization money is a developers dream.
    Even a manual error, such as not loading proper configuration files, can cause
    huge problems. In this section, we will see how to automate most things and become
    aware of continuous integration and continuous delivery (CI and CD). Let's get
    started with understanding the overall build pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Deciding release plan
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While it is good to have confidence, it is bad to have overconfidence. We should
    always be ready for rolling back the new changes in case of major critical issues
    while deploying to production. An overall build pipeline is needed as it helps
    us to plan for the overall process. We will adopt this technique while doing a
    production build:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/727d2a3e-ddc3-4320-8e5e-3dfb23ff35ac.png)'
  prefs: []
  type: TYPE_IMG
- en: Build pipeline
  prefs: []
  type: TYPE_NORMAL
- en: The overall build process begins by the **Start** block. Whenever any commit
    occurs, WebHooks (provided by both Bitbucket and GitHub) trigger the build pipeline.
    Bitbucket has build pipeline tools, too ([https://bitbucket.org/product/features/pipelines](https://bitbucket.org/product/features/pipelines)).
    This build pipeline can be triggered whenever there is a merge in the master branch.
    Once it comes to the build stage, we first run some code coverage analysis and
    unit tests. If the test results do not meet required SLAs, we abort the process.
    If it meets the overall SLAs, we then create an image out of it and build it on
    the staging server (if we do not have a staging server, we can directly move to
    the production server). Once you have a docker image ready, you set the environment
    depending on where you are deploying. Afterwards, some sanity checks are run to
    make sure that we don't deploy broken code. To run them at all, levels in the
    pipeline is an excellent idea that minimizes chances of error. Now, once the service
    meets SLAs, it's now time to deploy it on a real environment. A good practice
    that I usually follow is production servers should not have version controls.
    Depending on whatever tool we use (OpenShift, Kubernetes, Docker, and so on) we
    pass those tools to start the image. We then need to start integration testing,
    which will include things such as checking whether or not the container is healthy
    and checking with the service registry and API Gateway whether the service is
    registered or not. In order to make sure that nothing breaks, we need a rolling
    update where we deploy new instances and remove old instances one at a time. Our
    code base should be able to handle old/legacy code and it should be deprecated
    only after acceptance from every dependent. After completing integration testing,
    the next task involves running contract testing and acceptance testing. Once these
    tests have been successfully run, we can move from staging to production or going
    live. If the pipeline fails, the previous last successful source code is deployed
    back as part of a rollback strategy.
  prefs: []
  type: TYPE_NORMAL
- en: 'The entire process should be automated as we are more prone to error. We will
    look at CI/CD and how they make our life a lot easier. CI/CD promises that we
    could deploy a feature whenever it is complete and still be pretty confident that
    it won''t break the product. The pipeline we looked at has loads of tasks and
    stages associated with it. Let''s look at the following stages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dev stage/feature branch**: We start our development by creating feature
    branches. We keep the master as it is and only keep verified and tested code in
    the master branch. This way our production is a replica of the master branch and
    we can do any number of experiments in the development branch. If something fails,
    we can always revert back to the master branch and discard or delete a branch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Testing stage/QA branch**: Once our development is done, we push the code
    to the QA branch. Once our development is done, we push the code to the QA branch
    so it can be tested. Modern development approaches go one step beyond and we go
    with TDD/BDD. Whenever we push the code to QA we run test cases to get exact code
    coverage. We run some lint tools, which give us an idea about code quality. After
    all that, if these tests are successful, then only, do we push the code to the
    QA branch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Release stage/master branch**: Once our QA is done and our test cases coverage
    are passed, we push the code to the master in hopes of getting it pushed to production.
    We again run our test cases and code coverage tools and check whether something
    has been broken or not. Once successful, we push the code to the production server
    and run some smoke testing and contract testing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Released/tag**: Once the code is pushed to production and it runs successfully,
    we create a branch/tag for the release. This helps us to make sure that we can
    return to this point in the near future.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Doing such processes at each and every stage manually is a cumbersome process.
    We need automation as humans are prone to error. We need a continuous delivery
    mechanism where a single commit in my code ensures me that the code deployed is
    safe for my ecosystem. In the next section, we are going to look at continuous
    integration and continuous delivery:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Continuous integration**: It is the practice of integrating or merging a
    new feature from other branches to the master and making sure that new changes
    don''t break the existing features. A common CI workflow is that, along with the
    code, you also write test cases. Then you create a pull request representing the
    change. Build software that can run tests, check for code coverage, and decide
    whether the pull request is acceptable or not. Once the **Pull Request** (**PR**)
    is merged, it goes into the CD portion—that is continuous delivery.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Continuous delivery:** It is an approach wherein we aim to deliver a small,
    testable, and easily deployable piece of code seamlessly at any point in time.
    CD is highly automatable, and in some tools, it is highly configurable. Such automation
    helps in quickly distributing components, features, and fixes to customers and
    gives anyone an exact idea as to how much and what is present in a production
    environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With constant improvement in DevOps and the rise of containers, there is a rise
    in new automation tools to help with the CI/CD pipeline. These tools integrate
    with day to day tools, such as code repository management (GitHub can be used
    with Travis and CircleCI, and Bitbucket can be used with Bitbucket pipelines)
    to tracking systems such as slack and Jira. Also, a new trend is emerging, which
    is serverless deployment, where developers just have to worry about their code
    and deployments and other headaches would be sorted by the providers (for example,
    Amazon has AWS and Google has GCP functions). In the next section, we are going
    to look at various available deployment options.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment options
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will look at some of the famous deployment options available
    and look at each of their strengths and weaknesses. We will start with the world
    of containers and look at why everything is dockerized these days. So, let's get
    started.
  prefs: []
  type: TYPE_NORMAL
- en: Before we get started, let's get acquainted with DevOps 101 so as to understand
    all terminologies that we will be using.
  prefs: []
  type: TYPE_NORMAL
- en: DevOps 101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will look at some basic DevOps fundamentals. We will understand
    what is a container and what advantages it has. We will see the difference between
    containers and VM machines.
  prefs: []
  type: TYPE_NORMAL
- en: Containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As cloud computing advances, the world is seeing the re-entry of containers
    systems. Containers have become widely adopted due to simplification in technology
    (Docker follows the same commands as GIT). Containers give private spaces on top
    of operating systems. This technique is also termed as virtualization in the system.
    Containers are easy mechanisms to build, package, and run compartmentalized (software
    residing and limiting to that container only). Containers handle their own filesystem,
    network information, inbuilt internal processes, OS utilities, and other application
    configuration. Containers ship multiple software inside it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Containers have the following advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Independent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lightweight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy to scale
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy to move
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lower license and infrastructure cost
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automated via DevOps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Version controlled just like GIT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reusable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Immutable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Containers versus Virtual Machine (VMs)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While a birds eye's view would seem both are stating the same thing, containers
    and VMs are hugely different. VM's provide hardware virtualization, too, such
    as number of CPUs, memory storage, and so on. VM is a separate unit along with
    OS. VMs replicate the full operating system, thus they are heavyweight. VM offers
    complete isolation to the processes running on it, but it limits the number of
    VMs that can be spun up as it is heavyweight and resource consuming and it will
    take effort to maintain it. Unlike VMs, containers share kernels and host systems
    and thus resource utilization of containers is very less. Containers are lightweight
    as they provide an isolation layer on top of a host operating system. A developer's
    life becomes much easier as container images can be made publicly available (there
    is a huge Docker repository). The lightweight nature of containers helps to automate
    builds, publish artifact anywhere, download and copy on a need basis, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Docker and the world of containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Virtualization is one of the biggest trends right now in DevOps. Virtualization
    enables us to share hardware across various software instances. Just like microservices
    supports isolation, Docker provides resources isolation by creating containers.
    Using a Docker container for microservices makes it possible to package the entire
    service along with its dependencies within a container and run it on any server.
    Wow! Gone are the days of installing software in each environment. Docker is an
    open source project used to pack, ship, and run any application as a lightweight
    container without much hassle to install everything on a new environment again.
    Docker containers are both platform and hardware agnostic, making it easy to run
    a container anywhere, right from a laptop to any server without using any particular
    language framework or packaging software. Containerization is nowadays referred
    to as dockerization. We are already dockerized starting from [Chapter 2](54b04f3b-39de-4147-9aa2-0b0a242515c5.xhtml),
    *Gearing up for the Journey*. So let's understand the overall process and concepts
    involved.
  prefs: []
  type: TYPE_NORMAL
- en: We already saw the installation of Docker in [Chapter 2](54b04f3b-39de-4147-9aa2-0b0a242515c5.xhtml),
    *Gearing up for the Journey*. Now, let's dive deep into understanding Docker.
  prefs: []
  type: TYPE_NORMAL
- en: Docker components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Docker has the following three components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Docker client**: Docker client is a command-line program that actually talks
    with the Docker daemon residing inside the Docker host through either socket based
    communication or communication over REST APIs. A Docker client with CLI options
    is used to build, package, ship, and run any Docker container.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Docker host**: Docker host is basically a server-side component that includes
    a docker daemon, containers, and images:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker daemon is a server-side component that runs on the host machine and contains
    the script for building, packaging, running, and distributing Docker containers.
    Docker daemon exposes RESTful APIs for the Docker client as one of the ways to
    interact with itself.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Along with the Docker daemon, Docker host also includes containers and images
    running in that particular container. Whatever containers are up and running,
    Docker host contains a list of those along with options such as starting, stopping,
    restarting, log files, and so on. Docker images are those that are either built
    or pulled from public repositories.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Docker registry**: The registry is a publicly available repository, just
    like GitHub. Developers can push their container image there, make it some common
    library, or use it as version control among a team.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following diagram, we can see the overall flow among all three Docker
    components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/da4d11d3-3d0a-4817-97ae-abd01f7647c0.png)'
  prefs: []
  type: TYPE_IMG
- en: Docker components and flow
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a typical Docker flow:'
  prefs: []
  type: TYPE_NORMAL
- en: Whenever we run any command such as `sudo docker run ubuntu /bin/echo 'hello
    carbon five!'`, the command goes to a daemon. It tries to search whether there
    is any existing image with the name Ubuntu. If not it goes to the registry and
    finds the image there. From there it will download that container image inside
    the host, create a container, and run the `echo` command. It adds the Ubuntu image
    in the list of images available inside the docker host.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Most of our images would be on top of available images in the Docker Hub repository
    ([https://hub.docker.com/](https://hub.docker.com/)). We don't reinvent the wheel
    until and unless it is very much required. Docker pull issues a command to Docker
    host to pull out a particular image from the repository and make it available
    in the list of images in Docker host.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `docker build` command builds Docker images from a Dockerfile and an available
    context. A build's context is the set of files that are located in the specified
    PATH or URL mentioned in Dockerfile. The build process can refer to any of the
    files in the context. For example in our case, we downloaded Node.js and then
    did `npm install` based on `package.json`. Docker build creates an image and makes
    it available in the list of images inside Docker host.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Docker concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have understood the core Docker processes, let''s move on to understanding
    various concepts involved in Docker. These concepts will make our lives easy to
    write Docker files and create our own microservice container image:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Docker image**: A Docker image is just a snapshot of the components that
    make up Docker''s business capability. It is a read-only copy of OS libraries,
    applications, and its dependencies. Once an image is created it will run on any
    Docker platform without any problems. For example, a Docker image for our microservice
    would contain all components required to fulfill the business capability achieved
    by that microservice. In our case, web server (NGINX), Node.js, PM2, and database
    (NoSQL or SQL) are all configured for runtime. So, when someone who wants to use
    that microservice or deploy it somewhere, they just need to download the image
    and run it. The image would contain all layers right from Linux kernel (`bootfs`)
    to OS (Ubuntu/CentOS) to application environment needs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Docker containers**: A Docker container is just a running instance of a Docker
    image. You download (or build) or pull a Docker image. It runs in a Docker container.
    Containers use the kernel of the host operating system on which the image has
    been run. So they essentially share the host kernel with other containers running
    on the same host (as seen in the preceding diagram). Docker runtime ensures that
    containers have their own isolated process environment as well as filesystem and
    network configurations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Docker Registry**: Docker Registry is just like GitHub, a central place where
    Docker images are published and downloaded [https://hub.docker.com](https://hub.docker.com)
    is the centrally available public registry provided by Docker. Just like GitHub
    (a repository providing version control), Docker also provides a public and private
    images repository that is specific to a needs basis (we can make our repository
    private). We can create an image and register it to Docker Hub. So, next time
    when we want the same image on any other machine, we just refer to the repository
    to pull the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dockerfile**: A Dockerfile is a build or a scripting file that contains written
    instructions on how to build a Docker image. There can be multiple steps documented,
    starting right from obtaining some public image to building our application on
    top of it. We already have written Docker files (recall `.Dockerfile` in [Chapter
    2](54b04f3b-39de-4147-9aa2-0b0a242515c5.xhtml), *Gearing up for the Journey*).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Docker Compose**: Compose is a tool provided by Docker to run multi-container
    Docker applications inside one container. Taking the same example of our product-catalog
    microservice, we need a MongoDB container as well as a Node.js container. Docker
    compose is just the tool for that. Docker compose is a three-step process wherein
    we define the application''s environment in Docker file, we make other services
    run together in the isolated environment through `docker-compose.yml`, and we
    run the app using `docker-compose up`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker command reference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have had a look at Docker concepts, let''s go through Docker commands
    so we can add them in our playground:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Command** | **What does it do** |'
  prefs: []
  type: TYPE_TB
- en: '| `docker images` | See all Docker images available on my machine. |'
  prefs: []
  type: TYPE_TB
- en: '| `docker run <options> <docker_image_name>:<version> <operation>` | Launch
    a Docker image into a container. |'
  prefs: []
  type: TYPE_TB
- en: '| `docker ps` | Check whether the Docker container is running or not. |'
  prefs: []
  type: TYPE_TB
- en: '| `docker exec -ti <container-id> bash` | See what''s inside the Docker image
    by actually running on bash prompt. Able to use commands such as `ls` and `ps`.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `docker exec <container_id> ifconfig` | Find out the IP address of a Docker
    container. |'
  prefs: []
  type: TYPE_TB
- en: '| `docker build` | Build an image based on instructions in `.DockerFile`. |'
  prefs: []
  type: TYPE_TB
- en: '| `docker kill <containername> && docker rm <containername>` | Kill a running
    Docker container. |'
  prefs: []
  type: TYPE_TB
- en: '| `docker rmi <imagename>` | Delete a Docker image from local repository. |'
  prefs: []
  type: TYPE_TB
- en: '| `docker ps -q &#124; x args docker kill &#124; xargs docker rm` | Kill all
    running Docker containers. |'
  prefs: []
  type: TYPE_TB
- en: Setting up Docker with NGINX, Node.js, and MongoDB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we know fundamental commands, let''s write Dockerfile and Docker compose
    file for a product-catalog service with NGINX in front to handle load balancing,
    in the same way we wrote `docker compose up` for MongoDB and Node.js in [Chapter
    4](720d1d4e-1795-457c-903e-65c5a5fb5433.xhtml), *Beginning Your Microservice Journey*.
    You can follow along with the example in `chapter 9/Nginx-node-mongo`, which is
    just a copy of a product-catalog microservice with NGINX added on top, so that
    services are only accessed through NGINX. Create a structure like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6748e0aa-115c-4f07-bc86-de307ecad01a.png)'
  prefs: []
  type: TYPE_IMG
- en: NGINX-mongodb-node.js file structure
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s write some rules:'
  prefs: []
  type: TYPE_NORMAL
- en: We will create Dockerfile for Node.js. It will be the same as what we previously
    used.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will write Dockerfile for NGINX. We basically tell NGINX to enable rules
    for applications defined in the `sites-enabled` folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define some hardening rules inside NGINX, so as to take care of our
    load balancing as well as caching and other needs. We will write our rules in
    two places—`nodejs_project` and `nginx.conf`. In `nodejs_project` we define all
    the proxy level settings and the NIGINX server settings. Write the following code
    inside `nodejs_project`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see some of the example rules for configuring NGINX for production grade
    (hardening our web server). We will write these rules inside `nginx.conf`. For
    compressing all input and output requests coming to our NGINX server, we use the
    following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The preceding parameters simply configure any inbound or outbound HTTP requests
    with those attributes. Say, for instance, that it will gzip the response, gzip
    all sorts of files, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Whatever resources are exchanged between servers, we have the option to cache
    those, so each time we don''t need to query again. This is caching at the web
    server layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Lastly, we create our `docker compose` file to start MongoDB, Node.js, and NGINX
    to define. Copy the `docker-compose.yml` file from source to execute the build.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open up the Terminal to hit `docker-compose up --build` to see our deployment
    live in action.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'All internal ports will be blocked now. The only accessible port is the default
    port, which is `80`. Hit the `localhost/products/products/products-listing` URL
    to see our deployment live in action. Hit the URL again, which will load the response
    from the cache. See the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c9386336-7251-4e1e-905b-213df0b086bc.png)'
  prefs: []
  type: TYPE_IMG
- en: Cache response
  prefs: []
  type: TYPE_NORMAL
- en: Now that we are up and running with a container image that includes the web
    layer, in the next section we will look at our build pipeline and how WebHooks
    plays an important role in it.
  prefs: []
  type: TYPE_NORMAL
- en: WebHooks in our build pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'WebHooks are something that can be used for binding events in the project whenever
    something is happening. Say a pull request is merged and we want to immediately
    trigger a build—WebHooks does our job for that. A WebHook is essentially an HTTP
    callback. You can configure WebHook in your repository by going to settings and
    adding WebHook. A typical WebHook screen looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/38bddbf4-9f5d-45a5-b677-0ad20f06fbd8.png)'
  prefs: []
  type: TYPE_IMG
- en: WebHook
  prefs: []
  type: TYPE_NORMAL
- en: As seen in the preceding screenshot, it has various triggers for things such
    as Push, Fork, Updated, Pull requests, Issues, and so on. We can set alerts and
    trigger various actions on the basis of this WebHook.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will see a new trend emerging in microservices development,
    which is serverless deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Please check extraced source/pipeline to see end to end pipeline in action.
  prefs: []
  type: TYPE_NORMAL
- en: Serverless architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The new trend emerging these days is serverless topology. It does not actually
    mean serverless or no server. Servers are abstracted from the users and users
    only focus on development aspects and leave everything else to the vendors. AWS
    Lambda is an example of a serverless architecture where you just package the microservice
    as a ZIP and upload it to AWS Lambda. Amazon takes care of other things, including
    spawning up enough instances to handle bulk service requests.
  prefs: []
  type: TYPE_NORMAL
- en: 'A Lambda function is a stateless function. It handles the request by invoking
    AWS services. We are simply billed on a number of hit requests and time taken
    to serve those requests. Similarly, Google has cloud functions. However, this
    pattern has the following pros and cons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pros:**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We just focus on the code and we don't need to worry about low-level infrastructure
    details. AWS has a built-in gateway to be used with Lambda functions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extremely elastic architecture. It automatically handles load requests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You pay for each request rather than renting out the entire VM and paying monthly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cons:**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Few supported languages only. No freedom of polyglot environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are always stateless applications. AWS Lambda cannot be used for queue
    processing like RabbitMQ.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the application doesn't start quickly, serverless architecture is not the
    fit for us.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That's pretty much about deployment. In the next section, we will look at logging
    and how to create customized centralized logging solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Logging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Microservices being totally distributed as a single request can trigger multiple
    requests to other microservices, and it becomes problematic to track what was
    the root cause of a failure or a breakdown or the overall flow of request across
    all services.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will learn about how to track different Node.js microservices
    by doing logging the right way. Recall the concepts of logging and types of log,
    which we saw in [Chapter 4](720d1d4e-1795-457c-903e-65c5a5fb5433.xhtml), *Beginning
    Your Microservice Journey*. We are going to move ahead in that direction and create
    a centralized log store. Let's start by understanding our logging requirements
    in a distributed environment and some of the best practices that we are going
    to follow to handle distributed logging.
  prefs: []
  type: TYPE_NORMAL
- en: Logging best practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once in post development, let's say any issue comes up. We would be completely
    lost as we are not dealing with a single server. We are dealing with multiple
    servers and the entire system is constantly moving. Whoa! We need a full proof
    strategy as we can't just wander here and there, checking each and every service's
    logs. We are completely clueless as to which microservice runs on which host and
    which microservice served the request. To open up log files across all containers,
    grepping the logs and then relating them to all requests is indeed a cumbersome
    process. If our environment has auto scalability enabled, then debugging an issue
    becomes exponentially complex as we actually have to find the instance of microservice
    that served the request.
  prefs: []
  type: TYPE_NORMAL
- en: Here are some of the golden rules for logging in microservices that will make
    life easier.
  prefs: []
  type: TYPE_NORMAL
- en: Centralizing and externalizing log storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Microservices are distributed across the ecosystem to ease up development and
    enable faster development. As microservices run on multiple hosts, it is unwise
    to have logs at each container or server level. Rather we should send all the
    generated logs to an external and centralized place from where we can easily get
    the log information from a single place. This might be any another physical system
    or any highly available storage option. Some of the famous options are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**ELK or Elastic stack**: The ELK stack ([https://www.elastic.co/elk-stack](https://www.elastic.co/elk-stack))
    consists of Elasticsearch (a distributed, full-text scalable search database that
    allows storing large volumes of datasets), Logstash (it collects log events from
    multiple types of sources, and transforms it as per need), and Kibana (visualizes
    log events or anything that is stored in Elasticsearch). Using the ELK stack,
    we can have centralized logs in Elasticsearch powered by utilities from **Kibana**
    and **Logstash**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CloudWatch (only if your environment is in AWS)**: Amazon CloudWatch ([https://aws.amazon.com/cloudwatch/](https://aws.amazon.com/cloudwatch/))
    is a monitoring service for resources and applications that are running on your
    AWS environment. We can utilize Amazon CloudWatch to collect and track metrics,
    monitor log files, set some critical alarms, and automatically react to changes
    in deployments in AWS resources. CloudWatch has the ability to monitor AWS resources,
    which includes Amazon EC2 instances, DynamoDB tables, RDS DB instances, or any
    custom metrics that your application generates. It monitors log files of all the
    applications. It provides system wise visibility into utilizing resources and
    monitors performance and health.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Structured data in logs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Log messages go beyond just raw messages and should include several things,
    such as the timestamp; the log level type; the time taken for requests; metadata,
    such as device type, microservice name, service request name, instance name, filename,
    line number; and so on, from which we get the right data available in the logs
    to debug any issues.
  prefs: []
  type: TYPE_NORMAL
- en: Identifiers via correlational IDs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We generate a unique identifier or a correlation ID when we are making the very
    first service request. The generated unique ID is passed downstream to other calling
    microservices. That way, we can use the uniquely generated ID coming from the
    response to get logs specified to any service request. For that, we have a so-called
    correlation identifier or uniquely generated UUID to pass it to all services that
    the transaction goes through. To generate a unique ID, NPM has module UUID ([https://www.npmjs.com/package/uuid](https://www.npmjs.com/package/uuid)).
  prefs: []
  type: TYPE_NORMAL
- en: Log levels and logging mechanisms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Based on different aspects of an application, we need different log levels in
    our code, along with enough logging statements in code. We will use `winston`
    ([https://www.npmjs.com/package/winston](https://www.npmjs.com/package/winston)),
    which will have the ability to change log level dynamically. Furthermore, we will
    use async log appenders so that our thread won't be blocked by log requests. We
    will leverage **Async Hooks** ([https://nodejs.org/api/async_hooks.html](https://nodejs.org/api/async_hooks.html)),
    which will help us track the life cycle of resources during our process. An Async
    Hook enables us to tap any life cycle events by registering callbacks to any life
    cycle events. At resource initialization, we get a unique identifier ID (`asyncId`)
    and parent identifier ID (`triggerAsyncId`) that created the resource.
  prefs: []
  type: TYPE_NORMAL
- en: Searchable logs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The logs files collected at a single place should be searchable. For example,
    if we get any UUID, our logging solution should be able to search based on that
    to find out the request flow. Now, let''s look at a custom logging solution that
    we are going to implement and understand how it will take care of our logging
    problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/27d70bfb-2e91-4ce1-bd1e-6f0a9b09e733.png)'
  prefs: []
  type: TYPE_IMG
- en: Log custom flow
  prefs: []
  type: TYPE_NORMAL
- en: 'The diagram explains the core components along with their defined purpose.
    Let''s look at all the components along with their purposes before moving on to
    the implementation part:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Log Dashboard**: It is the UI front piece of our customized central logging
    solution. We will be using Kibana ([https://www.elastic.co/products/kibana](https://www.elastic.co/products/kibana))
    on top of Elasticsearch datastore as it provides many out-of-the-box features.
    We will be able to search indexed logs with whatever parameters we have logged.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Log Store**: To facilitate real-time logging and storing huge amount of logs
    we will use Elasticsearch as the datastore for our customized logging solution.
    Elasticsearch allows any client to query on any parameters based on text-based
    indexes. One of the other famous options is using Hadoop''s `MapReduce` program
    for processing logs offline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Log stream processor**: Log stream processors analyze real-time log events
    for processing quick decision making. For example, if any service is throwing
    a 404 error continuously, stream processors come in handy in such cases where
    they are capable of reacting to a specific stream of events. In our case, a stream
    processor gets data from our queue and processes it on the fly before sending
    it to Elasticsearch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Log shipper**: Log shippers usually collect log messages, which come from
    different endpoints and sources. Log shippers send these messages to another set
    of endpoints or write them to datastores or push them to stream processing endpoints
    for further real-time processing. We would be using tools such as RabbitMQ and
    ActiveMQ for processing streams of logs. Now that we have seen the architecture
    of our custom implementation, in the next section we will see how to implement
    that in our current application. So, let''s get started.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Centralized custom logging solution implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we are going to look at the practical implementation of customized
    log architecture, which we have seen in the previous section. So, let''s commence
    our journey. As a set of pre-requisites, we will need the following software installed:'
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch 6.2.4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logstash 6.2.4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kibana 6.2.4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Java 8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RabbitMQ 3.7.3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up our environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We talked about quite a number of software in the previous section. We need
    to make sure that each software is installed properly and up and running on their
    respective ports. Also, we need to make sure that Kibana knows about our Elasticsearch
    host and Logstash knows about our Kibana and Elasticsearch host. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Download Elasticsearch from: [https://www.elastic.co/downloads/elasticsearch](https://www.elastic.co/downloads/elasticsearch)
    and extract it in the location of your choice. Once extracted, start your server
    by `eitherelasticsearch.bat` or `./bin/elasticsearch`. Hit `http://localhost:9200/`
    and you should be able to see the JSON tagline: You Know, for Search along with
    the Elasticsearch version.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next up is Kibana. Download Kibana from: [https://www.elastic.co/downloads/kibana](https://www.elastic.co/downloads/kibana)
    and extract it to the location of choice. Then open `<kibana_home>/config/kibana.yml`
    and add the line `elasticsearch.url: "http://localhost:9200"`. This tells Kibana
    about Elasticsearch. Then start Kibana from the `bin` folder and navigate to `http://localhost:5601`.
    You should see the Kibana dashboard.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Download Logstash from [https://www.elastic.co/downloads/logstash](https://www.elastic.co/downloads/logstash).
    Extract it to the location of your choice. We will check Logstash installation
    by writing a simple script. Create one file, `logstash-simple.conf`, and write
    the following code. You can find this snippet in `Chapter 9/logstash-simple.conf`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Now hit `logstash -f logstash-simple.conf`.
  prefs: []
  type: TYPE_NORMAL
- en: You should be able to see Elasticsearch information printed out. This ensures
    us that our Logstash installation is running perfectly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to install RabbitMQ. RabbitMQ is written in Erlang and it requires
    Erlang installation. Install Erlang and make sure that environment variable `ERLANG_HOME`
    is set. Then install RabbitMQ. Once installed, start the `rabbitmq` service as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now hit the URL `http://localhost:15672`. You should be able to log in using
    guest/guest credentials, which are by default, and be able to see the RabbitMQ
    dashboard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If you are not able to see the server, you probably need to enable the plugin,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`rabbitmq-plugins.bat enable rabbitmq_management rabbitmq_web_mqtt rabbitmq_amqp1_0`'
  prefs: []
  type: TYPE_NORMAL
- en: We have successfully installed RabbitMQ, Logstash, Elasticsearch, and Kibana.
    Now we can move onto our implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Please check extracted source `/customlogging` to see our solution in action.
    The solution makes use or previous architecture as explained.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed tracing in Node.js
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Distributed tracing is like tracing a particular service request that spans
    across all of the services that are involved in serving that request. These services
    construct a graph like they form a tree rooted at the client that starts the initial
    request. Zipkin provides an instrumentation layer to generate IDs for a service
    request, based on which we can trace data from all applications by using that
    ID. In this section, we will look at how to use Zipkin. You can find the complete
    source at `Chapter 9/Zipkin`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Spin up our first microservice or any single microservice project from [Chapter
    4](2dd92134-2db3-4427-8565-1be5bb13be1f.xhtml), *Beginning Your Microservices
    Journey*. We will add the `zipkin` dependencies to it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We now need a Zipkin server. We will configure it to use a Zipkin server along
    with its defaults and just install its jar. Download the `jar` from [https:](https://search.maven.org/remote_content?g=io.zipkin.java&a=zipkin-server&v=LATEST&c=exec)[//search.maven.org/remote_content?g=io.zipkin.java&a=zipkin-server&v=LATEST&c=exec](https://search.maven.org/remote_content?g=io.zipkin.java&a=zipkin-server&v=LATEST&c=exec)
    or you can find it in the extracted source in the `server` folder under `chapter
    9/zipkin`. Once downloaded, open the Zipkin server as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows a Zipkin server:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a3867b62-7ad7-4c7f-89a0-566dfee3c2d1.png)'
  prefs: []
  type: TYPE_IMG
- en: log Zipkin
  prefs: []
  type: TYPE_NORMAL
- en: As seen in the screenshot, the Zipkin server has lots of options, including
    providing a collector for receiving trace information, storage, and UI options
    to examine it.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will configure multiple Express servers so as to observe how Zipkin
    instruments the whole thing. We will first set up Zipkin on a single microservice
    followed by multiple microservices later on. Our code from the previous chapter
    adds any product information in our MongoDB database. We will be configuring Zipkin
    here. We need to tell Zipkin where to send tracing data (that's pretty obvious!
    This will be our Zipkin server running on `9411`) and how to send tracing data
    (that's the question—Zipkin has three support options HTTP, Kafka, and Fluentd.
    We will be using HTTP). So basically we send a POST request to the Zipkin server.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We need some imports to configure our Zipkin server. Open `Express.ts` and
    add the following lines of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '`Tracer` is used to give information such as where and how to send tracing
    data. It handles generating `traceIds` and tells the transport layer when to record
    what.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BatchRecorder` formats tracing data to be sent to the Zipkin collector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HTTPLogger` is our HTTP Transport layer. It knows how to post Zipkin data
    over HTTP.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CLSContext` object refers to Continuation Local Storage. Continuation passing
    is the pattern where the function calls the next function in a chain of functions
    with the data it needs. An example of this is Node.js custom middleware layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We''re now putting all the pieces together. Add the following lines of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This will set up Zipkin essentials along with a tracer that will generate a
    64-bit trace ID. Now we need to instrument our Express server.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will tell our `express` application to use `ZipkinMiddleware` in its
    middleware layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The name of service in our case `'products-service'` will actually come in tracing
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s hit our service to see what is the actual result. Run the program, make
    a POST request to `products/add-update-product`, and open up Zipkin. You will
    be able to see `products-service` (the name of the service under which we registered
    to Zipkin server) in the Service Name dropdown. And when you do a search query
    you will be able to see something like the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/2ed10c2e-8c07-4924-a91c-628676ef15f7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Zipkin service log
  prefs: []
  type: TYPE_NORMAL
- en: This is how it looks when we are dealing with one microservice. You get traces
    about successful as well as failed service calls here too, as seen in the figure.
    We want to wrap our head around services that have more than one microservices
    involved.
  prefs: []
  type: TYPE_NORMAL
- en: For those, who are directly running the code; please ensure that the following
    lines are commented out in the `ProductsController.tslet` file—`userRes= await
    this.zipkinFetch('http://localhost:3000/users/user-by-id/parthghiya');` and `console.log("user-res",userRes.text());`.
  prefs: []
  type: TYPE_NORMAL
- en: Let's assume in our case that we have one more microservice involved based on
    our business capability that plays with owners authenticity. So, whenever a product
    is added, we want to check whether the owner is an actual user or not.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will just create two projects with dummy logic.
  prefs: []
  type: TYPE_NORMAL
- en: Create another microservice project with a user and create a GET request with
    `@Get('/user-by-id/:userId')`, which basically returns whether a user exists or
    not. We will be calling that microservice from our existing project. You can follow
    along from `chapter-9/user`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the existing project, we moved out the configurations of Zipkin to the external
    file so it can be reused throughout the project. Check out the source code of
    `ZipkinConfig.ts`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In `ProductController.ts`, instantiate a new object of Zipkin instrumentation
    fetch, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Make a fetch request, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Open up the Zipkin dashboard and you will be able to see the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/1239f4ec-7071-47d5-ac1d-386301963cff.png)'
  prefs: []
  type: TYPE_IMG
- en: Zipkin combined
  prefs: []
  type: TYPE_NORMAL
- en: 'The overall report can be viewed by clicking on the request:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e0bf0c1c-5dbf-42aa-8d50-37c406a5bb96.png)'
  prefs: []
  type: TYPE_IMG
- en: Tracing report
  prefs: []
  type: TYPE_NORMAL
- en: Tracing is an invaluable tool that helps to diagnose problems when they occur
    by tracing out any request across the entire microservices ecosystem. In the next
    section, we will look at monitoring microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Microservices are truly distributed systems with a vast technological and deployment
    topology. Without proper monitoring in place, the operational team may soon run
    into trouble managing large-scale microservice systems. To add complications to
    our problem, microservices dynamically change their topologies based on load.
    This demands a proper monitoring service. In this section, we will learn about
    the need of monitoring and look at some monitoring tools.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring 101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start by discussing Monitoring 101\. Monitoring, in general, can be defined
    as a collection of some metrics, predefined **service level agreements** (**SLAs**),
    aggregation, and their validations and adherence to prefixed baseline values.
    Whenever there is a service level breach, a monitoring tool has to generate an
    alert and send it across to administrators. In this section, we will look at monitoring
    to understand the behavior of a system from a user experience point of view, and
    the challenges of monitoring, and to understand all aspects involved in Node.js
    monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring challenges
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Similar to logging issues, the key challenge in monitoring microservices ecosystems
    is that there are too many dynamic parts. Being totally dynamic, the key challenges
    in monitoring microservices are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Statistics and metrics are distributed across many services, multiple instances,
    and multiple machines or containers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Polyglot environment adds more difficulties. A single monitoring tool does not
    suffice all the required monitoring options.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microservices deployment topologies differ in huge variations. Several parameters
    such as scalability, auto configuration, circuit breaker, and so on, change the
    architecture on-demand basis. This makes it impossible to monitor preconfigure
    servers, instances, or any other monitoring parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we are going to look at the next part of monitoring, which
    is alerting. We cannot alert every time due to errors. We need some definitive
    rules.
  prefs: []
  type: TYPE_NORMAL
- en: When to alert and when not to alert?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'No one is excited to wake up at 3.00 AM in the morning on Sunday. The general
    rule for alerting can be if something is not stopping customers from using your
    system and increasing your funds, then the situation is not worth waking up at
    3.00 AM. In this section, we will look at some instances and decide when to alert
    and when not to:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Service goes down**: Had it been monolithic, this would surely be a huge
    blow but, being a good microservice coder, you already have set up multiple instances
    and clustering. This would impact just a single user who would get functionality
    back on again service request and would prevent the failure to cascade up. However,
    if many services go down then this is definitely something worth alerting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory leak**: Memory leaks are another painful thing, as only after careful
    monitoring can we actually find the leak. A good microservice practice suggests
    setting up the environment so that it should be able to decommission an instance
    once it surpasses a certain memory threshold. The problem will fix itself on system
    restart. But if processes are running out of memory quickly then it is something
    worth alerting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Slow services**: A slow usable service is not worth alerting until or unless
    it occupies a huge resource pool. A good microservice practice suggests using
    an async architecture with event-based and queue-based implementations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Increasing 400s and 500s**: If there is an exponential increase in the number
    of 400s and 500s then there is something fishy worth alerting. 4xx codes usually
    indicate erroneous services or misconfigured core tools. This is definitely worth
    alerting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will get the actual implementation of monitoring tools
    available in the Node.js community. We will see hands-on examples with those in
    sections of Keymetrics and Prometheus with Grafana.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will look at some of the available tools for monitoring
    and how those tools help us to solve different monitoring challenges. When monitoring
    a microservice, we are mostly interested in hardware resources and application
    metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **  Hardware resources** |'
  prefs: []
  type: TYPE_TB
- en: '| Memory utilization metrics | The amount of memory that is consumed by the
    application, such as RAM utilization, hard disk occupied, and so on. |'
  prefs: []
  type: TYPE_TB
- en: '| CPU utilization metrics | How much percentage of total available CPU memory
    is it using at a given time. |'
  prefs: []
  type: TYPE_TB
- en: '| Disk Utilization metrics | The I/O memory in a hard drive, such as swap space,
    available space, used space, and so on. |'
  prefs: []
  type: TYPE_TB
- en: '| **Application metrics** |'
  prefs: []
  type: TYPE_TB
- en: '| Errors thrown per unit of time | The number of critical errors that are thrown
    from the application. |'
  prefs: []
  type: TYPE_TB
- en: '| Calls made/service occupancy per unit of time | This metric basically tells
    us about the traffic on a service. |'
  prefs: []
  type: TYPE_TB
- en: '| Response time | How much time is being utilized to respond to a service request.
    |'
  prefs: []
  type: TYPE_TB
- en: '| The number of restarts of service | Node.JS being single threaded, this thing
    should be monitored. |'
  prefs: []
  type: TYPE_TB
- en: The power of LINUX makes it easy to query hardware metrics. The `/proc` folder
    in Linux has all the necessary information. Basically, it has a directory for
    each of the running processes in the system, including kernel processes. Each
    directory there contains other useful metadata.
  prefs: []
  type: TYPE_NORMAL
- en: 'When it comes to application metrics it becomes hard to go with some inbuilt
    tools. Some of the widely used monitoring tools are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: AppDynamics, Dynatrace, and New Relic are leaders in application performance
    monitoring. But these are in the commercial segment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud vendors come with their own monitoring tools, like AWS uses Amazon Cloudwatch
    and Google Cloud platform uses Cloud monitoring.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loggly, ELK, Splunk, and Trace are top candidates in open source segments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will now look at some of the available tools in the Node.js community.
  prefs: []
  type: TYPE_NORMAL
- en: PM2 and keymetrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We already looked at the power of PM2 and how it helps us solve various issues,
    such as clustering, keeping Node.js processes running forever, zero downtimes,
    and so on. PM2 has a monitoring tool, too, that maintains several application
    metrics. PM2 introduced keymetrics as a complete tool with in-built features,
    such as the dashboard, optimization process, code manipulation from keymetrics,
    exception reporting, load balancer, transaction tracing, CPU and memory monitoring,
    and so on. It is an SAAS-based product with an option of free tier. In this section,
    we will use the free tier. So, let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: The first thing we need to do is to sign up for the free tier. Create an account
    and once you log in, you will be able to see the main screen. Once registered
    we will come to a screen where we configure our bucket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A bucket is a container on which multiple servers and multiple apps are attached.
    A bucket is something through which keymetrics define the context. For example,
    our shopping cart microservice has different services (payments, product catalog,
    inventory, and so on) hosted somewhere, and we could monitor all the servers in
    one bucket so that everything is easy to access.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we create our bucket we will get a screen like the following. This screen
    has all the information and necessary documentation required for getting started
    with keymetrics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/17044d3e-ccc1-4e78-8aaa-57bfc3f7a291.png)'
  prefs: []
  type: TYPE_IMG
- en: Keymetrics after bucket created
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see commands for connecting PM2 to keymetrics and Docker with keymetrics,
    which we will be using further on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Also as part of the installation, you will need the PM2 monitor. Once PM2 is
    installed, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The next step is to configure PM2 to push data in keymetrics. Now, to enable
    communication between server and keymetrics, the following ports need to be opened: Ports
    80 (TCP out) and 43554 (TCP in/out) must be opened. PM2 pushes data to port `80`
    on keymetrics, whereas keymetrics pushes data back on port `43554`. Now, we will
    configure keymetrics in our product-catalog microservice.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Make sure that PM2 is installed in your system. If not, just install it as
    a global module by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Then link your PM2 with keymetrics by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Once open, just change your `package.json` script to start with PM2 instead
    of a simple node process. Just add the following script in `package.json`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Once started as a PM2 process you should be able to see the process started
    and the dashboard URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/16c6686c-a4cc-4f88-8831-80634dd158b2.png)'
  prefs: []
  type: TYPE_IMG
- en: PM2 start with keymetrics
  prefs: []
  type: TYPE_NORMAL
- en: 'Head over to keymetrics and you will be able to see the live dashboard:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/2d29e270-86ae-40e8-b8f5-96b057ae3220.png)'
  prefs: []
  type: TYPE_IMG
- en: Keymetrics dashboard
  prefs: []
  type: TYPE_NORMAL
- en: It gives us interesting metrics, such as CPU usage, available memory, HTTP average
    response, available disk memory, errors, processes, and so on. In the next section,
    we will look at utilizing keymetrics to solve our monitoring challenges.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Keymetrics to monitor application exceptions and runtime problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although PM2 does a pretty good job of keeping the server up and running, we
    need to monitor all unknown exceptions that occur or potential sources of memory
    leaks. PMX provides just the module for that. You can follow the example in `chapter
    9/pmx-utilities`. Initialize `pmx` as usual. Just whenever there is an error,
    notify `pmx` with the `notify` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This is enough to send out an error to keymetrics to give it information about
    application exceptions. You will receive email notifications, too.
  prefs: []
  type: TYPE_NORMAL
- en: PMX monitors constant usage of the service, too, in order to detect memory leaks,
    if any. Check the route `/memory-leak`, for example.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following shows several important keymetrics highlighted:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7f15decc-9a17-423a-82a0-b207195786fb.png)'
  prefs: []
  type: TYPE_IMG
- en: Pmx utilities
  prefs: []
  type: TYPE_NORMAL
- en: Adding custom metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Lastly, we will see how to add our own custom metrics based on our business
    capabilities and on a need basis. Most of the time, we often need some customization
    or we are not able to use out of the box functionalities as such. Keymetrics provides
    us with probes for this. A probe in keymetrics is a custom metric that is sent
    to keymetrics programmatically. There are four kinds of probes that we will see,
    with examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Simple metrics**: Values that can be read instantly, that is, used to monitor
    any variable value. It is a very basic metric where the developer can set a value
    to the data that is pushed to keymetrics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Counter**: Things that increment or decrement, that is, downloads being processed,
    a user connected, number of times a service request is hit, the database goes
    down, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Meter**: Things that are measured as events/intervals, that is, requests
    per minute for an HTTP server, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Histogram**: It keeps a reservoir of statistically relevant values especially
    biased towards the last five minutes to explore their distribution, such as monitoring
    the mean of execution of a query into a database for the last five minutes, and
    so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will be using `pmx` ([https://www.npmjs.com/package/pmx](https://www.npmjs.com/package/pmx))
    to see examples of custom metrics. PMX is one of the leading modules for PM2 runner
    that allows exposure of metrics that are associated with the application. It can
    reveal useful patterns that can help scale the service as per demand or to efficiently
    utilize resources.
  prefs: []
  type: TYPE_NORMAL
- en: Simple metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Setting a PM2 metric value is just a matter of initializing a probe and setting
    a value in it. We can create a simple metric with the following steps. You can
    follow the source in `chapter 9/simple_metric`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Copy our `first microservice` skeleton from [Chapter 2](c1987454-3c62-4e25-abf5-28a9abf833e8.xhtml),
    *Gearing up for the Journey*. We will add our changes here. Install `pm2` and
    `pmx` modules as a dependency:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'In `HelloWorld.ts`, initialize `pmx` with the following code. We will add a
    simple metric name `''Simple Custom metric''` along with variable initializations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We initialized pmx with a few options, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`http`: HTTP routes should be logged and PM2 will be enabled to perform HTTP
    watching for HTTP related metrics'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`errors`: Exceptions logging'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`custom_probes`: JS Loop latency and HTTP requests should be automatically
    exposed as custom metrics'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ports`: It should show which ports our app is listening to'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now you can initialize this value anywhere using the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'You can now see it in the keymetrics dashboard, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/95b1e043-fcf4-45dc-86b8-03d680d2a9a6.png)'
  prefs: []
  type: TYPE_IMG
- en: Simple metric
  prefs: []
  type: TYPE_NORMAL
- en: Counter metric
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This metric is very useful in order to see things such as how many times an
    event has occurred. In this exercise, we will see the number of times that our
    `/hello-world` is invoked. You can follow along with the example in `Chapter 9/counter-metric`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize the project as before. Add the `pmx` dependency. Create one `CustomMiddleware`
    with the option of routing controller:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Add that annotation before `HelloWorld.ts` and run the application:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'You should be able to see something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b8088b76-83df-4685-9c86-b67ea87f2502.png)'
  prefs: []
  type: TYPE_IMG
- en: Counter metric
  prefs: []
  type: TYPE_NORMAL
- en: Meter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This metric allows us to record when an event actually occurs and the number
    of occurrences of events per time unit. Calculating average is quite useful as
    it essentially gives us an idea about the load in the system. In this exercise,
    we will look at how to utilize meter metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize project as usual. Install the `pmx` and `pm2` dependency. It consists
    of the following keywords:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**samples:** This parameter corresponds to interval based on which we want
    to measure the metric. In our case, it is the number of calls per minute, hence
    `60`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**timeframe:** This is how long we want to hold the keymetrics data, the overall
    time frame over which it will be analyzed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Add the following code in the constructor to initialize meter metric dependency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: In route, `@Get('/')` will initialize this mark. This will give us an average
    number of calls per minute for the route `<server_url>/hello-world: this.metric.mark();`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, run this metric. You will be able to see the value in the keymetrics dashboard.
    Similarly, you can use histogram metric.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the next section, we will look at the more advanced tools available.
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus and Grafana
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Prometheus is a famous open-source tool, which provides powerful data compression
    options along with fast data querying for time series data analysis for Node.js
    monitoring. Prometheus has built-in visualization methods, but it''s not configurable
    enough to leverage in dashboards. That''s where Grafana steps in. In this section,
    we will look at how to monitor a Node.js microservice using Prometheus and Grafana.
    So let''s get our hands dirty with coding. You can follow along with the example
    in `Chapter 9/prometheus-grafana` in the source:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As always, initialize a new project from `chapter-2/first microservice`. Add
    the following dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: These dependencies will make sure that we will be able to monitor the Node.js
    engine as well as be able to collect response time from the service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will write some middlewares to be used across the microservice stages,
    such as injecting in Express, and using after middleware. Create a `MetricModule.ts` file
    and add the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we will create some custom functions to be used as middlewares. Here,
    we will create one function; you can check out other functions in `Chapter 9/prometheus-grafana/config/metrics-module/MetricModule.ts`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following functions mentioned in the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: The  first function starts a new counter with variable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second function starts Prometheus metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third function is a middleware that increments the number of requests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The function counter except for metrics route
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, we add the metrics route:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we inject middleware in our `express` application. In `express.ts`, simply
    add the following LOC:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Node.js setup is done. Now it''s time to start Prometheus. Create one folder
    called `prometheus-data` and inside it create one `yml config` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Spawn up the Docker process by running the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Your Prometheus should be up and running and you should see a screen like the
    following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/1ed18efc-4bcd-4457-8476-90bb1f3002de.png)'
  prefs: []
  type: TYPE_IMG
- en: Prom dashboard
  prefs: []
  type: TYPE_NORMAL
- en: Perform something on your application or use some stress testing tools, such
    as JMeter or [https://www.npmjs.com/package/loadtest](https://www.npmjs.com/package/loadtest).
    Then open up Prometheus and, in the query shell, write `sum(numOfRequests)`. You
    will be able to see live graph and results. These are the same results that can
    be seen when we hit `<server_url>/metrics`. Hit the following query to try to
    see Node.js memory usage the `avg(nodejs_external_memory_bytes / 1024 / 1024)
    by (service)`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Prometheus is great, but it cannot be used as a dashboard. Hence, we utilize
    Grafana, which has excellent and pluggable visualization platform features. It
    has built-in Prometheus data source support. Hit the following command to open
    up Docker images of Grafana:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Once up, go to `localhost:3000` and add `admin/admin` in username/password to
    log in.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once logged in, add a data source with Prometheus as type (open up the Add
    Data source screen) and enter your IP address: `9090` in the HTTP URL (your Prometheus
    running URL) and `Server (Default)` (the way you are accessing Prometheus) in
    the Access text box, so as to configure Prometheus as a data source. Click on
    save and test to confirm whether the settings are working or not. You can checkout 
    the following screenshot for better understanding:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c0b1f294-7cad-4351-a6f3-0873375f7fda.png)'
  prefs: []
  type: TYPE_IMG
- en: Grafana
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the data source is configured, you can have custom graphs or anything,
    and design your own custom dashboard through GUI tools. It will look as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/907fdaa8-2b11-4945-abfd-ec8e210eb8cc.png)'
  prefs: []
  type: TYPE_IMG
- en: Grafana
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus is a powerful tool for not only monitoring single Node.js applications,
    but it can be used in a polyglot environment too. With Grafana, you can create
    the dashboard that fits your needs best.
  prefs: []
  type: TYPE_NORMAL
- en: These are prominent tools used in deployment in Node.js Monitoring. There are
    other tools, too, but integrating them involves need of Polyglot environment.
    For instance, [Simian Army](https://github.com/Netflix/SimianArmy/wiki/Chaos-Monkey).
    It is widely used and promoted by Netflix to handle various cloud computing challenges.
    It is built with a variety of simian army monkey tools to maintain network health,
    handle traffic, and locate security problems.
  prefs: []
  type: TYPE_NORMAL
- en: Production-ready microservice criteria
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are quickly going to summarize a production-ready microservice and its criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A production-ready to go microservice is reliable and stable for service requests:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It follows a standard development cycle adhering to 12-factor app standards
    (recall [Chapter 1](2eeeb09d-ecd0-403b-8a64-ac754090cebe.xhtml), *Debunking Microservices*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Its code is thoroughly tested through linters, unit test cases, integration,
    contract, and E2E test cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It uses CI/CD pipelines and incremental build strategy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are either backups, alternatives, fallbacks, and cache in place in case
    of service failures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has stable service registration and discovery process as per standards
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A production-ready to go microservice is scalable and highly available:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has auto scalability based on load coming at any time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It utilizes hardware resources efficiently and does not block resource pool
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Its dependencies scale with the application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Its traffic can be rerouted on a need basis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It handles tasks and processes in a performant nonblocking and preferably asynchronous
    reactive manner
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A production-ready to go microservice is ready for any unprepared catastrophe:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It does not have any single point of failure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is tested for resiliency through enough code testing and load testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Failure detection, stopping the failure from cascading, and remediation towards
    failure have been automated along with auto scalability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A production-ready to go microservice is properly monitored:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has its identified keymetrics (custom metrics, errors, the memory occupied,
    and so on) monitored constantly not only pertaining to microservice level, but
    also expanding to host and infrastructure level
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has a dashboard that is easy to interpret and has all important keymetrics
    (you bet, PM2 is our only choice)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Actionable alerts defined by signal providing thresholds (Prometheus and time
    series query)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A production-ready to go microservice is documented:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comprehensive document generated through tools such as Swagger
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Architecture is audited frequently and well reviewed to support polyglot environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about the deployment process. We saw some go live
    criteria, a deployment pipeline, and finally got acquainted with Docker. We saw
    some Docker commands and got acquainted with the world of dockerization. Then
    we saw some of the challenges involved with logging and monitoring when dealing
    with huge distributed microservices. We explored various solutions for logging
    and implemented a custom centralized logging solution using the famous ELK stack.
    In the latter half of the chapter, we saw monitoring tools, such as keymetrics
    and Prometheus.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next chapter will explore the final part of our product: security and scalability.
    We will see how to protect our Node.js applications against brute force attacks
    and what exactly our security plan should be. Then, we will look at scalability
    and scale our microservice through AWS—auto scalability.'
  prefs: []
  type: TYPE_NORMAL
