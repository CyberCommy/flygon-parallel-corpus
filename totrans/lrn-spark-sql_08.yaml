- en: Using Spark SQL with SparkR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many data scientists use R to perform exploratory data analysis, data visualization,
    data munging, data processing, and machine learning tasks. SparkR is an R package
    that enables practitioners to work with data by leveraging the Apache Spark's
    distributed processing capabilities. In this chapter, we will cover SparkR (an
    R frontend package) that leverages Spark's engine to perform data analysis at
    scale. We will also describe the key elements of SparkR's design and implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'More specifically, in this chapter, you will learn the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What is SparkR?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the SparkR architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding SparkR DataFrames
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using SparkR for **Exploratory Data Analysis** (**EDA**) and data munging tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using SparkR for data visualization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using SparkR for machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing SparkR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: R is a language and environment for statistical computing and data visualization.
    It is one of the most popular tools used by statisticians and data scientists.
    R is open source and provides a dynamic interactive environment with a rich set
    of packages and powerful visualization features. It is an interpreted language
    that includes extensive support for numerical computing, with data types for vectors,
    matrices, arrays, and libraries for performing numerical operations.
  prefs: []
  type: TYPE_NORMAL
- en: R provides support for structured data processing using DataFrames. R DataFrames
    make data manipulation simpler and more convenient. However, R's dynamic design
    limits the extent of possible optimizations. Additionally, interactive data analysis
    capabilities and overall scalability are also limited, as the R runtime is single
    threaded and can only process Datasets that fit in a single machine's memory.
  prefs: []
  type: TYPE_NORMAL
- en: For more details on R, refer to the *R project website* at [https://www.r-project.org/about.html](https://www.r-project.org/about.html).
  prefs: []
  type: TYPE_NORMAL
- en: SparkR addresses these shortcomings to enable data scientists to work with data
    at scale in a distributed environment. SparkR is an R package that provides a
    light-weight frontend so you can use Apache Spark from R. It combines Spark's
    distributed processing features, easy connectivity to varied data sources, and
    off-memory data structures with R's dynamic environment, interactivity, packages,
    and visualization features.
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally, data scientists have been using R with other frameworks, such
    as Hadoop MapReduce, Hive, Pig, and so on. However, with SparkR, they can avoid
    using multiple big data tools and platforms, and working in multiple different
    languages to accomplish their objectives. SparkR allows them to do their work
    in R and use Spark's distributed computation model.
  prefs: []
  type: TYPE_NORMAL
- en: SparkR interfaces are similar to R and R packages rather than the Python/Scala/Java
    interfaces we have encountered thus far. SparkR implements a distributed dataframe
    that supports operations, such as statistical computations, selection of columns,
    SQL execution, filtering rows, performs aggregations, and so on, on large Datasets.
  prefs: []
  type: TYPE_NORMAL
- en: SparkR supports conversion to/from local R DataFrames. The tight integration
    of SparkR with the Spark project enables SparkR to reuse other Spark modules,
    including Spark SQL, MLlib, and so on. Additionally, the Spark SQL data sources
    API enables reading input from a variety of sources, such as HDFS, HBase, Cassandra,
    and file formats, such as CSV, JSON, Parquet, Avro, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we present a brief overview of the SparkR architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the SparkR architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SparkR's distributed DataFrame enables programming syntax that is familiar to
    R users. The high-level DataFrame API integrates the R API with the optimized
    SQL execution engine in Spark.
  prefs: []
  type: TYPE_NORMAL
- en: 'SparkR''s architecture primarily consists of two components: an R to JVM binding
    on the driver that enables R programs to submit jobs to a Spark cluster and support
    for running R on the Spark executors.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00223.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: SparkR's design consists of support for launching R processes on Spark executor
    machines. However, there is an overhead associated with serializing the query
    and deserializing the results after they have been computed. As the amount of
    data transferred between R and the JVM increases, these overheads can become more
    significant as well. However, caching can enable efficient interactive query processing
    in SparkR.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a detailed description of SparkR design and implementation, refer: "SparkR:
    Scaling R Programs with Spark" by Shivaram Venkataraman1, Zongheng Yang, *et al,*
    available at: [https://cs.stanford.edu/~matei/papers/2016/sigmod_sparkr.pdf.](https://cs.stanford.edu/~matei/papers/2016/sigmod_sparkr.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we next present an overview of the distributed DataFrame
    component of SparkR - the Spark DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding SparkR DataFrames
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main component of SparkR is a distributed DataFrame called **SparkR DataFrames**.
    The Spark DataFrame API is similar to local R DataFrames but scales to large Datasets
    using Spark's execution engine and the relational query optimizer. It is a distributed
    collection of data organized into columns similar to a relational database table
    or an R DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Spark DataFrames can be created from many different data sources, such as data
    files, databases, R DataFrames, and so on. After the data is loaded, developers
    can use familiar R syntax for performing various operations, such as filtering,
    aggregations, and merges. SparkR performs a lazy evaluation on DataFrame operations.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, SparkR supports many functions on DataFrames, including statistical
    functions.  We can also use libraries such as magrittr to chain commands. Developers
    can execute SQL queries on SparkR DataFrames using the SQL commands. Finally,
    SparkR DataFrames can be converted into a local R DataFrame by using the collect
    operator.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we introduce typical SparkR programming operations used
    in EDA and data munging tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Using SparkR for EDA and data munging tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will use Spark SQL and SparkR for preliminary exploration
    of our Datasets. The examples in this chapter use several publically available
    Datasets to illustrate the operations and can be run in the SparkR shell.
  prefs: []
  type: TYPE_NORMAL
- en: The entry point into SparkR is the SparkSession. It connects the R program to
    a Spark cluster. If you are working in the SparkR shell, the SparkSession is already
    created for you.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this time, start SparkR shell, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You can install the required libraries, such as ggplot2, in your SparkR shell,
    as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Reading and writing Spark DataFrames
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SparkR supports operating on a variety of data sources through the Spark DataFrames
    interface. SparkR's DataFrames supports a number of methods to read input, perform
    structured data analysis, and write DataFrames to the distributed storage.
  prefs: []
  type: TYPE_NORMAL
- en: The `read.df` method can be used for creating Spark DataFrames from a variety
    of data sources. We will need to specify the path to the input data file and the
    type of data source. The data sources API natively supports formats, such as CSV,
    JSON, and Parquet.
  prefs: []
  type: TYPE_NORMAL
- en: 'A complete list of functions can be found in the API docs available at: [http://spark.apache.org/docs/latest/api/R/](http://spark.apache.org/docs/latest/api/R/).
    For the initial set of code examples, we will use the Dataset from [Chapter 3](part0045.html#1AT9A0-e9cbc07f866e437b8aa14e841622275c),
    *Using Spark SQL for Data Exploration,* that contains data related to the direct
    marketing campaigns (phone calls) of a Portuguese banking institution.'
  prefs: []
  type: TYPE_NORMAL
- en: The input file is in **Comma-Separated values** (**CSV**) format contains a
    header and the fields are delimited by a semicolon. The input file can be any
    of the Spark data sources; for example, if it is JSON or Parquet format, then
    we just need to change the source parameter to `json` or `parquet`, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create a `SparkDataFrame` by loading our input CSV file using `read.df`,
    as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Similarly, we can write the DataFrame to the distributed storage using `write.df`.
    We specify the output DataFrame name and format in the source parameter (as in
    the `read.df` function).
  prefs: []
  type: TYPE_NORMAL
- en: 'The data sources API can be used to save the Spark DataFrames into multiple
    different file formats. For example, we can save the Spark DataFrame created in
    the previous step to a Parquet file using `write.df`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The `read.df` and `write.df` functions are used to bring data from the storage
    to the workers and write data from the workers to the storage, respectively. It
    does not bring this data into the R process.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring structure and contents of Spark DataFrames
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we explore the dimensions, schema, and data contained in the
    Spark DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we cache the Spark DataFrame for performance using either the cache
    or the persist function. We can also specify storage level options, such as `DISK_ONLY`,
    `MEMORY_ONLY`, `MEMORY_AND_DISK`, and so on, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We can list the columns and associated data types of the Spark DataFrames by
    typing the name of the DataFrame, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Spark DataFrames[age:`int`, job:`string`, marital:`string`, education:`string`,
    default:`string`, housing:`string`, loan:`string`, contact:`string`, month:`string`,
    day_of_week:`string`, duration:`int`, campaign:`int`, pdays:`int`, previous:`int`,
    poutcome:`string`, emp.var.rate:`double`, cons.price.idx:`double`, cons.conf.idx:double,
    euribor3m:`double`, nr.employed:`double`, y:`string`]
  prefs: []
  type: TYPE_NORMAL
- en: 'SparkR can automatically infer the schema from the input file''s header row.
    We can print the DataFrame schema, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00224.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also display the names of the columns in our DataFrame using the `names`
    function, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00225.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we display a few sample values (from each of the columns) and records
    from the Spark DataFrame, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00226.gif)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00227.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'We can display the dimensions of the DataFrame, as shown. Executing dim after
    the cache or persist function with the `MEMORY_ONLY` option is a good way to ensure
    the DataFrame is loaded and kept in memory for faster operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also use the count or the `nrow` function to compute the number of rows
    in our DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Additionally, we can use the `distinct` function to get the number of distinct
    values contained in the specified column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Running basic operations on Spark DataFrames
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we use SparkR to execute some basic operations on Spark DataFrames,
    including aggregations, splits, and sampling. For example, we can select columns
    of interest from the DataFrame. Here, we select only the `education` column from
    the DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, we can also specify the column name, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use the subset function to select rows meeting certain conditions, for
    example, rows with `marital` status `married`, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00228.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'We can use the filter function to only retain rows with `education` level `basic.4y`,
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00229.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'SparkR DataFrames support a number of common aggregations on the data after
    grouping. For example, we can compute a histogram of the `marital` status values
    in the Dataset, as follows. Here, we use the `n` operator to count the number
    of times each marital status appears:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also sort the output from the aggregation to get the most common set
    of marital statuses, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Next, we use the `magrittr` package to pipeline functions instead of nesting
    them, as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, install the `magrittr` package using the `install.packages` command,
    if the package is not installed already:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that when loading and attaching a new package in R, it is possible to
    have a name conflict, where a function is masking another function. Depending
    on the load order of the two packages, some functions from the package loaded
    first are masked by those in the package loaded after. In such cases, we need
    to prefix such calls with the package name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We pipeline the `filter`, `groupBy`, and the `summarize` functions, as shown
    in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00230.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we create a local DataFrame from the distributed Spark version we have
    been working with so far. We use the `collect` function to move Spark DataFrame
    to a local/R DataFrame on the Spark driver, as shown. Typically, you would summarize
    or take a sample of your data before moving it to a local DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We can create a `sample` from our DataFrame and move it to a local DataFrame,
    as shown. Here, we take 10% of the input records and create a local DataFrame
    from it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: SparkR also provides a number of functions that can directly be applied to the
    columns for data processing and aggregations.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we can add a new column to our DataFrame containing a new column
    with the call duration converted from seconds to minutes, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00231.gif)'
  prefs: []
  type: TYPE_IMG
- en: Executing SQL statements on Spark DataFrames
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Spark DataFrames can also be registered as a temporary view in Spark SQL that
    allows us to run SQL queries over its data. The `sql` function enables applications
    to run SQL queries programmatically and return the result as a Spark DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we register the Spark DataFrame as a temporary view:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we execute the SQL statements using the `sql` function. For example,
    we select the `education`, `age`, `marital`, `housing`, and `loan` columns for
    customers aged between 13 and 19 years, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00232.gif)'
  prefs: []
  type: TYPE_IMG
- en: Merging SparkR DataFrames
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can explicitly specify the columns that SparkR should merge the DataFrames
    on using the operation parameters `by` and `by.x`/`by.y`. The merge operation
    determines how SparkR should merge DataFrames based on the values, `all.x` and
    `all.y`, which indicate which rows in `x` and `y` should be included in the join,
    respectively. For example, we can specify an `inner join` (default) by explicitly
    specifying `all.x = FALSE`, `all.y = FALSE`, or a left `outer join` with `all.x
    = TRUE`, `all.y = FALSE`.
  prefs: []
  type: TYPE_NORMAL
- en: For more details on join and merge operations, refer to [https://github.com/UrbanInstitute/sparkr-tutorials/blob/master/merging.md.](https://github.com/UrbanInstitute/sparkr-tutorials/blob/master/merging.md)
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, we can also use the `join` operation to merge the DataFrames
    by row.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the following example, we use the crimes Dataset available at: [https://archive.ics.uci.edu/ml/Datasets/Communities+and+Crime+Unnormalized](https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime+Unnormalized).'
  prefs: []
  type: TYPE_NORMAL
- en: 'As before, we read in the input Dataset, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we select specific columns related to the type of crime and rename the
    default column names to more meaningful names, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00233.gif)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00234.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we read in a Dataset containing the names of US states, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We list out the columns of the two DataFrames using the names function. The
    common column between the two DataFrames is the "code" column (containing state
    codes):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we perform an inner join using the common column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00235.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we perform an inner join based on specifying the expression explicitly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00236.gif)'
  prefs: []
  type: TYPE_IMG
- en: For the following example, we use the tennis tournament match statistics Dataset
    available at: [http://archive.ics.uci.edu/ml/Datasets/Tennis+Major+Tournament+Match+Statistics.](http://archive.ics.uci.edu/ml/datasets/Tennis+Major+Tournament+Match+Statistics)
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the output obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00237.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Using User Defined Functions (UDFs)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In SparkR, several types of **User Defined Functions** (**UFDs**) are supported.
    For example, we can run a given function on a large Dataset using `dapply` or
    `dapplyCollect`. The `dapply` function applies a function to each partition of
    a Spark DataFrame. The output of the function should be a data.frame.
  prefs: []
  type: TYPE_NORMAL
- en: 'The schema specifies the row format of the resulting a Spark DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00238.gif)'
  prefs: []
  type: TYPE_IMG
- en: Similar to dapply, the `dapplyCollect` function applies the function to each
    partition of a Spark DataFrames and collects the result back. The output of the
    function should be a `data.frame` and the schema parameter is not required. Note
    that `dapplyCollect` can fail if the output of UDF cannot be transferred to the
    driver or fit in the driver's memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use `gapply` or `gapplyCollect` to run a given function on a large Dataset grouping
    by input columns. In the following example, we determine a set of top duration
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00239.gif)'
  prefs: []
  type: TYPE_IMG
- en: The `gapplyCollect` similarly applies a function to each partition of a Spark
    DataFrames but also collects the result back to an `R data.frame`.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we introduce SparkR functions to compute summary statistics
    for our example Datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Using SparkR for computing summary statistics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The describe (or summary) operation creates a new DataFrame that contains count,
    mean, max, mean, and standard deviation values for a specified DataFrame or a
    list of numerical columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00240.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Computing these values on a large Dataset can be computationally expensive.
    Hence, we present the individual computation of these statistical measures here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create a DataFrame that lists the minimum and maximum values and the
    range width:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we compute the sample variance and standard deviation as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The operation `approxQuantile` returns approximate quantiles for a DataFrame
    column. We specify the quantiles to be approximated using the probabilities parameter
    and the acceptable error by the `relativeError` parameter. We define a new DataFrame,
    `df1`, that drops missing values for `age` and then computes the approximate `Q1`,
    `Q2`, and `Q3` values, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We can measure the magnitude and the direction of the skew in the distribution
    of a column by using the `skewness` operation. In the following example, we measure
    the skewness for the `age` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, we can measure the kurtosis of a column. Here, we measure the kurtosis
    for the `age` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we compute the sample covariance and correlation between two DataFrame
    columns. Here, we compute the covariance and correlation between the `age` and
    `duration` columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create a relative frequency table for the job column. The relative
    frequency for each distinct job category value is shown in the Percentage column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00241.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, we create a contingency table between two categorical columns with
    the operation `crosstab`. In the following example, we create a contingency table
    for the job and marital columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00242.gif)'
  prefs: []
  type: TYPE_IMG
- en: In the next section, we use SparkR to execute various data visualization tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Using SparkR for data visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The SparkR extension of the ggplot2 package, `ggplot2.SparkR`, allows SparkR
    users to build powerful visualizations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we use various plots to visualize our data. Additionally,
    we also present examples of plotting of data on maps and visualizing graphs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Refer to the ggplot website for different options available to improve the displays
    of each of your plots at [http://docs.ggplot2.org](http://docs.ggplot2.org).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next step, we plot a basic bar graph that gives frequency counts for
    the different marital statuses in the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00243.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following example, we plot a histogram for the age column and several
    bar graphs that give frequency counts for the education, marital status, and job
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00244.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following expression creates a bar graph that describes the proportional
    frequency of education levels grouped over marital types:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00245.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following expression plots a histogram that gives frequency counts across
    binned age values in the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00246.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following expression returns a frequency polygon equivalent to the histogram
    plotted previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00247.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following expression gives a boxplot of call duration values across types
    of marital statuses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00248.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following expression facets age histograms across different levels of education:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00249.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following example, we show several box plots simultaneously for different
    columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00250.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: When building expressions or functions in SparkR, we should avoid computationally
    expensive operations. For example, even though the collect operation in SparkR
    allows us to leverage ggplot2 features, we should collect data as sparingly as
    possible, since we need to ensure the operation results fit into a single node's
    available memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'The problem with the following scatterplot is overplotting. The points are
    plotted on top of one another, distorting the visual appearance of the plot. We
    can adjust the value of the alpha parameter to use transparent points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00251.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'To display as a two-dimensional set of panels or to wrap the panels into multiple
    rows, we use `facet_wrap`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00252.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The adjusted alpha-value improves the visualization of scatterplots; however,
    we can summarize the points to average values and plot them to get a much clearer
    visualization, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00253.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the next example, we create a Density plot and overlay a line passing through
    the mean. Density plots are a good way to view the distribution of a variable;
    for example, in our example, we plot the call duration values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00254.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the next section, we will present an example of plotting values on a map.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing data on a map
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we describe how to merge two data sets and plot the results
    on a map:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'The Dataset we want to visualize is the average number of arsons by state,
    as computed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we read `states.csv` Dataset into an R DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we replace the state code with the state name using a `factor` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'To create a map of the United States with the states colored according to the
    average number of arsons per state, we can use the `ggplot2''s map_data` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we merge the Dataset with the map and use `ggplot` to display the
    map, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00255.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: For more on plotting on geographical maps, refer to the *Exploring geographical
    data using SparkR and ggplot2* by Jose A. Dianes at [https://www.codementor.io/spark/tutorial/exploratory-geographical-data-using-sparkr-and-ggplot2](https://www.codementor.io/spark/tutorial/exploratory-geographical-data-using-sparkr-and-ggplot2).
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we present an example of graph visualization.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing graph nodes and edges
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is key to visualize graphs to get a sense of the overall structural properties.
    In this section, we will plot several graphs in the SparkR shell.
  prefs: []
  type: TYPE_NORMAL
- en: For more details, refer to *Static and dynamic network visualization with R*
    by Katherine Ognynova at [http://kateto.net/network-visualization](http://kateto.net/network-visualization).
  prefs: []
  type: TYPE_NORMAL
- en: For the following example, we use a Dataset containing the network of interactions
    on the stack exchange website Ask Ubuntu available at: [https://snap.stanford.edu/data/sx-askubuntu.html](https://snap.stanford.edu/data/sx-askubuntu.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'We create a local DataFrame from ten percent sample of the data and create
    a plot of the graph, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00256.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can obtain a clearer visualization in this example by reducing the sample
    size further and removing certain edges, such as loops, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00257.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the next section, we explore using SparkR for machine learning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Using SparkR for machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SparkR supports a growing list of machine learning algorithms, such as **Generalized
    Linear Model** (**glm**), Naive Bayes Model, K-Means Model, Logistic Regression
    Model, **Latent Dirichlet Allocation** (**LDA**) Model, Multilayer Perceptron
    Classification Model, Gradient Boosted Tree Model for Regression and Classification,
    Random Forest Model for Regression and Classification, **Alternating Least Squares**
    (**ALS**) matrix factorization Model, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: SparkR uses Spark MLlib to train the model. The summary and predict functions
    are used to print a summary of the fitted model and make predictions on new data,
    respectively. The `write.ml`/`read.ml` operations can be used to save/load the
    fitted models. SparkR also supports a subset of the available R formula operators
    for model fitting, such as `~`, `.`, `:`, `+`, and `-`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the following examples, we use a wine quality Dataset available at [https://archive.ics.uci.edu/ml/Datasets/Wine+Quality](https://archive.ics.uci.edu/ml/datasets/Wine+Quality):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00258.gif)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'We create the training and test DataFrames using the sample function, as shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we fit a logistic regression model against a SparkDataFrame, as shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Following, we use the summary function to print a summary of the fitted model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00259.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we use the predict function to make predictions on the test DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00260.gif)'
  prefs: []
  type: TYPE_IMG
- en: only showing top 5 rows
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we count the number of mismatches between the labels and the predicted
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following example, we fit a Random Forest Classification model on a
    Spark DataFrames. We then use the `summary` function to get a summary of the fitted
    Random Forest model and the `predict` function to make predictions on the test
    data, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00261.gif)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00262.gif)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Similar to the previous examples, we fit a generalized linear model in the
    following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00263.gif)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00264.gif)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00265.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we present an example of clustering, where we fit a multivariate Gaussian
    mixture model against a Spark DataFrames:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00266.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we perform a two-sided **Kolmogorov-Smirnov** (**KS**) test for data
    sampled from a continuous distribution. We compare the largest difference between
    the empirical cumulative distribution of the data and the theoretical distribution
    to test the null hypothesis that the sample data comes from that theoretical distribution.
    In the following example, we illustrate the test on the `fixed_acidity` column
    against the normal distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'Kolmogorov-Smirnov test summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, in the following example, we perform distributed training of multiple
    models using `spark.lapply`. The results of all the computations must fit in a
    single machine''s memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'We pass a read-only list of arguments for the family parameter of the generalized
    linear model, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'The following statement returns a list of the model summaries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can print the summary of both the models, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'The summary of model 1 is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00267.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'The summary of model 2 is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00268.gif)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced SparkR. We covered SparkR architecture and SparkR
    DataFrames API. Additionally, we provided code examples for using SparkR for EDA
    and data munging tasks, data visualization, and machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will build Spark applications using a mix of Spark modules.
    We will present examples of applications that combine Spark SQL with Spark Streaming,
    Spark Machine Learning, and so on.
  prefs: []
  type: TYPE_NORMAL
