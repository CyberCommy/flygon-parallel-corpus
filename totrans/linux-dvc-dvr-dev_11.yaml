- en: Kernel Memory Management
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内核内存管理
- en: On Linux systems, every memory address is virtual. They do not point to any
    address in the RAM directly. Whenever one accesses a memory location, a translation
    mechanism is performed in order to match the corresponding physical memory.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在Linux系统上，每个内存地址都是虚拟的。它们不直接指向RAM中的任何地址。每当访问内存位置时，都会执行翻译机制以匹配相应的物理内存。
- en: Let us start with a short story to introduce the virtual memory concept. Given
    a hotel, there can be a phone in each room, having a private number. Any installed
    phone, of course belongs to the hotel. None of them can be joined directly from
    outside the hotel.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个简短的故事开始介绍虚拟内存的概念。给定一个酒店，每个房间都可以有一个私人号码的电话。当然，任何安装的电话都属于酒店。它们都不能直接从酒店外部连接。
- en: 'If you need to contact an occupant of a room, let us say your friend, he must
    have given you the hotel''s switchboard number and the room number in which he
    stays. Once you call the switchboard and give the room number of the occupant
    you need to talk to, just at this moment, the receptionist redirects your call
    to the real private phone of the room. Only the receptionist and the room occupant
    know the private number mapping:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要联系房间的居住者，比如您的朋友，他必须给您酒店的总机号码和他所住的房间号码。一旦您拨打总机并提供您需要交谈的居住者的房间号码，接待员会立即将您的电话转接到房间的真实私人电话。只有接待员和房间的居住者知道私人号码的映射：
- en: '[PRE0]'
  id: totrans-4
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Every time someone in the city (or over the world) wants to contact a room
    occupant, he has to pass by the hotline. He needs to know the right hotline number
    of the hotel, and the room number. This way, `switchboard number + room number`
    = virtual address, whereas `private phone number` corresponds to the physical
    address. There are some rules related to hotels that apply on Linux as well:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 每当城市（或世界各地）的某人想要联系房间的居住者时，他必须经过热线。他需要知道酒店的正确热线号码和房间号码。这样，`总机号码+房间号码` = 虚拟地址，而`私人电话号码`对应于物理地址。在Linux上也适用一些与酒店相关的规则：
- en: '| **Hotel** | **Linux** |'
  id: totrans-6
  prefs: []
  type: TYPE_TB
  zh: '| **酒店** | **Linux** |'
- en: '| You cannot contact an occupant who has no private phone in the room. There
    is not even a way to attempt to do this. Your call will be ended suddenly. | You
    cannot access a non-existing memory in your address space. This will cause a segmentation
    fault. |'
  id: totrans-7
  prefs: []
  type: TYPE_TB
  zh: '| 您无法联系没有房间里的私人电话的居住者。甚至没有办法尝试这样做。您的电话将突然结束。| 您无法访问地址空间中不存在的内存。这将导致段错误。'
- en: '| You cannot contact an occupant who does not exist, or whose check-in the
    hotel is not aware of, or whose information is not found by the switchboard. |
    If you access unmapped memory, the CPU raises a page fault and the OS handles
    it. |'
  id: totrans-8
  prefs: []
  type: TYPE_TB
  zh: '| 您无法联系不存在的居住者，或者酒店不知道他们的入住情况，或者总机找不到他们的信息。| 如果访问未映射的内存，CPU会引发页面错误，操作系统会处理它。'
- en: '| You can''t contact an occupant whose stay is over. | You cannot access freed
    memory. Maybe it has been allocated to another process |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| 您无法联系已经离开的居住者。| 您无法访问已释放的内存。也许它已经分配给另一个进程。'
- en: '| Many hotels may have the same brand, but located at different places, each
    of them having a different hotline number. If you make a mistake with the hotline
    number. | Different processes may have the same virtual addresses mapped in their
    address space, but pointing to another different physical addresses. |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| 许多酒店可能属于同一品牌，但位于不同的地方，每个酒店都有不同的热线号码。如果您犯了热线号码的错误。| 不同的进程可能在其地址空间中具有相同的虚拟地址，但指向另一个不同的物理地址。'
- en: '| There is a book (or software with a database) holding the mapping between
    the room number and the private phone number, and consulted by the receptionist
    on demand. | Virtual addresses are mapped to the physical memory by page tables,
    which are maintained by the operating system kernel and consulted by the processor.
    |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| 有一本书（或带有数据库的软件）保存着房间号和私人电话号码之间的映射，并在需要时由接待员查询。| 虚拟地址通过页表映射到物理内存，页表由操作系统内核维护并由处理器查询。'
- en: That is how one can imagine the virtual addresses work on a Linux system.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是人们如何想象虚拟地址在Linux系统上工作的方式。
- en: 'In this chapter, we will deal with the whole Linux memory management system
    covering following topics:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涉及整个Linux内存管理系统，涵盖以下主题：
- en: Memory layout along with address translation and MMU
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存布局以及地址转换和MMU
- en: Memory allocation mechanisms (page allocator, slab allocator, kmalloc allocator,
    and so on)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存分配机制（页面分配器、slab分配器、kmalloc分配器等）
- en: I/O memory access
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: I/O内存访问
- en: Mapping kernel memory to user space and implementing `mmap()` callback function
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将内核内存映射到用户空间并实现`mmap()`回调函数
- en: Introducing Linux caching system
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍Linux缓存系统
- en: Introducing the device managed resource framework (devres)
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍设备管理的资源框架（devres）
- en: System memory layout - kernel space and user space
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 系统内存布局-内核空间和用户空间
- en: 'Throughout this chapter, terms such as kernel space and user space will refer
    to their virtual address space. On Linux systems, each process owns a virtual
    address space. It is a kind of memory sandbox during the process life. That address
    space is 4 GB in size on 32-bits systems (even on a system with physical memory
    less than 4 GB). For each process, that 4 GB address space is split in two parts:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，诸如内核空间和用户空间的术语将指的是它们的虚拟地址空间。在Linux系统上，每个进程拥有一个虚拟地址空间。这是进程生命周期中的一种内存沙盒。在32位系统上，该地址空间大小为4GB（即使在物理内存小于4GB的系统上也是如此）。对于每个进程，这4GB地址空间分为两部分：
- en: User space virtual addresses
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户空间虚拟地址
- en: Kernel space virtual addresses
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内核空间虚拟地址
- en: 'The way the split is done depends on a special kernel configuration option,
    `CONFIG_PAGE_OFFSET` , which defines where the kernel addresses section starts
    in a process address space. The common value is `0xC0000000` by default on 32-bit
    systems, but this may be changed, as it is the case for i.MX6 family processors
    from NXP, which uses `0x80000000` . In the whole chapter, we will consider `0xC0000000`
    by default. This is called 3G/1G split, where the user space is given the lower
    3 GB of virtual address space, and the kernel uses the upper remaining 1 GB. A
    typical process''s virtual address space layout looks like:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 分割的方式取决于一个特殊的内核配置选项`CONFIG_PAGE_OFFSET`，它定义了内核地址部分在进程地址空间中的起始位置。32位系统上默认的常见值是`0xC0000000`，但这可能会改变，就像NXP的i.MX6系列处理器使用的`0x80000000`一样。在整个章节中，我们将默认考虑`0xC0000000`。这被称为3G/1G分割，其中用户空间被赋予虚拟地址空间的较低3GB，内核使用剩余的1GB。典型的进程虚拟地址空间布局如下：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Both addresses used in the kernel and the user space are virtual addresses.
    The difference is that accessing a kernel address needs a privileged mode. Privileged
    mode has extended privileges. When the CPU runs the user space side code, the
    active process is said to be running in the user mode; when the CPU runs the kernel
    space side code, the active process is said to be running in the kernel mode.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 内核和用户空间中使用的地址都是虚拟地址。不同之处在于访问内核地址需要特权模式。特权模式具有扩展特权。当CPU运行用户空间代码时，活动进程被认为是在用户模式下运行；当CPU运行内核空间代码时，活动进程被认为是在内核模式下运行。
- en: Given an address (virtual of course), one can distinguish whether it is a kernel
    space or a user space address by using process layout shown above. Every address
    falling into 0-3 GB, comes from the user space; otherwise, it is from the kernel.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用上面显示的进程布局，可以区分一个地址（虚拟的当然）是内核空间地址还是用户空间地址。落入0-3GB的每个地址都来自用户空间；否则，它来自内核。
- en: 'There is a reason why the kernel shares its address space with every process:
    because every single process at a given moment uses system calls, which will involve
    the kernel. Mapping the kernel''s virtual memory address into each process''s
    virtual address space allow us to avoid the cost of switching out the memory address
    space on each entry to (and exit from) the kernel. It is the reason why the kernel
    address space is permanently mapped on top of each process in order to speed up
    kernel access through system calls.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 内核与每个进程共享地址空间的原因是因为每个单独的进程在某一时刻都会使用系统调用，这将涉及内核。将内核的虚拟内存地址映射到每个进程的虚拟地址空间中，可以避免在每次进入（和退出）内核时切换内存地址空间的成本。这就是为什么内核地址空间永久映射在每个进程的顶部，以加速通过系统调用访问内核的原因。
- en: 'The memory management unit organizes memory into units of fixed size called
    pages. A page consists of 4,096 bytes (4 KB). Even if this size may differ on
    other systems, it is fixed on ARM and x86, which are architectures we are interested
    in:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 内存管理单元将内存组织成固定大小的单元，称为页。一个页由4096字节（4KB）组成。即使这个大小在其他系统上可能有所不同，在我们感兴趣的ARM和x86架构上是固定的：
- en: A memory page, virtual page, or simply page are terms one uses to refer to a
    fixed-length contiguous block of virtual memory. The same name `page` is used
    as a kernel data structure to represent a memory page.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存页、虚拟页或简单地说页是用来指代固定长度的连续虚拟内存块的术语。相同的名称“页”被用作内核数据结构来表示内存页。
- en: On the other hand, a frame (or page frame) refers to a fixed-length contiguous
    block of physical memory on top of which the operating system maps a memory page.
    Each page frame is given a number, called **page frame number** (**PFN** ). Given
    a page, one can easily get its PFN and vice versa, using the `page_to_pfn` and
    `pfn_to_page` macros, which will be discussed in detail in the next sections.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一方面，框架（或页框）指的是操作系统映射内存页的物理内存上的固定长度连续块。每个页框都有一个号码，称为页框号（PFN）。给定一个页面，可以很容易地得到它的PFN，反之亦然，使用`page_to_pfn`和`pfn_to_page`宏，这将在接下来的章节中详细讨论。
- en: A page table is the kernel and architecture data structure used to store the
    mapping between virtual addresses and physical addresses. The key pair page/frame
    describes a single entry in the page table. This represents a mapping.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 页表是内核和体系结构数据结构，用于存储虚拟地址和物理地址之间的映射。键对页/页框描述了页表中的单个条目。这代表了一个映射。
- en: Since a memory page is mapped to a page frame, it goes without saying that pages
    and page frames have the same sizes, 4 K in our case. The size of a page is defined
    in the kernel through the `PAGE_SIZE` macro.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 由于内存页映射到页框，不言而喻，页和页框的大小相同，在我们的情况下是4K。页的大小在内核中通过`PAGE_SIZE`宏定义。
- en: There are situations where one needs memory to be page-aligned. One says a memory
    is page-aligned if its address starts exactly at the beginning of a page. For
    example, on a 4 K page size system, 4,096, 20,480, and 409,600 are instances of
    page-aligned memory addresses. In other words, any memory whose address is a multiple
    of the system page size is said to be page-aligned.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 有时需要内存对齐到页。如果内存的地址恰好从一个页面的开头开始，就说内存是对齐的。例如，在一个4K页面大小的系统上，4096、20480和409600是对齐的内存地址的实例。换句话说，任何地址是系统页面大小的倍数的内存都被认为是对齐的。
- en: Kernel addresses – concept of low and high memory
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内核地址-低内存和高内存的概念
- en: 'The Linux kernel has its own virtual address space as every user mode process
    does. The virtual address space of the kernel (1 GB sized in 3G/1G split) is divided
    into two parts:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Linux内核有自己的虚拟地址空间，就像每个用户模式进程一样。内核的虚拟地址空间（在3G/1G分割中大小为1GB）分为两部分：
- en: Low memory or LOWMEM, which is the first 896 MB
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低内存或LOWMEM，即前896MB
- en: High Memory or HIGHMEM, represented by the top 128 MB
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高内存或HIGHMEM，表示顶部128MB
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Low memory
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 低内存
- en: 'The first 896 MB of kernel address space constitutes the low memory region.
    Early at boot, the kernel permanently maps those 896 MB. Addresses that result
    from that mapping are called **logical addresses** . These are virtual addresses,
    but can be translated into physical addresses by subtracting a fixed offset, since
    the mapping is permanent and known in advance. Low memory match with lower bound
    of physical addresses. One could define low memory as being the memory for which
    logical addresses exist in the kernel space. Most of the kernel memory function
    returns low memory. In fact, to serve different purposes, kernel memory is divided
    into a zone. Actually, the first 16 MB of LOWMEM is reserved for DMA usage. Because
    of hardware limitations, the kernel cannot treat all pages as identical. We can
    then identify three different memory zones in the kernel space:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '`ZONE_DMA` : This contains page frames of memory below 16 MB, reserved for
    **Direct Memory Access** (**DMA** )'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ZONE_NORMAL` : This contains page frames of memory above 16 MB and below 896
    MB, for normal use'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ZONE_HIGHMEM` : This contains page frames of memory at and above 896 MB'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That says on a 512 MB system, there will be no `ZONE_HIGHMEM` , 16 MB for `ZONE_DMA`
    , and 496 MB for `ZONE_NORMAL` .
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 'Another definition of logical addresses: addresses in kernel space, mapped
    linearly on physical addresses, which can be converted into physical addresses
    just with an offset, or applying a bitmask. One can convert a physical address
    into a logical address using the `__pa(address)` macro, and then revert with the
    `__va(address)` macro.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: High memory
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The top 128 MB of the kernel address space is called the high memory region.
    It is used by the kernel to temporarily map physical memory above 1 G. When physical
    memory above 1 GB (or more precisely, 896 MB), needs to be accessed, the kernel
    uses those 128 MB to create temporary mapping to its virtual address space, thus
    achieving the goal of being able to access all physical pages. One could define
    high memory as being memory for which logical addresses do not exist, and which
    is not mapped permanently into kernel address space. The physical memory above
    896 MB is mapped on demand to the 128 MB of the HIGHMEM region.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Mapping to access high memory is created on the fly by the kernel, and destroyed
    when done. This makes high memory access slower. That said, the concept of high
    memory does not exist on the 64-bits systems, due to the huge address range (2^(64)
    ), where the 3G/1G split does not make sense anymore.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: User space addresses
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will deal with the user space by means of processes. Each
    process is represented in the kernel as an instance of `struct task_struct` (see
    `*include/linux/sched.h*` ), which characterizes and describes a process. Each
    process is given a table of memory mapping, stored in a variable of type `struct
    mm_struct` (see `*include/linux/mm_types.h*` ). You can then guess that there
    is at least one `mm_struct` field embedded in each `task_struct` . The following
    line is the part of struct `task_struct` definition that we are interested in:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The kernel global variable `current` , points to the current process. The field
    `*mm` , points to its memory mapping table. By definition, `current->mm` points
    to the current process memory mappings table.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let us see what a `struct mm_struct` looks like:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'I intentionally removed some fields we are not interested in. There are some
    fields we will talk about later: `pgd` for example, which is a pointer to the
    process''s base (first entry) level `1` table (PGD), written in the translation
    table base address of the CPU at context switching. Anyway, before going further,
    let us see the representation of a process address space:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Image00019.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
- en: Process memory layout
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: From the process point of view, a memory mapping can be seen as nothing but
    a set of page table entries dedicated to a consecutive virtual address range.
    That *consecutive virtual address range* is called memory area, or **virtual memory
    area** (**VMA** ). Each memory mapping is described by a start address and length,
    permissions (such as whether the program can read, write, or execute from that
    memory), and associated resources (such as physical pages, swap pages, file contents,
    and so on).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 从进程的角度来看，内存映射实际上可以看作是一组专门用于连续虚拟地址范围的页表条目。这个*连续虚拟地址范围*称为内存区域，或**虚拟内存区域**（**VMA**）。每个内存映射由起始地址和长度、权限（例如程序是否可以从该内存读取、写入或执行）以及关联资源（例如物理页面、交换页面、文件内容等）描述。
- en: 'A `mm_struct` has two ways to store process regions (VMA):'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`mm_struct`有两种存储进程区域（VMA）的方式：'
- en: In a red-black tree, whose root element is pointed by the field `mm_struct->mm_rb`
    .
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在红黑树中，根元素由字段`mm_struct->mm_rb`指向。
- en: In a linked list, where the first element is pointed by the field ``mm_struct->mmap``
    .
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在一个链表中，第一个元素由字段``mm_struct->mmap``指向。
- en: Virtual Memory Area (VMA)
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 虚拟内存区域（VMA）
- en: The kernel uses virtual memory areas to keep track of the processes memory mappings,
    for example, a process having one VMA for its code, one VMA for each type of data,
    one VMA for each distinct memory mapping (if any), and so on. VMAs are processor-independent
    structures, with permissions and access control flags. Each VMA has a start address,
    a length, and their sizes are always a multiple of page size (`PAGE_SIZE` ). A
    VMA consists of a number of pages, each of which has an entry in the page table.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 内核使用虚拟内存区域来跟踪进程的内存映射，例如，一个进程对于其代码有一个VMA，对于每种类型的数据有一个VMA，对于每个不同的内存映射（如果有的话）有一个VMA等等。VMAs是处理器无关的结构，具有权限和访问控制标志。每个VMA都有一个起始地址和长度，它们的大小始终是页面大小（`PAGE_SIZE`）的倍数。VMA由多个页面组成，每个页面在页表中都有一个条目。
- en: Memory regions described by VMA are always virtually contiguous, not physically.
    One can check all VMAs associated with a process through the `/proc/<pid>/maps`
    file, or using the `pmap` command on a process ID.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: VMA描述的内存区域始终是虚拟连续的，而不是物理的。可以通过`/proc/<pid>/maps`文件或使用`pmap`命令来检查与进程关联的所有VMA。
- en: '![](img/Image00020.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Image00020.jpg)'
- en: 'Image source: http://duartes.org/gustavo/blog/post/how-the-kernel-manages-your-memory/'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：http://duartes.org/gustavo/blog/post/how-the-kernel-manages-your-memory/
- en: '[PRE5]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Each line in the preceding excerpt represents a VMA, and fields map the following
    pattern: `{address (start-end)} {permissions} {offset} {device (major:minor)}
    {inode} {pathname (image)}` :'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 前面摘录中的每一行都代表一个VMA，字段映射以下模式：`{address（start-end）} {permissions} {offset} {device（major:minor）}
    {inode} {pathname（image）}`：
- en: '`address` : This represents the starting and ending address of the VMA.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`address`：这表示VMA的起始和结束地址。'
- en: '`permissions` : This describes access right of the region: `r` (read), `w`
    (write), and `x` (execute), including `p` (if the mapping is private) and `s`
    (for shared mapping).'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`permissions`：这描述了区域的访问权限：`r`（读取）、`w`（写入）和`x`（执行），包括`p`（如果映射是私有的）和`s`（用于共享映射）。'
- en: '`Offset` **:** In the case of file mapping (`mmap` system call), it is the
    offset in the file where the mapping takes place. It is `0` otherwise.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Offset`：在文件映射（`mmap`系统调用）的情况下，它是映射发生的文件中的偏移量。否则为`0`。'
- en: '`major:minor` **:** In case of file mapping, these represent the major and
    minor number of the devices in which the file is stored (device holding the file).'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`major:minor`：在文件映射的情况下，它们表示文件存储的设备的主要和次要编号（保存文件的设备）。'
- en: '`inode` : In the case of mapping from a file, the inode number of the mapped
    file.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inode`：在从文件映射的情况下，表示映射文件的inode号。'
- en: '`pathname` : This is the name of the mapped file, or left blank otherwise.
    There are other region name such as `[heap]` , `[stack]` , or `[vdso]` , which
    stands for virtual dynamic shared object, which is a shared library mapped by
    the kernel into every process address space, in other to reduce performance penalties
    when system calls switch to kernel mode.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pathname`：这是映射文件的名称，否则为空白。还有其他区域名称，如`[heap]`，`[stack]`或`[vdso]`，表示虚拟动态共享对象，这是内核映射到每个进程地址空间的共享库，以减少系统调用切换到内核模式时的性能损失。'
- en: Each page allocated to a process belongs to an area; thus, any page that does
    not live in the VMA does not exist and cannot be referenced by the process.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 分配给进程的每个页面都属于一个区域；因此，任何不在VMA中的页面都不存在，也不能被进程引用。
- en: High memory is perfect for user space because user space's virtual address must
    be explicitly mapped. Thus, most high memory is consumed by user applications.
    `__GFP_HIGHMEM` and `GFP_HIGHUSER` are the flags for requesting the allocation
    of (potentially) high memory. Without these flags, all kernel allocations return
    only low memory. There is no way to allocate contiguous physical memory from user
    space in Linux.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 高内存非常适合用户空间，因为用户空间的虚拟地址必须显式映射。因此，大多数高内存被用户应用程序占用。`__GFP_HIGHMEM`和`GFP_HIGHUSER`是请求分配（可能）高内存的标志。没有这些标志，所有内核分配只返回低内存。在Linux中，没有办法从用户空间分配连续的物理内存。
- en: One can use the `find_vma` function to find the VMA that corresponds to a given
    virtual address. `find_vma` is declared in `linux/mm.h` *:*
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`find_vma`函数找到与给定虚拟地址对应的VMA。`find_vma`在`linux/mm.h`中声明。
- en: '[PRE6]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This is an example:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个例子：
- en: '[PRE7]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The whole process of memory mapping can be obtained by reading files: `/proc/<PID>/map`
    , `/proc/<PID>/smap` , and `/proc/<PID>/pagemap` .'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过读取文件`/proc/<PID>/map`，`/proc/<PID>/smap`和`/proc/<PID>/pagemap`来获取内存映射的整个过程。
- en: Address translation and MMU
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 地址转换和MMU
- en: Virtual memory is a concept, an illusion given to a process so it thinks it
    has large and almost infinite memory, and sometimes more than the system really
    has. It is up to the CPU to make the conversion from virtual to physical address
    every time one accesses a memory location. That mechanism is called address translation,
    and is performed by the **Memory Management Unit (MMU** ), which is a part of
    the CPU.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: MMU protects memory from unauthorized access. Given a process, any page that
    needs to be accessed must exist in one of the process VMAs, and thus, must live
    in the process page table (every process has its own).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Memory is organized by chunks of fixed size named **pages** for virtual memory,
    and **frames** for physical memory, sized 4 KB in our case. Anyway, you do not
    need to guess the page size of the system you write the driver for. It is defined
    and accessible with the `PAGE_SIZE` macro in the kernel. Remember therefore, page
    size is imposed by the hardware (CPU). Considering a 4 KB page sized system, bytes
    0 to 4095 fall in page 0, bytes 4096-8191 fall in page 1, and so on.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: The concept of page table is introduced to manage mapping between pages and
    frames. Pages are spread over tables, so that each PTE corresponds to a mapping
    between a page and a frame. Each process is then given a set of page tables to
    describe its whole memory space.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to walk through pages, each page is assigned an index (like an array),
    called the page number. When it comes to frame, it is PFN**.** This way, virtual
    memory addresses are composed of two parts: a page number and an offset. The offset
    represents the 12 less significant bits of the address, whereas 13 less significant
    bits represent it on 8 KB page size systems:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Image00021.gif)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
- en: Virtual address representation
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: 'How do the OS or CPU know which physical address corresponds to a given virtual
    address? They use the page table as the translation table, and know that each
    entry''s index is a virtual page number, and the value is the PFN. To access physical
    memory given a virtual memory, the OS first extracts the offset, the virtual page
    number, and then walks through the process''s page tables in order to match virtual
    page number to physical page. Once a match occurs, it is then possible to access
    data into that page frame:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Image00022.gif)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
- en: Address translation
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: The offset is used to point to the right location into the frame. Page table
    does not only hold mapping between physical and virtual page number, but also
    access control information (read/write access, privileges, and so on).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Image00023.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
- en: Virtual to physical address translation
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: The number of bits used to represent the offset is defined by the kernel macro
    `PAGE_SHIFT` . `PAGE_SHIFT` is the number of bits to shift one bit left to obtain
    the `PAGE_SIZE` value. It is also the number of bits to right-shift to convert
    the virtual address to the page number and the physical address to the page frame
    number. The following are the definitions of these macros from `/include/asm-generic/page.h`
    in the kernel sources*:*
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Page table is a partial solution. Let us see why. Most architecture requires
    32 bits (4 bytes) to represent a PTE. Each process having its private 3 GB user
    space address, we need 786,432 entries to characterize and cover a process address
    space. It represents too much physical memory spent per process, just to characterize
    the memory mappings. In fact, a process generally uses a small but scattered portion
    of its virtual address space. To resolve that issue, the concept of *level* is
    introduced. Page tables are hierarchized by level (page level). The space necessary
    to store a multi-level page table only depends on the virtual address space actually
    in use, instead of being proportional to the maximum size of the virtual address
    space. This way, unused memory is no longer represented, and the page table walk
    through time is reduced. This way, each table entry in level N will point to an
    entry in table of level N+1\. Level 1 is the higher level.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 页表是一个部分解决方案。让我们看看为什么。大多数架构需要32位（4字节）来表示PTE。每个进程都有其私有的3GB用户空间地址，我们需要786,432个条目来描述和覆盖进程的地址空间。这代表了每个进程花费太多的物理内存，只是为了描述内存映射。事实上，进程通常只使用其虚拟地址空间的一小部分但分散的部分。为了解决这个问题，引入了*级别*的概念。页表通过级别（页级别）进行层次化。存储多级页表所需的空间仅取决于实际使用的虚拟地址空间，而不是与虚拟地址空间的最大大小成比例。这样，未使用的内存不再表示，页表遍历时间缩短。这样，级别N中的每个表项将指向级别N+1的表中的一个条目。级别1是更高级别。
- en: 'Linux uses a four-level paging model:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Linux使用四级分页模型：
- en: '**Page Global Directory** (**PGD** ): It is the first level (level 1) page
    table. Each entry''s type is `pgd_t` in kernel (generally an `unsigned long` ),
    and point on an entry in table at the second level. In kernel, the structure `tastk_struct`
    represents a process''s description, which in turn has a member (`mm` ) whose
    type is `mm_struct` , and that characterizes and represents the process''s memory
    space. In the `mm_struct` , there is a processor-specific field `pgd` , which
    is a pointer on the first entry (entry 0) of the process''s level-1 (PGD) page
    table. Each process has one and only one PGD, which may contain up to 1024 entries.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**页全局目录**（**PGD**）：这是第一级（级别1）页表。内核中每个条目的类型是`pgd_t`（通常是`unsigned long`），并指向第二级表中的一个条目。在内核中，`tastk_struct`结构表示进程的描述，它又有一个成员（`mm`），其类型是`mm_struct`，用于描述和表示进程的内存空间。在`mm_struct`中，有一个特定于处理器的字段`pgd`，它是进程级别1（PGD）页表的第一个条目（条目0）的指针。每个进程只有一个PGD，最多可以包含1024个条目。'
- en: '**P** **age Upper Directory** (**PUD** ): This exist only on architectures
    using four-level tables. It represent the socong level of indirection.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**页上级目录**（**PUD**）：这仅存在于使用四级表的架构中。它代表间接的第二级。'
- en: '**P** **age Middle Directory** ( **PMD** ): This is the third indirection level,
    and exists only on architectures using four-level tables.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**页中间目录**（**PMD**）：这是第三级间接层，仅存在于使用四级表的架构中。'
- en: '**Page Table** (**PTE** ): Leaves of the tree. It is an array of `pte_t` ,
    where each entry points to the physical page.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**页表**（**PTE**）：树的叶子。它是一个`pte_t`数组，其中每个条目指向物理页。'
- en: All levels are not always used. The i.MX6's MMU only supports a 2 level page
    table (`PGD` and `PTE` ), it is the case for almost all 32-bit CPUs) In this case,
    `PUD` and `PMD` are simply ignored.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有级别都总是被使用。i.MX6的MMU只支持2级页表（`PGD`和`PTE`），几乎所有32位CPU都是如此。在这种情况下，`PUD`和`PMD`被简单地忽略。
- en: '![](img/Image00024.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Image00024.jpg)'
- en: Two-level tables overview
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 两级表概述
- en: 'You might ask how MMU is aware of the process page table. It is simple, MMU
    does not store any address. Instead, there is a special register in the CPU, called
    **page table base register** (**PTBR** ) or **Translation Table Base Register
    0** (**TTBR0** ), which points to the base (entry 0) of the level-1 (top level)
    page table (PGD) of the process. It is exactly where the field `pdg` of `struct
    mm_struct` points: `current->mm.pgd == TTBR0` .'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问MMU如何知道进程的页表。很简单，MMU不存储任何地址。相反，在CPU中有一个特殊的寄存器，称为**页表基址寄存器**（**PTBR**）或**转换表基址寄存器0**（**TTBR0**），它指向进程的一级（顶级）页表（PGD）的基址（条目0）。这正是`struct
    mm_struct`的`pdg`字段指向的地方：`current->mm.pgd == TTBR0`。
- en: At context switch (when a new process is scheduled and given the CPU), the kernel
    immediately configures the MMU, and updates the PTBR with the new process's `pgd`
    . Now when a virtual address is given to MMU, it uses the PTBR's content to locate
    the process's level-1 page table (PGD), and then it uses the level-1 index, extracted
    from the **most significant bits** (**MSBs** ) of the virtual address, to find
    the appropriate table entry, which contains a pointer to the base address of the
    appropriate level-2 page table. Then, from that base address, it uses the level-2
    index to find the appropriate entry and so on until it reaches the PTE. ARM architecture
    (i.MX6 in our case) has a 2-level page table. In this case, the level-2 entry
    is a PTE, and points to the physical page (PFN). Only the physical page is found
    at this step. To access the exact memory location in the page, the MMU extracts
    the memory offset, also part of the virtual address, and points on the same offset
    in the physical page.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在上下文切换（当新进程被调度并获得CPU时），内核立即配置MMU，并使用新进程的`pgd`更新PTBR。现在，当虚拟地址被提供给MMU时，它使用PTBR的内容来定位进程的一级页表（PGD），然后使用从虚拟地址的**最高有效位**（**MSBs**）提取的一级索引来找到适当的表项，其中包含指向适当的二级页表的基地址的指针。然后，从该基地址开始，它使用二级索引来找到适当的条目，依此类推，直到达到PTE。
    ARM架构（我们的情况下是i.MX6）具有2级页表。在这种情况下，二级条目是PTE，并指向物理页（PFN）。只有在这一步找到物理页。为了访问页面中的确切内存位置，MMU提取内存偏移量，也是虚拟地址的一部分，并指向物理页面中的相同偏移量。
- en: When a process needs to read from or write into a memory location (of course
    we're talking about virtual memory), the MMU performs a translation into that
    process's page table, to find the right entry (`PTE` ). The virtual page number
    is extracted (from the virtual address) and used by the processor as an index
    into the processes page table to retrieve its page table entry. If there is a
    valid page table entry at that offset, the processor takes the page frame number
    from this entry. If not, it means the process accessed an unmapped area of its
    virtual memory. A page fault is then raised and the OS should handle it.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 当进程需要从内存位置读取或写入（当然我们谈论的是虚拟内存）时，MMU执行该进程的页表中的翻译，以找到正确的条目（`PTE`）。从虚拟地址中提取虚拟页号，并由处理器用作进程页表的索引，以检索其页表条目。如果该偏移处有有效的页表条目，则处理器从该条目中获取页框号。如果没有，则意味着进程访问了其虚拟内存的未映射区域。然后引发页面错误，操作系统应该处理它。
- en: 'In the real world, address translation requires a page table walk, and it is
    not always a one-shot operation. There are at least as many memory accesses as
    there are table levels. A four-level page table would require four memory accesses.
    In other words, every virtual access would result in five physical memory accesses.
    The virtual memory concept would be useless if its access were four times slower
    than a physical access. Fortunately, SoC manufacturers worked hard to find a clever
    trick to address this performance issue: modern CPUs use a small associative and
    very fast memory called **translation lookaside buffer** (**TLB** ), in order
    to cache the PTEs of recently accessed virtual pages.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，地址转换需要进行页表遍历，并且它并不总是一次性操作。至少有与表级别相同数量的内存访问。四级页表将需要四次内存访问。换句话说，每次虚拟访问都会导致五次物理内存访问。如果虚拟内存访问比物理访问慢四倍，那么虚拟内存概念将是无用的。幸运的是，SoC制造商努力找到了一个聪明的技巧来解决这个性能问题：现代CPU使用一个小的关联和非常快速的内存，称为**翻译后备缓冲器**（**TLB**），以缓存最近访问的虚拟页面的PTE。
- en: Page look up and TLB
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 页面查找和TLB
- en: Before the MMU proceeds to address translation, there is another step involved.
    As there is a cache for recently accessed data, there is also a cache for recently
    translated addresses. As a data cache speeds up the data accessing process, TLB
    speeds up virtual address translation (yes, address translation is a tricky task.
    It is content-addressable memory, abbreviated (**CAM** ), where the key is the
    virtual address and the value is the physical address. In other words, the TLB
    is a cache for the MMU. At each memory access, the MMU first checks for recently
    used pages in the TLB, which contains a few of the virtual address ranges to which
    physical pages are currently assigned.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在MMU进行地址转换之前，还有另一步骤。就像有一个用于最近访问数据的缓存一样，还有一个用于最近转换地址的缓存。数据缓存加快了数据访问过程，TLB加快了虚拟地址转换（是的，地址转换是一项棘手的任务。它是内容可寻址存储器，简称（**CAM**），其中键是虚拟地址，值是物理地址。换句话说，TLB是MMU的缓存。在每次内存访问时，MMU首先检查TLB中最近使用的页面，其中包含一些当前分配给物理页面的虚拟地址范围。
- en: How does TLB work
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TLB是如何工作的
- en: On a virtual memory access, the CPU walks through the TLB trying to find the
    virtual page number of the page that is being accessed. This step is called TLB
    lookup. When a TLB entry is found (a match occurred), one says there is a **TLB
    hit** and the CPU just keeps running and uses the PFN found in the TLB entry to
    calculate the target physical address. There is no page fault when a TLB hit occurs.
    As one can see, as long as a translation can be found in the TLB, virtual memory
    access will be as fast as a physical access. If no TLB entry is found (no match
    occured), one says there is a **TLB miss** .
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在虚拟内存访问时，CPU通过TLB尝试查找正在访问的页面的虚拟页号。这一步称为TLB查找。当找到TLB条目（发生匹配）时，就会发生**TLB命中**，CPU继续运行并使用在TLB条目中找到的PFN来计算目标物理地址。当发生TLB命中时，不会发生页面错误。可以看到，只要在TLB中找到翻译，虚拟内存访问就会像物理访问一样快。如果没有找到TLB条目（未发生匹配），就会发生**TLB未命中**。
- en: 'On a TLB miss event, there are two possibilities, depending on the processor
    type, TLB miss events can be handled by the software, or by the hardware, through
    the MMU:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在TLB未命中事件中，根据处理器类型，TLB未命中事件可以由软件或硬件通过MMU处理：
- en: '**Software handling** : The CPU raises a TLB miss interruption, caught by the
    OS. The OS then walks through the process''s page table to find the right PTE.
    If there is a matching and valid entry, then the CPU installs the new translation
    in the TLB. Otherwise, the page fault handler is executed.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**软件处理**：CPU引发TLB未命中中断，被操作系统捕获。然后操作系统遍历进程的页表以找到正确的PTE。如果有匹配和有效的条目，那么CPU会在TLB中安装新的翻译。否则，将执行页面错误处理程序。'
- en: '**Hardware handling** : It is up to the CPU (the MMU in fact) to walk through
    the process''s page table in hardware. If there is a matching and valid entry,
    the CPU adds the new translation in the TLB. Otherwise, the CPU raises a page
    fault interruption, handled by the OS.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**硬件处理**：由CPU（实际上是MMU）在硬件中遍历进程的页表。如果有匹配和有效的条目，CPU将新的转换添加到TLB中。否则，CPU会引发页面错误中断，由操作系统处理。'
- en: 'In both cases, the page fault handler is the same: the `do_page_fault()` function
    is executed, which is architecture-dependent. For ARM, the `do_page_fault` is
    defined in `arch/arm/mm/fault.c` :'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，页面错误处理程序是相同的：执行`do_page_fault()`函数，这取决于体系结构。对于ARM，`do_page_fault`在`arch/arm/mm/fault.c`中定义：
- en: '![](img/Image00025.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Image00025.jpg)'
- en: MMU and TLB walkthrough process
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: MMU和TLB遍历过程
- en: Page table and Page directory entries are architecture-dependent. It is up to
    the Operating system to ensure that the structure of the table corresponds to
    a structure recognized by the MMU. On the ARM processor, you must write the location
    of the translation table in CP15 (coprocessor 15) register c2, and then enable
    the caches and the MMU by writing to the CP15 register c1\. Have a look at both
    [http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.dui0056d/BABHJIBH.htm](http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.dui0056d/BABHJIBH.htm)
    and [http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.ddi0433c/CIHFDBEJ.html](http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.ddi0433c/CIHFDBEJ.html)
    for detailed information.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 页表和页目录条目是与体系结构相关的。操作系统需要确保表的结构与MMU识别的结构相对应。在ARM处理器上，您必须将转换表的位置写入CP15（协处理器15）寄存器c2，然后通过写入CP15寄存器c1来启用缓存和MMU。详细信息请参阅[http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.dui0056d/BABHJIBH.htm](http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.dui0056d/BABHJIBH.htm)和[http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.ddi0433c/CIHFDBEJ.html](http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.ddi0433c/CIHFDBEJ.html)。
- en: Memory allocation mechanism
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内存分配机制
- en: 'Let us look at the following figure, showing us different memory allocators
    existing on a Linux-based system, and discuss it later:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下下图，显示了Linux系统上存在的不同内存分配器，并稍后讨论它：
- en: 'Inspired from: [http://free-electrons.com/doc/training/linux-kernel/linux-kernel-slides.pdf](http://free-electrons.com/doc/training/linux-kernel/linux-kernel-slides.pdf)
    .'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 灵感来自：[http://free-electrons.com/doc/training/linux-kernel/linux-kernel-slides.pdf](http://free-electrons.com/doc/training/linux-kernel/linux-kernel-slides.pdf)。
- en: '![](img/Image00026.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Image00026.jpg)'
- en: Overview of kernel memory allocator
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 内核内存分配器概述
- en: There is an allocation mechanism to satisfy any kind of memory request. Depending
    on what you need memory for, you can choose the one closer to your goal. The main
    allocator is the **Page Allocator** , which only works with pages (a page being
    the smallest memory unit it can deliver). Then comes the **SLAB Allocator** that
    is built on top of the page allocator, getting pages from it and returning smaller
    memory entities (by mean of slabs and caches). This is the allocator on which
    the **kmalloc Allocator** relies.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种分配机制可以满足任何类型的内存请求。根据您需要内存的用途，您可以选择最接近您目标的分配机制。主要的分配器是**页面分配器**，它只处理页面（页面是它可以提供的最小内存单位）。然后是建立在页面分配器之上的**SLAB分配器**，它从页面中获取页面并返回较小的内存实体（通过slab和缓存）。这是**kmalloc分配器**依赖的分配器。
- en: Page allocator
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 页面分配器
- en: Page allocator is the low-level allocator on the Linux system, the one on which
    other allocators rely on. System's physical memory is made up of fixed-size blocks
    (called page frames). A page frame is represented in the kernel as an instance
    of the `struct page` structure. A page is the smallest unit of memory that the
    OS will give to any memory request at low level.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 页面分配器是Linux系统上的低级分配器，其他分配器依赖于它。系统的物理内存由固定大小的块（称为页面帧）组成。页面帧在内核中表示为`struct page`结构的实例。页面是操作系统在低级别对任何内存请求提供的最小内存单位。
- en: Page allocation API
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 页面分配API
- en: 'You will have understood that the kernel page allocator allocates and deallocates
    blocks of pages using the buddy algorithm. Pages are allocated in blocks that
    are powers of 2 in size (in order to get the best from the buddy algorithm). That
    means that it can allocate a block 1 page, 2 pages, 4 pages, 8, 16, and so on:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 您将了解到，内核页面分配器使用伙伴算法分配和释放页面块。页面以2的幂大小分配在块中（为了从伙伴算法中获得最佳效果）。这意味着它可以分配1页、2页、4页、8页、16页等等：
- en: '`alloc_pages(mask, order)` allocates 2^(order) pages and returns an instance
    of `struct page` which represents the first page of the reserved block. To allocate
    only one page, order should be 0\. It is what `alloc_page(mask)` does:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`alloc_pages(mask, order)`分配2^(order)页并返回表示保留块的第一页的`struct page`实例。要分配一个页面，顺序应为0。这就是`alloc_page(mask)`的作用：'
- en: '[PRE9]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`__free_pages()` is used to free memory allocated with `alloc_pages()` function.
    It takes a pointer to the allocated page(s) as a parameter, with the same order
    as was used for allocation.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '`__free_pages()`用于释放使用`alloc_pages()`函数分配的内存。它接受指向分配页面的指针作为参数，与分配时使用的顺序相同。'
- en: '[PRE10]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'There are other functions working in the same way, but instead of an instance
    of struct page, they return the address (virtual of course) of the reserved block.
    These are `__get_free_pages(mask, order)` and `__get_free_page(mask)` :'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 还有其他以相同方式工作的函数，但它们返回保留块的地址（虚拟地址）。这些是`__get_free_pages(mask, order)`和`__get_free_page(mask)`：
- en: '[PRE11]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '`free_pages()` is used to free page allocated with `__get_free_pages()` . It
    takes the kernel address representing the start region of allocated page(s), along
    with the order, which should be the same as that used for allocation:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`free_pages()`用于释放使用`__get_free_pages()`分配的页面。它接受表示分配页面起始区域的内核地址，以及应该与分配时使用的相同的顺序：'
- en: '[PRE12]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In either case, `mask` specifies details about the request, which are the memory
    zones and the behavior of allocators. Choices available are:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在任一情况下，`mask`指定有关请求的详细信息，即内存区域和分配器的行为。可用选择是：
- en: '`GFP_USER` , for user memory allocation.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GFP_USER`，用于用户内存分配。'
- en: '`GFP_KERNEL` , the commonly used flag for kernel allocation.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GFP_KERNEL`，用于内核分配的常用标志。'
- en: '`GFP_HIGHMEM` **,** which requests memory from the HIGH_MEM zone.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GFP_HIGHMEM`，从HIGH_MEM区域请求内存。'
- en: '`GFP_ATOMIC` , which allocates memory in an atomic manner that cannot sleep.
    Used when one needs to allocate memory from an interrupt context.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GFP_ATOMIC`，以无法休眠的原子方式分配内存。在需要从中断上下文中分配内存时使用。'
- en: 'There is a warning on using `GFP_HIGHMEM` , which should not be used with `__get_free_pages()`
    (or `__get_free_page()` ). Since HIGHMEM memory is not guaranteed to be contiguous,
    you can''t return an address of a memory allocated from that zone. Globally only
    a subset of `GFP_*` is allowed in memory-related functions:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The maximum number of pages one can allocate is 1024\. It means that on a 4
    Kb sized system, you can allocate up to 1024*4 Kb = 4 MB at most. It is the same
    for `kmalloc` .
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Conversion functions
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `page_to_virt()` function is used to convert the struct page (as returned
    by `alloc_pages()` for example) into the kernel address. `virt_to_page()` takes
    a kernel virtual address and returns its associated struct page instance (as if
    it was allocated using the `alloc_pages()` function). Both `virt_to_page()` and
    `page_to_virt()` are defined in `<asm/page.h>` :'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The macro `page_address()` can be used to return the virtual address that corresponds
    to the beginning address (the logical address of course) of a struct page instance:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We can see how it is used in the `get_zeroed_page()` function:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '`__free_pages()` and `free_pages()` can be mixed. The main difference between
    them is that `free_page()` takes a virtual address as a parameter, whereas `__free_page()`
    takes a `struct page` structure.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Slab allocator
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Slab allocator is the one on which `kmalloc()` relies. Its main purpose is to
    eliminate the fragmentation caused by memory (de)allocation that would be caused
    by the buddy system in the case of small size memory allocation, and speed up
    memory allocation for commonly used objects.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: The buddy algorithm
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To allocate memory, the requested size is round up to a power of two, and the
    buddy allocator searches the appropriate list. If no entries exist on the requested
    list, an entry from the next upper list (which has blocks of twice the size of
    the previous list) is split into two halves (called **buddies** ). The allocator
    uses the first half, while the other is added to the next list down. This is a
    recursive approach, which stops when either the buddy allocator successfully finds
    a block which we can be split, or reaches the largest size of block and there
    are no free blocks available.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: The following case study is heavily inspired from [http://dysphoria.net/OperatingSystems1/4_allocation_buddy_system.html](http://dysphoria.net/OperatingSystems1/4_allocation_buddy_system.html)
    . For example, if the minimum allocation size is 1 KB, and the memory size is
    1 MB, the buddy allocator will create an empty list for 1 KB holes, empty list
    for 2 KB holes, one for 4 KB holes, 8 KB, 16 KB, 32 KB, 64 KB, 128 KB, 256 KB,
    512 KB, and one list for 1 MB holes. All of them are initially empty, except for
    the 1 MB list which has only one hole.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let us imagine a scenario where we want to allocate a **70K** block. The
    buddy allocator will round it up to **128K** , and end up splitting the 1 MB into
    two **512K** blocks, then **256K** , and finally **128K** , then it will allocate
    one of the **128K** blocks to the user. The following are schemes that summarize
    this scenario:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Image00027.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
- en: Allocation using buddy algorithm
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'The deallocation is as fast as allocation. The following figure summarize the
    deallocation algorithm:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Image00028.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
- en: Deallocation using buddy algorithm
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: A journey into the slab allocator
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we introduce the slab allocator, let us define some terms it uses:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '**Slab** : This is a contiguous piece of physical memory made of several page
    frames. Each slab is divided into equal chunks of the same size, used to store
    specific types of kernel object, such as inodes, mutexes, and so on. Each slab
    is then an array of objects.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cache** : It is made of one or more slabs in a linked list, and they are
    represented in the kernel as instances the of `struct kmem_cache_t` structure.
    The cache only stores objects of the same type (for example, inodes only, or only
    address space structures)'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Slabs may be in one of the following states:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '**Empty** : This is where all objects (chunks) on the slab are marked as free'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Partial** : Both used and free objects exist in the slab'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Full** : All objects on the slab are marked as used'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is up to the memory allocator to build caches. Initially, each slab is marked
    as empty. When one (code) allocates memory for a kernel object, the system looks
    for a free location for that object on a partial/free slab in a cache for that
    type of object. If not found, the system allocates a new slab and adds it into
    the cache. The new object gets allocated from this slab, and the slab is marked
    as **partial** . When the code is done with the memory (memory freed), the object
    is simply returned to the slab cache in its initialized state.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 'It is the reason why the kernel also provides helper functions to obtain zeroed
    initialized memory, in order to get rid of the previous content. The slab keeps
    a reference count of how many of its objects are being used, so that when all
    slabs in a cache are full and another object is requested, the slab allocator
    is responsible for adding new slabs:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Image00029.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
- en: Slab cache overview
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: It is a bit like creating a per-object allocator. The system allocate one cache
    per type of object, and only objects of the same type can be stored in a cache
    (For example, only `task_struct` structure).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'There are different kinds of slab allocator in the kernel, depending on whether
    or not one needs compactness, cache-friendliness, or raw speed:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: The **SLOB** , which is as compact as possible
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **SLAB** , which is as cache-friendly as possible
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **SLUB** , which is quite simple and requires fewer instruction cost counts
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: kmalloc family allocation
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`kmalloc` is a kernel memory allocation function, such as `malloc()` in user
    space. Memory returned by `kmalloc` is contiguous in physical memory and in virtual
    memory:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Image00030.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
- en: 'The kmalloc allocator is the general and higher-level memory allocator in the
    kernel, which relies on the SLAB allocator. Memory returned from kmalloc has a
    kernel logical address because it is allocated from the `LOW_MEM` region, unless
    `HIGH_MEM` is specified. It is declared in `<linux/slab.h>` , which is the header
    to include when using kmalloc in your driver. The following is the prototype:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '`size` specifies the size of the memory to be allocated (in bytes). `flag`
    determines how and where memory should be allocated. Available flags are the same
    as the page allocator (`GFP_KERNEL` , `GFP_ATOMIC` , `GFP_DMA` , and so on).'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '`GFP_KERNEL` : This is the standard flag. We cannot use this flag in the interrupt
    handler because its code may sleep. It always returns memory from `LOM_MEM` zone
    (hence a logical address).'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`GFP_ATOMIC` : This guarantees the atomicity of the allocation. The only flag
    to use when we are in the interrupt context. Please do not abuse this, since it
    uses an emergence pool of memory.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`GFP_USER` : This allocates memory to a user space process. Memory is then
    distinct and separated from that allocated to the kernel.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`GFP_HIGHUSER` : This allocates memory from `HIGH_MEMORY` zone'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`GFP_DMA` : This allocates memory from `DMA_ZONE` .'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On successful allocation of memory, kmalloc returns the virtual address of the
    chunk allocated, guaranteed to be physically contiguous. On error, it returns
    `NULL` .
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Kmalloc relies on SLAB caches when allocating small size memories. In this case,
    the kernel rounds the allocated area size up to the size of the smallest SLAB
    cache in which it can fit. Always use it as your default memory allocator. In
    architectures used in this book (ARM and x86), the maximum size per allocation
    is 4 MB, and 128 MB for total allocations. Have a look at [https://kaiwantech.wordpress.com/2011/08/17/kmalloc-and-vmalloc-linux-kernel-memory-allocation-api-limits/
    .](https://kaiwantech.wordpress.com/2011/08/17/kmalloc-and-vmalloc-linux-kernel-memory-allocation-api-limits/)
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: The `kfree` function is used to free the memory allocated by kmalloc. The following
    is the prototype of `kfree()` ;
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let us see an example:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Other family-like functions are:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '`krealloc()` is the kernel equivalent of the user space `realloc()` function.
    Because memory returned by `kmalloc()` retains the contents from its previous
    incarnation, there could be a security risk if it''s exposed to user space. To
    get zeroed kmalloc''ed memory, one should use `kzalloc` . `kzfree()` is the freeing
    function for `kzalloc()` , whereas `kcalloc()` allocates memory for an array,
    and its parameters `n` and `size` represent respectively the number of elements
    in the array and the size of an element.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Since `kmalloc()` returns a memory area in the kernel permanent mapping (which
    mean physically contiguous), the memory address can be translated to a physical
    address using `virt_to_phys()` , or to a IO bus address using `virt_to_bus()`
    . These macros internally call either `__pa()` or `__va()` if necessary. The physical
    address (`virt_to_phys(kmalloc'ed address)` ), downshifted by `PAGE_SHIFT` , will
    produce a PFN of the first page from which the chunk is allocated.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: vmalloc allocator
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`vmalloc()` is the last kernel allocator we will discuss in the book. It returns
    memory only contiguous on the virtual space (not physically contiguous):'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Image00031.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
- en: The returned memory always comes from `HIGH_MEM` zone. Addresses returned cannot
    be translated into a physical one or into bus address, because one cannot assert
    that the memory is physically contiguous. It means memory returned by `vmalloc()`
    can't be used outside the microprocessor (you cannot easily use it for DMA purposes).
    It is correct to use `vmalloc()` to allocate memory for a large (it does not make
    sense to use it to allocate one page for example) sequential that exists only
    in software, for example, a network buffer. It is important to note that `vmalloc()`
    is slower than `kmalloc()` or page allocator functions, because it must retrieve
    the memory, build the page tables, or even remap into a virtually contiguous range,
    whereas `kmalloc()` never does that.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'Before using this vmalloc API, you should include this header in the code:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The following are the vmalloc family prototype:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '`size` is the size of memory you need to allocate. Upon successful allocation
    of memory, it returns the address of the first byte of the allocated memory block.
    On failure, it returns a `NULL` . `vfree` function, which is used to free the
    memory allocated by `vmalloc()` .'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of using `vmalloc` :'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: One can use `/proc/vmallocinfo` to display all vmalloc'ed memory on the system.
    `VMALLOC_START` and `VMALLOC_END` are two symbols that delimit the vmalloc address
    range. They are architecture-dependent and defined in `<asm/pgtable.h>` .
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Process memory allocation under the hood
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let us focus on the lower level allocator, which allocates pages of memory.
    The kernel will report allocation of frame pages (physical pages) until really
    necessary (when those are actually accessed, by reading or writing). This on-demand
    allocation is called **lazy-allocation** , eliminating the risk of allocating
    pages that will never be used.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Whenever a page is requested, only the page table is updated, in most of the
    cases, a new entry is created, which means only virtual memory is allocated. Only
    when you access the page, an interrupt called **page fault** is raised. This interrupt
    has a dedicated handler, called the page fault handler, and is called by the MMU
    in response to an attempt to access virtual memory, which did not immediately
    succeed.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 'Actually, a page fault interrupt is raised whatever the access type is (read,
    write, execute), to a page whose entry in the page table has not got the appropriate
    permission bits set to allow that type of access. The response to that interrupt
    falls in one of the following three ways:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '**The hard fault** : The page does not reside anywhere (neither in the physical
    memory nor a memory-mapped file), which means the handler cannot immediately resolve
    the fault. The handler will perform I/O operations in order to prepare the physical
    page needed to resolve the fault, and may suspend the interrupted process and
    switch to another while the system works to resolve the issue.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The soft fault** : The page resides elsewhere in memory (in the working set
    of another process). It means the fault handler may resolve the fault by immediately
    attaching a page of physical memory to the appropriate page table entry, adjusting
    the entry, and resuming the interrupted instruction.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The fault cannot be resolved** : This will result in a bus error or segv.
    `SIGSEGV` is sent to the faulty process, killing it (the default behavior) unless
    a signal handler has been installed for `SIGSEV` to change the default behavior.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory mappings generally start out with no physical pages attached, by defining
    the virtual address ranges without any associated physical memory. The actual
    physical memory is allocated later in response to a page fault exception, when
    the memory is accessed, since the kernel provides some flags to determine whether
    the attempted access was legal, and specify the behavior of the page fault handler.
    Thus, the user space `brk(), mmap()` and similar allocate (virtual) space, but
    physical memory is attached later.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: A page fault occurring in the interrupt context causes a **double fault** interrupt,
    which usually panics the kernel (calling the `panic()` function) . It is the reason
    why memory allocated in the interrupt context is taken from a memory pool, which
    does not raise page fault interrupts. If an interrupt occurs when a double fault
    is being handled, a triple fault exception is generated, causing the CPU to shut
    down and the OS immediately reboots. This behavior is actually arc-dependent.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: The copy-on-write (CoW) case
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The CoW (heavily used with `fork()` ) is a kernel feature that does not allocate
    several time the memory for a data shared by two or more processes, until a process
    touches it (write into it); in this case memory is allocated for its private copy.
    The following shows how a page fault handler manages CoW (one-page case study):'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: A PTE is added to the process page table, and marked as un-writable.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The mapping will result in a VMA creation in the process VMA list. The page
    is added to that VMA and that VMA is marked as writable.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On page access (at the first write), the fault handler notices the difference,
    which means: **this is a Copy on write** . It will then allocate a physical page,
    which is assigned to the PTE added above, update the PTE flags, flush the TLB
    entry, and execute the `do_wp_page()` function, which can copy the content from
    the shared address to the new location.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Work with I/O memory to talk with hardware
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Apart from performing data RAM-oriented operations, one can perform I/O memory
    transactions, to talk with the hardware. When it comes to the access device''s
    register, the kernel offers two possibilities depending on the system architecture:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '**Through the I/O ports** : This is also called **Port Input Output** (**PIO**
    ). Registers are accessible through a dedicated bus, and specific instructions
    (`in` and `out` , in assembler generally) are needed to access those registers.
    It is the case on x86 architectures.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory Mapped Input Output** (**MMIO** ): This is the most common and most
    used method. The device''s registers are mapped to memory. Simply read and write
    to a particular address to write to the registers of the device. It is the case
    on ARM architectures.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PIO devices access
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: On a system on which PIO is used, there are two different address spaces, one
    for memory, which we have already discussed, and the other one for I/O ports,
    called the port address space, limited to 65,536 ports only. This is a old way,
    and very uncommon nowadays.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 'The kernel exports a few functions (symbols) to handle I/O port. Prior to accessing
    any port regions, we must first inform the kernel that we are using a range of
    ports using the `request_region()` function, which will return `NULL` on error.
    Once done with the region, one must call `release_region()` . These are both declared
    in `linux/ioport.h` . Their prototypes are:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Those functions inform the kernel about your intention to use/release of a region
    `len` ports, starting from `start` . The `name` parameter should be set with the
    name of your device. Their use is not mandatory. This is a kind of politeness,
    which prevents two or more drivers from referencing the same range of ports. One
    can display information about the ports actually in use on the system by reading
    the content of `/proc/ioports` files.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: 'Once one is done with region reservation, one can access the port using the
    following functions:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'which respectively access (read) 8-, 16-, or 32-bits sized (wide) ports, and
    the following functions:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: which write `b` data, 8-, 16-, or 32-bits sized, into `addr` port.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: The fact that PIO uses a different set of instruction to access I/O ports or
    MMIO is a disadvantage because PIO requires more instructions than normal memory
    to accomplish the same task. For instance, 1-bit testing has only one instruction
    in MMIO, whereas PIO requires reading the data into a register before testing
    the bit, which is more than one instruction.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: MMIO devices access
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Memory-mapped I/O reside same address space than memory. The kernel uses part
    of the address space normally used by RAM (`HIGH_MEM` actually) to map the devices
    registers, so that instead of having real memory (that is, RAM) at that address,
    I/O device take place. Thus, communicating to an I/O device becomes like reading
    and writing to memory addresses devoted to that I/O device.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: 'Like PIO, there are MMIO functions, to inform the kernel about our intention
    to use a memory region. Remember it is a pure reservation only. These are `request_mem_region()`
    and `release_mem_region()` :'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: It is also a politeness.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: One can display memory regions actually in use on the system by reading the
    content of the `/proc/iomem` file.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'Prior to accessing a memory region (and after you successfully request it),
    the region must be mapped into kernel address space by calling special architecture-dependent
    functions (which make use of MMU to build the page table, and thus cannot be called
    from the interrupt handler). These are `ioremap()` and `iounmap()` , which handle
    cache coherency too:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '`ioremap()` returns a `__iomem void` pointer to the start of the mapped region.
    Do not be tempted to deference (get/set the value by reading/writing to the pointer)
    such pointers. The kernel provides functions to access ioremap''ed memories. These
    are:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '`ioremap` builds new page tables, just as `vmalloc` does. However, it does
    not actually allocate any memory but instead, returns a special virtual address
    that one can use to access the specified physical address range.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: On 32-bit systems, the fact that MMIO steals physical memory address space to
    create mapping for memory-mapped I/O devices is a disadvantage, since it prevents
    the system from using the stolen memory for general RAM purpose.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: __iomem cookie
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`__iomem` is a kernel cookie used by Sparse, a semantic checker used by the
    kernel to find possible coding faults. To take advantage of the features offered
    by Sparse, it should be enabled at kernel compile time; if not, `__iomem` cookie
    will be ignored anyway.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 'The `C=1` in the command line will enable Sparse for you, but parse should
    be installed first on your system:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'For example, when building a module, use:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Alternatively, if the makefile is well written, just type:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The following shows how __iomem is defined in the kernel:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'It prevents us from faulty drivers performing I/O memory access. Adding the
    `__iomem` for all I/O accesses is a way to be stricter too. Since even I/O access
    is done through virtual memory (on systems with MMU), this cookie prevents us
    from using absolute physical addresses, and requires us to use `ioremap()` , which
    will return a virtual address tagged with `__iomem` cookie:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: So we can use dedicated functions, such as `ioread23()` and `iowrite32()` .
    You may wonder why one does not use the `readl()` /`writel()` function. Those
    are deprecated, since these do not make sanity checks and are less secure (no
    `__iomem` required), than `ioreadX()` /`iowriteX()` family functions, which accept
    only `__iomem` addresses.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, `noderef` is an attribute used by Sparse to make sure programmers
    do not dereference a `__iomem` pointer. Even though it could work on some architecture,
    you are not encouraged to do that. Use the special `ioreadX()` /`iowriteX()` function
    instead. It is portable and works on every architecture. Now let us see how Sparse
    will warn us when dereferencing a `__iomem` pointer:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'First, Sparse is not happy because of the wrong type initializer:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Or:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Sparse is still not happy:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'This last example makes Sparse happy:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The two rules that you must remember are:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: Always use `__iomem` where it is required whether it is as a return type or
    as a parameter type, and use Sparse to make sure you did so
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do not dereference a `__iomem` pointer; use a dedicated function instead
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory (re)mapping
  id: totrans-285
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kernel memory sometimes needs to be remapped, either from kernel to user space,
    or from kernel to kernel space. The common use case is remapping the kernel memory
    to user space, but there are other cases, when one need to access high memory
    for example.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: kmap
  id: totrans-287
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linux kernel permanently maps 896 MB of its address space to the lower 896 MB
    of the physical memory (low memory). On a 4 GB system, there is only 128 MB left
    to the kernel to map the remaining 3.2 GB of physical memory (high memory). Low
    memory is directly addressable by the kernel because of the permanent and one-to-one
    mapping. When it comes to high memory (memory above 896 MB), the kernel has to
    map the requested region of high memory into its address space, and the 128 MB
    mentioned before are especially reserved for this. The function used to perform
    this trick, `kmap()` . `kmap()` , is used to map a given page into the kernel
    address space.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '`page` is a pointer to the `struct page` structure to map. When a high memory
    page is allocated, it is not directly addressable. `kmap()` is the function one
    must call to temporarily map high memory into the kernel address space. The mapping
    will last until `kunmap()` is called:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: By temporarily, I mean the mapping should be undone as soon as it is not needed
    anymore. Remember, 128 MB is not enough to map 3.2 GB. The best programming practice
    is to unmap high memory mappings when no longer required. It is why the `kmap()`
    - `kunmap()` sequence has to be entered around every access to the high memory
    page. .
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: 'This function works on both high memory and low memory. That says, if the page
    structure resides in low memory, then just the virtual address of the page is
    returned (because low memory pages already have permanent mappings). If the page
    belongs to high memory, a permanent mapping is created in the kernel''s page tables
    and the address is returned:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Mapping kernel memory to user space
  id: totrans-295
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mapping physical addresses is one of the most useful functionalities, especially
    in embedded systems. Sometime you may want to share part of kernel memory with
    user space. As said earlier, CPU runs in unprivileged mode when running in user
    space. To let a process access a kernel memory region, we need to remap that region
    into the process address space.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: Using remap_pfn_range
  id: totrans-297
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`remap_pfn_range()` maps physical memory (by means of kernel logical address)
    to a user space process. It is particularly useful for implementing the `mmap()`
    system call.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: After calling the `mmap()` system call on a file (whether it is a device file
    or not), the CPU will switch to privileged mode, and run the corresponding `file_operations.mmap()`
    kernel function, which in turn will call `remap_pfn_range()` . The kernel PTE
    of the mapped region will be derived, and given to the process, of course, with
    different protection flags. The process's VMA list is updated with a new VMA entry
    (with appropriate attributes) , which will use PTE to access the same memory.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, instead of wasting memory by copying, the kernel just duplicates the
    PTEs. However, kernel and user space PTE have different attributes. `remap_pfn_range()`
    has the following prototype:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: A successful call will return `0` , and a negative error code on failure. Most
    of the arguments for `remap_pfn_range()` are provided when the `mmap()` method
    is called.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '`vma` : This is the virtual memory area provided by the kernel in the case
    of a `file_operations.mmap()` call. It corresponds to the user process `vma` into
    which the mapping should be done.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`addr` : This is the user virtual address where VMA should start (`vma->vm_start`
    ), which will result in a mapping from a virtual address range between `addr`
    and `addr + size` .'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pfn` : This represents the page frame number of the kernel memory region to
    map. It corresponds to the physical address right-shifted by `PAGE_SHIFT` bits.
    The `vma` offset (offset into the object where the mapping must start) should
    be taken into account to produce the PFN. Since the `vm_pgoff` field of the VMA
    structure contains the offset value in the form of the number of pages, it is
    precisely what you need (with a `PAGE_SHIFT` left-shifting) to extract the offset
    in the form of bytes: `offset = vma->vm_pgoff << PAGE_SHIFT` ). Finally, `pfn
    = virt_to_phys(buffer + offset) >> PAGE_SHIFT` .'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`size` : This is the dimension, in bytes, of the area being remapped.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prot` : This represents the protection requested for the new VMA. The driver
    can mangle the default value, but should use the value found in `vma->vm_page_prot`
    as the skeleton using the OR operator, since some of its bits are already set
    by user space. Some of these flags are:'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`VM_IO` , which specifies a device''s memory mapped I/O'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`VM_DONTCOPY` , which tells the kernel not to copy this `vma` on fork'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`VM_DONTEXPAND` , which prevents `vma` from expanding with `mremap(2)`'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`VM_DONTDUMP` , prevents the `vma` from being included in the core dump'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One may need to modify this value in order to disable caching if using this
    with I/O memory (`vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);` ).
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: Using io_remap_pfn_range
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `remap_pfn_range()` function discussed does not apply anymore when it comes
    to mapping I/O memory to user space. In this case, the appropriate function is
    `io_remap_pfn_range()` , whose parameters are the same. The only thing that changes
    is where the PFN comes from. Its prototype looks like:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: There is no need to use `ioremap()` when at tempting to map I/O memory to user
    space. -`ioremap()` is intended for kernel purposes (mapping I/O memory into kernel
    address space), where as `io_remap_pfn_range` is for user space purposes.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: Just pass your real physical I/O address (downshifted by `PAGE_SHIFT` to produce
    a PFN) directly to `io_remap_pfn_range()` . Even if there are some architectures
    where `io_remap_pfn_range()` is defined as being `remap_pfn_range()` , there are
    other architectures where it is not the case. For portability reasons, you should
    only use `remap_pfn_range()` in situations where the PFN parameter points to RAM,
    and `io_remap_pfn_range()` in situations where `phys_addr` refers to I/O memory.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: The mmap file operation
  id: totrans-318
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kernel `mmap` function is part of `struct file_operations` structure, which
    is executed when the user executes the system call `mmap(2)` , used to maps physical
    memory into a user virtual address. The kernel translates any access to that mapped
    region of memory through the usual pointer dereferences into a file operation.
    It is even possible to map device physical memory directly to user space (see
    `/dev/mem` ). Essentially writing to memory becomes like writing into a file.
    It is just a more convenient way of calling `write()` .
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: Normally, user space processes cannot access device memory directly for security
    purposes. Therefore, user space processes use the `mmap()` system call to ask
    kernel to map the device into the virtual address space of the calling process.
    After the mapping, the user space process can write directly into the device memory
    through the returned address.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: 'The mmap system call is declared as follows:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The driver should have defined the mmap file operation (`file_operations.mmap`
    ) in order to support `mmap(2)` . From the kernel side, the mmap field in the
    driver''s file operation structure (`struct file_operations` structure) has the
    following prototype:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'where:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '`filp` is a pointer to the open device file for the driver that results from
    the translation of the fd parameter.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vma` is allocated and given as a parameter by the kernel. It is a pointer
    to the user process''s vma where the mapping should go. To understand how the
    kernel creates the new vma, let''s recall the `mmap(2)` system call''s prototype:'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The parameters of this function somehow affect some fields of the vma:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '`addr` : is the user space''s virtual address where the mapping should start.
    It has an impact on `vma>vm_start` . If `NULL` (the most portable way) was specified,
    automatically determinate the correct address.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`length` : This specifies the length of the mapping, and indirectly has an
    impact on `vma->vm_end` . Remember, the size of a `vma` is always a multiple of
    `PAGE_SIZE` . In other words, `PAGE_SIZE` is always the smallest size a `vma`
    can have. The kernel will always alter the size of the `vma` so that is is a multiple
    of `PAGE_SIZE` .'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '`prot` : This affects the permissions of the VMA, which the driver can find
    in `vma->vm_pro` . As discussed earlier, the driver can update these values, but
    not alter them.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`flags` : This determine the type of mapping that the driver can find in `vma->vm_flags`
    . The mapping can be private or shared.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`offset` : This specifies the offset within the mapped region, thus mangling
    the value of `vma->vm_pgoff` .'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing mmap in the kernel
  id: totrans-336
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since user space code cannot access kernel memory, the purpose of the `mmap()`
    function is to derive one or more protected kernel page table entries (which correspond
    to the memory to be mapped) and duplicate the user space page tables, remove the
    kernel flag protection, and set permission flags that will allow the user to access
    the same memory as the kernel without needing special privileges.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps to write a mmap file operation are as follows:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: 'Get the mapping offset and check whether it is beyond our buffer size or not:'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Check if the mapping size is bigger than our buffer size:'
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Get the PFN which corresponds to the PFN of the page where the `offset` position
    of our buffer falls:'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Set the appropriate flag, whether I/O memory is present or not:'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Disable caching using `vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot)`
    .
  id: totrans-346
  prefs:
  - PREF_UL
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Set the `VM_IO` flag: `vma->vm_flags |= VM_IO` .'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prevent the VMA from swapping out: `vma->vm_flags |= VM_DONTEXPAND | VM_DONTDUMP`
    . In kernel versions older than 3.7, you should use only the `VM_RESERVED` flag
    instead.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Call `remap_pfn_range` with the PFN calculated, the size, and the protection
    flags:'
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Pass your mmap function to the `struct file_operations` structure:'
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Linux caching system
  id: totrans-353
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Caching is the process by which frequently accessed or newly written data is
    fetched from, or written to a small and faster memory, called a **cache** .
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: Dirty memory is data-backed (for example, file-backed) memory whose content
    has been modified (typically in a cache) but not written back to the disk yet.
    The cached version of the data is newer than the on-disk version, meaning that
    both versions are out of sync. The mechanism by which cached data is written back
    on the disk (back store) is called **writeback** . We will eventually update the
    on-disk version, bringing the two in sync. *Clean memory* is file-backed memory
    in which the contents are in sync with the disk.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: Linux delays write operations in order to speed up the read process, and reduces
    disk wear leveling by writing data only when necessary. A typical example is the
    `dd` command. Its complete execution does not mean that the data is written to
    the target device; this is the reason why `dd` in most cases is chained to a `sync`
    command.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: What is a cache?
  id: totrans-357
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A cache is temporary, small, and fast memory used to keep copies of data from
    larger and often very slow memory, typically placed in systems where there is
    a working set of data accessed far more often than the rest (for example, hard
    drive, memory).
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: When the first read occurs, let us say a process requests some data from the
    large and slower disk, the requested data is returned to the process, and a copy
    of accessed data is tracked and cached as well. Any consequent read will fetch
    data from the cache. Any data modification will be applied in the cache, not on
    the main disk. Then, the cache region whose content has been modified and differs
    (is newer than) from the on-disk version will be tagged as **dirty** . When the
    cache runs full, and since cached data is tacked, new data begins to evict the
    data that has not been accessed and has been sitting idle for the longest, so
    that if it is needed again, it will have to be fetched from the large/slow storage
    again.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: CPU cache – memory caching
  id: totrans-360
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are three cache memories on the modern CPU, ordered by size and access
    speed:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: The **L1** cache that has the smallest amount of memory (often between 1k and
    64k) is directly accessible by the CPU in a single clock cycle, which makes it
    the fastest as well. Frequently used things are in L1 and remain in L1 until some
    other thing's usage becomes more frequent than the existing one and there is less
    space in L1\. If so, it is moved to a bigger space L2.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **L2** cache is the middle level, with a larger amount of memory (up to
    several megabytes) adjacent to the processor, which can be accessed in a small
    number of clock cycles. This applies when moving things from L2 to L3.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The **L3** cache, even slower than L1 and L2, may be two times faster than
    the main memory (RAM). Each core may have its own L1 and L2 cache; therefore,
    they all share the L3 cache. Size and speed are the main criteria that change
    between each cache level: L1 < L2 < L3\. Whereas original memory access may be
    100 ns for example, the L1 cache access can be 0.5 ns.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A real-life example is how a library may put several copies of the most popular
    titles on display for easy and fast access, but have a large-scale archive with
    a far greater collection available, at the inconvenience of having to wait for
    a librarian to go get it for you. The display cases would be analogous to a cache,
    and the archive would be the large, slow memory.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: The main issue that a CPU cache addresses is latency, which indirectly increases
    the throughput, because access to uncached memory may take a while.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: The Linux page cache – disk caching
  id: totrans-367
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The page cache, as its name suggests, is a cache of pages in RAM, containing
    chunks of recently accessed files. The RAM acts as a cache for pages that resides
    on the disk. In other words, it is the kernel cache of file contents. Cached data
    may be regular filesystem files, block device files, or memory-mapped files. Whenever
    a `read()` operation is invoked, the kernel first checks whether the data resides
    in the page cache, and immediately returns it if found. Otherwise, the data will
    be read from the disk.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: If a process needs to write data without any caching involved, it has to use
    the `O_SYNC` flag, which guarantees the `write()` command will not return before
    all data has been transferred to the disk, or the `O_DIRECT` , flag, which only
    guarantees that no caching will be used for data transfer. That says, `O_DIRECT`
    actually depends on filesystem used and is not recommended.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: Specialized caches (user space caching)
  id: totrans-370
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Web browser cache** : This stores frequently accessed web pages and images
    onto the disk, instead of fetching them from the web. Whereas the first access
    to online data may last for more than hundreds of milliseconds, the second access
    will fetch data from the cache (which is a disk in this case) in only 10 ms.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**libc or user-app cache** : Memory and disk cache implementations will try
    to guess what you need to use next, while browser caches keep a local copy in
    case you need to use it again.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why delay writing data to disk?
  id: totrans-373
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are essentially two reasons to that:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: Better usage of the disk characteristics; this is efficiency
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allows the application to continue immediately after a write; this is performance
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, delaying disk access and processing data only when it reaches a
    certain size may improve disk performance, and reduce wear leveling of eMMC (on
    embedded systems). Every chunk write is merged into a single and contiguous write
    operation. Additionally, written data is cached, allowing the process to return
    immediately so that any subsequent read will fetch the data from the cache, resulting
    in a more responsive program. Storage devices prefer a small number of large operations
    instead of several small operations.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: By reporting write operation on the permanent storage later, we can get rid
    of latency issues introduced by these disks, which are relatively slow.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: Write caching strategies
  id: totrans-379
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Depending on the cache strategy, several benefits may be enumerated:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: Reduced latency on data accessing, thus increasing application performance
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improved storage lifetime
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduced system work load
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduced risk of data loss
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Caching algorithms usually fall into one of the following three different strategies:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: The **write-through** **cache** , where any write operation will automatically
    update both the memory cache and the permanent storage. This strategy is preferred
    for applications where data loss cannot be tolerated, and applications that write
    and then frequently re-read data (since data is stored in the cache and results
    in low read latency).
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **write-around** **cache** , which is similar to write-through, with the
    difference that it immediately invalidates the cache (which is also costly for
    the system since any write results in automatic cache invalidation). The main
    consequence is that any subsequent read will fetch data from the disk, which is
    slow, thus increasing latency. It prevents the cache from being flooded with data
    that will not be subsequently read.
  id: totrans-387
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Linux employs the third and last strategy, called **write back cache** , which
    can write data to the cache every time a change occurs without updating the corresponding
    location in the main memory. Instead, the corresponding pages in the page cache
    are marked as **dirty** (this task is done by MMU using TLB) and added to a so-called
    list, maintained by the kernel. The data is written into the corresponding location
    in the permanent storage only at specified intervals or under certain conditions.
    When the data in the pages is up to date with the data in the page cache, the
    kernel removes the pages from the list, and they are not marked dirty.
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On Linux systems, you can find this from `/proc/meminfo` under `Dirty` :'
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: The flusher threads
  id: totrans-391
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The write back cache defers I/O data operations in the page cache. A set or
    kernel threads, called flusher threads, are responsible for that. Dirty page write
    back occurs when any one of the following situations is satisfied:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: When free memory falls below a specified threshold to regain memory consumed
    by dirty pages.
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When dirty data lasts until a specific period. The oldest data is written back
    to the disk to ensure that dirty data does not remain dirty indefinitely.
  id: totrans-394
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When a user process invokes the `sync()` and `fsync()` system calls. This is
    an on demand write back.
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Device-managed resources – Devres
  id: totrans-396
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Devres is a kernel facility helping the developer by automatically freeing the
    allocated resource in a driver. It simplifies errors handling in `init` /`probe`
    /`open` functions. With devres, each resource allocator has its managed version
    that will take care of resource release and freeing for you.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: This section heavily relies on the *Documentation/driver-model/devres.txt* file
    in the kernel source tree, which deals with devres API and lists supported functions
    along with their descriptions.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: The memory allocated with resource-managed functions is associated with the
    device. devres consists of a linked list of arbitrarily sized memory areas associated
    with a `struct device` . Each devers resource allocator inserts the allocated
    resource in the list. The resource remains available until it is manually freed
    by the code, when the device is detached from the system, or when the driver is
    unloaded. Each devres entry is associated with a `release` function. There are
    different ways to release a devres. No matter what, all devres entries are released
    on driver detach. On release, the associated release function is invoked and then
    the devres entry is freed.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the list of resources available for a driver:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: Memory for private data structures
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interrutps (IRQs)
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory region allocation (`request_mem_region()` )
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I/O mapping of memory regions (`ioremap()` )
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Buffer memory (possibly with DMA mapping)
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Different framework data structures: Clocks, GPIOs, PWMs, USB phy, regulators,
    DMA, and so on'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Almost every function discussed in this chapter has its managed version. In
    the majority of cases, the name given to the managed version of a function is
    obtained by prefixing the original function name with `devm` . For example, `devm_kzalloc()`
    is the managed version of `kzalloc()` . Additionally, parameters remain unchanged,
    but are shifted to the right, since the first parameter is the struct device for
    which the resource is allocated. There is an exception for functions for which
    the non-managed version is already given a struct device in its parameters:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: When the device is detached from the system or the driver for the device is
    unloaded, that memory is freed automatically. It is possible to free the memory
    with `devm_kfree()` if it's no longer needed.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: 'The old way:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The right way:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-413
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Summary
  id: totrans-414
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter is one of the most important chapters. It demystifies memory management
    and allocation (how and where) in the kernel. Every memory aspect is discussed
    and detailed, as well as dvres is also explained. The caching mechanism is briefly
    discussed in order to give an overview of what goes on under the hood during I/O
    operations. It is a strong base from which introduce and understand the next chapter,
    which deals with DMA.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
