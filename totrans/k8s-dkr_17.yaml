- en: '*Chapter 14*: Provisioning a Platform'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Every chapter in this book, up until this point, has focused on the infrastructure
    of your cluster. We have explored how to deploy Kubernetes, how to secure it,
    and how to monitor it. What we haven't talked about is how to deploy applications.
  prefs: []
  type: TYPE_NORMAL
- en: In this, our final chapter, we're going to work on building an application deployment
    platform using what we've learned about Kubernetes. We're going to build our platform
    based on some common enterprise requirements. Where we can't directly implement
    a requirement, because building a platform on Kubernetes can fill its own book,
    we'll call it out and provide some insights.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Designing a pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing our cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying GitLab
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying Tekton
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying ArgoCD
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automating project onboarding using OpenUnison
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To perform the exercises in this chapter, you will need a clean KinD cluster
    with a minimum of 8 GB of memory, 75 GB storage, and 4 CPUs. The system we will
    build is minimalist but still requires considerable horsepower to run.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can access the code for this chapter at the following GitHub repository:
    [https://github.com/PacktPublishing/Kubernetes-and-Docker-The-Complete-Guide](https://github.com/PacktPublishing/Kubernetes-and-Docker-The-Complete-Guide).'
  prefs: []
  type: TYPE_NORMAL
- en: Designing a pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The term "pipeline" is used extensively in the Kubernetes and DevOps world.
    Very simply, a pipeline is a process, usually automated, that takes code and gets
    it running. This usually involves the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.1 – A simple pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_14.1_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.1 – A simple pipeline
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s quickly run through the steps involved in this process:'
  prefs: []
  type: TYPE_NORMAL
- en: Storing the source code in a central repository, usually Git
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When code is committed, building it and generating artifacts, usually a container
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Telling the platform – in this case, Kubernetes – to roll out the new containers
    and shut down the old ones
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This is about as basic as a pipeline can get and isn''t of much use in most
    deployments. In addition to building our code and deploying it, we want to make
    sure we scan containers for known vulnerabilities. We may also want to run our
    containers through some automated testing before going into production. In enterprise
    deployments, there''s often a compliance requirement where someone takes responsibility
    for the move to production as well. Taking this into account, the pipeline starts
    to get more complex:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.2 – Pipeline with common enterprise requirements'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_14.2_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.2 – Pipeline with common enterprise requirements
  prefs: []
  type: TYPE_NORMAL
- en: 'The pipeline has added some extra steps, but it''s still linear with one starting
    point, a commit. This is also very simplistic and unrealistic. The base containers
    and libraries your applications are built on are constantly being updated as new
    **Common Vulnerabilities and Exposures** (**CVEs**), a common way to catalog and
    identify security vulnerabilities, are discovered and patched. In addition to
    having developers that are updating application code for new requirements, you
    will want to have a system in place that scans both the code and the base containers
    for available updates. These scanners watch your base containers and can do something
    to trigger a build once a new base container is ready. While the scanners could
    call an API to trigger a pipeline, your pipeline is already waiting on your Git
    repository to do something, so it would be better to simply add a commit or a
    pull request to your Git repository to trigger the pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.3 – Pipeline with scanners integrated'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_14.3_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.3 – Pipeline with scanners integrated
  prefs: []
  type: TYPE_NORMAL
- en: This means your application code is tracked and your operational updates are
    tracked in Git. Git is now the source of truth for not only what your application
    code is but also operations updates. When it's time to go through your audits,
    you have a ready-made change log! If your policies require you to enter changes
    into a change management system, simply export the changes from Git.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have focused on our application code and just put **Rollout** at
    the end of our pipeline. The final rollout step usually means patching a **Deployment**
    or **StatefulSet** with our newly built container, letting Kubernetes do the work
    of spinning up new **Pods** and scaling down the old ones. This could be done
    with a simple API call, but how are we tracking and auditing that change? What's
    the source of truth?
  prefs: []
  type: TYPE_NORMAL
- en: 'Our application in Kubernetes is defined as a series of objects stored in **etcd**
    that are generally represented as code using YAML files. Why not store those files
    in a Git repository too? This gives us the same benefits as storing our application
    code in Git. We have a single source of truth for both the application source
    and the operations of our application! Now, our pipeline involves some more steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.4 – GitOps pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_14.4_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.4 – GitOps pipeline
  prefs: []
  type: TYPE_NORMAL
- en: In this diagram, our rollout updates a Git repository with our application's
    Kubernetes YAML. A controller inside our cluster watches for updates to Git and
    when it sees them, gets the cluster in sync with what's in Git. It can also detect
    drift in our cluster and bring it back to alignment with our source of truth.
  prefs: []
  type: TYPE_NORMAL
- en: This focus on Git is called **GitOps**. The idea is that all of the work of
    an application is done via code, not directly via APIs. How strict you are with
    this idea can dictate what your platform looks like. Next, we'll explore how opinions
    can shape your platform.
  prefs: []
  type: TYPE_NORMAL
- en: Opinionated platforms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Kelsey Hightower, a developer advocate for Google and leader in the Kubernetes
    world, once said: "Kubernetes is a platform for building platforms. It''s a better
    place to start; not the endgame." When you look at the landscape of vendors and
    projects building Kubernetes-based products, they all have their own opinions
    of how systems should be built. As an example, Red Hat''s **OpenShift Container
    Platform** (**OCP**) wants to be a one-stop-shop for multi-tenant enterprise deployment.
    It builds in a great deal of the pipeline we discussed. You define a pipeline
    that is triggered by a commit, which builds a container and pushes it into its
    own internal registry that then triggers a rollout of the new container. Namespaces
    are the boundaries of tenants. Canonical is a minimalist distribution that doesn''t
    include any pipeline components. Managed vendors such as Amazon, Azure, and Google
    provide the building blocks of a cluster and the hosted build tools of a pipeline
    but leave it to you to build out your platform.'
  prefs: []
  type: TYPE_NORMAL
- en: There is no correct answer as to which platform to use. Each is opinionated
    and the right one for your deployment will depend on your own requirements. Depending
    on the size of your enterprise, it wouldn't be surprising to see more than one
    platform deployed!
  prefs: []
  type: TYPE_NORMAL
- en: Having looked at the idea of opinionated platforms, let's explore the security
    impacts of building a pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Securing your pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Depending on your starting point, this can get complex quickly. How much of
    your pipeline is one integrated system, or could it be described using a colorful
    American colloquialism involving duct tape? Even in platforms where all the components
    are there, tying them together can often mean building a complex system. Most
    of the systems that are part of your pipeline will have a visual component. Usually,
    the visual component is a dashboard. Users and developers may need access to that
    dashboard. You don't want to maintain separate accounts for all those systems,
    do you? You'll want to have one login point and portal for all the components
    of your pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: After determining how to authenticate the users who use these systems, the next
    question is how to automate the rollout. Each component of your pipeline requires
    configuration. It can be as simple as an object that gets created via an API call
    or as complex as tying together a Git repo and build process with SSH keys to
    automate security. In such a complex environment, manually creating pipeline infrastructure
    will lead to security gaps. It will also lead to impossible-to-manage systems.
    Automating the process and providing consistency will help you both secure your
    infrastructure and keep it maintainable.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, it's important to understand the implications of GitOps on our cluster
    from a security standpoint. We discussed authenticating administrators and developers
    to use the Kubernetes API and authorizing access to different APIs in [*Chapter
    7*](B15514_07_Final_ASB_ePub.xhtml#_idTextAnchor203), *Integrating Authentication
    into Your Cluster*, and [*Chapter 8*](B15514_08_Final_ASB_ePub.xhtml#_idTextAnchor228)*,
    RBAC Policies Using Active Directory Users*. What is the impact if someone can
    check in a **RoleBinding** that assigns them the **admin** **ClusterRole** for
    a namespace and a GitOps controller automatically pushes it through to the cluster?
    As you design your platform, consider how developers and administrators will want
    to interact with it. It's tempting to say "Let everyone interact with their application's
    Git registry," but that means putting the burden on you as the cluster owner for
    many requests. As we discussed in [*Chapter 8*](B15514_08_Final_ASB_ePub.xhtml#_idTextAnchor228),
    *RBAC Policies Using Active Directory*, this could make your team the bottleneck
    in an enterprise. Understanding your customers, in this case, is important in
    knowing how they want to interact with their operations even if it's not how you
    intended.
  prefs: []
  type: TYPE_NORMAL
- en: Having touched on some of the security aspects of GitOps and a pipeline, let's
    explore the requirements for a typical pipeline and how we will build it.
  prefs: []
  type: TYPE_NORMAL
- en: Building our platform's requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Kubernetes deployments, especially in enterprise settings, will often have
    the following basic requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Development and test environments**: At least two clusters to test the impacts
    of changes on the cluster level to applications'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Developer sandbox**: A place where developers can build containers and test
    them without worrying about impacts on shared namespaces'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Source control and issue tracking**: A place to store code and track open
    tasks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to these basic requirements, enterprises will often have additional
    requirements, such as regular access reviews, limiting access based on policy,
    and workflows that assign responsibility for actions that could impact a shared
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our platform, we want to encompass as many of these requirements as possible.
    To better automate deployments onto our platform, we''re going to define each
    application as having the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A development namespace**: Developers are administrators.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A production namespace**: Developers are viewers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A source control project**: Developers can fork.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A build process**: Triggered by updates to Git.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A deploy process**: Triggered by updates to Git.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, we want our developers to have their own sandbox so that each user
    will get their own namespace for development.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: In a real deployment, you will want to separate your development and production
    environments into separate clusters. This makes it much easier to test cluster-wide
    operations, such as upgrades, without impacting running applications. We're doing
    everything in one cluster to make it easier for you to set up on your own.
  prefs: []
  type: TYPE_NORMAL
- en: 'To provide access to each application, we will define three roles:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Owners**: Users that are application owners can approve access for other
    roles inside their application. This role is assigned to the application requestor
    and can be assigned by application owners. Owners are also responsible for pushing
    changes into development and production.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Developers**: These are users that will have access to an application''s
    source control and can administer the application''s development namespace. They
    can view objects in the production namespace but can''t edit anything. This role
    can be requested by any users and is approved by an application owner.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Operations**: These users have the capabilities as developers, but can also
    make changes to the production namespace as needed. This role can be requested
    by any user and is approved by the application owner.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will also create some environment-wide roles:'
  prefs: []
  type: TYPE_NORMAL
- en: '**System approvers**: Users with this role can approve access to any system-wide
    roles.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cluster administrators**: This role is specifically for managing our cluster
    and the applications that comprise our pipeline. It can be requested by anyone
    and must be approved by a member of the system approvers role.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Developers**: Anyone who logs in gets their own namespace for development.
    These namespaces cannot be requested for access by other users. These namespaces
    are not directly connected to any CI/CD infrastructure or Git repositories.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even with our very simple platform, we have six roles that need to be mapped
    to the applications that make up our pipeline. Each application has its own authentication
    and authorization processes that these roles will need to be mapped to. This is
    just one example of why automation is so important to the security of your clusters.
    Provisioning this access manually based on email requests can become unmanageable
    quickly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The workflow that developers are expected to go through with an application
    will line up with the GitOps flow we designed previously:'
  prefs: []
  type: TYPE_NORMAL
- en: Application owners will request an application be created. Once approved, a
    Git repository will be created for application code, pipeline build manifests,
    and Kubernetes manifests. Development and production namespaces will be created
    as well with appropriate **RoleBinding** objects. Groups will be created that
    reflect the roles for each application, with approval for access to those groups
    delegated to the application owner.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developers and operations staff are granted access to the application by either
    requesting it or having it provided directly by an application owner. Once granted
    access, updates are expected in both the developer's sandbox and the development
    namespace. Updates are made in a user's fork for the Git repository, with pull
    requests used to merge code into the main repositories that drive automation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All builds are controlled via "scripts" in the application's source control.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All artifacts are published to a centralized container registry.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All production updates must be approved by application owners.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This basic workflow doesn't include typical components of a workflow, such as
    code and container scans, periodic access recertifications, or requirements for
    privileged access. The topic of this chapter can easily be a complete book on
    its own. The goal isn't to build a complete enterprise platform but to give you
    a starting point for building and designing your own system.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing our technology stack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous parts of this section, we talked about pipelines in a generic
    way. Now, let's get into the specifics of what technology is needed in our pipeline.
    We identified earlier that every application has application source code and Kubernetes
    manifest definitions. It also has to build containers. There needs to be a way
    to watch for changes to Git and update our cluster. Finally, we need an automation
    platform so that all these components work together.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on our requirements for our platform, we want technology that has the
    following features:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Open source**: We don''t want you to buy anything just for this book!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**API-driven**: We need to be able to provision components and access in an
    automated way.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Has a visual component that supports external authentication**: This book
    focuses on enterprise, and everyone in the enterprise loves their GUIs. Just not
    having different credentials for each application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Supported on Kubernetes**: This is a book on Kubernetes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To meet these requirements, we''re going to deploy the following components
    to our cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Git Registry – GitLab**: GitLab is a powerful system that provides a great
    UI and experience for working with Git that supports external authentication (that
    is, **Single Sign-On (SSO)**. It has integrated issue management and an extensive
    API. It also has a Helm chart that we have tailored for the book to run a minimal
    install.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automated Builds – Tekton**: Originally the build portion of the Knative
    project for Kubernetes function-as-a-service deployments, Tekton was spun off
    into its own project to provide build services for generic applications. It runs
    in Kubernetes with all interactions being via the Kubernetes API. There''s an
    early stage dashboard too!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Container Registry – simple Docker registry**: There are many very capable
    open source registries. Since this deployment will get complex quickly, we decided
    just to use the registry provided by Docker. There won''t be any security on it,
    so don''t use it in production!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GitOps – ArgoCD**: ArgoCD is a collaboration between Intuit and Weaveworks
    to build a feature-rich GitOps platform. It''s Kubernetes native, has its own
    API, and stores its objects as Kubernetes custom resources, making it easier to
    automate. Its UI and CLI tools both integrate with SSO using OpenID Connect.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Access, authentication, and automation – OpenUnison**: We''ll continue to
    use OpenUnison for authentication into our cluster. We''re also going to integrate
    the UI components of our technology stack as well to provide a single portal for
    our platform. Finally, we''ll use OpenUnison''s workflows to manage access to
    each system based on our role structure and provision the objects needed for everything
    to work together. Access will be provided via OpenUnison''s self-service portal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading through this technology stack, you might ask "Why didn't you choose
    *XYZ*?" The Kubernetes ecosystem is diverse with no shortage of great projects
    and products for your cluster. This is by no means a definitive stack, nor is
    it even a "recommended" stack. It's a collection of applications that meets our
    requirements and lets us focus on the processes being implemented, rather than
    learning a specific technology.
  prefs: []
  type: TYPE_NORMAL
- en: You might also find that there's quite a bit of overlap between even the tools
    in this stack. For instance, GitLab has GitOps capabilities and its own build
    system, but we chose not to use them for this chapter. We did that so that you
    can see how to tie different systems together to build a platform. Your platform
    may use GitHub's SaaS solution for source control but run builds internally and
    combine with Amazon's container registry. We wanted you to see how these systems
    can be connected to build a platform instead of focusing on specific tools.
  prefs: []
  type: TYPE_NORMAL
- en: This section was a very deep exploration of the theory behind pipeline design
    and looking at common requirements for building a Kubernetes-based platform. We
    identified technology components that can implement those requirements and why
    we chose them. With this knowledge in hand, it's time to build!
  prefs: []
  type: TYPE_NORMAL
- en: Preparing our cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we begin deploying our technology stack, we need to do a couple of things.
    I recommend starting with a fresh cluster. If you're using the KinD cluster from
    this book, start with a new cluster. We're deploying several components that need
    to be integrated and it will be simpler and easier to start fresh rather than
    potential struggling with previous configurations. Before we start deploying the
    applications that will make up our stack, we're going to deploy JetStack's cert-manager
    to automate certificate issuing, a simple container registry, and OpenUnison for
    authentication and automation.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying cert-manager
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: JetStack, a Kubernetes-focused consulting company, created a project called
    **cert-manager** to make it easier to automate the creation and renewal of certificates.
    This project works by letting you define issuers using Kubernetes custom resources
    and then using annotations on **Ingress** objects to generate certificates using
    those issuers. The end result is a cluster running with properly managed and rotated
    certificates without generating a single **certificate signing request** (**CSR**)
    or worrying about expiration!
  prefs: []
  type: TYPE_NORMAL
- en: The **cert-manager** project is most often mentioned with *Let's Encrypt* (https://letsencrypt.org/)
    to automate the publishing of certificates that have been signed by a commercially
    recognized certificate authority for free (as in beer). This is possible because
    *Let's Encrypt* automates the process. The certificates are only good for 90 days
    and the entire process is API-driven. In order to drive this automation, you must
    have some way of letting *Let's Encrypt* verify ownership of the domain you are
    trying to get a certificate for. Throughout this book, we have used **nip.io**
    to simulate DNS. If you have a DNS service that you can use and is supported by
    **cert-manager**, such as Amazon's Route 53, then this is a great solution.
  prefs: []
  type: TYPE_NORMAL
- en: Since we're using **nip.io**, we will deploy **cert-manager** with a self-signed
    certificate authority. This gives us the benefit of having a certificate authority
    that can quickly generate certificates without having to worry about domain validation.
    We will then instruct our workstation to trust this certificate as well as the
    applications we deploy so that everything is secured using properly built certificates.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Using a self-signed certificate authority is a common practice for most enterprises
    for internal deployments. This avoids having to deal with potential validation
    issues where a commercially signed certificate won't provide much value. Most
    enterprises are able to distribute an internal certificate authority's certificates
    via their Active Directory infrastructure. Chances are your enterprise has a way
    to request either an internal certificate or a wildcard that could be used too.
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps to deploy **cert-manager** are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'From your cluster, deploy the **cert-manager** manifests:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v0.16.1/cert-manager.yaml**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the Pods are running in the **cert-manager** namespace, create a self-signed
    certificate that we''ll use as our certificate authority. In the **chapter14/shell**
    directory of the Git repository for this book is a script called **makeca.sh**
    that will generate this certificate for you:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ cd Kubernetes-and-Docker-The-Complete-Guide/chapter14/shell/**'
  prefs: []
  type: TYPE_NORMAL
- en: '**$ sh ./makeca.sh**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Generating RSA private key, 2048 bit long modulus (2 primes)**'
  prefs: []
  type: TYPE_NORMAL
- en: '**.............................................................................................................................................+++++**'
  prefs: []
  type: TYPE_NORMAL
- en: '**....................+++++**'
  prefs: []
  type: TYPE_NORMAL
- en: '**e is 65537 (0x010001)**'
  prefs: []
  type: TYPE_NORMAL
- en: 'There is now an SSL directory with a certificate and a key. The next step is
    to create a secret from these files that will become our certificate authority:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ cd ssl/**'
  prefs: []
  type: TYPE_NORMAL
- en: '**$ kubectl create secret tls ca-key-pair --key=./tls.key --cert=./tls.crt
    -n cert-manager**'
  prefs: []
  type: TYPE_NORMAL
- en: '**secret/ca-key-pair created**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, create the **ClusterIssuer** object so that all of our **Ingress** objects
    can have properly minted certificates:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ cd ../../yaml/**'
  prefs: []
  type: TYPE_NORMAL
- en: '**$ kubectl create -f ./certmanager-ca.yaml**'
  prefs: []
  type: TYPE_NORMAL
- en: '**clusterissuer.cert-manager.io/ca-issuer created**'
  prefs: []
  type: TYPE_NORMAL
- en: 'With **ClusterIssuer** created, any **Ingress** object with the **cert-manager.io/cluster-issuer:
    "ca-issuer"** annotation will have a certificate signed by our authority created
    for them. One of the components we will be using for this is our container registry.
    Kubernetes uses Docker''s underlying mechanisms for pulling containers, and KinD
    will not pull images from registries running without TLS or using an untrusted
    certificate. To get around this issue, we need to import our certificate into
    both our worker and nodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ cd ~/**'
  prefs: []
  type: TYPE_NORMAL
- en: '**$ kubectl get secret ca-key-pair -n cert-manager -o json | jq -r ''.data["tls.crt"]''
    | base64 -d > internal-ca.crt**'
  prefs: []
  type: TYPE_NORMAL
- en: '**$ docker cp internal-ca.crt cluster01-worker:/usr/local/share/ca-certificates/internal-ca.crt**'
  prefs: []
  type: TYPE_NORMAL
- en: '**$ docker exec -ti cluster01-worker update-ca-certificates**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Updating certificates in /etc/ssl/certs...**'
  prefs: []
  type: TYPE_NORMAL
- en: '**1 added, 0 removed; done.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Running hooks in /etc/ca-certificates/update.d...**'
  prefs: []
  type: TYPE_NORMAL
- en: '**done.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**$ docker restart cluster01-worker**'
  prefs: []
  type: TYPE_NORMAL
- en: '**$ docker cp internal-ca.crt cluster01-control-plane:/usr/local/share/ca-certificates/internal-ca.crt**'
  prefs: []
  type: TYPE_NORMAL
- en: '**$ docker exec -ti cluster01-control-plane update-ca-certificates**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Updating certificates in /etc/ssl/certs...**'
  prefs: []
  type: TYPE_NORMAL
- en: '**1 added, 0 removed; done.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Running hooks in /etc/ca-certificates/update.d...**'
  prefs: []
  type: TYPE_NORMAL
- en: '**done.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**$ docker restart cluster01-control-plane**'
  prefs: []
  type: TYPE_NORMAL
- en: The first command extracts the certificate from the secret we created to host
    the certificate. The next set of commands copies the certificate to each container,
    instructs the container to trust it, and finally, restarts the container. Once
    your containers are restarted, wait for all the Pods to come back; it could take
    a few minutes.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Now would be a good time to download **internal-ca.crt**; install it onto your
    local workstation and potentially into your browser of choice. Different operating
    systems and browsers do this differently, so check the appropriate documentation
    on how to do this. Trusting this certificate will make things much easier when
    interacting with applications, pushing containers, and using command-line tools.
  prefs: []
  type: TYPE_NORMAL
- en: With **cert-manager** ready to issue certificates and both your cluster and
    your workstation trusting those certificates, the next step is to deploy a container
    registry.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the Docker container registry
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Docker, Inc. provides a simple registry. There is no security on this registry,
    so it is most certainly not a good option for production use. The **chapter14/yaml/docker-registry.yaml**
    file will deploy the registry for us and create an **Ingress** object. Before
    deploying, edit this file, changing all instances of **192-168-2-140** to a dash
    representation of your cluster''s IP address. For instance, my cluster is running
    on **192.168.2.114**, so I will replace **192-168-2-140** with **192-168-2-114**.
    Then, run **kubectl create** on the manifest to create the registry:'
  prefs: []
  type: TYPE_NORMAL
- en: $ kubectl create -f ./docker-registry.yaml
  prefs: []
  type: TYPE_NORMAL
- en: namespace/docker-registry created
  prefs: []
  type: TYPE_NORMAL
- en: statefulset.apps/docker-registry created
  prefs: []
  type: TYPE_NORMAL
- en: service/docker-registry created
  prefs: []
  type: TYPE_NORMAL
- en: ingress.extensions/docker-registry created
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the registry is running, you can try accessing it from your browser:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.5 – Accessing the container registry in a browser'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_14.5_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.5 – Accessing the container registry in a browser
  prefs: []
  type: TYPE_NORMAL
- en: You won't see much since the registry has no web UI, but you also shouldn't
    get a certificate error. That's because we deployed **cert-manager** and are issuing
    signed certificates! With our registry running, the last component to deploy is
    OpenUnison.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying OpenUnison
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [*Chapter 7*](B15514_07_Final_ASB_ePub.xhtml#_idTextAnchor203), *Integrating
    Authentication into Your Cluster*, we introduced OpenUnison to authenticate access
    to our KinD deployment. OpenUnison comes in two flavors. The first, which we have
    already deployed, is a login portal that lets us authenticate using a central
    source and pass group information to our RBAC policies. The second is an automation
    portal that we'll use as the basis for integrating the systems that will manage
    our pipeline. This portal will also give us a central UI for requesting projects
    to be created and managing access to our project's systems.
  prefs: []
  type: TYPE_NORMAL
- en: We defined that each project we deploy will have three "roles" that will span
    several systems. Will your enterprise let you create and manage groups for every
    project we create? Some might, but Active Directory is a critical component in
    most enterprises, and write access can be difficult to get. It's unlikely that
    the people who run your Active Directory are the same people who you report to
    when managing your cluster, complicating your ability to get an area of Active
    Directory that you have administrative rights in. The OpenUnison automation portal
    lets you manage access with local groups that can be easily queried, just like
    with Active Directory, but you have control to manage them. We'll still authenticate
    against our central SAML provider, though.
  prefs: []
  type: TYPE_NORMAL
- en: 'To facilitate OpenUnison''s automation capabilities, we need to deploy a database
    to store persistent data and an SMTP server to notify users when they have open
    requests or when requests have been completed. For the database, we''ll deploy
    the open source MariaDB. For an **Simple Mail Transfer Protocol** (**SMTP**) (email)
    server, most enterprises have very strict rules about sending emails. We don''t
    want to have to worry about getting email set up for notifications, so we''ll
    run a "black hole" email service that just disregards all SMTP requests:'
  prefs: []
  type: TYPE_NORMAL
- en: First, run the **chapter14/yaml/mariadb.yaml** manifest from the book's GitHub
    repository. No changes need to be made.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, deploy the SMTP black hole:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ kubectl create ns blackhole**'
  prefs: []
  type: TYPE_NORMAL
- en: '**namespace/blackhole created**'
  prefs: []
  type: TYPE_NORMAL
- en: '**$ kubectl create deployment blackhole --image=tremolosecurity/smtp-blackhole
    -n blackhole**'
  prefs: []
  type: TYPE_NORMAL
- en: '**deployment.apps/blackhole created**'
  prefs: []
  type: TYPE_NORMAL
- en: '**$ kubectl expose deployment/blackhole --type=ClusterIP --port 1025 --target-port=1025
    -n blackhole**'
  prefs: []
  type: TYPE_NORMAL
- en: '**service/blackhole exposed**'
  prefs: []
  type: TYPE_NORMAL
- en: With MariaDB and our SMTP service deployed, we're able to deploy OpenUnison.
    Follow *steps 1–5* in the *Deploying OpenUnison* section of [*Chapter 7*](B15514_07_Final_ASB_ePub.xhtml#_idTextAnchor203),
    *Integrating Authentication into Your Cluster*, to deploy the OpenUnison operator
    and Kubernetes dashboard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, create a **Secret** to store credentials for accessing MariaDB and the
    SMTP service. We hardcoded passwords into our deployment for MariaDB for simplicity''s
    sake, so make sure to generate long, random passwords for your production database
    account! Create the following **Secret** in your cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'apiVersion: v1'
  prefs: []
  type: TYPE_NORMAL
- en: 'type: Opaque'
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: orchestra-secrets-source'
  prefs: []
  type: TYPE_NORMAL
- en: 'namespace: openunison'
  prefs: []
  type: TYPE_NORMAL
- en: 'data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'K8S_DB_SECRET: aW0gYSBzZWNyZXQ='
  prefs: []
  type: TYPE_NORMAL
- en: 'SMTP_PASSWORD: ""'
  prefs: []
  type: TYPE_NORMAL
- en: 'OU_JDBC_PASSWORD: c3RhcnR0MTIz'
  prefs: []
  type: TYPE_NORMAL
- en: 'unisonKeystorePassword: aW0gYSBzZWNyZXQ='
  prefs: []
  type: TYPE_NORMAL
- en: 'kind: Secret'
  prefs: []
  type: TYPE_NORMAL
- en: We're going to reuse the Helm values we used in *step 2* in the *Configuring
    your cluster for impersonation* section of [*Chapter 7*](B15514_07_Final_ASB_ePub.xhtml#_idTextAnchor203),
    *Integrating Authentication into Your Cluster*, with three changes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: First, change the image from **docker.io/tremolosecurity/openunison-k8s-login-saml2:latest**
    to **docker.io/tremolosecurity/openunison-k8s-saml2:latest**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, Base64-encode your **internal-ca.crt** file into a single line and add
    it to the **trusted_certs** section of **values.yaml**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ base64 -w 0 < internal-ca.crt**'
  prefs: []
  type: TYPE_NORMAL
- en: '**LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0 tCk1JSUREVENDQWZXZ0F3SUJ…**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Add SMTP and database sections. The updates to **values.yaml** will look as
    follows. I removed most of the unchanged portions to save space:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'trusted_certs:'
  prefs: []
  type: TYPE_NORMAL
- en: '- name: internal-ca'
  prefs: []
  type: TYPE_NORMAL
- en: 'pem_b64: LS0tLS1CRUdJTiB…'
  prefs: []
  type: TYPE_NORMAL
- en: 'saml:'
  prefs: []
  type: TYPE_NORMAL
- en: 'idp_url: https://portal.apps.tremolo.io/idp-test/metadata/dfbe4040-cd32-470e-a9b6-809c8f857c40'
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata_xml_b64: ""'
  prefs: []
  type: TYPE_NORMAL
- en: 'database:'
  prefs: []
  type: TYPE_NORMAL
- en: 'hibernate_dialect: org.hibernate.dialect.MySQL5InnoDBDialect'
  prefs: []
  type: TYPE_NORMAL
- en: 'quartz_dialect: org.quartz.impl.jdbcjobstore.StdJDBCDelegate'
  prefs: []
  type: TYPE_NORMAL
- en: 'driver: com.mysql.jdbc.Driver'
  prefs: []
  type: TYPE_NORMAL
- en: 'url: jdbc:mysql://mariadb.mariadb.svc.cluster.local:3306/unison'
  prefs: []
  type: TYPE_NORMAL
- en: 'user: unison'
  prefs: []
  type: TYPE_NORMAL
- en: 'validation: SELECT 1'
  prefs: []
  type: TYPE_NORMAL
- en: 'smtp:'
  prefs: []
  type: TYPE_NORMAL
- en: 'host: blackhole.blackhole.svc.cluster.local'
  prefs: []
  type: TYPE_NORMAL
- en: 'port: 1025'
  prefs: []
  type: TYPE_NORMAL
- en: 'user: none'
  prefs: []
  type: TYPE_NORMAL
- en: 'from: donotreply@domain.com'
  prefs: []
  type: TYPE_NORMAL
- en: 'tls: false'
  prefs: []
  type: TYPE_NORMAL
- en: 'Deploy OpenUnison using the Helm chart:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ helm install orchestra tremolo/openunison-k8s-saml2 --namespace openunison
    -f ./openunison-values.yaml**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once OpenUnison is deployed, edit the **orchestra** OpenUnison object to remove
    the **unison-ca** key. Remove the block that looks like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '- create_data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'ca_cert: true'
  prefs: []
  type: TYPE_NORMAL
- en: 'key_size: 2048'
  prefs: []
  type: TYPE_NORMAL
- en: 'server_name: k8sou.apps.192-168-2-114.nip.io'
  prefs: []
  type: TYPE_NORMAL
- en: 'sign_by_k8s_ca: false'
  prefs: []
  type: TYPE_NORMAL
- en: 'subject_alternative_names:'
  prefs: []
  type: TYPE_NORMAL
- en: '- k8sdb.apps.192-168-2-114.nip.io'
  prefs: []
  type: TYPE_NORMAL
- en: '- k8sapi.apps.192-168-2-114.nip.io'
  prefs: []
  type: TYPE_NORMAL
- en: 'import_into_ks: certificate'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: unison-ca'
  prefs: []
  type: TYPE_NORMAL
- en: 'tls_secret_name: ou-tls-certificate'
  prefs: []
  type: TYPE_NORMAL
- en: 'Delete the **ou-tls-certificate** **Secret**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ kubectl delete secret ou-tls-certificate -n openunison**'
  prefs: []
  type: TYPE_NORMAL
- en: '**secret "ou-tls-certificate" deleted**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Edit the **openunison** **Ingress** object, adding **cert-manager.io/cluster-issuer:
    ca-issuer** to the list of **annotations**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Complete the SSO integration with the testing identity provider using *steps
    4–6* from the *Configuring your cluster for impersonation* section of [*Chapter
    7*](B15514_07_Final_ASB_ePub.xhtml#_idTextAnchor203), *Integrating Authentication
    into Your Cluster*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Log in to OpenUnison, then log out.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The OpenUnison automation portal doesn''t do anything with the groups from
    the testing identity provider. In order to become a cluster administrator, you
    must be "bootstrapped" into the environment''s groups:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ kubectl exec -ti mariadb-0 -n mariadb -- mysql -u \**'
  prefs: []
  type: TYPE_NORMAL
- en: '**  unison --password=''startt123'' \**'
  prefs: []
  type: TYPE_NORMAL
- en: '**  -e "insert into userGroups (userId,groupId) values (2,1);" \**'
  prefs: []
  type: TYPE_NORMAL
- en: '**  unison**'
  prefs: []
  type: TYPE_NORMAL
- en: '**$ kubectl exec -ti mariadb-0 -n mariadb -- mysql -u \**'
  prefs: []
  type: TYPE_NORMAL
- en: '**  unison --password=''startt123'' \**'
  prefs: []
  type: TYPE_NORMAL
- en: '**  -e "insert into userGroups (userId,groupId) values (2,2);" \  unison**'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, log back in. You will be a global administrator and a cluster administrator
    for your cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With OpenUnison deployed, you can now remotely administer your cluster. Depending
    on how you are accessing your cluster, it may be easier to use your workstation
    to directly manage your cluster for the rest of the steps in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: You'll notice that there are different "badges" in OpenUnison now. In addition
    to getting a token or accessing the dashboard, you can request a new namespace
    to be created or access the ActiveMQ dashboard. You'll also see that the title
    bar has additional options, such as **Request Access**. OpenUnison will become
    our self-service portal for deploying our pipelines without having to manually
    create objects in our applications or cluster. We're not going to go into these
    in detail until we talk about using OpenUnison to automate the deployment of our
    pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: With your cluster prepared, the next step is to deploy the components for our
    pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying GitLab
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When building a GitOps pipeline, one of the most important components is a Git
    repository. GitLab has many components besides just Git, including a UI for navigating
    code, a web-based **integrated development environment** (**IDE**) for editing
    code, and a robust identity implementation to manage access to projects in a multi-tenant
    environment. This makes it a great solution for our platform since we can map
    our "roles" to GitLab groups.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we're going to deploy GitLab into our cluster and create two
    simple repositories that we'll use later when we deploy Tekton and ArgoCD. We'll
    focus on the automation steps when we revisit OpenUnison to automate our pipeline
    deployments.
  prefs: []
  type: TYPE_NORMAL
- en: 'GitLab deploys with a Helm chart. For this book, we built a custom **values**
    file to run a minimal install. While GitLab comes with features that are similar
    to ArgoCD and Tekton, we won''t be using them. We also didn''t want to worry about
    high availability. Let''s begin:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new namespace called **gitlab**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ kubectl create ns gitlab**'
  prefs: []
  type: TYPE_NORMAL
- en: '**namespace/gitlab created**'
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to add our certificate authority as a secret for GitLab to trust talking
    to OpenUnison and the webhooks we will eventually create for Tekton:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ kubectl get secret ca-key-pair \**'
  prefs: []
  type: TYPE_NORMAL
- en: '**  -n cert-manager -o json | jq -r ''.data["tls.crt"]'' \**'
  prefs: []
  type: TYPE_NORMAL
- en: '**  | base64 -d > tls.crt**'
  prefs: []
  type: TYPE_NORMAL
- en: '**$ kubectl create secret generic \**'
  prefs: []
  type: TYPE_NORMAL
- en: '**  internal-ca --from-file=. -n gitlab**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open **chapter14/gitlab/secret/provider** in your favorite text editor. Replace
    **local.tremolo.dev** with the full domain suffix for your cluster. For instance,
    my cluster is running on **192.168.2.114**, so I''m using the **apps.192-168-2-114.nip.io**
    suffix. Here''s my updated **Secret**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'name: openid_connect'
  prefs: []
  type: TYPE_NORMAL
- en: 'label: OpenUnison'
  prefs: []
  type: TYPE_NORMAL
- en: 'args:'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: openid_connect'
  prefs: []
  type: TYPE_NORMAL
- en: 'scope:'
  prefs: []
  type: TYPE_NORMAL
- en: '- openid'
  prefs: []
  type: TYPE_NORMAL
- en: '- profile'
  prefs: []
  type: TYPE_NORMAL
- en: 'response_type: code'
  prefs: []
  type: TYPE_NORMAL
- en: 'issuer: **https://k8sou.apps.192-168-2-114.nip.io/auth/idp/k8sIdp**'
  prefs: []
  type: TYPE_NORMAL
- en: 'discovery: true'
  prefs: []
  type: TYPE_NORMAL
- en: 'client_auth_method: query'
  prefs: []
  type: TYPE_NORMAL
- en: 'uid_field: sub'
  prefs: []
  type: TYPE_NORMAL
- en: 'send_scope_to_token_endpoint: false'
  prefs: []
  type: TYPE_NORMAL
- en: 'client_options:'
  prefs: []
  type: TYPE_NORMAL
- en: 'identifier: gitlab'
  prefs: []
  type: TYPE_NORMAL
- en: 'secret: secret'
  prefs: []
  type: TYPE_NORMAL
- en: 'redirect_uri: **https://gitlab.apps.192-168-2-114.nip.io/users/auth/openid_connect/callback**'
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: We're using a client secret of **secret**. This should not be done for a production
    cluster. If you're deploying GitLab into production using our templates as a starting
    point, make sure to change this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the **secret** for GitLab to integrate with OpenUnison for SSO. We''ll
    finish the process when we revisit OpenUnison:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ kubectl create secret generic gitlab-oidc --from-file=. -n gitlab**'
  prefs: []
  type: TYPE_NORMAL
- en: '**secret/gitlab-oidc created**'
  prefs: []
  type: TYPE_NORMAL
- en: Edit **chapter14/yaml/gitlab-values.yaml**. Just as in *step 3*, replace **local.tremolo.dev**
    with the full domain suffix for your cluster. For instance, my cluster is running
    on **192.168.2.114**, so I'm using the **apps.192-168-2-114.nip.io** suffix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If your cluster is running on a single virtual machine, now would be a good
    time to create a snapshot. If something goes wrong during the GitLab deployment,
    it's easier to revert back to a snapshot since the Helm chart doesn't do a great
    job of cleaning up after itself on a delete.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add the chart to your local repository and deploy GitLab:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ helm repo add gitlab https://charts.gitlab.io**'
  prefs: []
  type: TYPE_NORMAL
- en: '**$ "gitlab" has been added to your repositories**'
  prefs: []
  type: TYPE_NORMAL
- en: '**$ helm install gitlab gitlab/gitlab -n gitlab -f chapter14/yaml/gitlab-values.yaml**'
  prefs: []
  type: TYPE_NORMAL
- en: '**NAME: gitlab**'
  prefs: []
  type: TYPE_NORMAL
- en: '**LAST DEPLOYED: Sat Aug  8 14:50:13 2020**'
  prefs: []
  type: TYPE_NORMAL
- en: '**NAMESPACE: gitlab**'
  prefs: []
  type: TYPE_NORMAL
- en: '**STATUS: deployed**'
  prefs: []
  type: TYPE_NORMAL
- en: '**REVISION: 1**'
  prefs: []
  type: TYPE_NORMAL
- en: '**WARNING: Automatic TLS certificate generation with cert-manager is disabled
    and no TLS certificates were provided. Self-signed certificates were generated.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'It will take a few minutes to run. Even once the Helm chart has been installed,
    it can take 15–20 minutes for all the Pods to finish deploying. While GitLab is
    initializing, we need to update the web frontend''s **Ingress** object to use
    a certificate signed by our certificate authority. Edit the **gitlab-webservice**
    **Ingress** object in the **gitlab** namespace. Change the **kubernetes.io/ingress.class:
    gitlab-nginx** annotation to **kubernetes.io/ingress.class: nginx**. Also, change
    **secretName** from **gitlab-wildcard-tls** to **gitlab-web-tls**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'apiVersion: extensions/v1beta1'
  prefs: []
  type: TYPE_NORMAL
- en: 'kind: Ingress'
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: 'annotations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'cert-manager.io/cluster-issuer: ca-issuer'
  prefs: []
  type: TYPE_NORMAL
- en: '**kubernetes.io/ingress.class: nginx**'
  prefs: []
  type: TYPE_NORMAL
- en: 'kubernetes.io/ingress.provider: nginx'
  prefs: []
  type: TYPE_NORMAL
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: 'tls:'
  prefs: []
  type: TYPE_NORMAL
- en: '- hosts:'
  prefs: []
  type: TYPE_NORMAL
- en: '- gitlab.apps.192-168-2-114.nip.io'
  prefs: []
  type: TYPE_NORMAL
- en: '**secretName: gitlab-web-tls**'
  prefs: []
  type: TYPE_NORMAL
- en: 'status:'
  prefs: []
  type: TYPE_NORMAL
- en: 'loadBalancer: {}'
  prefs: []
  type: TYPE_NORMAL
- en: 'We next need to update our GitLab shell to accept SSH connections on port **2222**.
    This way, we can commit code without having to worry about blocking SSH access
    to your KinD server. Edit the **gitlab-gitlab-shell** **Deployment** in the **gitlab**
    namespace. Find **containerPort: 2222** and insert **hostPort: 2222** underneath,
    making sure to maintain the spacing. Once the Pod relaunches, you''ll be able
    to SSH to your GitLab hostname on port **2222**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To get your root password to log in to GitLab, get it from the secret that
    was generated:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ kubectl get secret gitlab-gitlab-initial-root-password -o json -n gitlab
    | jq -r ''.data.password'' | base64 -d**'
  prefs: []
  type: TYPE_NORMAL
- en: '**10xtSWXfbvH5umAbCk9NoN0wAeYsUo9jRVbXrfLn KbzBoPLrCGZ6kYRe8wdREcDl**'
  prefs: []
  type: TYPE_NORMAL
- en: You now can log in to your GitLab instance by going to **https://gitlab.apps.x-x-x-x.nip.io**,
    where **x-x-x-x** is the IP of your server. Since my server is running on **192.168.2.114**,
    my GitLab instance is running on **https://gitlab.apps.192-168-2-114.nip.io/**.
  prefs: []
  type: TYPE_NORMAL
- en: Creating example projects
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To explore Tekton and ArgoCD, we will create two projects. One will be for
    storing a simple Python web service, while the other will store the manifests
    for running the service. Let''s begin:'
  prefs: []
  type: TYPE_NORMAL
- en: The top of the GitLab screen will ask you to add an SSH key. Do that now so
    that we can commit code. Since we're going to be centralizing authentication via
    SAML, GitLab won't have a password for authentication.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a project and call it **hello-python**. Keep the visibility **private**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Clone the project using SSH. Because we're running on port **2222**, we need
    to change the URL provided by GitLab to be a proper SSH URL. For instance, my
    GitLab instance gives me the URL [git@gitlab.apps.192-168-2-114.nip.io:root/hello-python.git](mailto:git@gitlab.apps.192-168-2-114.nip.io:root/hello-python.git).
    This needs to be changed to [ssh://git@gitlab.apps.192-168-2-114.nip.io:2222/root/hello-python.git](mailto:ssh://git@gitlab.apps.192-168-2-114.nip.io:2222/root/hello-python.git).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once cloned, copy the contents of **chapter14/python-hello** into your repository
    and push to GitLab:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ cd chapter14/python-hello**'
  prefs: []
  type: TYPE_NORMAL
- en: '**$ git archive --format=tar HEAD > /path/to/hello-python/data.tar**'
  prefs: []
  type: TYPE_NORMAL
- en: '**$ cd /path/to/hello-python**'
  prefs: []
  type: TYPE_NORMAL
- en: '**$ tar -xvf data.tar**'
  prefs: []
  type: TYPE_NORMAL
- en: '**README.md**'
  prefs: []
  type: TYPE_NORMAL
- en: '**source/**'
  prefs: []
  type: TYPE_NORMAL
- en: '**source/Dockerfile**'
  prefs: []
  type: TYPE_NORMAL
- en: '**source/helloworld.py**'
  prefs: []
  type: TYPE_NORMAL
- en: '**source/requirements.txt**'
  prefs: []
  type: TYPE_NORMAL
- en: '**$ git add ***'
  prefs: []
  type: TYPE_NORMAL
- en: '**$ git commit -m ''initial commit''**'
  prefs: []
  type: TYPE_NORMAL
- en: '**$ git push**'
  prefs: []
  type: TYPE_NORMAL
- en: In GitLab, create another project called **hello-python-operations** with visibility
    set to private. Clone this project and copy the contents of **chapter14/python-hello-operations**
    into the repository, and then push it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that GitLab is deployed with some example code, we are able to move on to
    the next step, building an actual pipeline!
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Tekton
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tekton is the pipeline system we're using for our platform. Originally part
    of the Knative project for building function-as-a-service on Kubernetes, Tekton
    was broken out into its own project. The biggest difference between Tekton and
    other pipeline technologies you may have run is that Tekton is Kubernetes-native.
    Everything from its execution system, definition, and webhooks for automation
    are able to run on just about any Kubernetes distribution you can find. For example,
    we'll be running it in KinD and Red Hat has moved to Tekton as the main pipeline
    technology used for OpenShift starting in 4.1\.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of deploying Tekton is pretty straightforward. Tekton is a series
    of operators that look for the creation of custom resources that define a build
    pipeline. The deployment itself only takes a couple of **kubectl** commands:'
  prefs: []
  type: TYPE_NORMAL
- en: $ kubectl apply --filename \  https://storage.googleapis.com/tekton-releases/pipeline/latest/release.yaml
  prefs: []
  type: TYPE_NORMAL
- en: $ kubectl apply --filename \ https://storage.googleapis.com/tekton-releases/triggers/latest/release.yaml
  prefs: []
  type: TYPE_NORMAL
- en: The first command deploys the base system needed to run Tekton pipelines. The
    second command deploys the components needed to build webhooks so that pipelines
    can be launched as soon as code is pushed. Once both commands are done and the
    Pods in the **tekton-pipelines** namespace are running, you're ready to start
    building a pipeline! We'll use our Python Hello World web service as an example.
  prefs: []
  type: TYPE_NORMAL
- en: Building Hello World
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our Hello World application is really straightforward. It''s a simple service
    that echoes back the obligatory "hello" and the host the service is running on
    just so we feel like our service is doing something interesting. Since the service
    is written in Python, we don''t need to "build" a binary, but we do want to build
    a container. Once the container is built, we want to update the Git repository
    for our running namespace and let our GitOps system reconcile the change to redeploy
    our application. The steps for our build will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Check out our latest code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a tag based on a timestamp.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build our image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Push to our registry.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Patch a Deployment YAML file in the **operations** namespace.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We''ll build our pipeline one object at a time. The first set of tasks is to
    create an SSH key that Tekton will use to pull our source code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create an SSH key pair that we''ll use for our pipeline to check out our code.
    When prompted for a passphrase, just hit *Enter* to skip adding a passphrase:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ ssh-keygen -f ./gitlab-hello-python**'
  prefs: []
  type: TYPE_NORMAL
- en: Log in to GitLab and navigate to the **hello-python** project we created. Click
    on **Settings** | **Repository** | **Deploy Keys**, and click **Expand**. Use
    **tekton** as the title and paste the contents of the **github-hello-python.pub**
    file you just created into the **Key** section. Keep **Write access allowed**
    *unchecked* and click **Add Key**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, create the **python-hello-build** namespace and the following secret.
    Replace the **ssh-privatekey** attribute with the Base64-encoded content of the
    **gitlab-hello-python** file we created in *step 1*. The annotation is what tells
    Tekton which server to use this key with. The server name is the **Service** in
    the GitLab namespace:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'apiVersion: v1'
  prefs: []
  type: TYPE_NORMAL
- en: 'data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'ssh-privatekey: ...'
  prefs: []
  type: TYPE_NORMAL
- en: 'kind: Secret'
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: 'annotations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'tekton.dev/git-0: gitlab-gitlab-shell.gitlab.svc.cluster.local'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: git-pull'
  prefs: []
  type: TYPE_NORMAL
- en: 'namespace: python-hello-build'
  prefs: []
  type: TYPE_NORMAL
- en: 'type: kubernetes.io/ssh-auth'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create an SSH key pair that we''ll use for our pipeline to push to the **operations**
    repository. When prompted for a passphrase, just hit *Enter* to skip adding a
    passphrase:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ ssh-keygen -f ./gitlab-hello-python-operations**'
  prefs: []
  type: TYPE_NORMAL
- en: Log in to GitLab and navigate to the **hello-python-operations** project we
    created. Click on **Settings** | **Repository** | **Deploy Keys**, and click **Expand**.
    Use **tekton** as the title and paste the contents of the **github-hello-python-operations.pub**
    file you just created into the **Key** section. Make sure **Write access allowed**
    is *checked* and click **Add Key**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, create the following secret. Replace the **ssh-privatekey** attribute
    with the Base64-encoded content of the **gitlab-hello-python-operations** file
    we created in *step 4*. The annotation is what tells Tekton which server to use
    this key with. The server name is the **Service** we created in *step 6* in the
    GitLab namespace:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'apiVersion: v1'
  prefs: []
  type: TYPE_NORMAL
- en: 'data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'ssh-privatekey: ...'
  prefs: []
  type: TYPE_NORMAL
- en: 'kind: Secret'
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: git-write'
  prefs: []
  type: TYPE_NORMAL
- en: 'namespace: python-hello-build'
  prefs: []
  type: TYPE_NORMAL
- en: 'type: kubernetes.io/ssh-auth'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a service account for tasks to run, as with our secret:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ kubectl create -f chapter14/tekton-serviceaccount.yaml**'
  prefs: []
  type: TYPE_NORMAL
- en: 'We need a container that contains both **git** and **kubectl** in it. We''ll
    build **chapter14/docker/PatchRepoDockerfile** and push it to our internal registry.
    Make sure to replace **192-168-2-114** with the hostname for your server''s IP
    address:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ docker build -f ./PatchRepoDockerfile -t \**'
  prefs: []
  type: TYPE_NORMAL
- en: '**  docker.apps.192-168-2-114.nip.io/gitcommit/gitcommit .**'
  prefs: []
  type: TYPE_NORMAL
- en: '**$ docker push \**'
  prefs: []
  type: TYPE_NORMAL
- en: '**  docker.apps.192-168-2-114.nip.io/gitcommit/gitcommit**'
  prefs: []
  type: TYPE_NORMAL
- en: Every **Task** object can take inputs and produce results that can be shared
    with other **Task** objects. Tekton can provide runs (whether it's **TaskRun**
    or **PipelineRun**) with a workspace where the state can be stored and retrieved
    from. Writing to workspaces allows us to share data between **Task** objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before deploying our task and pipeline, let''s step through the work done by
    each task. The first task generates an image tag and gets the SHA hash of the
    latest commit. The full source is in **chapter14/yaml/tekton-task1.yaml**:'
  prefs: []
  type: TYPE_NORMAL
- en: '- name: create-image-tag'
  prefs: []
  type: TYPE_NORMAL
- en: 'image: docker.apps.192-168-2-114.nip.io/gitcommit/gitcommit'
  prefs: []
  type: TYPE_NORMAL
- en: 'script: |-'
  prefs: []
  type: TYPE_NORMAL
- en: '#!/usr/bin/env bash'
  prefs: []
  type: TYPE_NORMAL
- en: export IMAGE_TAG=$(date +"%m%d%Y%H%M%S")
  prefs: []
  type: TYPE_NORMAL
- en: echo -n "$(resources.outputs.result-image.url):$IMAGE_TAG" > /tekton/results/image-url
  prefs: []
  type: TYPE_NORMAL
- en: echo "'$(cat /tekton/results/image-url)'"
  prefs: []
  type: TYPE_NORMAL
- en: cd $(resources.inputs.git-resource.path)
  prefs: []
  type: TYPE_NORMAL
- en: RESULT_SHA="$(git rev-parse HEAD | tr -d '\n')"
  prefs: []
  type: TYPE_NORMAL
- en: 'echo "Last commit : $RESULT_SHA"'
  prefs: []
  type: TYPE_NORMAL
- en: echo -n "$RESULT_SHA" > /tekton/results/commit-tag
  prefs: []
  type: TYPE_NORMAL
- en: Each step in a task is a container. In this case, we're using the container
    we built previously that has **kubectl** and **git** in it. We don't need **kubectl**
    for this task but we do need **git**. The first block of code generates an image
    name from the **result-image** URL and a timestamp. We could use the latest commit,
    but I like having a timestamp so that I can quickly tell how old a container is.
    We save the full image URL to **/text/results/image-url**, which corresponds to
    a result we defined in our task called **image-url**. This can be referenced by
    our pipeline or other tasks by referencing **$(tasks.generate-image-tag.results.image-url)**,
    where **generate-image-tag** is the name of our **Task**, and **image-url** is
    the name of our result.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our next task, in **chapter14/yaml/tekton-task2.yaml**, generates a container
    from our application''s source using Google''s Kaniko project ([https://github.com/GoogleContainerTools/kaniko](https://github.com/GoogleContainerTools/kaniko)).
    Kaniko lets you generate a container without needing access to a Docker daemon.
    This is great because you don''t need a privileged container to build your image:'
  prefs: []
  type: TYPE_NORMAL
- en: 'steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '- args:'
  prefs: []
  type: TYPE_NORMAL
- en: '- --dockerfile=$(params.pathToDockerFile)'
  prefs: []
  type: TYPE_NORMAL
- en: '- --destination=$(params.imageURL)'
  prefs: []
  type: TYPE_NORMAL
- en: '- --context=$(params.pathToContext)'
  prefs: []
  type: TYPE_NORMAL
- en: '- --verbosity=debug'
  prefs: []
  type: TYPE_NORMAL
- en: '- --skip-tls-verify'
  prefs: []
  type: TYPE_NORMAL
- en: 'command:'
  prefs: []
  type: TYPE_NORMAL
- en: '- /kaniko/executor'
  prefs: []
  type: TYPE_NORMAL
- en: 'env:'
  prefs: []
  type: TYPE_NORMAL
- en: '- name: DOCKER_CONFIG'
  prefs: []
  type: TYPE_NORMAL
- en: 'value: /tekton/home/.docker/'
  prefs: []
  type: TYPE_NORMAL
- en: 'image: gcr.io/kaniko-project/executor:v0.16.0'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: build-and-push'
  prefs: []
  type: TYPE_NORMAL
- en: 'resources: {}'
  prefs: []
  type: TYPE_NORMAL
- en: The Kaniko container is what's called a "distro-less" container. It's not built
    with an underlying shell, nor does it have many of the command-line tools you
    may be used to. It's just a single binary. This means that any variable manipulation,
    such as generating a tag for the image, needs to be done before this step. Notice
    that the image being created doesn't reference the result we created in the first
    task. It instead references a parameter called **imageURL**. While we could have
    referenced the result directly, it would make it harder to test this task because
    it is now tightly bound to the first task. By using a parameter that is set by
    our pipeline, we can test this task on its own. Once run, this task will generate
    and push our container.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our last task, in **chapter14/yaml/tekton-task-3.yaml**, does the work to trigger
    ArgoCD to roll out a new container:'
  prefs: []
  type: TYPE_NORMAL
- en: '- image: docker.apps.192-168-2-114.nip.io/gitcommit/gitcommit'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: patch-and-push'
  prefs: []
  type: TYPE_NORMAL
- en: 'resources: {}'
  prefs: []
  type: TYPE_NORMAL
- en: 'script: |-'
  prefs: []
  type: TYPE_NORMAL
- en: '#!/bin/bash'
  prefs: []
  type: TYPE_NORMAL
- en: export GIT_URL="$(params.gitURL)"
  prefs: []
  type: TYPE_NORMAL
- en: export GIT_HOST=$(sed 's/.*[@]\(.*\)[:].*/\1/' <<< "$GIT_URL")
  prefs: []
  type: TYPE_NORMAL
- en: mkdir /usr/local/gituser/.ssh
  prefs: []
  type: TYPE_NORMAL
- en: cp /pushsecret/ssh-privatekey /usr/local/gituser/.ssh/id_rsa
  prefs: []
  type: TYPE_NORMAL
- en: chmod go-rwx /usr/local/gituser/.ssh/id_rsa
  prefs: []
  type: TYPE_NORMAL
- en: ssh-keyscan -H $GIT_HOST > /usr/local/gituser/.ssh/known_hosts
  prefs: []
  type: TYPE_NORMAL
- en: cd $(workspaces.output.path)
  prefs: []
  type: TYPE_NORMAL
- en: git clone $(params.gitURL) .
  prefs: []
  type: TYPE_NORMAL
- en: kubectl patch --local -f src/deployments/hello-python.yaml -p '{"spec":{"template":{"spec":{"containers":[{"name":"python-hello","image":"$(params.imageURL)"}]}}}}'
    -o yaml > /tmp/hello-python.yaml
  prefs: []
  type: TYPE_NORMAL
- en: cp /tmp/hello-python.yaml src/deployments/hello-python.yaml
  prefs: []
  type: TYPE_NORMAL
- en: git add src/deployments/hello-python.yaml
  prefs: []
  type: TYPE_NORMAL
- en: git commit -m 'commit $(params.sourceGitHash)'
  prefs: []
  type: TYPE_NORMAL
- en: git push
  prefs: []
  type: TYPE_NORMAL
- en: The first block of code copies the SSH keys into our home directory, generates
    **known_hosts**, and clones our repository into a workspace we defined in the
    **Task**. We don't rely on Tekton to pull the code from our **operations** repository
    because Tekton assumes we won't be pushing code, so it disconnects the source
    code from our repository. If we try to run a commit, it will fail. Since the step
    is a container, we don't want to try to write to it, so we create a workspace
    with **emptyDir**, just like **emptyDir** in a **Pod** we might run. We could
    also define workspaces based on persistent volumes. This could come in handy to
    speed up builds where dependencies get downloaded.
  prefs: []
  type: TYPE_NORMAL
- en: We're copying the SSH key from **/pushsecret**, which is defined as a volume
    on the task. Our container runs as user **431**, but the SSH keys are mounted
    as root by Tekton. We don't want to run a privileged container just to copy the
    keys from a **Secret**, so instead, we mount it as if it were just a regular **Pod**.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have our repository cloned, we patch our deployment with the latest
    image and finally, commit the change using the hash of the source commit in our
    application repository. Now we can track an image back to the commit that generated
    it! Just as with our second task, we don't reference the results of tasks directly
    to make it easier to test.
  prefs: []
  type: TYPE_NORMAL
- en: 'We pull these tasks together in a pipeline – specifically, **chapter14/yaml/tekton-pipeline.yaml**.
    This YAML file is several pages long, but the key piece defines our tasks and
    links them together. You should never hardcode values into your pipeline. Take
    a look at our third task''s definition in the pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '- name: update-operations-git'
  prefs: []
  type: TYPE_NORMAL
- en: 'taskRef:'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: patch-deployment'
  prefs: []
  type: TYPE_NORMAL
- en: 'params:'
  prefs: []
  type: TYPE_NORMAL
- en: '- name: imageURL'
  prefs: []
  type: TYPE_NORMAL
- en: 'value: $(tasks.generate-image-tag.results.image-url)'
  prefs: []
  type: TYPE_NORMAL
- en: '- name: gitURL'
  prefs: []
  type: TYPE_NORMAL
- en: 'value: $(params.gitPushUrl)'
  prefs: []
  type: TYPE_NORMAL
- en: '- name: sourceGitHash'
  prefs: []
  type: TYPE_NORMAL
- en: 'value: $(tasks.generate-image-tag.results.commit-tag)'
  prefs: []
  type: TYPE_NORMAL
- en: 'workspaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '- name: output'
  prefs: []
  type: TYPE_NORMAL
- en: 'workspace: output'
  prefs: []
  type: TYPE_NORMAL
- en: 'We reference parameters and task results, but nothing is hardcoded. This makes
    our **Pipeline** reusable. We also include the **runAfter** directive in our second
    and third task to make sure that our tasks are run in order. Otherwise, tasks
    will be run in parallel. Given each task has dependencies on the task before it,
    we don''t want to run them at the same time. Next, let''s deploy our pipeline
    and run it:'
  prefs: []
  type: TYPE_NORMAL
- en: Add the **chapter14/yaml/tekton-source-git.yaml** file to your cluster; this
    tells Tekton where to pull your application code from.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Edit **chapter14/yaml/tekton-image-result.yaml**, replacing **192-168-2-114**
    with the hash representation of your server's IP address, and add it to your cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Edit **chapter14/yaml/tekton-task1.yaml**, replacing the image host with the
    host for your Docker registry, and add the file to your cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add **chapter14/yaml/tekton-task2.yaml** to your cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Edit **chapter14/yaml/tekton-task3.yaml**, replacing the image host with the
    host for your Docker registry, and add the file to your cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add **chapter14/yaml/tekton-pipeline.yaml** to your cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add **chapter14/yaml/tekton-pipeline-run.yaml** to your cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can check on the progress of your pipeline using **kubectl**, or you can
    use Tekton's CLI tool called **tkn** ([https://github.com/tektoncd/cli](https://github.com/tektoncd/cli)).
    Running **tkn pipelinerun describe build-hello-pipeline-run -n python-hello-build**
    will list out the progress of your build. You can rerun the build by recreating
    your **run** object, but that's not very efficient. Besides, what we really want
    is for our pipeline to run on a commit!
  prefs: []
  type: TYPE_NORMAL
- en: Building automatically
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We don''t want to manually run builds. We want builds to be automated. Tekton
    provides the trigger project to provide webhooks so whenever GitLab receives a
    commit, it can tell Tekton to build a **PipelineRun** object for us. Setting up
    a trigger involves creating a Pod, with its own service account that can create
    **PipelineRun** objects, a Service for that Pod, and an **Ingress** object to
    host HTTPS access to the Pod. You also want to protect the webhook with a secret
    so that it isn''t triggered inadvertently. Let''s deploy these objects to our
    cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: Add **chapter14/yaml/tekton-webhook-cr.yaml** to your cluster. This **ClusterRole**
    will be used by any namespace that wants to provision webhooks for builds.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Edit **chapter14/yaml/tekton-webhook.yaml**. At the bottom of the file is an
    **Ingress** object. Change **192-168-2-114** to represent the IP of your cluster,
    with dashes instead of dots. Then, add the file to your cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'apiVersion: extensions/v1beta1'
  prefs: []
  type: TYPE_NORMAL
- en: 'kind: Ingress'
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: gitlab-webhook'
  prefs: []
  type: TYPE_NORMAL
- en: 'namespace: python-hello-build'
  prefs: []
  type: TYPE_NORMAL
- en: 'annotations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'cert-manager.io/cluster-issuer: ca-issuer'
  prefs: []
  type: TYPE_NORMAL
- en: 'spec:'
  prefs: []
  type: TYPE_NORMAL
- en: 'rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '- host: "python-hello-application.build.**192-168-2-114**.nip.io"'
  prefs: []
  type: TYPE_NORMAL
- en: 'http:'
  prefs: []
  type: TYPE_NORMAL
- en: 'paths:'
  prefs: []
  type: TYPE_NORMAL
- en: '- backend:'
  prefs: []
  type: TYPE_NORMAL
- en: 'serviceName: el-gitlab-listener'
  prefs: []
  type: TYPE_NORMAL
- en: 'servicePort: 8080'
  prefs: []
  type: TYPE_NORMAL
- en: 'pathType: ImplementationSpecific'
  prefs: []
  type: TYPE_NORMAL
- en: 'tls:'
  prefs: []
  type: TYPE_NORMAL
- en: '- hosts:'
  prefs: []
  type: TYPE_NORMAL
- en: '- "python-hello-application.build.**192-168-2-114**.nip.io"'
  prefs: []
  type: TYPE_NORMAL
- en: 'secretName: ingresssecret'
  prefs: []
  type: TYPE_NORMAL
- en: Log in to GitLab. Go to the **Admin Area** | **Network**. Click on **Expand**
    next to **Outbound Requests**. Check **Allow requests to the local network from
    web hooks and services** and click **Save changes**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to the **hello-python** project we created and click on **Settings** | **Webhooks**.
    For the URL, use your **Ingress** host with HTTPS – for instance, **https://python-hello-application.build.192-168-2-114.nip.io/**.
    For **Secret Token**, use **notagoodsecret**, and for **Push events**, set the
    branch name to **master**. Finally, click on **Add webhook**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once added, click on **Test**, choosing **Push Events**. If everything is configured
    correctly, a new **PipelineRun** object should have been created. You can run
    **tkn pipelinerun list -n python-hello-build** to see the list of runs; there
    should be a new one running. After a few minutes, you'll have a new container
    and a patched Deployment in the **python-hello-operations** project!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We covered quite a bit in this section to build our application and deploy it
    using GitOps. The good news is that everything is automated; a push will create
    a new instance of our application! The bad news is that we had to create over
    a dozen Kubernetes objects and manually make updates to our projects in GitLab.
    In the last section, we'll automate this process. First, let's deploy ArgoCD so
    that we can get our application running!
  prefs: []
  type: TYPE_NORMAL
- en: Deploying ArgoCD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have a way to get into our cluster, a way to store code, and a system
    for building our code and generating images. The last component of our platform
    is our GitOps controller. This is the piece that lets us commit manifests to our
    Git repository and make changes to our cluster. ArgoCD is a collaboration between
    Intuit and Weaveworks. It provides a great UI and is driven by a combination of
    custom resources and Kubernetes-native **ConfigMap** and **Secret** objects. It
    has a CLI tool, and both the web and CLI tools are integrated with OpenID Connect,
    so it will be easy to add SSO with our OpenUnison. Let''s deploy ArgoCD and use
    it to launch our **hello-python** web service:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Deploy using the standard YAML from [https://argoproj.github.io/argo-cd/getting_started/](https://argoproj.github.io/argo-cd/getting_started/):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ kubectl create namespace argocd**'
  prefs: []
  type: TYPE_NORMAL
- en: '**$ kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml**'
  prefs: []
  type: TYPE_NORMAL
- en: Create the **Ingress** object for ArgoCD by editing **chapter14/yaml/argocd-ingress.yaml**.
    Replace all instances of **192-168-2-140** with your IP address, replacing the
    dots with dashes. My server's IP is **192.168.2.114**, so I'm using **192-168-2-114**.
    Once done, add the file to your cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get the root password by running **kubectl get pods -n argocd -l app.kubernetes.io/name=argocd-server
    -o name | cut -d'/' -f 2**. Save this password.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Edit the **argocd-server** **Deployment** in the **argocd** namespace. Add
    **--insecure** to the command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'spec:'
  prefs: []
  type: TYPE_NORMAL
- en: 'containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '- command:'
  prefs: []
  type: TYPE_NORMAL
- en: '- argocd-server'
  prefs: []
  type: TYPE_NORMAL
- en: '- --staticassets'
  prefs: []
  type: TYPE_NORMAL
- en: '- /shared/app'
  prefs: []
  type: TYPE_NORMAL
- en: '**- --repo-server**'
  prefs: []
  type: TYPE_NORMAL
- en: '**        - argocd-repo-server:8081**'
  prefs: []
  type: TYPE_NORMAL
- en: '**        - --insecure**'
  prefs: []
  type: TYPE_NORMAL
- en: You can now log in to ArgoCD by going to the **Ingress** host you defined in
    *step 2*. You will need to download the ArgoCD CLI utility as well from https://github.com/argoproj/argo-cd/releases/latest.
    Once downloaded, log in by running **./argocd login grpc-argocd.apps.192-168-2-114.nip.io**,
    replacing **192-168-2-114** with the IP of your server, with dashes instead of
    dots.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the **python-hello** namespace.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Before we can add our GitLab repository, we need to tell ArgoCD to trust our
    GitLab instance''s SSH host. Since we will have ArgoCD talk directly to the GitLab
    shell service, we''ll need to generate **known_host** for that Service. To make
    this easier, we included a script that will run **known_host** from outside the
    cluster but rewrite the content as if it were from inside the cluster. Run the
    **chapter14/shell/getSshKnownHosts.sh** script and pipe the output into the **argocd**
    command to import **known_host**. Remember to change the hostname to reflect your
    own cluster''s IP address:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ ./chapter14/shell/getSshKnownHosts.sh gitlab.apps.192-168-2-114.nip.io
    | argocd cert add-ssh --batch**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Enter SSH known hosts entries, one per line. Press CTRL-D when finished.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Successfully created 3 SSH known host entries**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to generate an SSH key to access the **python-hello-operations**
    repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ ssh-keygen -f ./argocd-python-hello**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the public key to the **python-hello-operations** repository by going to
    the project and clicking on **Settings** | **Repository**. Next to **Deploy Keys**,
    click **Expand**. For **Title**, use **argocd**. Use the contents of **argocd-python-hello.pub**
    and click **Add key**. Then, add the key to ArgoCD using the CLI and replace the
    public GitLab host with the **gitlab-gitlab-shell** **Service** hostname:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ argocd repo add git@gitlab-gitlab-shell.gitlab.svc.cluster.local:root/hello-python-operations.git
    --ssh-private-key-path ./argocd-python-hello**'
  prefs: []
  type: TYPE_NORMAL
- en: '**repository ''git@gitlab-gitlab-shell.gitlab.svc.cluster.local:root/hello-python-operations.git''
    added**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our last step is to create an **Application** object. You can create it through
    the web UI or the CLI. You can also create it by creating an **Application** object
    in the **argocd** namespace, which is what we''ll do. Create the following object
    in your cluster (**chapter14/yaml/argocd-python-hello.yaml**):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'apiVersion: argoproj.io/v1alpha1'
  prefs: []
  type: TYPE_NORMAL
- en: 'kind: Application'
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: python-hello'
  prefs: []
  type: TYPE_NORMAL
- en: 'namespace: argocd'
  prefs: []
  type: TYPE_NORMAL
- en: 'spec:'
  prefs: []
  type: TYPE_NORMAL
- en: 'destination:'
  prefs: []
  type: TYPE_NORMAL
- en: 'namespace: python-hello'
  prefs: []
  type: TYPE_NORMAL
- en: 'server: https://kubernetes.default.svc'
  prefs: []
  type: TYPE_NORMAL
- en: 'project: default'
  prefs: []
  type: TYPE_NORMAL
- en: 'source:'
  prefs: []
  type: TYPE_NORMAL
- en: 'directory:'
  prefs: []
  type: TYPE_NORMAL
- en: 'jsonnet: {}'
  prefs: []
  type: TYPE_NORMAL
- en: 'recurse: true'
  prefs: []
  type: TYPE_NORMAL
- en: 'path: src'
  prefs: []
  type: TYPE_NORMAL
- en: 'repoURL: git@gitlab-gitlab-shell.gitlab.svc.cluster.local:root/hello-python-operations.git'
  prefs: []
  type: TYPE_NORMAL
- en: 'targetRevision: HEAD'
  prefs: []
  type: TYPE_NORMAL
- en: 'syncPolicy:'
  prefs: []
  type: TYPE_NORMAL
- en: 'automated: {}'
  prefs: []
  type: TYPE_NORMAL
- en: This is about as basic of a configuration as possible. We're working off of
    simple manifests. ArgoCD can work off of jsonet and Helm too. After this application
    is created, look at the Pods in the **python-hello** namespace. You should have
    one running! Making updates to your code will result in updates to the namespace.
  prefs: []
  type: TYPE_NORMAL
- en: We now have a code base that can be deployed automatically with a commit. We
    spent a couple dozen pages, ran dozens of commands, and created more than 20 objects
    to get there. Instead of manually creating these objects, it would be best to
    automate the process. Now that we have the objects that need to be created, we
    can automate the onboarding. In the next section, we will take the manual process
    of building the links between GitLab, Tekton, and ArgoCD to line up with our business
    processes.
  prefs: []
  type: TYPE_NORMAL
- en: Automating project onboarding using OpenUnison
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Earlier in this chapter, we deployed the OpenUnison automation portal. This
    portal lets users request new namspaces to be created and allows developers to
    request access to these namespaces via a self-service interface. The workflows
    built into this portal are very basic but create the namespace and appropriate
    **RoleBinding** objects. What we want to do is build a workflow that integrates
    our platform and creates all of the objects we created manually earlier in this
    chapter. The goal is that we''re able to deploy a new application into our environment
    without having to run the **kubectl** command (or at least minimize its use).
    This will require careful planning. Here''s how our developer workflow will run:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.6 – Platform developer workflow'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_14.6_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.6 – Platform developer workflow
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s quickly run through the workflow that we see in the preceding figure:'
  prefs: []
  type: TYPE_NORMAL
- en: An application owner will request an application be created.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The infrastructure admin approves the creation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At this point, OpenUnison will deploy the objects we manually created. We'll
    detail those objects shortly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once created, a developer is able to request access to the application
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The application owner(s) approve access to the application.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once approved, the developer will fork the application source base and do their
    work. They can launch the application in their developer workspace. They can also
    fork the build project to create a pipeline and the development environment operations
    project to create manifests for the application. Once the work is done and tested
    locally, the developer will push the code into their own fork, then request a
    merge request.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The application owner will approve the request and merge the code from GitLab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the code is merged, ArgoCD will synchronize the build and operations projects.
    The webhook in the application project will kick off a Tekton pipeline that will
    build our container and update the development operations project with the tag
    for the latest container. ArgoCD will synchronize the updated manifest into our
    application's development namespace. Once testing is completed, the application
    owner submits a merge request from the development operations workspace to the
    production operations workspace, triggering ArgoCD to launch into production.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nowhere in this flow is there a step called "operations staff uses **kubectl**
    to create a namespace." This is a simple flow and won''t totally avoid your operations
    staff from using **kubectl**, but it should be a good starting point. All this
    automation requires an extensive set of objects to be created:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.7 – Application onboarding object map'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_14.7_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.7 – Application onboarding object map
  prefs: []
  type: TYPE_NORMAL
- en: In GitLab, we create a project for our application code, operations, and build
    pipeline. We also fork the operations project as a development operations project.
    For each project, we generate deploy keys and register webhooks. We also create
    groups to match the roles we defined earlier in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: For Kubernetes, we create namespaces for the development and production environments.
    We also create a namespace for the Tekton pipeline. We add the keys as needed
    to **Secrets**. In the build namespace, we create all the scaffolding to support
    the webhook that will trigger automatic builds. That way, our developers only
    need to worry about creating their pipeline objects.
  prefs: []
  type: TYPE_NORMAL
- en: In our last application, ArgoCD, we will create an **AppProject** that hosts
    our build and both operations namespaces. We will also add the SSH keys we generated
    when creating our GitLab projects. Each project also gets an **Application** object
    in our **AppProject** that instructs ArgoCD how to synchronize from GitLab. Finally,
    we add RBAC rules to ArgoCD so that our developers can view their application
    synchronization status but owners and operations can make updates and changes.
  prefs: []
  type: TYPE_NORMAL
- en: You don't need to build this out yourself! **chapter14/openunison** is the source
    for OpenUnison that implements this flow. If you want to see every object we create,
    refer to **chapter14/openunison/src/main/webapp/WEB-INF/workflows/30-NewK8sNamespace.xml**.
    This workflow does everything we just described. We also included **chapter14/python-hello**
    as our example application, **chapter14/python-hello-operations** for our manifests,
    and **chapter14/python-hello-build** as our pipeline. You'll need to tweak some
    of the objects in these three folders to match your environment, mostly updating
    the hostnames.
  prefs: []
  type: TYPE_NORMAL
- en: With our developer workflow designed and example projects ready to go, next
    we'll update OpenUnison, GitLab, and ArgoCD to get all this automation to work!
  prefs: []
  type: TYPE_NORMAL
- en: Integrating GitLab
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We configured GitLab for SSO when we first deployed the Helm chart. The **gitlab-oidc**
    **Secret** we deployed has all the information GitLab needs to access SSO from
    OpenUnison. We still need to configure OpenUnison though. We could hardcode the
    SSO configuration into our OpenUnison source base or we could dynamically add
    it as a custom resource. In this instance, we''ll add the SSO connection via a
    custom resource:'
  prefs: []
  type: TYPE_NORMAL
- en: Edit **chapter14/yaml/gitlab-trust.yaml**, replacing **192-168-2-140** with
    the server IP your cluster is running on. My cluster is on **192.168.2.114**,
    so I'll replace it with **192-168-2-114**. Add **chapter14/yaml/gitlab-trust.yaml**
    to your cluster. This file tells OpenUnison to establish a trust with GitLab for
    SSO.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Edit **chapter14/yaml/gitlab-url.yaml**, replacing **192-168-2-140** with the
    server IP your cluster is running on. My cluster is on **192.168.2.114**, so I'll
    replace it with **192-168-2-114**. Add **chapter14/yaml/gitlab-url.yaml** to your
    cluster. This file tells OpenUnison to add a badge to the portal for GitLab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Log in to GitLab as root. Go to your user's profile area and click on **Access
    Tokens**. For **Name**, use **openunison**. Leave **Expires** blank and check
    the API scope. Click **Create personal access token**. Copy and paste the token
    into a notepad or some other place. Once you leave this screen, you can't retrieve
    this token again.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Edit the **orchestra-secrets-source** Secret in the **openunison** namespace.
    Add two keys:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'apiVersion: v1'
  prefs: []
  type: TYPE_NORMAL
- en: 'data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'K8S_DB_SECRET: aW0gYSBzZWNyZXQ='
  prefs: []
  type: TYPE_NORMAL
- en: 'OU_JDBC_PASSWORD: c3RhcnR0MTIz'
  prefs: []
  type: TYPE_NORMAL
- en: 'SMTP_PASSWORD: ""'
  prefs: []
  type: TYPE_NORMAL
- en: 'unisonKeystorePassword: aW0gYSBzZWNyZXQ='
  prefs: []
  type: TYPE_NORMAL
- en: '**  gitlab: c2VjcmV0  GITLAB_TOKEN: S7CCuqHfpw3a6GmAqEYg**'
  prefs: []
  type: TYPE_NORMAL
- en: 'kind: Secret'
  prefs: []
  type: TYPE_NORMAL
- en: Remember to Base64-encode the values. The **gitlab** key matches the secret
    in our **oidc-provider** Secret. **GITLAB_TOKEN** is going to be used by OpenUnison
    to interact with GitLab to provision the projects and groups we defined in our
    onboarding workflow. With GitLab configured, next is ArgoCD.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating ArgoCD
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'ArgoCD has built-in support for OpenID Connect. It wasn''t configured for us
    in the deployment, though:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Edit the **argocd-cm** **ConfigMap** in the **argocd** namespace, adding the
    **url** and **oidc.config** keys, as shown in the following cde block. Make sure
    to update **192-168-2-140** to match your cluster''s IP address. Mine is **192.168.2.114**,
    so I''ll be using **192-168-2-114**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'apiVersion: v1'
  prefs: []
  type: TYPE_NORMAL
- en: 'data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'url: https://argocd.apps.192-168-2-140.nip.io'
  prefs: []
  type: TYPE_NORMAL
- en: 'oidc.config: |-'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: OpenUnison'
  prefs: []
  type: TYPE_NORMAL
- en: 'issuer: https://k8sou.apps.192-168-2-140.nip.io/auth/idp/k8sIdp'
  prefs: []
  type: TYPE_NORMAL
- en: 'clientID: argocd'
  prefs: []
  type: TYPE_NORMAL
- en: 'requestedScopes: ["openid", "profile", "email", "groups"]'
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: We don't specify a client secret with ArgoCD because it has both a CLI and a
    web component. Just like with the API server, it makes no sense to worry about
    a client secret that will need to reside on every single workstation that will
    be known to the user. It doesn't add any security in this case, so we will skip
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Edit **chapter14/yaml/argocd-trust.yaml**, replacing **192-168-2-140** with
    the server IP your cluster is running on. My cluster is on **192.168.2.114**,
    so I'll replace it with **192-168-2-114**. Add **chapter14/yaml/argocd-trust.yaml**
    to your cluster. This file tells OpenUnison to establish a trust with ArgoCD for
    SSO.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Edit **chapter14/yaml/argocd-url.yaml**, replacing **192-168-2-140** with the
    server IP your cluster is running on. My cluster is on **192.168.2.114**, so I'll
    replace it with **192-168-2-114**. Add **chapter14/yaml/argocd-url.yaml** to your
    cluster. This file tells OpenUnison to add a badge to the portal for ArgoCD.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'While most of ArgoCD is controlled with Kubernetes custom resources, there
    are some ArgoCD-specific APIs. To work with these APIs, we need to create a service
    account. We''ll need to create this account and generate a key for it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ kubectl patch configmap argocd-cm -n argocd -p ''{"data":{"accounts.openunison":"apiKey","accounts.openunison.enabled":"true"}}''**'
  prefs: []
  type: TYPE_NORMAL
- en: '**$ argocd account generate-token --account openunison**'
  prefs: []
  type: TYPE_NORMAL
- en: Take the output of the **generate-token** command and add it as the **ARGOCD_TOKEN**
    key to the **orchestra-secrets-source** **Secret** in the **openunison** namespace.
    Don't forget to Base64-encode it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, we want to create ArgoCD RBAC rules so that we can control who can
    access the web UI and the CLI. Edit the **argocd-rbac-cm** **ConfigMap** and add
    the following keys. The first key will let our systems administrators and our
    API key do anything in ArgoCD. The second key maps all users that aren''t mapped
    by **policy.csv** into a role into a nonexistent role so that they won''t have
    access to anything:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'policy.csv: |-'
  prefs: []
  type: TYPE_NORMAL
- en: g, k8s-cluster-administrators,role:admin
  prefs: []
  type: TYPE_NORMAL
- en: g, openunison,role:admin
  prefs: []
  type: TYPE_NORMAL
- en: 'policy.default: role:none'
  prefs: []
  type: TYPE_NORMAL
- en: With ArgoCD integrated, the last step to world automation is updating our OpenUnison
    custom resource!
  prefs: []
  type: TYPE_NORMAL
- en: Updating OpenUnison
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'OpenUnison is already deployed. The last step to launching an automation portal
    with our developer workflows built in is to update the **orchestra** OpenUnison
    custom resource. Update the image as in the following code block. Add **non_secret_data**,
    replacing **hosts** to match with your cluster''s IP. Finally, add the new secrets
    we created to the list of secrets the operator needs to import:'
  prefs: []
  type: TYPE_NORMAL
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: '**image: docker.io/tremolosecurity/openunison-k8s-definitive-guide:latest**'
  prefs: []
  type: TYPE_NORMAL
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: 'non_secret_data:'
  prefs: []
  type: TYPE_NORMAL
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: '**- name: GITLAB_URL**'
  prefs: []
  type: TYPE_NORMAL
- en: '**    value: https://gitlab.apps.192-168-2-140.nip.io**'
  prefs: []
  type: TYPE_NORMAL
- en: '**  - name: GITLAB_SSH_HOST**'
  prefs: []
  type: TYPE_NORMAL
- en: '**    value: gitlab-gitlab-shell.gitlab.svc.cluster.local**'
  prefs: []
  type: TYPE_NORMAL
- en: '**  - name: GITLAB_WEBHOOK_SUFFIX**'
  prefs: []
  type: TYPE_NORMAL
- en: '**    value: gitlab.192-168-2-140.nip.io**'
  prefs: []
  type: TYPE_NORMAL
- en: '**  - name: ARGOCD_URL**'
  prefs: []
  type: TYPE_NORMAL
- en: '**    value: https://argocd.apps.192-168-2-140.nip.io**'
  prefs: []
  type: TYPE_NORMAL
- en: '**  - name: GITLAB_WRITE_SSH_HOST**'
  prefs: []
  type: TYPE_NORMAL
- en: '**    value: gitlab-write-shell.gitlab.svc.cluster.local**'
  prefs: []
  type: TYPE_NORMAL
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: 'secret_data:'
  prefs: []
  type: TYPE_NORMAL
- en: '- K8S_DB_SECRET'
  prefs: []
  type: TYPE_NORMAL
- en: '- unisonKeystorePassword'
  prefs: []
  type: TYPE_NORMAL
- en: '- SMTP_PASSWORD'
  prefs: []
  type: TYPE_NORMAL
- en: '- OU_JDBC_PASSWORD'
  prefs: []
  type: TYPE_NORMAL
- en: '**- GITLAB_TOKEN**'
  prefs: []
  type: TYPE_NORMAL
- en: '**  - ARGOCD_TOKEN**'
  prefs: []
  type: TYPE_NORMAL
- en: In just a few minutes, the automation portal will be running. When you log in,
    you'll see badges for GitLab and ArgoCD. You'll also be able to click on **New
    Application** to begin deploying applications according to our workflow! You can
    use this as a starting point for designing your own automation platform or use
    it as a map for creating the various objects needed to integrate the tools on
    your platform.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Coming into this chapter, we hadn't spent much time on deploying applications.
    We wanted to close things out with a brief introduction to application deployment
    and automation. We learned about pipelines, how they are built, and how they run
    on a Kubernetes cluster. We explored the process of building a platform by deploying
    GitLab for source control, built out a Tekton pipeline to work in a GitOps model,
    and used ArgoCD to make the GitOps model a reality. Finally, we automated the
    entire process with OpenUnison.
  prefs: []
  type: TYPE_NORMAL
- en: Using the information in this chapter should give you direction as to how you
    want to build your own platform. Using the practical examples in this chapter
    will help you map the requirements in your organization to the technology needed
    to automate your infrastructure. The platform we built in this chapter is far
    from complete. It should give you a map for planning your own platform that matches
    your needs.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, thank you. Thank you for joining us on this adventure of building out
    a Kubernetes cluster. We hope you have as much fun reading this book and building
    out the examples as we did creating it!
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'True or false: A pipeline must be implemented to make Kubernetes work.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. True
  prefs: []
  type: TYPE_NORMAL
- en: B. False
  prefs: []
  type: TYPE_NORMAL
- en: What are the minimum steps of a pipeline?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. Build, scan, test, and deploy
  prefs: []
  type: TYPE_NORMAL
- en: B. Build and deploy
  prefs: []
  type: TYPE_NORMAL
- en: C. Scan, test, deploy, and build
  prefs: []
  type: TYPE_NORMAL
- en: D. None of the above
  prefs: []
  type: TYPE_NORMAL
- en: What is GitOps?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. Running GitLab on Kubernetes
  prefs: []
  type: TYPE_NORMAL
- en: B. Using Git as an authoritative source for operations configuration
  prefs: []
  type: TYPE_NORMAL
- en: C. A silly marketing term
  prefs: []
  type: TYPE_NORMAL
- en: D. A product from a new start-up
  prefs: []
  type: TYPE_NORMAL
- en: What is the standard for writing pipelines?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. All pipelines should be written in YAML.
  prefs: []
  type: TYPE_NORMAL
- en: B. There are no standards; every project and vendor has its own implementation.
  prefs: []
  type: TYPE_NORMAL
- en: C. JSON combined with Go.
  prefs: []
  type: TYPE_NORMAL
- en: D. Rust.
  prefs: []
  type: TYPE_NORMAL
- en: How do you deploy a new instance of a container in a GitOps model?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. Use **kubectl** to update the **Deployment** or **StatefulSet** in the namespace.
  prefs: []
  type: TYPE_NORMAL
- en: B. Update the **Deployment** or **StatefulSet** manifest in Git, letting the
    GitOps controller update the objects in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: C. Submit a ticket that someone in operations needs to act on.
  prefs: []
  type: TYPE_NORMAL
- en: D. None of the above.
  prefs: []
  type: TYPE_NORMAL
- en: 'True or false: All objects in GitOps needs to be stored in your Git repository.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. True
  prefs: []
  type: TYPE_NORMAL
- en: B. False
  prefs: []
  type: TYPE_NORMAL
- en: 'True or false: Your way is the right way to automate your processes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. True
  prefs: []
  type: TYPE_NORMAL
- en: B. False
  prefs: []
  type: TYPE_NORMAL
