- en: '*Chapter 14*: Provisioning a Platform'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Every chapter in this book, up until this point, has focused on the infrastructure
    of your cluster. We have explored how to deploy Kubernetes, how to secure it,
    and how to monitor it. What we haven't talked about is how to deploy applications.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: In this, our final chapter, we're going to work on building an application deployment
    platform using what we've learned about Kubernetes. We're going to build our platform
    based on some common enterprise requirements. Where we can't directly implement
    a requirement, because building a platform on Kubernetes can fill its own book,
    we'll call it out and provide some insights.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Designing a pipeline
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing our cluster
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying GitLab
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying Tekton
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying ArgoCD
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automating project onboarding using OpenUnison
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To perform the exercises in this chapter, you will need a clean KinD cluster
    with a minimum of 8 GB of memory, 75 GB storage, and 4 CPUs. The system we will
    build is minimalist but still requires considerable horsepower to run.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 'You can access the code for this chapter at the following GitHub repository:
    [https://github.com/PacktPublishing/Kubernetes-and-Docker-The-Complete-Guide](https://github.com/PacktPublishing/Kubernetes-and-Docker-The-Complete-Guide).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Designing a pipeline
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The term "pipeline" is used extensively in the Kubernetes and DevOps world.
    Very simply, a pipeline is a process, usually automated, that takes code and gets
    it running. This usually involves the following:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.1 – A simple pipeline'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_14.1_B15514.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.1 – A simple pipeline
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s quickly run through the steps involved in this process:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Storing the source code in a central repository, usually Git
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When code is committed, building it and generating artifacts, usually a container
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Telling the platform – in this case, Kubernetes – to roll out the new containers
    and shut down the old ones
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This is about as basic as a pipeline can get and isn''t of much use in most
    deployments. In addition to building our code and deploying it, we want to make
    sure we scan containers for known vulnerabilities. We may also want to run our
    containers through some automated testing before going into production. In enterprise
    deployments, there''s often a compliance requirement where someone takes responsibility
    for the move to production as well. Taking this into account, the pipeline starts
    to get more complex:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.2 – Pipeline with common enterprise requirements'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_14.2_B15514.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.2 – Pipeline with common enterprise requirements
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: 'The pipeline has added some extra steps, but it''s still linear with one starting
    point, a commit. This is also very simplistic and unrealistic. The base containers
    and libraries your applications are built on are constantly being updated as new
    **Common Vulnerabilities and Exposures** (**CVEs**), a common way to catalog and
    identify security vulnerabilities, are discovered and patched. In addition to
    having developers that are updating application code for new requirements, you
    will want to have a system in place that scans both the code and the base containers
    for available updates. These scanners watch your base containers and can do something
    to trigger a build once a new base container is ready. While the scanners could
    call an API to trigger a pipeline, your pipeline is already waiting on your Git
    repository to do something, so it would be better to simply add a commit or a
    pull request to your Git repository to trigger the pipeline:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.3 – Pipeline with scanners integrated'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_14.3_B15514.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.3 – Pipeline with scanners integrated
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: This means your application code is tracked and your operational updates are
    tracked in Git. Git is now the source of truth for not only what your application
    code is but also operations updates. When it's time to go through your audits,
    you have a ready-made change log! If your policies require you to enter changes
    into a change management system, simply export the changes from Git.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have focused on our application code and just put **Rollout** at
    the end of our pipeline. The final rollout step usually means patching a **Deployment**
    or **StatefulSet** with our newly built container, letting Kubernetes do the work
    of spinning up new **Pods** and scaling down the old ones. This could be done
    with a simple API call, but how are we tracking and auditing that change? What's
    the source of truth?
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 'Our application in Kubernetes is defined as a series of objects stored in **etcd**
    that are generally represented as code using YAML files. Why not store those files
    in a Git repository too? This gives us the same benefits as storing our application
    code in Git. We have a single source of truth for both the application source
    and the operations of our application! Now, our pipeline involves some more steps:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.4 – GitOps pipeline'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_14.4_B15514.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.4 – GitOps pipeline
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: In this diagram, our rollout updates a Git repository with our application's
    Kubernetes YAML. A controller inside our cluster watches for updates to Git and
    when it sees them, gets the cluster in sync with what's in Git. It can also detect
    drift in our cluster and bring it back to alignment with our source of truth.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: This focus on Git is called **GitOps**. The idea is that all of the work of
    an application is done via code, not directly via APIs. How strict you are with
    this idea can dictate what your platform looks like. Next, we'll explore how opinions
    can shape your platform.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: Opinionated platforms
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Kelsey Hightower, a developer advocate for Google and leader in the Kubernetes
    world, once said: "Kubernetes is a platform for building platforms. It''s a better
    place to start; not the endgame." When you look at the landscape of vendors and
    projects building Kubernetes-based products, they all have their own opinions
    of how systems should be built. As an example, Red Hat''s **OpenShift Container
    Platform** (**OCP**) wants to be a one-stop-shop for multi-tenant enterprise deployment.
    It builds in a great deal of the pipeline we discussed. You define a pipeline
    that is triggered by a commit, which builds a container and pushes it into its
    own internal registry that then triggers a rollout of the new container. Namespaces
    are the boundaries of tenants. Canonical is a minimalist distribution that doesn''t
    include any pipeline components. Managed vendors such as Amazon, Azure, and Google
    provide the building blocks of a cluster and the hosted build tools of a pipeline
    but leave it to you to build out your platform.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: There is no correct answer as to which platform to use. Each is opinionated
    and the right one for your deployment will depend on your own requirements. Depending
    on the size of your enterprise, it wouldn't be surprising to see more than one
    platform deployed!
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Having looked at the idea of opinionated platforms, let's explore the security
    impacts of building a pipeline.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Securing your pipeline
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Depending on your starting point, this can get complex quickly. How much of
    your pipeline is one integrated system, or could it be described using a colorful
    American colloquialism involving duct tape? Even in platforms where all the components
    are there, tying them together can often mean building a complex system. Most
    of the systems that are part of your pipeline will have a visual component. Usually,
    the visual component is a dashboard. Users and developers may need access to that
    dashboard. You don't want to maintain separate accounts for all those systems,
    do you? You'll want to have one login point and portal for all the components
    of your pipeline.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的起点，这可能会很快变得复杂起来。您的管道有多少是一个集成系统，或者可以用涉及胶带的美国俚语来描述？即使在所有组件都在的平台上，将它们联系在一起通常意味着构建一个复杂的系统。您的管道中的大多数系统都将具有视觉组件。通常，视觉组件是一个仪表板。用户和开发人员可能需要访问该仪表板。您不想为所有这些系统维护单独的帐户，对吧？您会希望为管道的所有组件设置一个登录点和门户。
- en: After determining how to authenticate the users who use these systems, the next
    question is how to automate the rollout. Each component of your pipeline requires
    configuration. It can be as simple as an object that gets created via an API call
    or as complex as tying together a Git repo and build process with SSH keys to
    automate security. In such a complex environment, manually creating pipeline infrastructure
    will lead to security gaps. It will also lead to impossible-to-manage systems.
    Automating the process and providing consistency will help you both secure your
    infrastructure and keep it maintainable.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 确定如何对使用这些系统的用户进行身份验证后，下一个问题是如何自动化推出。您的管道的每个组件都需要配置。它可以是通过API调用创建的对象，也可以是将Git存储库和构建过程与SSH密钥联系在一起以自动化安全的复杂过程。在这样一个复杂的环境中，手动创建管道基础设施将导致安全漏洞。这也将导致无法管理的系统。自动化流程并提供一致性将帮助您确保基础设施的安全性并使其易于维护。
- en: Finally, it's important to understand the implications of GitOps on our cluster
    from a security standpoint. We discussed authenticating administrators and developers
    to use the Kubernetes API and authorizing access to different APIs in [*Chapter
    7*](B15514_07_Final_ASB_ePub.xhtml#_idTextAnchor203), *Integrating Authentication
    into Your Cluster*, and [*Chapter 8*](B15514_08_Final_ASB_ePub.xhtml#_idTextAnchor228)*,
    RBAC Policies Using Active Directory Users*. What is the impact if someone can
    check in a **RoleBinding** that assigns them the **admin** **ClusterRole** for
    a namespace and a GitOps controller automatically pushes it through to the cluster?
    As you design your platform, consider how developers and administrators will want
    to interact with it. It's tempting to say "Let everyone interact with their application's
    Git registry," but that means putting the burden on you as the cluster owner for
    many requests. As we discussed in [*Chapter 8*](B15514_08_Final_ASB_ePub.xhtml#_idTextAnchor228),
    *RBAC Policies Using Active Directory*, this could make your team the bottleneck
    in an enterprise. Understanding your customers, in this case, is important in
    knowing how they want to interact with their operations even if it's not how you
    intended.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，从安全的角度来看，了解GitOps对我们的集群的影响是很重要的。我们在《第7章》，*将认证集成到您的集群*和《第8章》，*使用Active Directory用户的RBAC策略*中讨论了对管理员和开发人员进行身份验证以使用Kubernetes
    API，并授权访问不同API的访问。如果有人可以提交一个分配给他们命名空间的**admin ClusterRole**的**RoleBinding**，并且GitOps控制器自动将其推送到集群，那会有什么影响？在设计平台时，考虑开发人员和管理员将如何与其互动。诱人的做法是说“让每个人与他们的应用程序的Git注册表互动”，但这意味着您作为集群所有者需要处理许多请求的负担。正如我们在《第8章》，*使用Active
    Directory的RBAC策略*中讨论的那样，这可能使您的团队成为企业的瓶颈。了解您的客户在这种情况下是重要的，因为这样可以知道他们希望如何与他们的操作进行交互，即使这不是您的本意。
- en: Having touched on some of the security aspects of GitOps and a pipeline, let's
    explore the requirements for a typical pipeline and how we will build it.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在涉及GitOps和流水线的一些安全方面后，让我们探讨一下典型流水线的要求以及我们将如何构建它。
- en: Building our platform's requirements
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 满足我们平台的要求。
- en: 'Kubernetes deployments, especially in enterprise settings, will often have
    the following basic requirements:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes部署，特别是在企业环境中，通常会有以下基本要求：
- en: '**Development and test environments**: At least two clusters to test the impacts
    of changes on the cluster level to applications'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发和测试环境：至少有两个集群，以测试对应用程序的集群级别的更改的影响
- en: '**Developer sandbox**: A place where developers can build containers and test
    them without worrying about impacts on shared namespaces'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发人员沙盒：开发人员可以构建容器并测试它们，而不必担心对共享命名空间的影响
- en: '**Source control and issue tracking**: A place to store code and track open
    tasks'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 源代码控制和问题跟踪：存储代码并跟踪未完成的任务的地方
- en: In addition to these basic requirements, enterprises will often have additional
    requirements, such as regular access reviews, limiting access based on policy,
    and workflows that assign responsibility for actions that could impact a shared
    environment.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些基本要求之外，企业通常还会有额外的要求，例如定期访问审查、基于策略限制访问以及分配对可能影响共享环境的操作负责的工作流程。
- en: 'For our platform, we want to encompass as many of these requirements as possible.
    To better automate deployments onto our platform, we''re going to define each
    application as having the following:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的平台，我们希望尽可能包含这些要求。为了更好地自动化部署到我们的平台，我们将定义每个应用程序具有以下内容：
- en: '**A development namespace**: Developers are administrators.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发命名空间：开发人员是管理员。
- en: '**A production namespace**: Developers are viewers.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生产命名空间：开发人员是查看者。
- en: '**A source control project**: Developers can fork.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**源代码控制项目**：开发人员可以fork。'
- en: '**A build process**: Triggered by updates to Git.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**构建流程**：由对Git的更新触发。'
- en: '**A deploy process**: Triggered by updates to Git.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**部署流程**：由对Git的更新触发。'
- en: In addition, we want our developers to have their own sandbox so that each user
    will get their own namespace for development.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们希望我们的开发人员有他们自己的沙盒，这样每个用户都将获得自己的开发命名空间。
- en: Important Note
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: In a real deployment, you will want to separate your development and production
    environments into separate clusters. This makes it much easier to test cluster-wide
    operations, such as upgrades, without impacting running applications. We're doing
    everything in one cluster to make it easier for you to set up on your own.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际部署中，您会希望将开发和生产环境分开到不同的集群中。这样可以更容易地测试集群范围的操作，例如升级，而不会影响正在运行的应用程序。我们在一个集群中做所有操作是为了让您更容易自行设置。
- en: 'To provide access to each application, we will define three roles:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供对每个应用程序的访问权限，我们将定义三个角色：
- en: '**Owners**: Users that are application owners can approve access for other
    roles inside their application. This role is assigned to the application requestor
    and can be assigned by application owners. Owners are also responsible for pushing
    changes into development and production.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**所有者**：应用程序所有者可以批准其他角色在其应用程序内的访问权限。这个角色由应用程序请求者分配，并可以由应用程序所有者分配。所有者还负责推送更改到开发和生产环境中。'
- en: '**Developers**: These are users that will have access to an application''s
    source control and can administer the application''s development namespace. They
    can view objects in the production namespace but can''t edit anything. This role
    can be requested by any users and is approved by an application owner.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开发人员**：这些用户将可以访问应用程序的源代码控制，并可以管理应用程序的开发命名空间。他们可以查看生产命名空间中的对象，但不能编辑任何内容。任何用户都可以请求此角色，并由应用程序所有者批准。'
- en: '**Operations**: These users have the capabilities as developers, but can also
    make changes to the production namespace as needed. This role can be requested
    by any user and is approved by the application owner.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**运维人员**：这些用户具有开发人员的能力，但也可以根据需要对生产命名空间进行更改。任何用户都可以请求此角色，并由应用程序所有者批准。'
- en: 'We will also create some environment-wide roles:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将创建一些环境范围的角色：
- en: '**System approvers**: Users with this role can approve access to any system-wide
    roles.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**系统审批者**：拥有此角色的用户可以批准对任何系统范围角色的访问权限。'
- en: '**Cluster administrators**: This role is specifically for managing our cluster
    and the applications that comprise our pipeline. It can be requested by anyone
    and must be approved by a member of the system approvers role.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集群管理员**：这个角色专门用于管理我们的集群和构成我们流水线的应用程序。任何人都可以请求此角色，并必须得到系统审批者角色的批准。'
- en: '**Developers**: Anyone who logs in gets their own namespace for development.
    These namespaces cannot be requested for access by other users. These namespaces
    are not directly connected to any CI/CD infrastructure or Git repositories.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开发人员**：任何登录的用户都会获得自己的开发命名空间。这些命名空间不能被其他用户请求访问。这些命名空间与任何CI/CD基础设施或Git存储库没有直接连接。'
- en: Even with our very simple platform, we have six roles that need to be mapped
    to the applications that make up our pipeline. Each application has its own authentication
    and authorization processes that these roles will need to be mapped to. This is
    just one example of why automation is so important to the security of your clusters.
    Provisioning this access manually based on email requests can become unmanageable
    quickly.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们的平台非常简单，我们有六个角色需要映射到构成我们流水线的应用程序上。每个应用程序都有自己的身份验证和授权流程，这些角色需要映射到这些流程上。这就是为什么自动化对于集群安全如此重要的一个例子。根据电子邮件请求手动提供访问权限可能会迅速变得难以管理。
- en: 'The workflow that developers are expected to go through with an application
    will line up with the GitOps flow we designed previously:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 开发人员预期通过应用程序进行的工作流程将与我们之前设计的GitOps流程一致：
- en: Application owners will request an application be created. Once approved, a
    Git repository will be created for application code, pipeline build manifests,
    and Kubernetes manifests. Development and production namespaces will be created
    as well with appropriate **RoleBinding** objects. Groups will be created that
    reflect the roles for each application, with approval for access to those groups
    delegated to the application owner.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序所有者将请求创建一个应用程序。一旦获得批准，将为应用程序代码、流水线构建清单和Kubernetes清单创建一个Git存储库。还将创建开发和生产命名空间，并创建适当的**RoleBinding**对象。将创建反映每个应用程序角色的组，并将访问这些组的批准委托给应用程序所有者。
- en: Developers and operations staff are granted access to the application by either
    requesting it or having it provided directly by an application owner. Once granted
    access, updates are expected in both the developer's sandbox and the development
    namespace. Updates are made in a user's fork for the Git repository, with pull
    requests used to merge code into the main repositories that drive automation.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发人员和运维人员可以通过请求或直接由应用程序所有者提供来获得对应用程序的访问权限。一旦获得访问权限，预期在开发人员的沙盒和开发命名空间中进行更新。更新是在用户的Git存储库分支中进行的，使用拉取请求将代码合并到驱动自动化的主存储库中。
- en: All builds are controlled via "scripts" in the application's source control.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有构建都通过应用程序的源代码中的“脚本”进行控制。
- en: All artifacts are published to a centralized container registry.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有工件都发布到集中式容器注册表中。
- en: All production updates must be approved by application owners.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有生产更新必须得到应用程序所有者的批准。
- en: This basic workflow doesn't include typical components of a workflow, such as
    code and container scans, periodic access recertifications, or requirements for
    privileged access. The topic of this chapter can easily be a complete book on
    its own. The goal isn't to build a complete enterprise platform but to give you
    a starting point for building and designing your own system.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这个基本的工作流程不包括工作流的典型组件，比如代码和容器扫描，定期访问重新认证，或者特权访问的要求。本章的主题很容易成为一本完整的书。目标不是构建一个完整的企业平台，而是为您构建和设计自己的系统提供一个起点。
- en: Choosing our technology stack
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择我们的技术栈
- en: In the previous parts of this section, we talked about pipelines in a generic
    way. Now, let's get into the specifics of what technology is needed in our pipeline.
    We identified earlier that every application has application source code and Kubernetes
    manifest definitions. It also has to build containers. There needs to be a way
    to watch for changes to Git and update our cluster. Finally, we need an automation
    platform so that all these components work together.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的前几部分中，我们以一种通用的方式讨论了流水线。现在，让我们进入我们的流水线所需的技术的具体细节。我们之前确定，每个应用程序都有应用程序源代码和Kubernetes清单定义。它还必须构建容器。需要一种方式来监视Git的更改并更新我们的集群。最后，我们需要一个自动化平台，使所有这些组件能够协同工作。
- en: 'Based on our requirements for our platform, we want technology that has the
    following features:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们对平台的要求，我们希望拥有以下功能的技术：
- en: '**Open source**: We don''t want you to buy anything just for this book!'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开源**：我们不希望您为了这本书而购买任何东西！'
- en: '**API-driven**: We need to be able to provision components and access in an
    automated way.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于API的：我们需要能够以自动化的方式配置组件并访问。
- en: '**Has a visual component that supports external authentication**: This book
    focuses on enterprise, and everyone in the enterprise loves their GUIs. Just not
    having different credentials for each application.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**具有支持外部身份验证的视觉组件**：这本书侧重于企业，企业中的每个人都喜欢他们的图形用户界面。只是不想为每个应用程序使用不同的凭据。'
- en: '**Supported on Kubernetes**: This is a book on Kubernetes.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在Kubernetes上支持**：这是一本关于Kubernetes的书。'
- en: 'To meet these requirements, we''re going to deploy the following components
    to our cluster:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足这些要求，我们将在我们的集群中部署以下组件：
- en: '**Git Registry – GitLab**: GitLab is a powerful system that provides a great
    UI and experience for working with Git that supports external authentication (that
    is, **Single Sign-On (SSO)**. It has integrated issue management and an extensive
    API. It also has a Helm chart that we have tailored for the book to run a minimal
    install.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Git注册表 - GitLab**：GitLab是一个强大的系统，提供了一个很好的UI和体验，用于与支持外部身份验证的Git一起工作（即**单点登录（SSO）**）。它集成了问题管理和广泛的API。它还有一个Helm图表，我们已经为本书定制了一个最小的安装。'
- en: '**Automated Builds – Tekton**: Originally the build portion of the Knative
    project for Kubernetes function-as-a-service deployments, Tekton was spun off
    into its own project to provide build services for generic applications. It runs
    in Kubernetes with all interactions being via the Kubernetes API. There''s an
    early stage dashboard too!'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动化构建 - Tekton**：最初是Kubernetes函数即服务部署的Knative项目的构建部分，Tekton被分离出来成为自己的项目，为通用应用程序提供构建服务。它在Kubernetes中运行，所有交互都通过Kubernetes
    API进行。它也有一个早期阶段的仪表板！'
- en: '**Container Registry – simple Docker registry**: There are many very capable
    open source registries. Since this deployment will get complex quickly, we decided
    just to use the registry provided by Docker. There won''t be any security on it,
    so don''t use it in production!'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容器注册表 - 简单的Docker注册表**：有许多功能强大的开源注册表。由于这个部署很快就会变得复杂，我们决定只使用Docker提供的注册表。它不会有任何安全性，所以不要在生产中使用！'
- en: '**GitOps – ArgoCD**: ArgoCD is a collaboration between Intuit and Weaveworks
    to build a feature-rich GitOps platform. It''s Kubernetes native, has its own
    API, and stores its objects as Kubernetes custom resources, making it easier to
    automate. Its UI and CLI tools both integrate with SSO using OpenID Connect.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GitOps - ArgoCD**：ArgoCD是Intuit和Weaveworks之间合作构建功能丰富的GitOps平台。它是Kubernetes本地的，有自己的API，并将其对象存储为Kubernetes自定义资源，使自动化变得更容易。它的UI和CLI工具都使用OpenID
    Connect与SSO集成。'
- en: '**Access, authentication, and automation – OpenUnison**: We''ll continue to
    use OpenUnison for authentication into our cluster. We''re also going to integrate
    the UI components of our technology stack as well to provide a single portal for
    our platform. Finally, we''ll use OpenUnison''s workflows to manage access to
    each system based on our role structure and provision the objects needed for everything
    to work together. Access will be provided via OpenUnison''s self-service portal.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**访问、身份验证和自动化 - OpenUnison**：我们将继续使用OpenUnison进行对集群的身份验证。我们还将整合我们技术堆栈的UI组件，以提供一个平台的单一门户。最后，我们将使用OpenUnison的工作流根据我们的角色结构管理对每个系统的访问，并提供一切工作所需的对象。访问将通过OpenUnison的自助门户提供。'
- en: Reading through this technology stack, you might ask "Why didn't you choose
    *XYZ*?" The Kubernetes ecosystem is diverse with no shortage of great projects
    and products for your cluster. This is by no means a definitive stack, nor is
    it even a "recommended" stack. It's a collection of applications that meets our
    requirements and lets us focus on the processes being implemented, rather than
    learning a specific technology.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读这个技术栈时，你可能会问：“为什么你没有选择*XYZ*？”Kubernetes生态系统多样且拥有众多出色的项目和产品供您的集群使用。这绝不是一个确定的技术栈，甚至不是一个“推荐”的技术栈。这是一个满足我们需求并让我们专注于正在实施的流程的应用程序集合，而不是学习特定技术。
- en: You might also find that there's quite a bit of overlap between even the tools
    in this stack. For instance, GitLab has GitOps capabilities and its own build
    system, but we chose not to use them for this chapter. We did that so that you
    can see how to tie different systems together to build a platform. Your platform
    may use GitHub's SaaS solution for source control but run builds internally and
    combine with Amazon's container registry. We wanted you to see how these systems
    can be connected to build a platform instead of focusing on specific tools.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能还会发现，即使在这个技术栈中，工具之间也存在相当大的重叠。例如，GitLab具有GitOps功能和自己的构建系统，但我们选择不在本章中使用它们。我们这样做是为了让您看到如何将不同的系统连接在一起构建平台。您的平台可能会使用GitHub的SaaS解决方案进行源代码控制，但在内部运行构建，并与亚马逊的容器注册表结合。我们希望您看到这些系统如何连接在一起构建平台，而不是专注于特定工具。
- en: This section was a very deep exploration of the theory behind pipeline design
    and looking at common requirements for building a Kubernetes-based platform. We
    identified technology components that can implement those requirements and why
    we chose them. With this knowledge in hand, it's time to build!
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分是对管道设计理论的深入探讨，以及对构建基于Kubernetes的平台的常见要求的审视。我们确定了可以实现这些要求的技术组件以及我们为什么选择它们。有了这些知识，现在是时候开始构建了！
- en: Preparing our cluster
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备我们的集群
- en: Before we begin deploying our technology stack, we need to do a couple of things.
    I recommend starting with a fresh cluster. If you're using the KinD cluster from
    this book, start with a new cluster. We're deploying several components that need
    to be integrated and it will be simpler and easier to start fresh rather than
    potential struggling with previous configurations. Before we start deploying the
    applications that will make up our stack, we're going to deploy JetStack's cert-manager
    to automate certificate issuing, a simple container registry, and OpenUnison for
    authentication and automation.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始部署技术栈之前，我们需要做一些事情。我建议从一个新的集群开始。如果您正在使用本书中的KinD集群，请从一个新的集群开始。我们正在部署几个需要集成的组件，最好从头开始，而不是可能与先前的配置斗争。在我们开始部署构成我们技术栈的应用程序之前，我们将部署JetStack的cert-manager来自动化证书签发，一个简单的容器注册表，以及用于身份验证和自动化的OpenUnison。
- en: Deploying cert-manager
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署cert-manager
- en: JetStack, a Kubernetes-focused consulting company, created a project called
    **cert-manager** to make it easier to automate the creation and renewal of certificates.
    This project works by letting you define issuers using Kubernetes custom resources
    and then using annotations on **Ingress** objects to generate certificates using
    those issuers. The end result is a cluster running with properly managed and rotated
    certificates without generating a single **certificate signing request** (**CSR**)
    or worrying about expiration!
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: JetStack是一家专注于Kubernetes的咨询公司，他们创建了一个名为**cert-manager**的项目，以便更容易地自动创建和更新证书。该项目通过让您使用Kubernetes自定义资源定义发行者，然后在**Ingress**对象上使用注释来使用这些发行者生成证书来工作。最终结果是集群运行正常管理和轮换证书，而不需要生成一个**证书签名请求**（**CSR**）或担心过期！
- en: The **cert-manager** project is most often mentioned with *Let's Encrypt* (https://letsencrypt.org/)
    to automate the publishing of certificates that have been signed by a commercially
    recognized certificate authority for free (as in beer). This is possible because
    *Let's Encrypt* automates the process. The certificates are only good for 90 days
    and the entire process is API-driven. In order to drive this automation, you must
    have some way of letting *Let's Encrypt* verify ownership of the domain you are
    trying to get a certificate for. Throughout this book, we have used **nip.io**
    to simulate DNS. If you have a DNS service that you can use and is supported by
    **cert-manager**, such as Amazon's Route 53, then this is a great solution.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**cert-manager**项目通常与*Let''s Encrypt*（https://letsencrypt.org/）一起提到，以自动发布由商业认可的证书颁发机构免费签名的证书（就像啤酒一样）。这是可能的，因为*Let''s
    Encrypt*自动化了这个过程。证书只有90天的有效期，整个过程都是API驱动的。为了驱动这种自动化，您必须有一种让*Let''s Encrypt*验证您正在尝试获取证书的域的所有权的方法。在本书中，我们使用**nip.io**来模拟DNS。如果您有一个可以使用并且受**cert-manager**支持的DNS服务，比如亚马逊的Route
    53，那么这是一个很好的解决方案。'
- en: Since we're using **nip.io**, we will deploy **cert-manager** with a self-signed
    certificate authority. This gives us the benefit of having a certificate authority
    that can quickly generate certificates without having to worry about domain validation.
    We will then instruct our workstation to trust this certificate as well as the
    applications we deploy so that everything is secured using properly built certificates.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用**nip.io**，我们将部署**cert-manager**与自签名证书颁发机构。这使我们能够拥有一个证书颁发机构，可以快速生成证书，而不必担心域验证的问题。然后，我们将指示我们的工作站信任此证书以及我们部署的应用程序，以便一切都使用正确构建的证书进行保护。
- en: Important Note
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Using a self-signed certificate authority is a common practice for most enterprises
    for internal deployments. This avoids having to deal with potential validation
    issues where a commercially signed certificate won't provide much value. Most
    enterprises are able to distribute an internal certificate authority's certificates
    via their Active Directory infrastructure. Chances are your enterprise has a way
    to request either an internal certificate or a wildcard that could be used too.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数企业来说，使用自签名证书颁发机构是内部部署的常见做法。这避免了处理商业签名证书无法提供太多价值的潜在验证问题。大多数企业能够通过其Active
    Directory基础架构分发内部证书颁发机构的证书。很可能您的企业也有一种方式可以请求内部证书或可用的通配符。
- en: 'The steps to deploy **cert-manager** are as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 部署**cert-manager**的步骤如下：
- en: 'From your cluster, deploy the **cert-manager** manifests:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从您的集群中，部署**cert-manager**清单：
- en: '**$ kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v0.16.1/cert-manager.yaml**'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**$ kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v0.16.1/cert-manager.yaml**'
- en: 'Once the Pods are running in the **cert-manager** namespace, create a self-signed
    certificate that we''ll use as our certificate authority. In the **chapter14/shell**
    directory of the Git repository for this book is a script called **makeca.sh**
    that will generate this certificate for you:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦Pod在**cert-manager**命名空间中运行，创建一个自签名证书，我们将用作我们的证书颁发机构。在本书的Git存储库的**chapter14/shell**目录中有一个名为**makeca.sh**的脚本，它将为您生成此证书：
- en: '**$ cd Kubernetes-and-Docker-The-Complete-Guide/chapter14/shell/**'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**$ cd Kubernetes-and-Docker-The-Complete-Guide/chapter14/shell/**'
- en: '**$ sh ./makeca.sh**'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**$ sh ./makeca.sh**'
- en: '**Generating RSA private key, 2048 bit long modulus (2 primes)**'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**生成RSA私钥，2048位长模数（2个质数）**'
- en: '**.............................................................................................................................................+++++**'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**.............................................................................................................................................+++++**'
- en: '**....................+++++**'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '....................+++++'
- en: '**e is 65537 (0x010001)**'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: e为65537（0x010001）
- en: 'There is now an SSL directory with a certificate and a key. The next step is
    to create a secret from these files that will become our certificate authority:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在有一个带有证书和密钥的SSL目录。下一步是从这些文件创建一个secret，这将成为我们的证书颁发机构：
- en: '**$ cd ssl/**'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: $ cd ssl/
- en: '**$ kubectl create secret tls ca-key-pair --key=./tls.key --cert=./tls.crt
    -n cert-manager**'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: $ kubectl create secret tls ca-key-pair --key=./tls.key --cert=./tls.crt -n
    cert-manager
- en: '**secret/ca-key-pair created**'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: secret/ca-key-pair已创建
- en: 'Next, create the **ClusterIssuer** object so that all of our **Ingress** objects
    can have properly minted certificates:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，创建ClusterIssuer对象，以便所有的Ingress对象都可以拥有正确颁发的证书：
- en: '**$ cd ../../yaml/**'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: $ cd ../../yaml/
- en: '**$ kubectl create -f ./certmanager-ca.yaml**'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: $ kubectl create -f ./certmanager-ca.yaml
- en: '**clusterissuer.cert-manager.io/ca-issuer created**'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: clusterissuer.cert-manager.io/ca-issuer已创建
- en: 'With **ClusterIssuer** created, any **Ingress** object with the **cert-manager.io/cluster-issuer:
    "ca-issuer"** annotation will have a certificate signed by our authority created
    for them. One of the components we will be using for this is our container registry.
    Kubernetes uses Docker''s underlying mechanisms for pulling containers, and KinD
    will not pull images from registries running without TLS or using an untrusted
    certificate. To get around this issue, we need to import our certificate into
    both our worker and nodes:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '创建了ClusterIssuer后，任何带有cert-manager.io/cluster-issuer: "ca-issuer"注释的Ingress对象都将获得我们为它们创建的由我们授权签名的证书。我们将用于此的一个组件是我们的容器注册表。Kubernetes使用Docker的底层机制来拉取容器，并且KinD不会从没有TLS或使用不受信任证书的注册表中拉取镜像。为了解决这个问题，我们需要将我们的证书导入到我们的worker和节点中：'
- en: '**$ cd ~/**'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: $ cd ~/
- en: '**$ kubectl get secret ca-key-pair -n cert-manager -o json | jq -r ''.data["tls.crt"]''
    | base64 -d > internal-ca.crt**'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: $ kubectl get secret ca-key-pair -n cert-manager -o json | jq -r '.data["tls.crt"]'
    | base64 -d > internal-ca.crt
- en: '**$ docker cp internal-ca.crt cluster01-worker:/usr/local/share/ca-certificates/internal-ca.crt**'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: $ docker cp internal-ca.crt cluster01-worker:/usr/local/share/ca-certificates/internal-ca.crt
- en: '**$ docker exec -ti cluster01-worker update-ca-certificates**'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: $ docker exec -ti cluster01-worker update-ca-certificates
- en: '**Updating certificates in /etc/ssl/certs...**'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在/etc/ssl/certs中更新证书...
- en: '**1 added, 0 removed; done.**'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 1个添加，0个移除；完成。
- en: '**Running hooks in /etc/ca-certificates/update.d...**'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在/etc/ca-certificates/update.d中运行钩子...
- en: '**done.**'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 完成。
- en: '**$ docker restart cluster01-worker**'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: $ docker restart cluster01-worker
- en: '**$ docker cp internal-ca.crt cluster01-control-plane:/usr/local/share/ca-certificates/internal-ca.crt**'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: $ docker cp internal-ca.crt cluster01-control-plane:/usr/local/share/ca-certificates/internal-ca.crt
- en: '**$ docker exec -ti cluster01-control-plane update-ca-certificates**'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: $ docker exec -ti cluster01-control-plane update-ca-certificates
- en: '**Updating certificates in /etc/ssl/certs...**'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在/etc/ssl/certs中更新证书...
- en: '**1 added, 0 removed; done.**'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 1个添加，0个移除；完成。
- en: '**Running hooks in /etc/ca-certificates/update.d...**'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在/etc/ca-certificates/update.d中运行钩子...
- en: '**done.**'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 完成。
- en: '**$ docker restart cluster01-control-plane**'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: $ docker restart cluster01-control-plane
- en: The first command extracts the certificate from the secret we created to host
    the certificate. The next set of commands copies the certificate to each container,
    instructs the container to trust it, and finally, restarts the container. Once
    your containers are restarted, wait for all the Pods to come back; it could take
    a few minutes.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个命令从我们创建的secret中提取证书。接下来的一系列命令将证书复制到每个容器中，指示容器信任它，并最后重新启动容器。一旦容器重新启动，等待所有Pod重新启动；可能需要几分钟。
- en: Important Note
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Now would be a good time to download **internal-ca.crt**; install it onto your
    local workstation and potentially into your browser of choice. Different operating
    systems and browsers do this differently, so check the appropriate documentation
    on how to do this. Trusting this certificate will make things much easier when
    interacting with applications, pushing containers, and using command-line tools.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是下载**internal-ca.crt**的好时机；将其安装到您的本地工作站，可能还要安装到您选择的浏览器中。不同的操作系统和浏览器的操作方式不同，因此请查阅相应的文档了解如何操作。信任此证书将使与应用程序交互、推送容器和使用命令行工具变得更加容易。
- en: With **cert-manager** ready to issue certificates and both your cluster and
    your workstation trusting those certificates, the next step is to deploy a container
    registry.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 有了**cert-manager**准备好颁发证书，以及您的集群和您的工作站信任这些证书，下一步是部署容器注册表。
- en: Deploying the Docker container registry
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署Docker容器注册表
- en: 'Docker, Inc. provides a simple registry. There is no security on this registry,
    so it is most certainly not a good option for production use. The **chapter14/yaml/docker-registry.yaml**
    file will deploy the registry for us and create an **Ingress** object. Before
    deploying, edit this file, changing all instances of **192-168-2-140** to a dash
    representation of your cluster''s IP address. For instance, my cluster is running
    on **192.168.2.114**, so I will replace **192-168-2-140** with **192-168-2-114**.
    Then, run **kubectl create** on the manifest to create the registry:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: Docker, Inc.提供了一个简单的注册表。这个注册表没有安全性，因此绝对不是生产使用的好选择。**chapter14/yaml/docker-registry.yaml**文件将为我们部署注册表并创建一个**Ingress**对象。在部署之前，编辑此文件，将所有的**192-168-2-140**实例更改为集群IP地址的破折号表示法。例如，我的集群正在运行**192.168.2.114**，所以我将**192-168-2-140**替换为**192-168-2-114**。然后，运行**kubectl
    create**在清单上创建注册表：
- en: $ kubectl create -f ./docker-registry.yaml
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: $ kubectl create -f ./docker-registry.yaml
- en: namespace/docker-registry created
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: namespace/docker-registry created
- en: statefulset.apps/docker-registry created
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: statefulset.apps/docker-registry created
- en: service/docker-registry created
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: service/docker-registry created
- en: ingress.extensions/docker-registry created
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ingress.extensions/docker-registry created
- en: 'Once the registry is running, you can try accessing it from your browser:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 注册表运行后，您可以尝试从浏览器访问它：
- en: '![Figure 14.5 – Accessing the container registry in a browser'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '![图14.5 - 在浏览器中访问容器注册表'
- en: '](image/Fig_14.5_B15514.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '](image/Fig_14.5_B15514.jpg)'
- en: Figure 14.5 – Accessing the container registry in a browser
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.5 - 在浏览器中访问容器注册表
- en: You won't see much since the registry has no web UI, but you also shouldn't
    get a certificate error. That's because we deployed **cert-manager** and are issuing
    signed certificates! With our registry running, the last component to deploy is
    OpenUnison.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 您不会看到太多，因为注册表没有Web UI，但您也不应该收到证书错误。这是因为我们部署了**cert-manager**并颁发了签名证书！当我们的注册表运行时，部署的最后一个组件是OpenUnison。
- en: Deploying OpenUnison
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署OpenUnison
- en: In [*Chapter 7*](B15514_07_Final_ASB_ePub.xhtml#_idTextAnchor203), *Integrating
    Authentication into Your Cluster*, we introduced OpenUnison to authenticate access
    to our KinD deployment. OpenUnison comes in two flavors. The first, which we have
    already deployed, is a login portal that lets us authenticate using a central
    source and pass group information to our RBAC policies. The second is an automation
    portal that we'll use as the basis for integrating the systems that will manage
    our pipeline. This portal will also give us a central UI for requesting projects
    to be created and managing access to our project's systems.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: We defined that each project we deploy will have three "roles" that will span
    several systems. Will your enterprise let you create and manage groups for every
    project we create? Some might, but Active Directory is a critical component in
    most enterprises, and write access can be difficult to get. It's unlikely that
    the people who run your Active Directory are the same people who you report to
    when managing your cluster, complicating your ability to get an area of Active
    Directory that you have administrative rights in. The OpenUnison automation portal
    lets you manage access with local groups that can be easily queried, just like
    with Active Directory, but you have control to manage them. We'll still authenticate
    against our central SAML provider, though.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'To facilitate OpenUnison''s automation capabilities, we need to deploy a database
    to store persistent data and an SMTP server to notify users when they have open
    requests or when requests have been completed. For the database, we''ll deploy
    the open source MariaDB. For an **Simple Mail Transfer Protocol** (**SMTP**) (email)
    server, most enterprises have very strict rules about sending emails. We don''t
    want to have to worry about getting email set up for notifications, so we''ll
    run a "black hole" email service that just disregards all SMTP requests:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: First, run the **chapter14/yaml/mariadb.yaml** manifest from the book's GitHub
    repository. No changes need to be made.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, deploy the SMTP black hole:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ kubectl create ns blackhole**'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '**namespace/blackhole created**'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '**$ kubectl create deployment blackhole --image=tremolosecurity/smtp-blackhole
    -n blackhole**'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '**deployment.apps/blackhole created**'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '**$ kubectl expose deployment/blackhole --type=ClusterIP --port 1025 --target-port=1025
    -n blackhole**'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '**service/blackhole exposed**'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: With MariaDB and our SMTP service deployed, we're able to deploy OpenUnison.
    Follow *steps 1–5* in the *Deploying OpenUnison* section of [*Chapter 7*](B15514_07_Final_ASB_ePub.xhtml#_idTextAnchor203),
    *Integrating Authentication into Your Cluster*, to deploy the OpenUnison operator
    and Kubernetes dashboard.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, create a **Secret** to store credentials for accessing MariaDB and the
    SMTP service. We hardcoded passwords into our deployment for MariaDB for simplicity''s
    sake, so make sure to generate long, random passwords for your production database
    account! Create the following **Secret** in your cluster:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'apiVersion: v1'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'type: Opaque'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'name: orchestra-secrets-source'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'namespace: openunison'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: 'data:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'K8S_DB_SECRET: aW0gYSBzZWNyZXQ='
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'SMTP_PASSWORD: ""'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'OU_JDBC_PASSWORD: c3RhcnR0MTIz'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'unisonKeystorePassword: aW0gYSBzZWNyZXQ='
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 'kind: Secret'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: We're going to reuse the Helm values we used in *step 2* in the *Configuring
    your cluster for impersonation* section of [*Chapter 7*](B15514_07_Final_ASB_ePub.xhtml#_idTextAnchor203),
    *Integrating Authentication into Your Cluster*, with three changes.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: First, change the image from **docker.io/tremolosecurity/openunison-k8s-login-saml2:latest**
    to **docker.io/tremolosecurity/openunison-k8s-saml2:latest**.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, Base64-encode your **internal-ca.crt** file into a single line and add
    it to the **trusted_certs** section of **values.yaml**:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ base64 -w 0 < internal-ca.crt**'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '**LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0 tCk1JSUREVENDQWZXZ0F3SUJ…**'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'Add SMTP and database sections. The updates to **values.yaml** will look as
    follows. I removed most of the unchanged portions to save space:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'trusted_certs:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '- name: internal-ca'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 'pem_b64: LS0tLS1CRUdJTiB…'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'saml:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 'idp_url: https://portal.apps.tremolo.io/idp-test/metadata/dfbe4040-cd32-470e-a9b6-809c8f857c40'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata_xml_b64: ""'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'database:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'hibernate_dialect: org.hibernate.dialect.MySQL5InnoDBDialect'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 'quartz_dialect: org.quartz.impl.jdbcjobstore.StdJDBCDelegate'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'driver: com.mysql.jdbc.Driver'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 'url: jdbc:mysql://mariadb.mariadb.svc.cluster.local:3306/unison'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'user: unison'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: 'validation: SELECT 1'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'smtp:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'host: blackhole.blackhole.svc.cluster.local'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: 'port: 1025'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'user: none'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'from: donotreply@domain.com'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: 'tls: false'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 'Deploy OpenUnison using the Helm chart:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ helm install orchestra tremolo/openunison-k8s-saml2 --namespace openunison
    -f ./openunison-values.yaml**'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: 'Once OpenUnison is deployed, edit the **orchestra** OpenUnison object to remove
    the **unison-ca** key. Remove the block that looks like this:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '- create_data:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 'ca_cert: true'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'key_size: 2048'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 'server_name: k8sou.apps.192-168-2-114.nip.io'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'sign_by_k8s_ca: false'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'subject_alternative_names:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '- k8sdb.apps.192-168-2-114.nip.io'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '- k8sapi.apps.192-168-2-114.nip.io'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 'import_into_ks: certificate'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'name: unison-ca'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 'tls_secret_name: ou-tls-certificate'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'Delete the **ou-tls-certificate** **Secret**:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ kubectl delete secret ou-tls-certificate -n openunison**'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '**secret "ou-tls-certificate" deleted**'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 'Edit the **openunison** **Ingress** object, adding **cert-manager.io/cluster-issuer:
    ca-issuer** to the list of **annotations**.'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Complete the SSO integration with the testing identity provider using *steps
    4–6* from the *Configuring your cluster for impersonation* section of [*Chapter
    7*](B15514_07_Final_ASB_ePub.xhtml#_idTextAnchor203), *Integrating Authentication
    into Your Cluster*.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Log in to OpenUnison, then log out.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The OpenUnison automation portal doesn''t do anything with the groups from
    the testing identity provider. In order to become a cluster administrator, you
    must be "bootstrapped" into the environment''s groups:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ kubectl exec -ti mariadb-0 -n mariadb -- mysql -u \**'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '**  unison --password=''startt123'' \**'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '**  -e "insert into userGroups (userId,groupId) values (2,1);" \**'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '**  unison**'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '**$ kubectl exec -ti mariadb-0 -n mariadb -- mysql -u \**'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '**  unison --password=''startt123'' \**'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '**  -e "insert into userGroups (userId,groupId) values (2,2);" \  unison**'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Finally, log back in. You will be a global administrator and a cluster administrator
    for your cluster.
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With OpenUnison deployed, you can now remotely administer your cluster. Depending
    on how you are accessing your cluster, it may be easier to use your workstation
    to directly manage your cluster for the rest of the steps in this chapter.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: You'll notice that there are different "badges" in OpenUnison now. In addition
    to getting a token or accessing the dashboard, you can request a new namespace
    to be created or access the ActiveMQ dashboard. You'll also see that the title
    bar has additional options, such as **Request Access**. OpenUnison will become
    our self-service portal for deploying our pipelines without having to manually
    create objects in our applications or cluster. We're not going to go into these
    in detail until we talk about using OpenUnison to automate the deployment of our
    pipelines.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: With your cluster prepared, the next step is to deploy the components for our
    pipeline.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Deploying GitLab
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When building a GitOps pipeline, one of the most important components is a Git
    repository. GitLab has many components besides just Git, including a UI for navigating
    code, a web-based **integrated development environment** (**IDE**) for editing
    code, and a robust identity implementation to manage access to projects in a multi-tenant
    environment. This makes it a great solution for our platform since we can map
    our "roles" to GitLab groups.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we're going to deploy GitLab into our cluster and create two
    simple repositories that we'll use later when we deploy Tekton and ArgoCD. We'll
    focus on the automation steps when we revisit OpenUnison to automate our pipeline
    deployments.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 'GitLab deploys with a Helm chart. For this book, we built a custom **values**
    file to run a minimal install. While GitLab comes with features that are similar
    to ArgoCD and Tekton, we won''t be using them. We also didn''t want to worry about
    high availability. Let''s begin:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new namespace called **gitlab**:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ kubectl create ns gitlab**'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '**namespace/gitlab created**'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to add our certificate authority as a secret for GitLab to trust talking
    to OpenUnison and the webhooks we will eventually create for Tekton:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ kubectl get secret ca-key-pair \**'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '**  -n cert-manager -o json | jq -r ''.data["tls.crt"]'' \**'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '**  | base64 -d > tls.crt**'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '**$ kubectl create secret generic \**'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '**  internal-ca --from-file=. -n gitlab**'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: 'Open **chapter14/gitlab/secret/provider** in your favorite text editor. Replace
    **local.tremolo.dev** with the full domain suffix for your cluster. For instance,
    my cluster is running on **192.168.2.114**, so I''m using the **apps.192-168-2-114.nip.io**
    suffix. Here''s my updated **Secret**:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'name: openid_connect'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 'label: OpenUnison'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 'args:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'name: openid_connect'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'scope:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '- openid'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '- profile'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'response_type: code'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: 'issuer: **https://k8sou.apps.192-168-2-114.nip.io/auth/idp/k8sIdp**'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: 'discovery: true'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: 'client_auth_method: query'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 'uid_field: sub'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 'send_scope_to_token_endpoint: false'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: 'client_options:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 'identifier: gitlab'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: 'secret: secret'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 'redirect_uri: **https://gitlab.apps.192-168-2-114.nip.io/users/auth/openid_connect/callback**'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: We're using a client secret of **secret**. This should not be done for a production
    cluster. If you're deploying GitLab into production using our templates as a starting
    point, make sure to change this.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the **secret** for GitLab to integrate with OpenUnison for SSO. We''ll
    finish the process when we revisit OpenUnison:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ kubectl create secret generic gitlab-oidc --from-file=. -n gitlab**'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '**secret/gitlab-oidc created**'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Edit **chapter14/yaml/gitlab-values.yaml**. Just as in *step 3*, replace **local.tremolo.dev**
    with the full domain suffix for your cluster. For instance, my cluster is running
    on **192.168.2.114**, so I'm using the **apps.192-168-2-114.nip.io** suffix.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If your cluster is running on a single virtual machine, now would be a good
    time to create a snapshot. If something goes wrong during the GitLab deployment,
    it's easier to revert back to a snapshot since the Helm chart doesn't do a great
    job of cleaning up after itself on a delete.
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add the chart to your local repository and deploy GitLab:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ helm repo add gitlab https://charts.gitlab.io**'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '**$ "gitlab" has been added to your repositories**'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '**$ helm install gitlab gitlab/gitlab -n gitlab -f chapter14/yaml/gitlab-values.yaml**'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '**NAME: gitlab**'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '**LAST DEPLOYED: Sat Aug  8 14:50:13 2020**'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '**NAMESPACE: gitlab**'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '**STATUS: deployed**'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '**REVISION: 1**'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '**WARNING: Automatic TLS certificate generation with cert-manager is disabled
    and no TLS certificates were provided. Self-signed certificates were generated.**'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: 'It will take a few minutes to run. Even once the Helm chart has been installed,
    it can take 15–20 minutes for all the Pods to finish deploying. While GitLab is
    initializing, we need to update the web frontend''s **Ingress** object to use
    a certificate signed by our certificate authority. Edit the **gitlab-webservice**
    **Ingress** object in the **gitlab** namespace. Change the **kubernetes.io/ingress.class:
    gitlab-nginx** annotation to **kubernetes.io/ingress.class: nginx**. Also, change
    **secretName** from **gitlab-wildcard-tls** to **gitlab-web-tls**:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'apiVersion: extensions/v1beta1'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 'kind: Ingress'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: 'annotations:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: 'cert-manager.io/cluster-issuer: ca-issuer'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '**kubernetes.io/ingress.class: nginx**'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: 'kubernetes.io/ingress.provider: nginx'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: .
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: .
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: .
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: 'tls:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '- hosts:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '- gitlab.apps.192-168-2-114.nip.io'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '**secretName: gitlab-web-tls**'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: 'status:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: 'loadBalancer: {}'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: 'We next need to update our GitLab shell to accept SSH connections on port **2222**.
    This way, we can commit code without having to worry about blocking SSH access
    to your KinD server. Edit the **gitlab-gitlab-shell** **Deployment** in the **gitlab**
    namespace. Find **containerPort: 2222** and insert **hostPort: 2222** underneath,
    making sure to maintain the spacing. Once the Pod relaunches, you''ll be able
    to SSH to your GitLab hostname on port **2222**.'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To get your root password to log in to GitLab, get it from the secret that
    was generated:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ kubectl get secret gitlab-gitlab-initial-root-password -o json -n gitlab
    | jq -r ''.data.password'' | base64 -d**'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '**10xtSWXfbvH5umAbCk9NoN0wAeYsUo9jRVbXrfLn KbzBoPLrCGZ6kYRe8wdREcDl**'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: You now can log in to your GitLab instance by going to **https://gitlab.apps.x-x-x-x.nip.io**,
    where **x-x-x-x** is the IP of your server. Since my server is running on **192.168.2.114**,
    my GitLab instance is running on **https://gitlab.apps.192-168-2-114.nip.io/**.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: Creating example projects
  id: totrans-305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To explore Tekton and ArgoCD, we will create two projects. One will be for
    storing a simple Python web service, while the other will store the manifests
    for running the service. Let''s begin:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: The top of the GitLab screen will ask you to add an SSH key. Do that now so
    that we can commit code. Since we're going to be centralizing authentication via
    SAML, GitLab won't have a password for authentication.
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a project and call it **hello-python**. Keep the visibility **private**.
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Clone the project using SSH. Because we're running on port **2222**, we need
    to change the URL provided by GitLab to be a proper SSH URL. For instance, my
    GitLab instance gives me the URL [git@gitlab.apps.192-168-2-114.nip.io:root/hello-python.git](mailto:git@gitlab.apps.192-168-2-114.nip.io:root/hello-python.git).
    This needs to be changed to [ssh://git@gitlab.apps.192-168-2-114.nip.io:2222/root/hello-python.git](mailto:ssh://git@gitlab.apps.192-168-2-114.nip.io:2222/root/hello-python.git).
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once cloned, copy the contents of **chapter14/python-hello** into your repository
    and push to GitLab:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ cd chapter14/python-hello**'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '**$ git archive --format=tar HEAD > /path/to/hello-python/data.tar**'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '**$ cd /path/to/hello-python**'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '**$ tar -xvf data.tar**'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '**README.md**'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: '**source/**'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '**source/Dockerfile**'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: '**source/helloworld.py**'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: '**source/requirements.txt**'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '**$ git add ***'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '**$ git commit -m ''initial commit''**'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '**$ git push**'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: In GitLab, create another project called **hello-python-operations** with visibility
    set to private. Clone this project and copy the contents of **chapter14/python-hello-operations**
    into the repository, and then push it.
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that GitLab is deployed with some example code, we are able to move on to
    the next step, building an actual pipeline!
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Tekton
  id: totrans-325
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tekton is the pipeline system we're using for our platform. Originally part
    of the Knative project for building function-as-a-service on Kubernetes, Tekton
    was broken out into its own project. The biggest difference between Tekton and
    other pipeline technologies you may have run is that Tekton is Kubernetes-native.
    Everything from its execution system, definition, and webhooks for automation
    are able to run on just about any Kubernetes distribution you can find. For example,
    we'll be running it in KinD and Red Hat has moved to Tekton as the main pipeline
    technology used for OpenShift starting in 4.1\.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of deploying Tekton is pretty straightforward. Tekton is a series
    of operators that look for the creation of custom resources that define a build
    pipeline. The deployment itself only takes a couple of **kubectl** commands:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: $ kubectl apply --filename \  https://storage.googleapis.com/tekton-releases/pipeline/latest/release.yaml
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: $ kubectl apply --filename \ https://storage.googleapis.com/tekton-releases/triggers/latest/release.yaml
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: The first command deploys the base system needed to run Tekton pipelines. The
    second command deploys the components needed to build webhooks so that pipelines
    can be launched as soon as code is pushed. Once both commands are done and the
    Pods in the **tekton-pipelines** namespace are running, you're ready to start
    building a pipeline! We'll use our Python Hello World web service as an example.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: Building Hello World
  id: totrans-331
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our Hello World application is really straightforward. It''s a simple service
    that echoes back the obligatory "hello" and the host the service is running on
    just so we feel like our service is doing something interesting. Since the service
    is written in Python, we don''t need to "build" a binary, but we do want to build
    a container. Once the container is built, we want to update the Git repository
    for our running namespace and let our GitOps system reconcile the change to redeploy
    our application. The steps for our build will be as follows:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: Check out our latest code.
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a tag based on a timestamp.
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build our image.
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Push to our registry.
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Patch a Deployment YAML file in the **operations** namespace.
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We''ll build our pipeline one object at a time. The first set of tasks is to
    create an SSH key that Tekton will use to pull our source code:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: 'Create an SSH key pair that we''ll use for our pipeline to check out our code.
    When prompted for a passphrase, just hit *Enter* to skip adding a passphrase:'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ ssh-keygen -f ./gitlab-hello-python**'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: Log in to GitLab and navigate to the **hello-python** project we created. Click
    on **Settings** | **Repository** | **Deploy Keys**, and click **Expand**. Use
    **tekton** as the title and paste the contents of the **github-hello-python.pub**
    file you just created into the **Key** section. Keep **Write access allowed**
    *unchecked* and click **Add Key**.
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, create the **python-hello-build** namespace and the following secret.
    Replace the **ssh-privatekey** attribute with the Base64-encoded content of the
    **gitlab-hello-python** file we created in *step 1*. The annotation is what tells
    Tekton which server to use this key with. The server name is the **Service** in
    the GitLab namespace:'
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'apiVersion: v1'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: 'data:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: 'ssh-privatekey: ...'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: 'kind: Secret'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: 'annotations:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: 'tekton.dev/git-0: gitlab-gitlab-shell.gitlab.svc.cluster.local'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: 'name: git-pull'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: 'namespace: python-hello-build'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: 'type: kubernetes.io/ssh-auth'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: 'Create an SSH key pair that we''ll use for our pipeline to push to the **operations**
    repository. When prompted for a passphrase, just hit *Enter* to skip adding a
    passphrase:'
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ ssh-keygen -f ./gitlab-hello-python-operations**'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: Log in to GitLab and navigate to the **hello-python-operations** project we
    created. Click on **Settings** | **Repository** | **Deploy Keys**, and click **Expand**.
    Use **tekton** as the title and paste the contents of the **github-hello-python-operations.pub**
    file you just created into the **Key** section. Make sure **Write access allowed**
    is *checked* and click **Add Key**.
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, create the following secret. Replace the **ssh-privatekey** attribute
    with the Base64-encoded content of the **gitlab-hello-python-operations** file
    we created in *step 4*. The annotation is what tells Tekton which server to use
    this key with. The server name is the **Service** we created in *step 6* in the
    GitLab namespace:'
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'apiVersion: v1'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: 'data:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: 'ssh-privatekey: ...'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: 'kind: Secret'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: 'name: git-write'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: 'namespace: python-hello-build'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: 'type: kubernetes.io/ssh-auth'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a service account for tasks to run, as with our secret:'
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ kubectl create -f chapter14/tekton-serviceaccount.yaml**'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: 'We need a container that contains both **git** and **kubectl** in it. We''ll
    build **chapter14/docker/PatchRepoDockerfile** and push it to our internal registry.
    Make sure to replace **192-168-2-114** with the hostname for your server''s IP
    address:'
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ docker build -f ./PatchRepoDockerfile -t \**'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: '**  docker.apps.192-168-2-114.nip.io/gitcommit/gitcommit .**'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: '**$ docker push \**'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '**  docker.apps.192-168-2-114.nip.io/gitcommit/gitcommit**'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: Every **Task** object can take inputs and produce results that can be shared
    with other **Task** objects. Tekton can provide runs (whether it's **TaskRun**
    or **PipelineRun**) with a workspace where the state can be stored and retrieved
    from. Writing to workspaces allows us to share data between **Task** objects.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: 'Before deploying our task and pipeline, let''s step through the work done by
    each task. The first task generates an image tag and gets the SHA hash of the
    latest commit. The full source is in **chapter14/yaml/tekton-task1.yaml**:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: '- name: create-image-tag'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: 'image: docker.apps.192-168-2-114.nip.io/gitcommit/gitcommit'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: 'script: |-'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: '#!/usr/bin/env bash'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: export IMAGE_TAG=$(date +"%m%d%Y%H%M%S")
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: echo -n "$(resources.outputs.result-image.url):$IMAGE_TAG" > /tekton/results/image-url
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: echo "'$(cat /tekton/results/image-url)'"
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: cd $(resources.inputs.git-resource.path)
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: RESULT_SHA="$(git rev-parse HEAD | tr -d '\n')"
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: 'echo "Last commit : $RESULT_SHA"'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: echo -n "$RESULT_SHA" > /tekton/results/commit-tag
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: Each step in a task is a container. In this case, we're using the container
    we built previously that has **kubectl** and **git** in it. We don't need **kubectl**
    for this task but we do need **git**. The first block of code generates an image
    name from the **result-image** URL and a timestamp. We could use the latest commit,
    but I like having a timestamp so that I can quickly tell how old a container is.
    We save the full image URL to **/text/results/image-url**, which corresponds to
    a result we defined in our task called **image-url**. This can be referenced by
    our pipeline or other tasks by referencing **$(tasks.generate-image-tag.results.image-url)**,
    where **generate-image-tag** is the name of our **Task**, and **image-url** is
    the name of our result.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: 'Our next task, in **chapter14/yaml/tekton-task2.yaml**, generates a container
    from our application''s source using Google''s Kaniko project ([https://github.com/GoogleContainerTools/kaniko](https://github.com/GoogleContainerTools/kaniko)).
    Kaniko lets you generate a container without needing access to a Docker daemon.
    This is great because you don''t need a privileged container to build your image:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: 'steps:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: '- args:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: '- --dockerfile=$(params.pathToDockerFile)'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: '- --destination=$(params.imageURL)'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: '- --context=$(params.pathToContext)'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: '- --verbosity=debug'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: '- --skip-tls-verify'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: 'command:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: '- /kaniko/executor'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: 'env:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: '- name: DOCKER_CONFIG'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: 'value: /tekton/home/.docker/'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: 'image: gcr.io/kaniko-project/executor:v0.16.0'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: 'name: build-and-push'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: 'resources: {}'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: The Kaniko container is what's called a "distro-less" container. It's not built
    with an underlying shell, nor does it have many of the command-line tools you
    may be used to. It's just a single binary. This means that any variable manipulation,
    such as generating a tag for the image, needs to be done before this step. Notice
    that the image being created doesn't reference the result we created in the first
    task. It instead references a parameter called **imageURL**. While we could have
    referenced the result directly, it would make it harder to test this task because
    it is now tightly bound to the first task. By using a parameter that is set by
    our pipeline, we can test this task on its own. Once run, this task will generate
    and push our container.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: 'Our last task, in **chapter14/yaml/tekton-task-3.yaml**, does the work to trigger
    ArgoCD to roll out a new container:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: '- image: docker.apps.192-168-2-114.nip.io/gitcommit/gitcommit'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: 'name: patch-and-push'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: 'resources: {}'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: 'script: |-'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: '#!/bin/bash'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: export GIT_URL="$(params.gitURL)"
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: export GIT_HOST=$(sed 's/.*[@]\(.*\)[:].*/\1/' <<< "$GIT_URL")
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: mkdir /usr/local/gituser/.ssh
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: cp /pushsecret/ssh-privatekey /usr/local/gituser/.ssh/id_rsa
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: chmod go-rwx /usr/local/gituser/.ssh/id_rsa
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: ssh-keyscan -H $GIT_HOST > /usr/local/gituser/.ssh/known_hosts
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: cd $(workspaces.output.path)
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: git clone $(params.gitURL) .
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: kubectl patch --local -f src/deployments/hello-python.yaml -p '{"spec":{"template":{"spec":{"containers":[{"name":"python-hello","image":"$(params.imageURL)"}]}}}}'
    -o yaml > /tmp/hello-python.yaml
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: cp /tmp/hello-python.yaml src/deployments/hello-python.yaml
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: git add src/deployments/hello-python.yaml
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: git commit -m 'commit $(params.sourceGitHash)'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: git push
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: The first block of code copies the SSH keys into our home directory, generates
    **known_hosts**, and clones our repository into a workspace we defined in the
    **Task**. We don't rely on Tekton to pull the code from our **operations** repository
    because Tekton assumes we won't be pushing code, so it disconnects the source
    code from our repository. If we try to run a commit, it will fail. Since the step
    is a container, we don't want to try to write to it, so we create a workspace
    with **emptyDir**, just like **emptyDir** in a **Pod** we might run. We could
    also define workspaces based on persistent volumes. This could come in handy to
    speed up builds where dependencies get downloaded.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: We're copying the SSH key from **/pushsecret**, which is defined as a volume
    on the task. Our container runs as user **431**, but the SSH keys are mounted
    as root by Tekton. We don't want to run a privileged container just to copy the
    keys from a **Secret**, so instead, we mount it as if it were just a regular **Pod**.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: Once we have our repository cloned, we patch our deployment with the latest
    image and finally, commit the change using the hash of the source commit in our
    application repository. Now we can track an image back to the commit that generated
    it! Just as with our second task, we don't reference the results of tasks directly
    to make it easier to test.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: 'We pull these tasks together in a pipeline – specifically, **chapter14/yaml/tekton-pipeline.yaml**.
    This YAML file is several pages long, but the key piece defines our tasks and
    links them together. You should never hardcode values into your pipeline. Take
    a look at our third task''s definition in the pipeline:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: '- name: update-operations-git'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: 'taskRef:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: 'name: patch-deployment'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: 'params:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: '- name: imageURL'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: 'value: $(tasks.generate-image-tag.results.image-url)'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: '- name: gitURL'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: 'value: $(params.gitPushUrl)'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: '- name: sourceGitHash'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: 'value: $(tasks.generate-image-tag.results.commit-tag)'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: 'workspaces:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: '- name: output'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: 'workspace: output'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: 'We reference parameters and task results, but nothing is hardcoded. This makes
    our **Pipeline** reusable. We also include the **runAfter** directive in our second
    and third task to make sure that our tasks are run in order. Otherwise, tasks
    will be run in parallel. Given each task has dependencies on the task before it,
    we don''t want to run them at the same time. Next, let''s deploy our pipeline
    and run it:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: Add the **chapter14/yaml/tekton-source-git.yaml** file to your cluster; this
    tells Tekton where to pull your application code from.
  id: totrans-440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Edit **chapter14/yaml/tekton-image-result.yaml**, replacing **192-168-2-114**
    with the hash representation of your server's IP address, and add it to your cluster.
  id: totrans-441
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Edit **chapter14/yaml/tekton-task1.yaml**, replacing the image host with the
    host for your Docker registry, and add the file to your cluster.
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add **chapter14/yaml/tekton-task2.yaml** to your cluster.
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Edit **chapter14/yaml/tekton-task3.yaml**, replacing the image host with the
    host for your Docker registry, and add the file to your cluster.
  id: totrans-444
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add **chapter14/yaml/tekton-pipeline.yaml** to your cluster.
  id: totrans-445
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add **chapter14/yaml/tekton-pipeline-run.yaml** to your cluster.
  id: totrans-446
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can check on the progress of your pipeline using **kubectl**, or you can
    use Tekton's CLI tool called **tkn** ([https://github.com/tektoncd/cli](https://github.com/tektoncd/cli)).
    Running **tkn pipelinerun describe build-hello-pipeline-run -n python-hello-build**
    will list out the progress of your build. You can rerun the build by recreating
    your **run** object, but that's not very efficient. Besides, what we really want
    is for our pipeline to run on a commit!
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: Building automatically
  id: totrans-448
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We don''t want to manually run builds. We want builds to be automated. Tekton
    provides the trigger project to provide webhooks so whenever GitLab receives a
    commit, it can tell Tekton to build a **PipelineRun** object for us. Setting up
    a trigger involves creating a Pod, with its own service account that can create
    **PipelineRun** objects, a Service for that Pod, and an **Ingress** object to
    host HTTPS access to the Pod. You also want to protect the webhook with a secret
    so that it isn''t triggered inadvertently. Let''s deploy these objects to our
    cluster:'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: Add **chapter14/yaml/tekton-webhook-cr.yaml** to your cluster. This **ClusterRole**
    will be used by any namespace that wants to provision webhooks for builds.
  id: totrans-450
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Edit **chapter14/yaml/tekton-webhook.yaml**. At the bottom of the file is an
    **Ingress** object. Change **192-168-2-114** to represent the IP of your cluster,
    with dashes instead of dots. Then, add the file to your cluster:'
  id: totrans-451
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'apiVersion: extensions/v1beta1'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: 'kind: Ingress'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: 'name: gitlab-webhook'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: 'namespace: python-hello-build'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: 'annotations:'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: 'cert-manager.io/cluster-issuer: ca-issuer'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: 'spec:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: 'rules:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: '- host: "python-hello-application.build.**192-168-2-114**.nip.io"'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: 'http:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: 'paths:'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: '- backend:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: 'serviceName: el-gitlab-listener'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
- en: 'servicePort: 8080'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
- en: 'pathType: ImplementationSpecific'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: 'tls:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
- en: '- hosts:'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
- en: '- "python-hello-application.build.**192-168-2-114**.nip.io"'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
- en: 'secretName: ingresssecret'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: Log in to GitLab. Go to the **Admin Area** | **Network**. Click on **Expand**
    next to **Outbound Requests**. Check **Allow requests to the local network from
    web hooks and services** and click **Save changes**.
  id: totrans-472
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to the **hello-python** project we created and click on **Settings** | **Webhooks**.
    For the URL, use your **Ingress** host with HTTPS – for instance, **https://python-hello-application.build.192-168-2-114.nip.io/**.
    For **Secret Token**, use **notagoodsecret**, and for **Push events**, set the
    branch name to **master**. Finally, click on **Add webhook**.
  id: totrans-473
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once added, click on **Test**, choosing **Push Events**. If everything is configured
    correctly, a new **PipelineRun** object should have been created. You can run
    **tkn pipelinerun list -n python-hello-build** to see the list of runs; there
    should be a new one running. After a few minutes, you'll have a new container
    and a patched Deployment in the **python-hello-operations** project!
  id: totrans-474
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We covered quite a bit in this section to build our application and deploy it
    using GitOps. The good news is that everything is automated; a push will create
    a new instance of our application! The bad news is that we had to create over
    a dozen Kubernetes objects and manually make updates to our projects in GitLab.
    In the last section, we'll automate this process. First, let's deploy ArgoCD so
    that we can get our application running!
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
- en: Deploying ArgoCD
  id: totrans-476
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have a way to get into our cluster, a way to store code, and a system
    for building our code and generating images. The last component of our platform
    is our GitOps controller. This is the piece that lets us commit manifests to our
    Git repository and make changes to our cluster. ArgoCD is a collaboration between
    Intuit and Weaveworks. It provides a great UI and is driven by a combination of
    custom resources and Kubernetes-native **ConfigMap** and **Secret** objects. It
    has a CLI tool, and both the web and CLI tools are integrated with OpenID Connect,
    so it will be easy to add SSO with our OpenUnison. Let''s deploy ArgoCD and use
    it to launch our **hello-python** web service:'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: 'Deploy using the standard YAML from [https://argoproj.github.io/argo-cd/getting_started/](https://argoproj.github.io/argo-cd/getting_started/):'
  id: totrans-478
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ kubectl create namespace argocd**'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: '**$ kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml**'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
- en: Create the **Ingress** object for ArgoCD by editing **chapter14/yaml/argocd-ingress.yaml**.
    Replace all instances of **192-168-2-140** with your IP address, replacing the
    dots with dashes. My server's IP is **192.168.2.114**, so I'm using **192-168-2-114**.
    Once done, add the file to your cluster.
  id: totrans-481
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get the root password by running **kubectl get pods -n argocd -l app.kubernetes.io/name=argocd-server
    -o name | cut -d'/' -f 2**. Save this password.
  id: totrans-482
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Edit the **argocd-server** **Deployment** in the **argocd** namespace. Add
    **--insecure** to the command:'
  id: totrans-483
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'spec:'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: 'containers:'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: '- command:'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: '- argocd-server'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
- en: '- --staticassets'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
- en: '- /shared/app'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: '**- --repo-server**'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
- en: '**        - argocd-repo-server:8081**'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
- en: '**        - --insecure**'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: You can now log in to ArgoCD by going to the **Ingress** host you defined in
    *step 2*. You will need to download the ArgoCD CLI utility as well from https://github.com/argoproj/argo-cd/releases/latest.
    Once downloaded, log in by running **./argocd login grpc-argocd.apps.192-168-2-114.nip.io**,
    replacing **192-168-2-114** with the IP of your server, with dashes instead of
    dots.
  id: totrans-493
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the **python-hello** namespace.
  id: totrans-494
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Before we can add our GitLab repository, we need to tell ArgoCD to trust our
    GitLab instance''s SSH host. Since we will have ArgoCD talk directly to the GitLab
    shell service, we''ll need to generate **known_host** for that Service. To make
    this easier, we included a script that will run **known_host** from outside the
    cluster but rewrite the content as if it were from inside the cluster. Run the
    **chapter14/shell/getSshKnownHosts.sh** script and pipe the output into the **argocd**
    command to import **known_host**. Remember to change the hostname to reflect your
    own cluster''s IP address:'
  id: totrans-495
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ ./chapter14/shell/getSshKnownHosts.sh gitlab.apps.192-168-2-114.nip.io
    | argocd cert add-ssh --batch**'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
- en: '**Enter SSH known hosts entries, one per line. Press CTRL-D when finished.**'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
- en: '**Successfully created 3 SSH known host entries**'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to generate an SSH key to access the **python-hello-operations**
    repository:'
  id: totrans-499
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ ssh-keygen -f ./argocd-python-hello**'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the public key to the **python-hello-operations** repository by going to
    the project and clicking on **Settings** | **Repository**. Next to **Deploy Keys**,
    click **Expand**. For **Title**, use **argocd**. Use the contents of **argocd-python-hello.pub**
    and click **Add key**. Then, add the key to ArgoCD using the CLI and replace the
    public GitLab host with the **gitlab-gitlab-shell** **Service** hostname:'
  id: totrans-501
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ argocd repo add git@gitlab-gitlab-shell.gitlab.svc.cluster.local:root/hello-python-operations.git
    --ssh-private-key-path ./argocd-python-hello**'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
- en: '**repository ''git@gitlab-gitlab-shell.gitlab.svc.cluster.local:root/hello-python-operations.git''
    added**'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
- en: 'Our last step is to create an **Application** object. You can create it through
    the web UI or the CLI. You can also create it by creating an **Application** object
    in the **argocd** namespace, which is what we''ll do. Create the following object
    in your cluster (**chapter14/yaml/argocd-python-hello.yaml**):'
  id: totrans-504
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'apiVersion: argoproj.io/v1alpha1'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
- en: 'kind: Application'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
- en: 'name: python-hello'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
- en: 'namespace: argocd'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
- en: 'spec:'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
- en: 'destination:'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
- en: 'namespace: python-hello'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
- en: 'server: https://kubernetes.default.svc'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
- en: 'project: default'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
- en: 'source:'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
- en: 'directory:'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
- en: 'jsonnet: {}'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
- en: 'recurse: true'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
- en: 'path: src'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
- en: 'repoURL: git@gitlab-gitlab-shell.gitlab.svc.cluster.local:root/hello-python-operations.git'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
- en: 'targetRevision: HEAD'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
- en: 'syncPolicy:'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
- en: 'automated: {}'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
- en: This is about as basic of a configuration as possible. We're working off of
    simple manifests. ArgoCD can work off of jsonet and Helm too. After this application
    is created, look at the Pods in the **python-hello** namespace. You should have
    one running! Making updates to your code will result in updates to the namespace.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
- en: We now have a code base that can be deployed automatically with a commit. We
    spent a couple dozen pages, ran dozens of commands, and created more than 20 objects
    to get there. Instead of manually creating these objects, it would be best to
    automate the process. Now that we have the objects that need to be created, we
    can automate the onboarding. In the next section, we will take the manual process
    of building the links between GitLab, Tekton, and ArgoCD to line up with our business
    processes.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
- en: Automating project onboarding using OpenUnison
  id: totrans-526
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Earlier in this chapter, we deployed the OpenUnison automation portal. This
    portal lets users request new namspaces to be created and allows developers to
    request access to these namespaces via a self-service interface. The workflows
    built into this portal are very basic but create the namespace and appropriate
    **RoleBinding** objects. What we want to do is build a workflow that integrates
    our platform and creates all of the objects we created manually earlier in this
    chapter. The goal is that we''re able to deploy a new application into our environment
    without having to run the **kubectl** command (or at least minimize its use).
    This will require careful planning. Here''s how our developer workflow will run:'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.6 – Platform developer workflow'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_14.6_B15514.jpg)'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.6 – Platform developer workflow
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s quickly run through the workflow that we see in the preceding figure:'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
- en: An application owner will request an application be created.
  id: totrans-532
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The infrastructure admin approves the creation.
  id: totrans-533
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At this point, OpenUnison will deploy the objects we manually created. We'll
    detail those objects shortly.
  id: totrans-534
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once created, a developer is able to request access to the application
  id: totrans-535
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The application owner(s) approve access to the application.
  id: totrans-536
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once approved, the developer will fork the application source base and do their
    work. They can launch the application in their developer workspace. They can also
    fork the build project to create a pipeline and the development environment operations
    project to create manifests for the application. Once the work is done and tested
    locally, the developer will push the code into their own fork, then request a
    merge request.
  id: totrans-537
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The application owner will approve the request and merge the code from GitLab.
  id: totrans-538
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the code is merged, ArgoCD will synchronize the build and operations projects.
    The webhook in the application project will kick off a Tekton pipeline that will
    build our container and update the development operations project with the tag
    for the latest container. ArgoCD will synchronize the updated manifest into our
    application's development namespace. Once testing is completed, the application
    owner submits a merge request from the development operations workspace to the
    production operations workspace, triggering ArgoCD to launch into production.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
- en: 'Nowhere in this flow is there a step called "operations staff uses **kubectl**
    to create a namespace." This is a simple flow and won''t totally avoid your operations
    staff from using **kubectl**, but it should be a good starting point. All this
    automation requires an extensive set of objects to be created:'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.7 – Application onboarding object map'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_14.7_B15514.jpg)'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.7 – Application onboarding object map
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
- en: In GitLab, we create a project for our application code, operations, and build
    pipeline. We also fork the operations project as a development operations project.
    For each project, we generate deploy keys and register webhooks. We also create
    groups to match the roles we defined earlier in this chapter.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
- en: For Kubernetes, we create namespaces for the development and production environments.
    We also create a namespace for the Tekton pipeline. We add the keys as needed
    to **Secrets**. In the build namespace, we create all the scaffolding to support
    the webhook that will trigger automatic builds. That way, our developers only
    need to worry about creating their pipeline objects.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
- en: In our last application, ArgoCD, we will create an **AppProject** that hosts
    our build and both operations namespaces. We will also add the SSH keys we generated
    when creating our GitLab projects. Each project also gets an **Application** object
    in our **AppProject** that instructs ArgoCD how to synchronize from GitLab. Finally,
    we add RBAC rules to ArgoCD so that our developers can view their application
    synchronization status but owners and operations can make updates and changes.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
- en: You don't need to build this out yourself! **chapter14/openunison** is the source
    for OpenUnison that implements this flow. If you want to see every object we create,
    refer to **chapter14/openunison/src/main/webapp/WEB-INF/workflows/30-NewK8sNamespace.xml**.
    This workflow does everything we just described. We also included **chapter14/python-hello**
    as our example application, **chapter14/python-hello-operations** for our manifests,
    and **chapter14/python-hello-build** as our pipeline. You'll need to tweak some
    of the objects in these three folders to match your environment, mostly updating
    the hostnames.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
- en: With our developer workflow designed and example projects ready to go, next
    we'll update OpenUnison, GitLab, and ArgoCD to get all this automation to work!
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
- en: Integrating GitLab
  id: totrans-549
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We configured GitLab for SSO when we first deployed the Helm chart. The **gitlab-oidc**
    **Secret** we deployed has all the information GitLab needs to access SSO from
    OpenUnison. We still need to configure OpenUnison though. We could hardcode the
    SSO configuration into our OpenUnison source base or we could dynamically add
    it as a custom resource. In this instance, we''ll add the SSO connection via a
    custom resource:'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
- en: Edit **chapter14/yaml/gitlab-trust.yaml**, replacing **192-168-2-140** with
    the server IP your cluster is running on. My cluster is on **192.168.2.114**,
    so I'll replace it with **192-168-2-114**. Add **chapter14/yaml/gitlab-trust.yaml**
    to your cluster. This file tells OpenUnison to establish a trust with GitLab for
    SSO.
  id: totrans-551
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Edit **chapter14/yaml/gitlab-url.yaml**, replacing **192-168-2-140** with the
    server IP your cluster is running on. My cluster is on **192.168.2.114**, so I'll
    replace it with **192-168-2-114**. Add **chapter14/yaml/gitlab-url.yaml** to your
    cluster. This file tells OpenUnison to add a badge to the portal for GitLab.
  id: totrans-552
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Log in to GitLab as root. Go to your user's profile area and click on **Access
    Tokens**. For **Name**, use **openunison**. Leave **Expires** blank and check
    the API scope. Click **Create personal access token**. Copy and paste the token
    into a notepad or some other place. Once you leave this screen, you can't retrieve
    this token again.
  id: totrans-553
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Edit the **orchestra-secrets-source** Secret in the **openunison** namespace.
    Add two keys:'
  id: totrans-554
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'apiVersion: v1'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
- en: 'data:'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
- en: 'K8S_DB_SECRET: aW0gYSBzZWNyZXQ='
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
- en: 'OU_JDBC_PASSWORD: c3RhcnR0MTIz'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
- en: 'SMTP_PASSWORD: ""'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
- en: 'unisonKeystorePassword: aW0gYSBzZWNyZXQ='
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
- en: '**  gitlab: c2VjcmV0  GITLAB_TOKEN: S7CCuqHfpw3a6GmAqEYg**'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
- en: 'kind: Secret'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
- en: Remember to Base64-encode the values. The **gitlab** key matches the secret
    in our **oidc-provider** Secret. **GITLAB_TOKEN** is going to be used by OpenUnison
    to interact with GitLab to provision the projects and groups we defined in our
    onboarding workflow. With GitLab configured, next is ArgoCD.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
- en: Integrating ArgoCD
  id: totrans-564
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'ArgoCD has built-in support for OpenID Connect. It wasn''t configured for us
    in the deployment, though:'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
- en: 'Edit the **argocd-cm** **ConfigMap** in the **argocd** namespace, adding the
    **url** and **oidc.config** keys, as shown in the following cde block. Make sure
    to update **192-168-2-140** to match your cluster''s IP address. Mine is **192.168.2.114**,
    so I''ll be using **192-168-2-114**:'
  id: totrans-566
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'apiVersion: v1'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
- en: 'data:'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
- en: 'url: https://argocd.apps.192-168-2-140.nip.io'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
- en: 'oidc.config: |-'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
- en: 'name: OpenUnison'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
- en: 'issuer: https://k8sou.apps.192-168-2-140.nip.io/auth/idp/k8sIdp'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
- en: 'clientID: argocd'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
- en: 'requestedScopes: ["openid", "profile", "email", "groups"]'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
- en: We don't specify a client secret with ArgoCD because it has both a CLI and a
    web component. Just like with the API server, it makes no sense to worry about
    a client secret that will need to reside on every single workstation that will
    be known to the user. It doesn't add any security in this case, so we will skip
    it.
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
- en: Edit **chapter14/yaml/argocd-trust.yaml**, replacing **192-168-2-140** with
    the server IP your cluster is running on. My cluster is on **192.168.2.114**,
    so I'll replace it with **192-168-2-114**. Add **chapter14/yaml/argocd-trust.yaml**
    to your cluster. This file tells OpenUnison to establish a trust with ArgoCD for
    SSO.
  id: totrans-577
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Edit **chapter14/yaml/argocd-url.yaml**, replacing **192-168-2-140** with the
    server IP your cluster is running on. My cluster is on **192.168.2.114**, so I'll
    replace it with **192-168-2-114**. Add **chapter14/yaml/argocd-url.yaml** to your
    cluster. This file tells OpenUnison to add a badge to the portal for ArgoCD.
  id: totrans-578
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'While most of ArgoCD is controlled with Kubernetes custom resources, there
    are some ArgoCD-specific APIs. To work with these APIs, we need to create a service
    account. We''ll need to create this account and generate a key for it:'
  id: totrans-579
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**$ kubectl patch configmap argocd-cm -n argocd -p ''{"data":{"accounts.openunison":"apiKey","accounts.openunison.enabled":"true"}}''**'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
- en: '**$ argocd account generate-token --account openunison**'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
- en: Take the output of the **generate-token** command and add it as the **ARGOCD_TOKEN**
    key to the **orchestra-secrets-source** **Secret** in the **openunison** namespace.
    Don't forget to Base64-encode it.
  id: totrans-582
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, we want to create ArgoCD RBAC rules so that we can control who can
    access the web UI and the CLI. Edit the **argocd-rbac-cm** **ConfigMap** and add
    the following keys. The first key will let our systems administrators and our
    API key do anything in ArgoCD. The second key maps all users that aren''t mapped
    by **policy.csv** into a role into a nonexistent role so that they won''t have
    access to anything:'
  id: totrans-583
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'data:'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
- en: 'policy.csv: |-'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
- en: g, k8s-cluster-administrators,role:admin
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
- en: g, openunison,role:admin
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
- en: 'policy.default: role:none'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
- en: With ArgoCD integrated, the last step to world automation is updating our OpenUnison
    custom resource!
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
- en: Updating OpenUnison
  id: totrans-590
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'OpenUnison is already deployed. The last step to launching an automation portal
    with our developer workflows built in is to update the **orchestra** OpenUnison
    custom resource. Update the image as in the following code block. Add **non_secret_data**,
    replacing **hosts** to match with your cluster''s IP. Finally, add the new secrets
    we created to the list of secrets the operator needs to import:'
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
- en: .
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
- en: .
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
- en: .
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
- en: '**image: docker.io/tremolosecurity/openunison-k8s-definitive-guide:latest**'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
- en: .
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
- en: .
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
- en: .
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
- en: 'non_secret_data:'
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
- en: .
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
- en: .
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
- en: .
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
- en: '**- name: GITLAB_URL**'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
- en: '**    value: https://gitlab.apps.192-168-2-140.nip.io**'
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
- en: '**  - name: GITLAB_SSH_HOST**'
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
- en: '**    value: gitlab-gitlab-shell.gitlab.svc.cluster.local**'
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
- en: '**  - name: GITLAB_WEBHOOK_SUFFIX**'
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
- en: '**    value: gitlab.192-168-2-140.nip.io**'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
- en: '**  - name: ARGOCD_URL**'
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
- en: '**    value: https://argocd.apps.192-168-2-140.nip.io**'
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
- en: '**  - name: GITLAB_WRITE_SSH_HOST**'
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
- en: '**    value: gitlab-write-shell.gitlab.svc.cluster.local**'
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
- en: .
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
- en: .
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
- en: .
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
- en: 'secret_data:'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
- en: '- K8S_DB_SECRET'
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
- en: '- unisonKeystorePassword'
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
- en: '- SMTP_PASSWORD'
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
- en: '- OU_JDBC_PASSWORD'
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
- en: '**- GITLAB_TOKEN**'
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
- en: '**  - ARGOCD_TOKEN**'
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
- en: In just a few minutes, the automation portal will be running. When you log in,
    you'll see badges for GitLab and ArgoCD. You'll also be able to click on **New
    Application** to begin deploying applications according to our workflow! You can
    use this as a starting point for designing your own automation platform or use
    it as a map for creating the various objects needed to integrate the tools on
    your platform.
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-624
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Coming into this chapter, we hadn't spent much time on deploying applications.
    We wanted to close things out with a brief introduction to application deployment
    and automation. We learned about pipelines, how they are built, and how they run
    on a Kubernetes cluster. We explored the process of building a platform by deploying
    GitLab for source control, built out a Tekton pipeline to work in a GitOps model,
    and used ArgoCD to make the GitOps model a reality. Finally, we automated the
    entire process with OpenUnison.
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
- en: Using the information in this chapter should give you direction as to how you
    want to build your own platform. Using the practical examples in this chapter
    will help you map the requirements in your organization to the technology needed
    to automate your infrastructure. The platform we built in this chapter is far
    from complete. It should give you a map for planning your own platform that matches
    your needs.
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
- en: Finally, thank you. Thank you for joining us on this adventure of building out
    a Kubernetes cluster. We hope you have as much fun reading this book and building
    out the examples as we did creating it!
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-628
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'True or false: A pipeline must be implemented to make Kubernetes work.'
  id: totrans-629
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. True
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
- en: B. False
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
- en: What are the minimum steps of a pipeline?
  id: totrans-632
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. Build, scan, test, and deploy
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
- en: B. Build and deploy
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
- en: C. Scan, test, deploy, and build
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
- en: D. None of the above
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
- en: What is GitOps?
  id: totrans-637
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. Running GitLab on Kubernetes
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
- en: B. Using Git as an authoritative source for operations configuration
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
- en: C. A silly marketing term
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
- en: D. A product from a new start-up
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
- en: What is the standard for writing pipelines?
  id: totrans-642
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. All pipelines should be written in YAML.
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
- en: B. There are no standards; every project and vendor has its own implementation.
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
- en: C. JSON combined with Go.
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
- en: D. Rust.
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
- en: How do you deploy a new instance of a container in a GitOps model?
  id: totrans-647
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. Use **kubectl** to update the **Deployment** or **StatefulSet** in the namespace.
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
- en: B. Update the **Deployment** or **StatefulSet** manifest in Git, letting the
    GitOps controller update the objects in Kubernetes.
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
- en: C. Submit a ticket that someone in operations needs to act on.
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
- en: D. None of the above.
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
- en: 'True or false: All objects in GitOps needs to be stored in your Git repository.'
  id: totrans-652
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. True
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
- en: B. False
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
- en: 'True or false: Your way is the right way to automate your processes.'
  id: totrans-655
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. True
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
- en: B. False
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
