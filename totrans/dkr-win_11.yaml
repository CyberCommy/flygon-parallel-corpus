- en: Administering and Monitoring Dockerized Solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Applications built on Docker are inherently portable, and the process of deployment
    is the same for every environment. As you promote your application through system
    tests and user tests to production, you'll use the same artifacts every time.
    The Docker images you use in production are the exact same versioned images that
    were signed off in the test environments, and any environmental differences can
    be captured in compose-file overrides, Docker configuration objects, and secrets.
  prefs: []
  type: TYPE_NORMAL
- en: In a later chapter, I'll cover how continuous deployment works with Docker,
    so your whole deployment process can be automated. But when you adopt Docker,
    you'll be moving to a new application platform, and the path to production is
    about more than just the deployment process. Containerized applications run in
    fundamentally different ways to apps deployed on VMs or bare-metal servers. In
    this chapter, I'll look at administering and monitoring applications running in
    Docker.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the tools you use to manage Windows applications today can still be
    used when the apps are moved to Docker, and I'll start by looking at some examples.
    But there are different management needs and opportunities for apps running in
    containers, and the main focus of this chapter will be management products specific
    to Docker.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter I''ll be using simple Dockerized applications to show you how
    to manage containers, including:'
  prefs: []
  type: TYPE_NORMAL
- en: Connecting  **Internet Information Services** (**IIS**) manager to IIS services
    running in containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connecting Windows Server Manager to containers, to see event logs and features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using open source projects to view and administer Docker swarms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using **Universal Control Plane** (**UCP**) with **Docker Enterprise**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will need Docker running on Windows 10 update 18.09, or Windows Server 2019
    to follow along with the examples. The code for this chapter is available at [https://github.com/sixeyed/docker-on-windows/tree/second-edition/ch08](https://github.com/sixeyed/docker-on-windows/tree/second-edition/ch08)
  prefs: []
  type: TYPE_NORMAL
- en: Managing containers with Windows tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many of the administration tools in Windows are able to manage services running
    on remote machines. IIS Manager, server manager, and **SQL Server Management Studio**
    (**SSMS**) can all be connected to a remote server on the network for inspection
    and administration.
  prefs: []
  type: TYPE_NORMAL
- en: Docker containers are different than remote machines, but they can be set up
    to allow remote access from these tools. Typically, you need to set up access
    for the tool explicitly by exposing management ports, enabling Windows features,
    and running PowerShell cmdlets. This can all be done in the Dockerfile for your
    application, and I'll cover the setup steps for each of these tools.
  prefs: []
  type: TYPE_NORMAL
- en: Being able to use familiar tools can be helpful, but there are limits to what
    you should do with them; remember, containers are meant to be disposable. If you
    connect to a web application container with IIS Manager and tweak the app pool
    settings, that tweak will be lost when you update the app with a new container
    image. You can use the graphical tools to inspect a running container and diagnose
    problems, but you should make changes in the Dockerfile and redeploy.
  prefs: []
  type: TYPE_NORMAL
- en: IIS Manager
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The IIS web management console is a perfect example. Remote access is not enabled
    by default in the Windows base images, but you can configure it with a simple
    PowerShell script. Firstly, the web management feature needs to be installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, you need to enable remote access with a registry setting and start the
    web management Windows service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'You also need an `EXPOSE` instruction in the Dockerfile to allow traffic into
    the management service on the expected port `8172`. This will allow you to connect,
    but IIS management console requires user credentials for the remote machine. To
    support this without having to connect the container to **Active Directory** (**AD**),
    you can create a user and password in the setup script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: There are security issues here. You need to create an administrative account
    in the image, expose a port, and run an additional service—all increasing the
    attack surface of your application. Instead of running the setup script in the
    Dockerfile, it would be better to attach to a container and run the script interactively
    if you need remote access.
  prefs: []
  type: TYPE_NORMAL
- en: 'I''ve set up a simple web server in an image, packaged with a script to enable
    remote management in the Dockerfile for `dockeronwindows/ch08-iis-with-management:2e`.
    I''ll run a container from this image, publishing the HTTP- and IIS-management
    ports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'When the container is running, I''ll execute the `EnableIisRemoteManagement.ps1`
    script inside the container, which sets up remote access with the IIS management
    service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The setup script ends by running `ipconfig`, so I can see the internal IP address
    of the container (I can also see this from `docker container inspect`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now I can run IIS Manager on my Windows host, choose Start Page | Connect to
    a Server, and enter the IP address of the container. When IIS challenges me to
    authenticate, I use the credentials for the `iisadmin` user I created in the setup
    script:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/789fefbf-e5c3-4b47-8fd9-1504fc86ae7e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, I can navigate around the application pools and the website hierarchy
    as if I were connected to a remote server:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/e325acd8-cdab-4b78-8936-40753d777478.png)'
  prefs: []
  type: TYPE_IMG
- en: This is a good way of checking the configuration of IIS or an ASP.NET application
    running on IIS. You can check the virtual directory setup, application pools,
    and application configuration, but this should be used for investigation only.
  prefs: []
  type: TYPE_NORMAL
- en: If I find that something in the application is not configured correctly, I need
    to go back to the Dockerfile and fix it and not make a change to the running container.
    This technique can be very useful when you're migrating an existing app to Docker.
    If you install an MSI with the web app in the Dockerfile, you can't see what the
    MSI actually does—but you can connect with IIS Manager and see the results.
  prefs: []
  type: TYPE_NORMAL
- en: SQL Server Management Studio (SSMS)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SSMS is more straightforward, because it uses the standard SQL client port `1433`.
    You don't need to expose any extra ports or start any extra services; SQL Server
    images from Microsoft and from this book already have everything set up. You can
    connect using SQL Server authentication with the `sa` credentials you use when
    you run the container.
  prefs: []
  type: TYPE_NORMAL
- en: 'This command runs a SQL Server 2019 Express Edition container, publishing port
    `1433` to the host and specifying the `sa` credentials:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This publishes the standard SQL Server port `1433`, so you have three options
    for connecting to SQL Server inside the container:'
  prefs: []
  type: TYPE_NORMAL
- en: On the host, use `localhost` as the server name.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the host, use the container's IP address as the server name.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On a remote machine, use the Docker host's machine name or AP address.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'I''ve fetched the IP address for my container, so in SSMS on the Docker host,
    I just specify the SQL credentials:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/0f7b835a-5d10-4170-ad44-d20e7d961ce5.png)'
  prefs: []
  type: TYPE_IMG
- en: You can administer this SQL instance in the same way as any SQL Server—creating
    databases, assigning user permissions, restoring Dacpacs, and running SQL scripts.
    Remember that any changes you make won't impact the image, and you'll need to
    build your own image if you want the changes to be available to new containers.
  prefs: []
  type: TYPE_NORMAL
- en: This approach lets you build a database through SSMS, if that's your preference,
    and get it working in a container without installing and running SQL Server. You
    can perfect your schema, add service accounts and seed data, and then export the
    database as a script.
  prefs: []
  type: TYPE_NORMAL
- en: 'I''ve done this for a simple example database, exporting the schema and data
    to a single file called `init-db.sql`. The Dockerfile for `dockeronwindows/ch08-mssql-with-schema:2e`
    takes the SQL script and packages it into a new image, with a bootstrap PowerShell
    script that deploys the database when you create a container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: There's a `HEALTHCHECK` in the SQL Server image here, which is good practice—it
    lets Docker check whether the database is running correctly. In this case, the
    test will fail if the schema has not been created, so the container won't report
    as healthy until the schema deployment has completed successfully.
  prefs: []
  type: TYPE_NORMAL
- en: 'I can run a container from this image in the usual way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'By publishing port `1433`, the database container is available at a random
    port on the host, so I connect to the database with an SQL client and see the
    schema and data from the script:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/86e5cff3-5407-4e0f-a8f1-ae1bd82b2e7b.png)'
  prefs: []
  type: TYPE_IMG
- en: This represents a fresh deployment of an application database, and in this case,
    I've used SQL Server's development edition to work out my schema but SQL Server
    Express for the actual database, all running in Docker with no local SQL Server
    instances.
  prefs: []
  type: TYPE_NORMAL
- en: If you think using SQL Server authentication is a retrograde step, you need
    to remember that Docker enables a different runtime model. You won't have a single
    SQL Server instance running multiple databases; they could all be targets if the
    credentials were compromised. Each SQL workload will be in a dedicated container
    with its own set of credentials, so you effectively have one SQL instance per
    database, and you could potentially have one database per service.
  prefs: []
  type: TYPE_NORMAL
- en: Security can be increased by running in Docker. Unless you need to connect to
    SQL Server remotely, you don't need to publish the port from the SQL container.
    Any applications that need database access will run as containers in the same
    Docker network as the SQL container and will be able to access port `1433` without
    publishing it to the host. This means SQL is only accessible to other containers
    running in the same Docker network, and in production, you can use Docker secrets
    for the connection details.
  prefs: []
  type: TYPE_NORMAL
- en: If you need to use Windows authentication with an AD account, you can still
    do that in Docker. Containers can be domain-joined when they start, so you can
    use service accounts for SQL Server instead of SQL Server authentication.
  prefs: []
  type: TYPE_NORMAL
- en: Event logs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can connect Event Viewer on your local machine to a remote server, but currently
    the remote event log services are not enabled on the Windows Server Core or the
    Nano Server images. This means you can't connect to a container and read event
    log entries with the Event Viewer UI—but you can do that with the server manager
    UI, which I'll cover in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you just want to read event logs, you can execute PowerShell cmdlets against
    running containers to get the log entries. This command reads the two latest event
    log entries for the SQL Server application from my database container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Reading event logs can be useful if you have an issue with a container that
    you can't diagnose any other way. But it's an approach that doesn't scale when
    you have dozens or hundreds of containers running. It's better to relay the event
    logs that are of interest to the console, so they're collected by the Docker platform,
    and you can read them with `docker container logs`, or a management tool that
    can access the Docker API.
  prefs: []
  type: TYPE_NORMAL
- en: Relaying event logs is simple to do, taking a similar approach to relaying IIS
    logs in [Chapter 3](ee527f27-ee07-40e1-a39d-86aa2d11da72.xhtml), *Developing Dockerized
    .NET Framework and .NET Core Applications*. For any apps that write to the event
    log, you use a startup script as the entry point, which runs the app and then
    enters a read loop—getting entries from the event log and writing them out to
    the console.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a useful approach for apps that run as Windows Services, and it''s
    an approach Microsoft has used in the SQL Server Windows images. The Dockerfile
    uses a PowerShell script as `CMD`, and that script ends with a loop that calls
    the same `Get-EventLog` cmdlet to relay logs to the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This script reads the event log every 2 seconds, gets any entries since the
    last read, and writes them out to the console. The script runs in the process
    started by Docker, so the log entries are captured and can be surfaced by the
    Docker API.
  prefs: []
  type: TYPE_NORMAL
- en: This is not a perfect approach—it uses a timed loop and only selects some of
    the data from the log, and it means storing data in both the container's event
    log and in Docker. It is valid if your application already writes to the event
    log and you want to Dockerize it without rebuilding the app. In this case, you
    need to be sure you have a mechanism to keep your application process running,
    such as a Windows Service, and a health check in the Dockerfile, because Docker
    is monitoring only the event log loop.
  prefs: []
  type: TYPE_NORMAL
- en: Server manager
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Server manager is a great tool to administer and monitor servers remotely, and
    it works well with containers based on Windows Server Core. You need to take a
    similar approach to the IIS management console, configuring a user in the container
    with administrator access and then connecting from the host.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just as with IIS, you can add a script to the image, which enables access so
    you can run it when you need it. This is safer than always enabling remote access
    in the image. The script just needs to add a user, configure the server to allow
    remote access from administrator accounts, and ensure the **Windows Remote Management**
    (**WinRM**) service is running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'I have a sample image which shows this approach, `dockeronwindows/ch08-iis-with-server-manager:2e`. It
    is based on IIS and packages a script to enable remote access with server manager.
    The Dockerfile also exposes the ports used by WinRM, `5985` and `5986`. I can
    start a container running IIS in the background and then enable remote access:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'You can connect to the container with the server manager, using the container''s
    IP address, but the container isn''t domain-joined. The server manager will try
    to authenticate over a secure channel and fail, so you''ll get a WinRM authentication
    error. To add a server that isn''t domain-joined, you need to add it as a trusted
    host. The trusted host list needs to use the hostname of the container, and not
    the IP address, so first I''ll get the hostname of the container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'I will add that as an entry in the `hosts` file for my server, at `C:\Windows\system32\drivers\etc\hosts`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'And now, I can add the container to the trusted list. This command needs to
    run on the host, and not in the container. You''re adding the container''s hostname
    to the local machine''s list of trusted servers. I run this on my Windows Server
    2019 host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: I'm running Windows Server 2019, but you can use the server manager on Windows
    10 too. Install the **Remote Server Administration Tools** (**RSAT**), and you
    can use the server manager on Windows 10 in the same way.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the server manager, navigate to All Servers | Add Servers, and open the
    DNS tab. Here, you can enter the hostname of the container, and the server manager
    will resolve the IP address:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/281c2292-4fc3-4a73-a045-1267e090f3a5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Select the server details and click on OK—now the server manager will try to
    connect to the container. You''ll see an updated status in the All Servers tab,
    which says the server is online but that access is denied. Now, you can right-click
    on the container in the server list and click on Manage As to provide the credentials
    for the local administrator account. You need to specify the hostname as the domain
    part of the username. The local user created in the script is called `serveradmin`, but
    I need to authenticate with  `9c097d80c08b\serveradmin`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/35b53a20-2065-4844-b52e-ec397a9106d4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now the connection succeeds, and you''ll see the data from the container surfaced
    in the server manager, including the event log entries, Windows Services, and
    all the installed roles and features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/73b37959-9d87-40bc-a0b8-dce0d6f392e9.png)'
  prefs: []
  type: TYPE_IMG
- en: You can even add features to the container from the remote server manager UI—but
    that wouldn't be a good practice. Like the other UI management tools, it's better
    to use them for exploration and investigation but not to make any changes in the
    Dockerfile.
  prefs: []
  type: TYPE_NORMAL
- en: Managing containers with Docker tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You've seen that you can use existing Windows tools to administer containers,
    but what you can do with these tools doesn't always apply in the Docker world.
    A container will run a single web application, so the hierarchy navigation of
    IIS Manager isn't really helpful. Checking event logs in the server manager can
    be useful, but it is much more useful to relay entries to the console, so they
    can be surfaced from the Docker API.
  prefs: []
  type: TYPE_NORMAL
- en: Your application images also need to be set up explicitly to enable access to
    remote management tools, exposing ports, adding users, and running additional
    Windows services. All this adds to the attack surface of your running container.
    You should see these existing tools as useful in debugging in development and
    test environments, but they're not really suitable for production.
  prefs: []
  type: TYPE_NORMAL
- en: The Docker platform provides a consistent API for any type of application running
    in a container, and that's an opportunity for a new type of admin interface. For
    the rest of the chapter, I'll be looking at management tools that are Docker-aware
    and provide an alternative management interface to the Docker command line. I'll
    start with some open source tools and move on to the commercial **Containers-as-a-Service**
    (**CaaS**) platform in Docker Enterprise.
  prefs: []
  type: TYPE_NORMAL
- en: Docker visualizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Visualizer** is a very simple web UI that shows basic information about nodes
    and containers in a Docker swarm. It''s an open source project on GitHub in the
    `dockersamples/docker-swarm-visualizer` repository. It''s a Node.js application,
    and it comes packaged in Docker images for Linux and Windows.'
  prefs: []
  type: TYPE_NORMAL
- en: 'I''ve deployed a hybrid Docker Swarm in Azure for this chapter, with a Linux
    manager node, two Linux worker nodes, and two Windows worker nodes. I can run
    the visualizer as a Linux container on the manager node, by deploying a service
    that binds to the Docker Engine API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The constraint ensures the container runs only on a manager node, and as my
    manager runs on Linux, I can use the `mount` option to let the container talk
    to the Docker API. In Linux, you can treat sockets as filesystem mounts, so the
    container can use the API socket, without having to expose it publicly over the **Transmission
    Control Protocol** (**TCP**).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run the visualizer in an all-Windows swarm. Docker currently supports
    Windows named pipes as volumes on a single server, but not in Docker Swarm; however,
    you can mount the API using TCP, as I did with Traefik in [Chapter 7](bf6a5e90-bbba-435b-b0a0-734611e0e834.xhtml),
    *Orchestrating Distributed Solutions with Docker Swarm*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The visualizer gives you a read-only view of the containers in the swarm. The
    UI shows the status of hosts and containers and gives you a quick way to check
    the distribution of the workload on your swarm. This is how my Docker Enterprise
    cluster in Azure looks with the NerdDinner stack deployed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/732dc814-28a2-4751-83f3-b518681e96ba.png)'
  prefs: []
  type: TYPE_IMG
- en: I can see at a glance whether my nodes and containers are healthy, and I can
    see that Docker has distributed containers across the swarm as evenly as it can.
    Visualizer uses the API in the Docker service, which exposes all the Docker resources
    with a RESTful interface.
  prefs: []
  type: TYPE_NORMAL
- en: The Docker API also provides write access, so you can create and update resources.
    An open source project called **Portainer** provides administration using these
    APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Portainer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Portainer is a lightweight management UI for Docker. It runs as a container,
    and it can manage single Docker hosts and clusters running in swarm mode. It's
    an open source project hosted on GitHub in the `portainer/portainer` repository.
    Portainer is written in Go, so it's cross-platform, and you can run it as a Linux
    or a Windows container.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two parts to Portainer: You need to run an agent on each node and
    then run the management UI. All these run in containers, so you can use a Docker
    Compose file such as the one in the source code for this chapter under `ch08-portainer`.
    The Compose file defines a global service that is the Portainer agent, running
    in a container on each node in the swarm. Then there is the Portainer UI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The `portainer/portainer` image on Docker Hub is a multi-arch image, which means
    you can use the same image tag on Linux and Windows, and Docker will use the matching
    image for the host OS. You can't mount the Docker socket on Windows, but the Portainer
    documentation shows you how to access the Docker API on Windows.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you first browse to Portainer, you need to specify an administrator password.
    Then, the service connects to the Docker API and surfaces details about all the
    resources. In swarm mode, I can see a count of the nodes in the swarm, the number
    of stacks, the services and containers running, and the images, volumes, and networks
    in my cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/179bc3ae-5b55-4487-b2c8-dad341998b9d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The cluster visualizer link shows a UI very much like the Docker Swarm visualizer,
    with the containers running on each node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/8b3a9fe6-5690-4bbc-af88-1fb2d3191700.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Services view shows me all the running services, and, from here, I can
    drill down into service details, and there''s a quick link to update the scale
    of the service:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/3f0d001c-3a79-4cf4-8f68-f0028ca0fe9e.png)'
  prefs: []
  type: TYPE_IMG
- en: Portainer has been evolving in line with new Docker functionality, and you can
    deploy stacks and services and manage them from Portainer. You can drill down
    into service logs, connect a console session to containers, and deploy common
    applications from Docker Compose templates built into the UI.
  prefs: []
  type: TYPE_NORMAL
- en: You can create multiple users and teams in Portainer and apply access control
    to resources. You can create services that have access limited to certain teams.
    Authentication is managed by Portainer with a local user database, or through
    connection to an existing **Lightweight Directory Access Protocol** (**LDAP**)
    provider.
  prefs: []
  type: TYPE_NORMAL
- en: Portainer is a great tool and an active open source project, but you should
    evaluate the latest version before you adopt it as your management tool. Portainer
    was originally a Linux tool, and there are still a few areas where Windows features
    are not fully supported. At the time of writing, the agent container needs special
    configuration on Windows nodes, which means you can't deploy it as a global service
    across your swarm, and you can't see Windows containers in Portainer without it.
  prefs: []
  type: TYPE_NORMAL
- en: In a production environment, you may have a requirement to run software with
    support. Portainer is open source, but there is a commercial support option available.
    For enterprise deployments or environments with strict security processes, Docker
    Enterprise offers a complete feature set.
  prefs: []
  type: TYPE_NORMAL
- en: CaaS with Docker Enterprise
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker Enterprise is the commercial edition from Docker, Inc. It's a complete
    CaaS platform and makes full use of Docker to provide a single pane of glass to
    manage any number of containers running on any number of hosts.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Enterprise is a production-grade product that you run on a cluster of
    machines in your data center or in the cloud. The clustering functionality supports
    multiple orchestrators, Kubernetes, as well as Docker Swarm. In production, you
    could have a 100-node cluster using the exact same application platform as your
    development laptop running as a single-node cluster.
  prefs: []
  type: TYPE_NORMAL
- en: There are two parts to Docker Enterprise. There's the **Docker Trusted Registry**
    (**DTR**), which is like running your own private instance of Docker Hub, complete
    with image signing and security scanning. I'll cover DTR in [Chapter 9](ea2edfd1-c625-4599-8ec2-d5ae811941ef.xhtml),
    *Understanding the Security Risks and Benefits of Docker*, when I look at security
    in Docker. The administration component is called **Universal Control Plane**
    (**UCP**), and it's a new type of management interface.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Universal Control Plane
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: UCP is a web-based interface used to manage nodes, images, services, containers,
    secrets, and all the other Docker resources. UCP itself is a distributed application
    that runs in containers across connected services in the swarm. UCP gives you
    a single place to administer all your Docker applications in the same way. It
    provides role-based access control to resources so you can set fine-grained controls
    over who can do what.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Enterprise runs Kubernetes and Docker Swarm. Kubernetes will support
    Windows nodes in a future release, so you will be able to deploy Windows containers
    to Docker Swarm or Kubernetes on a single Docker Enterprise cluster. You can deploy
    a stack to UCP with a Docker Compose file, target either Docker Swarm or Kubernetes,
    and UCP will create all the resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'UCP gives you the full range of administration features: you can create, scale,
    and remove services, inspect, and connect to the tasks running the services, and
    manage the nodes running the swarm. All the additional resources you need, such
    as Docker networks, configs, secrets, and volumes, are surfaced in UCP for management
    in the same way.'
  prefs: []
  type: TYPE_NORMAL
- en: You can run a hybrid Docker Enterprise cluster with Linux nodes for UCP and
    DTR and Windows nodes for your user workloads. As a subscription service from
    Docker, you have support from Docker's team for the setting up of your cluster
    and for dealing with any issues, covering all the Windows and Linux nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Navigating the UCP UI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You log into UCP from the home page. You can either use the authentication built
    into Docker Enterprise, managing users manually from UCP, or you can connect to
    any LDAP authentication store. This means you can set up Docker Enterprise to
    use your organization's AD and have users log in with their Windows accounts.
  prefs: []
  type: TYPE_NORMAL
- en: 'The UCP home page is a dashboard that shows the key performance indicators
    of your cluster, the number of nodes, services, and the Swarm and Kubernetes services
    running at that moment, together with the overall compute utilization of the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/6daec606-9f03-42a5-bc82-c6ed29acc086.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the dashboard, you can navigate to the resource views that give you access
    grouped by the resource type: Services, Containers, Images, Nodes, Networks, Volumes,
    and Secrets. For most of the resource types, you can list the existing resources,
    inspect them, delete them, and create new ones.'
  prefs: []
  type: TYPE_NORMAL
- en: UCP is a multi-orchestrator container platform, so you can have some applications
    running in Kubernetes and others in Docker Swarm on the same cluster. The Shared
    Resources section of the navigation shows the resources that are shared between
    the orchestrators, including images, containers, and stacks. This is a great way
    to support heterogeneous deliveries, or to evaluate different orchestrators in
    controlled environments.
  prefs: []
  type: TYPE_NORMAL
- en: UCP provides **Role-Based Access Control** (**RBAC**) for all the resources.
    You can apply a permission label to any resource and secure access based on that
    label. Teams can be assigned permissions to labels—ranging from no access to full
    control—which secures access to team members for all the resources that have these
    labels.
  prefs: []
  type: TYPE_NORMAL
- en: Managing nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The node view shows you all the nodes in the cluster, listing the operating
    system and CPU architecture, the node status, and the node manager status:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/65b4a710-75ef-4d74-acd5-78405f58ec28.png)'
  prefs: []
  type: TYPE_IMG
- en: 'I have six nodes in my cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Two Linux nodes used for mixed workloads: these can run Kubernetes or Docker
    Swarm services'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two Linux nodes that are only configured for Docker Swarm services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two Windows nodes that are only for Docker Swarm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These nodes are running all the UCP and DTR containers. Docker Enterprise can
    be configured to exempt manager nodes from running user workloads, and this can
    also be done for running DTR. This is a good way to ring-fence compute power for
    the Docker Enterprise services, to ensure your application workload doesn't starve
    the management components of resources.
  prefs: []
  type: TYPE_NORMAL
- en: In node administration, you have a graphical way to view and manage the cluster
    servers you have access to. You can put nodes into the drain mode, allowing you
    to run Windows updates or to upgrade Docker on the node. You can promote workers
    to managers, demote managers to workers, and see the tokens you need to join new
    nodes to the swarm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Drilling into each node, you can see the total CPU, memory, and disk usage
    of the server, with a graph showing usage, which you can aggregate for periods
    from 30 minutes to 24 hours:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/58297ff5-c3fc-4b0c-b5df-a7744e6acdd6.png)'
  prefs: []
  type: TYPE_IMG
- en: In the Metrics tab, there is a list of all the containers on the node, showing
    their current status and the image the container is running from. From the container
    list, you can navigate to the container view, which I'll cover shortly.
  prefs: []
  type: TYPE_NORMAL
- en: Volumes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Volumes** exist at the node level rather than the swarm level, but you can
    manage them in UCP across all the swarm nodes. How you manage volumes in the swarm
    depends on the type of volume you''re using. Local volumes are fine for scenarios
    such as global services that write logs and metrics to the disk and then forward
    them centrally.'
  prefs: []
  type: TYPE_NORMAL
- en: Persistent data stores running as clustered services could also use local storage.
    You might create a local volume on every node but add labels to servers with high-volume
    RAID arrays. When you create the data service, you can use a constraint to limit
    it to RAID nodes, so other nodes will never have a task scheduled on them, and
    where the tasks are running, they will write data to the volume on the RAID array.
  prefs: []
  type: TYPE_NORMAL
- en: For on-premises data centers and in the cloud, you can use shared storage with
    volume plugins. With shared storage, services can continue to access data even
    if containers move to different swarm nodes. Service tasks will read and write
    data to the volume that gets persisted on the shared storage device. There are
    many volume plugins available on Docker Store, including for-cloud services such
    as AWS and Azure, physical infrastructure from HPE and Nimble, and virtualization
    platforms, such as vSphere.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Enterprise uses the Cloudstor plugin to provide cluster-wide storage,
    and if you deploy with Docker Certified Infrastructure, then this is configured
    for you. At the time of writing, the plugin is only supported on Linux nodes,
    so Windows nodes are restricted to running local volumes. There are still many
    stateful application architectures that can work well in Docker Swarm with local
    volumes, but you need to configure them carefully.
  prefs: []
  type: TYPE_NORMAL
- en: Storage is an area that there is a lot of focus on in the container ecosystem.
    Technologies are emerging that create cluster-wide storage options, without the
    need for specific infrastructure. As these mature, you will be able to run stateful
    services with high availability and scale, just by pooling the disks on your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Volumes have a limited number of options, so creating them is a case of specifying
    the Driver and applying any driver Options:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/2b4af10f-fd2f-4090-99e7-480862e189a9.png)'
  prefs: []
  type: TYPE_IMG
- en: Permissions can be applied to volumes, such as other resources, by specifying
    a collection where the resource belongs to. Collections are how UCP enforces role-based
    access control to limit access.
  prefs: []
  type: TYPE_NORMAL
- en: 'Local volumes are created on each node, so containers that need a named volume
    can run on any node and still find the volume. In a hybrid swarm that UCP creates,
    local volumes are created on each node, and show the physical location on the
    server where the volume data is mounted:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/adff0283-7fa3-42ee-b1bc-5260cf8f8a29.png)'
  prefs: []
  type: TYPE_IMG
- en: UCP gives you a single view for all the resources in your cluster, including
    the volumes on each node and the images which are available for running containers.
  prefs: []
  type: TYPE_NORMAL
- en: Images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'UCP is not an image registry. DTR is the enterprise private registry in Docker
    Enterprise, but you can manage the images that are in the Docker cache on each
    node, using UCP. In the images view, UCP shows you which images have been pulled
    on the cluster nodes, and it also allows you to pull images, which get downloaded
    onto every node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/c64c83a7-f56b-46e5-ab53-05edf617388c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Docker images are compressed for distribution, and the Docker engine decompresses
    the layers when you pull an image. There are OS-specific optimizations to start
    containers as soon as the pull completes, which is why you can''t pull Windows
    images on Linux hosts, or vice versa. UCP will try to pull the image on every
    host, but if some fail because of an OS mismatch, it will continue with the remaining
    nodes. You will see errors if there is a mismatch:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/03333dcf-1a6d-49b1-ba17-063ec6e62f06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the image view, you can drill down and see the details of an image, including
    the history of the layers, the health check, any environment variables, and the
    exposed ports. The basic details also show you the OS platform of the image, the
    virtual size, and the date on which it was created:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/161ff9ad-3d91-44d2-8864-8b1606fbcbe7.png)'
  prefs: []
  type: TYPE_IMG
- en: In UCP, you can also remove images from the cluster. You may have a policy of
    retaining just the current and previous image versions on the cluster to allow
    rollback. Other images can be safely removed from the Docker Enterprise nodes,
    leaving all previous image versions in DTR so they can be pulled if needed.
  prefs: []
  type: TYPE_NORMAL
- en: Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Network management is straightforward and UCP presents the same interface as
    for other resource types. The network list shows the networks in the cluster and
    these can be added to a collection with RBAC applied, so you'll only see networks
    you're allowed to see.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several low-level options for networks, allowing you to specify IPv6
    and custom MTU packet sizes. Swarm mode supports encrypted networks, where the
    traffic between nodes is transparently encrypted, and it can be enabled through
    UCP. In a Docker Enterprise cluster, you''ll use the overlay driver to allow services
    to communicate in a virtual network across the cluster nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fb40e1e0-5d11-45d4-8471-8326cb0fe93e.png)'
  prefs: []
  type: TYPE_IMG
- en: Docker supports a special type of swarm network called an **ingress network**.
    Ingress networks have load balancing and service discovery for external requests.
    This makes port publishing very flexible. On a 10-node cluster, you could publish
    port `80` on a service with three replicas. If a node receives an incoming request
    on port `80` but it isn't running one of the service tasks, Docker will intelligently
    redirect it to a node that is running a task.
  prefs: []
  type: TYPE_NORMAL
- en: Ingress networks are a powerful feature that work the same for Linux and Windows
    nodes in a Docker Swarm cluster. I cover them in more detail in [Chapter 7](bf6a5e90-bbba-435b-b0a0-734611e0e834.xhtml),
    *Orchestrating Distributed Solutions with Docker Swarm*.
  prefs: []
  type: TYPE_NORMAL
- en: Networks can also be deleted through UCP, but only if there are no containers
    attached. If you have services defined that use the network, you'll get a warning
    if you try to delete it.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying stacks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are two ways to deploy your applications with UCP, which are analogous
    to deploying individual services with `docker service create`, and deploying a
    full compose file with `docker stack deploy`. Stacks are the easiest to deploy
    and will let you use a compose file that you've verified in pre-production environments.
  prefs: []
  type: TYPE_NORMAL
- en: In the source code for this chapter, the folder `ch08-docker-stack` contains
    the deployment manifests to run NerdDinner on Docker Enterprise, using swarm mode.
    The `core docker-compose.yml` file is the same as the one mentioned in [Chapter
    7](bf6a5e90-bbba-435b-b0a0-734611e0e834.xhtml),* Orchestrating Distributed Solutions
    with Docker Swarm*, but there are some changes in the override file to deploy
    to my production cluster. I'm taking advantage of the hybrid cluster I have in
    Docker Enterprise, and I'm using Linux containers for all the open source infrastructure
    components.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are only two changes for a service to use Linux containers instead of
    Windows: the image name, and a deployment constraint to ensure containers are
    scheduled to run on Linux nodes. Here''s the override for the NATS message queue
    in the file `docker-compose.hybrid-swarm.yml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'I''ve used the same approach as [Chapter 7](bf6a5e90-bbba-435b-b0a0-734611e0e834.xhtml),
    *Orchestrating Distributed Solutions with Docker Swarm* with `docker-compose config`
    to join the override files together and export them into `docker-swarm.yml`. I
    could connect my Docker CLI to the cluster and deploy the application with `docker
    stack deploy`, or I could use the UCP UI. From the Stacks view, under Shared Resources,
    I can click on Create Stack and select the orchestrator and upload a compose YML
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/def85847-d39d-44ba-b2ff-f6d639a85227.png)'
  prefs: []
  type: TYPE_IMG
- en: 'UCP validates the contents and highlights any issues. Valid compose files are
    deployed as a stack, and you will see all the resources in UCP: networks, volumes,
    and services. After a few minutes, all the images for my application are pulled
    on to the cluster nodes and UCP schedules replicas for each of the services. The
    service list shows me that all the components are running at the required level
    of scale:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/e94a002b-0c9d-437a-bb82-48eae1560c01.png)'
  prefs: []
  type: TYPE_IMG
- en: My modernized NerdDinner application is now running across 15 containers in
    a six-node Docker Enterprise swarm. I have high availability and scale in a supported
    production environment, and I've switched the four open source components from
    my custom images to the official Docker images, without any changes to my application
    images.
  prefs: []
  type: TYPE_NORMAL
- en: Stacks are the preferred deployment model, as they continue to use the known
    compose file format, and they automate all the resources. But stacks are not suitable
    for every solution, particularly when you're moving legacy applications to containers.
    In a stack deployment, there's no guarantee about the order in which the services
    will be created; the `depends_on` option used by Docker Compose doesn't apply.
    This is a deliberate design decision based on the idea that services should be
    resilient, but not all services are.
  prefs: []
  type: TYPE_NORMAL
- en: Modern applications should be built for failure. If a web component can't connect
    to the database, it should use a policy-based retry mechanism to reconnect repeatedly rather
    than failing to start. Traditional applications usually expect their dependencies
    to be available and don't have graceful retries built in. NerdDinner is like that,
    so if I deploy a stack from the compose file, the web app could start before the
    database service is created, and it will fail.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the container should exit, so Docker knows the application is
    not running. Then it will schedule a new container to run, and by the time it
    starts up, the dependency should be available. If not, the new container will
    end, and Docker will schedule a replacement, and this will keep happening until
    the application is working correctly. If your legacy applications don't have any
    dependency checks, you can build this logic into the Docker image, using startup
    checks and health checks in the Dockerfile.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, that might not be possible, or it might be that new containers
    starting repeatedly cause a problem with your legacy application. You can still
    manually create services rather than deploying a stack. UCP supports this workflow
    too, and this lets you manually ensure that all the dependencies are running before
    you start each service.
  prefs: []
  type: TYPE_NORMAL
- en: This is the imperative approach to managing your apps, which you really should
    try to avoid. It's far better to encapsulate your application manifest in a simple
    set of Docker Compose files that can be managed in source control, but it may
    be difficult to do that with some legacy apps.
  prefs: []
  type: TYPE_NORMAL
- en: Creating services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are dozens of options for the `docker service create` command. UCP supports
    them all in a guided UI, which you start with Create a Service from the services
    view. First, you specify the basic details, the name of the image to use for the
    service; the service name, which is how other services will discover this one;
    and the command arguments, if you want to override the default startup in the
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/6fc4a829-65d8-4dbd-8560-d98948433217.png)'
  prefs: []
  type: TYPE_IMG
- en: 'I won''t cover all the details; they map to the options in the `docker service
    create` command, but the Scheduling tab is worth looking at. This is where you
    set the service mode to be replicated or global, add the required number of replicas,
    and the configuration for rolling updates:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/9053565b-46a2-4e08-a97f-f66405add85c.png)'
  prefs: []
  type: TYPE_IMG
- en: The Restart Policy defaults to Always. This works in conjunction with the replica
    count, so if any tasks fail or are stopped, they will be restarted to maintain
    the service level. You can configure the update settings for automated roll-outs,
    and you can also add scheduling constraints. Constraints work with node labels
    to limit which nodes can be used to run service tasks. You can use this to restrict
    tasks to high-capacity nodes or to nodes that have strict access controls.
  prefs: []
  type: TYPE_NORMAL
- en: In the other sections, you can configure how the service integrates with other
    resources in the cluster, including networks and volumes, configs, and secrets,
    and you can specify compute reservations and limits. This lets you restrict services
    to a limited amount of CPU and memory, and you can also specify a minimum share
    of CPU and memory that each container should have.
  prefs: []
  type: TYPE_NORMAL
- en: When you deploy the service, UCP takes care of pulling the image on to any nodes
    that need it and starts the required number of containers. That would be one container
    per node for global services or the specified number of tasks for replicated services.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: UCP lets you deploy any type of application in the same way, either with a stack
    compose file or by creating services. The application can use many services with
    any combination of technologies—parts of the new NerdDinner stack are running
    on Linux now in my hybrid cluster. I've deployed Java, Go, and Node.js components
    as Linux containers and .NET Framework and .NET Core components as Windows containers
    on the same cluster.
  prefs: []
  type: TYPE_NORMAL
- en: All these different technology platforms  are managed in the same way with UCP,
    which is what makes it such a valuable platform for companies with a large application
    estate. The service view shows all services with basic information, such as the
    overall status, the number of tasks, and the last time an error was reported.
    For any service, you can drill down into a detailed view that shows all the information
    about the service.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the Overview tab of the core NerdDinner ASP.NET web app:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/a5081765-3382-4623-a3bf-5e2a7dfb6feb.png)'
  prefs: []
  type: TYPE_IMG
- en: I've scrolled this view so I can see the secrets that the service has available,
    along with the environment variables (none in this case), the labels, which include
    the Traefik routing setup and the constraints, including the platform constraint
    to ensure this runs on Windows nodes. The Metrics view shows me a graph of the
    CPU and memory usage, and a list of all the running containers.
  prefs: []
  type: TYPE_NORMAL
- en: You can use the service view to check the overall status of the service and
    make changes—you can add environment variables, change the networks or volumes,
    and change the scheduling constraints. Any changes you make to the service definition
    will be implemented by restarting the service, so you need to understand the application
    impact. Stateless apps and apps that gracefully handle transient failures can
    be amended on-the-fly, but there may be application downtime—depending on your
    solution architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can adjust the scale of the service without restarting existing tasks.
    Just specify the new level of scale in the Scheduling tab, and UCP will create
    or remove containers to meet the service level:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/aced9423-5414-406d-98d8-8d9b2c5f2c91.png)'
  prefs: []
  type: TYPE_IMG
- en: When you increase the scale, the existing containers are retained and new ones
    are added, so that won't affect the availability of your application (unless the
    app keeps the state in individual containers).
  prefs: []
  type: TYPE_NORMAL
- en: 'From the service view or the container list, under Shared Resources, you can
    select a task to drill down into the container view, which is where the consistent
    management experience makes administering Dockerized applications so straightforward.
    Every detail about the running container is surfaced, including the configuration
    and the actual process list inside the container. This is the container for my
    Traefik proxy, which just has the `traefik` process running:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/77b433fa-222e-45e8-9eed-97b20eebef3f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You can read the logs of the container, which show all the output from the
    container''s standard output streams. These are the logs from Elasticsearch, which
    is a Java application, so these are in `log4j` format:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/718b5db5-c0cf-4b0e-88cf-9bf978e7995f.png)'
  prefs: []
  type: TYPE_IMG
- en: You can view the logs of any container in the cluster in the same way, whether
    it's a new Go app running in a minimal Linux container, or a legacy ASP.NET app
    running in a Windows container. This is why it's so important to build your Docker
    image so that log entries from your app are relayed out to the console.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can even connect to a command-line shell running in the container, if you
    need to troubleshoot a problem. This is the equivalent of running `docker container
    exec -it powershell` in the Docker CLI, but all from the UCP interfaces so you
    don''t need to connect to a specific node on the cluster. You can run any shell
    that''s installed in the container image, so in the Kibana Linux image, I can
    use `bash`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/8a0a0818-0629-407e-83b2-8b2022582d98.png)'
  prefs: []
  type: TYPE_IMG
- en: UCP gives you an interface that lets you drill down from the overall health
    of the cluster, through the status of all the running services, to the individual
    containers running on specific nodes. You can easily monitor the health of your
    applications, check application logs, and connect to containers for debugging—all
    within the same management UI. You can also download a **client bundle**, which
    is a set of scripts and certificates you can use to securely manage the cluster
    from a remote Docker **command-line interface** (**CLI**) client.
  prefs: []
  type: TYPE_NORMAL
- en: The client bundle script points your local Docker CLI to the Docker API running
    on the cluster manager and also sets up client certificates for secure communication.
    The certificates identify a specific user in UCP, whether they have been created
    in UCP or whether they're an external LDAP user. So, users can log into the UCP
    UI or use the `docker` commands to manage resources, and for both options, they
    will have the same access defined by the UCP RBAC policies.
  prefs: []
  type: TYPE_NORMAL
- en: RBAC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Authorization in UCP gives you fine-grained access control over all the Docker
    resources. RBAC in UCP is defined by creating a grant for a subject to access
    a resource set. The subject of a grant can be an individual user, a team of users,
    or an organization containing many teams. A resource set could be an individual
    resource, such as a Docker Swarm service, or a set of resources, such as all the
    Windows nodes in the cluster. The grant defines the level of access, from no access
    to full control.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s a very flexible approach to security, because it allows you to enforce
    security rules at whatever level makes sense for your company. I can use an application-first
    approach, where I have a resource collection called `nerd-dinner` that represents
    the NerdDinner application, and this collection is the parent of other collections
    that represent deployment environments: production, UAT, and system test. The
    collection hierarchy is on the right-hand side of this diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/5fcce0e5-c3dd-4779-8b2c-af4ada0805ec.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Collections are groups of resources—so I would deploy each environment as a
    stack with the resources all belonging to the relevant collection. Organizations
    are the ultimate grouping of users, and here I have a **nerd-dinner** organization
    shown on the left, which is the grouping of all the people who work on NerdDinner.
    In the organization, there are two teams: **Nerd Dinner Ops** are the application
    administrators, and **Nerd Dinner Testers** are the testers. There''s only one
    user shown in the diagram, **elton**, who is a member of the **Nerd Dinner Ops**
    team.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This structure lets me create grants to give access to different resources
    at different levels:'
  prefs: []
  type: TYPE_NORMAL
- en: The **nerd-dinner** organization has **View Only** access to the **nerd-dinner**
    collection, which means any user in any team in the organization can list and
    view the details any of the resources in any environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Nerd Dinner Ops** team also has **Restricted Control** over the **nerd-dinner**
    collection, which means they can run and manage resources in any environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The user **elton** in the **Nerd Dinner Ops** team also has **Full Control**
    over the **nerd-dinner-uat** collection, which gives full admin control to resources
    in the UAT environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Nerd Dinner Testers** team has **Scheduler** access to the **nerd-dinner-test**
    collection, which means members of the team can manage nodes in the test environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The default roles for Docker Swarm collections are **View Only**, **Restricted
    Control**, **Full Control**, and **Scheduler**. You can create your own roles,
    and set specific permissions for specific types of resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'You create grants in UCP to create a role that links a subject to a set of
    resources, giving them known permissions. I''ve deployed the security access diagram
    in my Docker Enterprise cluster, and I can see my grants alongside the default
    system grants:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/6c89cc56-1c9d-4066-a684-0a8fd8b70d1a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You create grants and collections independently of the resources you want to
    secure. Then you specify a collection when you create resources by adding a label,
    with the key `com.docker.ucp.access.label` and the value of the collection name.
    You can do this imperatively in Docker''s create commands, declaratively in Docker
    Compose files, and through the UCP UI. Here, I''ve specified that the reverse-proxy
    service belongs in the `nerd-dinner-prod` collection:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/9b6f6205-6a0e-484f-bc98-825d666d4b17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If I log into UCP as a member of the Nerd Dinner Testers team, I will see only
    that one service. The test users don''t have access to view services from the
    default collection, and only the proxy service has been explicitly put into the
    `nerd-dinner-prod` collection:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/0e34116f-3197-4748-addd-5e1249dd100d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As this user, I only have view access, so if I try to modify the service in
    any way—such as restarting it—I''ll get an error:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/6b0cd91d-15b3-40d0-bdd5-ae53ebf017a1.png)'
  prefs: []
  type: TYPE_IMG
- en: Teams can have multiple permissions for different resource sets, and users can
    belong to multiple teams, so the authorization system in UCP is flexible enough
    to suit many different security models. You could take a DevOps approach and build
    collections for specific projects, with all the team members getting complete
    control over the project resources, or you could have a dedicated admin team with
    complete control over everything. Or you could have individual developer teams,
    where the members have restricted control over the apps they work on.
  prefs: []
  type: TYPE_NORMAL
- en: RBAC is a major feature of UCP, and it complements the wider security story
    of Docker, which I will cover in [Chapter 9](ea2edfd1-c625-4599-8ec2-d5ae811941ef.xhtml),
    *Understanding the Security Risks and Benefits of Docker*.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter focused on the operations side of running Dockerized solutions.
    I showed you how to use existing Windows management tools with Docker containers
    and how that can be useful for investigation and debugging. The main focus was
    on a new way of administering and monitoring applications, using UCP in Docker
    Enterprise to manage all kinds of workloads in the same way.
  prefs: []
  type: TYPE_NORMAL
- en: You learned how to use existing Windows management tools, such as IIS Manager
    and the server manager, to administer Docker containers, and you also learned
    about the limitations of this approach. Sticking with the tools you know can be
    useful when you start with Docker, but dedicated container management tools are
    a better option.
  prefs: []
  type: TYPE_NORMAL
- en: 'I covered two open source options to manage containers: the simple visualizer
    and the more advanced Portainer. Both run as containers and connect to the Docker
    API, and they are cross-platform apps packaged in Linux and Windows Docker images.'
  prefs: []
  type: TYPE_NORMAL
- en: Lastly I walked you through the main features in Docker Enterprise used to manage
    production workloads. I demonstrated UCP as a single pane of glass to administer
    a diverse range of containerized applications, running in multiple technology
    stacks on Linux and Windows containers in the same cluster, and showed how RBAC
    lets you secure access to all of your Docker resources.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter is focused on security. Applications running in containers
    potentially offer a new avenue of attack. You need to be aware of the risks, but
    security is at the center of the Docker platform. Docker lets you easily set up
    an end-to-end security story, where policies are enforced by the platform at runtime—something
    which is very hard to do without Docker.
  prefs: []
  type: TYPE_NORMAL
