- en: Introduction to Large-Scale Machine Learning and Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"Information is the oil of the 21^(st) century, and analytics is the combustion
    engine."'
  prefs: []
  type: TYPE_NORMAL
- en: --Peter Sondergaard, Gartner Research
  prefs: []
  type: TYPE_NORMAL
- en: By 2018, it is estimated that companies will spend $114 billion on big data-related
    projects, an increase of roughly 300%, compared to 2013 ([https://www.capgemini-consulting.com/resource-file-access/resource/pdf/big_data_pov_03-02-15.pdf](https://www.capgemini-consulting.com/resource-file-access/resource/pdf/big_data_pov_03-02-15.pdf)).
    Much of this increase in expenditure is due to how much data is being created
    and how we are better able to store such data by leveraging distributed filesystems
    such as Hadoop.
  prefs: []
  type: TYPE_NORMAL
- en: However, collecting the data is only half the battle; the other half involves
    data extraction, transformation, and loading into a computation system, which
    leverage the power of modern computers to apply various mathematical methods in
    order to learn more about data and patterns, and extract useful information to
    make relevant decisions. The entire data workflow has been boosted in the last
    few years by not only increasing the computation power and providing easily accessible
    and scalable cloud services (for example, Amazon AWS, Microsoft Azure, and Heroku)
    but also by a number of tools and libraries that help to easily manage, control,
    and scale infrastructure and build applications. Such a growth in the computation
    power also helps to process larger amounts of data and to apply algorithms that
    were impossible to apply earlier. Finally, various computation-expensive statistical
    or machine learning algorithms have started to help extract nuggets of information
    from data.
  prefs: []
  type: TYPE_NORMAL
- en: One of the first well-adopted big data technologies was Hadoop, which allows
    for the  MapReduce computation by saving intermediate results on a disk. However,
    it still lacks proper big data tools for information extraction. Nevertheless,
    Hadoop was just the beginning. With the growing size of machine memory, new in-memory
    computation frameworks appeared, and they also started to provide basic support
    for conducting data analysis and modeling—for example, SystemML or Spark ML for
    Spark and FlinkML for Flink. These frameworks represent only the tip of the iceberg—there
    is a lot more in the big data ecosystem, and it is permanently evolving, since
    the volume of data is constantly growing, demanding new big data algorithms and
    processing methods. For example, the **Internet of Things** (**IoT**) represents
    a new domain that produces huge amount of streaming data from various sources
    (for example, home security system, Alexa Echo, or vital sensors) and brings not
    only an unlimited potential to mind useful information from data, but also demands
    new kind of data processing and modeling methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nevertheless, in this chapter, we will start from the beginning and explain
    the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Basic working tasks of data scientists
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aspect of big data computation in distributed environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The big data ecosystem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark and its machine learning support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data science
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finding a uniform definition of data science, however, is akin to tasting wine
    and comparing flavor profiles among friends—everyone has their own definition
    and no one description is *more accurate* than the other. At its core, however,
    data science is the art of asking intelligent questions about data and receiving
    intelligent answers that matter to key stakeholders. Unfortunately, the opposite
    also holds true—ask lousy questions of the data and get lousy answers! Therefore,
    careful formulation of the question is the key for extracting valuable insights
    from your data. For this reason, companies are now hiring *data scientists* to
    help formulate and ask these questions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00005.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1 - Growing Google Trend of big data and data science
  prefs: []
  type: TYPE_NORMAL
- en: The sexiest role of the 21st century – data scientist?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At first, it''s easy to paint a stereotypical picture of what a typical data
    scientist looks like: t-shirt, sweatpants, thick-rimmed glasses, and debugging
    a chunk of code in IntelliJ... you get the idea. Aesthetics aside, what are some
    of the traits of a data scientist? One of our favorite posters describing this
    role is shown here in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00006.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2 - What is a data scientist?
  prefs: []
  type: TYPE_NORMAL
- en: 'Math, statistics, and general knowledge of computer science is given, but one
    pitfall that we see among practitioners has to do with understanding the business
    problem, which goes back to asking intelligent questions of the data. It cannot
    be emphasized enough: asking more intelligent questions of the data is a function
    of the data scientist''s understanding of the business problem and the limitations
    of the data; without this fundamental understanding, even the most intelligent
    algorithm would be unable to come to solid conclusions based on a wobbly foundation.'
  prefs: []
  type: TYPE_NORMAL
- en: A day in the life of a data scientist
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This will probably come as a shock to some of you—being a data scientist is
    more than reading academic papers, researching new tools, and model building until
    the wee hours of the morning, fueled on espresso; in fact, this is only a small
    percentage of the time that a data scientist gets to truly *play* (the espresso
    part however is 100% true for everyone)! Most part of the day, however, is spent
    in meetings, gaining a better understanding of the business problem(s), crunching
    the data to learn its limitations (take heart, this book will expose you to a
    ton of different feature engineering or feature extractions tasks), and how best
    to present the findings to non data-sciencey people. This is where the true *sausage
    making* process takes place, and the best data scientists are the ones who relish
    in this process because they are gaining more understanding of the requirements
    and benchmarks for success. In fact, we could literally write a whole new book
    describing this process from top-to-tail!
  prefs: []
  type: TYPE_NORMAL
- en: 'So, what (and who) is involved in asking questions about data? Sometimes, it
    is process of saving data into a relational database and running SQL queries to
    find insights into data: "for the millions of users that bought this particular
    product, what are the top 3 OTHER products also bought?" Other times, the question
    is more complex, such as, "Given the review of a movie, is this a positive or
    negative review?" This book is mainly focused on complex questions, like the latter.
    Answering these types of questions is where businesses really get the most impact
    from their big data projects and is also where we see a proliferation of emerging
    technologies that look to make this Q and A system easier, with more functionality.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the most popular, open source frameworks that look to help answer data
    questions include R, Python, Julia, and Octave, all of which perform reasonably
    well with small (X < 100 GB) datasets. At this point, it''s worth stopping and
    pointing out a clear distinction between big versus small data. Our general rule
    of thumb in the office goes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*If you can open your dataset using Excel, you are working with small data.*'
  prefs: []
  type: TYPE_NORMAL
- en: Working with big data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'What happens when the dataset in question is so vast that it cannot fit into
    the memory of a single computer and must be distributed across a number of nodes
    in a large computing cluster? Can''t we just rewrite some R code, for example,
    and extend it to account for more than a single-node computation? If only things
    were that simple! There are many reasons why the scaling of algorithms to more
    machines is difficult. Imagine a simple example of a file containing a list of
    names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We would like to compute the number of occurrences of individual words in the
    file. If the file fits into a single machine, you can easily compute the number
    of occurrences by using a combination of the Unix tools, `sort` and `uniq`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as shown ahead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: However, if the file is huge and distributed over multiple machines, it is necessary
    to adopt a slightly different computation strategy. For example, compute the number
    of occurrences of individual words for every part of the file that fits into the
    memory and merge the results together. Hence, even simple tasks, such as counting
    the occurrences of names, in a distributed environment can become more complicated.
  prefs: []
  type: TYPE_NORMAL
- en: The machine learning algorithm using a distributed environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Machine learning algorithms combine simple tasks into complex patterns, that
    are even more complicated in distributed environment. Let''s take a simple decision
    tree algorithm (reference), for example. This particular algorithm creates a binary
    tree that tries to fit training data and minimize prediction errors. However,
    in order to do this, it has to decide about the branch of tree it has to send
    every data point to (don''t worry, we''ll cover the mechanics of how this algorithm
    works along with some very useful parameters that you can learn in later in the
    book). Let''s demonstrate it with a simple example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00007.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3 - Example of red and blue data points covering 2D space.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the situation depicted in preceding figure. A two-dimensional board
    with many points colored in two colors: red and blue. The goal of the decision
    tree is to learn and generalize the shape of data and help decide about the color
    of a new point. In our example, we can easily see that the points almost follow
    a chessboard pattern. However, the algorithm has to figure out the structure by
    itself. It starts by finding the best position of a vertical or horizontal line,
    which would separate the red points from the blue points.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The found decision is stored in the tree root and the steps are recursively
    applied on both the partitions. The algorithm ends when there is a single point
    in the partition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00008.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4 - The final decision tree and projection of its prediction to the original
    space of points.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting of data into multiple machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For now, let's assume that the number of points is huge and cannot fit into
    the memory of a single machine. Hence, we need multiple machines, and we have
    to partition data in such a way that each machine contains only a subset of data.
    This way, we solve the memory problem; however, it also means that we need to
    distribute the computation around a cluster of machines. This is the first difference
    from single-machine computing. If your data fits into a single machine memory,
    it is easy to make decisions about data, since the algorithm can access them all
    at once, but in the case of a distributed algorithm, this is not true anymore
    and the algorithm has to be "clever" about accessing the data. Since our goal
    is to build a decision tree that predicts the color of a new point in the board,
    we need to figure out how to make the tree that will be the same as a tree built
    on a single machine.
  prefs: []
  type: TYPE_NORMAL
- en: The naive solution is to build a trivial tree that separates the points based
    on machine boundaries. But this is obviously a bad solution, since data distribution
    does not reflect color points at all.
  prefs: []
  type: TYPE_NORMAL
- en: Another solution tries all the possible split decisions in the direction of
    the *X* and *Y* axes and tries to do the best in separating both colors, that
    is, divides the points into two groups and minimizes the number of points of another
    color. Imagine that the algorithm is testing the split via the line, *X = 1.6*.
    This means that the algorithm has to ask each machine in the cluster to report
    the result of splitting the machine's local data, merge the results, and decide
    whether it is the right splitting decision. If it finds an optimal split, it needs
    to inform all the machines about the decision in order to record which partition
    each point belongs to.
  prefs: []
  type: TYPE_NORMAL
- en: Compared with the single machine scenario, the distributed algorithm constructing
    decision tree is more complex and requires a way of distributing the computation
    among machines. Nowadays, with easy access to a cluster of machines and an increasing
    demand for the analysis of larger datasets, it becomes a standard requirement.
  prefs: []
  type: TYPE_NORMAL
- en: 'Even these two simple examples show that for a larger data, proper computation
    and distributed infrastructure is required, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A distributed data storage, that is, if the data cannot fit into a single node,
    we need a way to distribute and process them on multiple machines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A computation paradigm to process and transform the distributed data and to
    apply mathematical (and statistical) algorithms and workflows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support to persist and reuse defined workflows and models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support to deploy statistical models in production
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In short, we need a framework that will support common data science tasks. It
    can be considered an unnecessary requirement, since data scientists prefer using
    existing tools, such as R, Weka, or Python's scikit. However, these tools are
    neither designed for large-scale distributed processing nor for the parallel processing
    of large data. Even though there are libraries for R or Python that support limited
    parallel or distributed programming, their main limitation is that the base platforms,
    that is R and Python, were not designed for this kind of data processing and computation.
  prefs: []
  type: TYPE_NORMAL
- en: From Hadoop MapReduce to Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With a growing amount of data, the single-machine tools were not able to satisfy
    the industry needs and thereby created a space for new data processing methods
    and tools, especially Hadoop MapReduce, which is based on an idea originally described
    in the Google paper, *MapReduce: Simplified Data Processing on Large Clusters* ([https://research.google.com/archive/mapreduce.html](https://research.google.com/archive/mapreduce.html)).
    On the other hand, it is a generic framework without any explicit support or libraries
    to create machine learning workflows. Another limitation of classical MapReduce
    is that it performs many disk I/O operations during the computation instead of
    benefiting from machine memory.'
  prefs: []
  type: TYPE_NORMAL
- en: As you have seen, there are several existing machine learning tools and distributed
    platforms, but none of them is an exact match for performing machine learning
    tasks with large data and distributed environment. All these claims open the doors
    for Apache Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Enter the room, Apache Spark!
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00009.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Created in 2010 at the UC Berkeley AMP Lab (Algorithms, Machines, People), the
    Apache Spark project was built with an eye for speed, ease of use, and advanced
    analytics. One key difference between Spark and other distributed frameworks such
    as Hadoop is that datasets can be cached in memory, which lends itself nicely
    to machine learning, given its iterative nature (more on this later!) and how
    data scientists are constantly accessing the same data many times over.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark can be run in a variety of ways, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Local mode:** This entails a single **Java Virtual Machine** (**JVM**) executed
    on a single host'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Standalone Spark cluster:** This entails multiple JVMs on multiple hosts'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Via resource manager such as Yarn/Mesos:** This application deployment is
    driven by a resource manager, which controls the allocation of nodes, application,
    distribution, and deployment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is Databricks?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you know about the Spark project, then chances are high that you have also
    heard of a company called *Databricks*. However, you might not know how Databricks
    and the Spark project are related to one another. In short, Databricks was founded
    by the creators of the Apache Spark project and accounts for over 75% of the code
    base for the Spark project. Aside from being a huge force behind the Spark project
    with respect to development, Databricks also offers various certifications in
    Spark for developers, administrators, trainers, and analysts alike. However, Databricks
    is not the only main contributor to the code base; companies such as IBM, Cloudera,
    and Microsoft also actively participate in Apache Spark development.
  prefs: []
  type: TYPE_NORMAL
- en: As a side note, Databricks also organizes the Spark Summit (in both Europe and
    the US), which is the premier Spark conference and a great place to learn about
    the latest developments in the project and how others are using Spark within their
    ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this book, we will give recommended links that we read daily that
    offer great insights and also important changes with respect to the new versions
    of Spark. One of the best resources here is the Databricks blog, which is constantly
    being updated with great content. Be sure to regularly check this out at [https://databricks.com/blog](https://databricks.com/blog).
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, here is a link to see the past Spark Summit talks, which you may find
    helpful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://slideshare.net/databricks](http://slideshare.net/databricks).'
  prefs: []
  type: TYPE_NORMAL
- en: Inside the box
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, you have downloaded the latest version of Spark (depending on how you plan
    on launching Spark) and you have run the standard *Hello, World!* example....what
    now?!
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark comes equipped with five libraries, which can be used separately--or
    in unison--depending on the task we are trying to solve. Note that in this book,
    we plan on using a variety of different libraries, all within the same application
    so that you will have the maximum exposure to the Spark platform and better understand
    the benefits (and limitations) of each library. These five libraries are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Core**: This is the Spark core infrastructure, providing primitives to represent
    and store data called **Resilient Distributed Dataset** (**RDDs**) and manipulate
    data with tasks and jobs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SQL** : This library provides user-friendly API over core RDDs by introducing
    DataFrames and SQL to manipulate with the data stored.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MLlib (Machine Learning Library)** : This is Spark''s very own machine learning
    library of algorithms developed in-house that can be used within your Spark application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Graphx** : This is used for graphs and graph-calculations; we will explore
    this particular library in depth in a later chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Streaming** : This library allows real-time streaming of data from various
    sources, such as Kafka, Twitter, Flume, and TCP sockets, to name a few. Many of
    the applications we will build in this book will leverage the MLlib and Streaming
    libraries to build our applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00010.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The Spark platform can also be extended by third-party packages. There are many
    of them, for example, support for reading CSV or Avro files, integration with
    Redshift, and Sparkling Water, which encapsulates the H2O machine learning library.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing H2O.ai
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: H2O is an open source, machine learning platform that plays extremely well with
    Spark; in fact, it was one of the first third-party packages deemed "Certified
    on Spark".
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00011.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Sparkling Water (H2O + Spark) is H2O's integration of their platform within
    the Spark project, which combines the machine learning capabilities of H2O with
    all the functionality of Spark. This means that users can run H2O algorithms on
    Spark RDD/DataFrame for both exploration and deployment purposes. This is made
    possible because H2O and Spark share the same JVM, which allows for seamless transitions
    between the two platforms. H2O stores data in the H2O frame, which is a columnar-compressed
    representation of your dataset that can be created from Spark RDD and/or DataFrame.
    Throughout much of this book, we will be referencing algorithms from Spark's MLlib
    library and H2O's platform, showing how to use both the libraries to get the best
    results possible for a given task.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a summary of the features Sparkling Water comes equipped with:'
  prefs: []
  type: TYPE_NORMAL
- en: Use of H2O algorithms within a Spark workflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformations between Spark and H2O data structures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use of Spark RDD and/or DataFrame as inputs to H2O algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use of H2O frames as inputs into MLlib algorithms (will come in handy when we
    do feature engineering later)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transparent execution of Sparkling Water applications on top of Spark (for example,
    we can run a Sparkling Water application within a Spark stream)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The H2O user interface to explore Spark data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Design of Sparkling Water
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sparkling Water is designed to be executed as a regular Spark application. Consequently,
    it is launched inside a Spark executor created after submitting the application.
    At this point, H2O starts services, including a distributed key-value (K/V) store
    and memory manager, and orchestrates them into a cloud. The topology of the created
    cloud follows the topology of the underlying Spark cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'As stated previously, Sparkling Water enables transformation between different
    types of RDDs/DataFrames and H2O''s frame, and vice versa. When converting from
    a hex frame to an RDD, a wrapper is created around the hex frame to provide an
    RDD-like API. In this case, data is not duplicated but served directly from the
    underlying hex frame. Converting from an RDD/DataFrame to a H2O frame requires
    data duplication because it transforms data from Spark into H2O-specific storage.
    However, data stored in an H2O frame is heavily compressed and does not need to
    be preserved as an RDD anymore:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00012.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Data sharing between sparkling water and Spark
  prefs: []
  type: TYPE_NORMAL
- en: What's the difference between H2O and Spark's MLlib?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As stated previously, MLlib is a library of popular machine learning algorithms
    built using Spark. Not surprisingly, H2O and MLlib share many of the same algorithms
    but differ in both their implementation and functionality. One very handy feature
    of H2O is that it allows users to visualize their data and perform feature engineering
    tasks, which we will cover in depth in later chapters. The visualization of data
    is accomplished by a web-friendly GUI and allows users a friendly interface to
    seamlessly switch between a code shell and a notebook-friendly environment. The
    following is an example of the H2O notebook - called *Flow* - that you will become
    familiar with soon:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00013.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: One other nice addition is that H2O allows data scientists to grid search many
    hyper-parameters that ship with their algorithms. Grid search is a way of optimizing
    all the hyperparameters of an algorithm to make model configuration easier. Often,
    it is difficult to know which hyperparameters to change and how to change them;
    the grid search allows us to explore many hyperparameters simultaneously, measure
    the output, and help select the best models based on our quality requirements.
    The H2O grid search can be combined with model cross-validation and various stopping
    criteria, resulting in advanced strategies such as *picking 1000 random parameters
    from a huge parameters hyperspace and finding the best model that can be trained
    under two minutes and with AUC greater than 0.7*
  prefs: []
  type: TYPE_NORMAL
- en: Data munging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Raw data for problems often comes from multiple sources with different and often
    incompatible formats. The beauty of the Spark programming model is its ability
    to define data operations that process the incoming data and transform it into
    a regular form that can be used for further feature engineering and model building.
    This process is commonly referred to as data munging and is where much of the
    battle is won with respect to data science projects. We keep this section intentionally
    brief because the best way to showcase the power--and necessity!--of data munging
    is by example. So, take heart; we have *plenty* of practice to go through in this
    book, which emphasizes this essential process.
  prefs: []
  type: TYPE_NORMAL
- en: Data science - an iterative process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Often, the process flow of many big data projects is iterative, which means
    a lot of back-and-forth testing new ideas, new features to include, tweaking various
    hyper-parameters, and so on, with a *fail fast* attitude. The end result of these
    projects is usually a model that can answer a question being posed. Notice that
    we didn't say *accurately* answer a question being posed! One pitfall of many
    data scientists these days is their inability to generalize a model for new data,
    meaning that they have overfit their data so that the model provides poor results
    when given new data. Accuracy is extremely task-dependent and is usually dictated
    by the business needs with some sensitivity analysis being done to weigh the cost-benefits
    of the model outcomes. However, there are a few standard accuracy measures that
    we will go over throughout this book so that you can compare various models to
    see *how* changes to the model impact the result.
  prefs: []
  type: TYPE_NORMAL
- en: H2O is constantly giving meetup talks and inviting others to give machine learning
    meetups around the US and Europe. Each meetup or conference slides is available
    on SlideShare ([http://www.slideshare.com/0xdata](http://www.slideshare.com/0xdata))
    or YouTube. Both the sites serve as great sources of information not only about
    machine learning and statistics but also about distributed systems and computation.
    For example, one of the most interesting presentations highlights the "Top 10
    pitfalls in a data scientist job" ([http://www.slideshare.net/0xdata/h2o-world-top-10-data-science-pitfalls-mark-landry](http://www.slideshare.net/0xdata/h2o-world-top-10-data-science-pitfalls-mark-landry))
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we wanted to give you a brief glimpse into the life of a data
    scientist, what this entails, and some of the challenges that data scientists
    consistently face. In light of these challenges, we feel that the Apache Spark
    project is ideally positioned to help tackle these topics, which range from data
    ingestion and feature extraction/creation to model building and deployment. We
    intentionally kept this chapter short and light on verbiage because we feel working
    through examples and different use cases is a better use of time as opposed to
    speaking abstractly and at length about a given data science topic. Throughout
    the rest of this book, we will focus solely on this process while giving best-practice
    tips and recommended reading along the way for users who wish to learn more. Remember
    that before embarking on your next data science project, be sure to clearly define
    the problem beforehand, so you can ask an intelligent question of your data and
    (hopefully) get an intelligent answer!
  prefs: []
  type: TYPE_NORMAL
- en: One awesome website for all things data science is KDnuggets ([http://www.kdnuggets.com](http://www.kdnuggets.com)).
    Here's a great article on the language all data scientists must learn in order
    to be successful ([http://www.kdnuggets.com/2015/09/one-language-data-scientist-must-master.html](http://www.kdnuggets.com/2015/09/one-language-data-scientist-must-master.html)).
  prefs: []
  type: TYPE_NORMAL
