- en: Scraping at 100x
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By now, you should have a very broad understanding of how to build a solid
    web scraper. Up to this point, you have learned how to collect information from
    the internet efficiently, safely, and respectfully. The tools that you have at
    your disposal are enough to build web scrapers on a small to medium scale, which
    may be just what you need to accomplish your goals. However, there may come a
    day when you need to upscale your application to handle large and production-sized
    projects. You may be lucky enough to make a living out of offering services, and,
    as that business grows, you will need an architecture that is robust and manageable.
    In this chapter, we will review the architectural components that make a good
    web scraping system, and look at example projects from the open source community.
    Here are the topics we will discuss:'
  prefs: []
  type: TYPE_NORMAL
- en: Components of a web scraping system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scraping HTML pages with colly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scraping JavaScript pages with chrome-protocol
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed scraping with dataflowkit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Components of a web scraping system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 7](1f846c4e-a180-4341-bced-ebf1ed109211.xhtml), *Scraping with Concurrency*,
    about concurrency, we saw how defining a clear separation of roles between the
    worker goroutines and the main goroutine helped mitigate issues in the program.
    By clearly giving the main goroutine the responsibility of maintaining the state
    of the target URLs, and allowing the scraper threads to focus on scraping, we
    laid the groundwork for making a modular system which can easily scale components
    independently. This separation of concerns is the foundation for building large-scale
    systems of any kind.
  prefs: []
  type: TYPE_NORMAL
- en: There are a few main components that make up a web scraper. Each of these components
    should be able to scale without affecting other parts of the system, if they are
    properly decoupled. You will know if this decoupling is solid if you can break
    this system into its own package and reuse it for other projects. You might even
    want to release it to the open source community! Let's take a look at some of
    these components.
  prefs: []
  type: TYPE_NORMAL
- en: Queue
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before a web scraper can start collecting information, it needs to know where
    to go. It also needs to know where it has been. A proper queuing system will accomplish
    both of these goals. Queues can be set up in many different ways. In many of the
    previous examples, we used a `[]string` or a `map[string]string` to hold the target
    URLs the scraper should pursue. This works for smaller scale web scrapers where
    the work is being pushed to the workers.
  prefs: []
  type: TYPE_NORMAL
- en: In larger applications, a work-stealing queue would be preferred. In a work-stealing
    queue, the worker threads would take the first available job out of the queue
    as fast as they can accomplish the task. This way, if you need your system to
    increase throughput, you can simply add more worker threads. In this system, the
    queue does not need to concern itself with the status of the workers, and focuses
    only on the status of the jobs. This is beneficial to systems that push to the
    workers because it must be aware of how many workers there are, which workers
    are busy or free, and handles workers coming on and offline.
  prefs: []
  type: TYPE_NORMAL
- en: Queuing systems are not always a part of the main scraping application. There
    are many suitable solutions for external queues, such as databases, or streaming
    platforms, such as Redis and Kafka. These tools will support your queuing system
    to the limits of your own imagination.
  prefs: []
  type: TYPE_NORMAL
- en: Cache
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have seen in [Chapter 3](16487efd-3a75-4823-ad19-627a83752cd4.xhtml), *Web
    Scraping Etiquette*, caching web pages is an essential part of an efficient web
    scraper. With a cache, we are able to avoid requesting content from a website
    if we know nothing has changed. In our previous examples, we used a local cache
    which saves the content into a folder on the local machine. In larger web scrapers
    with multiple machines, this causes problems, as each machine would need to maintain
    its own cache. Having a shared caching solution would solve this problem and increase
    the efficiency of your web scraper.
  prefs: []
  type: TYPE_NORMAL
- en: There are many different ways to approach this problem. Much like the queuing
    system, a database can help store a cache of your information. Most databases
    support storage of binary objects, so whether you are storing HTML pages, images,
    or any other content, it is possible to put it into a database. You can also include
    a lot of metadata about a file, such as a date it was recovered, the date it expires,
    the size, the Etag, and so on. Another caching solution you can use is a form
    of cloud object storage, such as Amazon S3, Google Cloud Store, and Microsoft
    object storage. These services typically offer low-cost storage solutions that
    mimic a file system and require a specific SDK, or use of their APIs. A third
    solution you could use is a **Network File System** (**NFS**) where each node
    would connect. Writing to cache on an NFS would be the same as if it were on the
    local file system, as far as your scraper code is concerned. There can be challenges
    in configuring your worker machines to connect to an NFS. Each of these approaches
    has its own unique set of pros and cons, depending on your own setup.
  prefs: []
  type: TYPE_NORMAL
- en: Storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In most cases, when you are scraping the web, you will be looking for very specific
    information. This is probably going to be a very small amount of data relative
    to the size of the web page itself. Because of the cache stores the entire contents
    of the web page, you will need some other storage system to store the parsed information.
    The storage component of a web scraper could be as simple as a text file, or as
    large as a distributed database.
  prefs: []
  type: TYPE_NORMAL
- en: These days, there are many database solutions available to satisfy different
    needs. If you have data that has many intricate relationships, then an SQL database
    might be a good fit for you. If you have data that has more of a nested structure,
    then you may want to look at NoSQL databases. There are also solutions that offer
    full-text indexing to make searching for documents easier, and time-series databases
    if you need to relate your data to some chronological order. Because there is
    no one-size-fits-all solution, the Go standard library only offers a package to
    handle the most common family of databases through the `sql` package.
  prefs: []
  type: TYPE_NORMAL
- en: The `sql` package was built to provide a common set of functions used to communicate
    with SQL databases such as MySQL, PostgreSQL, and Couchbase. For each of these
    databases, a separate driver has been written to fit into the framework defined
    by the `sql` package. These drivers, along with various others, can be found on
    GitHub and easily integrated with your project. The core of the `sql` package
    provides methods for opening and closing database connections, querying the database,
    iterating through rows of results, and performing inserts and modifications to
    the data. By mandating a standard interface for drivers, Go allows you to swap
    out your database for another SQL database with less effort.
  prefs: []
  type: TYPE_NORMAL
- en: Logs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One system that is often overlooked during the design of a scraping system is
    the logging system. It is important, first and foremost, to have clear log statements
    without logging too many unnecessary items. These statements should be informing
    the operator of the current status of scraping and any errors, or successes, the
    scraper encounters. This helps you get a picture of the overall health of your
    web scraper.
  prefs: []
  type: TYPE_NORMAL
- en: The simplest logging that can be done is printing messages to the terminal with
    `println()` or `fmt.Println()` type statements. This works well enough for a single
    node, but, as your scraper grows into a distributed architecture, it causes problems.
    In order to check how things are running in your system an operator would need
    to log into each machine to look at the logs. If there is an actual problem in
    the system, it may be difficult to diagnose by trying to piece together logs from
    multiple sources. A logging system built for distributed computing would be ideal
    at this point.
  prefs: []
  type: TYPE_NORMAL
- en: There are many logging solutions available in the open source world. One of
    the more popular choices is Graylog. Setting up a Graylog server is a simple process,
    requiring a MongoDB database and an Elasticsearch database to support it. Graylog
    defines a JSON format called GELF for sending log data to its servers, and accepts
    a very flexible set of keys. Graylog servers can accept log streams from multiple
    sources and you can define post-processing actions as well, such as reformatting
    data and sending alerts based on user-defined rules. There are many other similar
    systems, as well as paid services, that offer very similar features.
  prefs: []
  type: TYPE_NORMAL
- en: As there are various logging solutions, the open source community has built
    a library that eases the burden of integrating with different systems. The `logrus`
    package by GitHub user `sirupsen` provides a standard utility for writing log
    statements, as well as a plugin architecture for log formatters. Many people have
    built formatters for logging statements, including one for GELF statements to
    be sent to a Graylog server. If you decide to change your logging server during
    the development of your scraper, you need only to change the formatter instead
    of replacing all of your log statements.
  prefs: []
  type: TYPE_NORMAL
- en: Scraping HTML pages with colly
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`colly` is one of the available projects on GitHub that covers most of the
    systems discussed earlier. This project is built to run on a single machine, due
    to its reliance on a local cache and queuing system.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3d2669cc-87d2-4eb3-979a-7a70d075a46c.png)'
  prefs: []
  type: TYPE_IMG
- en: The main worker object in `colly`, the `Collector`, is built to run in its own
    goroutine, allowing you to run multiple `Collectors` simultaneously. This design
    offers you the ability to scrape from multiple sites at the same time with different
    parameters, such as crawl delays, white and blacklists, and proxies.
  prefs: []
  type: TYPE_NORMAL
- en: '`colly` is built to only process HTML and XML files. It does not offer support
    for JavaScript execution. However, you would be surprised at how much information
    you can collect with pure HTML. The following example is adapted from the GitHub
    `README`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Before running this example, download `colly` via:'
  prefs: []
  type: TYPE_NORMAL
- en: '`go get github.com/gocolly/colly/...`'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, a `Collector` is created and defines a whitelist for `go-colly.org`,
    and a callback using the `OnHTML()` function. In this function, it performs a
    CSS query for `<a>` tags containing the `href` attribute. The callback specifies
    that the collector should navigate to the endpoint contained in that link. For
    each new page it visits, it repeats the process of visiting each link. Another
    callback is added to the collector using the `OnRequest()` function. This callback
    prints the name of the URL of each site it visits. As you can see, the `Collector`
    performs a depth-first crawl of the website because it follows each link as deep
    as it can go, before checking the other links on the same page.
  prefs: []
  type: TYPE_NORMAL
- en: '`colly` provides many other features, such as respecting `robots.txt`, an extendable
    storage system for the queue, and various callbacks for different events in the
    system. This project is a great starting point for any web scraper that only requires
    HTML pages. It does not need much to set up and have a flexible system for parsing
    HTML pages.'
  prefs: []
  type: TYPE_NORMAL
- en: Scraping JavaScript pages with chrome-protocol
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 5](a4a15c5d-908d-4a1c-bce4-0bf2181c80e3.xhtml), *Web Scraping Navigation*,
    we looked at navigating websites that require JavaScript using `selenium` and
    the WebDriver protocol. There is another protocol that has been developed recently
    that offers many more features you can take advantage of to drive a web browser.
    The Chrome DevTools Protocol was started for use on Chrome browsers, but it has
    been adopted by the *W3C's Web Platform Incubator Community Group* as a project.
    The major web browsers work together to develop a standard protocol called the
    DevTools Protocol to adopt for all of their browsers.
  prefs: []
  type: TYPE_NORMAL
- en: The DevTools Protocol allows external programs to connect to a web browser and
    send commands to run JavaScript, and collect information from the browser. Most
    importantly, the protocol allows the program to collect the HTML on demand. This
    way, if you were scraping a web page in which search results were loaded via JavaScript,
    you could wait for the results to display, request the HTML page, and continue
    parsing the information needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `chrome-protocol` project on GitHub, developed by the GitHub user `4ydx`,
    provides access to use the DevTools Protocol to drive compatible web browsers.
    Because these browsers expose a port, much like a web server does, you can run
    browsers on multiple machines. Using the `chrome-protocol` package, you would
    connect to the browser through a port and start building a series of tasks such
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Navigate`: Opens a web page'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FindAll`: Searches for elements by CSS query'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Click`: Sends click events to a specific element'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many more actions that you can send to a browser, and, by building
    your own custom script, you can navigate through JavaScript websites and collect
    the data that you need.
  prefs: []
  type: TYPE_NORMAL
- en: Example – Amazon Daily Deals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following example, we will use `chrome-protocol` and `goquery` to retrieve
    the Daily Deals from [amazon.com](http://amazon.com). This example is a bit complex
    so the program has been broken into smaller chunks, which we will go through piece
    by piece. Let''s begin with the package and `import` statements, as shown in the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This block of code imports the necessary packages to run the rest of the program.
    Some new packages that we have not seen before are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`encoding/json`: Go standard library for handling JSON data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`github.com/4ydx/chrome-protocol`: Open source library for using the DevTools
    Protocol'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`github.com/4ydx/chrome-protocol/actions`: Open source library defining the
    DevTools Protocol actions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`github.com/4ydx/cdp/protocol/dom`: Open source library for handling DOM nodes
    with `chrome-protocol`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The rest of the imported libraries should be familiar to you as we have used
    them in previous chapters. Next, we will define two functions: one function for
    retrieving the HTML page from Amazon, and the second to parse the results with
    `goquery`. The following code shows the function for retrieving the HTML data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The function begins by opening a new instance of a Google Chrome browser and
    obtaining a handle for it for future commands. We use the `actions.EnableAll()`
    function to ensure that all of the events happening in the Chrome browser are
    sent back to our program so we do not miss anything. Next, we navigate to [https://www.amazon.com/gp/goldbox](https://www.amazon.com/gp/goldbox),
    which is Amazon's Daily Deals web page.
  prefs: []
  type: TYPE_NORMAL
- en: If you were to retrieve this page with a simple `GET` command, you would get
    a fairly empty shell of HTML code with a lot of JavaScript files waiting to be
    run. Making the request in the browser automatically runs the JavaScript that
    populates the remaining content.
  prefs: []
  type: TYPE_NORMAL
- en: The function then enters a `for` loop which checks for our HTML element that
    contains the daily deals data to populate in the page. The `for` loop will check
    every second for 5 seconds (as defined by the retries variable) before it either
    finds results or gives up. If there are no results, we exit the program. Next,
    the function sends a request to the browser to retrieve the `<body>` element via
    a JavaScript command. The processing of the results is a bit tricky as the value
    of the reply needs to be processed as a JSON string in order to return the raw
    HTML content. Once the content is parsed out, the function returns it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second function, responsible for parsing the HTML content is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Much like the example, we saw in [Chapter 4](e6f2de6a-420b-4691-9ba1-969ed6ad32ea.xhtml), *Parsing
    HTML*, we use `goquery` to first look for the HTML element that contains the results.
    Within that container, we iterate through the details for each daily deal item,
    pulling out the title and the price for each item. We then append each product's
    title and price string to an array and return that array.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `main` function ties these two functions together, first retrieving the
    body of the HTML page, then passing that on to parse the results. The `main` function
    then prints the title and price of each of the daily deals. The `main` function
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, driving a web browser can be more difficult than scraping with
    just simple HTTP requests, but it can be done.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed scraping with dataflowkit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you have seen the progression of building fully featured web scrapers,
    I would like to introduce you to the most complete web scraping project in Go
    that has been built today. `dataflowkit`, by GitHub user `slotix`, is a fully
    featured web scraper that is modular and extensible for building scalable, large-scale
    distributed applications. It allows for multiple backends for storage of cached
    and computed information and is capable of both simple HTTP requests as well as
    driving browsers through the DevTools Protocol. Going above and beyond, `dataflowkit`
    has both a command-line interface and a JSON format to declare web scraping scripts.
  prefs: []
  type: TYPE_NORMAL
- en: 'The architecture of `dataflowkit` is separated into two distinct parts: fetching
    and parsing. Both Fetch and Parse phases of the system are built as separate binaries
    to be run on different machines. They communicate over HTTP via an API, as would
    you if you need to send or receive any information. By running these as separate
    entities, fetching operations and parsing operations can scale independently as
    the system grows. Depending on what type of sites you scrape, you may need more
    fetchers than scrapers, as JavaScript sites tend to require more resources. Once
    the page has been received, parsing the page often provides little overhead.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started using `dataflowkit`, you can either clone it from GitHub using
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'or via `go get`, using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The Fetch service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Fetch service is responsible for retrieving HTML data either via simple
    HTTP requests, or driving a web browser such as Google Chrome. To get started
    using the Fetch service, first, navigate to your local repository and run `go
    build` from the `cmd/fetch.d` directory. Once the build completes, you can start
    the service via `./fetch.d`.
  prefs: []
  type: TYPE_NORMAL
- en: An instance of Google Chrome browser must be started prior to starting the Fetch
    service. This instance must be started with the `--remote-debugging-port` option
    set (usually to 9222). You may use the `--headless` flag as well to run without
    displaying any content.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Fetch service is now ready to accept commands. You should now open a second
    terminal window and navigate to the `cmd/fetch.cli` directory and run `go build`.
    This builds the CLI tool that you can use to send commands to the Fetch service.
    Using the CLI, you can tell the Fetch service to retrieve a web page on your behalf,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This can also be done with a simple JSON `POST` request to `/fetch` of the
    Fetch service. In Go, you would write something like the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `fetch.Request` object is a convenient way of structuring our `POST` request
    data, and the `json` library makes it easy to attach as the request body. Most
    of the rest of the code you have already seen in earlier chapters. In this example,
    we use the basic type of fetcher which only uses HTTP requests. If we needed to
    drive a browser instead, we would be able to send actions to the browser in our
    request.
  prefs: []
  type: TYPE_NORMAL
- en: 'Actions are sent as an array of JSON objects representing a small subset of
    commands. As of right now, only the click and paginate commands are supported.
    If you want to send a `click` command to the browser, your Fetch request would
    look similar to the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: By communicating with the external Fetch service, you can easily control the
    switch back and forth between HTTP requests and driving web browsers. Combined
    with the power of remote execution, you can make sure that you size the right
    machines for the right jobs.
  prefs: []
  type: TYPE_NORMAL
- en: The Parse service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Parse service is responsible for parsing data out of an HTML page and returning
    it in an easily usable format, such as CSV, XML, or JSON. The Parse service relies
    on the Fetch service to retrieve the page, and does not function on its own. To
    get started using the Parse service, first navigate to your local repository and
    run `go build` from the `cmd/parse.d` directory. Once the build completes, you
    can start the service via `./parse.d`. There are many options you can set when
    configuring the Parse service that will determine the backend it uses to cache
    the results: how to handle pagination, the location of the Fetch service, and
    so on. For now, we will use the standard defaults.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To send commands to the Parse service, you use `POST` requests to the `/parse`
    endpoint. The body of the request contains information on what site to open, how
    to map HTML elements to fields and, and how to format the returned data.  Let''s
    look at the daily deals example from [Chapter 4](e6f2de6a-420b-4691-9ba1-969ed6ad32ea.xhtml), *Parsing
    HTML*, and build a request for the Parse service. First, we will look at the `package`
    and `import` statements, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, you can see where we import the necessary `dataflowkit` packages. The
    `fetch` package is used in this example to build the request for the Parse service
    to send to the Fetch service. You can see it in the `main` function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This `scrape.Payload` object is what we use to communicate with the Parse service.
    It defines the request to make to the Fetch service, as well as how to collect
    and format our data. In our case, we want to collect rows of two fields: the title
    and the price. We use CSS selectors to define where to find the fields, and where
    to extract the data from. The `Extractor` that this program will use is the text
    extractor which will copy all of the inner text for the matching element.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we send the request to the Parse service and wait for the result,
    as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The Parse service replies with a JSON object summarizing the whole process,
    including where we can find the file containing the results, as shown in the following
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The convenience that the Parse service offers, allows you as a user to be even
    more creative by building on top of it. With systems that are open source, and
    composable, you can start with a solid foundation and apply your best skills towards
    making a complete system. You are armed with enough knowledge and enough tools
    to build efficient and powerful systems, but I hope your learning does not stop
    here!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked under the hood at the components that make a solid
    web scraping system. We used `colly` to scrape HTML pages that did not require
    JavaScript. We used `chrome-protocol` to drive web browsers to scrape sites that
    do require JavaScript. Finally, we examined `dataflowkit` and saw how its architecture
    opens the door for building distributed web crawlers. There is more to learn and
    do when it comes to building distributed systems in Go, but this is where the
    scope of this book ends. I hope you check out some other publications on building
    applications in Go and continue to hone your skills!
  prefs: []
  type: TYPE_NORMAL
