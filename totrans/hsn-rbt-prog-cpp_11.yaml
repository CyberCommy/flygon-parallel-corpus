- en: Face Detection and Tracking Using the Haar Classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we programmed the robot to detect a ball object and
    follow it. In this chapter, we will take our detection skills to the next level
    by detecting and tracking a human face, detecting human eyes, and recognizing
    a smile.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn about the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Face detection using the Haar cascade
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting the eyes and smile
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Face-tracking robot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you will need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Three LEDs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **Raspberry Pi** (**RPi**) robot (with the Raspberry Pi camera module connected
    to the RPi)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code files for this chapter can be downloaded from [https://github.com/PacktPublishing/Hands-On-Robotics-Programming-with-Cpp/tree/master/Chapter08](https://github.com/PacktPublishing/Hands-On-Robotics-Programming-with-Cpp/tree/master/Chapter08).
  prefs: []
  type: TYPE_NORMAL
- en: Face detection using the Haar cascade
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Paul Viola and Micheal Jones proposed the Haar feature-based cascade classifier
    in their paper, *Rapid Object Detection using a Boosted Cascade of Simple Features*,
    in 2001\. The Haar feature-based cascade classifier is trained using facial images
    as well as non-facial images. The Haar cascade classifier can not only detect
    a frontal face but can also detect the eyes, mouth, and nose of a person. The
    Haar feature-based classifier is also referred to as the Viola-Jones algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Basic working of the Viola-Jones algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So, put simply, the Viola-Jones algorithm used Haar features to detect a face.
    Haar generally consists of two main features: **edge features** and **line features**.
    We will first understand these two features and then we will see how these features
    are used to detect faces:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Edge features**: This is generally used to detect edges. The edge feature
    consists of white and black pixels. Edge features can be further categorized into
    horizontal edge features and vertical edge features. In the following diagram,
    we can see the vertical edge feature on the left block and the horizontal edge
    feature on the right block:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/16d96ff3-7843-4623-aece-7785b805f5c7.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Line features**: This is generally used to detect lines. In line features,
    a white pixel is sandwiched between two black pixels, or a black pixel will be
    sandwiched between two white pixels. In the following diagram, you can see the
    two horizontal line features on the left, one below the other, and the vertical
    line features on the right, next to each other:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/678d0003-8bce-4105-b5b2-1dfbfeb26524.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Face detection is always performed on a grayscale image, but this means that
    in a grayscale image, we may not have completely black and white pixels. So, let''s
    refer to white pixels as brighter pixels and black pixels as darker pixels. If
    we look at the following grayscale face picture, the forehead region is lighter
    (brighter pixels) compared to the eyebrow region (darker pixels):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1c6d2bdc-a728-45e0-8594-b7b60f82c44d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The area of the nose line is brighter compared to the eye and cheeks regions.
    Similarly, if we look at the mouth region, the upper lip region is darker, the
    teeth region is brighter, and the lower lip region is dark again:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/086200cb-307d-472a-9aee-79207f357ced.png)'
  prefs: []
  type: TYPE_IMG
- en: This is how, by using the edge and line features of the Haar cascade, we can
    detect the most relevant feature points in a human face, such as the eyes, nose,
    and mouth.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenCV 4.0 consists of different pre-trained Haar detectors, which can be used
    to detect a human face, including its eyes, nose, smile, and so on. Inside the
    `Opencv-4.0.0` folder, there is a `Data` folder, and inside the `Data` folder,
    you will find the `haarcascades` folder. In this folder, you will find different
    Haar cascade classifiers. For frontal face detection, we will use the`haarcascade_frontalface_alt2.xml`detector. In
    the following screenshot, you can see the path of the `haarcascades` folders,
    with different Haar cascade classifiers present inside:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7defffc4-3716-40fe-8d85-5d5ed6b368c1.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we understand the basics of Viola-Jones features, we will program our
    robot to detect a human face using the Haar cascade.
  prefs: []
  type: TYPE_NORMAL
- en: Face-detection program
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's write a program to detect a human face. I have named this program `FaceDetection.cpp`
    and you can download it from the `Chapter08` folder of this book's GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: Since we will be using `haarcascade_frontalface_alt2.xml` to detect faces, please
    make sure that the `FaceDetection.cpp` and `haarcascade_frontalface_alt2.xml` files
    are in the same folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'To program face detection, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `FaceDetection.cpp` program, load the Haar''s pre-trained frontal face
    XML using the `CascadeClassifier` class, as shown in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Declare two matrix variables, called `videofeed` and `grayfeed`, along with
    a `VideoCapture` variable, called `vid(0)`, to capture footage from the RPi camera:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Inside the `for` loop, read the camera feed. Then, flip the camera feed horizontally.
    Using the `cvtColor` function, we can convert our `videofeed` into `grayscale`.
    If your Pi camera is placed upside-down, set the third parameter inside `flip`
    function to `0`. The `grayscale` output is stored in the `grayfeed` variable.
    The following code shows how to complete this step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s perform a histogram equalization to improve the brightness and contrast
    of `videofeed`. Histogram equalization is required because sometimes, in low lighting,
    the camera may not be able to detect the face. To perform histogram equalization,
    we will use the `equalizeHist` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s detect some faces. For this, the `detectMultiScale` function is used,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The `detectMultiScale` function that''s shown in the preceding code snippet
    consists of the following seven parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`image`: Represents the input video feed. In our case, it is `grayfeed`, as
    we will detect the face from the grayscale video.'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '`object`: Represents the vectors of a rectangle where each rectangle contains
    the detected faces.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scalefactor`: Specifies how much the image size must be reduced. The ideal
    value of scale factor is between 1.1 and 1.3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`flags`: This parameter can be set to `CASCADE_SCALE_IMAGE`, `CASCADE_FIND_BIGGEST_OBJECT`,
    `CASCADE_DO_ROUGH_SEARCH`, or `CASCADE_DO_CANNY_PRUNING`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CASCADE_SCALE_IMAGE`: This is the most popular flag; it informs the classifier
    that the Haar features for detecting the face are applied to the video or image'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CASCADE_FIND_BIGGEST_OBJECT`: This flag will tell the classifier to find the
    biggest face in the image or video'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CASCADE_DO_ROUGH_SEARCH`: This flag will stop the classifier once a face is
    detected'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CASCADE_DO_CANNY_PRUNNING`: This flag informs the classifier to not detect
    sharp edges, thus increasing the chances of face detection'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min neighbors`: The minimum neighbors parameter affects the quality of the
    detected faces. Higher **min neighbor values** will recognize fewer faces, but
    whatever it detects will definitely be a face. Lower `min neighbors` values may
    recognize multiple faces, but sometimes it may also recognize objects that are
    not faces. The ideal `min neighbors` values for detecting faces is between 3 and
    5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min size`: The minimum size parameter will detect the minimum face size. For
    example, if we set the min size to 50 x 50 pixels, the classifier will only detect
    faces that are bigger than 50 x 50 pixels and ignore faces that are lower than
    50 x 50 pixels. Ideally, we can set the min size to 30 x 30 pixels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max size`: The maximum size parameter will detect the maximum face size. For
    example, if we set the max size to 80 x 80 pixels, the classifier will only detect
    faces that are smaller than 80 x 80 pixels. So, if you move too close to the camera
    and your face size exceeds the max size, your face will not be detected by the
    classifier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since the `detectMultiScale` function provides a vector of rectangles as its
    output, we have to declare a vector as the `Rect` type. The variable name as `face`. `scalefactor`
    is set to `1.1`, `min neighbors` is set to `5`, and the minimum scale size is
    set 30 x 30 pixels. The max size is ignored here because if your face size becomes
    bigger than the max size, your face will not be detected. To complete this step,
    use the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'After detecting faces, we will create a rectangle around the detected faces
    and display text on the top-left side of the rectangle that states "`Face detected`":'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Inside the `for` loop, we are determining how many faces are detected using
    the `face.size()` function. If one is detected, `face.size()` equals `1`, and
    the `for` loop will be satisfied. Inside the `for` loop, we have the rectangle
    and `putText` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The rectangle function will create a rectangle around the detected face. It
    consists of four parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: The first parameter represents the image or video feed on which we want to draw
    the rectangle, which in our case is `videofeed`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second parameter of `face[f]` represents the detected face on which we have
    to draw the rectangle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third parameter represents the color of the rectangle (for this example,
    we have set the color to blue)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fourth and final parameter represents the thickness of the rectangle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `putText` function is used to display text in an image or video feed. It
    consists of seven parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: The first parameter represents the image or video feed on which we want to draw
    the rectangle.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second parameter represents the text message that we want to display.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third parameter represents the point on which we want the text to be displayed.
    The `face[f].x` and `face[f].y` functions represent the top-left point of the
    rectangle, so the text will be displayed on the top-left side of the rectangle.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fourth parameter represents the font type, which we have set to `FONT_HERSHEY_PLAIN`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fifth parameter represents the font size of the text, which we have set
    to `1`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sixth parameter represents the color of the text, which is set to green
    (`Scalar(0,255,0)`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The seventh and final parameter represents the thickness of the font, which
    is set to `1.0`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, using the `imshow` function, we will view the video feed, along with
    the rectangle and text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'After using the preceding code, and if you have compiled and built the program,
    you will see that a rectangle has been drawn around the detected face:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aefc64d5-2d06-4ce2-babe-78d97c9a2df2.png)'
  prefs: []
  type: TYPE_IMG
- en: Next, we will detect human eyes as well as recognize a smile. Once the eyes
    and smile have been recognized, we will create circles around them.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting the eyes and smile
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The program for detecting the eyes and smile is called `SmilingFace.cpp`, and
    you can download it from the `Chapter08` folder of this book's GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting the eyes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `SmilingFace.cpp` program is basically an extension of the `FaceDetection.cpp`
    program, meaning that we will first find the region of interest, which is the
    face. Next, using the Haar `CascadeClassifier` for the eyes, we will detect the
    eyes and then draw circles around them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before writing the program, let''s first understand the different eye `CascadeClassifier` that
    are available. OpenCV 4.0 has three main eye cascade classifiers:'
  prefs: []
  type: TYPE_NORMAL
- en: '`haarcascade_eye.xml`: This classifier will detect both of the eyes simultaneously'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`haarcascade_lefteye_2splits.xml`: This classifier will detect only the left
    eye'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`haarcascade_righteye_2splits.xml`: This classifier will detect only the right
    eye'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Depending on your requirements, you can use the `haarcascade_eye` classifier
    to detect both of the eyes, or you can use the `haarcascade_lefteye_2splits` classifier
    to detect only the left eye and the `haarcascade_righteye_2splits` classifier
    to detect only the right eye. In the `SmilingFace.cpp` program, we will first
    test the output with the `haarcascade_eye`classifier and then we will test the
    output with the `haarcascade_lefteye_2splits`and `haarcascade_righteye_2splits`
    classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: Eye detection using haarcascade_eye
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To test the `haarcascade_eye`output, observe the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load this classifier inside our program:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'To detect the eyes, we need to find the face region (region of interest) in
    the image (video feed). Inside the face-detection `for` loop, we will create a
    `Mat` variable called `faceroi`. `videofeed(face[f])`, which will find faces in
    `videofeed` and store them inside the `faceroi` variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a vector of the `Rect` type, called `eyes`, and then use the `detectMultiScale`
    function to detect the eye region:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In the `detectMultiScale` function, the first parameter is set to `faceroi`,
    which means that we want to detect the eyes from the face region only and not
    from the entire video feed. The detected eyes will be stored in the eyes variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create circles around the eyes, we will use a `for` loop. Let''s find the
    center of the eyes. To find the center of an eye, we will use the `Point` datatype,
    and the equation inside the `eyecenter` variable will give us the center of the
    eye:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The results of this can be seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7387dccd-ec92-4337-974e-18bc25e312b4.png)'
  prefs: []
  type: TYPE_IMG
- en: Using the `radius` variable, we have calculated the radius of the circle and
    then used the `circle` function to create red circles around the eyes.
  prefs: []
  type: TYPE_NORMAL
- en: Eye detection using haarcascade_lefteye_2splits and haarcascade_righteye_2splits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After detecting both eyes using the `haarcascade_eye` classifier, let's try
    to detect only the left eye or the right eye by using the `haarcascade_lefteye_2splits`and `haarcascade_righteye_2splits` classifiers,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting the left eye
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To detect the left eye, perform these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the `haarcascade_lefteye_2splits` cascade classifier inside our program:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we want to detect the left eye in the face region, we will create a `Mat`
    variable called `faceroi` and inside it we will store the face region value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `detectMultiScale` function to create a vector of the `Rect` type called
    `lefteye` to detect the left eye region. The `min neighbors` parameter is set
    to `25` so that the classifier detects only the left eye. If we set `min neighbors`
    lower than 25, the `haarcascade_lefteye_2splits`classifier may also detect the
    right eye, which is not what we want. To complete this step, use the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e234758b-61ca-4ddd-98c5-48e213cb1c1b.png)'
  prefs: []
  type: TYPE_IMG
- en: The `for` loop code for detecting the left and right eye separately is a part
    of the `SmilingFace.cpp` program, but it is commented. To test the code, first
    comment the `for` loop for detecting both the eyes simultaneously and then uncomment
    the other two `for` loops for detecting the left and right eyes.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting the right eye
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The programming logic for detecting the right eye is very similar to detecting
    the left eye. The only thing that we have to change is the classifier name and
    some variable names to distinguish the left and right eyes. To detect the right
    eye, perform these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the `haarcascade_righteye_2splits` cascade classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Inside the face-detection for loop, find the face region. Then, use the `detectMultiScale`
    function to detect the right eye. Use the `circle` function to create a green
    circle around the right eye. To do so, use the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/89315812-5bdc-45ea-937a-11f6569bd558.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we combine the left- and the right-eye detector code, the final output will
    be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8edd5a4b-37d9-46c4-aad6-cead16da27a1.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, the left eye in the picture is surrounded by a red circle and
    the right eye is surrounded by a green circle.
  prefs: []
  type: TYPE_NORMAL
- en: Recognizing a smile
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After detecting the eyes from the face region, let''s write the program to
    recognize a smiling face. The webcam will recognize a smiling face when it detects
    a black-white-black line feature around the mouth, that is, the upper and lower
    lip are generally a bit darker compared to the teeth region:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c969cfe1-1672-4b17-ab75-83ac508ddb17.png)'
  prefs: []
  type: TYPE_IMG
- en: Programming logic for smile recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The programming logic for smile recognition is similar to eye detection, and
    we will also write the smile-recognition program inside the face-detection `for`
    loop. To program smile recognition, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the smile `CascadeClassifier`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to detect the mouth region, which is inside the face region. The face
    region is again our region of interest, and to find the face region from the video
    feed, use will use the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Declare a `smile` variable, which is a vector of the `Rect` type. Then, use
    the `detectMultiScale` function. In the `detectMultiScale` function, `min neighbors`
    is set to `25` so that a circle is created only when a person smiles (if we set
    min neighbor to lower than 25, a circle may be created around the mouth, even
    if the person is not smiling). You can vary the `min neighbors` value between
    25-35\. Next, inside the `for` loop, we have written the program to create a green
    circle around the mouth. To complete this step, use the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/785e0ca6-77a7-4e49-b61f-e4ad84056b2e.png)'
  prefs: []
  type: TYPE_IMG
- en: In the next section, we will turn on different LEDs on the robot when the eyes
    and smile are detected. We will also make our robot follow the detected face when
    the face moves.
  prefs: []
  type: TYPE_NORMAL
- en: Face-tracking robot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The program for turning LEDs on/off and tracking a human face is called `Facetrackingrobot.cpp`,
    and you can download it from the `Chapter08` folder of this book's GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: In the `Facetrackingrobot` program, we will first detect the face, and then
    the left eye, right eye, and smile. Once the eyes and smile are detected, we will
    turn the LEDs on/off. After this, we will create a small dot in the center of
    the face rectangle, and then use this dot as a reference to move our robot.
  prefs: []
  type: TYPE_NORMAL
- en: Wiring connections
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the `Facetrackingrobot`program, we will need a minimum of three LEDs: one
    for the left eye, one for the right eye, and one LED for smile recognition. The
    three LEDs are shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/09d4f5e2-8e71-4b59-a306-e0c8e99fa8a1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The wiring connections of the LEDs and the robot are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The left LED, which corresponds to the **left eye**, is connected to **wiringPi
    pin 0**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The right LED, which corresponds to the **right eye**, is connected to **wiringPi
    pin 2**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The middle LED, which corresponds to a **smile**, is connected to **wiringPi
    pin 3**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **IN1** pin of the motor driver is connected to **wiringPi pin 24**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **IN2** pin of the motor driver is connected to **wiringPi pin 27**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **IN3** pin of the motor driver is connected to **wiringPi pin 25**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **IN4** pin of the motor driver is connected to **wiringPi pin 28**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On my robot, I have taped the left and right LEDs on the top chassis of the
    robot. The third LED (the middle LED) is taped to the bottom chassis of the robot.
    I have used green LEDs for the eyes and a red LED for the smile:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0f5efdb7-3959-47e4-a7e8-cd50dbf74e34.png)'
  prefs: []
  type: TYPE_IMG
- en: The programming logic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the `Facetrackingrobot` program, wiringPi pins 0, 2, and 3 are set as output
    pins:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'From the face-detection program, you may have noticed that the face-tracking
    process is very slow. Therefore, when you move your face to the left or right,
    you have to make sure that the motors don''t move too fast. To reduce the speed
    of the motor, we will use the `softPwm.h` library, which we also used in [Chapter
    2](0db97f63-8947-4436-9265-3680d34bece6.xhtml), *Implementing Blink with wiringPi*:'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the `softPwm.h` library, use the `softPwmCreate` function todeclare the
    four motor pins (`24`, `27`, `25`, and `28`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The first parameter inside the `softPwmCreate` function denotes the wiringPi
    pin of the RPi. The second parameter represents the minimum speed at which we
    can move the motor, and the third parameter represents the maximum speed at which
    we can move the motor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the face, left eye, right eye, and smile `CascadeClassifiers`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Inside the `for` loop, declare three Boolean variables, called `lefteyedetect`,
    `righteyedetect`, and `isSmiling`. Set all three variables to `false`. Using these
    three variables, we will detect whether the left eye, right eye, and smile are
    detected. Declare the `facex` and `facey` variables, which will be used to find
    the center of the face rectangle. To complete this step, use the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `detectMultiScale` function to detect the face and then, inside the
    `for` loop, we will write the program to create a rectangle around the detected
    face:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '`face[f].x + face[f].width/2` will return the *x* center value of the rectangle
    and `face[f].y + face[f].height/2` will return the *y* center value of the rectangle.
    The *x* center value is stored in the `facex` variable and the *y* center value
    is stored in the `facey` variable.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To find the center of the rectangle, provide `facex` and `facey` as input to
    the `Point` variable, called `facecenter`. Inside the circle function, use the
    `facecenter` point variable as an input to create a dot in the center of the face
    rectangle:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/47290ac4-3a89-450a-8cce-5b2590f4bd7c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When the left eye is detected, we will create a red circle around it and set
    the `lefteyedetect` variable to `true`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'When the right eye is detected, we will create a light blue circle around it
    and set the `righteyedetect` variable to `true`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'When the smile is detected, we will create a green circle around the mouth
    and set `isSmiling` to `true`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following screenshot, you can see a red circle drawn around the left
    eye, a light blue circle drawn around the right eye, a green circle drawn around
    the mouth, and a white dot in the center of the blue rectangle that surrounds
    the face:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7eeb20e3-f8af-452c-86b4-c69022a3d216.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Using three `if` conditions, we will check when the `lefteyedetect`, `righteyedetect`,
    and `isSmiling` variables are `true`, and when they are `true`, we will turn on
    their respective LEDs:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `lefteyedetect` variable will be `true` when the left eye is detected.
    When the left eye is detected, we will turn on the left LED on the robot, which
    is connected to wiringPi pin 0, as shown in the following code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The `righteyedetect` variable will be `true` when the right eye is detected.
    When the right eye is detected, we will turn on the right LED on the robot, which
    is connected to wiringPi pin 2:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the `isSmiling`variable will be true when the smile is recognized.
    When the smile is recognized, we will turn on the middle LED, which is connected
    to wiringPi pin 3:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will use the white dot (point) on the face rectangle to move the robot
    left and right.
  prefs: []
  type: TYPE_NORMAL
- en: Using the white dot on the face triangle to move the robot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Similar to [Chapter 7](b5b2dffa-7833-47cc-98d9-e60e6aba7299.xhtml), *Building
    an Object-Following Robot with OpenCV*, we will divide the camera screen into
    three sections: the left section, the middle section, and the right section. When
    the white dot is in the left or right section, we will turn the robot left or
    right, thus tracking the face. Even though I have not resized `videofeed`, the
    resolution of `videofeed` is set to 640 x 480 (a width of 640 and a height of
    480).'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can vary the range as per your requirements, but as shown in the following
    diagram, the left section is set to an x range from 0 to 280, the middle section
    is set to a range of 280-360, and the right section is set to a range of 360 to
    640:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4fd8cbfd-0afb-421f-841d-bc4db4dbab54.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When we move our face, the face rectangle will move, and when the face rectangle
    moves, the white dot in the center of the rectangle will also move. When the dot
    moves, there will be changes in the `facex` and `facey` values. When dividing
    the camera screen into three sections, we will use the `facex` variable as a reference,
    and then we will use three if conditions to check which section the white dot
    is in. The code for comparing the `facex` values is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: If the first `if` condition is satisfied, this means that the white dot is between
    0 to 280\. In this case, we are printing the `Left` text on `videofeed` and then
    using the `softPwmWrite` function so that the robot will take an axial left turn.
    Inside the `softPwmWrite` function, the first parameter represents the pin number
    and the second parameter represents the speed at which our motor will move. Since
    wiringPi pin 24 is set to 0 (low), and wiringPi pin 27 is set to 30, the left
    motor will move backwards with a speed of 30\. Similarly, since wiringPi pin 25
    is set to 30, and wiringPi pin 28 is set to 0 (low), the right motor will move
    forward with a speed of 30.
  prefs: []
  type: TYPE_NORMAL
- en: The speed value of 30 is in the range of 0 to 100, which we set in the `softPwmCreate`
    function. You can also vary the speed value.
  prefs: []
  type: TYPE_NORMAL
- en: If the white dot is between 360 and 640, the `Right` text will be printed and
    the robot will take an axial right turn at a speed of 30.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, when the white dot is between 280 and 360, the `Middle` text will be
    printed and the robot will stop moving.
  prefs: []
  type: TYPE_NORMAL
- en: This is how we can make the robot track a face and follow it.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we used the Haar face classifier to detect a face from a video
    feed and then we drew a rectangle around it. Next, we detected the eyes and smile
    from a given face, and drew circles around the eyes and mouth. After this, using
    our knowledge of face, eye, and smile detection, we turned the LEDs of our robot
    on and off when the eyes and smile were detected. Finally, by creating a white
    dot in the center of the face rectangle, we made the robot follow our face.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to control our robot using our voice.
    We will also create an Android application that will recognize what we are speaking
    about. When the Android application detects certain keywords, the Android smartphone's
    Bluetooth will send bits of data to the Raspberry Pi Bluetooth. Once our robot
    recognizes these keywords, we will use them to move the robot in different directions.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is the name of the classifier that we use to detect faces?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When we open the mouth, which type of feature is created?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which cascade can be used to detect only the left eye?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When detecting the eyes from the face, what is that region generally referred
    to as?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the use of the `equalizeHist` function?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
