- en: Organizing Distributed Solutions with Docker Compose
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Shipping software is an integral part of the Docker platform. The official repositories
    on Docker Hub make it easy to design a distributed solution using tried-and-tested
    components. In the previous chapter, I showed you how to integrate these components
    into your own solution, taking a container-first design approach. The end result
    is a distributed solution with several moving parts. In this chapter, you'll learn
    how to organize all those moving parts into one unit, using Docker Compose.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Compose is another open source product from Docker, Inc., that extends
    the Docker ecosystem. The Docker **command-line interface** (**CLI**) and Docker
    API work on individual resources, like images and containers. Docker Compose works
    at a higher level, with services and applications. An *application* is a single
    unit composed of one or more services which are deployed as containers at runtime.
    You use Docker Compose to define all the resources of the application-services,
    networks, volumes, and other Docker objects—and the dependencies between them.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two parts to Docker Compose. The design-time element captures the
    application definition in a markup file using a YAML specification, and at runtime
    Docker Compose can manage an application from the YAML file. We''ll cover both
    in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining applications with Docker Compose
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing applications with Docker Compose
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring application environments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker Compose is installed as part of Docker Desktop on Windows. If you install
    Docker on Windows Server using the PowerShell installer, that doesn't give you
    Docker Compose. You can download it from the releases on GitHub at `docker/compose`.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will need Docker running on Windows 10 with update 18.09, or Windows Server
    2019 to follow along with the examples. The code for this chapter is available
    at [https://github.com/sixeyed/docker-on-windows/tree/second-edition/ch06](https://github.com/sixeyed/docker-on-windows/tree/second-edition/ch06).
  prefs: []
  type: TYPE_NORMAL
- en: Defining applications with Docker Compose
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Docker Compose file format is very simple. YAML is a human-readable markup
    language, and the Compose file specification captures your application configuration,
    using the same option names that the Docker CLI uses. In the Compose file, you
    define the services, networks, and volumes that make up your application. Networks
    and volumes are the same concepts that you use with the Docker engine. Services
    are an abstraction over containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'A *container* is a single instance of a component, which could be anything
    from a web app to a message handler. A service can be multiple instances of the
    same component running in different containers, all using the same Docker image
    and the same runtime options. You could have three containers in the service used
    for your web application and two containers in the service you use for a message
    handler:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/350dcfa9-dead-4008-84d7-0c4d7e999a8e.png)'
  prefs: []
  type: TYPE_IMG
- en: A *service* is like a template to run a container from an image, with a known
    configuration. Using services, you can scale up the components of the application—running
    multiple containers from the same image and configuring and managing them as a
    single unit. Services are not used in the standalone Docker engine, but they are
    used in Docker Compose, and also with a cluster of Docker engines running in Docker
    Swarm mode (which I cover in the next chapter, [Chapter 7](bf6a5e90-bbba-435b-b0a0-734611e0e834.xhtml),
    *Orchestrating Distributed Solutions with Docker Swarm*.
  prefs: []
  type: TYPE_NORMAL
- en: Docker provides discoverability for services in the same way that it does for
    containers. Consumers access the service by name, and Docker can load-balance
    requests across multiple containers in a service. The number of instances in the
    service is transparent to consumers; they always refer to the service name, and
    the traffic is always directed to a single container by Docker.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, I'll use Docker Compose to organize the distributed solution
    I built in the previous chapter, replacing the brittle `docker container run`
    PowerShell scripts with a reliable and production-ready Docker Compose file.
  prefs: []
  type: TYPE_NORMAL
- en: Capturing service definitions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Services can be defined in any order in the Compose file. To make it easier
    to read, I prefer to start with the simplest services, which have no dependencies—**infrastructure
    components**, such as the message queue, reverse proxy, and databases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Docker Compose files are conventionally called `docker-compose.yml`, and they
    start with an explicit statement of the API version; the latest is version 3.7\.
    Application resources are defined at the top level—this is a template Compose
    file with sections for services, networks, and volumes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The Docker Compose specification is documented at the Docker website at [https://docs.docker.com/compose/compose-file/](https://docs.docker.com/compose/compose-file/).
    This lists the full specification for all supported versions, and the changes
    between the versions.
  prefs: []
  type: TYPE_NORMAL
- en: All resources need a unique name, and the name is how resources refer to other
    resources. Services may have a dependency on networks, volumes, and other services,
    which are all captured by name. The configuration for each resource is in its
    own section, and the attributes available are broadly the same as the respective
    `create` command in the Docker CLI, such as `docker network create` and `docker
    volume create`.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, I'll build a Compose file for the distributed NerdDinner application
    and show you how to use Docker Compose to manage the application. I'll start my
    Compose file with the common services first.
  prefs: []
  type: TYPE_NORMAL
- en: Defining infrastructure services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The simplest service I have is the message queue **NATS**, which has no dependencies.
    Each service needs a name and the image name to start containers from. Optionally,
    you can include parameters that you would use in `docker container run`. For the
    NATS message queue, I add a network name, which means any containers created for
    this service will all be attached to the `nd-net` network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In this service definition, I have all the parameters required to start message
    queue containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '`message-queue` is the name of the service. This becomes the DNS entry for
    other services to access NATS.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image` is the full name of the image to start containers from. In this case,
    it''s my Windows Server 2019 variation of the official NATS image from Docker
    Hub, but you can also use an image from a private registry by including the registry
    domain in the image name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`networks` is a list of the networks to connect containers to when they start.
    This service connects to one network named `nd-net`. This will be a Docker network
    used for all the services in this application. Later in the Docker Compose file,
    I''ll explicitly capture the details of the network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I haven't published any ports for the NATS service. The message queue is only
    used internally by other containers. Within a Docker network, containers can access
    ports on other containers without them being published to the host. This keeps
    the message queue secure, as it is only accessible through the Docker platform
    by other containers in the same network. No external server and no applications
    running on the server can access the message queue.
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The next infrastructure service is **Elasticsearch**, which also has no dependencies
    on other services. It will be used by the message handler, which also uses the
    NATS message queue, so I will need to join all these services to the same Docker
    network. For Elasticsearch, I also want to limit the amount of memory it uses
    and use a volume for the data so it will be stored outside the container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Here, `elasticsearch` is the name of the service and `sixeyed/elasticsearch`
    is the name of the image, which is my public image on Docker Hub. I'm connecting
    the service to the same `nd-net` network, and I also mount a volume to a known
    location in the container. When Elasticsearch writes data to `C:\data` on the
    container, it will actually be stored in a volume.
  prefs: []
  type: TYPE_NORMAL
- en: Just as with networks, volumes are first-class resources in the Docker Compose
    file. For Elasticsearch, I'm mapping a volume called `es-data` to the data location
    in the container. I'll specify how the `es-data` volume should be created later
    in the Compose file.
  prefs: []
  type: TYPE_NORMAL
- en: Traefik
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next is the reverse proxy, Traefik. The proxy builds its routing rules from
    labels when containers are created, so it needs to connect to the Docker API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The Traefik container publishes to port `80` on the host, connects to the application
    network, and uses a volume for the Docker API-named pipe. These are the same options
    that I used when I started Traefik using `docker container run`; typically, you
    can just copy your run commands into your Docker Compose file.
  prefs: []
  type: TYPE_NORMAL
- en: Port publishing is the same in Docker Compose as it is when running a container.
    You specify which container port to publish and which host port it should publish
    to, so Docker routes incoming host traffic to the container. The `ports` section
    allows multiple mappings, and you can optionally specify TCP or UDP protocols
    if you have a specific requirement.
  prefs: []
  type: TYPE_NORMAL
- en: I'm also publishing port `8080` and using the `--api` flag in the Traefik configuration.
    This gives me access to Traefik's dashboard, where I can see all the routing rules
    Traefik has configured. This is useful in non-production environments, to check
    your proxy rules are correct, but this is not something you want exposed publicly
    in production.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Compose also supports extended definitions, which I'm using for the `volume`
    specification. Rather than using a single line to define the volume mount, I've
    split out the type of the volume, the source, and the target into different lines.
    This is optional, but it makes the file easier to read.
  prefs: []
  type: TYPE_NORMAL
- en: Kibana
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Kibana** is the first service that depends on other services—it needs Elasticsearch
    running so that it can connect to the database. Docker Compose doesn''t give any
    guarantees about the order in which it creates containers, so if you have a start-up
    dependency between services, you need to capture that in the service definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `depends_on` attribute shows how to capture dependencies between services.
    In this case, Kibana is dependent on Elasticsearch, so Docker will ensure the
    `elasticsearch` service is up and running before starting the `kibana` service.
  prefs: []
  type: TYPE_NORMAL
- en: Capturing dependencies like this is fine for running distributed applications
    on a single machine, but it doesn't scale. When you're running in a cluster you
    want the orchestrator to manage distributing the workload. It can't do that effectively
    if you have explicit dependencies, because it needs to make sure all the containers
    running the dependent service are healthy before it starts the consuming containers.
    There are better ways of managing dependencies that we'll see when we look at
    Docker Swarm.
  prefs: []
  type: TYPE_NORMAL
- en: Kibana will be proxied by Traefik, but Traefik does not need to be running before
    Kibana. When Traefik starts, it gets a list of running containers from the Docker
    API to build its initial routing map. Then it subscribes to the event stream from
    Docker to update the routing rules when containers are created or removed. So,
    Traefik can start before or after the web containers.
  prefs: []
  type: TYPE_NORMAL
- en: Containers for the `kibana` service also connect to the application network.
    In an alternative configuration, I could have separate backend and frontend networks.
    All the infrastructure services would connect to the backend network, and the
    public-facing services would connect to the backend and frontend networks. These
    are both Docker networks, but separating them would give me the flexibility to
    configure the networks differently.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring application services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The infrastructure services I've specified so far haven't needed much application-level
    configuration. I've configured the integration points between the containers and
    the Docker platform with networks, volumes, and ports, but the applications use
    the configuration built into each Docker image.
  prefs: []
  type: TYPE_NORMAL
- en: The Kibana image connects to Elasticsearch by convention using the hostname
    `elasticsearch`, which is the service name I've used in the Docker Compose file
    to support that convention. The Docker platform will route any requests to the
    `elasticsearch` hostname to the service, load-balancing between containers if
    there are multiple containers running the service, so Kibana will be able to find
    Elasticsearch at the expected domain name.
  prefs: []
  type: TYPE_NORMAL
- en: My custom applications need configuration settings specified, which I can include
    in the Compose file using environment variables. Defining environment variables
    for a service in the Compose file sets these environment variables for every container
    running the service.
  prefs: []
  type: TYPE_NORMAL
- en: 'The index-dinner message handler service subscribes to the NATS message queue
    and inserts documents in Elasticsearch, so it needs to connect to the same Docker
    network, and it also depends on these services. I can capture these dependencies
    in the Compose file and specify the configuration for the application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Here, I'm using the `environment` section to specify two environment variables—each
    with a key-value pair—to configure the URLs for the message queue and Elasticsearch.
    These are actually the default values baked into the message handler image, so
    I don't need to include them in the Compose file, but it can be useful to explicitly
    set them.
  prefs: []
  type: TYPE_NORMAL
- en: You can think of the Compose file as the complete deployment guide for the distributed
    solution. If you explicitly specify the environment values, it makes it clear
    what configuration options are available, at the cost of making your Compose file
    less manageable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Storing configuration variables in plain text is fine for simple application
    settings, but using a separate environment file is better for sensitive values,
    which is the approach I used in the previous chapter. This is also supported in
    the Compose file format. For the database service, I can use an environment file
    for the administrator password, specified with the `env-file` attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'When the database service starts, Docker will set up the environment variables
    from the file called `db-credentials.env`. I''ve used a relative path, so that
    file needs to be in the same location as the Compose file. As earlier, the contents
    of that file are key-value pairs with one line per environment variable. In this
    file, I''ve included the connection strings for the application, as well as the
    password for the database, so the credentials are all in one place:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The sensitive data is still in plain text, but by isolating it in a separate
    file, I can do two things:'
  prefs: []
  type: TYPE_NORMAL
- en: First, I can secure the file to restrict access.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Second, I can take advantage of the separation of the service configuration
    from the application definition and use the same Docker Compose file for different
    environments, substituting different environment files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Environment variables are not secure even if you secure access to the file.
    You can view environment variable values when you inspect a container, so anyone
    with access to the Docker API can read this data. For sensitive data such as passwords
    and API keys, you should use Docker secrets with Docker Swarm, which I cover in
    the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the save-dinner message handler, I can make use of the same environment
    file for database credentials. The handler depends on the message queue and database
    services, but there are no new attributes in this definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Next are my frontend services that are proxied by Traefik—the REST API, the
    new home page, and the legacy NerdDinner web application. The REST API uses the
    same credentials file to configure the SQL Server connection, and includes the
    Traefik-routing rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The home page includes the Traefik-routing rule, and also a high-priority value,
    to ensure this rule gets evaluated before the more general rule the NerdDinner
    web app uses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The last service is the website itself. Here, I''m using a combination of environment
    variables and environment files. Variable values that are typically consistent
    across environments can be explicitly stated to make the configuration clear—I''m
    doing that for the feature flags. Sensitive data can be read from separate files,
    in this case containing the database credentials and the API keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The website containers don't need to be publicly available, so there's no port
    publishing. The application needs access to the other services, so it's connected
    to the same network.
  prefs: []
  type: TYPE_NORMAL
- en: All the services are configured now, so I just need to specify the network and
    volume resources to complete the Compose file.
  prefs: []
  type: TYPE_NORMAL
- en: Specifying application resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker Compose separates network and volume definitions from service definitions,
    which allows flexibility between environments. I'll cover this flexibility later
    in the chapter, but to finish the NerdDinner Compose file, I'll start with the
    simplest approach, using default values.
  prefs: []
  type: TYPE_NORMAL
- en: The services in my Compose file all use a network called `nd-net`, which needs
    to be specified in the Compose file. Docker networks are a good way to segregate
    applications. You could have several solutions that all use Elasticsearch but
    that have different SLAs and storage requirements. If you have a separate network
    for each application, you can run separate Elasticsearch services in different
    Docker networks, individually configured for each application, but all named `elasticsearch`.
    This keeps to the expected conventions but segregates by the network, so services
    only see the Elasticsearch instance in their own network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Docker Compose can create networks at runtime, or you can define the resource
    to use an external network that already exists on the host. This specification
    for the NerdDinner network uses the default `nat` network that Docker creates
    when it is installed, so this setup will work for all standard Docker hosts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Volumes also need to be specified. Both of my stateful services, Elasticsearch
    and SQL Server, use named volumes for data storage:  `es-data` and `nd-data`,
    respectively. As with other networks, volumes can be specified as external, so
    Docker Compose will use existing volumes. Docker doesn''t create any default volumes,
    so if I use an external volume, I would need to create it on each host before
    running the application. Instead, I''ll specify the volumes without any options,
    so Docker Compose will create them for me:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: These volumes will store the data on the host, rather than in the container's
    writeable layer. They're not host-mounted volumes, so although the data is stored
    on the local disk, I'm not specifying the location. Each volume will write its
    data in the Docker data directory at `C:\ProgramData\Docker`. I'll look at managing
    these volumes later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: My Compose file has services, networks and volumes all specified, so it's ready
    to run. The full file is in the source code for this chapter at `ch06\ch06-docker-compose`.
  prefs: []
  type: TYPE_NORMAL
- en: Managing applications with Docker Compose
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker Compose presents a similar interface to the Docker CLI. The `docker-compose`
    command uses some of the same command names and arguments for the functionality
    it supports—which is a subset of the functionality of the full Docker CLI. When
    you run commands through the Compose CLI, it sends requests to the Docker engine
    to act on the resources in the Compose file.
  prefs: []
  type: TYPE_NORMAL
- en: The Docker Compose file is the desired state of your application. When you run
    `docker-compose` commands, it compares the Compose file to the objects that already
    exist in Docker and makes any changes needed to get to the desired state. That
    could be stopping containers, starting containers, or creating volumes.
  prefs: []
  type: TYPE_NORMAL
- en: Compose treats all the resources in a Compose file as a single application,
    and to disambiguate applications running on the same host, the runtime adds a
    project name to all the resources it creates for the application. When you run
    an application through Compose and then look at the containers running on your
    host, you won't see a container with a name that exactly matches the service name.
    Compose adds the project name and an index to container names to support multiple
    containers in the service, but this doesn't affect Docker's DNS system, so containers
    still access one another by the service name.
  prefs: []
  type: TYPE_NORMAL
- en: Running applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I have the first Compose file for NerdDinner in the `ch06-docker-compose` directory,
    which also contains the environment variable files. From that directory, I can
    start the whole application with a single `docker-compose` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see the description of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: The `up` command is used to start the application, creating networks, volumes,
    and running containers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `-d` option runs all the containers in the background, and is the same as
    the `--detach` option in `docker container run`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can see that Docker Compose honors the `depends_on` settings for services.
    Any services that are dependencies for others are created first. Services that
    don't have any dependencies will be created in a random order. In this case, the
    `message-queue` service was created first, as many other services depend on it,
    and the `nerd-dinner-web` and `nerd-dinner-save-handler` services are the last
    of all, as they have the most dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: The names in the output are the individual container names, with the naming
    format `{project}_{service}_{index}`. Each service has only one container running,
    which is the default, so the indexes are all `1`. The project name is a sanitized
    version of the directory name where I ran the `compose` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you run a `docker-compose up` command and it completes, you can manage
    the containers with Docker Compose or with the standard Docker CLI. The containers
    are just normal Docker containers, with some extra metadata used by compose to
    manage them as a whole unit. Listing containers shows me all the service containers
    created by `compose`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The container running Traefik publishes port `80` to the local machine, and
    I have entries in my hosts file for the local NerdDinner domains. The NerdDinner
    application with its new home page, the REST API, and the Kibana analytics will
    behave as expected, because the full configuration is captured in the Compose
    file, and all the components are started by Docker Compose.
  prefs: []
  type: TYPE_NORMAL
- en: This is one of the most powerful features of the Compose file format. The file
    contains the complete specification to run your application, and anyone can use
    it to run your app. In this case, all the components use public Docker images
    on Docker Hub, so anyone can start the app from this Compose file. You don't need
    any prerequisites other than Docker and Docker Compose to run NerdDinner, which
    is now a distributed application containing .NET Framework, .NET Core, Java, Go,
    and Node.js components.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling application services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker Compose lets you scale services up and down easily, adding or removing
    containers to a running service. When a service is running with multiple containers,
    it's still accessible to other services in the network. Consumers use the service
    name for discovery, and the DNS server in Docker load-balances requests across
    all the containers in the service.
  prefs: []
  type: TYPE_NORMAL
- en: Adding more containers doesn't automatically give scale and resilience to your
    service, though; that depends on the application running the service. You won't
    get a SQL Server failover cluster just by adding another container to a SQL database
    service, because SQL Server needs to be explicitly configured for failover. If
    you add another container, you'll have just two distinct database instances with
    separate data stores.
  prefs: []
  type: TYPE_NORMAL
- en: Web applications typically scale well if they are designed to support scale-out.
    Stateless applications can run in any number of containers because any container
    can handle any request. But if your application maintains the session state locally,
    requests from the same user need to be handled by the same service, which prevents
    you from load-balancing across many containers, unless you use sticky sessions.
  prefs: []
  type: TYPE_NORMAL
- en: Services that publish ports to the host can't be scaled if they're running on
    a single Docker engine. Ports can have one only operating system process listening
    on them, and that's also true for Docker—you can't have the same host port mapped
    to multiple container ports. On a Docker Swarm where you have multiple hosts,
    you can scale services with published ports, and Docker will run each containers
    on different hosts.
  prefs: []
  type: TYPE_NORMAL
- en: In NerdDinner, the message handlers are truly stateless components. They receive
    a message from the queue that contains all the information they need, and they
    process it. NATS supports grouping of subscribers on the same message queue, which
    means I can have several containers running the save-dinner handler, and NATS
    will ensure only one handler gets a copy of each message, so I don't have duplicate
    message processing. The code in the message handlers already takes advantage of
    that.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scaling up the message handler is something I can do at peak time to increase
    the throughput for message processing. I can do that with the `up` command and
    the `--scale` option, specifying the service name and the desired number of instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Docker Compose compares the state of the running application with the configuration
    in the Compose file and the overrides specified in the command. In this case,
    all the services are unchanged except for the save-dinner handler, so they are
    listed as `up-to-date`. The save-handler has a new service level, so Docker Compose
    creates two more containers.
  prefs: []
  type: TYPE_NORMAL
- en: With three instances of the save-message handler running, they share the incoming
    message load in a round-robin approach. That's a great way to increase scale.
    The handlers concurrently process messages and write to the SQL database, which
    increases the throughput for saves and reduces the time taken for messages to
    be handled. But there is still a strict limit to the number of processes writing
    to SQL Server, so the database won't become a bottleneck for this feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'I can create multiple dinners through the web application, and the message
    handlers will share the load when the event messages are published. I can see
    in the logs that different handlers process different messages, and there is no
    duplicate processing of events:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'I''m running on a single Docker engine, so I can''t scale the Traefik service,
    because only one container can be published to port `80`. But I can scale the
    frontend containers that Traefik is proxying, which is a great way to test that
    my application works correctly when it''s scaled out to multiple instances. I''ll
    add another two instances of the original NerdDinner web application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Look closely at this output—something happened here that is correct, but is
    not what I intended. Compose has created two new NerdDinner web containers, to
    meet the scale of 3 that I specified, but it's also stopped and removed two of
    the save-handler containers.
  prefs: []
  type: TYPE_NORMAL
- en: This is because Compose is implicitly using my `docker-compose.yml` file as
    the application definition, which uses a single instance of each service. Then
    it adds the scale value from the command for the web service, and builds a desired
    state that says every service should have one container running, except the web
    service, which should have three. It sees the web service only has one container,
    so it creates two more. And it sees the save-handler has three containers, so
    it removes two.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mixing the Compose file definition with changes from the command is not recommended,
    precisely because of this situation. The Compose file alone should be the desired
    state of your application. But in this case, you can''t specify a scale option
    in the Compose file (you could in older versions, but not from v3 of the specification),
    so you need to explicitly add scale levels for all services:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now I have three save-handler containers, which are sharing work from the message
    queue, and three web containers. Traefik will load-balance requests between these
    three web containers. I can check that configuration from the Traefik dashboard,
    which I have published on port `8080`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/dc786f47-93ea-4ef3-a5dc-2bb67d24bacd.png)'
  prefs: []
  type: TYPE_IMG
- en: Traefik shows the frontend routing rules in blue boxes on the left, and the
    backend services they map to as green boxes on the right. There is a frontend
    routing rule for `nerddinner.local` with a path prefix of `/`, which sends all
    traffic to the `nerd-dinner-web` backend (except the home page, which has a different
    rule). The backend is shown with three servers listed, which are the three containers
    I scaled with Docker Compose. The `172.20.*.*` server addresses are the internal
    IP address on the Docker network that containers can use to communicate.
  prefs: []
  type: TYPE_NORMAL
- en: 'I can browse the NerdDinner app, and it works correctly, with Traefik load-balancing
    requests across the backend containers. As soon as I try to log in, though, I''ll
    find that NerdDinner wasn''t designed to scale out to multiple instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/3819e3df-ac67-4493-9326-587e96033019.png)'
  prefs: []
  type: TYPE_IMG
- en: 'That error message tells me that NerdDinner expects all the requests from one
    user to be handled by the same instance of the web app. Traefik supports sticky
    sessions for exactly this situation, so to fix this, I just need to add a new
    label to the web service definition in my Compose file. This enables sticky sessions
    for the NerdDinner backend:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now I can deploy again, making sure to include my scale arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Compose recreates the web service containers, removing the old containers and
    starting new ones with the new configuration. Now, Traefik is using sticky sessions,
    so every request from my browser session will go to the same container. Traefik
    powers this with a custom cookie that specifies the container IP address the request
    should route to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/d8aba133-97bc-4aa2-bdea-94bf34bd68d6.png)'
  prefs: []
  type: TYPE_IMG
- en: In this case, the cookie is called `_d18b8` and it fixes all my requests to
    be routed to the container with the IP address `172.20.26.74`.
  prefs: []
  type: TYPE_NORMAL
- en: Finding issues when you run at scale used to only happen in test environments,
    or even in production. Running everything in Docker means I can test the functionality
    of my app at scale on my dev laptop, and find these problems before release. Using
    modern technology such as Traefik also means there are nice ways to fix these
    problems, without having to change my legacy application.
  prefs: []
  type: TYPE_NORMAL
- en: Stopping and starting application services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several commands for managing the container life cycle in Docker Compose.
    It's important to understand the differences between the options, so you don't
    remove resources unexpectedly.
  prefs: []
  type: TYPE_NORMAL
- en: The `up` and `down` commands are blunt tools to start and stop the whole application.
    The `up` command creates any resources specified in the Compose file that don't
    exist, and it creates and starts containers for all the services. The `down` command
    does the reverse—it stops any running containers and removes the application resources.
    Containers and networks are removed if they were created by Docker Compose, but
    volumes are not removed—so any application data you have is retained.
  prefs: []
  type: TYPE_NORMAL
- en: The `stop` command just stops all the running containers without removing them
    or other resources. Stopping the container ends the running process with a graceful
    shutdown. The `kill` command stops all the containers by forcibly ending the running
    process. Stopped application containers can be started again with `start`, which
    runs the entry point program in the existing container.
  prefs: []
  type: TYPE_NORMAL
- en: 'Stopped containers retain all their configuration and data, but they don''t
    use any compute resources. Starting and stopping containers is a very efficient
    way to switch context if you work on multiple projects. If I''m developing on
    NerdDinner when another piece of work comes in as a priority, I can stop the whole
    NerdDinner application to free up my development environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Now I have no containers running, and I can switch to the other project. When
    that work is done, I can fire up NerdDinner again by running `docker-compose start`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also stop individual services by specifying a name, which is very useful
    if you want to test how your application manages failures. I can check how the
    index-dinner handlers behave if they can''t access Elasticsearch by stopping the
    Elasticsearch service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: All of these commands are processed by comparing the Compose file to the services
    running in Docker. You need to have access to the Docker Compose file to run any
    Docker Compose commands. This is one of the biggest drawbacks of using Docker
    Compose on a single host to run your applications. The alternative is to use the
    same Compose file but to deploy it as a stack to a Docker Swarm, which I'll cover
    in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The `stop` and `start` commands use the Compose file, but they work on the containers
    that currently exist, not just the definition in the Compose file. So, if you
    scale a service, then stop the whole application and then start it again—you'll
    still have all the containers you scaled to. Only the `up` command uses the Compose
    file to reset the application to the desired state.
  prefs: []
  type: TYPE_NORMAL
- en: Upgrading application services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you run `docker compose up` repeatedly from the same Compose file, no changes
    will be made after the first run. Docker Compose compares the configuration in
    the Compose file with the active containers at runtime and won't change resources
    unless the definition has changed. This means you can use Docker Compose to manage
    application upgrades.
  prefs: []
  type: TYPE_NORMAL
- en: My Compose file is currently using the database service from the image I built
    in [Chapter 3](ee527f27-ee07-40e1-a39d-86aa2d11da72.xhtml), *Developing Dockerized
    .NET Framework and .NET Core Applications*, tagged `dockeronwindows/ch03-nerd-dinner-db:2e`.
    For this chapter, I've added audit fields to the tables in the database schema
    and built a new version of the database image, tagged `dockeronwindows/ch06-nerd-dinner-db:2e`.
  prefs: []
  type: TYPE_NORMAL
- en: 'I have a second Compose file in the same `ch06-docker-compose` directory, called
    `docker-compose-db-upgrade.yml`. The upgrade file is not a full application definition;
    all it contains is a single part of the database service definition, using the
    new image tag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Docker Compose supports override files. You can run `docker-compose` commands
    and pass multiple Compose files as arguments. Compose will join all the files
    together in the order specified in the command, from left to right. Override files
    can be used to add new sections to the application definition, or they can replace
    existing values.
  prefs: []
  type: TYPE_NORMAL
- en: 'While the application is running, I can execute `docker compose up` again,
    specifying both the original Compose file, and the `db-upgrade` override file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This command uses the `db-upgrade` file as an override to the main `docker-compose.yml`
    file. Docker Compose merges them both, so the final service definition contains
    all the values from the original file, except the image specification that comes
    from the override. The new service definition doesn't match what's running in
    Docker, so Compose recreates the database service.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Compose recreates services by removing the old container and starting
    a new one, using the new image specification. Services that don't depend on the
    database are left as they are, with the log entry `up-to-date`, and any services
    that do depend on the database are also recreated once the new database container
    is running.
  prefs: []
  type: TYPE_NORMAL
- en: 'My database container uses the pattern I described in [Chapter 3](ee527f27-ee07-40e1-a39d-86aa2d11da72.xhtml), *Developing
    Dockerized .NET Framework and .NET Core Applications,* with a volume to store
    the data and a script that can upgrade the database schema when a container is
    replaced. In the Compose file, I use a default definition for the volume called
    `db-data`, so Docker Compose creates it for me. Just like the containers created
    by Compose, volumes are a standard Docker resource and can be managed with the
    Docker CLI. The `docker volume ls` lists all the volumes on the host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'I have two volumes for my NerdDinner deployment. They both use the local driver,
    which means the data is stored on the local disk. I can inspect the SQL Server
    volume to see where the data is physically stored on the host (in the `Mountpoint`
    attribute) and then check the contents to see the database files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The volume is stored outside of the container, so when Docker Compose removes
    the old container database, all the data is preserved. The new database image
    bundles a Dacpac and is configured to do schema upgrades for the existing data
    file in the same way as the SQL Server database from [Chapter 3](ee527f27-ee07-40e1-a39d-86aa2d11da72.xhtml),
    *Developing Dockerized .NET Framework and .NET Core Applications*.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the new container has started, I can check the logs and see that the new
    container attached the database files from the volume and then altered the Dinners
    table to add the new audit column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The new audit column adds a timestamp when rows are updated, so now when I
    create a dinner through the web UI, I can see when the row was last updated in
    the database. In my development environment, I haven''t published the SQL Server
    port for client connections, but I can run `docker container inspect` to get the
    container''s local IP address. Then I can connect my SQL client directly to the
    container and run a query to see the new audit timestamp:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/eb36d243-f04d-4ed7-a10c-cad9ea1d5188.png)'
  prefs: []
  type: TYPE_IMG
- en: Docker Compose looks for any differences between resources and their definitions,
    and not just the name of the Docker image. If you change the environment variables,
    port mappings, volume setup, or any other configuration, Docker Compose will remove
    or create resources to bring the running application to the desired state.
  prefs: []
  type: TYPE_NORMAL
- en: You need to be careful with modifying Compose files to run applications. If
    you remove the definition for a running service from the file, Docker Compose
    won't recognize that the existing service containers are part of the application,
    so they won't be included in the difference checks. You can end up with orphaned
    service containers.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring application containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Treating a distributed application as a single unit makes it easier to monitor
    and trace problems. Docker Compose provides its own `top` and `logs` commands,
    which operate over all the containers in the application services and display
    the collected results.
  prefs: []
  type: TYPE_NORMAL
- en: 'To check the memory and CPU usage of all the components, run `docker-compose
    top`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Containers are listed in alphabetical order by name, and processes in each container
    are listed without a specific order. There's no way to change the ordering, so
    you can't show the most intensive processes in the hardest-working container first,
    but the result is in plain text, so you can manipulate it in PowerShell.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see the log entries from all the containers, run `docker-compose logs`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: On the screen, the container names are color-coded, so you can easily distinguish
    entries from different components. One advantage of reading logs through Docker
    Compose is that it shows output for all the containers, even if the component
    has shown errors and the container is stopped. These error messages are useful
    to see in context—you may see that one component throws a connection error before
    another component logs that it has started, which may highlight a missing dependency
    in the Compose file.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Compose shows all the log entries for all the service containers, so
    the output can be extensive. You can limit this with the `--tail` option, restricting
    the output to a specified number of the most recent log entries for each container.
  prefs: []
  type: TYPE_NORMAL
- en: These are useful commands when you are running in development or in a low-scale
    project with a single server running a small number of containers. It doesn't
    scale for large projects running on multiple containers across multiple hosts
    with Docker Swarm. For those, you need container-centric administration and monitoring,
    which I'll demonstrate in [Chapter 8](98e12163-b4ad-4b5d-aecc-827f5e204caa.xhtml),
    *Administering and Monitoring Dockerized Solutions*.
  prefs: []
  type: TYPE_NORMAL
- en: Managing application images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker Compose can manage Docker images, as well as containers. In the Compose
    file, you can include attributes that tell Docker Compose how to build your images.
    You can specify the location of the build context to send to the Docker service,
    which is the root folder for all your application content—and the location of
    the Dockerfile.
  prefs: []
  type: TYPE_NORMAL
- en: The context path is relative to the location of the Compose file, and the Dockerfile
    path is relative to the context. This is very useful for complex source trees,
    such as the demo source for this book, where the context for each image is in
    a different folder. In the `ch06-docker-compose-build` folder, I have a full Compose
    file with application specification, complete with the build attributes specified.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is how the build details are specified for my images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: When you run `docker-compose build`, any services that have the `build` attribute
    specified will be built and tagged with the name in the `image` attribute. The
    build process uses the normal Docker API, so the image layer cache is still used,
    and only changed layers are rebuilt. Adding build details to your Compose file
    is a very efficient way of building all your application images, and it's also
    a central place to capture how the images are built.
  prefs: []
  type: TYPE_NORMAL
- en: 'One other useful feature of Docker Compose is the ability to manage whole groups
    of images. The Compose file for this chapter uses images that are all publicly
    available on Docker Hub, so you can run the full application with `docker-compose
    up`—but the first time you run it, all the images will be downloaded, which is
    going to take a while. You can preload images before you use them with `docker-compose
    pull`, which will pull all the images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Similarly, you can use `docker-compose push` to upload images to remote repositories.
    For both commands, Docker Compose uses the authenticated user from the most recent
    `docker login` command. If your Compose file contains images you don't have access
    to push, those pushes will fail. For any repositories you are authorized to write
    to, whether in Docker Hub or a private registry, these images will be pushed.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring application environments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you define your application in Docker Compose, you have a single artifact
    that describes all the components of the application and the integration points
    between them. This is often referred to as the **application manifest**—a document
    that lists all the parts of your app. In the same way that the Dockerfile explicitly
    defines the steps to install and configure one piece of software, the Docker Compose
    file explicitly defines the steps to deploy the whole solution.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Compose also lets you capture application definitions that can be deployed
    to different environments, so your Compose files are usable throughout the deployment
    pipeline. Usually, there are differences between environments, either in the infrastructure
    setup or the application settings. Docker Compose gives you two options to manage
    these environmental differences—using external resources, or using override files.
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure typically differs between production and non-production environments,
    which affects volumes and networks in Docker applications. On a development laptop,
    your database volume may be mapped to a known location on the local disk, which
    you periodically clean up. In production, you could have a volume plugin for a
    shared storage hardware device. Similarly, with networks, production environments
    may need to be explicit about subnet ranges, which are not a concern in development.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Compose lets you specify resources as being external to the Compose file,
    so the application will use resources that already exist. These resources need
    to be created in advance, but that means each environment can be configured differently
    and still use the same Compose file.
  prefs: []
  type: TYPE_NORMAL
- en: Compose also supports an alternative approach, where you explicitly capture
    the configuration of your resources for each environment in different Compose
    files and use multiple Compose files when you run the application. I'll demonstrate
    both of these options. As with other design decisions, Docker doesn't impose specific
    practices, and you can use whichever best suits your processes.
  prefs: []
  type: TYPE_NORMAL
- en: Specifying external resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Volume and network definitions in the Compose file follow the same pattern as
    service definitions—each resource is named and can be configured using the same
    options available in the relevant `docker ... create` command. There's an extra
    option in Compose files to point to an existing resource.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use existing volumes for my SQL Server and Elasticsearch data, I need to
    specify the `external` attribute and optionally, a name for the resource. In the
    `ch06-docker-compose-external` directory, my Docker Compose file has these volume
    definitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'With external resources declared, I can''t just run the application using `docker-compose
    up`. Compose won''t create volumes defined as external; they need to exist before
    the application starts. And these volumes are required by services, so Docker
    Compose won''t create any containers either. Instead, you''ll see an error message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The error message tells you the command you need to run to create the missing
    resource. This will create basic volumes with default configurations, and that
    will allow Docker Compose to start the application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Docker lets you create volumes with different configuration options, so you
    can specify an explicit mount point—such as a RAID array or an NFS share. Windows
    doesn't support options for the local driver currently, but you can use mapped
    drives as a workaround. There are drivers for other types of storage—using volume
    plugins for cloud services, such as Azure storage, and enterprise storage units,
    such as HPE 3PAR.
  prefs: []
  type: TYPE_NORMAL
- en: 'The same approach can be used to specify networks as external resources. In
    my Compose file, I initially used the default `nat` network, but in this Compose
    file, I specify a custom external network for the application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Docker on Windows has several networking options. The default, and the easiest,
    is network address translation, with the `nat` network. This driver isolates containers
    from the physical network, and each container gets its own IP address in a subnet
    managed by Docker. On the host, you can access containers by their IP address,
    but outside the host, you can only access containers through published ports.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can create other networks with the `nat` driver, or you can also use other
    drivers for different network configurations:'
  prefs: []
  type: TYPE_NORMAL
- en: The `transparent` driver, which gives each container an IP address provided
    by the physical router
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `l2bridge` driver, which is for specifying static container IP addresses
    on the physical network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `overlay` driver, which is for running distributed applications in Docker
    Swarm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For my setup with Traefik on a single server, `nat` is the best option, so
    I''ll create a custom network for my application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: When the containers start, I can access Traefik using the `nerddinner.local`
    domains I have set up in my `hosts` file.
  prefs: []
  type: TYPE_NORMAL
- en: Using external resources lets you have a single Docker Compose file, which is
    used for every environment, with the actual implementation of the network and
    volume resources different between environments. Developers can use basic storage
    and networking options, and in production, the ops team can deploy a more complex
    infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Using Docker Compose overrides
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Resources aren't the only things that change between environments, though. You
    will also have different configuration settings, different ports being published,
    different setups for your container health checks, and more. It can be tempting
    to have completely different Docker Compose files for each environment, but that's
    something you should try hard to avoid.
  prefs: []
  type: TYPE_NORMAL
- en: Having multiple Compose files means additional overhead to keep them in sync—and,
    more importantly, there's a risk of environments drifting if they aren't kept
    in sync. Using Docker Compose overrides addresses this and means your requirements
    for each environment are explicitly stated.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Compose looks for files called `docker-compose.yml` and `docker-compose.override.yml`
    by default, and if it finds both, it will use the override file to add to or replace
    parts of the definitions in the main Docker Compose file. When you run the Docker
    Compose CLI, you can pass additional files to be combined for the whole application
    specification. This lets you keep the core solution definition in one file and
    have explicit environment-dependent overrides in other files.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `ch06-docker-compose-override` folder, I''ve taken this approach. The
    core `docker-compose.yml` file has the service definitions that describe the structure
    of the solution and the environment configuration to run in development. There
    are three override files in the same folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '`docker-compose.test.yml`  adds configuration settings for the test environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`docker-compose.production.ym` adds configuration settings for the live environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`docker-compose.build.yml` adds configuration settings for building the images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The standard `docker-compose.yml` file can be used on its own, and it will just
    work. This is important to make sure that your deployment process doesn't make
    life difficult for developers. Specifying the development settings in the main
    file means developers can just run `docker-compose up -d`, as they don't need
    to know anything about the overrides to get started.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the reverse proxy configuration in  `docker-compose.yml`, and it''s
    set up to publish random ports and to start the Traefik dashboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'This is useful for developers who may be using port `80` for other apps, and
    who want to dig into the dashboard to see Traefik''s routing rules. The `test`
    override file changes the port definition to use `80` and `8080` on the host server,
    but the dashboard is still exposed, so the command section is unchanged:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The `production` override changes the startup command, removing the `--api`
    flag in the command, so the dashboard doesn''t run, and it only publishes port
    `80`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The rest of the service configuration, the image to use, the volume mount for
    the Docker Engine-named pipe and the network to connect to are the same in every
    environment, so the override files don't need to specify them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another example is the new home page, which contains the domain name for the
    URL in the Traefik labels for the service. That''s environment-specific, and in
    the development Docker Compose file, it''s set to use `nerddinner.local`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `test` override file, the domain is `nerd-dinner.test`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'In production, it''s `nerd-dinner.com`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The rest of the configuration is the same in every environment, so the override
    files only specify the new labels.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Compose doesn't merge the contents of a list when it adds the override;
    the new list replaces the old list completely. That's why the `traefik.frontend.priority`
    label is there in every file, so you can't just have the frontend rule value in
    the labels in the override file, because the priority value wouldn't be merged
    in from the labels in the main file.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are other differences in the test environment that are captured in the
    override file:'
  prefs: []
  type: TYPE_NORMAL
- en: The SQL Server and Elasticsearch ports are published, to help troubleshooting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The volumes for the databases are mounted from paths on the `E:` drive, which
    is a RAID array on the server.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Traefik rules all use the `nerd-dinner.test` domain.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The application network is specified as external, to allow admins to create
    their own network configuration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These are different again in the production override file:'
  prefs: []
  type: TYPE_NORMAL
- en: The SQL Server and Elasticsearch ports are not published, to keep them as private
    components.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The volumes for the databases are specified as external, so admins can configure
    their own storage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Traefik rules all use the `nerd-dinner.com` domain.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The application network is specified as external, to allow admins to create
    their own network configuration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deployment to any environment is as simple as running `docker-compose up`,
    specifying the override file to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: This approach is a great way to keep your Docker Compose files simple, and to
    capture all the variable environment settings in separate files. You can even
    combine several Docker Compose files. If you have multiple test environments that
    share a lot of commonality, you can define the application setup in the base Compose
    file, shared test configuration in one override file, and each specific test environment
    in an additional override file.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter I covered Docker Compose, the tool used to organize distributed
    Docker solutions. With Compose, you explicitly define all the components of your
    solution, the configuration of the components, and the relationship between them
    in a simple, clean format.
  prefs: []
  type: TYPE_NORMAL
- en: The Compose file lets you manage all the application containers as a single
    unit. You learned in this chapter how you can use the `docker-compose` command
    line to spin up and tear down the application, creating all the resources and
    starting or stopping containers. You also learned that you can use Docker Compose
    to scale components up or down and to release upgrades to your solution.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Compose is a powerful tool for defining complex solutions. The Compose
    file effectively replaces lengthy deployment documents and fully describes every
    part of the application. With external resources and Compose overrides, you can
    even capture the differences between environments and build a set of YAML files
    that you can use to drive your whole deployment pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: The limitation of Docker Compose is that it's a client-side tool. The `docker-compose`
    command needs access to the Compose file to execute any commands. There is a logical
    grouping of resources into a single application, but that happens only in the
    Compose file. The Docker engine only sees a set of resources; it does not recognize
    them as being part of the same application. Docker Compose is also limited to
    single-node Docker deployments.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, I'll move on to clustered Docker deployments, with multiple
    nodes running in a Docker Swarm. In a production environment, this gives you high
    availability and scale. Docker Swarm is a powerful orchestrator for container
    solutions, and it is very easy to use. It also supports the Compose file format,
    so you can use your existing Compose files to deploy applications, but Docker
    stores the logical architecture within the swarm, allowing you to manage your
    application without needing the Compose file.
  prefs: []
  type: TYPE_NORMAL
