- en: Service State and Interservice Communication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have developed some microservices, seen API Gateway, and understood
    service registry and discovery, it's time to dive deeper into microservices and
    understand the system from an individual microservice point of view. To gain the
    most benefits out of microservices architecture, every component in the system
    has to collaborate in just the right way—a way that ensures that there is most
    likely no coupling between microservices, which will enable us to be Agile.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will understand various communication styles between microservices
    and see how services exchange data among themselves. Then we will move on to the
    service bus, an enterprise way of how system components communicate with each
    other. Many services would need to persist some state in one form or another.
    We will see how to make our service stateless. We will see the current database
    landscape and understand service state. We will understand pub-sub pattern and
    look at tools such as Kafka, and RabbitMQ, to understand event-driven architecture.
    This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Core concepts—state, communication, and dependencies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Communication styles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synchronous versus asynchronous way of data sharing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microservice versioning and failure handling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service bus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data sharing between microservices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cache via Redis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Publish-subscribe pattern
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Core concepts – state, communication, and dependencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Each microservice implements a single capability such as shipping, and deducting
    from the inventory. However, to deliver an end user, a service request such as
    business capability, user need, or user-specific requests; it may or may not be
    a set of business capabilities. For example, a person who wants to buy the product
    is a single service request from the user's point of view. However, multiple requests
    are involved here, such as add to cart microservice, payment microservice, shipping
    microservice.  Hence, to deliver, microservices need to collaborate among each
    other. In this section, we will look at core concepts of microservice collaboration
    such as service state, communication styles, and more. Choosing the correct communication
    style helps us to design a loosely coupled architecture that ensures that each
    microservice has clear boundaries and it stays inside its bounded context. In
    this section, we will look at some core concepts that will affect our microservice
    design and architecture. So, let's get started.
  prefs: []
  type: TYPE_NORMAL
- en: Microservice state
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While we should indeed thrive on making services as stateless as possible, there
    are some instances where we do need stateful services. A state is simply any condition
    or quality at any specific point in time. A stateful service is one that is dependent
    on the state of the system. Being stateful means to rely on these moments in time,
    whereas statelessness means to be independent of any sort of state.
  prefs: []
  type: TYPE_NORMAL
- en: Stateful service is essential in instances where we have a workflow that calls
    some REST services, we need to support retries on failures, we need to track progress,
    store intermediate results, and so on. We need to keep state somewhere outside
    of our service instances boundaries. This is where databases come into the picture.
  prefs: []
  type: TYPE_NORMAL
- en: Databases are an important and interesting part to think on. Introducing database
    in a microservice should be done in such a way that no other team can directly
    talk to our database. They, in fact, shouldn't even know the type of our database.
    The current database landscape has various options available to us, both in the
    SQL and NoSQL category. There are even graph databases, in-memory databases, and
    databases with either high read and write capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Our microservices can have both stateless and stateful microservices. If a service
    relies on the state it should be separated out in a dedicated container that is
    easily accessible and not shared with anyone. Microservices have the ability to
    scale better when stateless. We scale the container rather than scaling the VM.
    Hence each state store should be in a container that can be scaled at any point
    in time. We use Docker to scale the database store, which creates a separate persistence
    layer that is host-independent. The new cloud data stores such as Redis, Cassandra,
    and DynamoDB maximize availability with a minimum delay in consistency. Designing
    a stateful microservice with asynchronous and scalable nature needs some thinking
    on the problem—to find some means of communication state between any sequential
    messages and to ensure that messages don't mix up with any context where they
    don't belong. We will see various synchronous patterns such as CQRS and Saga for
    achieving this in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Maintaining state is not something that can be done on the service level only.
    Actually, there are three places wherein state can be maintained in the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '**HTTP**: This is actually the application layer from where the state is maintained
    mostly session-based or persisting in the database. Generally speaking by maintaining
    a communication layer between a client and an application or a service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TCP**: This is actually the transportation layer. The objective of maintaining
    state here is to ensure a reliable delivery channel between the client and an
    application or a service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SSL**: This is the layer without any home between the TCP and HTTP layer.
    It provides confidentiality and privacy in data. State here is maintained, as
    encryption and decryption rely solely on information that is unique to the connection
    between client and application or a service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So even if our services are stateless, the TCP and SSL layer do need to maintain
    state. So you are never pure stateless. Anyway, we will just stick to the application
    layer for the scope of the book.
  prefs: []
  type: TYPE_NORMAL
- en: Interservice communication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Being finely grained and closely bound to a scope, microservices need to collaborate
    in some way or another to deliver functionality to end users. They either need
    to share state or dependencies or talk to other services. Let''s take a look at
    a practical example. Consider the frequent buyers rewards program microservice.
    This microservice is responsible for rewards for frequent buyers business capability.
    The program is simple—whenever customers purchase something, some points are accumulated
    in their account. Now, when a customer buys something, he can use those rewards
    points for a discount on the selling price. The rewards microservice depends on
    the customer purchase microservice and other business capabilities. Other business
    capabilities depend on the rewards program. As shown in the following diagram,
    microservices need to collaborate with other microservices:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/35077dd7-21de-47d6-a900-73df46b1d16f.png)'
  prefs: []
  type: TYPE_IMG
- en: Need of microservices
  prefs: []
  type: TYPE_NORMAL
- en: As seen in the preceding diagram, microservices are finely divided into business
    capabilities. However, end user functionality needs several business capabilities
    for which microservices must need to collaborate with each other to deliver use
    cases to end users. When any microservices collaborate, the collaboration style
    follows in majorly three categories—**commands**, **queries**, and **events**.
    Let's understand all the three categories with some examples.
  prefs: []
  type: TYPE_NORMAL
- en: Commands
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Commands** are used whenever any microservice wants another microservice
    to perform an action. They are synchronous in nature and are generally implemented
    using HTTP POST or PUT requests. For example, in the preceding figure, the rewards
    program microservice sends a command to user profile microservice or invoice microservice
    regarding promotional offers based on rewards. When sending a command fails, the
    sender won''t know if the receiver processed the command or not. This can result
    in errors or some degraded functionalities if a set of rules is not followed by
    the sender as well as receiver side.'
  prefs: []
  type: TYPE_NORMAL
- en: Queries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to commands, queries are used when one microservice needs some information
    from another microservice. For example, during invoice in our shopping cart microservice,
    we need information on the total number of reward points so as to give a promotional
    discount, so the invoice microservice queries the rewards points microservice.
    This is a synchronous mode of communication and is generally implemented using
    HTTP GET requests. Whenever a query fails, the caller does not get the data it
    needs. If the caller handles exceptions well enough then there is a minimal impact
    with some degraded functionality. If it does not handle errors well enough, the
    error propagates throughout the system.
  prefs: []
  type: TYPE_NORMAL
- en: Events
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While deviating from the standard approaches, the third approach is more of
    a reactive approach. Events are generally preferred whenever a microservice needs
    to react to something that has occurred in another microservice. The custom logging
    microservice listens to all other services for log entries so that it can push
    logs to Elasticsearch. Similarly, the rewards microservices listens to shopping
    tracker microservices in order to update user rewards accordingly based on user
    shopping. When a subscriber polls any event feed and if the call fails the impact
    is very limited. The subscriber can still poll the event feed later until the
    event feed is up and start receiving events at any time. Some of the events will
    be delayed, but this should not be a problem as everything is done asynchronously.
  prefs: []
  type: TYPE_NORMAL
- en: Exchanging data formats
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The essence or fundamental of interservice communication is the exchange of
    messages in any formats. Messages usually contain data and so a very important
    design aspect is the format of data. This can greatly impact the efficiency of
    communication, the usability and changes, and evolving the service in time. It
    is very necessary to select cross-message format. There are two types of message
    formats—**text** and **binary**. In this section, we will look at both.
  prefs: []
  type: TYPE_NORMAL
- en: Text-based message formats
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Commonly used message formats such as JSON and XML are human-readable and self-describing.
    These formats enable a user to pick out the values that the consumer is interested
    in and discard the rest. Any changes to the schema format can easily be backward-compatible.
    The downsides to using text-based formats include being too verbose in nature
    and overhead of parsing the entire text. For stronger efficiency, binary formats
    are recommended to be used.
  prefs: []
  type: TYPE_NORMAL
- en: Binary message formats
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These formats provide a typed identity language for defining a structure for
    the messages. A compiler then generates the code that serializes and deserializes
    the messages for us (we will be seeing Apache Thrift later in the chapter). If
    the client has a statically typed language then the compiler checks if the API
    is used correctly or not. Avro, Thrift, and Google's protobuf are prominent binary
    message formats.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a clear idea about communication essentials, we can move on
    to the next section on dependencies. Let's summarize the points before moving
    on.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can opt for using commands and queries if the following use cases are met:'
  prefs: []
  type: TYPE_NORMAL
- en: In order to process the service request, the service client needs a response
    to move further along its process. For example, for the payment microservice,
    we need customer information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The situation demands asynchronous operation. For example, inventory should
    only be deducted if payment has been made and product processed for customer delivery.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Request to other services is a simple query or command, that is, something that
    can be processed via HTTP `GET`, `PUT`, `POST`, and `DELETE` methods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can opt for using events if the following use cases are met:'
  prefs: []
  type: TYPE_NORMAL
- en: When you need to scale the application as pure commands and queries do not scale
    over a larger problem set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Producer or sender does not care how much extra processing is done on the receiver
    or consumer's end and it has no such effect on the producers end.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When multiple clients read a single message. For example, an order has been
    invoiced, then multiple processes need to be done such as getting it ready to
    be dispatched, updating inventory, sending client notifications, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dependencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we are aware of communication styles in microservices, we will learn
    about the next obvious thing in development—dependencies and avoiding dependency
    hell. With developing more and more microservices, you will spot code duplication
    across multiple microservices. To resolve these, we need to understand dependencies
    and how to separate supporting code. Node.js has the package manager NPM that
    grabs application dependencies (as well as dependencies of your dependencies).
    NPM has support for private repositories, directly downloaded from GitHub, setting
    up your own repository (such as JFrog, Artifactory), which not only helps to avoid
    code but also in deployment processes.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, we must not forget **Microservices 101**. We make microservices to
    ensure that each service can release and deploy independently, for which we must
    avoid dependency hell. To understand dependency hell, let''s consider the following
    example, shopping cart microservice has API listing products that have now upgraded
    to API listing products with specific brands. Now all dependencies on the shopping
    cart microservice may send a message to listing products with specific brands,
    which are originally meant for listing products. If backward-compatibility is
    not handled, then this evolves into dependency hell. To avoid dependency hell,
    strategies that can be used are—APIs must be forward and backward-compatible,
    they must have accurate documentation, contract testing must be done (we will
    see this in [Chapter 8](a7273aa2-2981-4013-8d5f-dbee87462d35.xhtml), *Testing,
    Debugging, and Documenting*, under *PACT*), and using an appropriate tooling library
    that has a clear objective to throw out errors if there is an unknown field encountered.
    To make sure that we want to avoid dependency hell we must simply follow these
    rules:'
  prefs: []
  type: TYPE_NORMAL
- en: A microservice cannot call another microservice nor access its data source directly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A microservice can only call another microservice only through either event-based
    mechanism or some microservice script (a script can be anything such as API Gateway,
    UI, service registry, and so on)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will look at microservice communication styles and see
    how they collaborate with each other. We will look at the widely used patterns
    based on different classification factors and understand the scenarios for using
    which pattern at what moment.
  prefs: []
  type: TYPE_NORMAL
- en: Communication styles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A microservice ecosystem is essentially a distributed system that is running
    on multiple machines. Each service instance is just another process. We saw in
    the earlier diagram different process communications. In this section, we will
    learn about communication styles in much more detail.
  prefs: []
  type: TYPE_NORMAL
- en: A service consumer and service responder may communicate through many different
    types of communication styles, each one targeting some scenario and outcome in
    mind. The communication types can be categorized into two different aspects.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first aspect deals with the type of protocol, whether it is synchronous
    or asynchronous:'
  prefs: []
  type: TYPE_NORMAL
- en: Communication invoked via commands and queries such as HTTP are synchronous
    in nature. The client sends a request to wait for a response from the service.
    This waiting is language-dependent, that is, it can be synchronous (languages
    such as Java) and it can be asynchronous (response can be processed via callbacks,
    promises, and so on, in our case, Node.js). The important point is that a service
    request can only be served once the client receives a proper HTTP server response.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other protocols such as AMQP, sockets, and so on are asynchronous in nature
    (the log and shopping tracker microservice). The client code or message sender
    does not wait for a response, it simply sends the message to any queue or message
    broker and simply.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The second aspect deals with the number of receivers, whether there is just
    a single receiver or there are multiple receivers:'
  prefs: []
  type: TYPE_NORMAL
- en: For a single receiver, each request has to be processed by only one receiver
    or service. Command and query pattern are examples of this kind of communication.
    One-to-one interaction includes models such as request/response, one-way requests
    such as notifications, and request/async responses.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For multiple receivers, each request can be processed by zero or multiple services
    or receivers. This is an asynchronous mode of communication with an example of
    the publisher-subscriber mechanism promoting event-driven architecture. Data updates
    between multiple microservices are propagated through events that are implemented
    through some service bus (Azure service bus) or any message brokers (AMQP, Kafka,
    RabbitMQ, and so on).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NextGen communication styles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While we saw some common communication styles, the world is constantly changing.
    With evolution everywhere, even the fundamental HTTP protocol has evolved and
    we now have the HTTP 2.X protocol with some added advantages. In this section,
    we are going to look at next-gen communication styles and see the advantages they
    have to offer.
  prefs: []
  type: TYPE_NORMAL
- en: HTTP/2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'HTTP/2 gives significant enhancements and focuses more on improving the usage
    of TCP connections. The following are some major enhancements as compared to HTTP/1.1:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Compression and binary framing**: HTTP/2 has header compression inbuilt in
    order to reduce the footprint of HTTP headers that can grow in kilobytes (say,
    for example, cookies). It also controls headers that are repeated across multiple
    requests and responses. Also, the client and the server maintain a list of frequently
    visible fields along with their compressed values, so when these fields are repeated,
    the individual simply includes the reference to the compressed value. Besides
    this, HTTP/2 uses binary encoding for frames.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multiplexing**: As compared to a single request and response flow (client
    has to wait for a response before issuing next request), HTTP/2 introduces fully
    asynchronous multiplexing of a request by implementing streams (behold reactive
    programming!). Clients and servers can both start multiple requests on a single
    TCP connection. For example, when a client requests a web page, the server can
    start a separate stream to transfer images and videos that are needed for that
    web page.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flow Control**: With multiplexing introduced, there is a need for a flow
    control in place to avoid destructive behavior across any of the streams. HTTP/2
    provides building blocks for client and server to have proper flow controls suitable
    for any specific situations. Flow control can allow the browser to get only a
    part of the particular resource, put that operation on hold by reducing window
    down to zero, and resuming at any point in time. Also, priorities can be set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this section, we are going to look at how to implement HTTP/2 in our microservice
    system. You can check `example http2` under `chapter 7`, source code to follow
    the implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'HTTP/2 is supported on Node.js 10.XX, but there are other ways to achieve support
    without upgrading to the latest version, which was introduced just two weeks ago
    (Node.js 10.XX was introduced just two weeks ago at the time of writing). We will
    use node module `spdy`, which provides HTTP/2 support to our `Express` application.
    Copy our `first-microservice` skeleton from [Chapter 2](c1987454-3c62-4e25-abf5-28a9abf833e8.xhtml),
    *Gearing up for the Journey*, and install `spdy` as a node module by using the
    following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'For HTTP/2 to work, the SSL/TLS has to be enabled. For our development environment
    to work properly we will self-generate the CSR and certificates that can easily
    be replaced by procuring certificates in production environments. To generate
    the certificates, follow along with these commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The outcome of all these steps will result in three files: `server.crt`, `server.csr`,
    and `server.key`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to change the way we start our express server. Instead of using
    default methods, we need to use one provided by `spdy`. Make the following changes
    in `Application.ts`. Replace  `this.express.app.listen` with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We are ready to start dishing out HTTP/2 requests. Start the server and open
    up `https://localhost:3000/hello-world`. Open up the Developer console and you
    should be able to see HTTP/2 just like in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/cf4057f7-f62e-4842-ab95-c2ed89c68b71.png)'
  prefs: []
  type: TYPE_IMG
- en: HTTP support
  prefs: []
  type: TYPE_NORMAL
- en: These are HTTP calls. In the next section, we will look at the RPC mechanism,
    which is another way to do collaboration among microservices.
  prefs: []
  type: TYPE_NORMAL
- en: gRPC with Apache Thrift
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**gRPC** is a framework designed for writing cross-language RPC (remote procedure
    calls) clients and servers. It works on binary formats and focuses on an API First
    approach for designing any services. It provides fixed IDL (interactive data language
    fixed formats) to later generate client-side stubs and server-side skeletons adhering
    to that fixed IDL format. The compiler can generate code for most of the languages
    and they exchange data using HTTP/2, which is beneficial in the long run. Apache
    Thrift is a great alternative for writing cross-language RPC clients and servers.
    It has a C-style IDL Definition language. The compiler generates code for a variety
    of languages, which includes C++, Java, and even TypeScript. A Thrift definition
    is very analogous to the TypeScript interface. Thrift methods can give out any
    value or they can be just one-way communication. Methods that have a return type
    implement the request/response model, whereas methods that do not have a return
    type are defined to implement the notification model. Thrift has support for JSON
    as well as binary. Let''s get started with an example. You can follow along with
    the `thrift-rpc` folder in [Chapter 7](162a0f25-2890-4a58-aa41-e9c9b5fc6c2d.xhtml), *Service
    State and Interservice Communication*, in the extracted source code.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The overall process that we are going to do is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Write ourselves a `.thrift` file that will describe our product microservice
    and popularity microservice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate source code for TypeScript, the language in which we are going to write
    our service communication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Import the generated code and start writing our service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Include the generated source of popularity in product and write our service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create API Gateway as a single entry point
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Though Thrift provides Node.js and TypeScript libraries, we are going to use
    `npm` modules of **CreditKarma** ([https://github.com/creditkarma](https://github.com/creditkarma)),
    as original modules lack in generating strict types. So let's get started.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize a Node.js project. Instead of downloading Thrift, I am going to
    use `npm` modules. Hence, install the following modules as dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Create one folder called `thrift` and inside it create two Thrift files—`PopularityService.thrift`
    (`thrift/popularity/PopularityService.thrift`) and `ProductService.thrift` (`thrift/product/ProductService.thrift`).
    A Thrift file is like a TypeScript interface:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Since we need popularity inside the product, we will import that in `ProductService.thrift`,
    and you can check other default syntax here [https://thrift.apache.org/docs/idl](https://thrift.apache.org/docs/idl).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we will generate our code using IDL files defined in the preceding step.
    Open up `package.json` and add the following scripts inside the `scripts` tag:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This script will generate the code for us, as we simply will have to type `npm
    run codegen`.
  prefs: []
  type: TYPE_NORMAL
- en: The next part involves writing `findByProductId` and `findPopularityOfProduct`
    methods. Check out `src/popularity/data.ts` and `src/product/data.ts` for dummy
    data and dummy find methods in the extracted source code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will now write code for starting `PopluarityThriftService` and `ProductThriftService`.
    Create one `serviceHandler` inside `src/popularity/server.ts` as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Start this `server.ts` as `express` by adding `ThriftServerExpress` as middleware:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, inside `src/product/server.ts`, add the following code that will make
    an RPC call to the `PopularityService` to get popularity by `productId`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, `create gateway/server.ts`. Define a route for `/product/: productId`
    and it will make an RPC call to `ProductMicroservice` to fetch the data of `productId`
    passed.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the program and make a request to `localhost:9000/product/1` and you will
    be able to see combined communicated response via RPC calls.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this section, we had hands-on experience with some microservice communication
    styles along with some practicals. In the next section, we are going to see how
    to version microservices and make our microservices have a fail-safe mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: Versioning microservices and failure handling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Evolution is necessary and we can't prevent it. Whenever we allow one of the
    services to evolve, one of the most considerable aspects of maintaining is service
    versioning. In this section, we are going to see various aspects related to handling
    changes in the system and overcoming failures if any are introduced in the system.
  prefs: []
  type: TYPE_NORMAL
- en: Versioning 101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Service versioning should be thought of firstly and not be taken up as an after-development
    exercise. An API is a published contract between a server and consumer. Maintaining
    versions helps us to release new services without breaking anything for existing
    customers (not everyone accepts change in the first attempt). Both the old version
    and the new version should coexist side by side.
  prefs: []
  type: TYPE_NORMAL
- en: 'Prevalent styles of versioning are using the semantic versions. Any semantic
    version will have three major components—**major** (whenever there is a groundbreaking
    change), **minor** (whenever there is a backward-compatible behavior), and **patch**
    (backward-compatible with any bug fix). Versioning is extremely problematic whenever
    there is more than a single service inside a microservice. A recommended way is
    to version any services at the service level rather than doing it at the operations
    level. If there is a single change in any of the operations the service is upgraded
    and deployed to **Version2** (**V2**), which is applicable to all operations in
    the service. There are three ways in which we can version any service:'
  prefs: []
  type: TYPE_NORMAL
- en: '**URI versioning**: The version number of the service is included in the URL
    itself. We just need to worry about major versions of this approach, as that would
    change the URL route. If there is a minor version or any patch available the consumer
    does not need to worry about the change. Keeping an alias for the latest version
    to non-versioned URI is one of the good practices that needs to be followed. For
    example, the URL `/api/v5/product/1234` should be aliased to `/api/product/1234`—aliased
    to `v5`. Moreover, passing version number can also be done as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '**Media type versioning**: The media type versioning follows a slightly different
    approach. Here the version is set by the client on the HTTP Accept header. Its
    structure is something similar to, `Accept: application/vnd.api+json`. The Accept
    header gives us a way to specify generic and less generic content types as well
    as giving fallbacks. A command such as `Accept: application/vnd.api.v5+json` specifically
    asks for `v5` of the API. If the Accept header is omitted, the consumer interacts
    with the latest version, which may not be production-grade. GitHub uses such kinds
    of versioning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Custom header**: The last approach is to maintain our own custom header.
    Consumers would still use an Accept header and add a new one on top of that. It
    would be something like this: `X-my-api-version:1`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When comparing the preceding three approaches, it is simple for clients to consume
    services in a URI approach, but also managing nested URI resources can be complex
    in a URI approach. Migrating clients is complex when it comes to a URI-based approach
    as compared to media type versioning, as we need to maintain caching of multiple
    versions. However, most of the big players, such as Google, Salesforce, and so
    on, go with the URI approach.
  prefs: []
  type: TYPE_NORMAL
- en: When a developer's nightmare comes true
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'All systems will experience failures. Microservices being distributed, the
    probability increases very high. How we handle failures and respond to failures
    is what defines a developer. While making the overall product ecosystem resilient
    is spectacular (activities involve clustering servers, setting up application
    load balancers, distributing infrastructure between multiple locations, and setting
    up disaster recovery), our work does not stop there. This part only addresses
    the complete loss of a system. However, whenever a service is running slow or
    there is memory leak, it is extremely difficult to detect the problem for the
    following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: A service degradation starts slow, but rapidly gains momentum and spreads just
    like an infection. The application container exhausts its thread pool resources
    completely and the system goes down.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Too many synchronous calls where a caller has to wait endlessly for the service
    to return a response.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications don't deal with partial degradations. As long as any service is
    completely down, the application continues to make calls to that service that
    soon exists in resource exhaustion.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The worst thing about such situations is that such failures cascade up and adversely
    affect the system just like a contagion. A single poorly performing system can
    soon take out multiple dependent systems. It becomes necessary to protect a service's
    resources from getting exhausted because of some other poorly performing service.
    In the next section, we will look at some of these patterns to avoid failure cascading
    in the system and causing the ripple effect.
  prefs: []
  type: TYPE_NORMAL
- en: Client resiliency patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Client resiliency patterns allow the client to fail fast and not block database
    connections or thread pools. These patterns are implemented in the client layer,
    which calls any remote resource. There are the following four common client resiliency
    patterns:'
  prefs: []
  type: TYPE_NORMAL
- en: Bulkhead and retry
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Client-side load balancing or queue-based load leveling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Circuit breaker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fallback and compensating transaction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The four patterns can be seen in the diagram as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/236387ce-c92c-4ff1-a1e8-04fadc93b610.png)'
  prefs: []
  type: TYPE_IMG
- en: Client resiliency patterns
  prefs: []
  type: TYPE_NORMAL
- en: Bulkhead and retry pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The bulkhead pattern is similar to the pattern of building a ship, where a ship
    is divided into completely isolated and watertight compartments called bulkheads.
    Even if the ship's hull is punctured, the ship is not affected as it is divided
    into watertight compartments. The bulkheads keep the water confined to the specific
    region of the ship where the puncture occurred and prevent the ship from sinking.
  prefs: []
  type: TYPE_NORMAL
- en: A similar concept is applied in the bulkhead pattern for a service interacting
    with many remote resources. By using this pattern we break the calls to remote
    resources into their own bulkheads (their own thread pools) and reduce the risk
    and prevent the application from going down because of a slow remote resource.
    If one service is slow, then the thread pool for that type of service will become
    saturated to stop processing further requests. Service calls to another service
    won't be hampered as each one has its own thread pool. The retry pattern helps
    an application to handle any anticipated, temporary failure whenever it tries
    to connect to a service or any network resource by transparently retrying an operation
    that has previously failed due to some criteria. Instead of waiting, it rather
    does a fixed number of retries.
  prefs: []
  type: TYPE_NORMAL
- en: Client-side load balancing or queue-based load leveling pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We saw client-side load balancing in [Chapter 6](0c5e001e-6dca-4805-866c-7be793a91c70.xhtml), *Service
    Registry and Discovery*. It involves having the client look up all of a service's
    individual instances from any service discovery agent (Eureka/Consul) and then
    caching the location of available service instances. Whenever any further request
    comes, the client-side load balancer will return a location from the pool of service
    locations it is maintained at the client-side. Locations are periodically refreshed
    based on some interval. If the client-side load balancer detects a problem in
    any service location, it removes it from the pool and prevents any further requests
    from hitting that service. For example, Netflix Ribbon. Another resiliency approach
    includes adding a queue that acts as a buffer between any task and/or service
    that it invokes so as to smoothly handle any intermittent loads and prevent loss
    of data.
  prefs: []
  type: TYPE_NORMAL
- en: Circuit breaker pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We already saw this pattern in [Chapter 1](2eeeb09d-ecd0-403b-8a64-ac754090cebe.xhtml), *Debunking
    Microservices*. Let's quickly recall that. Whenever we have a circuit breaker
    installed and a remote service is being called, the circuit breaker monitors the
    call. If calls are taking too long, the circuit breaker will kill the call and
    make the circuit open, making it impossible to allow any further calls. This is
    the concept of fail fast, recover fast.
  prefs: []
  type: TYPE_NORMAL
- en: The fallback and compensating transaction pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this pattern, whenever a remote service call fails, rather than generating
    an exception, the consumer will try to carry out an alternative way to do that
    action. Ways to achieve this usually include looking for data from an alternate
    data source (let''s say cache) or queuing user''s input for future processing.
    Users will be notified that their requests will be addressed later on and if all
    routes fail the system tries to compensate whatever actions that have been processed.
    Some common approaches to fallback that we use are (as highlighted by Netflix):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cache**: Get data from local or remote cache if the real-time dependency
    is missing, periodically refresh cache data to avoid stale data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Eventual Consistency**: Persist data in queues to be processed further when
    service is available'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stubbed Data**: Keep default values and use those when personalized or service
    responses are not available'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Empty Response**: Return null or empty list'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's look at some practical case studies to handle failures and prevent
    them from cascading or causing a ripple effect.
  prefs: []
  type: TYPE_NORMAL
- en: Case Study – The NetFlix Stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this case study we are going to embrace the Netflix stack and adopt it in
    our microservices. Since the beginning in time we heard : polyglot development
    environment. We are going to do the same here. In this section we will set up
    API Gateway using ZUUL, add auto discovery using Java and Typescript. The user
    will not know which request actually hit, as he is only going to access the Gateway.
    The first part of the case study deals with Introducing Zuul, Eureka and registering
    some services in it and how communication occurs via central Gateway. The next
    part will deal with more significant things such as how to deal with load balancing,
    security, etc. So let''s get started. You can follow along with the example in
    `Chapter 7/netflix` cloud folder. We don''t reinvent the wheel until and unless
    it is very much necessary. Lets leverage things the most we can. The following
    case study supports and encourages polyglot architecture. So let''s get moving.'
  prefs: []
  type: TYPE_NORMAL
- en: Part A – Zuul and Polyglot Environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s have a look at the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: First off we need is a gateway ([Chapter 5](720d1d4e-1795-457c-903e-65c5a5fb5433.xhtml),
    *Understanding API Gateway*) and service registry and discovery ([Chapter 6](0c5e001e-6dca-4805-866c-7be793a91c70.xhtml),
    *Service Registry and Discovery*) solution. We will leverage Zuul and Eureka from
    the Netflix OSS.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: First off we need a Eureka server, copy the source from `Chapter-6/ eureka/eureka-server`
    to a new folder or follow the steps from [Chapter 6](0c5e001e-6dca-4805-866c-7be793a91c70.xhtml), *Service
    Registry and Discovery*, in the Eureka section to create a new server which would
    run on JVM.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Doing nothing fancy, just add annotations `@EnableEurekaServer` and `@SpringBootApplication`
    at relevant places—`DemoServiceDiscoveryApplication.java.`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Configure properties like port number, health check in the `application.properties`
    file by adding the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the Eureka server by the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: You should be able to see Eureka server up and running in port `8761`.
  prefs: []
  type: TYPE_NORMAL
- en: Next up is Zuul or our API Gateway. Zuul will act as routing point for any service
    requests as well as it will be in constant touch with Eureka Server. We will enable
    auto registration of service with Zuul, that is, if any service registers or deregisters
    we won't have to restart Zuul. Having our Gateway in JVM rather than Node.js will
    also give a significant durable boost.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open [https://start.spring.io/](https://start.spring.io/) and generate project
    by adding Zuul and Eureka discovery as dependency. (You can find `zuuul-server`
    under `Chapter 7/netflix cloud`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open `NetflixOsssApplication` and add the following annotations on top of it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Next up we will configure our Zuul server with application level properties:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Run the application by `mvn clean install && java -jar target\netflix-osss-0.0.1-SNAPSHOT.jar`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You should be able to see your Zuul server registered in Eureka dashboard meaning
    that Zuul has run up and successfully.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next up is we create a service in Node.js and Java and register it in Eureka,
    as Zuul has Auto registration enabled, our services will be directly routed without
    any other configurations. Wow!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'So first let''s create a Node.js microservice. Register your microservice with
    Eureka by adding following code in `Application.ts` (place where Express is initialized):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We did nothing new, this is same code we had in [Chapter 6](0c5e001e-6dca-4805-866c-7be793a91c70.xhtml),
    *Service Registry and Discovery*. Just remember that `instanceId`, `vipAddress`
    should be same.
  prefs: []
  type: TYPE_NORMAL
- en: Now run the service by `npm start`. It will open in port `3001`, but our Zuul
    server is listening at port `8762`. So hit the URL `http://localhost:8762/hello-world-chapter-6`
    where `hello-world-chapter-6` is the `vipAddress` or the application name. You
    will be able to see the same output. This confirms our working of Zuul server.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To further understand microservices, I have added a microservice (`http://localhost:8080/product`)
    in Java (nothing fancy, just a GET call, check folder `java-microservice`). After
    registering microservice which runs at port `8080`, when i check via my gateway
    (`http://localhost:8762/java-microservice-producer/product`) it works like a charm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Another feasible option for us includes using Netflix Sidecar. 14\. Let's take
    a break and pat yourself. We have achieved auto registration/deregistration which
    can handle service in any language. We have created a polyglot environment.
  prefs: []
  type: TYPE_NORMAL
- en: Part B – Zuul, Load balancing and failure resiliency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whoa!! *Part A* was awesome. We will go on the same track. The next part which
    comes in our plate is, what will happen when there is heavy traffic. In this part
    we will see how to leverage Zuul which has in built Netflix Ribbon support to
    load balance the requests without much huss or fuss. Whenever a request come to
    Zuul, it pick up one of the available locations it finds and forwards the service
    request to the actual service instance present there. The whole process of caching
    the location of instances and periodically refreshing it and
  prefs: []
  type: TYPE_NORMAL
- en: 'forwarding the request to actual location is given out of the box without any
    configurations needed. Behind the scenes Zuul uses Eureka to administer the routing.
    Furthermore we will be seeing circuit breaker in this example and configure it
    in Hystrix dashboard to see real time analytics. In this section we will configure
    circuit breaker and send those streams to Hystrix. So let us get started. You
    can follow along the example at `Chapter 7/ hystrix`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In extracted source grab the `standalone-hystrix-dashboard-all.jar` and hit
    the `java -jar standalone-hystrix-dashboard-all.jar` command. This will open up
    Hystrix dashboard in port `7979`. Verify URL `http://localhost:7979/hystrix-dashboard`
    to check:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00fa516c-e477-4428-90f8-2055fba54958.png)'
  prefs: []
  type: TYPE_IMG
- en: Time to write a simple program which will trip open a circuit at some time.
    We will be leveraging `opossum` module ([https://www.npmjs.com/package/opossum](https://www.npmjs.com/package/opossum))
    to trip open a circuit. Install the opossum module by the `npm install opossum
    --save` command and write down its custom types as they are not available yet.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We would write a simple logic. We will initialize a number, if it reaches threshold,
    then the circuit would be broken—open state and our fallback function will hit.
    Let's do the needful.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s define our variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We start with count 20 and use two variables to compare in time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We define `circuitBreaker` and instruct our express app to use it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We define a function which increases over time, until it trips open. And we
    define a fall back function like Oops! Service down:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'That''s it! Open up Hystrix, enter URL `http://localhost:3000/hystrix.stream`
    in Hystrix streams, and you will be able to see live monitoring of the circuit:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/efa92def-d38f-4d51-8656-8cbf38b0fd0e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once it reaches the peak stage, it will trip open:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4992df36-cc89-4678-bbeb-0f219ea4b1c8.png)'
  prefs: []
  type: TYPE_IMG
- en: After preconfigured time, it will again be closed state and ready to serve requests.
    A full detailed API can be found here [https://www.npmjs.com/package/opossum](https://www.npmjs.com/package/opossum).
  prefs: []
  type: TYPE_NORMAL
- en: Message queues and brokers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A message queue is an answer for problem of application to application communication.
    This communication occurs regardless of where my application or my data is, whether
    I am on same server, separate server, server with different OS or anything similar.
    Message queuing is build for scenarios such as a task list or work queue. A message
    queue solves the problem by passing and sending data via queues. Application then
    make use of information in the messages to interact further. The platform provided
    is secured and reliable. Whereas a Message broker is build to extend the functionality
    of message queue and it is able to understand the content of each message which
    moves through out the broker. A set of operations defined on each message are
    processed. Message processing nodes which are packaged along with message broker
    are able to understand messages from various sources such as JMS, HTTP, and files.
    In this section we will explore message bus and message brokers in detail. Popular
    message brokers include Kakfa, RabbitMQ, Redis, and NSQ. We will see Apache Kakfa
    in much detail in the next section which is a advanced version of messaging queues.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to pub/sub pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Just as message queuing, pub-sub (publish-subscribe) pattern moves information
    from producer to a consumer. However the major difference here is this pattern
    allows multiple consumers to receive each message in a topic. It ensures that
    the consumer receives messages in a topic in exact same order in which it was
    received in the messaging system. This pattern can be better understood by taking
    a real life scenario. Consider a stock market. It is used by large number of people
    and applications, all of whom should be send messages real time and just the exact
    sequence of prices. There is a huge difference between a stock going up to down
    and stock going down to up. Lets see an example Apache Kafka is one of the shining
    solution when it comes to pub sub pattern. As per docs of Apache Kafka—Kafka is
    a distributed, partitioned, replicated commit log service. It provides the functionality
    of a messaging system, but with a unique design.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kafka is a streaming platform that allows applications to get and take messages.
    It is used for making real time data pipeline streaming apps. Let''s get acquainted
    with Kafka terminology:'
  prefs: []
  type: TYPE_NORMAL
- en: Producers are someone who send data to Kafka.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consumers are someone who read data from Kafka.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data is send in the form of record. Each record is associated with a topic.
    A topic has a category and it consists of a key, a value and a timestamp.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consumers usually subscribe to given topics and get a stream of records and
    they are alerted whenever a new record comes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a consumer goes down, they can restart streaming by tracking the last offset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Order of messages is guaranteed.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will commence this case study with three phases:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install Kakfa Locally:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To setup Kakfa locally, download the bundle and extract it to a location of
    choice. Once extracted we need to setup Zookeeper. To do so start `zookeeper`
    by the following command—`bin\windows\zookeeper-server-start.bat config\zookeeper.properties`.
    Java 8 is essential for this case study. Since I am working on windows for this
    example, my commands has `Windows` folder in it. Be sure to be aware about the
    `.sh` and the `.bat` difference.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 2\. Next we will start the Kakfa server. Hit the the following command—
  prefs: []
  type: TYPE_NORMAL
- en: '`bin\windows\kafka-server-start.bat config\server.properties`.'
  prefs: []
  type: TYPE_NORMAL
- en: 3\. We will create a topic named offers with a single partition and only one
    replica—`bin\windows\kafka-topics.bat --create --zookeeper localhost:2181 --replication-factor
    1 --partitions 1 --topic offers`. You will get a prompt Created topic offers.
    To see the topic we can hit `bin\windows\kafka-topics.bat --list --zookeeper localhost:2181`.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Kakfa is up and running on `localhost:2181`. We can even create topics via
    our broker or Node.js client.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Kafka producer
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will leverage `kakfa-node` module ([https://www.npmjs.com/package/kafka-node](https://www.npmjs.com/package/kafka-node)).
    As per need we can either setup a separate service or integrate in existing application
    service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Right now we will just write two seperate files in two different projects to
    test out our application.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can check `Chapter-8/kakfka/node-producer` to check the source:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'You can bind the on message event like this. Through the same module we can
    create client who is going to listen on message of offers and process event accordingly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Kafka is a powerful player which can be used in variety of things where we actually
    need real time data processing. The pub/sub pattern is a great way to achieve
    event driven communication.
  prefs: []
  type: TYPE_NORMAL
- en: Sharing dependencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Microservices are great when it comes to building scalable code bases with independent
    deployments, separating concerns, better resilience, polyglot technologies and
    better modularity, reusability, and development life cycle. However, modularity
    and reusability come at a cost. More modularity and reusability may often result
    in high coupling or code duplications. Having many different services attached
    to the same shared library will soon lead us back to square one and we will end
    up with monolithic hell.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we are going to see how to overcome this hell. We will see
    some options with practical implementations and understand the sharing code and
    common code process. So let's get started.
  prefs: []
  type: TYPE_NORMAL
- en: The problem and solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sharing code between microservices is always tricky. We need to make sure that
    a common dependency does not break our microservices freedom. The major goals
    that we want to achieve while sharing code are:'
  prefs: []
  type: TYPE_NORMAL
- en: Share common code among our microservices, while making sure that our code is
    **Don't Repeat Yourself** (**DRY**)—it is a coding principle with the main aim
    to reduce any repetition of code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoid tight coupling through any common shared library, as it eliminates the
    freedom of microservices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enable simple changes in order to sync the code we can share between our microservices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microservices are something that introduce code duplications. Creating an `npm`
    package with a new code base for any such business use case is highly impractical
    as it will generate a lot of overhead to make it harder to maintain any code changes.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to use **bit** ([https://bitsrc.io/](https://bitsrc.io/)) to solve
    our dependency problem and to achieve our goals. Bit operates on the philosophy
    that components are the building blocks, you are the architect. Using bit, we
    don't have to create a new repository or add packages to share code instead of
    duplicating it. You just need to define reusable parts of any existing microservices
    and share them to other microservices as any package or tracked source code. This
    way, we can easily make parts of any service reusable without modifying any single
    line of code and not introduce tight coupling among services. The major advantage
    of bit is that it gives us the flexibility to make the changes to code that are
    shared with any other service, thus allowing us to develop and modify the code
    from anywhere in our microservice ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with bit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Coupling microservices via common libraries is very bad. Bit promotes building
    components. We simply isolate and sync any reusable code and let bit handle how
    to isolate and track source code among the projects. This can still be installed
    with NPM and make changes from any end. Let''s say you are making some great system
    with top-notch functionalities that are common everywhere. You want to share code
    among these services. You can follow along with the code inside the `bit-code-sharing`
    folder in [Chapter 7](162a0f25-2890-4a58-aa41-e9c9b5fc6c2d.xhtml), *Service State
    and Interservice Communication*:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Bit would be installed as a global module. Install `bit` by typing the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: For this example, check out `demo-microservice`, which has common utilities
    such as fetching from the cache, common logging utility, and so on. We want these
    functionalities everywhere. That's where we are going to use `bit` to make our
    file `common/logging.ts` available everywhere.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Time to initialize `bit` and tell `bit` to add `logging.ts` in the tracking
    list. Open up `demo-microservice` in the Terminal and type the `bit init `command.
    This will create a `bit.json` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we will tell `bit` to start tracking the `common` folder. Hit the following
    command in Terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Here we used `*` as a global pattern so we can track multiple components on
    the same path. It will track all the components inside the `common` folder and
    you should be able to see a message tracking two new components.
  prefs: []
  type: TYPE_NORMAL
- en: Bit components are added to our bit tracking list. We can simply hit `bit status`
    to check the current status of bit in our microservices. It will show two components
    under the New Components section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we will add build and test environments so we don't introduce any abnormalities
    in our ecosystem before sharing the component. First off is our build environment.
    The build environment is essentially a build task that is used by bit to run and
    compile the component, since our file is written in TypeScript. To import dependencies
    you need to create an account at [https://bitsrc.io](https://bitsrc.io) and sign
    up for the public tier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the TypeScript compiler by adding the following line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: You will need to enter the user credentials for the account you just made. Once
    installed, we will go with a public scope as of now.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hit the command `bit build` to see the `distribution` folder with our generated
    files. You can write tests similarly to check whether unit test cases pass or
    not. Bit has support in-built for mocha and jest. We will just create a `hello-world`
    test right now. We need to specify to bit for what component, which would be the
    `test` file explicitly. So let''s untrack previously added files, as we need to
    pass on our spec files:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `test` folder inside `src` and install testing libraries by hitting
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Create `logging.spec.ts` inside the `tests` folder and add the following code.
    Similarly, create `cacheReader.spec.ts`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: We will see detailed testing concepts in [Chapter 8](a7273aa2-2981-4013-8d5f-dbee87462d35.xhtml),
    *Testing, Debugging, and Documenting*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To tell `bit` about our testing strategy, hit the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Hit the command `bit test` and it will print the test results against each component
    added.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We are all done. Time to share our brand new component with the world. First,
    we will lock a version and isolate it from other components from this project.
    Hit the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: You should be able to see an output stating added components `common/logging@1.0.0`
    and `common@cache-reader@1.0.0`. When you do a `bit status` you will be able to
    see that these components have moved from new components to staged components.
  prefs: []
  type: TYPE_NORMAL
- en: 'To share it with other services we export it using `bit export`. We will push
    it to the remote scope so it can be accessed from anywhere. Go to [http://bitsrc.io/](http://bitsrc.io/),
    log in, and then create a new scope there. Now we will push our code to that scope:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: You can log in to your account and then check code in pushed repositories.
  prefs: []
  type: TYPE_NORMAL
- en: 'To import in other workspaces, you can follow these steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We need to tell the node that bit is one of our registries from where to download
    modules. So add the `bit` repository as one of the registries with alias `@bit`
    in `npm config`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'To download from any other project, use the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The command is similar to `npm i <alias we created>/<username>.<scopename>.<username>`.
    Once installed, you can use it just like any other node module. Check out `chapter
    9/bit-code-sharing/consumer` for this.
  prefs: []
  type: TYPE_NORMAL
- en: You can also use `bit import` and other utilities such as making changes, syncing
    code, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Sharing code is a must for development and maintenance offers. However, tightly
    coupling services through shared libraries ruins the point of having microservices.
    Creating new repositories in NPM for any new common use case is impractical as
    we have to make many changes. Tools such as bit have the best of both worlds.
    We can easily share code and also make and sync changes from any end.
  prefs: []
  type: TYPE_NORMAL
- en: The problem of shared data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having common shared data among microservices is a huge pitfall. Firstly, all
    the microservice's requirements may not be satisfied with a single database. Also,
    it increases development time coupling. For example, `InventoryService` will need
    to coordinate the schema changes with the developers of other services that use
    the same tables. It also increases runtime coupling. If, for instance, a long-running
    `ProductCheckOut` service holds a lock on the `ORDER` table, then any other service
    using that same table will be blocked. Each service must have its own databases
    and data must not be directly accessible by any other service.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, there is a huge situation that we need to take care of. The problem
    of transactions and how to handle them. Even though keeping transaction-related
    entities in the same database and leveraging database transactions seems the only
    option, we cannot go with that. Let''s see what should we do:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Option 1**: If any update happens only in one microservice, then we can leverage
    an asynchronous messaging/service bus to handle that for us. Service bus will
    maintain two-way communication so as to ensure business capability is achieved.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Option 2**: This is where we want transactional data to be handled. For example,
    checkout should only occur if payment is done. If not, then it should not proceed
    further with anything. Either we need to merge the services or we can use transactions
    (something like Google Spanner for Distributed transactions). We are stuck with
    two options, either settle via transactions or handle situations accordingly.
    Let''s look at how to handle these scenarios in various ways.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To manage data consistency, one of the most widely used patterns is the saga
    pattern. Let's understand a practical use case that we have. We have a customers
    rewards point service that maintains the total allowed points to buy. The application
    must ensure that new orders must not exceed customers allowed rewards points.
    Since orders and customers rewards points are in different databases, we must
    maintain data consistency.
  prefs: []
  type: TYPE_NORMAL
- en: 'As per the saga pattern, we must implement each business transaction that spans
    across multiple services. It would be a sequence of local transactions. Each individual
    transaction updates the database and publishes a message or an event that will
    trigger the next local transaction in the saga. If the local transaction fails,
    then saga executes a series of compensating transactions that actually undo changes
    that were made by the previous transaction. Here are the steps that we will execute
    in our case. This is one case for maintaining consistency for consistency via
    events:'
  prefs: []
  type: TYPE_NORMAL
- en: Rewards service creates an order in pending state and publishes a points processed
    event.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customer service receives the event and attempts to block rewards for that order.
    It publishes a rewards blocked event or a rewards blocked failed event.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The order service receives the event and changes the state accordingly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The most predominant patterns used are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**State store**: A service records all the state changes in a state store.
    When any failure occurs, we can query the state store to find and recover any
    incomplete transactions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Process manager**: A process manager that listens to events generated by
    any operations and decides on whether to complete the transaction or not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Routing slip**: Another dominant approach is making all operations asynchronous.
    A service makes a message with two request commands (a debit and shipping instruction)
    in a slip called a routing slip. This message is passed to the debit service from
    the routing slip. The debit service executes the first command and fills the routing
    slip before passing the message to a shipping service that completes the shipping
    operation. If there is a failure, the message is sent back to error queue, where
    the service can watch the state and error status to compensate if needs arise.
    The following diagram describes the same process:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/77501b19-b2df-4878-9ce0-0e26bc299433.png)'
  prefs: []
  type: TYPE_IMG
- en: Routing slip
  prefs: []
  type: TYPE_NORMAL
- en: Data sharing in microservices always remains a pain if not handled properly.
    There are various solutions to distributed transactions across microservices.
    We saw widely used solutions such as saga and went through various ways to handle
    data eventual consistency.
  prefs: []
  type: TYPE_NORMAL
- en: Cache
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we are pretty much in the driver''s seat of our microservice development.
    We have developed microservices, connected them via a gateway, and set up a communication
    layer between them. Since we have distributed our code into various services,
    one of the problems that may arise is accessing the much-needed data at the right
    time. Using in-memory has its own set of challenges that we never want to introduce
    (for example, you need to introduce load balancers, session replicators, and so
    on). We need some way to access temporary data across services. This would be
    our caching mechanism: one service creates and stores data in cache, while others
    may use it on need and situation basis or fail basis. This is where we will introduce
    Redis as our cache database. Prominent caching solutions include Redis and Hazelcast.'
  prefs: []
  type: TYPE_NORMAL
- en: Blessing and curse of caching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whenever we are told to optimize the performance aspects of our application,
    the first thing that comes to mind is caching. Caching can be defined as a process
    of temporarily holding retrieved or computed data in either data store (server's
    RAM, a key-value store like Redis) in the hope that future access to this information
    will be faster. Updating of this information can be triggered or this value can
    be invalidated after some fixed interval of time. The advantages of caching seem
    huge at first. Calculating resources once and then fetching from cache (read-efficient
    resources) avoids frequent network calls and hence it can result in shorter load
    times, more responsive websites, and a more revenue-generating end user experience.
  prefs: []
  type: TYPE_NORMAL
- en: However, caching is not a one-stop solution. Caching is indeed an effective
    strategy for static content and for APIs that can tolerate stale data up to some
    point, but it is not applicable elsewhere in situations as data is very huge and
    dynamic. For example, consider the inventory of a given product in our shopping
    cart microservices. This count will change very rapidly for popular products,
    while it might change for some other products. So determining the right age for
    the cache is quite a conundrum here. Adding caching introduces other components
    needed to be managed (such as Redis, Hazelcast, Memcached, and so on). It adds
    costs, the process of procuring, configuring, integrating, and maintaining. Caching
    can introduce other dangers too. Sometimes reading from the cache can be slow
    (cache layer not properly maintained, the cache is within network boundaries,
    and so on). Maintaining cache with updated deployments is also a huge nightmare.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some of the practices that need to be maintained in order
    to use cache effectively, that is, to make our service work less:'
  prefs: []
  type: TYPE_NORMAL
- en: Using HTTP standards (standards such as If-modified-Since and Last-Modified
    response headers).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other options include ETag and If-none-match. A unique **Entity tag** (**ETag**)
    is generated and sent to service request after the first call, which the client
    sends in *if-none-match-header*. When the server finds that ETag has not been
    changed it sends an empty body with a `304 Not Modified` response.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The HTTP Cache-Control header can be used to help the service to control all
    caching entities. It has various attributes such as **private** (not allowed to
    cache the content if this header is included), **no-cache** (force server to resubmit
    to make a fresh call), **public** (mark any response as cacheable), and **max-age**
    (maximum time to be cached).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Look at the following diagram to understand some caching scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/63da78bd-f083-4e76-bcbe-0c706ca5bc3f.png)'
  prefs: []
  type: TYPE_IMG
- en: Cache scenarios
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Redis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Redis is a simple NoSQL database which focuses on simple data structure (key-value
    pair) with high availability and read efficiency. Redis is open source, in memory
    data structure store which can be used as database as well as cache or message
    broker. It has option for built in data structure like strings, hashes, lists,
    sets, range queries, geospatial indexes,and so on. It has out of the box built
    in replication, transactions, different levels of disk persistence, and options
    of high availability and automatic partitioning. We can also add persistent storage
    as well rather than going for in-memory storage.
  prefs: []
  type: TYPE_NORMAL
- en: Redis when combined with Node.js is like a match made in heaven as Node.js is
    highly efficient in network I/O. NPM repository has a lot of Redis packages to
    smoothen our development. The forerunners are `redis` ([https://www.npmjs.com/package/redis](https://www.npmjs.com/package/redis)),
    `ioredis` ([https://www.npmjs.com/package/ioredis](https://www.npmjs.com/package/ioredis))
    and `hiredis` ([https://www.npmjs.com/package/hiredis](https://www.npmjs.com/package/hiredis)).
    The `hiredis` package has lots of performance benefits. To get started with our
    development we first need to install `redis`. In the next section, we will setup
    our distributed caching in our project.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up our distributed caching using redis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To understand the caching mechanism let''s take a practical example and implement
    distributed caching. We will evolve around the shopping cart example. As dividing
    business capabilities in to different services is a good thing, we are dividing
    our inventory service and checkout service into two different services. So whenever
    a user adds anything to cart, we will never persist the data, but rather store
    it temporarily as this ain''t permanent or functionality changing data. We would
    persist such kind of ephemeral data into Redis as its read efficiency is super
    awesome. Our solution for this problem would be divided in to following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: First we focus on setting up our `redis` client. Like everything pull out a
    docker image by `docker pull redis`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the image is in our local just hit `docker run --name tsms -d redis`. There
    are also options for persistence storage volume. You just have to append a parameter
    `docker run --name tsms -d redis redis-server --appendonly yes`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Verify redis running by command just hit `redis-cli`, you should be able see
    output pong.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Time to pull strings at Node.js. Install the module by adding `npm install redis
    --save` and `npm install @types/redis --save`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a client by `import * as redis from 'redis'; let client=redis.createClient('127.0.0.1',
    6379);`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use Redis just like any other datastore:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Similarly you can play with redis as and where required. It can be even used
    as command library. For detailed documentation please check this link ([https://www.npmjs.com/package/redis](https://www.npmjs.com/package/redis)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We had to replicate the code in each of the three service for Redis. To avoid
    that in the later section we would be using Bit: a code sharing tool.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we are going to see How to version microservices and make
    our microservices to have fail safe mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at collaboration among microservices. There are three
    kinds of microservice collaborations. Command-based collaboration (where one microservice
    uses an HTTP POST or PUT to make another microservice to perform any action),
    query-based collaboration (one microservice leverages an HTTP GET to query state
    of another service), and event-based collaboration (one microservice exposes an
    event feed to another microservice that can subscribe by polling the feed constantly
    for any new events). We saw various collaboration techniques, which included the
    pub-sub pattern and NextGen communication techniques such as gRPC, Thrift, and
    so on. We saw communication via service bus and saw how to share code among microservices.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to look into aspects of testing, monitoring,
    and documentation. We will look into different kinds of tests that we can do and
    how to write test cases and execute them before releasing them to production.
    Next we will look into contract testing using PACT. Then we will move on to debugging
    and looking into how to leverage debugging and profiling tools to effectively
    monitor the bottlenecks in our collaboration portal. Finally, we will generate
    documentation for our microservices using Swagger, which can be read by anyone.
  prefs: []
  type: TYPE_NORMAL
