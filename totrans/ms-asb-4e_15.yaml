- en: '*Chapter 12*: Infrastructure Provisioning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Almost everything in data centers is becoming software-defined, from networks
    to the server infrastructure on which our software runs. **Infrastructure as a
    Service** (**IaaS**) providers offer APIs for programmatically managing images,
    servers, networks, and storage components. These resources are often expected to
    be created just-in-time, in order to reduce costs and increase efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, a great deal of effort has gone into the cloud provisioning aspect
    of Ansible over the years, with more than 30 infrastructure providers catered
    for in the official Ansible release. These range from open source solutions such
    as OpenStack and oVirt to proprietary providers such as VMware and cloud providers
    such as AWS, GCP, and Azure.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are more use cases than we can cover in this chapter, but nonetheless,
    we will explore the following ways in which Ansible can interact with a variety
    of these services:'
  prefs: []
  type: TYPE_NORMAL
- en: Managing an on-premise cloud infrastructure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing a public cloud infrastructure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interacting with Docker containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building containers with Ansible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To follow the examples presented in this chapter, you will need a Linux machine
    running **Ansible 4.3** or newer. Almost any flavor of Linux should do – for those
    interested in specifics, all the code presented in this chapter was tested on
    Ubuntu Server 20.04 LTS unless states otherwise, and on Ansible 4.3\. The example
    code that accompanies this chapter can be downloaded from GitHub at this URL:
    [https://github.com/PacktPublishing/Mastering-Ansible-Fourth-Edition/tree/main/Chapter12](https://github.com/PacktPublishing/Mastering-Ansible-Fourth-Edition/tree/main/Chapter12).'
  prefs: []
  type: TYPE_NORMAL
- en: Check out the following video to see the Code in Action:[https://bit.ly/3BU6My2](https://bit.ly/3BU6My2)
  prefs: []
  type: TYPE_NORMAL
- en: Managing an on-premise cloud infrastructure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The cloud is a popular but vague term, used to describe IaaS. There are many
    types of resources that can be provided by a cloud, although the most commonly discussed are
    compute and storage. Ansible is capable of interacting with numerous cloud providers
    in order to discover, create, or otherwise manage resources within them. Note
    that although we will focus on the compute and storage resources in this chapter,
    Ansible has a module for interacting with many more cloud resource types, such
    as load balancers, and even cloud role-based access controls.
  prefs: []
  type: TYPE_NORMAL
- en: One such cloud provider that Ansible can interact with is OpenStack (an open
    source cloud operating system), and this is a likely solution for those with a
    need for on-premise IaaS functionality. A suite of services provides interfaces
    to manage compute, storage, and networking services, plus many other supportive
    services. There is not a single provider of OpenStack; instead, many public and
    private cloud providers build their products with OpenStack, and thus although
    the providers may themselves be disparate, they provide the same APIs and software
    interfaces so that Ansible can automate tasks with ease in these environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ansible has supported OpenStack services since very early in the project, and
    this support can now be found as part of the `OpenStack.Cloud` collection. That
    initial support has grown to include over 70 modules, with support for managing
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Compute
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bare-metal compute
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Authentication accounts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Block storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to performing **create, read, update, and delete** (**CRUD**) actions
    on the preceding types of resources, Ansible also includes the ability to use
    OpenStack (and other clouds) as an inventory source, and we touched on this earlier,
    in [*Chapter 1*](B17462_01_Final_JC_ePub.xhtml#_idTextAnchor015), *The System
    Architecture and Design of Ansible*.Again, the dynamic inventory provider maybe
    found in the `OpenStack.Cloud` collection. Each execution of `ansible` or `ansible-playbook` that
    utilizes an OpenStack cloud as an inventory source will get on-demand information
    about what compute resources exist, and various facts about those compute resources.
    Since the cloud service is already tracking these details, this can reduce overheads
    by eliminating the manual tracking of resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate Ansible''s ability to manage and interact with cloud resources,
    we''ll walk through two scenarios: a scenario to create and then interact with
    new compute resources and a scenario that will demonstrate using OpenStack as
    an inventory source.'
  prefs: []
  type: TYPE_NORMAL
- en: Creating servers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The OpenStack compute service provides an API for creating, reading, updating,
    and deleting virtual machine servers. Through this API, we'll be able to create
    the server for our demonstration. After accessing and modifying the server through
    SSH, we'll also use the API to delete the server. This self-service ability is
    a key feature of cloud computing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ansible can be used to manage these servers by using the various `openstack.cloud` modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '`openstack.cloud.server`: This module is used to create and delete virtual
    servers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`openstack.cloud.server_info`: This module is used to gather information about
    a server – in Ansible 2.9 and earlier, it returned these as facts, but this is
    no longer the case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`openstack.cloud.server_action`: This module is used to perform various actions
    on a server.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`openstack.cloud.server_group`: This module is used to create and delete server
    groups.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`openstack.cloud.server_volume`: This module is used to attach or detach block
    storage volumes from a server.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`openstack.cloud.server_metadata`: This module is used to create, update, and
    delete metadata for virtual servers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Booting virtual servers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For our demonstration, we will use `openstack.cloud.server`. We'll need to provide
    authentication details about our cloud, such as the auth URL and our login credentials.
    In addition to this, we will need to set up our Ansible host with the correct
    prerequisite software for this module to function. As we discussed earlier in
    the book when addressing dynamic inventories, Ansible sometimes requires additional
    software or libraries on the host in order to function. In fact, it is a policy
    of the Ansible developers to not ship cloud libraries with Ansible itself, as
    they would rapidly become out of date, and different operating systems would require
    different versions – even the advent of collections has not changed this.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can always find the software dependencies in the Ansible documentation
    for each module, so it is worth checking this when using a module for the first
    time (especially a cloud provider module). The Ansible host used for the demos
    throughout this book is based on Ubuntu Server 20.04 and in order for the `openstack.cloud.server`
    module to function, I had to run the following command first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The exact software and version will depend on our host operating system and
    may change with newer Ansible releases. There may be native packages available
    for your operating system, or you could install this Python module with `pip`.
    It is worth spending a few minutes checking the best approach for your operating
    system before proceeding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the prerequisite modules are in place, we can proceed with the server
    creation. For this, we''ll need a flavor, an image, a network, and a name. You
    will also need a key, and this will need to be defined in the OpenStack GUI (or
    CLI) before proceeding. Naturally, these details may be different for each OpenStack
    cloud. For this demo, I am using a single, all-in-one VM based on **DevStack**,
    and I am using defaults as much as possible, to make it easy to follow. You can
    download DevStack and learn about getting started quickly here: [https://docs.openstack.org/devstack/latest/](https://docs.openstack.org/devstack/latest/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'I''ll name our playbook `boot-server.yaml`. Our play starts with a name and
    uses `localhost` as the host pattern as the module we are calling talks to the
    OpenStack API from the local Ansible machine directly. As we do not rely on any
    local facts, I''ll turn fact-gathering off as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'To create the server, I''ll use the `openstack.cloud.server` module and provide
    the `auth` details relevant to an OpenStack cloud that I have access to, as well
    as a flavor, image, network, and name. Note the `key_name`, which indicates the
    SSH public key from the keypair you would have created for yourself in OpenStack
    prior to writing this playbook (as discussed previously in this chapter). This
    SSH public key is integrated into the `Fedora34` image we are using when it is
    first booted on OpenStack so that we can subsequently gain access to it over SSH. I
    also uploaded a `Fedora34` image for demonstration purposes in this chapter, as
    it allows greater manipulation than the default Cirros image that is included
    with OpenStack distributions. These images can be freely downloaded, ready-made,
    from [https://alt.fedoraproject.org/cloud/](https://alt.fedoraproject.org/cloud/).
    Finally, as you''d expect, I''ve obfuscated my password:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Authentication details can be written to an external file, which will be read
    by the underlying module code. This module code uses `openstacksdk`, a standard
    library for managing OpenStack credentials. Alternatively, they can be stored
    in an Ansible vault, as we described in [*Chapter 3*](B17462_03_Final_JC_ePub.xhtml#_idTextAnchor061), *Protecting
    Your Secrets with Ansible*, and then passed to the module as variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Running this play as is will simply create the server, and nothing more. To
    test this out (assuming you have access to a suitable OpenStack environment),
    run the playbook with the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Ensuring the Correct Python Environment Is Used
  prefs: []
  type: TYPE_NORMAL
- en: Note that on Ubuntu Server 20.04, Ansible runs by default under Python 2.7 –
    this is not a problem and we have ignored this so far in this book – however,
    in this particular instance, we have installed the `openstacksdk` module only
    on Python 3, and as a result, we must tell Ansible to use the Python 3 environment.
    We do this here by setting an environment variable, but you could just as easily
    do this via an `ansible.cfg` file – this is left as an exercise for you to explore.
  prefs: []
  type: TYPE_NORMAL
- en: 'A successful run of the playbook should yield output similar to that shown
    in *Figure 12.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.1 – Creating a virtual instance in OpenStack with Ansible'
  prefs: []
  type: TYPE_NORMAL
- en: '](Images/B17462_12_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.1 – Creating a virtual instance in OpenStack with Ansible
  prefs: []
  type: TYPE_NORMAL
- en: 'I''ve truncated the output, as there is a lot of data returned from the module.
    Most importantly, we get data regarding the IP addresses of the host. This particular
    cloud uses a floating IP to provide public access to the server instance, which
    we can see the value of by registering the output and then debug printing the
    value of `openstack.accessIPv4`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute this playbook using a command similar to the preceding command (but
    without the added verbosity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This time, the first task does not result in a change, as the server that we
    want already exists – however, it still retrieves the information about the server,
    enabling us to discover its IP address:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.2 – Using Ansible to retrieve the IP address of the OpenStack virtual
    machine we booted in the previous example'
  prefs: []
  type: TYPE_NORMAL
- en: '](Images/B17462_12_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.2 – Using Ansible to retrieve the IP address of the OpenStack virtual
    machine we booted in the previous example
  prefs: []
  type: TYPE_NORMAL
- en: The output shows an IP address of `172.24.4.81`. I can use that information
    to connect to my newly created cloud server.
  prefs: []
  type: TYPE_NORMAL
- en: Adding to runtime inventory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Booting a server isn't all that useful by itself. The server exists to be used
    and will likely need some configuration to become useful. While it's possible
    to have one playbook to create resources and a completely different playbook to
    manage configuration, we can also do it all from the same playbook. Ansible provides
    a facility to add hosts to the inventory as a part of a play, which will allow
    for the use of those hosts in subsequent plays.
  prefs: []
  type: TYPE_NORMAL
- en: 'Working from the previous example, we have enough information to add the new
    host to the runtime inventory, by way of the `ansible.builtin.add_host` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: I know that this image has a default user of `fedora`, so I set a host variable
    accordingly, along with setting the IP address as the connection address.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: This example is also glossing over any required security group configuration
    in OpenStack, and any accepting of the SSH host key. Additional tasks can be added
    to manage these things, or you might pre-configure them as I have done in my environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the server added to our inventory, we can do something with it. Let''s
    imagine a scenario in which we want to use this cloud resource to convert an image
    file, using `ImageMagick` software. To accomplish this, we''ll need a new play
    to make use of the new host. I know that this particular Fedora image does not
    contain Python, so we need to add Python and the Python bindings for `dnf` (so
    we can use the `ansible.builtin.dnf` module) as our first task, using the `ansible.builtin.raw`
    module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll need the `ImageMagick` software, which we can install by using
    the `dnf` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the playbook at this point will show the changed tasks for our new
    host; note that this time, we must give `ansible-playbook` the location of our
    private key file from OpenStack, so that it can authenticate to the Fedora image,
    using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'A successful run of the playbook should yield output like that shown in *Figure
    12.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.3 – Performing post instantiation configuration on our OpenStack
    virtual'
  prefs: []
  type: TYPE_NORMAL
- en: machine using Ansible
  prefs: []
  type: TYPE_NORMAL
- en: '](Images/B17462_12_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.3 – Performing post instantiation configuration on our OpenStack virtual
    machine using Ansible
  prefs: []
  type: TYPE_NORMAL
- en: We can see Ansible reporting two changed tasks on the host `mastery1`, which
    we just created in the first play. This host does not exist in the `mastery-hosts` inventory
    file.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have turned off verbose reporting here, too, as the output would otherwise
    be very cumbersome to wade through; however, given that we have the private key
    file for our OpenStack instance, we can manually log in and check the results
    of our playbook, for example, using a command like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This command queries the RPM package database and displays a short list of
    the most recently installed ones. The output might look something like that shown
    in *Figure 12.4*, though dates will undoubtedly vary:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.4 – Checking the success of our playbook on our OpenStack VM'
  prefs: []
  type: TYPE_NORMAL
- en: '](Images/B17462_12_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.4 – Checking the success of our playbook on our OpenStack VM
  prefs: []
  type: TYPE_NORMAL
- en: From here, we could extend our second play to upload a source image file by
    using `ansible.builtin.copy`, then perform a command by using `ImageMagick` on
    the host to convert the image. Another task can be added to fetch the converted
    file back down by using the `ansible.builtin.slurp` module or the modified file
    can be uploaded to a cloud-based object store. Finally, a last play can be added
    to delete the server itself.
  prefs: []
  type: TYPE_NORMAL
- en: The entire lifespan of the server, from creation to configuration to use, and
    finally, to removal, can all be managed with a single playbook. The playbook can
    be made dynamic by reading runtime variable data, in order to define what file
    should be uploaded/modified and where it should be stored, essentially turning
    the playbook into a reusable program. Although somewhat simplistic, hopefully,
    this gives you a clear idea of how powerful Ansible is for working with infrastructure
    service providers.
  prefs: []
  type: TYPE_NORMAL
- en: Using OpenStack inventory sources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our previous example showed a single-use, short-lived cloud server. What if
    we want to create and use long-lived cloud servers, instead? Walking through the
    tasks of creating them and adding them to the temporary inventory each time we
    want to touch them seems inefficient. Manually recording the server details in
    a static inventory also seems inefficient, and also error-prone. Thankfully, there
    is a better way: using the cloud itself as a dynamic inventory source.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ansible ships with a number of dynamic inventory scripts for cloud providers,
    as we discussed in [*Chapter 1*](B17462_01_Final_JC_ePub.xhtml#_idTextAnchor015), *The
    System Architecture and Design of Ansible*. We''ll continue our examples here
    with OpenStack. To recap, the `openstack.cloud` collection provides the dynamic
    inventory script that we need. To make use of this script, we need to create a
    YAML file that tells Ansible to utilize this inventory script – this file must
    be named `openstack.yaml` or `openstack.yml`. It should contain code that looks
    something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The configuration file needs a bit more consideration. This file holds authentication
    details for the OpenStack cloud(s) to connect to. That makes this file sensitive,
    and it should only be made visible to the users that require access to this information.
    In addition, the inventory script will attempt to load the configuration from
    the standard paths used by `os-client-config` (https://docs.openstack.org/os-client-config/latest/user/configuration.html#config-files),
    the underlying authentication code. This means that the configuration for this
    inventory source can live in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`clouds.yaml` (in the current working directory when executing the inventory
    script)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`~/.config/openstack/clouds.yaml`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`/etc/openstack/clouds.yaml`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first file that's found will be used. You can override this by adding the
    `clouds_yaml_path` to the `openstack.yaml` we created earlier in this section.
    For our example, I'll use a `clouds.yaml` file in the playbook directory alongside
    the script itself, in order to isolate configuration from any other paths.
  prefs: []
  type: TYPE_NORMAL
- en: 'Your `clouds.yaml` file will look very similar to the `auth:` section of the
    parameters to the `openstack.cloud.server` module we used in our earlier examples.
    There is one key difference though – in our earlier examples, we used the `demo`
    account and limited ourselves to the `demo` project in OpenStack. For us to query
    all instances across all projects (which we want to do to demonstrate some functionality),
    we need an account with administrator privileges rather than the `demo` account.
    For this part of the chapter, my `clouds.yaml` file contains the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The actual dynamic inventory script has a built-in help function, which you
    can also use to learn more about it. If you can locate it on your system, you
    can run this – on my system I used this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'There is one final thing to know before we get started: If you are using the
    Ansible 4.0 release, this ships with version `1.4.0` of the `openstack.cloud`
    collection. This has a bug in it that renders the dynamic inventory script inoperable.
    You can query your installed collection version using this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'If you need to install a newer version, you can install it using this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This will install the collection in a hidden directory within your home directory,
    so if you are using the local copy, don''t use this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Use this instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The `help` output for the script shows a few possible arguments; however, the
    ones that Ansible will use are `--list` and `--host`, as *Figure 12.5* shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.5 – Demonstrating the help function of the openstack_inventory.py
    script'
  prefs: []
  type: TYPE_NORMAL
- en: '](Images/B17462_12_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.5 – Demonstrating the help function of the openstack_inventory.py
    script
  prefs: []
  type: TYPE_NORMAL
- en: The first is used to get a list of all of the servers visible to the account
    used, and the second would be used to get host variable data from each, except
    that this inventory script returns all of the host variables with the `--list` call.
    Returning the data with the host list is a performance enhancement, as we discussed
    earlier in the book, eliminating the need to call the OpenStack APIs for each
    and every host returned.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output from `--list` is quite long; here are the first few lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.6 – Demonstrating the data returned by the openstack_inventory.py
    dynamic inventory'
  prefs: []
  type: TYPE_NORMAL
- en: '](Images/B17462_12_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.6 – Demonstrating the data returned by the openstack_inventory.py
    dynamic inventory
  prefs: []
  type: TYPE_NORMAL
- en: 'The configured account only has one visible server, which has a UUID of `875f88bc-ae18-42da-b988-0e4481e35f7e`,
    the instance that we booted in a previous example. We see this instance listed
    in the `flavor-ds1G` and `image-Fedora34` groups, for example. The first group
    is for all of the servers running with the `ds1G` flavor, and the second is for
    all servers running from our `Fedora34` image. These groupings happen automatically
    within the inventory plugin and may vary according to the OpenStack setup that
    you use. The tail end of the output will show the other groups provided by the
    plugin:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.7 – Demonstrating more data returned by the openstack_inventory.py
    dynamic inventory'
  prefs: []
  type: TYPE_NORMAL
- en: '](Images/B17462_12_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.7 – Demonstrating more data returned by the openstack_inventory.py
    dynamic inventory
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that for the preceding groupings to appear, `expand_hostvars: True` must
    be set in the `openstack.yaml` file.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the additional groups are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`mastery_cloud`: All servers running on our `mastery_cloud` instance, as specified
    in our `clouds.yaml` file'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`flavor-ds1G`: All servers that use the `ds1G` flavor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image-Fedora 29`: All servers that use the `Fedora 29` image'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`instance-875f88bc-ae18-42da-b988-0e4481e35f7e`: A group named after the instance
    itself'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nova`: All servers running under the `nova` service'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many groups provided, each with a potentially different slice of the
    servers found by the inventory script. These groups make it easy to target just
    the right instances with plays. The hosts are defined as the UUIDs of the servers.
    As these are unique by nature, and also quite long, they are unwieldy as a target
    within a play. This makes groups all the more important.
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate using this script as an inventory source, we''ll recreate the
    previous example, skipping over the creation of the server and just writing the
    second play by using an appropriate group target. We''ll name this playbook `configure-server.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The default user of this image is `fedora`; however, that information isn't
    readily available via the OpenStack APIs, and thus, it is not reflected in the
    data that our inventory script provides. We can simply define the user to use
    at the play level.
  prefs: []
  type: TYPE_NORMAL
- en: This time, the host pattern is set to `all`, as we only have one host on our
    demo OpenStack server at this time; however, in real life, it's unlikely that
    you would be so open in your host targeting in Ansible.
  prefs: []
  type: TYPE_NORMAL
- en: 'The rest of the play is unchanged, and the output should look similar to previous
    executions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.8 – Reconfiguring our virtual instance via a dynamic inventory
    plugin'
  prefs: []
  type: TYPE_NORMAL
- en: '](Images/B17462_12_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.8 – Reconfiguring our virtual instance via a dynamic inventory plugin
  prefs: []
  type: TYPE_NORMAL
- en: This output differs from the last time that the `boot-server.yaml` playbook
    was executed in only a few ways. First, the `mastery1` instance is not created
    or booted. We're assuming that the servers we want to interact with have already
    been created and are running. Secondly, we have pulled the inventory for this
    playbook run directly from the OpenStack server itself, using a dynamic inventory
    plugin, rather than creating one in the playbook using `add_host`. Otherwise,
    the output is the same, barring two deprecation warnings. The warning regarding
    group names comes up because the dynamic inventory script provides automatically
    created group names that need to be sanitized – I imagine this will be fixed in
    a future release of the plugin. In addition, the Python deprecation warning is
    common to see right now during this transitionary phase as Ansible moves over
    completely to Python 3, and provided you are not missing any modules from your
    Python 2 environment, is benign.
  prefs: []
  type: TYPE_NORMAL
- en: As servers get added or removed over time, each execution of the inventory plugin
    will discover what servers are there at the moment of playbook execution. This
    can save a significant amount of time spent attempting to maintain an accurate
    list of servers in static inventory files.
  prefs: []
  type: TYPE_NORMAL
- en: Managing a public cloud infrastructure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The management of public cloud infrastructures with Ansible is no more difficult
    than the management of OpenStack with it, as we covered earlier. In general, for
    any IaaS provider supported by Ansible, there is a three-step process to getting
    it working:'
  prefs: []
  type: TYPE_NORMAL
- en: Establish the Ansible collections, modules, and inventory plugins available
    to support the cloud provider.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install any prerequisite software or libraries on the Ansible host.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the playbook and run it against the infrastructure provider.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'There are dynamic inventory plugins readily available for most providers too,
    and we have already demonstrated two in this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '`amazon.aws.aws_ec2` was discussed in [*Chapter 1*](B17462_01_Final_JC_ePub.xhtml#_idTextAnchor015), *The
    System Architecture and Design of Ansible*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`openstack.cloud.openstack` was demonstrated earlier in this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s take a look at **Amazon Web Services** (**AWS**), and specifically,
    the EC2 offering. We can boot up a new server from an image of our choosing, using
    exactly the same high-level process that we did with OpenStack earlier. However,
    as I''m sure you will have guessed by now, we have to use an Ansible module that
    offers specific EC2 support. Let''s build up the playbook. First of all, our initial
    play will once again run from the local host, as this will be making the calls
    to EC2 to boot up our new server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will use the `community.aws.ec2_instance` module in place of the `openstack.cloud.server`
    module to boot up our desired server. This code is really just an example to show
    you how to use the modules; normally, just like with our `openstack.cloud.server`
    example, you would not include the secret keys in the playbook, but would store
    them in a vault somewhere:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: The `community.aws.ec2_instance` module requires the Python `boto3` library
    to be installed on the Ansible host; the method for this will vary between operating
    systems, but on our Ubuntu Server 20.04 demo host, it was installed using the `sudo
    apt install python3-boto3` command. Also, if you are installing this module under
    Python 3, be sure that your Ansible installation uses Python 3 by setting the
    `ANSIBLE_PYTHON_INTERPRETER` variable.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding code is intended to perform the same job as our `openstack.cloud.server`
    example, and while it looks similar at a high level, there are many differences.
    Hence, it is essential to read the module documentation whenever working with
    a new module, in order to understand precisely how to use it. Of specific interest,
    do note that the `user_data` field can be used to send post-creation scripts to
    the new VM; this is incredibly useful when the initial configuration is needed
    immediately, lending itself to `ansible.builtin.raw` commands. In this case, we
    use it to install the Python 3 prerequisites required to install `ImageMagick`
    with Ansible later on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can obtain the public IP address of our newly created server by using
    the `newserver` variable that we registered in the preceding task. However, note
    the different variable structure, as compared to the way that we accessed this
    information when using the `openstack.cloud.server` module (again, always refer
    to the documentation):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Another key difference between the `community.aws.ec2_instance` module and
    the `openstack.cloud.server` one is that `community.aws.ec2_instance` does not
    necessarily wait for SSH connectivity to become available before completing –
    this can be set using the `wait` parameter; thus, it is good practice to define
    a task specifically for this purpose to ensure that our playbook doesn''t fail
    later on due to a lack of connectivity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Once this task has been completed, we will know that our host is alive and
    responding to SSH, so we can proceed to use `ansible.builtin.add_host` to add
    this new host to the inventory, and then install `ImageMagick` just like we did
    before (the image used here is the same Fedora 34 cloud-based image used in the
    OpenStack example):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Putting all of this together and running the playbook should result in something
    like the following screenshot. Note that I have turned SSH host key checking off,
    to prevent the SSH transport agent from asking about adding the host key on the
    first run, which would cause the playbook to hang and wait for user intervention,
    using this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'You will also note that I have saved my private SSH key from the keypair I
    generated on my AWS account as `mastery-key.pem` in the same directory as the
    playbook – you will need to save your own key in this location and reference it
    in the command line accordingly. A successful run should look something like the
    output shown in *Figure 12.9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.9 – Booting and setting up an Amazon EC2 instance using Ansible'
  prefs: []
  type: TYPE_NORMAL
- en: '](Images/B17462_12_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.9 – Booting and setting up an Amazon EC2 instance using Ansible
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen here, we can achieve the same result on a different cloud provider,
    using only a subtly different playbook. The key here is to read the documentation
    that comes with each module and ensure that both the parameters and return values
    are correctly referenced.
  prefs: []
  type: TYPE_NORMAL
- en: 'We could apply this methodology to Azure, Google Cloud, or any of the other
    cloud providers that Ansible ships with support for. If we wanted to repeat this
    example on Azure, then we would need to use the `azure.azcollection.azure_rm_virtualmachine`
    module. The documentation for this module states that we need Python 2.7 or newer
    (this is already a part of our Ubuntu Server 20.04 demo machine), and a whole
    suite of Python modules, the names of which along with required versions can be
    found in a file named `requirements-azure.txt`, which is included with the collection.
    The expectation is that you will install these requirements with `pip`, and you
    can do this by locating the aforementioned file on your filesystem and then installing
    the required modules. On my demo system, I achieved this with the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'With these prerequisites satisfied, we can build up our playbook again. Note
    that with Azure, multiple authentication methods are possible. For the sake of
    simplicity, I am using the Azure Active Directory credentials that I created for
    this demo; however, to enable this, I had to also install the official Azure CLI
    utility (following the instructions available here: [https://docs.microsoft.com/en-gb/cli/azure/install-azure-cli-linux?pivots=apt](https://docs.microsoft.com/en-gb/cli/azure/install-azure-cli-linux?pivots=apt)),
    and log in using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This ensures that your Ansible host is trusted by Azure. In practice, you would
    set up a **service principal** that removes the need for this, and you are encouraged
    to explore this option by yourself. To continue with the current simple example,
    we set up the header of our playbook like before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that this time, we will store a password for our new VM in a variable;
    normally, we would do this in a vault, but that is again left as an exercise for
    the reader. From here, we use the `azure.azcollection.azure_rm_virtualmachine`
    module to boot up our new VM. To make use of a `Fedora 34` image for continuity
    with the previous examples, I''ve had to go to the image marketplace on Azure,
    which requires some additional parameters, such as `plan`, to be defined. To enable
    the use of this image with Ansible, I first had to find it, then accept the terms
    of the author to enable its use, using the `az` command-line utility with these
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'I also had to create the resource group and network that the VM would use;
    these are very much Azure-specific steps and are well documented (and considered
    *bread and butter* if you are familiar with Azure). Once all of the prerequisites
    were completed, I was then able to write the following playbook code to boot up
    our Azure-based `Fedora 34` image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'As with the previous examples, we obtain the public IP address of our image
    (note the complex variable required to access this), ensure that SSH access is
    working, and then use `ansible.builtin.add_host` to add the new VM to our runtime
    inventory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Azure allows for either password- or key-based authentication for SSH on Linux
    VMs; we''re using password-based here for simplicity. Also, note the newly utilized
    `ansible_become_pass` connection variable, as the `Fedora 34` image that we are
    using will prompt for a password when `sudo` is used, potentially blocking execution.
    Finally, with this work complete, we install `ImageMagick`, like before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'With the code complete, run it with the following commands (setting your Python
    environment as necessary for your system):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at this in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.10 – Creating and configuring an Azure virtual machine using Ansible'
  prefs: []
  type: TYPE_NORMAL
- en: '](Images/B17462_12_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.10 – Creating and configuring an Azure virtual machine using Ansible
  prefs: []
  type: TYPE_NORMAL
- en: The output is very similar to our AWS example, demonstrating that we can very
    easily perform the same actions across different cloud platforms with just a little
    effort in terms of learning how the various modules that each cloud provider needs
    works. This section of the chapter is by no means definitive, given the number
    of platforms and operations supported by Ansible, but we hope that the information
    provided gives an idea of the process and steps required for getting Ansible to
    integrate with a new cloud platform. Next, we will look at using Ansible to interact
    with Docker containers.
  prefs: []
  type: TYPE_NORMAL
- en: Interacting with Docker containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linux container technologies, especially Docker, have grown in popularity in
    recent years, and this has continued since the previous edition of this book was
    published. Containers provide a fast path to resource isolation while maintaining
    the consistency of the runtime environment. They can be launched quickly and are
    efficient to run, as there is very little overhead involved. Utilities such as
    Docker provide a lot of useful tooling for container management, such as a registry
    of images to use as the filesystem, tooling to build the images themselves, clustering
    orchestration, and so on. Through its ease of use, Docker has become one of the
    most popular ways to manage containers, though others, such as Podman and LXC,
    are becoming much more prevalent. For now, though, we will focus on Docker, given
    its broad appeal and wide install base.
  prefs: []
  type: TYPE_NORMAL
- en: Ansible can interact with Docker in numerous ways as well. Notably, Ansible
    can be used to build images, to start or stop containers, to compose multiple
    container services, to connect to and interact with active containers, and even
    to discover inventory from containers. Ansible provides a full suite of tools
    for working with Docker, including relevant modules, a connection plugin, and
    an inventory script.
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrate working with Docker, we'll explore a few use cases. The first
    use case is building a new image to use with Docker. The second use case is launching
    a container from the new image and interacting with it. The last use case is using
    the inventory plugin to interact with an active container.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Creating a functional Docker installation is very much dependent on your underlying
    operating system. A great resource to start with is the Docker website, which
    provides detailed installation and usage instructions, at [https://docs.docker.com](https://docs.docker.com). Ansible
    works best with Docker on a Linux host, so we will continue with the Ubuntu Server
    20.04 LTS demo machine that we have used throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: Building images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Docker images are basically filesystems bundled with parameters to use at runtime.
    The filesystem is usually a small part of a Linux Userland, with enough files
    to start the desired process. Docker provides tooling to build these images, generally
    based on very small, preexisting base images. The tooling uses a Dockerfile as
    the input, which is a plain text file with directives. This file is parsed by
    the `docker build` command, and we can parse it via the `docker_image` module.
    The remaining examples will be from an Ubuntu Server 20.04 virtual machine using
    Docker CE version 20.10.8, with the `cowsay` and `nginx` packages added so that
    running the container will provide a web server that will display something from `cowsay`.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll need a Dockerfile. If you''ve not come across one of these before,
    they are a set of instructions used for building Docker containers – you can learn
    more about this here if you wish: [https://docs.docker.com/engine/reference/builder/](https://docs.docker.com/engine/reference/builder/).
    This file needs to live in a path that Ansible can read, and we''re going to put
    it in the same directory as my playbooks. The Dockerfile content will be very
    simple. We''ll need to define a base image, a command to run to install the necessary
    software, some minimal configuration of software, a port to expose, and a default
    action for running a container with this image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The build process performs the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: We're using the `Fedora 34` image from the `fedora` repository on the Docker
    Hub image registry.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To install the necessary `cowsay` and `nginx` packages, we're using `dnf`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To run `nginx` directly in the container, we need to turn `daemon` mode `off` in `nginx.conf`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We use `cowsay` to generate content for the default web page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we're instructing Docker to expose port `80` in the container, where `nginx` will
    listen for connections.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, the default action of this container will be to run `nginx`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The playbook to build and use the image can live in the same directory. We''ll
    name it `docker-interact.yaml`. This playbook will operate on `localhost` and
    will have two tasks; one will be to build the image using `community.docker.docker_image`,
    and the other will be to launch the container using `community.docker.docker_container`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we run our playbook, we''ll check for any possible container images,
    or running containers that might match our preceding playbook definitions – this
    will help us to have confidence that our code is producing the desired results.
    If you have any additional containers running from previous tests, you can run
    the following commands to check for `fedora`-based containers that match our specification
    by running these commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Unless you have run this code before, you should see that no containers are
    running, as shown in *Figure 12.11*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.11 – Checking for the absence of containers before running our
    playbook'
  prefs: []
  type: TYPE_NORMAL
- en: '](Images/B17462_12_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.11 – Checking for the absence of containers before running our playbook
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s run the playbook to build the image and start a container using
    that image – note that, as is common with many other Ansible modules, you might
    have to install additional Python modules for your code to work. On my Ubuntu
    Server 20.04 demo machine, I had to run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'With the Python support installed, you can then run the playbook with this
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'A successful playbook run should look similar to *Figure 12.12*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.12 – Building and running our first Docker container using Ansible'
  prefs: []
  type: TYPE_NORMAL
- en: '](Images/B17462_12_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.12 – Building and running our first Docker container using Ansible
  prefs: []
  type: TYPE_NORMAL
- en: 'The verbosity of this playbook execution was reduced to save screen space.
    Our output simply shows that the task to build the image resulted in a change,
    as did the task to start the container. A quick check of running containers and
    available images should reflect our work – you can use the same `docker` commands
    as we used before the playbook run to validate this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.13 – Verifying the results of our Ansible playbook run in Docker'
  prefs: []
  type: TYPE_NORMAL
- en: '](Images/B17462_12_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.13 – Verifying the results of our Ansible playbook run in Docker
  prefs: []
  type: TYPE_NORMAL
- en: 'We can test the functionality of our container by using `curl` to access the
    web server, which should show us a cow saying `boop,` as demonstrated in *Figure
    12.14*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.14 – Retrieving the results of our container created and run with
    Ansible'
  prefs: []
  type: TYPE_NORMAL
- en: '](Images/B17462_12_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.14 – Retrieving the results of our container created and run with
    Ansible
  prefs: []
  type: TYPE_NORMAL
- en: In this manner, we have already shown how easy it is to interact with Docker
    using Ansible. However, this example is still based on using a native Dockerfile,
    and, as we progress through this chapter, we'll see some more advanced Ansible
    usage that removes the need for this.
  prefs: []
  type: TYPE_NORMAL
- en: Building containers without a Dockerfile
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Dockerfiles are useful, but many of the actions performed inside of Dockerfiles
    could be completed with Ansible instead. Ansible can be used to launch a container
    using a base image, then interact with that container using the `docker` connection
    method (as opposed to SSH) to complete the configuration. Let''s demonstrate this
    by repeating the previous example, but without the need for a Dockerfile. Instead,
    all of the work will be handled by an entirely new playbook named `docker-all.yaml`.
    The first part of this playbook starts a container from a preexisting image of `Fedora
    34` from Docker Hub and adds the resulting container details to Ansible''s in-memory
    inventory by using `ansible.builtin.add_host`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, using this newly added inventory host, we define a second play that runs
    Ansible tasks within the container that was just launched, configuring our `cowsay` service
    like before, but without the need for a Dockerfile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: To recap, the playbook consists of two plays. The first play creates the container
    from the base `Fedora 34` image. The `community.docker.docker_container` task
    is given a `sleep` command to keep the container running for a period of time,
    as the `docker` connection plugin only works with active containers (unconfigured
    operating system images from Docker Hub generally exit immediately when they are
    run, as they have no default actions to perform). The second task of the first
    play creates a runtime inventory entry for the container. The inventory hostname
    must match the container name. The connection method is set to `docker` as well.
  prefs: []
  type: TYPE_NORMAL
- en: The second play targets the newly created host, and the first task uses the `ansible.builtin.raw` module
    to get the `python-dnf` package in place (which will bring the rest of `Python` in),
    so that we can use the `ansible.builtin.dnf` module in the next task. The `ansible.builtin.dnf`
    module is then used to install the desired packages, namely, `nginx` and `cowsay`.
    Then, the `ansible.builtin.lineinfile` module is used to add a new line to the `nginx` configuration.
    An `ansible.builtin.shell` task uses `cowsay` to create content for `nginx` to
    serve. Finally, `nginx` itself is started as a background process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before running the playbook, let''s remove any running containers from the
    previous example by running the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'You can verify this against the screenshot in *Figure 12.15*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.15 – Cleaning up running containers from our previous playbook
    run'
  prefs: []
  type: TYPE_NORMAL
- en: '](Images/B17462_12_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.15 – Cleaning up running containers from our previous playbook run
  prefs: []
  type: TYPE_NORMAL
- en: 'With the running container removed, we can now run our new playbook to recreate
    the container, bypassing the image build step, using this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of a successful run should look like that shown in *Figure 12.16*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.16 – Building a container without a Dockerfile using Ansible'
  prefs: []
  type: TYPE_NORMAL
- en: '](Images/B17462_12_16.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.16 – Building a container without a Dockerfile using Ansible
  prefs: []
  type: TYPE_NORMAL
- en: 'We see tasks from the first play execute on the `localhost`, and then the second
    play executes on the `playbook-container`. Once it''s complete, we can test the
    web service and list the running containers to verify our work using these commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Note the different filter this time; our container was built and run directly
    from the `fedora` image, without the intermediate step of creating the `fedora-moo` image
    – the output should look like that shown in *Figure 12.17*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.17 – Verifying the results of our playbook run'
  prefs: []
  type: TYPE_NORMAL
- en: '](Images/B17462_12_17.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.17 – Verifying the results of our playbook run
  prefs: []
  type: TYPE_NORMAL
- en: This method of using Ansible to configure the running container has some advantages.
    First, you can reuse existing roles to set up an application, easily switching
    from cloud virtual machine targets to containers, and even to bare-metal resources,
    if desired. Secondly, you can easily review all configuration that goes into an
    application, simply by reviewing the playbook content.
  prefs: []
  type: TYPE_NORMAL
- en: Another use case for this method of interaction is to use Docker containers
    to simulate multiple hosts, in order to verify playbook execution across multiple
    hosts. A container can be started with an `init` system as the running process,
    allowing for additional services to be started as if they were on a full operating
    system. This use case is valuable within a continuous integration environment,
    to validate changes to playbook content quickly and efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Docker inventory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to the OpenStack and EC2 inventory plugins detailed earlier in this
    book, a Docker inventory plugin is also available. You can locate the Docker inventory
    script if you wish to examine it or use it in a similar manner to the way in which
    we have used other dynamic inventory plugins earlier in this chapter by creating
    a YAML inventory file to reference the plugin.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by locating the inventory script itself – on my demo system, it
    is located here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you get used to the installation base path of Ansible, you will note that
    it is quite easy to navigate the directory structure through the collections to
    find what you are looking for. Let''s try running this script directly to see
    the options available to us when configuring it for playbook inventory purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The `help` output for the script shows many possible arguments; however, the
    ones that Ansible will use are `--list` and `--host` – your output will look similar
    to that shown in *Figure 12.18*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.18 – Examining the options available on the Docker dynamic inventory
    script'
  prefs: []
  type: TYPE_NORMAL
- en: '](Images/B17462_12_18.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.18 – Examining the options available on the Docker dynamic inventory
    script
  prefs: []
  type: TYPE_NORMAL
- en: 'If the previously built container is still running when this script is executed,
    you can list hosts using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'It should appear in the output (`grep` has been used to make this more obvious
    in the screenshot):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.19 – Running the Docker dynamic inventory plugin manually to explore
    its behavior'
  prefs: []
  type: TYPE_NORMAL
- en: '](Images/B17462_12_19.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.19 – Running the Docker dynamic inventory plugin manually to explore
    its behavior
  prefs: []
  type: TYPE_NORMAL
- en: 'Like earlier, a number of groups are presented, which have the running container
    as a member. The two groups that were shown earlier are the short container ID
    and the long container ID. Many variables are also defined as a part of the output,
    which has been heavily truncated in the preceding screenshot. The tail end of
    the output reveals a few more groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.20 – Further exploring the output of the dynamic inventory script
    output'
  prefs: []
  type: TYPE_NORMAL
- en: '](Images/B17462_12_20.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.20 – Further exploring the output of the dynamic inventory script
    output
  prefs: []
  type: TYPE_NORMAL
- en: 'The additional groups are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`docker_hosts`: All of the hosts running the Docker daemon that the dynamic
    inventory script has communicated with and queried for containers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_name`: A group for each image used by discovered containers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`container name`: A group that matches the name of the container'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`running`: A group of all the running containers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stopped`: A group of all the stopped containers – you can see in the preceding
    output that our container started previously has now stopped, as the 500-second
    sleep period has expired.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This inventory plugin, and the groups and data provided by it, can be used
    by playbooks to target various selections of containers available, in order to
    interact without the need for manual inventory management or the use of `add_host`.
    Using the plugin in a playbook is a simple matter of defining a YAML inventory
    file with the plugin name and connection details – to query the local Docker host,
    we could define our inventory as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'You can run ad hoc commands or playbooks with Ansible against this inventory
    definition in the normal manner and get details of all of the containers running
    on the local host. Connecting to remote hosts is not significantly more difficult,
    and the plugin documentation (available here: [https://docs.ansible.com/ansible/latest/collections/community/docker/docker_containers_inventory.html](https://docs.ansible.com/ansible/latest/collections/community/docker/docker_containers_inventory.html))
    shows you the options available to you for this. We''ve now looked at several
    methods for building and interacting with Docker containers, but what if we wanted
    a more joined-up approach? We''ll look at exactly this in the next section.'
  prefs: []
  type: TYPE_NORMAL
- en: Building containers with Ansible
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we mentioned at the beginning of the previous section, the world of containers
    has moved on greatly since the previous edition of this book was published. Although
    Docker is still a massively popular container technology, new and improved technologies
    have become favored, and indeed natively integrated into Linux operating systems.
    Canonical (the publisher of Ubuntu) is championing the **LXC** container environment,
    while Red Hat (the owner of Ansible) is championing **Buildah** and **Podman**.
  prefs: []
  type: TYPE_NORMAL
- en: If you read the third edition of this book, you will know that we covered a
    technology called **Ansible Container**, which was used to directly integrate
    Ansible with Docker and remove the need for *glue* steps such as adding hosts
    to the in-memory inventory, having two separate plays for instantiating the container,
    and building the container image contents. Ansible Container has now been deprecated,
    and all development work has ceased (according to their GitHub page – see [https://github.com/ansible/ansible-container](https://github.com/ansible/ansible-container)
    if you are interested).
  prefs: []
  type: TYPE_NORMAL
- en: Ansible Container has been succeeded by a new tool called **ansible-bender**,
    which features a pluggable architecture for different container build environments.
    At this early stage in its development, it only supports **Buildah**, but hopefully,
    further container technologies will be supported in the near future.
  prefs: []
  type: TYPE_NORMAL
- en: The Podman/Buildah toolsets are available on newer releases of Red Hat Enterprise
    Linux, CentOS, Fedora, and Ubuntu Server (but not 20.04, unless you go for a more
    bleeding edge version). As we have used Ubuntu Server for our demo machine throughout
    this book, we will stick to this operating system, but for this section of the
    chapter, we will switch to version 20.10 which, whilst not an LTS release, does
    have a native release of Buildah and Podman available.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install Buildah and Podman on Ubuntu Server 20.10 (and newer), simply run
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you have your container environment installed (don''t forget to install
    Ansible if you haven''t already – `ansible-bender` needs this to run!), you can
    install `ansible-bender` using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: That's it – now you're all ready to go! It is worth noting before we dive into
    our example code that `ansible-bender` is a lot simpler in its functionality than
    Ansible Container was. While Ansible Container could manage the entire lifecycle
    of a container, `ansible-bender` is only concerned with the build phase of containers
    – nonetheless, it provides a useful abstraction layer for easily building your
    container images using Ansible, and once it supports other containerization build
    platforms (such as LXC and/or Docker), it will become an incredibly valuable tool
    in your automation arsenal, as you will be able to build container images on a
    variety of platforms using almost identical playbook code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s build our first playbook for `ansible-bender`. The header of the play
    will look, by now, familiar – with one important exception. Notice the `vars:`
    section in the play definition – this section contains important reserved variables
    for use by `ansible-bender` and defines items such as the source container image
    (we''ll use `Fedora 34` once again), and the destination container image details,
    including the command to run when the container starts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'With this defined, we write our play tasks in exactly the same way as we did
    before. Notice that we don''t need to worry about inventory definition (either
    through a dynamic inventory provider or via `ansible.builtin.add_host`) – `ansible-bender`
    runs all our tasks on the container image it instantiates using the details from
    the `ansible_bender` variable structure. Thus, our code should look like this
    – it is identical to the second play we used before, only we''re not running the
    final `ansible.builtin.shell` task to start the `nginx` web server, as this is
    taken care of by details in the `ansible_bender` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'That''s it – the code is no more complex than that! Now, building your first
    container with `ansible-bender` is as simple as running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Note that the command must be run as root (that is, via `sudo`) – this is a
    specific related to Buildah and Podman and their behavior when run as an unprivileged
    user.
  prefs: []
  type: TYPE_NORMAL
- en: 'One oddity of `ansible-bender` is that when it starts to run, you will see
    some lines that state `ERROR` (see *Figure 12.21*). This is a bug in `ansible-bender`
    as these lines are not actually errors – they are simply information being returned
    from the Buildah tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.21 – Starting the container build process with ansible-bender,
    and the false ERROR messages'
  prefs: []
  type: TYPE_NORMAL
- en: '](Images/B17462_12_21.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.21 – Starting the container build process with ansible-bender, and
    the false ERROR messages
  prefs: []
  type: TYPE_NORMAL
- en: 'As the build continues, you should see Ansible playbook messages return in
    the manner with which you are now familiar. At the end of the process, you should
    have a successful build indicated by output like that shown in *Figure 12.22*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.22 – A successful container build with ansible-bender'
  prefs: []
  type: TYPE_NORMAL
- en: '](Images/B17462_12_22.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.22 – A successful container build with ansible-bender
  prefs: []
  type: TYPE_NORMAL
- en: 'From here, you can run your newly built container with this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The `fedora-moo` container name was set in the `ansible_bender` variable structure
    in the playbook file previously, whilst the `-d` flag is used to detach from the
    container and run it in the background. Similar to Docker, you can query the running
    containers on your system with this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The output from this process will look somewhat like that shown in *Figure
    12.23*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.23 – Running and querying our newly built container in Podman'
  prefs: []
  type: TYPE_NORMAL
- en: '](Images/B17462_12_23.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.23 – Running and querying our newly built container in Podman
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s see if we can actually retrieve our `cowsay` web page from
    our container. Unlike our Docker example, we have not instructed Podman to redirect
    the web server port to a port on our build machine, so we will need to query the
    IP address of the container itself. Having obtained the `CONTAINER ID` or `NAMES`
    from the output of `sudo podman ps`, we can query this with a command such as
    this (be sure to replace the container ID with the one from your system):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'As with Docker, you can abbreviate your container ID provided the characters
    you enter are unique in the list of running containers. Once you have retrieved
    the IP address, you can use `curl` to download the web page, just as we did before
    – for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'This whole process should look like that shown in *Figure 12.24*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.24 – Downloading our cowsay web page from our Podman container
    build with ansible-bender'
  prefs: []
  type: TYPE_NORMAL
- en: '](Images/B17462_12_24.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.24 – Downloading our cowsay web page from our Podman container build
    with ansible-bender
  prefs: []
  type: TYPE_NORMAL
- en: That's all there is to it! The `ansible-bender` tool shows great promise in
    providing an automation framework for building container images with one common
    language – our own favorite, Ansible! As the tool develops, hopefully, some of
    the rough edges (such as the false `ERROR` statements) will be resolved, and the
    addition of support for more container platforms will truly make this a valuable
    automation tool for container images. That concludes our look at infrastructure
    provisioning with Ansible – hopefully, you have found it valuable.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DevOps has pushed automation in many new directions, including the containerization
    of applications, and even the creation of infrastructure itself. Cloud computing
    services enable self-service management of fleets of servers for running services.
    Ansible can easily interact with these services to provide the automation and
    orchestration engine.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you learned how to manage on-premises cloud infrastructures,
    such as OpenStack, using Ansible. We then extended this with examples of public
    cloud infrastructure provision on both AWS and Microsoft Azure. Finally, you learned
    how to interact with Docker using Ansible, and how to neatly package Docker service
    definitions using Ansible Container.
  prefs: []
  type: TYPE_NORMAL
- en: Ansible can start just about any host, except for the one that it is running
    on, and with proper credentials, it can create the infrastructure that it wants
    to manage, either for one-off actions or to deploy a new version of an application
    into a production container management system. The end result is that once your
    hardware is in place and your service providers are configured, you can manage
    your entire infrastructure through Ansible, if you so desire!
  prefs: []
  type: TYPE_NORMAL
- en: 'In the final chapter of this book, we will look at a new and rapidly growing
    area of automation: network provisioning with Ansible.'
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When creating or deleting VM instances on OpenStack, which inventory host should
    you reference in your play?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) The OpenStack host
  prefs: []
  type: TYPE_NORMAL
- en: b) localhost
  prefs: []
  type: TYPE_NORMAL
- en: c) The VM Floating IP address
  prefs: []
  type: TYPE_NORMAL
- en: d) None of the above
  prefs: []
  type: TYPE_NORMAL
- en: How would you reference a newly created virtual machine in a second play without
    having to use a dynamic inventory script?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Use `ansible.builtin.raw` commands.
  prefs: []
  type: TYPE_NORMAL
- en: b) Use `ansible.builtin.shell` commands.
  prefs: []
  type: TYPE_NORMAL
- en: c) Use `ansible.builtin.add_host` to add the new VM to the in-memory inventory.
  prefs: []
  type: TYPE_NORMAL
- en: d) You need to use the dynamic inventory plugin.
  prefs: []
  type: TYPE_NORMAL
- en: You can still run dynamic inventory scripts directly in Ansible 4.x and newer,
    just as you could in Ansible 2.x releases.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) True
  prefs: []
  type: TYPE_NORMAL
- en: b) False
  prefs: []
  type: TYPE_NORMAL
- en: 'To use a dynamic inventory script, and set its parameters, you would now (assuming
    the collection is already installed):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Define a YAML inventory file with the plugin name and parameters.
  prefs: []
  type: TYPE_NORMAL
- en: b) Reference the dynamic inventory script in the `-i` parameter of `ansible`/`ansible-playbook`.
  prefs: []
  type: TYPE_NORMAL
- en: c) Put the plugin name in your play definition itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'When using a new module from a collection for the first time (for example,
    with a cloud provider), you should:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Always read the documentation to check for known issues.
  prefs: []
  type: TYPE_NORMAL
- en: b) Always read the documentation to see if you need to install additional Python
    modules.
  prefs: []
  type: TYPE_NORMAL
- en: c) Always read the documentation see how you should define your authentication
    parameters.
  prefs: []
  type: TYPE_NORMAL
- en: d) All of the above.
  prefs: []
  type: TYPE_NORMAL
- en: Ansible cannot function on a target host if there is no Python environment (this
    is sometimes the case on minimal cloud operating system images). If this is the
    case, you can still install Python from a playbook task with which module?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) `ansible.builtin.python`
  prefs: []
  type: TYPE_NORMAL
- en: b) `ansible.builtin.raw`
  prefs: []
  type: TYPE_NORMAL
- en: c) `ansible.builtin.command`
  prefs: []
  type: TYPE_NORMAL
- en: d) `ansible.builtin.shell`
  prefs: []
  type: TYPE_NORMAL
- en: All cloud provider modules will wait for a VM instance to come up before the
    play is allowed to move on to the next task.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) True
  prefs: []
  type: TYPE_NORMAL
- en: b) False
  prefs: []
  type: TYPE_NORMAL
- en: If you want to wait to ensure a host is accessible over SSH before you perform
    additional tasks, you can use which module?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) `ansible.builtin.wait_for`
  prefs: []
  type: TYPE_NORMAL
- en: b) `ansible.builtin.ssh`
  prefs: []
  type: TYPE_NORMAL
- en: c) `ansible.builtin.test_connection`
  prefs: []
  type: TYPE_NORMAL
- en: d) `ansible.builtin.connect`
  prefs: []
  type: TYPE_NORMAL
- en: Ansible can build Docker containers both with and without a Dockerfile.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) True
  prefs: []
  type: TYPE_NORMAL
- en: b) False
  prefs: []
  type: TYPE_NORMAL
- en: The `ansible-bender` tool currently supports which build environment?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Docker
  prefs: []
  type: TYPE_NORMAL
- en: b) LXC
  prefs: []
  type: TYPE_NORMAL
- en: c) Podman/Buildah
  prefs: []
  type: TYPE_NORMAL
- en: d) All of the above
  prefs: []
  type: TYPE_NORMAL
