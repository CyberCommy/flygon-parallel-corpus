- en: Storm Integration with Redis, Elasticsearch, and HBase
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we covered an overview of Apache Hadoop and its various
    components. We also presented an overview of Storm-YARN, and looked at deploying
    Storm-YARN on Apache Hadoop.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will explain how you can integrate Storm with other databases
    for storing the data, and how we can use Esper inside a Storm bolt to support
    the windowing operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the key points we are going to cover in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Integration of Storm with HBase
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration of Storm with Redis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration of Storm with Elasticsearch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration of Storm with Esper to perform the windowing operation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating Storm with HBase
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As explained in earlier chapters, Storm is meant for real-time data processing.
    However, in most cases, you will need to store the processed data in a data store
    so that you can use the stored data for further batch analysis and execute the
    batch analysis query on the data stored. This section explains how you can store
    the data processed by Storm in HBase.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before going to the implementation, I want to give a little overview of what
    HBase is. HBase is a NoSQL, multidimensional, sparse, horizontally scalable database that
    is modeled after **Google** **BigTable**. HBase is built on top of Hadoop, which
    means it relies on Hadoop and integrates with the MapReduce framework very well.
    Hadoop provides the following benefits to HBase:'
  prefs: []
  type: TYPE_NORMAL
- en: A distributed data store that runs on top of the commodity hardware
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fault tolerance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will assume that you have HBase installed and running on your system. You
    can refer to the article on HBase installation at [https://hbase.apache.org/cygwin.html](https://hbase.apache.org/cygwin.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will create a sample Storm topology that shows how you can store the data
    processed by Storm to HBase using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a Maven project using `com.stormadvance` for the group ID and `stormhbase`
    for the artifact ID.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add the following dependencies and repositories to the `pom.xml` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Create an `HBaseOperations` class in the `com.stormadvance.stormhbase` package.
    The `HBaseOperations` class contains two methods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`createTable(String tableName, List<String> ColumnFamilies)`: This method takes
    the name of the table and the HBase column family list as input to create a table
    in HBase.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`insert(Map<String, Map<String, Object>> record, String rowId)`: This method
    takes the record and its `rowID` parameter as input and inserts the input record
    to HBase. The following is the structure of the input record:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here, `columnfamily1` and `columnfamily2` are the names of HBase column families,
    and `column1`, `column2`, `column3`, and `column4` are the names of columns.
  prefs: []
  type: TYPE_NORMAL
- en: The `rowId` parameter is the HBase table row key that is used to uniquely identify
    each record in HBase.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the source code of the `HBaseOperations` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `SampleSpout` class in the `com.stormadvance.stormhbase` package.
    This class generates random records and passes them to the next action (bolt)
    in the topology. The following is the format of the record generated by the `SampleSpout`
    class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the source code of the `SampleSpout` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `StormHBaseBolt` class in the `com.stormadvance.stormhbase` package.
    This bolt receives the tuples emitted by `SampleSpout` and then calls the `insert()`
    method of the `HBaseOperations` class to insert the record into HBase. The following
    is the source code of the `StormHBaseBolt` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The constructor of the `StormHBaseBolt` class takes the HBase table name, column
    families list, ZooKeeper IP address, and ZooKeeper port as an argument and sets
    the class level variables. The `prepare()` method of the `StormHBaseBolt` class
    will create an instance of the `HBaseOperatons` class.
  prefs: []
  type: TYPE_NORMAL
- en: The `execute()` method of the `StormHBaseBolt` class takes an input tuple as
    an argument and converts it into the HBase structure format. It also uses the
    `java.util.UUID` class to generate the HBase row ID.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a `Topology` class in the `com.stormadvance.stormhbase` package. This
    class creates an instance of the spout and bolt classes and chains them together
    using a `TopologyBuilder` class. The following is the implementation of the main
    class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we covered how you can integrate Storm with a NoSQL database,
    HBase. In the next section, we are going to cover the integration of Storm with
    Redis.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating Storm with Redis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Redis is a key value data store. The key values can be strings, lists, sets,
    hashes, and so on. It is extremely fast because the entire dataset is stored in
    the memory. The following are the steps to install Redis:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you will need to install `make`, `gcc`, and `cc` to compile the Redis
    code using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Download, unpack, and make Redis, and copy it to `/usr/local/bin` using the
    following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following commands to make Redis a service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, run the following commands to add the service to `chkconfig`, set it to
    autostart, and actually start the service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Check the installation of Redis with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: If the result of the test command is `PONG`, then the installation has been
    successful.
  prefs: []
  type: TYPE_NORMAL
- en: We will assume that you have the Redis service up and running.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will create a sample Storm topology that will explain how you can store
    the data processed by Storm in Redis.
  prefs: []
  type: TYPE_NORMAL
- en: Create a Maven project using `com.stormadvance` for the `groupID` and `stormredis`
    for the `artifactID`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add the following dependencies and repositories in the `pom.xml` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `RedisOperations` class in the `com.stormadvance.stormredis` package.
    The `RedisOperations` class contains the following method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`insert(Map<String, Object> record, String id)`: This method takes the record
    and ID as input and inserts the input record in Redis. In the `insert()` method,
    we will first serialize the record into a string using the Jackson library and
    then store the serialized record into Redis. Each record must have a unique ID
    because it is used to retrieve the record from Redis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is the source code of the `RedisOperations` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We will use the same `SampleSpout` class created in the *Integrating Storm with
    HBase* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a `StormRedisBolt` class in the `com.stormadvance.stormredis` package.
    This bolt receives the tuples emitted by the `SampleSpout` class, converts them
    to the Redis structure, and then calls the `insert()` method of the `RedisOperations`
    class to insert the record into Redis. The following is the source code of the
    `StormRedisBolt` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In the `StormRedisBolt` class, we use the `java.util.UUID` class to generate
    the Redis key.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a `Topology` class in the `com.stormadvance.stormredis` package. This
    class creates an instance of the `spout` and `bolt` classes and chains them together
    using a `TopologyBuilder` class. The following is the implementation of the main
    class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we covered the installation of Redis and how we can integrate
    Storm with Redis.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating Storm with Elasticsearch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to cover the installation of Storm with Elasticsearch.
    Elasticsearch is an open source, distributed search engine platform developed
    on Lucene. It provides a multitenant-capable, full-text search engine capability.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are assuming that Elasticsearch is running on your environment. Please refer
    to [https://www.elastic.co/guide/en/elasticsearch/reference/2.3/_installation.html](https://www.elastic.co/guide/en/elasticsearch/reference/2.3/_installation.html)
    to install Elasticsearch on any of the boxes if you don''t have any running Elasticsearch
    cluster. Go through the following steps to integrate Storm with Elasticsearch:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a Maven project using `com.stormadvance` for the `groupID` and `storm_elasticsearch`
    for the `artifactID`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add the following dependencies and repositories to the `pom.xml` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Create an `ElasticSearchOperation` class in the `com.stormadvance.storm_elasticsearch`
    package. The `ElasticSearchOperation` class contains the following method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`insert(Map<String, Object> data, String indexName, String indexMapping, String
    indexId)`: This method takes the record data, `indexName`, `indexMapping`, and
    `indexId` as input, and inserts the input record in Elasticsearch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is the source code of the `ElasticSearchOperation` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We will use the same `SampleSpout` class created in the *Integrating Storm with
    HBas*e section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create an `ESBolt` class in the `com.stormadvance.storm_elasticsearch` package.
    This bolt receives the tuples emitted by the `SampleSpout` class, converts it
    to the `Map` structure, and then calls the `insert()` method of the `ElasticSearchOperation`
    class to insert the record into Elasticsearch. The following is the source code
    of the `ESBolt` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Create an `ESTopology` class in the `com.stormadvance.storm_elasticsearch`
    package. This class creates an instance of the `spout` and `bolt` classes and
    chains them together using a `TopologyBuilder` class. The following is the implementation
    of the main class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we covered how we can store the data into Elasticsearch by
    making the connection with Elasticsearch nodes inside the Storm bolts.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating Storm with Esper
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to cover how we can use the windowing operation
    inside Storm by using Esper. Esper is an open source event series analysis and
    event correlation engine for **complex event processing** (**CEP**).
  prefs: []
  type: TYPE_NORMAL
- en: 'Please refer to [http://www.espertech.com/products/esper.php](http://www.espertech.com/products/esper.php)
    to read more details about Esper. Go through the following steps to integrate
    Storm with Esper:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a Maven project using `com.stormadvance` for the `groupID` and `storm_esper`
    for the `artifactID`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add the following dependencies and repositories in the `pom.xml` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Create an `EsperOperation` class in the `com.stormadvance.storm_elasticsearch`
    package. The `EsperOperation` class contains the following method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`esperPut(Stock stock)`: This method takes the stock bean as an input and sends
    the event to the Esper listener.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The constructor of the `EsperOperation` class initializes the Esper listener
    and sets the Esper query. The Esper query buffers the events over 5 minutes and
    returns the total sales of each product during the 5 minutes window. Here, we
    are using the fixed batch window.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the source code of the `EsperOperation` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `SampleSpout` class in the `com.stormadvance.storm_esper` package.
    This class generates random records and passes them to the next action (bolt)
    in the topology. The following is the format of the record generated by the `SampleSpout`
    class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the source code of the `SampleSpout` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Create an `EsperBolt` class in the `com.stormadvance.storm_esper` package.
    This bolt receives the tuples emitted by the `SampleSpout` class, converts it
    to the stock bean, and then calls the `esperPut()` method of the `EsperBolt` class
    to pass the data to the Esper engine. The following is the source code of the
    `EsperBolt` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Create an `EsperTopology` class in the `com.stormadvance.storm_esper` package.
    This class creates an instance of the `spout` and `bolt` classes and chains them
    together using a `TopologyBuilder` class. The following is the implementation
    of the main class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we mostly focused on the integration of Storm with other databases.
    Also, we covered how we can use Esper inside Storm to perform the windowing operation.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will cover the Apache log processing case study. We
    will explain how you can generate business information by processing log files
    through Storm.
  prefs: []
  type: TYPE_NORMAL
