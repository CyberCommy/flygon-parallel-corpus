- en: Pipeline APIs for Spark ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will learn the basics of ML pipelines and how they can
    be used in a variety of contexts. The pipeline is made up of several components.
    ML pipelines leverage the Spark platform and machine learning to provide key features
    for making the construction of large-scale learning pipelines simple.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The pipeline API was introduced in Spark 1.2 and is inspired by scikit-learn.
    The concept of pipelines is to facilitate the creation, tuning, and inspection
    of ML workflows.
  prefs: []
  type: TYPE_NORMAL
- en: ML pipelines provide a set of high-level APIs built on top of DataFrames that
    help users create and tune practical machine learning pipelines. Multiple algorithms
    from Spark machine learning can be combined into a single pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: An ML pipeline normally involves a sequence of data pre-processing, feature
    extraction, model fitting, and validation stages.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take an example of text classification, where documents go through preprocessing
    stages, such as tokenization, segmentation and cleaning, extraction of feature
    vectors, and training a classification model with cross-validation. Many steps
    involving pre-processing and algorithms can be tied together using the pipeline.
    The pipeline typically sits above the ML library, orchestrating the workflow.
  prefs: []
  type: TYPE_NORMAL
- en: DataFrames
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Spark pipeline is defined by a sequence of stages where each stage is either
    a transformer or an estimator. These stages are run in order, and the input DataFrame
    is transformed as it passes through each stage.
  prefs: []
  type: TYPE_NORMAL
- en: A DataFrame is a basic data structure or tensor that flows through the pipeline.
    A DataFrame is represented by a dataset of rows, and supports many types, such
    as numeric, string, binary, boolean, datetime, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An ML pipeline or an ML workflow is a sequence of transformers and estimators
    arranged to fit a pipeline model to an input dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A transformer is an abstraction that includes feature transformers and learned
    models. The transformer implements the `transform()` method, which converts one
    DataFrame to another.
  prefs: []
  type: TYPE_NORMAL
- en: A feature transformer takes a DataFrame, reads the text, maps it to a new column,
    and outputs a new DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: A learning model takes a DataFrame, reads the column containing feature vectors,
    predicts the label for each feature vector, and outputs a new DataFrame with the
     predicted labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Custom transformers are required to follow the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Implement the `transform` method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Specify inputCol and outputCol.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Accept `DataFrame` as input and return `DataFrame` as output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In nutshell, the **transformer**: `DataFrame =[transform]=> DataFrame`.'
  prefs: []
  type: TYPE_NORMAL
- en: Estimators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An estimator is an abstraction of a learning algorithm that fits a model on
    a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: An estimator implements a `fit()` method that takes a DataFrame and produces
    a model. An example of a learning algorithm is `LogisticRegression`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, the **estimator** is: `DataFrame =[fit]=> Model`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, `PipelineComponentExample` introduces the concepts
    of transformers and estimators:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Code listing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_12/2.0.0/spark-ai-apps/src/main/scala/org/textclassifier/PipelineComponentExample.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_12/2.0.0/spark-ai-apps/src/main/scala/org/textclassifier/PipelineComponentExample.scala)'
  prefs: []
  type: TYPE_NORMAL
- en: How pipelines work
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We run a sequence of algorithms to process and learn from a given dataset. For
    example, in text classification, we split each document into words and convert
    the words into a numerical feature vector. Finally, we learn a predictive model
    using this feature vector and labels.
  prefs: []
  type: TYPE_NORMAL
- en: Spark ML represents such a workflow as a pipeline, which consists of a sequence
    of PipelineStages (transformers and estimators) to be run in a particular order.
  prefs: []
  type: TYPE_NORMAL
- en: Each stage in *PipelineStages* is one of the components, either a transformer
    or an estimator. The stages are run in a particular order while the input DataFrame
    flows through the stages.
  prefs: []
  type: TYPE_NORMAL
- en: The following images are taken from [https://spark.apache.org/docs/latest/ml-pipeline.html#dataframe](https://spark.apache.org/docs/latest/ml-pipeline.html#dataframe).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following figure, the** dp**Text document pipeline demonstrates the
    document workflow where Tokenizer, Hashing, and Logistic Regression are the components
    of the pipeline. The `Pipeline.fit()` method shows how the raw text gets transformed
    through the pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_12_001.png)'
  prefs: []
  type: TYPE_IMG
- en: When the `Pipeline.fit()` method is called, at the first stage, the raw text
    is tokenized into words using the **Tokenizer** transformer, and in the second
    stage, words are converted to the feature vector using the term frequency transformer.
    In the final stage, the `fit()` method is called on the **Estimator Logistic Regression**
    to get the **Logistic Regression Model** (PipelineModel) over the feature vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Pipeline is an estimator, and after `fit()` is run it, produces a PipelineModel,
    which is a transformer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_12_002.png)'
  prefs: []
  type: TYPE_IMG
- en: '`PipelineModels.transform` method is called on test data and predictions are
    made as shown.'
  prefs: []
  type: TYPE_NORMAL
- en: Pipelines can be linear that is, stages are specified as ordered array or non-linear
    where the data flow forms a **directed acyclic graph** (**DAG**). Pipelines and
    PipelineModels instead perform runtime checking before actually running the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'The DAG pipeline example is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_12_003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following example `TextClassificationPipeline` introduces the concepts
    of transformers and estimators:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Code listing: [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_12/2.0.0/spark-ai-apps/src/main/scala/org/textclassifier/TextClassificationPipeline.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_12/2.0.0/spark-ai-apps/src/main/scala/org/textclassifier/TextClassificationPipeline.scala)
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning pipeline with an example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed in the previous sections, one of the biggest features in the new
    ML library is the introduction of the pipeline. Pipelines provide a high-level
    abstraction of the machine learning flow and greatly simplify the complete workflow.
  prefs: []
  type: TYPE_NORMAL
- en: We will demonstrate the process of creating a pipeline in Spark using the `StumbleUpon`
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset used here can be downloaded from [http://www.kaggle.com/c/stumbleupon/data](http://www.kaggle.com/c/stumbleupon/data).
  prefs: []
  type: TYPE_NORMAL
- en: Download the training data (`train.tsv`)--you will need to accept the terms
    and conditions before downloading the dataset. You can find more information about
    the competition at [http://www.kaggle.com/c/stumbleupon](http://www.kaggle.com/c/stumbleupon).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a glimpse of the `StumbleUpon` dataset stored as a temporary table
    using Spark SQLContext:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_12_004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here is a visualization of the `StumbleUpon` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_12_005.png)'
  prefs: []
  type: TYPE_IMG
- en: StumbleUponExecutor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `StumbleUponExecutor` object can be used to choose and run the respective
    classification model, for example, to run `LogisiticRegression` and execute the
    logistic regression pipeline or set the program argument as `LR`. For other commands,
    refer to the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we proceed, a few words on the Logistic Regression estimator. Logistic
    Regression works for classification problems where classes are nearly linearly
    separable. It searches for a single linear decision boundary in the feature space.
    There are two types of logistic regression estimators available in Spark: binomial
    logistic regression estimators to predict a binary outcome, and multinomial logistic
    regression estimators to predict a multiclass outcome.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Code listing: [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_12/2.0.0/spark-ai-apps/src/main/scala/org/stumbleuponclassifier/StumbleUponExecutor.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_12/2.0.0/spark-ai-apps/src/main/scala/org/stumbleuponclassifier/StumbleUponExecutor.scala)
  prefs: []
  type: TYPE_NORMAL
- en: '**Decision Tree Pipeline: **Pipeline uses a decision tree estimator to classify
    the StumbleUpon dataset as part of the ML workflow.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A decision tree estimator in Spark essentially partitions the feature space
    into half-spaces using axis-aligned linear decision boundaries. The effect is
    that we have a non-linear decision boundary, possibly more than one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see the following output displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Code listing: [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_12/2.0.0/spark-ai-apps/src/main/scala/org/stumbleuponclassifier/DecisionTreePipeline.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_12/2.0.0/spark-ai-apps/src/main/scala/org/stumbleuponclassifier/DecisionTreePipeline.scala)
  prefs: []
  type: TYPE_NORMAL
- en: 'The visualization of predicted data in a 2-dimensional scatter plot is shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_12_006.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The visualization of the actual data in a 2 dimensional scatter plot is shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_12_007.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Naive Bayes Pipeline: **Pipeline uses a naive bayes estimator to classify
    the StumbleUpon dataset as part of the ML workflow.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A Naive Bayes estimator considers the presence of a particular feature in a
    class to be unrelated to the presence of any other feature. The Naive Bayes model
    is simple to build and especially useful for very large data sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see the following output displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Code listing: [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_12/2.0.0/spark-ai-apps/src/main/scala/org/stumbleuponclassifier/NaiveBayesPipeline.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_12/2.0.0/spark-ai-apps/src/main/scala/org/stumbleuponclassifier/NaiveBayesPipeline.scala)
  prefs: []
  type: TYPE_NORMAL
- en: 'A visualization of the predicted data in a 2-dimensional scatter plot is shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_12_008.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A visualization of the actual data in a 2 dimensional scatter plot is shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_12_009.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Gradient Boosted Pipeline : **Pipeline uses Gradient Boosted Tree estimator
    to classify StumbleUpon dataset as part of ML workflow.'
  prefs: []
  type: TYPE_NORMAL
- en: A Gradient Boosted Tree Estimator is a machine learning method for regression
    and classification problems. Both **Gradient-Boosted Trees** (**GBTs**) and Random
    Forests are algorithms for learning ensembles of trees. GBTs iteratively train
    decision trees to minimize a loss function. spark.mllib supports GBTs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see the following output displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Code listing: [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_12/2.0.0/spark-ai-apps/src/main/scala/org/stumbleuponclassifier/GradientBoostedTreePipeline.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_12/2.0.0/spark-ai-apps/src/main/scala/org/stumbleuponclassifier/GradientBoostedTreePipeline.scala)
  prefs: []
  type: TYPE_NORMAL
- en: 'Visualization of predictions in a 2-dimensional scatter plot is shown in the
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_12_010.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Visualization of actual data in 2 dimensional scatter plot is shown in the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_12_011.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered the basics of Spark ML Pipeline and its components.
    We saw how to train models on input DataFrame and how to evaluate their performance
    using standard metrics and measures while running them through spark ML pipeline
    APIs. We explored how to apply some of the techniques like transformers and estimators.
    Finally, we investigated the pipeline API by applying different algorithms on
    the StumbleUpon dataset from Kaggle.
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning is the rising star in the industry. It has certainly addressed
    many business problems and use cases. We hope that our readers will find new and
    innovative ways to make these approaches more powerful and extend the journey
    to understand the principles that hold learning and intelligence. For further
    practice and reading on Machine Learning and Spark refer [https://www.kaggle.com](https://www.kaggle.com)
    and [https://databricks.com/spark/](https://databricks.com/spark/) respectively.
  prefs: []
  type: TYPE_NORMAL
