- en: '*Chapter 5*: Kubernetes Bootcamp'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are sure that many of you have used Kubernetes in some capacity—you may have
    clusters running in production or you may have kicked the tires using kubeadm,
    Minikube, or Docker Desktop. Our goal for this book is to go beyond the basics
    of Kubernetes and, as such, we didn't want to rehash all of the basics of Kubernetes.
    Instead, we added this chapter as a bootcamp for anyone that may be new to Kubernetes
    or might have only played around with it a bit.
  prefs: []
  type: TYPE_NORMAL
- en: Since it is a bootcamp chapter we will not get in-depth on every topic, but
    by the end, you should know enough about the basics of Kubernetes to understand
    the remaining chapters. If you have a strong Kubernetes background, you may still
    find this chapter useful as a refresher, and we will get into more complex topics
    starting in [*Chapter 6*](B15514_06_Final_ASB_ePub.xhtml#_idTextAnchor174)*, Services,
    Load Balancing, and External DNS.*
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will cover the components of a running Kubernetes cluster,
    which include the control plane and the worker node(s). We will detail each Kubernetes
    object and its use cases. If you have used Kubernetes in the past and are comfortable
    using **kubectl** and fully understand Kubernetes objects (such as **DaemonSets**,
    **StatefulSets**, **ReplicaSets**, and so on…), you may want to jump to [*Chapter
    6*](B15514_06_Final_ASB_ePub.xhtml#_idTextAnchor174), *Services, Load Balancing,
    and External DNS*, where we will install Kubernetes using KinD.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: An overview of Kubernetes components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the control plane
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the worker node components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interacting with the API server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing Kubernetes objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter has the following technical requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: An Ubuntu 18.04 server with a minimum of 4 **gigabytes** (**GB**) of **random-access
    memory** (**RAM**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A KinD Kubernetes cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can access the code for this chapter at the following GitHub repository:
    [https://github.com/PacktPublishing/Kubernetes-and-Docker-The-Complete-Guide](https://github.com/PacktPublishing/Kubernetes-and-Docker-The-Complete-Guide).'
  prefs: []
  type: TYPE_NORMAL
- en: An overview of Kubernetes components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In any infrastructure, it is always a good idea to understand how the systems
    work together to provide services. With so many installer options out there today,
    many Kubernetes users have not had the need to understand how Kubernetes components
    integrate.
  prefs: []
  type: TYPE_NORMAL
- en: A few short years ago, if you wanted to run a Kubernetes cluster, you needed
    to install and configure each component manually. It was a steep learning curve
    to install a functioning cluster, which often led to frustration, causing many
    people and companies to say *"Kubernetes is just too difficult"*. The advantage
    of installing manually was that you truly understood how each component interacted,
    and if your cluster ran into issues after installation, you knew what to look
    for.
  prefs: []
  type: TYPE_NORMAL
- en: Nowadays, most people will click a button on a cloud provider and have a fully
    functioning Kubernetes cluster in minutes. On-premise installations have become
    just as easy, with options from Google, RedHat, Rancher, and more, removing the
    complexities of installing a Kubernetes cluster. The issues we see occur when
    you run into a problem or have questions after the installation. Since you didn't
    configure the Kubernetes components, you may not be able to explain to a developer
    how a Pod is scheduled on a worker node. Lastly, since you are running an installer
    provided by a third party, they may enable or disable features that you are not
    aware of, leading to an installation that may be against your company's security
    standards.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand how Kubernetes components work together, you must first understand
    the different components of a Kubernetes cluster. The following diagram is from
    the **Kubernetes.io** site and shows a high-level overview of a Kubernetes cluster
    component:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – Kubernetes cluster components'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_5.1_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.1 – Kubernetes cluster components
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the Kubernetes cluster is made up of several components. As
    we progress through the chapter, we'll discuss these components and the role they
    play in a Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the control plane
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As its name suggests, the control plane controls every aspect of a cluster.
    If your control plane goes down, you can probably imagine that your cluster will
    encounter issues. Without a control plane, a cluster will not have any scheduling
    abilities, which means that workloads that are running will remain running unless
    they are stopped and restarted. Since the control plane is extremely important,
    it is always suggested that you have at least three master nodes. Many production
    installations run more than three master nodes, but the number of installed nodes
    should always be an odd number. Let's look at why the control plane and its components
    are so vital to a running cluster by examining each one.
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes API server
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first component to understand in a cluster is the **kube-apiserver** component.
    Since Kubernetes is **application programming interface** (**API**)-driven, every
    request that comes into a cluster goes through the API server. Let''s look at
    a simple **get nodes** request using an API endpoint, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**https://10.240.100.100:6443/api/v1/nodes?limit=500**'
  prefs: []
  type: TYPE_NORMAL
- en: 'One common method users of Kubernetes deploy to interact with the API server
    is the kubectl utility. Every command that is issued using kubectl calls an API
    endpoint behind the scenes. In the preceding example, we executed a **kubectl
    get nodes** command, which sent an API request to the **kube-apiserver** process
    on **10.240.100.100** on port **6443**. The API call requested the **/api/vi/nodes**
    endpoint, which returned a list of the nodes in the cluster, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – List of Kubernetes nodes'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_5.2_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.2 – List of Kubernetes nodes
  prefs: []
  type: TYPE_NORMAL
- en: Without a running API server, all requests to your cluster will fail. So, as
    you can see, it is very important to have a **kube-apiserver** component running
    at all times. By running three or more master nodes, we can limit any impact of
    losing a master node.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: When running more than one master node, you need to have a load balancer in
    front of the cluster. The Kubernetes API server can be fronted by most standard
    solutions, including F5, HAProxy, and Seesaw.
  prefs: []
  type: TYPE_NORMAL
- en: The Etcd database
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It''s not a stretch to say that Etcd is your Kubernetes cluster. Etcd is a
    fast and highly available distributed key-value database that Kubernetes uses
    to store all cluster data. Each resource in a cluster has a key in the database.
    If you logged in to the node—or Pod—running Etcd, you could use the **etcdctl**
    executable to look at all of the keys in the database. The following code snippet
    shows an example from a cluster running KinD:'
  prefs: []
  type: TYPE_NORMAL
- en: EtcdCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt
    --key=/etc/kubernetes/pki/etcd/server.key --cert=/etc/kubernetes/pki/etcd/server.crt
    get / --prefix --keys-only
  prefs: []
  type: TYPE_NORMAL
- en: 'The output from the preceding command contains too much data to list it all
    in this chapter. A base KinD cluster will return approximately 317 entries. All
    keys start with **/registry/<object>**. For example, one of the keys that were
    returned is the **ClusterRole** for the **cluster-admin** key, as follows: **/registry/clusterrolebindings/cluster-admin**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the key name to retrieve the value using the **etcdctl** utility
    by slightly modifying our previous command, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: EtcdCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt
    --key=/etc/kubernetes/pki/etcd/server.key --cert=/etc/kubernetes/pki/etcd/server.crt
    get /registry/clusterrolebindings/cluster-admin
  prefs: []
  type: TYPE_NORMAL
- en: 'The output will contain characters that cannot be interpreted by your shell,
    but you will get the idea of the data stored in Etcd. For the **cluster-admin**
    key, the output shows us the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](image/Fig_5.3_B15514.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – etcdctl ClusterRoleBinding output
  prefs: []
  type: TYPE_NORMAL
- en: 'The reason we explain the entries in Etcd is to provide a background on how
    Kubernetes uses it to run a cluster. You have seen the output for the **cluster-admin**
    key directly from the database, but in everyday life you would query the API server
    using **kubectl get clusterrolebinding cluster-admin -o yaml**, which would return
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](image/Fig_5.4_B15514.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 – kubectl ClusterRoleBinding output
  prefs: []
  type: TYPE_NORMAL
- en: If you look at the output from the **kubectl** command and compare it with the
    output from the **etcdctl** query, you will see matching information. When you
    execute **kubectl** commands, the request goes to the API server, which then queries
    the Etcd database for the object's information.
  prefs: []
  type: TYPE_NORMAL
- en: kube-scheduler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the name suggests, the **kube-scheduler** component is in charge of scheduling
    running Pods. Whenever a Pod is started in a cluster, the API server receives
    the requests and decides where to run the workload, based on multiple pieces of
    criteria including host resources and cluster policies.
  prefs: []
  type: TYPE_NORMAL
- en: kube-controller-manager
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **kube-controller-manager** component is actually a collection of multiple
    controllers that are included in a single binary. Including the four controllers
    in a single executable reduces complexity by running all four in a single process.
    The four controllers included in the **kube-controller-manager** component are
    the node, replication, endpoints, and service account and token controller.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each controller provides a unique function to a cluster, and each controller
    and its function is listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](image/B15514_table_5.1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Each controller runs a non-terminating (never-ending) control loop. These control
    loops watch the state of each resource, making any changes required to normalize
    the state of the resource. For example, if you needed to scale a deployment from
    one to three nodes, the replication controller would notice that the current state
    has one Pod running, and the desired state is to have three Pods running. To move
    the current state to the desired state, two additional Pods will be requested
    by the replication controller.
  prefs: []
  type: TYPE_NORMAL
- en: cloud-controller-manager
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is one component that you may not have run into, depending on how your
    clusters are configured. Similar to the **kube-controller-manager** component,
    this controller containers four controllers in a single binary. The included controllers
    are the node, route, service, and volume controllers—each controller is responsible
    for interacting with their respective cloud service provider offering.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the worker node components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Worker nodes, as the name implies, are responsible for running workloads. When
    we discussed the **kube-scheduler** component of the control plane, we mentioned
    that when a new Pod is scheduled, the **kube-scheduler** component will decide
    which node to run the Pod on. It does this using information that has been sent
    to it from the worker nodes. This information is constantly updated to help spread
    Pods around a cluster to utilize resources efficiently. Here is a list of the
    worker node components.
  prefs: []
  type: TYPE_NORMAL
- en: kubelet
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You may hear a worker node referred to as a **kubelet**. **kubelet** is an agent
    that runs on all worker nodes, and it is responsible for running the actual containers.
  prefs: []
  type: TYPE_NORMAL
- en: kube-proxy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Contrary to the name, **kube-proxy** is not a proxy server at all. **kube-proxy**
    is responsible for routing network communication between a Pod and the outside
    network.
  prefs: []
  type: TYPE_NORMAL
- en: Container runtime
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is not represented in the picture, but each node also needs a container
    runtime. A container runtime is responsible for running the containers. The first
    thing you might think of is Docker. While Docker is a container runtime, it is
    not the only runtime option available. Over the last year, other options have
    become available and are quickly replacing Docker as the preferred container runtime.
    The two most prominent Docker replacements are CRI-O and containerd.
  prefs: []
  type: TYPE_NORMAL
- en: For the book exercises, we will create a Kubernetes cluster using KinD. At the
    time of this writing, KinD only offers official support for Docker as the container
    runtime, with limited support for Podman.
  prefs: []
  type: TYPE_NORMAL
- en: Interacting with the API server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we mentioned earlier, you interact with the API server using either direct
    API requests or the **kubectl** utility. We will focus on using **kubectl** for
    the majority of our interaction in this book, but we will call out using direct
    API calls where applicable.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Kubernetes kubectl utility
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**kubectl** is a single executable file that allows you to interact with the
    Kubernetes API using a **command-line interface** (**CLI**). It is available for
    most major operating systems and architectures, including Linux, Windows, and
    Mac.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Installation instructions for most operating systems are located on the Kubernetes
    site at [https://kubernetes.io/docs/tasks/tools/install-kubectl/](https://kubernetes.io/docs/tasks/tools/install-kubectl/).
    Since we are using Linux as our operating system for the exercises in the book,
    we will cover installing **kubectl** on a Linux machine. Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To download the latest version of **kubectl**, you can run a **curl** command
    that will download it, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl
    -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl**'
  prefs: []
  type: TYPE_NORMAL
- en: 'After downloading, you need to make the file executable by running the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**chmod +x ./kubectl**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we will move the executable to your path, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**sudo mv ./kubectl /usr/local/bin/kubectl**'
  prefs: []
  type: TYPE_NORMAL
- en: You now have the latest **kubectl** utility on your system and can execute **kubectl**
    commands from any working directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes is updated every 3 months. This includes upgrades to the base Kubernetes
    cluster components and the **kubectl** utility. You may run into a version mismatch
    between a cluster and your **kubectl** command, requiring you to either upgrade
    or download your **kubectl** executable. You can always check the version of both
    by running a **kubectl version** command, which will output the version of both
    the API server and the **kubectl** client. The output from a version check is
    shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Client Version: version.Info{Major:"1", Minor:"17", GitVersion:"v1.17.1", GitCommit:"d224476cd0730baca2b6e357d144171ed74192d6",
    GitTreeState:"clean", BuildDate:"2020-01-14T21:04:32Z", GoVersion:"go1.13.5",
    Compiler:"gc", Platform:"linux/amd64"}'
  prefs: []
  type: TYPE_NORMAL
- en: 'Server Version: version.Info{Major:"1", Minor:"17", GitVersion:"v1.17.0", GitCommit:"70132b0f130acc0bed193d9ba59dd186f0e634cf",
    GitTreeState:"clean", BuildDate:"2020-01-14T00:09:19Z", GoVersion:"go1.13.4",
    Compiler:"gc", Platform:"linux/amd64"}'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the output, the **kubectl** client is running version **1.17.1**
    and the cluster is running **1.17.0**. A minor version difference in the two will
    not cause any issues. In fact, the official supported version difference is within
    one major version release. So, if your client is running version 1.16 and the
    cluster is running 1.17, you would be within the supported version difference.
    While this may be supported, it doesn't mean that you won't run into issues if
    you are trying to use any new commands or objects included in the higher version.
    In general, you should try to keep your cluster and client version in sync to
    avoid any issues.
  prefs: []
  type: TYPE_NORMAL
- en: 'Through the remainder of this chapter, we will discuss Kubernetes objects and
    how you interact with the API server to manage each object. But before diving
    into the different objects, we wanted to mention one commonly overlooked option
    of the **kubectl** utility: the **verbose** option.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the verbose option
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you execute a **kubectl** command, the only outputs you will see by default
    are any direct responses to your command. If you were to look at all Pods in the
    **kube-system** namespace, you would receive a list of all Pods. In most cases
    this is the desired output, but what if you issued a **get Pods** request and
    received an error from the API server? How could you get more information about
    what might be causing the error?
  prefs: []
  type: TYPE_NORMAL
- en: By adding the **verbose** option to your **kubectl** command, you can get additional
    details about the API call itself and any replies from the API server. Often,
    the replies from the API server will contain additional information that may be
    useful to find the root cause of the issue.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **verbose** option has multiple levels ranging from 0 to 9; the higher
    the number, the more output you will receive. The following screenshot has been
    taken from the Kubernetes site, detailing each level and what the output will
    include:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.5 – Verbosity description'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_5.5_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.5 – Verbosity description
  prefs: []
  type: TYPE_NORMAL
- en: You can experiment with the levels by adding the **-v** or **--v** option to
    any **kubectl** command.
  prefs: []
  type: TYPE_NORMAL
- en: General kubectl commands
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The CLI allows you to interact with Kubernetes in an imperative and declarative
    manner. Using an imperative command involves you telling Kubernetes what to do—for
    example, **kubectl run nginx –image nginx**. This tells the API server to create
    a new deployment called **nginx** that runs an image called **nginx**. While imperative
    commands are useful for development and quick fixes or testing, you will use declarative
    commands more often in a production environment. In a declarative command, you
    tell Kubernetes what you want. To use declarative commands, you send a manifest
    to the API server, usually written in **YAML Ain't Markup Language** (**YAML**),
    which declares what you want Kubernetes to create.
  prefs: []
  type: TYPE_NORMAL
- en: '**kubectl** includes commands and options that can provide general cluster
    information or information about an object. The following table contains a cheat-sheet
    of commands and what they are used for. We will use many of these commands in
    future chapters, so you will see them in action throughout the book:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](image/B15514_table_5.2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: With an understanding of each Kubernetes component and how to interact with
    the API server using imperative commands, we can now move on to Kubernetes objects
    and how we use **kubectl** to manage them.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Kubernetes objects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will contain a lot of information and, since this is a bootcamp,
    we will not go into deep details on each object. As you can imagine, each object
    could have its own chapter, or multiple chapters, in a book. Since there are many
    books on Kubernetes that go into detail on the base objects, we will only cover
    the required details of each to have an understanding of each one. In the following
    chapters, we will include additional details for objects as we build out our cluster
    using the book exercises.
  prefs: []
  type: TYPE_NORMAL
- en: Before we go on to understand what Kubernetes objects really are, let's first
    explain Kubernetes manifests.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes manifests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The files that we will use to create Kubernetes objects are referred to as manifests.
    A manifest can be created using YAML or **JavaScript Object Notation** (**JSON**)—most
    manifests use YAML, and that is the format we will use throughout the book.
  prefs: []
  type: TYPE_NORMAL
- en: 'The content of a manifest will vary depending on the object, or objects, that
    will be created. At a minimum, all manifests require a base configuration that
    include the **apiVersion**, object **KinD**, and **metadata** fields, as can be
    seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'apiVersion: apps/v1'
  prefs: []
  type: TYPE_NORMAL
- en: 'KinD: Deployment'
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: 'labels:'
  prefs: []
  type: TYPE_NORMAL
- en: 'app: grafana'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: grafana'
  prefs: []
  type: TYPE_NORMAL
- en: 'namespace: monitoring'
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding manifest alone is not complete; we are only showing the beginning
    of a full deployment manifest. As you can see in the file, we start with the three
    required fields that all manifests are required to have: the **apiVersion**, **KinD**,
    and **metadata** fields.'
  prefs: []
  type: TYPE_NORMAL
- en: You may also notice that there are spaces in the file. YAML is very format-specific,
    and if the format of any line is off by even a single space, you will receive
    an error when you try to deploy the manifest. This takes time to get used to,
    and even after creating manifests for a long time, formatting issues will still
    pop up from time to time.
  prefs: []
  type: TYPE_NORMAL
- en: What are Kubernetes objects?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you want to add or delete something from a cluster, you are interacting
    with a Kubernetes object. An object is what a cluster uses to keep a list of a
    desired state. The desired state may be to create, delete, or scale an object.
    Based on the desired state of the object, the API server will make sure that the
    current state equals the desired state.
  prefs: []
  type: TYPE_NORMAL
- en: 'To retrieve a list of objects a cluster supports, you can use the **kubectl
    api-resources** command. The API server will reply with a list of all objects,
    including any valid short name, namespace support, and supported API group. There
    are approximately 53 base objects included with a base cluster, but an abbreviated
    list of the most common objects is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.6 – Kubernetes API resources'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_5.6_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.6 – Kubernetes API resources
  prefs: []
  type: TYPE_NORMAL
- en: Since this chapter is a bootcamp, we will offer a brief review of many of the
    objects in the list. In order to ensure that you can follow the remaining chapters,
    we will provide an overview of each object and how to interact with them. Some
    objects will also be explained in greater detail in future chapters, including
    **Ingress**, **RoleBindings**, **ClusterRoles**, **StorageClasses**, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing Kubernetes objects
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To make this section easier to follow, we will present each object in the order
    they were provided by the **kubectl api-services** command.
  prefs: []
  type: TYPE_NORMAL
- en: Most objects in a cluster are run in a namespace, and to create/edit/read them,
    you should supply the **-n <namespace>** option to any **kubectl** command. To
    find a list of objects that accept a namespace option, you can reference the output
    from our previous **get api-server** command. If an object can be referenced by
    a namespace, the namespaced column will show **true**. If the object is only referenced
    by the cluster level, the namespaced column will show **false**.
  prefs: []
  type: TYPE_NORMAL
- en: ConfigMaps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A ConfigMap stores data in key-value pairs, providing a way to keep your configuration
    separate from your application. ConfigMaps may contain data from a literal value,
    files, or directories.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an imperative example:'
  prefs: []
  type: TYPE_NORMAL
- en: kubectl create configmap <name> <data>
  prefs: []
  type: TYPE_NORMAL
- en: 'The **name** option will vary based on the source of the ConfigMap. To use
    a file or a directory, you supply the **--from-file** option and either the path
    to a file or an entire directory, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: kubectl create configmap config-test --from-file=/apps/nginx-config/nginx.conf
  prefs: []
  type: TYPE_NORMAL
- en: This would create a new ConfigMap named **config-test**, with the **nginx.conf**
    key containing the content of the **nginx.conf** file as the value.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you needed to have more than one key added in a single ConfigMap, you could
    put each file into a directory and create the ConfigMap using all of the files
    in the directory. As an example, you have three files in a directory located at
    **~/config/myapp**. In the directory are three files, each containing data, called
    **config1**, **config2**, and **config3**. To create a ConfigMap that would add
    each file into a key, you need to supply the **--from-file** option and point
    to the directory, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: kubectl create configmap config-test --from-file=/apps/config/myapp
  prefs: []
  type: TYPE_NORMAL
- en: This would create a new **ConfigMap** with three key values called **config1**,
    **config2**, and **config3**. Each key would contain a value equal to the content
    of each file in the directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'To quickly show a **ConfigMap**, using the example to create a **ConfigMap**
    from a directory, we can retrieve the **ConfigMap** using the get command, **kubectl
    get configmaps config-test**, resulting in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: NAME DATA AGE config-test 3 7s
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that the ConfigMap contains three keys, shown as a **3** under the
    **DATA** column. To look in greater detail, we can use the same **get** command
    and output the value of each key as YAML by adding the **-o yaml** option to the
    **kubectl get configmaps config-test -o yaml** command, resulting in the following
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.7 – kubectl ConfigMap output'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_5.7_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.7 – kubectl ConfigMap output
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the preceding output, you can see each key matches the filenames,
    and the value for each key contains the data in each respective file.
  prefs: []
  type: TYPE_NORMAL
- en: One limitation of ConfigMaps that you should keep in mind is that the data is
    easily accessible to anyone with permissions to the object. As you can see from
    the preceding output, a simple **get** command shows the data in cleartext. Due
    to this design, you should never store sensitive information such as a password
    in a ConfigMap. Later in this section, we will cover an object that was designed
    to store sensitive information, called a Secret.
  prefs: []
  type: TYPE_NORMAL
- en: Endpoints
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An endpoint maps a service to a Pod or Pods. This will make more sense when
    we explain the **Service** object. For now, you only need to know that you can
    use the CLI to retrieve endpoints by using the **kubectl get endpoints** command.
    In a new KinD cluster, you will see a value for the Kubernetes API server in the
    default namespace, as illustrated in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: NAMESPACE   NAME     ENDPOINTS         AGE
  prefs: []
  type: TYPE_NORMAL
- en: default   kubernetes 172.17.0.2:6443   22h
  prefs: []
  type: TYPE_NORMAL
- en: The output shows that the cluster has a service called **kubernetes** that has
    an endpoint at the **Internet Protocol** (**IP**) address **172.17.0.2** on port
    **6443**. Later, you will see when looking at endpoints that they can be used
    to troubleshoot service and ingress issues.
  prefs: []
  type: TYPE_NORMAL
- en: Events
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **Events** object will display any events for a namespace. To get a list
    of events for the **kube-system** namespace, you would use the **kubectl get events
    -n kube-system** command.
  prefs: []
  type: TYPE_NORMAL
- en: Namespaces
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A namespace is an object to divide a cluster into logical units. Each namespace
    allows granular management of resources, including permissions, quotas, and reporting.
  prefs: []
  type: TYPE_NORMAL
- en: The **namespace** object is used for namespace tasks, which are cluster-level
    operations. Using the **namespace** object, you can execute commands including
    **create**, **delete**, **edit**, and **get**.
  prefs: []
  type: TYPE_NORMAL
- en: The syntax for the command is **kubectl <verb> ns <namespace name>**.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, to describe the **kube-system** namespace, we would execute a
    **kubectl describe namespaces kube-system** command. This will return information
    for the namespace, including any labels, annotations, and assigned quotas, as
    illustrated in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Name: kube-system'
  prefs: []
  type: TYPE_NORMAL
- en: 'Labels: <none>Annotations: <none>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Status: Active'
  prefs: []
  type: TYPE_NORMAL
- en: No resource quota.
  prefs: []
  type: TYPE_NORMAL
- en: No LimitRange resource.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding output, you can see that this namespace does not have any labels,
    annotations, or resource quotas assigned.
  prefs: []
  type: TYPE_NORMAL
- en: This section is only meant to introduce the concept of namespaces as a management
    unit in multi-tenant clusters. If you plan to run clusters with multiple tenants,
    you need to understand how namespaces can be used to secure a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Nodes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **nodes** object is a cluster-level resource that is used to interact with
    the cluster's nodes. This object can be used with various actions including **get**,
    **describe**, **label**, and **annotate**.
  prefs: []
  type: TYPE_NORMAL
- en: 'To retrieve a list of all of the nodes in a cluster using **kubectl**, you
    need to execute a **kubectl get nodes** command. On a new KinD cluster running
    a simple one-node cluster, this would display as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: NAME STATUS ROLES AGE VERSION
  prefs: []
  type: TYPE_NORMAL
- en: KinD-control-plane Ready master 22h v1.17.0
  prefs: []
  type: TYPE_NORMAL
- en: You can also use the nodes object to get details of a single node using the
    **describe** command. To get a description of the KinD node listed previously,
    we can execute **kubectl describe node KinD-control-plane**, which would return
    details on the node, including consumed resources, running Pods, IP **classless
    inter-domain routing** (**CIDR**) ranges, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Persistent Volume Claims
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will describe **Persistent Volume Claims** (**PVCs**) in more depth in a
    later chapter, but for now you just need to know that a PVC is used by a Pod to
    consume persistent storage. A PVC uses a **persistent volume** (**PV**) to map
    the storage resource. As with most other objects we have discussed, you can issue
    **get**, **describe**, and **delete** commands on a PVC object. Since these are
    used by Pods, they are a **namespaced** object, and must be created in the same
    namespace as the Pod(s) that will use the PVC.
  prefs: []
  type: TYPE_NORMAL
- en: PVs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: PVs are used by PVCs to create a link between the PVC and the underlying storage
    system. Manually maintaining PVs is a messy task and in the real world it should
    be avoided, since Kubernetes includes the ability to manage most common storage
    systems using the **Container Storage Interface** (**CSI**). As mentioned in the
    **PVC** object section, we will discuss how Kubernetes can automatically create
    the PVs that will be linked to PVCs.
  prefs: []
  type: TYPE_NORMAL
- en: Pods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Pod object is used to interact with the Pods that are running your container(s).
    Using the **kubectl** utility you can use commands such as **get**, **delete**,
    and **describe**. For example, if you wanted to get a list of all Pods in the
    **kube-system** namespace, you would execute a **kubectl get Pods -n kube-system**
    command that would return all Pods in the namespace, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.8 – All Pods in the kube-system namespace'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_5.8_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.8 – All Pods in the kube-system namespace
  prefs: []
  type: TYPE_NORMAL
- en: While you can create a Pod directly, you should avoid doing so unless you are
    using a Pod for quick troubleshooting. Pods that are created directly cannot use
    many of the features provided by Kubernetes, including scaling, automatic restarts,
    or rolling upgrades. Instead of creating a Pod directly, you should use a Deployment,
    or in some rare cases a **ReplicaSet** object or replication controller.
  prefs: []
  type: TYPE_NORMAL
- en: Replication controllers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Replication controllers will manage the number of running Pods, keeping the
    desired replicas specified running at all times. If you create a replication controller
    and set the replica count to **5**, the controller will always keep five Pods
    of the application running.
  prefs: []
  type: TYPE_NORMAL
- en: Replication controllers have been replaced by the **ReplicaSet** object, which
    we will discuss in its own section. While you can still use replication controllers,
    you should consider using a Deployment or a **ReplicaSet** object.
  prefs: []
  type: TYPE_NORMAL
- en: ResourceQuotas
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It is becoming very common to share a Kubernetes cluster between multiple teams,
    referred to as a multi-tenant cluster. Since you will have multiple teams working
    in a single cluster, you should consider creating quotas to limit any potential
    of a single tenant consuming all the resources in a cluster or on a node. Limits
    can be set on most cluster objects, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Central processing unit** (**CPU**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PVCs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ConfigMaps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pods, and more
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting a limit will stop any additional objects being created once the limit
    is hit. If you set a limit of 10 Pods for a namespace and a user creates a new
    Deployment that attempts to start 11 Pods, the 11th Pod will fail to start up
    and the user will receive an error.
  prefs: []
  type: TYPE_NORMAL
- en: 'A basic manifest file to create a quota for memory and CPU would look this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'apiVersion: v1'
  prefs: []
  type: TYPE_NORMAL
- en: 'KinD: ResourceQuota'
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: base-memory-cpu'
  prefs: []
  type: TYPE_NORMAL
- en: 'spec:'
  prefs: []
  type: TYPE_NORMAL
- en: 'hard:'
  prefs: []
  type: TYPE_NORMAL
- en: 'requests.cpu: "2"'
  prefs: []
  type: TYPE_NORMAL
- en: 'requests.memory: 8Gi'
  prefs: []
  type: TYPE_NORMAL
- en: 'limits.cpu: "4"'
  prefs: []
  type: TYPE_NORMAL
- en: 'limits.memory: 16Gi'
  prefs: []
  type: TYPE_NORMAL
- en: This will set a limit on the total amount of resources the namespace can use
    for CPU and memory requests and limits.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once a quota has been created, you can view the usage using the **kubectl describe**
    command. In our example, we named the **ResourceQuota** **base-memory-cpu**. To
    view the usage, we would execute the **kubectl get resourcequotas base-memory-cpu**
    command, resulting in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Name: base-memory-cpu'
  prefs: []
  type: TYPE_NORMAL
- en: 'Namespace: default'
  prefs: []
  type: TYPE_NORMAL
- en: Resource Used Hard
  prefs: []
  type: TYPE_NORMAL
- en: '-------- ---- ----'
  prefs: []
  type: TYPE_NORMAL
- en: limits.cpu 0 4
  prefs: []
  type: TYPE_NORMAL
- en: limits.memory 0 16Gi
  prefs: []
  type: TYPE_NORMAL
- en: requests.cpu 0 2
  prefs: []
  type: TYPE_NORMAL
- en: requests.memory 0 8Gi
  prefs: []
  type: TYPE_NORMAL
- en: '**ResourceQuota** objects are used to control a cluster''s resources. By allocating
    the resources to a namespace, you can guarantee that a single tenant will have
    the required CPU and memory to run their application, while limiting the impact
    that a poorly written application can have on other applications.'
  prefs: []
  type: TYPE_NORMAL
- en: Secrets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Earlier we described how to use a **ConfigMap** object to store configuration
    information. We mentioned that **ConfigMap** objects should never be used to store
    any type of sensitive data. This is the job of a Secret.
  prefs: []
  type: TYPE_NORMAL
- en: Secrets are stored as Base64-encoded strings, which aren't a form of encryption.
    So, why separate Secrets from **ConfigMap** objects? Providing a separate object
    type offers an easier way to maintain access controls and the ability to inject
    sensitive information using an external system.
  prefs: []
  type: TYPE_NORMAL
- en: Secrets can be created using a file, directory, or from a literal string. As
    an example, we have a MySQL image we want to execute, and we would like to pass
    the password to the Pod using a Secret. On our workstation, we have a file called
    **dbpwd** in our current working directory that has our password in it. Using
    the **kubectl** command, we can create a Secret by executing **kubectl create
    secret generic mysql-admin --from-file=./dbpwd**.
  prefs: []
  type: TYPE_NORMAL
- en: 'This would create a new a Secret called **mysql-admin** in the current namespace,
    with the content of the **dbpwd** file. Using **kubectl**, we can get the output
    of the Secret by running the **kubectl get secret mysql-admin -o yaml** command,
    which would output the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'apiVersion: v1'
  prefs: []
  type: TYPE_NORMAL
- en: 'data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'dbpwd: c3VwZXJzZWNyZXQtcGFzc3dvcmQK'
  prefs: []
  type: TYPE_NORMAL
- en: 'KinD: Secret'
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: 'creationTimestamp: "2020-03-24T18:39:31Z"'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: mysql-admin'
  prefs: []
  type: TYPE_NORMAL
- en: 'namespace: default'
  prefs: []
  type: TYPE_NORMAL
- en: 'resourceVersion: "464059"'
  prefs: []
  type: TYPE_NORMAL
- en: 'selfLink: /api/v1/namespaces/default/secrets/mysql-admin'
  prefs: []
  type: TYPE_NORMAL
- en: 'uid: 69220ebd-c9fe-4688-829b-242ffc9e94fc'
  prefs: []
  type: TYPE_NORMAL
- en: 'type: Opaque'
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the preceding output, you can see that the **data** section contains
    the name of our file and then a Base64-encoded value, which was created from the
    content of the file.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we copy the Base64 value from the Secret and pipe it out to the **base64**
    utility, we can easily decode the password, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: echo c3VwZXJzZWNyZXQtcGFzc3dvcmQK | base64 -d
  prefs: []
  type: TYPE_NORMAL
- en: supersecret-password
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: When using the **echo** command to Base64-encode strings, add the **-n** flag
    to avoid adding an additional **\n**. Instead of **echo 'test' | base64**, use
    **echo -n 'test' | base64**.
  prefs: []
  type: TYPE_NORMAL
- en: Everything is stored in Etcd, but we are concerned that someone may be able
    to hack into the master server and steal a copy of the Etcd database. Once someone
    has a copy of the database, they could easily use the **etcdctl** utility to look
    through the content to retrieve all of our Base64-encoded Secrets. Luckily, Kubernetes
    added a feature to encrypt Secrets when they are written to a database.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling this feature can be fairly complex for many users, and while it sounds
    like a good idea, it does present some potential issues that you should consider
    before implementing it. If you would like to read the steps on encrypting your
    Secrets at rest, you can view these on the Kubernetes site at [https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/](https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/).
  prefs: []
  type: TYPE_NORMAL
- en: Another option to secure Secrets is to use a third-party Secrets management
    tool such as HashiCorp's Vault or CyberArk's Conjur.
  prefs: []
  type: TYPE_NORMAL
- en: Service accounts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kubernetes uses service accounts to enable access controls for workloads. When
    you create a Deployment, you may need to access other services or Kubernetes objects.
    Since Kubernetes is a secure system, each object or service your application tries
    to access will evaluate **role-based access control** (**RBAC**) rules to accept
    or deny the request.
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating a service account using a manifest is a straightforward process, requiring
    only a few lines in the manifest. The following code snippet shows a service account
    manifest to create a service account for a Grafana Deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'apiVersion: v1'
  prefs: []
  type: TYPE_NORMAL
- en: 'KinD: ServiceAccount'
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: grafana'
  prefs: []
  type: TYPE_NORMAL
- en: 'namespace: monitoring'
  prefs: []
  type: TYPE_NORMAL
- en: You combine the service account with role bindings and roles to allow access
    to the required services or objects.
  prefs: []
  type: TYPE_NORMAL
- en: Services
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to make an application running in a Pod(s) available to the network,
    you need to create a service. A service object stores information about how to
    expose the application, including which Pods are running on the application and
    the network ports to reach them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each service has a network type that is assigned when they are created, and
    they include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**ClusterIP**: A network type that is only accessible inside the cluster itself.
    This type can still be used for external requests using an ingress controller,
    which will be discussed in a later chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NodePort**: A network type that exposes the service to a random port between
    ports **30000**-**32767**.This port becomes accessible by targeting any worker
    node in a cluster on the assigned **NodePort**. Once created, each node in the
    cluster will receive the port information and incoming requests will be routed
    via **kube-proxy**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LoadBalancer**: This type requires an add-on to use inside a cluster. If
    you are running Kubernetes on a public cloud provider, this type will create an
    external load balancer that will assign an IP address to your service. Most on-premise
    Kubernetes installations do not include support for the **LoadBalancer** type,
    but some offerings such as Google''s Anthos do offer support for it. In a later
    chapter, we will explain how to add an open source project called **MetalLB**
    to a Kubernetes cluster to provide support for the **LoadBalancer** type.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ExternalName**: This type is different from the other three. Unlike the other
    three options, this type will not assign an IP address to the service. Instead,
    this is used to map the internal Kubernetes **Domain Name System** (**DNS**) name
    to an external service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As an example, we have deployed a Pod running Nginx on port **80**. We want
    to create a service that will allow this Pod to receive incoming requests on port
    **80** from within the cluster. The code for this can be seen in the following
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: 'apiVersion: v1'
  prefs: []
  type: TYPE_NORMAL
- en: 'KinD: Service'
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: 'labels:'
  prefs: []
  type: TYPE_NORMAL
- en: 'app: nginx-web-frontend'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: nginx-web'
  prefs: []
  type: TYPE_NORMAL
- en: 'spec:'
  prefs: []
  type: TYPE_NORMAL
- en: 'ports:'
  prefs: []
  type: TYPE_NORMAL
- en: '- name: http'
  prefs: []
  type: TYPE_NORMAL
- en: 'port: 80'
  prefs: []
  type: TYPE_NORMAL
- en: 'targetPort: 80'
  prefs: []
  type: TYPE_NORMAL
- en: 'selector:'
  prefs: []
  type: TYPE_NORMAL
- en: 'app: nginx-web'
  prefs: []
  type: TYPE_NORMAL
- en: In our manifest, we create a label with a value of **app** and assign a value
    of **nginx-web-frontend**. We have called the service itself **nginx-web** and
    we exposed the service on port **80**, targeting the Pod port of **80**. The last
    two lines of the manifest are used to assign the Pods that the service will forward
    to, also known as endpoints. In this manifest, any Pod that has the label of **app**
    with a value of **nginx-web** in the namespace will be added as an endpoint to
    the service.
  prefs: []
  type: TYPE_NORMAL
- en: CustomResourceDefinitions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**CustomResourceDefinitions** (**CRDs**) allow anyone to extend Kubernetes
    by integrating your application into a cluster as a standard object. Once a CRD
    is created, you can reference it using an API endpoint, and it can be interacted
    with using standard **kubectl** commands.'
  prefs: []
  type: TYPE_NORMAL
- en: DaemonSets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A **DaemonSet** allows you to deploy a Pod on every node in a cluster, or a
    subset of nodes. A common use for a **DaemonSet** is to deploy a log forwarding
    Pod such as FluentD to every node in a cluster. Once deployed, the **DaemonSet**
    will create a FluentD Pod on all existing nodes. Since a **DaemonSet** deploys
    to all nodes, any additional nodes that are added to a cluster will have a FluentD
    Pod started once the node has joined the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Deployments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We mentioned earlier that you should never deploy a Pod directly, and we also
    introduced the **ReplicationContoller** object as an alternative to creating Pods
    directly. While both of these will create your Pods, each comes with the following
    limitation: Pods created directly cannot be scaled and cannot be upgraded using
    rolling updates.'
  prefs: []
  type: TYPE_NORMAL
- en: Pods created by a **ReplicationController** can be scaled, and can perform rolling
    updates. However, they do not support rollbacks, and upgrades cannot be done declaratively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deployments offer you a few advantages, including a way to manage your upgrades
    declaratively and the ability to roll back to previous revisions. Creating a Deployment
    is actually a three-step process executed by the API server: a Deployment is created,
    which creates a ReplicaSet object, which then creates the Pod(s) for the application.'
  prefs: []
  type: TYPE_NORMAL
- en: Even if you don't plan to use these features, you should use Deployments by
    default so that you can leverage the features at a future date.
  prefs: []
  type: TYPE_NORMAL
- en: ReplicaSets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ReplicaSets can be used to create a Pod or a set of Pods (replicas). Similar
    to the **ReplicationController** object, a **ReplicaSet** object will maintain
    the set number of Pods defined in the replica count of the object. If there are
    too few Pods, Kubernetes will reconcile the difference and create the missing
    Pods. If there are too many Pods for a ReplicaSet, Kubernetes will delete Pods
    until the number is equal to the replica count set in the object.
  prefs: []
  type: TYPE_NORMAL
- en: In general, you should avoid creating ReplicaSets directly. Instead, you should
    create a Deployment, which will create and manage a ReplicaSet.
  prefs: []
  type: TYPE_NORMAL
- en: StatefulSets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'StatefulSets offer some unique features when creating Pods. They provide features
    that none of the other Pod creation methods offer, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Known Pod names
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ordered Deployment and scaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ordered updates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Persistent storage creation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The best way to understand the advantages of a StatefulSet is to review an
    example manifest from the Kubernetes site, shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.9 – StatefulSet manifest example'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_5.9_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.9 – StatefulSet manifest example
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can look at the objects that the **StatefulSet** object created.
  prefs: []
  type: TYPE_NORMAL
- en: 'The manifest specifies that there should be three replicas of a Pod named **nginx**.
    When we get a list of Pods, you will see that three Pods were created using the
    **nginx** name, with an additional dash and an incrementing number. This is what
    we meant in the overview when we mentioned that Pods will be created with known
    names, as illustrated in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: NAME READY STATUS RESTARTS AGE
  prefs: []
  type: TYPE_NORMAL
- en: web-0 1/1 Running 0 4m6s
  prefs: []
  type: TYPE_NORMAL
- en: web-1 1/1 Running 0 4m2s
  prefs: []
  type: TYPE_NORMAL
- en: web-2 1/1 Running 0 3m52s
  prefs: []
  type: TYPE_NORMAL
- en: The Pods are also created in order – w**eb-0** must be fully deployed before
    **web-1** is created, and then finally **web-2**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, for this example, we also added a PVC to each Pod using the **VolumeClaimTemplate**
    in the manifest. If you look at the output of the **kubectl get pvc** command,
    you would see that three PVCs were created with names we expected (note that we
    removed the **VOLUME** column due to space), as illustrated in the following code
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: NAME STATUS CAPACITY ACCESS MODES STORAGECLASS AGE
  prefs: []
  type: TYPE_NORMAL
- en: www-web-0 Bound 1Gi RWO nfs 13m
  prefs: []
  type: TYPE_NORMAL
- en: www-web-1 Bound 1Gi RWO nfs 13m
  prefs: []
  type: TYPE_NORMAL
- en: www-web-2 Bound 1Gi RWO nfs 12m
  prefs: []
  type: TYPE_NORMAL
- en: In the **VolumeClaimTemplate** section of the manifest, you will see that we
    assigned the name **www** to the PVC claim. When you assign a volume in a StatefulSet,
    the PVC name will combine the name used in the claim template, combined with the
    name of the Pod. Using this naming, you can see why Kubernetes assigned the PVC
    names **www-web-0**, **www-web-1**, and **www-web-2**.
  prefs: []
  type: TYPE_NORMAL
- en: HorizontalPodAutoscalers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the biggest advantages of running a workload on a Kubernetes cluster
    is the ability to easily scale your Pods. While you can scale using the **kubectl**
    command or by editing a manifest's replica count, these are not automated and
    require manual intervention.
  prefs: []
  type: TYPE_NORMAL
- en: '**HorizontalPodAutoscalers** (**HPAs**) provide the ability to scale an application
    based on a set of criteria. Using metrics such as CPU and memory usage, or your
    own custom metrics, you can set a rule to scale your Pods up when you need more
    Pods to maintain your service level. After a cooldown period, Kubernetes will
    scale the application back to the minimum number of Pods defined in the policy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To quickly create an HPA for an **nginx** Deployment, we can execute a **kubectl**
    command using the **autoscale** option, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: kubectl autoscale deployment nginx --cpu-percent=50 --min=1 --max=5
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also create a Kubernetes manifest to create your HPAs. Using the same
    options as those we did in the CLI, our manifest would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'apiVersion: autoscaling/v1'
  prefs: []
  type: TYPE_NORMAL
- en: 'KinD: HorizontalPodAutoscaler'
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: nginx-deployment'
  prefs: []
  type: TYPE_NORMAL
- en: 'spec:'
  prefs: []
  type: TYPE_NORMAL
- en: 'maxReplicas: 5'
  prefs: []
  type: TYPE_NORMAL
- en: 'minReplicas: 1'
  prefs: []
  type: TYPE_NORMAL
- en: 'scaleTargetRef:'
  prefs: []
  type: TYPE_NORMAL
- en: 'apiVersion: apps/v1'
  prefs: []
  type: TYPE_NORMAL
- en: 'KinD: Deployment'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: nginx-deployment'
  prefs: []
  type: TYPE_NORMAL
- en: 'targetCPUUtilizationPercentage: 50'
  prefs: []
  type: TYPE_NORMAL
- en: Both options will create an HPA that will scale our **nginx-deployment nginx**
    Deployment up to five replicas when the Deployment hits a CPU utilization of 50%.
    Once the Deployment usage falls below 50% and the cooldown period is reached (by
    default, 5 minutes), the replica count will be reduced down to 1.
  prefs: []
  type: TYPE_NORMAL
- en: CronJobs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you have used Linux cronjobs in the past, then you already know what a Kubernetes
    **CronJob** object is. If you don't have a Linux background, a cronjob is used
    to create a scheduled task. As another example, if you are a Windows person, it's
    similar to Windows scheduled tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example manifest that creates a **CronJob** is shown in the following code
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: 'apiVersion: batch/v1beta1'
  prefs: []
  type: TYPE_NORMAL
- en: 'KinD: CronJob'
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: hello-world'
  prefs: []
  type: TYPE_NORMAL
- en: 'spec:'
  prefs: []
  type: TYPE_NORMAL
- en: 'schedule: "*/1 * * * *"'
  prefs: []
  type: TYPE_NORMAL
- en: 'jobTemplate:'
  prefs: []
  type: TYPE_NORMAL
- en: 'spec:'
  prefs: []
  type: TYPE_NORMAL
- en: 'template:'
  prefs: []
  type: TYPE_NORMAL
- en: 'spec:'
  prefs: []
  type: TYPE_NORMAL
- en: 'containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '- name: hello-world'
  prefs: []
  type: TYPE_NORMAL
- en: 'image: busybox'
  prefs: []
  type: TYPE_NORMAL
- en: 'args:'
  prefs: []
  type: TYPE_NORMAL
- en: '- /bin/sh'
  prefs: []
  type: TYPE_NORMAL
- en: '- -c'
  prefs: []
  type: TYPE_NORMAL
- en: '- date; echo Hello World!'
  prefs: []
  type: TYPE_NORMAL
- en: 'restartPolicy: OnFailure'
  prefs: []
  type: TYPE_NORMAL
- en: 'The **schedule** format follows the standard **cron** format. From left to
    right, each ***** represents the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Minute (0 – 59)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hour (0 -23)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Day (1 -31)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Month (1 – 12)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Day of the week (0 – 6) (Sunday = 0, Saturday = 6)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cron jobs accept step values, which allow you to create a schedule that can
    execute every minute, every 2 minutes, or every hour.
  prefs: []
  type: TYPE_NORMAL
- en: Our example manifest will create a **cronjob** that runs an image called **hello-world**
    every minute and outputs **Hello World!** in the Pod log.
  prefs: []
  type: TYPE_NORMAL
- en: Jobs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Jobs allow you to execute a specific number of executions of a Pod or Pods.
    Unlike a **cronjob** object, these Pods are not run on a set schedule, but rather
    they will execute once created. Jobs are used to execute a task that may only
    need to be executed at the initial Deployment stage.
  prefs: []
  type: TYPE_NORMAL
- en: An example use case would be an application that may require the creation of
    Kubernetes CRDs that must exist before the main application is deployed. The Deployment
    would wait until the job execution completed successfully.
  prefs: []
  type: TYPE_NORMAL
- en: Events
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Events objects store information about events for Kubernetes objects. You do
    not create events; rather, you can only retrieve events. For example, to retrieve
    events for the **kube-system** namespace, you would execute **kubectl get events
    -n kube-system**, or to show events for all namespaces, you'd execute **kubectl
    get events --all-namespaces**.
  prefs: []
  type: TYPE_NORMAL
- en: Ingresses
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You may have noticed that the **Ingress** object was listed twice in our **api-server**
    output. This will happen to objects as Kubernetes upgrades are released and objects
    changed in the API server. In the case of Ingress, it was original part of the
    extensions API and was moved to the **networking.k8s.io** API in version 1.16\.
    The project will wait a few releases before deprecating the old API call, so in
    our example cluster running Kubernetes 1.17, using either API will work. In version
    1.18, they have plans to fully deprecate the Ingress extensions.
  prefs: []
  type: TYPE_NORMAL
- en: NetworkPolicies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**NetworkPolicy** objects let you define how network traffic can flow through
    your cluster. They allow you to use Kubernetes native constructs to define which
    Pods can talk to other Pods. If you''ve ever used Security Groups in **Amazon
    Web Services** (**AWS**) to lock down access between two groups of systems, it''s
    a similar concept. As an example, the following policy will allow traffic on port
    **443** to Pods in the **myns** namespace from any namespace with the **app.kubernetes.io/name:
    ingress-nginx** label on it (which is the default label for the **nginx-ingress**
    namespace):'
  prefs: []
  type: TYPE_NORMAL
- en: 'apiVersion: networking.k8s.io/v1'
  prefs: []
  type: TYPE_NORMAL
- en: 'KinD: NetworkPolicy'
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: allow-from-ingress'
  prefs: []
  type: TYPE_NORMAL
- en: 'namespace: myns'
  prefs: []
  type: TYPE_NORMAL
- en: 'spec:'
  prefs: []
  type: TYPE_NORMAL
- en: 'PodSelector: {}'
  prefs: []
  type: TYPE_NORMAL
- en: 'policyTypes:'
  prefs: []
  type: TYPE_NORMAL
- en: '- Ingress'
  prefs: []
  type: TYPE_NORMAL
- en: 'ingress:'
  prefs: []
  type: TYPE_NORMAL
- en: '- from:'
  prefs: []
  type: TYPE_NORMAL
- en: '- namespaceSelector:'
  prefs: []
  type: TYPE_NORMAL
- en: 'matchLabels:'
  prefs: []
  type: TYPE_NORMAL
- en: 'app.kubernetes.io/name: ingress-nginx ports:'
  prefs: []
  type: TYPE_NORMAL
- en: '- protocol: TCP'
  prefs: []
  type: TYPE_NORMAL
- en: 'port: 443'
  prefs: []
  type: TYPE_NORMAL
- en: A **NetworkPolicy** object is another object that you can use to secure a cluster.
    They should be used in all production clusters, but in a multi-tenant cluster
    they should be considered a **must-have** to secure each namespace in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: PodSecurityPolicies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**PodSecurityPolicies** (**PSPs**) are how your cluster protects your nodes
    from your containers. They allow you to limit the actions that a Pod can execute
    in a cluster. Some examples include denying access to the HostIPC and HostPath,
    and running a container in a privileged mode.'
  prefs: []
  type: TYPE_NORMAL
- en: We'll get into the details of PSPs in [*Chapter 10*](B15514_10_Final_ASB_ePub.xhtml#_idTextAnchor260),
    *Creating Pod Security Policies*. The key point to remember about PSPs is that
    without them, your containers can do almost anything on your nodes.
  prefs: []
  type: TYPE_NORMAL
- en: ClusterRoleBindings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once you have defined a **ClusterRole**, you bind it to a subject via a **ClusterRoleBinding**.
    A **ClusterRole** can be bound to a User, Group, or ServiceAccount.
  prefs: []
  type: TYPE_NORMAL
- en: We'll explore **ClusterRoleBinding** details in [*Chapter 8*](B15514_08_Final_ASB_ePub.xhtml#_idTextAnchor228)*,
    RBAC Policies and Auditing*.
  prefs: []
  type: TYPE_NORMAL
- en: ClusterRoles
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A **ClusterRole** combines a set of permissions for interacting with your cluster''s
    API. A **ClusterRole** combines a verb or action with an API group to define a
    permission. For instance, if you only want your **continuous integration/continuous
    delivery** (**CI/CD**) pipeline to be able to patch your Deployments so that it
    can update your image tag, you might use a **ClusterRole** like this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'apiVersion: rbac.authorization.k8s.io/v1'
  prefs: []
  type: TYPE_NORMAL
- en: 'KinD: ClusterRole'
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: patch-deployment'
  prefs: []
  type: TYPE_NORMAL
- en: 'rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '- apiGroups: ["apps/v1"]'
  prefs: []
  type: TYPE_NORMAL
- en: 'resources: ["deployments"]'
  prefs: []
  type: TYPE_NORMAL
- en: 'verbs: ["get", "list", "patch"]'
  prefs: []
  type: TYPE_NORMAL
- en: A **ClusterRole** can apply to APIs at both the cluster and namespace level.
  prefs: []
  type: TYPE_NORMAL
- en: RoleBindings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The **RoleBinding** object is how you associate a Role or **ClusterRole** to
    a subject and namespace. For instance, the following **RoleBinding** object will
    allow the **aws-codebuild** user to apply the **patch-openunison** ClusterRole
    to the **openunison** namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: 'apiVersion: rbac.authorization.k8s.io/v1'
  prefs: []
  type: TYPE_NORMAL
- en: 'KinD: RoleBinding'
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: patch-openunison'
  prefs: []
  type: TYPE_NORMAL
- en: 'namespace: openunison'
  prefs: []
  type: TYPE_NORMAL
- en: 'subjects:'
  prefs: []
  type: TYPE_NORMAL
- en: '- KinD: User'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: aws-codebuild'
  prefs: []
  type: TYPE_NORMAL
- en: 'apiGroup: rbac.authorization.k8s.io'
  prefs: []
  type: TYPE_NORMAL
- en: 'roleRef:'
  prefs: []
  type: TYPE_NORMAL
- en: 'KinD: ClusterRole'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: patch-deployment'
  prefs: []
  type: TYPE_NORMAL
- en: 'apiGroup: rbac.authorization.k8s.io'
  prefs: []
  type: TYPE_NORMAL
- en: Even though this references a **ClusterRole**, it will only apply to the **openunison**
    namespace. If the **aws-codebuild** user tries to patch a Deployment in another
    namespace, the API server will stop it.
  prefs: []
  type: TYPE_NORMAL
- en: Roles
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As with a **ClusterRole**, Roles combine API groups and actions to define a
    set of permissions that can be assigned to a subject. The difference between a
    **ClusterRole** and a **Role** is that a **Role** can only have resources defined
    at the namespace level and applies only within a specific namespace.
  prefs: []
  type: TYPE_NORMAL
- en: CsiDrivers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kubernetes uses the **CsiDriver** object to connect nodes to a storage system.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can list all CSI drivers that are available on a cluster by executing the
    **kubectl get csidriver** command. In one of our clusters we are using Netapp''s
    SolidFire for storage, so our cluster has the Trident CSI driver installed, as
    can be seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: NAME CREATED AT
  prefs: []
  type: TYPE_NORMAL
- en: csi.trident.netapp.io 2019-09-04T19:10:47Z
  prefs: []
  type: TYPE_NORMAL
- en: CsiNodes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To avoid storing storage information in the node API object, the **CSINode**
    object was added to the API server to store information generated by the CSI drivers.
    The information that is stored includes mapping Kubernetes node names to CSI node
    names, CSI driver availability, and the volume topology.
  prefs: []
  type: TYPE_NORMAL
- en: StorageClasses
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Storage classes are used to define a storage endpoint. Each storage class can
    be assigned labels and policies, allowing a developer to select the best storage
    location for their persistent data. You may create a storage class for a backend
    system that has all **Non-Volatile Memory Express** (**NVMe**) drives, assigning
    it the name **fast**, while assigning a different class to a Netapp **Network
    File System** (**NFS**) volume running standard drives, using the name **standard**.
  prefs: []
  type: TYPE_NORMAL
- en: When a PVC is requested, the user can assign a **StorageClass** that they wish
    to use. When the API server receives the request, it finds the matching name and
    uses the **StorageClass** configuration to create the volume on the storage system
    using a provisioner.
  prefs: []
  type: TYPE_NORMAL
- en: 'At a very high level, a **StorageClass** manifest does not require a lot of
    information. Here is an example of a storage class using a provisioner from the
    Kubernetes incubator project to provide NFS auto-provisioned volumes, named **nfs**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'apiVersion: storage.k8s.io/v1 KinD: StorageClass'
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: nfs'
  prefs: []
  type: TYPE_NORMAL
- en: 'provisioner: nfs'
  prefs: []
  type: TYPE_NORMAL
- en: Storage classes allow you to offer multiple storage solutions to your users.
    You may create a class for cheaper, slower storage while offering a second class
    that supports high throughput for high data requirements. By providing a different
    class to each offering, you allow developers to select the best choice for their
    application.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you were thrown into a Kubernetes bootcamp that presented a
    lot of technical material in a short amount of time. Try to remember that this
    will all become easier as you get into the Kubernetes world in more depth. We
    realize that this chapter had a lot of information on many objects. Many of the
    objects will be used in later chapters, and they will be explained in greater
    detail.
  prefs: []
  type: TYPE_NORMAL
- en: You learned about each Kubernetes component and how they interact to create
    a cluster. With this knowledge, you have the required skills to look at errors
    in a cluster and determine which component may be causing an error or issue. We
    covered the control plane of a cluster where the **api-server**, **kube-scheduler**,
    Etcd, and control managers run. The control plane is how users and services interact
    with a cluster; using the **api-server** and the **kube-scheduler** will decide
    which worker node to schedule your Pod(s) on. You also learned about Kubernetes
    nodes that run the **kubelet** and **kube-proxy** components, and a container
    runtime.
  prefs: []
  type: TYPE_NORMAL
- en: We covered the **kubectl** utility that you will use to interact with a cluster.
    You also learned some common commands that you will use on a daily basis, including
    **logs** and **describe**.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will create a development Kubernetes cluster that we
    will use as the base cluster for the remaining chapters. Throughout the remainder
    of the book, we will reference many of the objects that were presented in this
    chapter, helping to explain them by using them in real-world examples.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Kubernetes control plane does not include which of the following components?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. **api-server**
  prefs: []
  type: TYPE_NORMAL
- en: B. **kube-scheduler**
  prefs: []
  type: TYPE_NORMAL
- en: C. Etcd
  prefs: []
  type: TYPE_NORMAL
- en: D. Ingress controller
  prefs: []
  type: TYPE_NORMAL
- en: What is the name of the component that keeps all of the cluster information?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. **api-server**
  prefs: []
  type: TYPE_NORMAL
- en: B. Master controller
  prefs: []
  type: TYPE_NORMAL
- en: C. **kubelet**
  prefs: []
  type: TYPE_NORMAL
- en: D. Etcd
  prefs: []
  type: TYPE_NORMAL
- en: Which component is responsible for selecting the node that will run a workload?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. **kubelet**
  prefs: []
  type: TYPE_NORMAL
- en: B. **api-server**
  prefs: []
  type: TYPE_NORMAL
- en: C. **kube-scheduler**
  prefs: []
  type: TYPE_NORMAL
- en: D. **Pod-scheduler**
  prefs: []
  type: TYPE_NORMAL
- en: Which option would you add to a **kubectl** command to see additional output
    from a command?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. **Verbose**
  prefs: []
  type: TYPE_NORMAL
- en: B. **-v**
  prefs: []
  type: TYPE_NORMAL
- en: C. **–verbose**
  prefs: []
  type: TYPE_NORMAL
- en: D. **-log**
  prefs: []
  type: TYPE_NORMAL
- en: Which service type creates a randomly generated port, allowing incoming traffic
    to any worker node on the assigned port to access the service?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. **LoadBalancer**
  prefs: []
  type: TYPE_NORMAL
- en: B. **ClusterIP**
  prefs: []
  type: TYPE_NORMAL
- en: C. None—it's the default for all services
  prefs: []
  type: TYPE_NORMAL
- en: D. **NodePort**
  prefs: []
  type: TYPE_NORMAL
- en: If you need to deploy an application on a Kubernetes cluster that requires known
    node names and a controlled startup of each Pod, which object would you create?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. **StatefulSet**
  prefs: []
  type: TYPE_NORMAL
- en: B. **Deployment**
  prefs: []
  type: TYPE_NORMAL
- en: C. **ReplicaSet**
  prefs: []
  type: TYPE_NORMAL
- en: D. **ReplicationController**
  prefs: []
  type: TYPE_NORMAL
