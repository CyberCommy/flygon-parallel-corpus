- en: Patterns in Parallel Programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we introduced threading models in IIS and Kestrel and
    how they can be optimized to improve performance, as well as some new async feature
    support in .NET Core 3.0.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will introduce parallel programming patterns and focus on
    understanding the parallel code problem scenarios and solving them using parallel
    programming/async techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Although there are numerous patterns that have been utilized in parallel programming
    techniques, we will limit ourselves to explaining the most important ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: '`MapReduce`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aggregation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fork/join
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Speculative processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Laziness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shared state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Knowledge of C# and parallel programming is required in order to understand
    this chapter. The source code for this chapter can be found on GitHub at [https://github.com/PacktPublishing/-Hands-On-Parallel-Programming-with-C-8-and-.NET-Core-3/tree/master/Chapter13](https://github.com/PacktPublishing/-Hands-On-Parallel-Programming-with-C-8-and-.NET-Core-3/tree/master/Chapter13).
  prefs: []
  type: TYPE_NORMAL
- en: The MapReduce pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `MapReduce` pattern was introduced in order to handle big data problems
    such as large-scale computing requirements spanning across a cluster of servers.
    The pattern can also be used on single-core machines.
  prefs: []
  type: TYPE_NORMAL
- en: 'A `MapReduce` program is composed of two tasks: **map** and **reduce**. The
    input for the `MapReduce` program is passed as a set of key-value pairs and the
    output is also received as such.'
  prefs: []
  type: TYPE_NORMAL
- en: To implement this pattern, we need to start by writing a `map` function that
    takes in data (key/value pair) as a single input value and converts it into another
    set of intermediate data (key/value pair). The user then writes a `reduce` function
    that takes the output from the `map` function (key/value pair) as input and combines
    the data with a smaller dataset containing any number of rows of data.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at how to implement a basic `MapReduce` pattern using LINQ and convert
    it into a PLINQ-based implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing MapReduce using LINQ
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is a typical graphical representation of the `MapReduce` pattern.
    The input goes through various mapped functions, with each returning a set of
    mapped values as output. These are then grouped and joined by `Reduce()` functions
    to create the final output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ce927cbb-692b-46c2-919b-d261f96b84f5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Follow these steps to implement the `MapReduce` pattern using LINQ:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we need to write a `map` function with a single input value that returns
    a set of mapped values. We can use LINQ's `SelectMany` function for this.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we need to group data according to the intermediate key. We can use LINQ's
    `GroupBy` method for this.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we need a `reduce` method that will take an intermediate key as input.
    It will also take a corresponding set of values for that and produce output. We
    can use `SelectMany`for this.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Our final `MapReduce` pattern will now look as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can change the input and output so that it works with `ParallelQuery<T>`
    rather than `IEnumerable<T>`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is an example of using the custom implementation of `MapReduce` in
    .NET Core. The program generates some positive and negative random numbers in
    a range. Then, it applies a map to filter out any positive numbers and group them
    by number. Finally, it applies the `reduce` function to return a list of numbers,
    along with their counts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is an excerpt from the output we receive if we run the preceding
    program code in Visual Studio. As you can see, it iterates the provided list and
    finds the count of how many times the numbers occurred:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2e6c38af-ca3a-4de2-89ee-c6de40ac3d90.png)'
  prefs: []
  type: TYPE_IMG
- en: In the next section, we will discuss another common and important parallel design
    pattern called aggregation. While the `MapReduce` pattern acts as a filter, aggregation
    just combines all the data from the input and puts it in another format.
  prefs: []
  type: TYPE_NORMAL
- en: Aggregation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Aggregation is another common design pattern that's used in parallel applications.
    In parallel programs, the data is divided into units so that it can be processed
    across cores by a number of threads. At some point, there is a need to combine
    data from all the relevant sources before it can be presented to the user. This
    is where aggregation comes into the picture.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's explore the need for aggregation and what is provided by PLINQ.
  prefs: []
  type: TYPE_NORMAL
- en: 'A common use case of aggregation is as follows. Here, we are trying to iterate
    a set of values, perform some operations, and return the result to the caller:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The problem with the preceding code is that the output isn''t thread-safe.
    Due to this, to avoid cross-threading issues, we need to make use of synchronization
    primitives:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code works well if the computation that''s done per item is small.
    However, as the computation that''s done per item increases the cost of taking
    and releasing, the lock will also increase. This results in degraded performance.
    Concurrent collections, which we discussed in [Chapter 6](c10ffb38-9184-4518-8ae3-93c7a2fb4d5d.xhtml),
    *Using Concurrent Collections*, comes to the rescue here. With concurrent collections,
    we don''t have to worry about synchronizations. The following code snippet is
    using concurrent collection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'PLINQ also defines methods that help with aggregation and handle synchronization.
    Some of these methods are `ToArray`,  `ToList`,  `ToDictionary`, and `ToLookup`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, the `ToList()` method takes care of aggregating all the
    data while also dealing with synchronization. Some implementation patterns are
    available in TPL and are built into programming languages. One of them is the
    fork/join pattern, which we will discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: The fork/join pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In fork/join patterns, work is *forked* (*split*) into a set of tasks that
    can be executed asynchronously. Later, the forked work is joined in the same order
    or a different order, as per the requirements and scope of parallelization. We
    have already seen some common examples of fork/join patterns in this book when
    we discussed delightfully parallel loops. Some implementations of fork/join are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Parallel.For`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Parallel.ForEach`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Parallel.Invoke`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`System.Threading.CountdownEvent`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilizing these framework-provided methods aids in faster development without
    developers having to worry about synchronization overheads. These patterns result
    in high throughput. To achieve high throughput and to reduce latency, another
    pattern, called speculative processing, is widely used.
  prefs: []
  type: TYPE_NORMAL
- en: The speculative processing pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The speculative processing pattern is another parallel programming pattern that
    relies on high throughput to reduce latency. This is very useful in scenarios
    where there are multiple ways of performing a task but the user doesn't know which
    way will return the results fastest. This approach creates a task for each possible
    method, which is then executed across processors. The task that finishes first
    is used as output, ignoring the others (which may still complete successfully
    but are slow).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a typical `SpeculativeInvoke` representation. It accepts an
    array of `Func<T>` as parameters and executes them in parallel until one of them
    returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following method executes each action that''s passed to it in parallel
    and breaks out of a parallel loop by calling the `ParallelLoopState.Stop()` method
    as soon as any of the called implementations execute successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code uses two different logics to calculate the square of 5\.
    We will pass both approaches to the `SpeculativeInvoke` method and print the `result`
    as soon as possible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/54d8ada1-e36c-45c0-8c92-fc3f760b2e44.png)'
  prefs: []
  type: TYPE_IMG
- en: As you will see, both methods finish but only the output of the first finished
    execution is returned to the caller. Creating too many tasks can have an adverse
    effect on system memory as more and more variables need to be allocated and kept
    in memory. Therefore, it becomes very important to allocate objects only when
    they are actually required. Our next pattern helps us achieve this.
  prefs: []
  type: TYPE_NORMAL
- en: The lazy pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Lazy is another programming pattern that is used by application developers to
    improve application performance. Laziness is about delaying computation until
    it's actually needed. In a best-case scenario, the computation might not be needed
    at all, which helps in not wasting compute resources and thus improving the performance
    of the system as a whole. Lazy evaluation is not new to computing, and LINQ uses
    *lazy loading* heavily. LINQ follows the deferred execution model in which queries
    are not executed until we call `MoveNext()` on them using some iterator functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of a thread-safe lazy singleton pattern that utilizes
    some heavy compute operations for creation and is thus deferred:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: A lazy object is created by calling the `LazySingleton<T>` class's `Value` property.
    Laziness guarantees that an object is not created until the `Value` property is
    called. Once created, the singleton implementation ensures that the same object
    is returned on subsequent calls. A null check on `_value` avoids creating a lock
    on subsequent calls, thereby saving some memory I/O operations and improving performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can get around writing so much code by making use of `System.Lazy<T>`, as
    shown in the following code example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: While working with asynchronous programming, we can combine the power of `Lazy<T>`
    with TPL to achieve significant results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of using both `Lazy<T>` and `Task<T>` to implement
    lazy and asynchronous behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We can access the underlying `Task` through the `data.Value` property. The underlying
    lazy implementation will ensure that the same task instance is returned every
    time, no matter how many times you call the `data.Value` property. This is useful
    in scenarios where you don't want to launch many threads and just want to launch
    a single thread that may carry out some asynchronous processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following piece of code, which fetches data from a service and
    saves it to an Excel or CSV file from two different thread implementations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The following are two example methods that have logic we can save as text or
    in CSV format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code shows how we can wrap the service call inside `lazy` and
    make sure that a service call is made only once while the output can be used asynchronously.
    As you can see, we have wrapped the lazy initialization method as a task using
    `Task.Factory.StartNew(GetDataFromService)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c4b012ec-745d-4df6-ae05-ad40382ceb23.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the service is only called once. Whenever you need to create
    objects, the lazy pattern is an advisable proposition for developers. When we
    create multiple tasks, we face problems associated with the synchronization of
    resources. An understanding of shared state patterns comes in handy in these scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Shared state pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We covered the implementation of these patterns in [Chapter 5](a1a7572c-ff51-47d3-a5e6-3a256fd79b24.xhtml),
    *Synchronization Primitives*.
  prefs: []
  type: TYPE_NORMAL
- en: A parallel application has to deal with a shared state problem constantly. The
    application will have some data members that need to be protected when they're
    accessed in a multithreaded environment. There are many ways to deal with shared
    state problems, such as using `Synchronization`, `Isolation`, and `Immutability`.
    Synchronization can be achieved using the synchronization primitives provided
    by the .NET Framework and it can also provide mutual exclusion over a shared data
    member. Immutability guarantees only one state for a data member and never changes.
    Consequently, the same state can be shared across threads without any issues.
    Isolation deals with each thread having its own copy of data members.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we'll summarize what we learned in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced various patterns of parallel programming and
    provided examples of each. Though not an exhaustive list, these patterns can prove
    to be a good starting point for parallel application programming developers.
  prefs: []
  type: TYPE_NORMAL
- en: In a nutshell, we discussed the `MapReduce` pattern, the speculative processing
    pattern, the lazy pattern, and the aggregation pattern. We also introduced some
    implementation patterns, such as fork/join and the shared state pattern, both
    are which are used in .NET Framework libraries for parallel programming.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will introduce distributed memory management and focus
    on understanding the shared memory model as well as the distributed memory model.
    We will also discuss various types of communication networks and their properties
    with example implementations.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Which of these is not an implementation of the fork/join pattern?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`System.Threading.Barrier`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`System.Threading.Countdown`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Parallel.For`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Parallel.ForEach`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which of these is the implementation of the lazy pattern in TPL?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Lazy<T>`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`LazySingleton`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`LazyInitializer`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which pattern relies on achieving high throughput to reduce latency?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lazy
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Shared state
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Speculative processing
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which pattern can you use if you need to filter out data from a list and return
    a single output?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Aggregation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`MapReduce`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
