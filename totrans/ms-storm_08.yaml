- en: Integration of Storm and Kafka
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Kafka is a high-throughput, distributed, fault-tolerant, and replicated
    messaging system that was first developed at LinkedIn. The use cases of Kafka
    vary from log aggregation, to stream processing, to replacing other messaging
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: Kafka has emerged as one of the important components of real-time processing
    pipelines in combination with Storm. Kafka can act as a buffer or feeder for messages
    that need to be processed by Storm. Kafka can also be used as the output sink
    for results emitted from Storm topologies.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will be covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Kafka architecture--broker, producer, and consumer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installation of the Kafka cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharing the producer and consumer between Kafka
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Development of Storm topology using Kafka consumer as Storm spout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment of a Kafka and Storm integration topology
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to Kafka
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section we are going to cover the architecture of Kafka--broker, consumer,
    and producer.
  prefs: []
  type: TYPE_NORMAL
- en: Kafka architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kafka has an architecture that differs significantly from other messaging systems.
    Kafka is a peer to peer system (each node in a cluster has the same role) in which
    each node is called a **broker**. The brokers coordinate their actions with the
    help of a ZooKeeper ensemble. The Kafka metadata managed by the ZooKeeper ensemble
    is mentioned in the section *Sharing ZooKeeper between Storm and Kafka*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00051.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.1: A Kafka cluster'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the important components of Kafka:'
  prefs: []
  type: TYPE_NORMAL
- en: Producer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A producer is an entity that uses the Kafka client API to publish messages into
    the Kafka cluster. In a Kafka broker, messages are published by the producer entity
    to named entities called **topics**. A topic is a persistent queue (data stored
    into topics is persisted to disk).
  prefs: []
  type: TYPE_NORMAL
- en: 'For parallelism, a Kafka topic can have multiple partitions. Each partition
    data is represented in a different file. Also, two partitions of a single topic
    can be allocated on a different broker, thus increasing throughput as all partitions
    are independent of each other. Each message in a partition has a unique sequence
    number associated with it called an **offset**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00052.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.2: Kafka topic distribution'
  prefs: []
  type: TYPE_NORMAL
- en: Replication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kafka supports the replication of partitions of topics to support fault tolerance.
    Kafka automatically handles the replication of partitions and makes sure that
    the replica of a partition will be assigned to different brokers. Kafka elects
    one broker as the leader of a partition and all writes and reads must go to the
    partition leader. Replication features are introduced in Kafka 8.0.0 version.
  prefs: []
  type: TYPE_NORMAL
- en: The Kafka cluster manages the list of **in sync replica** (**ISR**)--the replicate
    which are in sync with the partition leader into ZooKeeper. If the partition leader
    goes down, then the followers/replicas that are present in the ISR list are only
    eligible for the next leader of the failed partition.
  prefs: []
  type: TYPE_NORMAL
- en: Consumer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Consumers read a range of messages from a broker. Each consumer has an assigned
    group ID. All the consumers with the same group ID act as a single logical consumer.
    Each message of a topic is delivered to one consumer from a consumer group (with
    the same group ID). Different consumer groups for a particular topic can process
    messages at their own pace as messages are not removed from the topics as soon
    as they are consumed. In fact, it is the responsibility of the consumers to keep
    track of how many messages they have consumed.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier, each message in a partition has a unique sequence number
    associated with it called an offset. It is through this offset that consumers
    know how much of the stream they have already processed. If a consumer decides
    to replay already processed messages, all it needs to do is just set the value
    of an offset to an earlier value before consuming messages from Kafka.
  prefs: []
  type: TYPE_NORMAL
- en: Broker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The broker receives the messages from the producer (push mechanism) and delivers
    the messages to the consumer (pull mechanism). Brokers also manage the persistence
    of messages in a file. Kafka brokers are very lightweight: they only open file
    pointers on a queue (topic partitions) and manage TCP connections.'
  prefs: []
  type: TYPE_NORMAL
- en: Data retention
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Each topic in Kafka has an associated retention time. When this time expires,
    Kafka deletes the expired data file for that particular topic. This is a very
    efficient operation as it's a file delete operation.
  prefs: []
  type: TYPE_NORMAL
- en: Installation of Kafka brokers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the time of writing, the stable version of Kafka is 0.9.x.
  prefs: []
  type: TYPE_NORMAL
- en: The prerequisites for running Kafka are a ZooKeeper ensemble and Java Version
    1.7 or above. Kafka comes with a convenience script that can start a single node
    ZooKeeper but it is not recommended to use it in a production environment. We
    will be using the ZooKeeper cluster we deployed in [Chapter 2](part0034.html#10DJ40-6149dd15e07b443593cc93f2eb31ee7b),
    *Storm Deployment, Topology Development, and Topology Options*.
  prefs: []
  type: TYPE_NORMAL
- en: We will see how to set up a single node Kafka cluster first and then how to
    add two more nodes to it to run a full-fledged, three node Kafka cluster with
    replication enabled.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a single node Kafka cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Following are the steps to set up a single node Kafka cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the Kafka 0.9.x binary distribution named `kafka_2.10-0.9.0.1.tar.gz`
    from [http://apache.claz.org/kafka/0.9.0.1/kafka_2.10-0.9.0.1.tgz](http://apache.claz.org/kafka/0.9.0.1/kafka_2.10-0.9.0.1.tgz).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Extract the archive to wherever you want to install Kafka with the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We will refer to the Kafka installation directory as `$KAFKA_HOME` from now
    on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Change the following properties in the `$KAFKA_HOME/config/server.properties`
    file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here, `zoo1`, `zoo2`, and `zoo3` represent the hostnames of the ZooKeeper nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the definitions of the important properties in the `server.properties`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '`broker.id`: This is a unique integer ID for each of the brokers in a Kafka
    cluster.'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '`port`: This is the port number for a Kafka broker. Its default value is `9092`.
    If you want to run multiple brokers on a single machine, give a unique port to
    each broker.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`host.name`: The hostname to which the broker should bind and advertise itself.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`log.dirs`: The name of this property is a bit unfortunate as it represents
    not the log directory for Kafka, but the directory where Kafka stores the actual
    data sent to it. This can take a single directory or a comma-separated list of
    directories to store data. Kafka throughput can be increased by attaching multiple
    physical disks to the broker node and specifying multiple data directories, each
    lying on a different disk. It is not much use specifying multiple directories
    on the same physical disk, as all the I/O will still be happening on the same
    disk.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num.partitions`: This represents the default number of partitions for newly
    created topics. This property can be overridden when creating new topics. A greater
    number of partitions results in greater parallelism at the cost of a larger number
    of files.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`log.retention.hours`: Kafka does not delete messages immediately after consumers
    consume them. It retains them for the number of hours defined by this property
    so that in the event of any issues the consumers can replay the messages from
    Kafka. The default value is `168` hours, which is 1 week.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`zookeeper.connect`: This is the comma-separated list of ZooKeeper nodes in
    `hostname:port` form.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Start the Kafka server by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If you get something similar to the preceding three lines on your console, then
    your Kafka broker is up-and-running and we can proceed to test it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we will verify that the Kafka broker is set up correctly by sending and
    receiving some test messages. First, let''s create a verification topic for testing
    by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s verify if the topic creation was successful by listing all the topics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The topic is created; let''s produce some sample messages for the Kafka cluster.
    Kafka comes with a command-line producer that we can use to produce messages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Write the following messages on your console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s consume these messages by starting a new console consumer on a new console
    window:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now, if we enter any message on the producer console, it will automatically
    be consumed by this consumer and displayed on the command line.
  prefs: []
  type: TYPE_NORMAL
- en: '**Using Kafka''s single node ZooKeeper** If you don''t want to use an external
    ZooKeeper ensemble, you can use the single node ZooKeeper instance that comes
    with Kafka for quick and dirty development. To start using it, first modify the
    `$KAFKA_HOME/config/zookeeper.properties` file to specify the data directory by
    supplying following property:'
  prefs: []
  type: TYPE_NORMAL
- en: '`dataDir=/var/zookeeper`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, you can start the Zookeeper instance with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '`> ./bin/zookeeper-server-start.sh config/zookeeper.properties`'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a three node Kafka cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far we have a single node Kafka cluster. Follow the steps to deploy the
    Kafka cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a three node VM or three physical machines.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform steps 1 and 2 mentioned in the section *Setting up a single node Kafka
    cluster*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Change the following properties in the file `$KAFKA_HOME/config/server.properties`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Make sure that the value of the `broker.id` property is unique for each Kafka
    broker and the value of `zookeeper.connect` must be the same on all nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start the Kafka brokers by executing the following command on all three boxes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s verify the setup. First we create a topic with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will list the topics to see if the topic was created successfully:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will verify the setup by using the Kafka console producer and consumer
    as done in the *Setting up a single node Kafka cluster* section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Write the following messages on your console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s consume these messages by starting a new console consumer on a new console
    window:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'So far, we have three brokers on the Kafka cluster working. In the next section,
    we will see how to write a producer that can produce messages to Kafka:'
  prefs: []
  type: TYPE_NORMAL
- en: Multiple Kafka brokers on a single node
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you want to run multiple Kafka brokers on a single node, then follow the
    following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Copy `config/server.properties` to create `config/server1.properties` and `config/server2.properties`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Populate the following properties in `config/server.properties`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Populate the following properties in `config/server1.properties`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Populate the following properties in `config/server2.properties`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the following commands on three different terminals to start Kafka brokers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Share ZooKeeper between Storm and Kafka
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can share the same ZooKeeper ensemble between Kafka and Storm as both store
    the metadata inside the different znodes (ZooKeeper coordinates between the distributed
    processes using the shared hierarchical namespace, which is organized similarly
    to a standard file system. In ZooKeeper, the namespace consisting of data registers
    is called znodes).
  prefs: []
  type: TYPE_NORMAL
- en: We need to open the ZooKeeper client console to view the znodes (shared namespace)
    created for Kafka and Storm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to `ZK_HOME` and execute the following command to open the ZooKeeper console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following command to view the list of znodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Here, consumers, `isr_change_notification`, and brokers are the znodes and the
    Kafka is managing its metadata information into ZooKeeper at this location.
  prefs: []
  type: TYPE_NORMAL
- en: Storm manages its metadata inside the Storm znodes in ZooKeeper.
  prefs: []
  type: TYPE_NORMAL
- en: Kafka producers and publishing data into Kafka
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section we are writing a Kafka producer that will publish events into
    the Kafka topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following step to create the producer:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a Maven project by using `com.stormadvance` as `groupId` and `kafka-producer`
    as `artifactId`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add the following dependencies for Kafka in the `pom.xml` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the following `build` plugins to the `pom.xml` file. It will let us execute
    the producer using Maven:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will create the `KafkaSampleProducer` class in the `com.stormadvance.kafka_producer`
    package. This class will produce each word from the first paragraph of Franz Kafka''s
    Metamorphosis into the `new_topic` topic in Kafka as single message. The following
    is the code for the `KafkaSampleProducer` class with explanations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, before running the producer, we need to create `new_topic` in Kafka. To
    do so, execute the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can run the producer by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let us verify that the message has been produced by using Kafka''s console
    consumer and executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: So, we are able to produce messages into Kafka. In the next section, we will
    see how we can use `KafkaSpout` to read messages from Kafka and process them inside
    a Storm topology.
  prefs: []
  type: TYPE_NORMAL
- en: Kafka Storm integration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we will create a Storm topology that will consume messages from the Kafka
    topic `new_topic` and aggregate words into sentences.
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete message flow is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00053.gif)'
  prefs: []
  type: TYPE_IMG
- en: We have already seen `KafkaSampleProducer`, which produces words into the Kafka
    broker. Now we will create a Storm topology that will read those words from Kafka
    to aggregate them into sentences. For this, we will have one `KafkaSpout` in the
    application that will read the messages from Kafka and two bolts, `WordBolt` that
    receive words from `KafkaSpout` and then aggregate them into sentences, which
    are then passed onto the `SentenceBolt`, which simply prints them on the output
    stream. We will be running this topology in a local mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow the steps to create the Storm topology:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a new Maven project with `groupId` as `com.stormadvance` and `artifactId`
    as `kafka-storm-topology`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add the following dependencies for Kafka-Storm and Storm in the `pom.xml` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the following Maven plugins to the `pom.xml` file so that we are able to
    run it from the command-line and also to package the topology to be executed in
    Storm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will first create the `WordBolt` that will aggregate the words into
    sentences. For this, create a class called `WordBolt` in the `com.stormadvance.kafka` package.
    The code for `WordBolt` is as follows, complete with explanation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Next is `SentenceBolt`, which just prints the sentences that it receives. Create
    `SentenceBolt` in the `com.stormadvance.kafka` package. The code is as follows,
    with explanations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will create the `KafkaTopology` that will define the `KafkaSpout` and
    wire it with `WordBolt` and `SentenceBolt`. Create a new class called `KafkaTopology`
    in the `com.stormadvance.kafka` package. The code is as follows, with explanations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Now we will the run the topology. Make sure the Kafka cluster is running and
    you have executed the producer in the last section so that there are messages
    in Kafka for consumption.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the topology by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'This will execute the topology. You should see messages similar to the following
    in your output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: So we are able to consume messages from Kafka and process them in a Storm topology.
  prefs: []
  type: TYPE_NORMAL
- en: Deploy the Kafka topology on Storm cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The deployment of Kafka and Storm integration topology on the Storm cluster
    is similar to the deployment of other topologies. We need to set the number of
    workers and the maximum spout pending Storm config and we need to use the `submitTopology`
    method of `StormSubmitter` to submit the topology on the Storm cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need to build the topology code as mentioned in the following steps
    to create a JAR of the Kafka Storm integration topology:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to project home.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Execute the command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, copy the Kafka Storm topology on the Nimbus machine and execute the following
    command to submit the topology on the Storm cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command runs `TopologyMainClass` with the argument. The main function
    of `TopologyMainClass` is to define the topology and submit it to Nimbus. The
    Storm JAR part takes care of connecting to Nimbus and uploading the JAR part.
  prefs: []
  type: TYPE_NORMAL
- en: 'Log in on the Storm Nimbus machine and execute the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Here, `~/ storm-kafka-topology-0.0.1-SNAPSHOT-jar-with-dependencies.jar` is
    the path of the `KafkaTopology` JAR we are deploying on the Storm cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about the basics of Apache Kafka and how to use
    it as part of a real-time stream processing pipeline build with Storm. We learned
    about the architecture of Apache Kafka and how it can be integrated into Storm
    processing by using `KafkaSpout`.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to cover the integration of Storm with Hadoop
    and YARN. We are also going to cover sample examples for this operation.
  prefs: []
  type: TYPE_NORMAL
