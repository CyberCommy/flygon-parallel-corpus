- en: Chapter 8. Deploying Microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are going to deploy microservices. We will use different
    technologies in order to provide the reader with the knowledge required to choose
    the right tool for every job. First, we will use PM2 and its deployment capabilities
    to run applications in remote servers. Then, we will play around Docker, which
    is one of the most advanced deployment platforms, and the entire ecosystem around
    containers. In this chapter, we will show how to automate all the deployments
    as highly as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Concepts in software deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Deployments are usually the ugly friend of the **Software Development Life
    Cycle** (**SDLC**) party. There is a missing contact point between development
    and systems administration that DevOps is going to solve in the next few years.
    The cost of fixing bugs at different stages of SDLC is shown in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Concepts in software deployment](img/B04889_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This diagram shows the cost of fixing a bug, depending on the stage of the SDLC
  prefs: []
  type: TYPE_NORMAL
- en: Fail early is one of my favorite concepts in the lean methodology. In the change
    management world, the cost of fixing a bug in the different stages of the software
    life cycle is called the **cost of change curve**.
  prefs: []
  type: TYPE_NORMAL
- en: Roughly, fixing a bug in production is estimated to cost 150 times the resources
    as compared to the costs to fix it when taking requirements.
  prefs: []
  type: TYPE_NORMAL
- en: No matter what the figure is, which depends a lot on the methodology and technology
    that we use, the lesson learned is that we can save a lot of time by catching
    bugs early.
  prefs: []
  type: TYPE_NORMAL
- en: From the continuous integration up to continuous delivery, the process should
    be automated as much as possible, where *as much as possible* means 100%. Remember,
    humans are imperfect and more prone to errors while carrying out manual repetitive
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous integration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Continuous integration** (**CI**) is the practice of integrating the work
    from different branches on daily basis (or more than once a day) and validating
    that the changes do not break existing features by running integration and unit
    tests.'
  prefs: []
  type: TYPE_NORMAL
- en: CI should be automated using the same infrastructure configuration as we will
    be using later in pre-production and production, so if there is any flaw, it can
    be caught early.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous delivery
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Continuous delivery** (**CD**) is a software engineering approach that aims
    to build small, testable, and easily deployable pieces of functionality that can
    be delivered seamlessly at any time.'
  prefs: []
  type: TYPE_NORMAL
- en: This is what we are aiming for with the microservices. Again, we should be pushing
    to automate the delivery process as, if we are doing it manually, we are only
    looking for problems.
  prefs: []
  type: TYPE_NORMAL
- en: When talking from the microservices' perspective, automation on deployments
    is the key. We need to tackle the overhead of having a few dozen of services instead
    of a few machines, or we can find ourselves maintaining a cloud of services instead
    of adding value to our company.
  prefs: []
  type: TYPE_NORMAL
- en: '**Docker** is our best ally here. With Docker, we are reducing the hassle of
    deploying a new software to pretty much moving a file (a container) around in
    different environments, as we will see later in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: Deployments with PM2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PM2 is an extremely powerful tool. No matter what stage of development we are
    in, PM2 always has something to offer.
  prefs: []
  type: TYPE_NORMAL
- en: In this phase of software development, the deployment is where PM2 really shines.
    Through a JSON configuration file, PM2 will manage a cluster of applications so
    that we can easily deploy, redeploy, and manage applications on remote servers.
  prefs: []
  type: TYPE_NORMAL
- en: PM2 – ecosystems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'PM2 calls a group of applications ecosystem. Every ecosystem is described by
    a JSON file, and the easiest way to generate it is executing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This should output something similar to the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The content of the `ecosystem.json` file varies, depending on the version of
    PM2, but what this file contains is the skeleton of a PM2 cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This file contains two applications configured for two environments. We are
    going to modify this skeleton to adapt it to our needs, modeling our entire ecosystem
    written in [Chapter 4](ch04.html "Chapter 4. Writing Your First Microservice in
    Node.js"), *Writing Your First Microservice in Node.js*.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, for now, let''s explain a bit for the configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have an array of applications (`apps`) defining two apps: API and WEB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As you can see, we have a few configuration parameters for each app:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`name`: This is the name of the application'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`script`: This is the startup script of the app'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`env`: These are the environment variables to be injected into the system by
    PM2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`env_<environment>`: This is same as `env`, but it is tailored per environment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are two environments defined in the default ecosystem under the `deploy`
    key, as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`production`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dev`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, between these two environments, there are no significant changes
    except for the fact that we are configuring one environment variable in development
    and the folder where we deploy our application.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying microservices with PM2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [Chapter 4](ch04.html "Chapter 4. Writing Your First Microservice in Node.js"),
    *Writing Your First Microservice in Node.js*, we wrote a simple e-commerce in
    order to show the different concepts and common catches in microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we are going to learn how to deploy them using PM2.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the server
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First thing we need to do in order to deploy software with PM2 is to configure
    the remote machine and local machine to be able to talk using SSH, with a public/private
    key schema.
  prefs: []
  type: TYPE_NORMAL
- en: 'The way of doing it is easy, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Generate one RSA key
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Install it into the remote server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s do it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'That should produce something similar to the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, if we go to the folder indicated in the preceding output, we can find
    the following two files:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pm2_rsa`: The first one, `pm2_rsa`, is your private key. As you can read from
    the name, no one should have access to this key as they may steal your identity
    in the servers that trust this key.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pm2_rsa.pub`: The `pm2_rsa.pub` is your public key. This key can be handed
    over to anyone so that using asymmetric cryptography techniques, they can verify
    your identity (or who you say you are).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'What we are going to do now is copy the public key to the remote server so
    that when our local machine PM2 tries to talk to the server, it knows who we are
    and let''s get into the shell without password:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The last step is to register your private key as a known identity in your local
    machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: That's about it.
  prefs: []
  type: TYPE_NORMAL
- en: From now on, whenever you SSH into the remote server using as a user `youruser`,
    you won't need to enter the password in order to get into the shell.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once this configuration is done, there is very little to do in order to deploy
    any application into this server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The first command will configure everything needed to accommodate the app. The
    second command will actually deploy the application itself as we configured earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Docker – a container for software delivery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Virtualization has been one of the biggest trends in the past few years. Virtualization
    enables the engineer to share the hardware across different software instances.
    Docker is not really a virtualization software, but it is conceptually the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'With a pure virtualization solution, a new OS runs on top of a hypervisor sitting
    on top of an existing operating system (host OS). Running the full OS means that
    we can be consuming a few gigabytes of hard drive in order to replicate the full
    stack from the kernel to the filesystem, which usually consumes a good chunk of
    resources. The structure of a virtualization solution is shown in the following
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Docker – a container for software delivery](img/B04889_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Layers diagram for a virtual machine environment
  prefs: []
  type: TYPE_NORMAL
- en: 'With Docker, we only replicate the filesystem and binaries so that there is
    no need to run the full stack of the OS where we don''t need it. Docker images
    are usually a few hundreds of megabytes, instead of gigabytes, and they are quite
    lightweight, therefore, we can run some containers on the same machine. The previous
    structure using Docker is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Docker – a container for software delivery](img/B04889_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Layers diagram for Docker
  prefs: []
  type: TYPE_NORMAL
- en: With Docker, we also eliminate one of the biggest problems of software deployment,
    that is, **the** **configuration management**.
  prefs: []
  type: TYPE_NORMAL
- en: We are switching a complicated per-environment configuration management, where
    we need to worry about how the application is deployed/configured into a container
    that is basically like a software package that can be installed in any Docker-ready
    machine.
  prefs: []
  type: TYPE_NORMAL
- en: The only Docker-ready OS nowadays is Linux, as Docker needs to make use of the
    advanced kernel features, forcing Windows and Mac users to run a virtual machine
    with Linux in order to provide support to run Docker containers.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the container
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Docker comes with a very powerful and familiar way (for developers) of configuring
    the containers.
  prefs: []
  type: TYPE_NORMAL
- en: You can create containers based on an existing image (there are thousands of
    images on the Internet) and then modify the image to fulfil your needs by adding
    new software packages or altering the filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: Once we are satisfied with it, we can use the new version of the image to create
    our containers using a version control system similar to **Git**.
  prefs: []
  type: TYPE_NORMAL
- en: However, we need to understand how Docker works first.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Docker
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As it was mentioned before, Docker needs a virtual machine to provide support
    on Mac and Windows, therefore, the installation on these systems may vary. The
    best way to install Docker on your system is to go to the official website and
    follow the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.docker.com/engine/installation/](https://docs.docker.com/engine/installation/)'
  prefs: []
  type: TYPE_NORMAL
- en: At the moment, it is a very active project, so you can expect changes every
    few weeks.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the image
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'By default, Docker comes with no images. We can verify this by running `docker
    images` on the terminal, which will produce an output very similar to the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Choosing the image](img/B04889_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: It is an empty list. There are no images stored in the local machine. The first
    thing we need to do is search for an image. In this case, we are going to use
    **CentOS** as our base for creating the images. CentOS is very close to Red Hat
    Enterprise Linux, which seems to be one of the most extended distributions of
    Linux in the industry. They provide great support and there is plenty of information
    available on the Internet to troubleshoot problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s search for a CentOS image as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Choosing the image](img/B04889_08_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, there is a long list of images based on CentOS, but only the
    first one is official.
  prefs: []
  type: TYPE_NORMAL
- en: This list of images is coming from something called the **Registry** in the
    Docker world. A Docker Registry is a simple repository of images available to
    the general public. You can also run your own Registry in order to prevent your
    images from going to the general one.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'More information can be found at the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.docker.com/registry/](https://docs.docker.com/registry/)'
  prefs: []
  type: TYPE_NORMAL
- en: There is one column in the table in the preceding screenshot that should have
    caught your attention almost immediately, the column called **STARS**. This column
    represents the rating given by the users for a given image. We can narrow the
    search based on the number of stars that the users have given to an image by using
    the `-s` flag.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you run the following command, you will see a list of images rated with
    1000 or more stars:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Be careful with the images you choose, there is nothing preventing a user to
    create an image with malicious software.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to fetch the CentOS image to the local machine, we need to run the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The output produced will be very similar to the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Choosing the image](img/B04889_08_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Once the command finishes, if we run Docker images again, we can see that **centos**
    is now appearing in the following list:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Choosing the image](img/B04889_08_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As we specified earlier, Docker does not use the full image, but it uses a reduced
    version of it, only virtualizing the last few layers of the OS. You can clearly
    see it, as the size of the image is not even 200 MB, which for a full version
    of CentOS, can go up to a few GB.
  prefs: []
  type: TYPE_NORMAL
- en: Running the container
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we have a copy of the image in our local machine, it is time to run
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Running the container](img/B04889_08_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the prompt of the terminal has changed to something like `root@debd09c7aa3b`,
    which means that we are inside the container.
  prefs: []
  type: TYPE_NORMAL
- en: From now on, every single command that we run will be executed inside a contained
    version of CentOS Linux.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is another interesting command in Docker:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'If we run this command in a new terminal (without exiting from the running
    container), we will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Running the container](img/B04889_08_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This output is self explanatory; it is an easy way to see what is going on in
    our Docker containers.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the required software
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s install Node.js in the container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This command will pull and execute the setup script for Node.js.
  prefs: []
  type: TYPE_NORMAL
- en: 'This will produce an output very similar to the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Installing the required software](img/B04889_08_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Follow the instructions, as this will install node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'It is highly recommended to install the development tools, as the installation
    process of a few modules requires a compilation step. Let''s do it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Once the command finishes, we are ready to run the node applications inside
    our container.
  prefs: []
  type: TYPE_NORMAL
- en: Saving the changes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the Docker world, an image is the configuration for a given container. We
    can use the image as a template to run as many containers as we want, however
    first, we need to save the changes made in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: If you are a software developer, you probably are familiar with control version
    systems such as CVS, Subversion, or Git. Docker was built with their philosophy
    in mind—a container can be treated like a versionable software component and then
    changes can be committed.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to do it, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This command will show a list of containers that have run in the past, as shown
    in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Saving the changes](img/B04889_08_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In my case, there are few containers, but the interesting one in this case is
    the second; this is where Node.js is installed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need to commit the status of the container in order to create a new
    image with our changes. We do it by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s explain the command:'
  prefs: []
  type: TYPE_NORMAL
- en: The `-a` flag indicates the author. In this case, `dgonzalez`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following parameter is `container id`. As we indicated earlier, the second
    container has the corresponding ID `62e7336a4627`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third parameter is a combination of the name given to the new image and
    the tag of the image. The tagging system can be very powerful when we are dealing
    with quite a few images, as it can get really complicated to identify small variations
    between them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It might take a few seconds, but after finishing, the output of the command
    must be very similar to the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Saving the changes](img/B04889_08_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This is the indication that we have a new image in our list with our software
    installed. Run `docker images` again and the output will confirm it, as shown
    in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Saving the changes](img/B04889_08_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In order to run a container based on the new image, we can run the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This will give us access to the shell in the container, and we can confirm that
    Node.js is installed by running `node -v`, which should output the version of
    Node, in this case, 4.2.4.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Node.js applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, it is time to deploy Node.js applications inside the container. In order
    to do it, we are going to need to expose the code from our local machine to the
    Docker container.
  prefs: []
  type: TYPE_NORMAL
- en: 'The correct way of doing it is by mounting a local folder in the Docker machine,
    but first, we need to create the small application to be run inside the container,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: It is a simple application, using Express that basically renders `Hello Earth!`
    into a browser. If we run it from a terminal and we access `http://localhost:80/hello`,
    we can see the results.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we are going to run it inside the container. In order to do it, we are
    going to mount a local folder as a volume in the Docker container and run it.
  prefs: []
  type: TYPE_NORMAL
- en: Docker comes from the experience of sysadmins and developers that have lately
    melted into a role called DevOps, which is somewhere in between them. Before Docker,
    every single company had its own way of deploying apps and managing configurations,
    so there was no consensus on how to do things the right way.
  prefs: []
  type: TYPE_NORMAL
- en: Now with Docker, the companies have a way to provide uniformity to deployments.
    No matter what your business is, everything is reduced to build the container,
    deploy the application, and run the container in the appropriate machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s assume that the application is in the `/apps/test/` folder. Now, in
    order to expose it to the container, we run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, Docker can get very verbose with parameters, but let''s explain
    them, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The `-i` and `-t` flags are familiar to us. They capture the input and send
    the output to a terminal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `-v` flag is new. It specifies a volume from the local machine and where
    to mount it in the container. In this case, we are mounting `/apps/test` from
    the local machine into `/test_app`. Please note the colon symbol to separate the
    local and the remote path.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `-p` flag specifies the port on the local machine that will expose the remote
    port in the container. In this case, we expose the port `3000` in the container
    through the port `8000` in the Docker machine, so accessing `docker-machine:8000`
    from the host machine will end up accessing the port `3000` in the container.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `centos-microservices:1.0` is the name and tag of the image that we have
    created in the preceding section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `/bin/bash` is the command that we want to execute inside the container.
    The `/bin/bash` is going to give us access to the prompt of the system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If everything worked well, we should have gotten access to the system prompt
    inside the container, as shown in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deploying Node.js applications](img/B04889_08_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see in the image, there is a folder called `/test_app` that contains
    our previous application, called `small-script.js`.
  prefs: []
  type: TYPE_NORMAL
- en: Now, it is time to access to the app, but first, let's explain how Docker works.
  prefs: []
  type: TYPE_NORMAL
- en: Docker is written in **Go**, which is a modern language created by Google, grouping
    all the benefits from a compiled language such as C++ with all the high-level
    features from a modern language such as Java.
  prefs: []
  type: TYPE_NORMAL
- en: It is fairly easy to learn and not hard to master. The philosophy of Go is to
    bring all the benefits of an interpreted language, such as reducing the compilation
    times (the complete language can be compiled in under a minute) to a compiled
    language.
  prefs: []
  type: TYPE_NORMAL
- en: Docker uses very specific features from the Linux kernel that forces Windows
    and Mac users to use a virtual machine to run Docker containers. This machine
    used to be called **boot2docker**, but the new version is called **Docker Machine**,
    which contains more advanced features such as deploying containers in remote virtual
    machines. For this example, we will only use the local capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Given that, if you run the app from within the container located in the `/test_app/`
    folder, and you are in Linux, accessing `http://localhost:8000/`, it would be
    enough to get into the application.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you are using Mac or Windows, Docker is running either in the Docker Machine
    or boot2docker so that the IP is given by this virtual machine, which is shown
    when the Docker terminal starts, as shown in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deploying Node.js applications](img/B04889_08_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the IP is `192.168.99.100`, so in order to access our application,
    we need to visit the `http://192.168.99.100:9000/` URL.
  prefs: []
  type: TYPE_NORMAL
- en: Automating Docker container creation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you remember, in the previous chapters, one of the most important concepts
    was automation. Automation is the key when working with microservices. Instead
    of operating one server, you probably will need to operate few dozens, reaching
    a point where you are almost fully booked on day-to-day tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Docker designers had that in mind when allowing the users to create containers
    from a script written in a file called `Dockerfile`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have ever worked on coding C or C++, even in college, you are probably
    familiar with `Makefiles`. A `Makefile` file is a script where the developer specifies
    the steps to automatically build a software component. Here is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The preceding `Makefile` contains a list of tasks and dependencies to be executed.
    For example, if we execute `make clean` on the same folder where the `Makefile`
    file is contained, it will remove the executable and all the files ending with
    `o`.
  prefs: []
  type: TYPE_NORMAL
- en: '`Dockerfile`, unlike `Makefile`, is not a list of tasks and dependencies (even
    though the concept is the same), it is a list of instructions to build a container
    from scratch to a ready status.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: These small few preceding lines are enough to build a container having Node.js
    installed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s explain it in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we choose the base image. In this case, it is `centos` as we used before.
    For doing this, we use the `FROM` command and then the name of the image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MAINTAINER` specifies the name of the person who created the container. In
    this case, it is `David Gonzalez`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RUN`, as its name indicates, runs a command. In this case, we use it twice:
    once to add the repository to `yum`, and then to install Node.js.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dockerfiles can contain a number of different commands. The documentation for
    them is pretty clear, but let''s take a look at the most common (aside from the
    ones seen before):'
  prefs: []
  type: TYPE_NORMAL
- en: '`CMD`: This is similar to run, but it actually gets executed after building
    the command. `CMD` is the command to be used to start an application once the
    container is instantiated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WORKDIR`: This is to be used in conjunction with `CMD`, it is the command
    used to specify the work directory for the next `CMD` command.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ADD`: This command is used to copy files from the local filesystem to the
    container instance filesystem. In the previous example, we can use `ADD` to copy
    the application from the host machine into the container, run `npm install` with
    the `CMD` command, and then run the app once again with the `CMD` command. It
    can also be used to copy the content from a URL to a destination folder inside
    the container.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ENV`: This is used to set environment variables. For example, you could specify
    a folder to store files uploaded by passing an environment variable to the container,
    as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '`EXPOSE`: This is used to expose ports to the rest of the containers in your
    cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As you can see, the **domain-specific language** (**DSL**) of `Dockerfiles`
    is quite rich and you can pretty much build every system required. There are hundreds
    of examples on the Internet to build pretty much everything: MySQL, MongoDB, Apache
    servers, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: It is strongly recommended to create containers through `Dockerfiles`, as it
    can be used as a script to replicate and make changes to the containers in the
    future, as well as being able to automatically deploy our software without much
    manual intervention.
  prefs: []
  type: TYPE_NORMAL
- en: Node.js event loop – easy to learn and hard to master
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We all know that Node.js is a platform that runs applications in a single-threaded
    way; so, why don't we use multiple threads to run applications so that we can
    get the benefit of multicore processors?
  prefs: []
  type: TYPE_NORMAL
- en: Node.js is built upon a library called **libuv**. This library abstracts the
    system calls, providing an asynchronous interface to the program that uses it.
  prefs: []
  type: TYPE_NORMAL
- en: I come from a very heavy Java background, and there, everything is synchronous
    (unless you are coding with some sort of non-blocking libraries), and if you issue
    a request to the database, the thread is blocked and resumed once the database
    replies with the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'This usually works fine, but it presents an interesting problem: a blocked
    thread is consuming resources that could be used to serve other requests. The
    event loop of Node.js is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Node.js event loop – easy to learn and hard to master](img/B04889_08_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This is the Node.js event loop diagram
  prefs: []
  type: TYPE_NORMAL
- en: JavaScript is, by default, an event-driven language. It executes the program
    that configures a list of event handlers that will react to given events, and
    after that, it just waits for the action to take place.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at a very familiar example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Then the JavaScript code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, this is a very simple example. HTML that shows a button and
    snippet of JavaScript code that, using JQuery, shows an alert box when the button
    is clicked.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the key: *when the button is clicked*.'
  prefs: []
  type: TYPE_NORMAL
- en: Clicking a button is an event, and the event is processed through the event
    loop of JavaScript using a handler specified in the JavaScript.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of the day, we only have one thread executing the events, and we
    never talk about parallelism in JavaScript, the correct word is concurrency. So,
    being more concise, we can say that Node.js programs are highly concurrent.
  prefs: []
  type: TYPE_NORMAL
- en: Your application will always be executed in only one thread, and we need to
    keep that in mind while coding.
  prefs: []
  type: TYPE_NORMAL
- en: If you have been working in Java or .NET or any other language/frameworks designed
    and implemented with thread-blocking techniques, you might have observed that
    Tomcat, when running an application, spawns a number of threads listening to the
    requests.
  prefs: []
  type: TYPE_NORMAL
- en: In the Java world, each of these threads are called **workers**, and they are
    responsible to handle the request from a given user from the beginning to the
    end. There is one type of data structure in Java that takes the benefit of it.
    It is called **ThreadLocal** and it stores the data in the local thread so that
    it can be recovered later on. This type of storage is possible because the thread
    that starts the request is also responsible to finish it, and if the thread is
    executing any blocking operation (such as reading a file or accessing a database),
    it will wait until it is completed.
  prefs: []
  type: TYPE_NORMAL
- en: This is usually not a big deal, but when your software relies heavily on I/O,
    the problems can become serious.
  prefs: []
  type: TYPE_NORMAL
- en: Another big point in favor of the non-blocking model of Node.js is the lack
    of context switch.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the CPU switches one thread with another, what happens is that all the
    data in the registers, and other areas of the memory, is stacked and allows the
    CPU to switch the context with a new process that has its own data to be placed
    in there, as shown in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Node.js event loop – easy to learn and hard to master](img/B04889_08_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This is a diagram showing context switching in threads from the theoretical
    point of view.
  prefs: []
  type: TYPE_NORMAL
- en: 'This operation takes time, and this time is not used by the application. It
    simply gets lost. In Node.js, your application runs in only one thread, so there
    is no such context switching while running (it is still present in the background,
    but hidden to your program). In the following image, we can see what happens in
    the real world when a CPU switches a thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Node.js event loop – easy to learn and hard to master](img/B04889_08_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This is a diagram showing context switching in threads from the practical (shows
    the dead times) point of view.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering Node.js applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By now, you know how Node.js applications work, and certainly, some of the readers
    may have a question that if the app runs on a single thread, then what happens
    with the modern multicore processors?
  prefs: []
  type: TYPE_NORMAL
- en: Before answering this question, let's take a look at the following scenario.
  prefs: []
  type: TYPE_NORMAL
- en: 'When I was in high school, there was a big technology leap in CPUs: the segmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It was the first attempt to introduce parallelism at the instruction level.
    As you probably are aware, the CPU interprets assembler instructions and each
    of these instructions are composed of a number of phases, as shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Clustering Node.js applications](img/B04889_08_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Before the Intel 4x86, the CPUs were executing one instruction at the time,
    so taking the instruction model from the preceding diagram, any CPU could only
    execute one instruction every six CPU cycles.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, the segmentation came into play. With a set of intermediate registers,
    the CPU engineers managed to parallelize the individual phases of instructions
    so that in the best-case scenario, the CPUs are able to execute one instruction
    per cycle (or nearly), as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Clustering Node.js applications](img/B04889_08_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The image describes the execution of instructions in a CPU with a segmented
    pipeline
  prefs: []
  type: TYPE_NORMAL
- en: This technical improvement led to faster CPUs and opened the door to native
    hardware multithreading, which led to the modern n-core processors that can execute
    a large number of parallel tasks, but when we are running Node.js applications,
    we only use one core.
  prefs: []
  type: TYPE_NORMAL
- en: If we don't cluster our app, we are going to have a serious performance degradation
    when compared to other platforms that take the benefit of the multiple cores of
    a CPU.
  prefs: []
  type: TYPE_NORMAL
- en: However, this time we are lucky, PM2 already allows you to cluster Node.js apps
    to maximize the usage of your CPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Also, one of the important aspects of PM2 is that it allows you to scale applications
    without any downtime.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s run a simple app in the cluster mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This time we have used the native HTTP library for Node.js in order to handle
    the incoming HTTP requests.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can run the application from the terminal and see how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Although it does not output anything, we can curl to the `http://localhost:3000/`
    URL in order to see how the server responds, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Clustering Node.js applications](img/B04889_08_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, Node.js has managed all the HTTP negotiation and it has also
    managed to reply with the `Here we are!` phrase as it was specified in the code.
  prefs: []
  type: TYPE_NORMAL
- en: This service is quite trivial, but it is the principle on which more complex
    web services work, so we need to cluster the web service to avoid bottlenecks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Node.js has one library called `cluster` that allows us to programmatically
    cluster our application, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Personally, I find it much easier to use specific software such as PM2 to accomplish
    effective clustering, as the code can get really complicated while trying to handle
    the clustered instances of our app.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given this, we can run the application through PM2 as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '![Clustering Node.js applications](img/B04889_08_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The `-i` flag in PM2, as you can see in the output of the command, is used to
    specify the number of cores that we want to use for our application.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we run `pstree`, we can see the process tree in our system and check whether
    PM2 is running only one process for our app, as shown in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Clustering Node.js applications](img/B04889_08_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this case, we are running the app in only one process, so it will be allocated
    in one core of the CPU.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we are not taking advantage of the multicore capabilities of the
    CPU that is running the app, but we still get the benefit of restarting the app
    automatically if one exception bubbles up from our algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we are going to run our application using all the cores available in our
    CPU so that we maximize the usage of it, but first, we need to stop the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '![Clustering Node.js applications](img/B04889_08_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: PM2, after stopping all the services
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we are in a position to rerun the application using all the cores of our
    CPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '![Clustering Node.js applications](img/B04889_08_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: PM2 showing four services running in a cluster mode
  prefs: []
  type: TYPE_NORMAL
- en: 'PM2 has managed to guess the number of CPUs in our computer, in my case, this
    is an iMac with four cores, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Clustering Node.js applications](img/B04889_08_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see in `pstree`, PM2 started four threads at the OS level, as shown
    in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Clustering Node.js applications](img/B04889_08_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: When clustering an application, there is an unwritten rule about the number
    of cores that an application should be using and this number is the number of
    cores minus one.
  prefs: []
  type: TYPE_NORMAL
- en: The reason behind this number is the fact that the OS needs some CPU power so
    that if we use all the CPUs in our application, once the OS starts carrying on
    with some other tasks, it will force context switching as all the cores will be
    busy and this will slow down the application.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing our application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, clustering our app is not enough and we need to scale our application
    horizontally.
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of ways to horizontally scale an app. Nowadays, with cloud
    providers such as Amazon, every single provider has implemented their own solution
    with a number of features.
  prefs: []
  type: TYPE_NORMAL
- en: One of my preferred ways of implementing the load balancing is using **NGINX**.
  prefs: []
  type: TYPE_NORMAL
- en: NGINX is a web server with a strong focus on the concurrency and low memory
    usage. It is also the perfect fit for Node.js applications as it is highly discouraged
    to serve static resources from within a Node.js application. The main reason is
    to avoid the application from being under stress due to a task that could be done
    better with another software, such as NGINX (which is another example of specialization).
  prefs: []
  type: TYPE_NORMAL
- en: 'However, let''s focus on load balancing. The following figure shows how NGINX
    works as a load balancer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Load balancing our application](img/B04889_08_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see in the preceding diagram, we have two PM2 clusters load balanced
    by an instance of NGINX.
  prefs: []
  type: TYPE_NORMAL
- en: The first thing we need to do is know how NGINX manages the configuration.
  prefs: []
  type: TYPE_NORMAL
- en: On Linux, NGINX can be installed via `yum`, `apt-get`, or any other package
    manager. It can also be built from the source, but the recommended method, unless
    you have very specific requirements, is to use a package manager.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, the main configuration file is `/etc/nginx/nginx.conf`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: This file is pretty straightforward, it specifies the number of workers (remember,
    processes to serve requests), the location of error logs, number connections that
    a worker can have active at the time, and finally, the HTTP configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last line is the most interesting one: we are informing NGINX to use `/etc/nginx/sites-enabled/*.conf`
    as potential configuration files.'
  prefs: []
  type: TYPE_NORMAL
- en: With this configuration, every file ending in `.conf` under the specified folder
    is going to be part of the NGINX configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, there is a default file already existing there. Modify it to
    look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This is all the configuration we need to build a load balancer. Let''s explain
    it in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The `upstream app` directive is creating a group of services called `app`. Inside
    this directive, we specify two servers as we've seen in the previous image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `server` directive specifies to NGINX that it should be listening to all
    the requests from port `80` and passing them to the group of upstream called `app`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, how does NGINX decide to send the request to which computer?
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we could specify the strategy used to spread the load.
  prefs: []
  type: TYPE_NORMAL
- en: By default, NGINX, when there is not a balancing method specifically configured,
    uses **Round Robin**.
  prefs: []
  type: TYPE_NORMAL
- en: One thing to bear in mind is that if we use round robin, our application should
    be stateless as we won't be always hitting the same machine, so if we save the
    status in the server, it might not be there in the following call.
  prefs: []
  type: TYPE_NORMAL
- en: Round Robin is the most elementary way of distributing load from a queue of
    work into a number of workers; it rotates them so that every node gets the *same
    amount of requests*.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are other mechanisms to spread the load, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '**Least connected**, as its name indicates, sends the request to the least
    connected node, equally distributing the load between all the nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '**IP hashing** is an interesting way of distributing the load. If you have
    ever worked with any web application, the concept of sessions is something present
    in almost any application. In order to remember who the user is, the browser sends
    a cookie to the server, which has stored who the user is in memory and what data
    he/she needs/can be accessed by that given user. The problem with the other type
    of load balancing is that we are not guaranteed to always hit the same server.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we are using Least connected as a policy for balancing, we could
    hit the server one in the first load, but then hit a different server on subsequent
    redirections that will result in the user not being displayed with the right information
    as the second server won't know who the user is.
  prefs: []
  type: TYPE_NORMAL
- en: With IP hashing, the load balancer will calculate a hash for a given IP. This
    hash will somehow result in a number from 1 to *N*, where *N* is the number of
    servers, and then, the user will always be redirected to the same machine as long
    as they keep the same IP.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also apply a weight to the load balancing, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: This will distribute the load in such way that, for every six requests, five
    will be directed to the first machine and one will be directed to the second machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have chosen our preferred load balancing method, we can restart NGINX
    for the changes to take effect, but first, we want to validate them as shown in
    the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Load balancing our application](img/B04889_08_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the configuration test can be really helpful in order to avoid
    configuration disasters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once NGINX has passed `configtest`, it is guaranteed that NGINX will be able
    to `restart/start/reload` without any syntax problem, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Reload will gracefully wait until the old threads are done, and then, reload
    the configuration and route the new requests with the new configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are interested in learning about NGINX, I found the following official
    documentation of NGINX quite helpful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://nginx.org/en/docs/](http://nginx.org/en/docs/)'
  prefs: []
  type: TYPE_NORMAL
- en: Health check on NGINX
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Health checking is one of the important activities on a load balancer. What
    happens if one of the nodes suffers a critical hardware failure and is unable
    to serve more requests?
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, NGINX comes with two types of health checks: **passive** and
    **active**.'
  prefs: []
  type: TYPE_NORMAL
- en: Passive health check
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here, NGINX is configured as a reverse proxy (as we did in the preceding section).
    It reacts to a certain type of response from the upstream servers.
  prefs: []
  type: TYPE_NORMAL
- en: If there is an error coming back, NGINX will mark the node as faulty, removing
    it from the load balancing for a certain period of time before reintroducing it*.*
    With this strategy, the number of failures will be drastically reduced as NGINX
    will be constantly removing the node from the load balancer.
  prefs: []
  type: TYPE_NORMAL
- en: There are a few configurable parameters, such as `max_fails` or `fail_timeout`,
    where we can configure the amount of failures required to mark a node as invalid
    or the time out for requests.
  prefs: []
  type: TYPE_NORMAL
- en: Active health check
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Active health checks, unlike passive health checks, actively issue connections
    to the upstream servers to check whether they are responding correctly to the
    experiencing problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most simplistic configuration for active health checks in NGINX is the
    following one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'There are two new lines in this config file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`health_check`: This enables the active health check. The default configuration
    is to issue a connection every five seconds to the host and port specified in
    the `upstream` section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`zone app test`: This is required by the NGINX configuration when enabling
    the health check.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a wide range of options to configure more specific health checks, and
    all of them are available in NGINX configuration that can be combined to satisfy
    the needs of different users.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned a wide range of technologies that we can use to
    deploy microservices. By now you know how to build, deploy, and configure software
    components in such a way that we are able to homogenize a very diverse range of
    technologies. The objective of this book is to provide you the concepts required
    to start working with microservices and enable the reader to know how to look
    for the needed information.
  prefs: []
  type: TYPE_NORMAL
- en: Personally, I have struggled to find a book that provides a summary of all the
    aspects of the life cycle of microservices and I really hope that this book covers
    this empty space.
  prefs: []
  type: TYPE_NORMAL
