- en: Multithreading and Synchronization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Embedded platforms span a vast landscape of computing power. There are microcontrollers
    with just a few kilobytes of memory; there are powerful **systems-on-chip** (**SoCs**)
    with gigabytes of memory; there are multi-core CPUs capable of running many applications
    at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: With more computational resources available for embedded developers, and more
    complex applications they can build on top of them, multithreading support has
    become very important. Developers need to know how to parallelize their applications
    to efficiently utilize all CPU cores. We will learn how to write applications
    that can utilize all available CPU cores in an efficient and safe way.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring thread support in C++
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring data synchronization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using condition variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using atomic variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the C++ memory model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring lock-free synchronization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using atomic variables in shared memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring async functions and futures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These recipes can be used as examples of building your own efficient multithreading
    and multiprocessing synchronization code.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring thread support in C++
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Prior to C++11, threads were completely out of the scope of C++ as a language.
    Developers could use platform-specific libraries, such as pthreads or the Win32
    **application programming interface** (**API**). Since each library has its own
    behavior, porting applications to another platform required significant development
    and testing efforts.
  prefs: []
  type: TYPE_NORMAL
- en: C++11 introduced threads as part of the C++ standard and defined a set of classes
    to create multithreaded applications in its standard library.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to use C++ to spawn multiple concurrent threads
    in a single application.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to create two worker threads that run concurrently.
  prefs: []
  type: TYPE_NORMAL
- en: In your `~/test` working directory, create a subdirectory called `threads`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use your favorite text editor to create a `threads.cpp` file in the `threads` subdirectory.
    Copy the code snippet into the `threads.cpp` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a file called `CMakeLists.txt` in the `loop` subdirectory, with the
    following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You can build and run the application.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this application, we defined a function called `worker`. To keep the code
    simple, it does not do much useful work, only printing `Worker X` starts and `Worker
    X` ends 10 times, with 50 milliseconds' delay between the messages.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `main` function, we create two worker threads, `worker1` and `worker2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We pass two parameters into the thread constructors:'
  prefs: []
  type: TYPE_NORMAL
- en: A function that runs in the thread.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A parameter for the function. Since we pass the previously defined `worker`
    function as a thread function, the parameter should match its type—in our case,
    it is `int`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This way, we defined two worker thread that do the same job but have different
    indices—`1` and `2`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The threads start running immediately as soon as they are created; there is
    no need to call any additional methods to start them. They are executed completely
    concurrently, as we can see from the program output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5772496b-0c8a-4c02-96c1-b9494da7fe2c.png)'
  prefs: []
  type: TYPE_IMG
- en: The output from our worker thread is mixed, and sometimes garbled, such as `Worker
    Worker 1 ends2 ends`. This happens because output to the Terminal is also working
    concurrently.
  prefs: []
  type: TYPE_NORMAL
- en: Since worker threads are executed independently, the main thread has nothing
    to do after creating the worker thread. However, if the execution of the main
    thread reaches the end of the `main` function, the program terminates. To avoid
    this, we added calls to the `join` method for each of our worker threads. This
    method blocks until the thread terminates. This way, we exit the main program
    only after both of the worker threads complete their work.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring data synchronization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data synchronization is an important aspect of any application that deals with
    multiple execution threads. Different threads often need to access the same variables
    or memory regions. Writing to the same memory at the same time by two or more
    independent threads can result in data corruption. Even reading the variable at
    the same time when it is being updated by another thread is dangerous, since it
    can be only partially updated at the moment of the read.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid these issues, concurrent threads can use so-called synchronization
    primitives, the API that makes access to the shared memory deterministic and predictable.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the case with thread support, the C++ language did not provide any
    synchronization primitives prior to the C++11 standard. Starting with C++11, a
    number of synchronization primitives were added into the C++ standard library
    as part of the standard.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to synchronize access to a variable, using
    a mutex and a lock guard.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the preceding recipe, we learned how to run two worker threads completely
    concurrently and noticed that it can lead to garbled output to the Terminal. We
    are going to modify the code from the preceding recipe to add synchronization,
    using a mutex and a lock guard, and see the difference.
  prefs: []
  type: TYPE_NORMAL
- en: In your `~/test` working directory, create a subdirectory called `mutex`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use your favorite text editor to create a `mutex.cpp` file in the `mutex` subdirectory.
    Copy the code snippet into the `mutex.cpp` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a file called `CMakeLists.txt` in the `loop` subdirectory, with the
    following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: You can build and run the application.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After we build and run our application, we can see that its output is similar
    to the output of the thread application. However, there are also noticeable differences:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/36c850c0-19b0-49b4-a851-d1878279476c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Firstly, the output is not garbled. Secondly, we can see a clear order—no worker
    is interrupted by another worker, and each begin is followed by the corresponding
    end. The difference lies in the highlighted fragments of the source code. We create
    a global `mutex m`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Then, we use `lock_guard` to protect our critical section of code, which starts
    from the line that prints `Worker X begins` and ends at the line that prints `Worker
    X ends`.
  prefs: []
  type: TYPE_NORMAL
- en: '`lock_guard` is a wrapper on top of a mutex that uses an **RAII **(short for **Resource
    Acquisition Is Initialization**) technique to automatically lock the corresponding
    mutex in the constructor when the lock object is defined, and unlock it in the
    destructor after reaching the end of its scope. That is why we add extra curly
    braces to define the scope of our critical section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Though it is possible to lock and unlock the mutex explicitly, by calling its
    lock and unlock methods, it is not recommended. Forgetting to unlock a locked
    mutex leads to multithreading synchronization issues that are hard to detect and
    hard to debug. The RAII approach unlocks mutexes automatically, making code safer,
    easier to read, and easier to understand.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Proper implementation of thread synchronization requires a lot of attention
    to detail and thorough analysis. A very common problem in multithreaded applications
    is a deadlock. This is a situation whereby a thread is blocked because it is waiting
    for another thread that, in turn, is blocked because it is waiting for the first
    thread. As a result, two threads are blocked infinitely.
  prefs: []
  type: TYPE_NORMAL
- en: A deadlock occurs if two or more mutexes are required for synchronization. C++17 
    introduced   *std::scoped_lock*, available at [https://en.cppreference.com/w/cpp/thread/scoped_lock](https://en.cppreference.com/w/cpp/thread/scoped_lock)
    an RAII wrapper for multiple mutexes that helps to avoid deadlocks.
  prefs: []
  type: TYPE_NORMAL
- en: Using condition variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We learned how to synchronize simultaneous access to the same variable from
    two or more threads. The particular order in which threads accessed the variable
    was not important; we only prevented simultaneous reads and writes to the variable.
  prefs: []
  type: TYPE_NORMAL
- en: A thread waiting for another thread to start processing data is a common scenario.
    In this case, the second thread should be notified by the first thread when the
    data is available. It can be done using condition variables, supported by C++,
    starting from the C++11 standard.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to use condition variables to activate data
    processing in a separate thread as soon as the data is available.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are going to implement an application with two worker threads, similar to
    the application we created in the *Exploring data synchronization* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: In your `~/test` working directory, create a subdirectory called `condvar`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use your favorite text editor to create a `condv.cpp` file in the `condvar` subdirectory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we put the required headers and define global variables in `condvar.cpp`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'After the global variables are defined, we add our `worker` function, which
    is similar to the `worker` function from the preceding recipes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we define our entry point—the `main` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a file called `CMakeLists.txt` in the `loop` subdirectory, with the
    following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: You can build and run the application.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similarly to the application that we created in the *Exploring data synchronization*
    recipe, we create two worker threads, `worker1` and `worker2`, that use the same
    `worker` function thread and differ only by the `index` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides printing messages to the console, the worker thread update a global
    vector result. Each worker just adds its index into the `result` variable in its
    loop, as shown in the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We want each worker to add its index to the result only on its turn— `worker
    1`, then `worker 2`, then `worker 1` again, and so on. It is not possible to do
    this without synchronization; however, simple synchronization using mutexes is
    not sufficient. It can guarantee that two concurrent threads will not access the
    same critical section of the code at the same time, but cannot guarantee the order.
    It is possible that `worker 1` will lock the mutex again before `worker 2` locks
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 'To solve the ordering problem, we define a `cv` condition variable and a `next` integer
    variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The `next` variable contains an index of the worker. It is initialized with
    `0` and set to a specific worker index in the `main` function. Since this variable
    is accessed from multiple threads, we do it under the protection of the lock guard:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Though the worker threads start executing after their creation, both of them
    are immediately blocked on the condition variables, waiting until the value of
    the `next` variable matches their index. Condition variables need `std::unique_lock`
    for waiting. We create it right before calling the `wait` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Though the condition variable `cv` was set to  `1` in the `main` function,
    it is not enough. We need to explicitly notify threads waiting on the condition
    variable. We do this using the `notify_all` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This wakes up all waiting threads, and they compare their index against the
    `next` variable. The matching thread unblocks, and all other threads go to sleep
    again.
  prefs: []
  type: TYPE_NORMAL
- en: 'The active thread writes a message to the console and updates the `result`
    variable. Then, it updates the `next` variable to choose a thread that will be
    activated next. We increment the index until it reaches the maximum value, then
    reset it to `1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Similar to the case with the code in the `main` function, after the index of
    the `next` thread is decided, we need to invoke `notify_all` to wake all threads
    up and let them decide whose turn it is to work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'While the worker threads work, the `main` function waits for their completion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'When all worker threads complete, the value of the `result` variable  is printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'After we build and run our program, we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e9547f19-9f61-4307-bed0-e7fa66406e5a.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, all threads were activated in the expected order.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we only used a few methods provided by the condition variable
    object. Besides the simple `wait` function, there are functions for waiting for
    a specific time or waiting until a specified time point is reached. Learn more
    about the *C++ condition variable class* at its [https://en.cppreference.com/w/cpp/thread/condition_variable](https://en.cppreference.com/w/cpp/thread/condition_variable)
    reference page.
  prefs: []
  type: TYPE_NORMAL
- en: Using atomic variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Atomic variables are named as such because they cannot be read or written partially.
    Compare, for example, the `Point` and `int` data types:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, modification of the `p` variable is equivalent to two assignments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This means that any concurrent thread reading the `p` variable can get partially
    modified data, such as `x=10`, `y=0`, which can lead to incorrect calculations
    that are hard to detect and hard to reproduce. That is why access to such data
    types should be synchronized.
  prefs: []
  type: TYPE_NORMAL
- en: 'How about the `b` variable? Can it be modified partially? The answer is: yes,
    depending on the platform. However, C++ provides a set of data types and templates
    to ensure that a variable changes all at once, as a whole, atomically.'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to use atomic variables for the synchronization
    of multiple threads. Since  atomic variables cannot be modified partially, there
    is no need to use mutexes or other expensive synchronization primitives.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will create an application that spawns two worker threads to concurrently
    update an array of data. Instead of mutexes, we will use atomic variables to make
    sure the concurrent updates are safe.
  prefs: []
  type: TYPE_NORMAL
- en: In your `~/test` working directory, create a subdirectory called `atomic`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use your favorite text editor to create an `atomic.cpp` file in the `atomic` subdirectory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we put the required headers, and define global variables in `atomic.cpp`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'After global variables are defined, we add our `worker` function. It resembles
    the `worker` function from the preceding recipes, but besides an `index`, it has
    an additional parameter—`timeout`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we define our entry point— the `main` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a file called `CMakeLists.txt` in the `loop` subdirectory, with the
    following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: You can build and run the application.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are creating an application that updates all elements of an array using multiple
    worker threads. For expensive update operations, this approach can result in substantial
    performance gains on a multi-core platform.
  prefs: []
  type: TYPE_NORMAL
- en: The difficulty is sharing the work between multiple worker threads, given that
    each of them may require a different amount of time to process a data element.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use a `shared_index` atomic variable to store an index of the next element
    that has not yet been claimed by any of the worker threads. This variable, along
    with the array to be processed, is declared as a global variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Our `worker` function resembles the `worker` function from earlier recipes but
    has important differences. Firstly, it has an additional parameter, `timeout`.
    This is used to simulate differences in the time required to process each element.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, instead of a fixed number of iterations, our worker threads run in
    a loop until the  `shared_index` variable reaches the maximum value. This indicates
    that all elements were processed, and the worker can terminate.
  prefs: []
  type: TYPE_NORMAL
- en: On each iteration, a worker reads the value of `shared_index`. If there are
    elements to process, it stores the value of the `shared_index` variable in a local `worker_index` variable and
    increments the `shared_index` variable at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Though it is possible to use an atomic variable in the same way as a regular
    variable—first, get its current value, and then increment the variable—it can
    lead to a race condition. Both worker threads can read the variable at almost
    the same time. In this case, both of them get the same value, then start processing
    the same element, interfering with each other. That is why we use a special method, `fetch_add`,
    which increments the variable and returns the value it had before the increment
    as a single, non-interruptible action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'If the `worker_index` variable reaches the size of the array, it means that
    all elements were processed, and the worker can terminate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'If the `worker_index` variable is valid, it is used by the worker to update
    the value of the array element by this index. In our case, we just multiply it
    by `2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'To simulate expensive data operation, we use a custom delay. The duration of
    the delay is determined by the `timeout` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `main` function, we add elements to process into the data vector. We
    use a loop to populate the vector with numbers from zero to nine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'After the initial dataset is ready, we create two worker threads, providing
    the `index` and the `timeout` parameters. Different timeouts of the worker thread
    are used to simulate different performances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we wait till both worker threads complete their jobs, and print the result
    to the console. When we build and run our application, we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c34579d8-b62e-4c9e-bcbe-a4441c2d5e89.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, `Worker 2` has processed more elements than `Worker 1` because
    its timeout was 20 milliseconds, compared to the 50 milliseconds of `Worker 1`.
    Also, all elements were processed without omissions and repetitions, as intended.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We learned how to work with integer atomic variables. Though this type of atomic
    variable is the most commonly used, C++ allows atomic variables of other types
    to be defined as well, including non-integral types, given that they are trivially
    copyable, copy constructible, and copy assignable.
  prefs: []
  type: TYPE_NORMAL
- en: Besides the `fetch_add` method we used in our example, atomic variables have
    other similar methods that help developers to query the value and modify the variable
    in a single operation. Consider using these methods to avoid race conditions or
    expensive synchronization using mutexes.
  prefs: []
  type: TYPE_NORMAL
- en: In C++20, atomic variables receive `wait`, `notify_all`, and `notify_one` methods,
    similar to the methods of condition variables. They allow implementation of the
    logic that previously required condition variables by using much more efficient
    and lightweight atomic variables.
  prefs: []
  type: TYPE_NORMAL
- en: More information about atomic variables can be found at [https://en.cppreference.com/w/cpp/atomic/atomic](https://en.cppreference.com/w/cpp/atomic/atomic).
  prefs: []
  type: TYPE_NORMAL
- en: Using the C++ memory model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Beginning with the C++11 standard, C++ defined an API and primitives for threads
    and synchronization as part of the language. Memory synchronization in a system
    that has multiple processor cores is complicated because modern processors can
    optimize code execution by reordering instructions. Even when using atomic variables,
    there is no guarantee that the data is modified or accessed in the desired order,
    since the order can be changed by a compiler.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid ambiguity, C++11 introduced the memory model, defining the behavior
    of the concurrent access to the memory region. As part of the memory model, C++
    defined the `std::memory_order` enum, which gives hints to a compiler regarding
    the intended model of access. This helps the compiler to optimize the code in
    a way that does not interfere with the intended code behavior.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to use the simplest form of the `std::memory_order` enum to
    implement a shared counter variable.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are implementing an application that has a shared counter that is incremented
    by two concurrent worker threads.
  prefs: []
  type: TYPE_NORMAL
- en: In your `~/test` working directory, create a subdirectory called `memorder`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use your favorite text editor to create a `memorder.cpp` file in the `atomic` subdirectory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we put the required headers and define global variables in `memorder.cpp`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'After global variables are defined, we add our `worker` function. The function
    only increments a counter, and then sleeps for a specific time interval:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we define our `main` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a file called `CMakeLists.txt` in the `loop` subdirectory, with the
    following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: You can build and run the application.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our application, we are going to create two worker threads that will increment
    a shared counter, and let them run for a specific amount of time.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a first step, we define two global atomic variables, `running` and `counter`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The `running` variable is a binary flag. When it is set to `true`, the worker
    threads should keep running. After it changes to `false`, the worker threads should
    terminate.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `counter` variable is our shared counter. The worker threads will concurrently
    increment it. We use the `fetch_add` method that we already used in the *Using
    atomic variables* recipe. It is used to increment a variable atomically. In this
    recipe, we pass an additional argument, `std::memory_order_relaxed`, to this method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: This argument is a hint. While consistency in atomicity and modification is
    important and should be guaranteed for an implementation of a counter, the order
    among concurrent memory accesses is not that important. `std::memory_order_relaxed` defines
    this kind of memory access for atomic variables. Passing it into the `fetch_add` method
    allows us to fine-tune it for a particular target platform, to avoid unneeded
    synchronization delays that can affect performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `main` function, we create two worker threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, the main thread is paused for 1 second. After the pause, the main thread
    sets the value of the `running` variable to `false`, indicating that the worker
    threads should terminate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'After the worker threads terminate, we print the value of the counter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a33e028f-a5f8-4fe4-b857-23fd84788a3a.png)'
  prefs: []
  type: TYPE_IMG
- en: The resulting counter value is determined by the timeout intervals passed to
    the `worker` functions. Changing the type of memory order in the `fetch_add` method
    does not result in a noticeable change in the resulting value in our example.
    However, it can result in the better performance of highly concurrent applications
    that use atomic variables, because a compiler can reorder operations in concurrent
    threads without breaking the application logic. This kind of optimization is highly
    dependent on a developer's intents, and cannot be inferred automatically without
    hints from the developer.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The C++ memory model and memory ordering types are complex topics that require
    a deep understanding of how modern CPUs access memory and optimize their code
    execution. *C++ Memory Model reference* , [https://en.cppreference.com/w/cpp/language/memory_model](https://en.cppreference.com/w/cpp/language/memory_model) provides
    lots of information and is a good starting point to learn advanced techniques
    for the optimization of multithreaded applications.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring lock-free synchronization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the preceding recipes, we learned how to synchronize access of multiple threads
    to shared data, using mutexes and locks. If several threads try to run critical
    sections of the code protected by a lock, only one thread at a time can do it.
    All other threads have to wait until that thread leaves the critical section.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, however, it is possible to synchronize access to shared data
    without mutexes and explicit locks. The idea is to use a local copy of data for
    modification, and then update the shared copy in a single, uninterruptible, and
    undividable operation.
  prefs: []
  type: TYPE_NORMAL
- en: This type of synchronization depends on the hardware. Target processors should
    provide some form of **Compare And Swap** (**CAS**) instruction. This checks whether
    the value in a memory location matches a given value, and replaces it with a new
    given value only if they match. Since it is a single-processor instruction, it
    cannot be interrupted by a context switch. This makes it a basic building block
    for more complex atomic operations.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to check whether an atomic variable is lock-free
    or implemented using mutexes or other locking operations. We will also implement
    a lock-free push operation for a custom stack, based on the example for the atomic
    compare-exchange family of functions for C++11, available at [https://en.cppreference.com/w/cpp/atomic/atomic_compare_exchange](https://en.cppreference.com/w/cpp/atomic/atomic_compare_exchange)
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are implementing a simple `Stack` class that provides a constructor and a
    function named `Push`.
  prefs: []
  type: TYPE_NORMAL
- en: In your `~/test` working directory, create a subdirectory called `lockfree`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use your favorite text editor to create a `lockfree.cpp` file in the `lockfree` subdirectory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we put in the required headers, and define a `Node` helper data type in
    the  `lockfree.cpp` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define a simple `Stack` class. This uses the `Node` data type to organize
    data storage:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we define a simple `main` function that creates an instance of `Stack`
    and pushes an element into it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a file called `CMakeLists.txt` in the `loop` subdirectory, with the
    following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: You can build and run the application.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We created a simple application that implements a simple stack of integer values.
    We store elements of the stack in dynamic memory, and for each element, we should
    be able to determine the elements that follow it.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this purpose, we define a `Node` helper structure that has two data fields.
    The `data` field stores the actual value of an element, while the `next` field
    is a pointer to the next element in the stack:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we define the `Stack` class. Normally, a stack implies two operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Push`: to place an element on top of the stack'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Pull`: to fetch an element from the top of the stack'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To track the top of the stack, we create a `top` variable that holds a pointer
    to the `Node` object. It will be the top of our stack:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'We also define a simple constructor that initializes the value of our `top`
    variable and checks whether it is lock-free or not. In C++, atomic variables can
    be implemented using atomic **Consistency, Availability, and Partition tolerance**
    (**CAP**) operations or using regular mutexes. It depends on the target CPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: In our application, we implement only the `Push` method, to demonstrate how
    it can be done in a lock-free way.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Push` method accepts a value to put on top of the stack. To do this, we
    create a new instance of the `Node` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Since we put the element on the top of the stack, the pointer to the newly created
    instance should be assigned to the `top` variable, and the old value of the `top` variable
    should be assigned to the `next` pointer of our new `Node` object.
  prefs: []
  type: TYPE_NORMAL
- en: However, doing it directly is not thread-safe. Two or more threads can modify
    the `top` variable simultaneously, causing data corruption. We need some kind
    of data synchronization. We can do this using locks and mutexes, but it is also
    possible to do it in a lock-free way.
  prefs: []
  type: TYPE_NORMAL
- en: 'That is why we initially update only the next pointer. Since our new `Node`
    object is not yet part of the stack, we can do it without synchronization, since
    other threads do not have access to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we need to add it as a new `top` variable of the stack. We do this using
    a loop over the `std::atomic_compare_exchange_weak` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: This function compares the value of the `top` variable to the value stored in
    the `next` pointer of the new element. If they match, it replaces the value of
    the `top` variable with the pointer to the new node and returns `true`. Otherwise,
    it writes the value of the `top` variable into the `next` pointer of the new element
    and returns `false`. Since we updated the `next` pointer to match the `top` variable
    on the next step, this can only happen if another thread modified it before the
    `std::atomic_compare_exchange_weak` function was invoked. Eventually, the function
    will return `true`, indicating that the `top` header is updated with the pointer
    to our element.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `main` function creates an instance of stack and pushes one element to
    it. In the output, we can see if the underlying implementation is lock-free or
    not:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1c7151c3-b9d3-44d2-afb2-8a5caa5119f2.png)'
  prefs: []
  type: TYPE_IMG
- en: For our target, the implementation is lock-free.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Lock-free synchronization is an extremely complex topic. The development of
    lock-free data structures and algorithms requires lots of effort. Even the implementation
    of simple `Push` logic using lock-free operations is not easy to understand. An
    even larger effort is needed for proper analysis and debugging of your code. Often,
    it can lead to subtle issues that are hard to notice and hard to implement.
  prefs: []
  type: TYPE_NORMAL
- en: Though the implementation of a lock-free algorithm can improve the performance
    of your application, consider using one of the existing libraries of lock-free
    data structures instead of writing of your own. For example, [Boost.Lockfree](https://www.boost.org/doc/libs/1_66_0/doc/html/lockfree.html)
    provides a collection of lock-free data types for you to use.
  prefs: []
  type: TYPE_NORMAL
- en: Using atomic variables in shared memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We learned how to use atomic variables for the synchronization of two or more
    threads in a multithreaded application. However, atomic variables can also be
    used to synchronize independent applications that run as separate processes.
  prefs: []
  type: TYPE_NORMAL
- en: We already know how to use shared memory for exchanging data between two applications.
    Now, we can combine these two techniques—shared memory and atomic variables—to
    implement both the data exchange and synchronization of two independent applications.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will modify an application we created in [Chapter 6](ce2d6580-cf8f-42ca-bd14-de8d3265d07e.xhtml), *Memory
    Management*, for exchanging data between two processors using a shared memory
    region.
  prefs: []
  type: TYPE_NORMAL
- en: In your `~/test` working directory, create a subdirectory called `shmatomic`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use your favorite text editor to create a `shmatomic.cpp` file in the `shmatomic` subdirectory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We reuse the shared memory data structure we created in the `shmem` application.
    Put the common headers and constants into the `shmatomic.cpp` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, start defining the templated `SharedMem` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The class will have a constructor, a destructor, and a getter method. Let''s
    add the constructor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The simple destructor and the getter follow:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we define the data type we will use for data exchange and synchronization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define a function that will generate data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'It is followed by the function that consumes the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we add our `main` function, which ties everything together:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a file called `CMakeLists.txt` in the `loop` subdirectory, with the
    following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: You can build and run the application.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our application, we reuse the templated `SharedMem` class we introduced in
    [Chapter 6](ce2d6580-cf8f-42ca-bd14-de8d3265d07e.xhtml), *Memory Management*.
    This class is used to store an element of a specific type in a shared memory region.
    Let's quickly recap how it works.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `SharedMem` class is a wrapper on top of the **Portable Operating System
    Interface** (**POSIX**) shared memory API. It defines three private data fields
    to hold system-specific handlers and pointers, and exposes a public interface
    consisting of two functions:'
  prefs: []
  type: TYPE_NORMAL
- en: A constructor function that accepts the name of a shared region and the ownership
    flag
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `get` method that returns a reference to the object stored in shared memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The class also defines a destructor that performs all operations needed to properly
    close the shared object. As a result, the `SharedMem` class can be used for safe
    resource management using the C++ RAII idiom.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `SharedMem` class is a templated class. It is parameterized by the data
    type we want to store in the shared memory. For this purpose, we define a structure
    called `Payload`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: It has an `index` integer variable that we are going to use as a data exchange
    field, and two atomic Boolean flags, `data_ready` and `data_processed`, that are
    used for data synchronization.
  prefs: []
  type: TYPE_NORMAL
- en: We also define two functions, `producer` and `consumer`, that will work in separate
    processes and exchange data between each other using a shared memory region.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `producer` function is producing data chunks. Firstly, it creates an instance
    of the `SharedMem` class, parametrized by the `Payload` data type. It passes a
    path to the shared memory region to the `SharedMem` constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'After the shared memory instance is created, it gets the reference to the payload
    data stored there and checks whether any of the atomic flags we defined in the
    `Payload` data type are lock-free:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'The function produces 10 chunks of data in a loop. An index of the chunk is
    put into the `index` field of the payload:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: However, besides putting the data into shared memory, we need to synchronize
    access to this data. This is when we use our atomic flags.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each iteration, before updating the `index` field, we reset the `data_processed`
    flag. After the index is updated, we set the `data ready` flag, which is an indicator
    to the consumer that a new chunk of data is ready, and wait till the data is processed
    by the consumer. We loop until the `data_processed` flag becomes `true`, and then
    go to the next iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'The `consumer` function works in a similar way. Since it works in a separate
    process, it opens the same shared memory region by creating an instance of the
    `SharedMem` class using the same path. We also make the `consumer` function the
    owner of the shared memory instance. It means it is responsible for removing the
    shared memory region after its instance of `SharedMem` is destroyed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Similarly to the `producer` function, the `consumer` function checks whether
    an atomic flag is lock-free, and enters the loop of data consumption.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each iteration, it waits in a tight loop until the data is ready:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'After the `producer` function sets the `data_ready` flag to `true`, the `consumer`
    function can safely read and process data. In our implementation, it only prints
    the `index` field to the console. After the data is processed, the `consumer`
    function indicates this by setting the `data_processed` flag to `true`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'This triggers the next iteration of data production on the `producer` function
    side:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/96155edc-9e5c-42dc-b8d6-46969182a299.png)'
  prefs: []
  type: TYPE_IMG
- en: As a result, we can see a deterministic output of processed data chunks, with
    no omissions or duplications; this is common in cases where data access is not
    synchronized.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring async functions and futures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dealing with data synchronization in multithreaded applications is hard, error-prone,
    and requires developers to write a lot of code to properly align data exchange
    and data notifications. In order to simplify development,  C++11 introduced a
    standard API for writing asynchronous code in a way that resembles regular synchronous
    function calls and hides lots of the synchronization complexities under the hood.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to use asynchronous function invocations and
    futures to run our code in multiple threads with virtually no extra effort, for
    data synchronization.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will implement a simple application that invokes a long-running function
    in a separate thread and waits for its result. While the function is running,
    the application can keep working on other calculations.
  prefs: []
  type: TYPE_NORMAL
- en: In your `~/test` working directory, create a subdirectory called `async`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use your favorite text editor to create an `async.cpp` file in the `async` subdirectory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Put the code of our application into the `async.cpp` file, starting from the
    common headers and our long-running function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, add the `test` function, which invokes the long-running function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, add a `main` minimalistic function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a file called `CMakeLists.txt` in the `loop` subdirectory, with the
    following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: You can build and run the application.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In our application, we defined a `calculate` function that should take a long
    time to run. Technically, our function calculates the square of an integer argument,
    but we added an artificial delay to make it run for 1 second. We use a `sleep_for` standard
    library function to add a delay to the application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: Besides calculations, the function logs to the console when it started working,
    when it completed, and how much time it took.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we defined a `test` function that invokes the `calculate` function, to
    demonstrate how asynchronous invocation works.
  prefs: []
  type: TYPE_NORMAL
- en: The function has two parameters. The first parameter is a value that is passed
    to the `calculate` function. The second parameter is the amount of time the `test`
    function is going to spend after running the `calculate` function and before requesting
    the result. This way, we model the useful work the function can perform in parallel
    to the calculations it requested.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `test` function starts working by running the `calculate` function in asynchronous
    mode and passing it the first parameter, `value`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: The `async` function implicitly spawns a thread and starts the execution of
    the `calculate` function.
  prefs: []
  type: TYPE_NORMAL
- en: Since we run the function asynchronously, the result is not yet ready. Instead,
    the `async` function returns an instance of `std::future`, an object that will
    hold the result when it is available.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we simulate the useful work. In our case, it is the pause for the specified
    interval of time. After the work that can be done in parallel is completed, we
    need to get the result of the `calculate` function to proceed. To request the
    result, we use the `get` method of our `std::future` object, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: The `get` method blocks until the result is available. Then, we can calculate
    the amount of time we have spent waiting for the result, and output the result—along
    with the wait time—to the console.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `main` function, we run the `test` function to evaluate two scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: The useful work takes less time than the calculation of the result.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The useful work takes more time than the calculation of the result.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running the application produces the following output.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first scenario, we can see that we are starting the calculations, and
    then started waiting for the result before the calculation has been completed.
    As a result, the `get` method blocked for 600 milliseconds until the result was
    ready:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/46a609c8-dcd7-4286-b46c-dc2341addc93.png)'
  prefs: []
  type: TYPE_IMG
- en: In the second scenario, the useful work took `1200` milliseconds. As we can
    see, the calculation has been done before the result was requested, and because
    of that, the `get` method did not block, and immediately returned the result.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Futures and async functions provide a powerful mechanism for writing parallel
    and understandable code. Async functions are flexible and support different execution
    policies. Promises are another mechanism that enables developers to overcome the
    complexities of asynchronous programming. More information can be found in the
    reference pages for `std::future` at [[https://en.cppreference.com/w/cpp/thread/future](https://en.cppreference.com/w/cpp/thread/future)], 
    `std::promise` at [[https://en.cppreference.com/w/cpp/thread/promise](https://en.cppreference.com/w/cpp/thread/promise)],
    and `std::async` at [[https://en.cppreference.com/w/cpp/thread/async](https://en.cppreference.com/w/cpp/thread/async)].
  prefs: []
  type: TYPE_NORMAL
