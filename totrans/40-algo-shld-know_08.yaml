- en: Unsupervised Machine Learning Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter is about unsupervised machine learning algorithms. The chapter
    starts with an introduction to unsupervised learning techniques. Then, we will
    learn about two clustering algorithms: k-means clustering and hierarchical clustering
    algorithms. The next section looks at a dimensionality reduction algorithm, which
    may be effective when we have a large number of input variables. The following
    section shows how unsupervised learning can be used for anomaly detection. Finally,
    we will look at one of the most powerful unsupervised learning techniques, association
    rules mining. This section also explains how patterns discovered from association
    rules mining represent interesting relationships between the various data elements
    across transactions that can help us in our data-driven decision making.'
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, the reader should be able to understand how unsupervised
    learning can be used to solve some real-world problems. The reader will understand
    the basic algorithms and methodologies that are currently being used for unsupervised
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dimensionality reduction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anomaly detection algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Association rules mining
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing unsupervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The simplest definition of unsupervised learning is that it is the process
    of providing some sort of structure to unstructured data by discovering and utilizing
    the inherent patterns of the data.  If data is not produced by some random process,
    it will have some patterns between its data elements in its multidimensional problem
    space. Unsupervised learning algorithms work by discovering these patterns and
    using them to provide some structure to the dataset.  This concept is shown in
    the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/32f60651-b8ff-4ec0-96fa-7b7145366467.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that unsupervised learning adds structure by discovering new features from
    the existing patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning in the data-mining life cycle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To understand the role of unsupervised learning, it is important to first look
    at the overall life cycle of the data-mining process. There are different methodologies
    that divide the life cycle of the data-mining process into different independent
    stages, called  **phases**. Currently, there are two popular ways to represent
    the data-mining life cycle:'
  prefs: []
  type: TYPE_NORMAL
- en: '**CRISP-DM**  (**Cross-Industry Standard Process for Data Mining**) life cycle'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SEMMA**  (**Sample, Explore, Modify, Model, Access**) data-mining process'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CRISP-DM was developed by a consortium of data miners who belonged to various
    companies, including Chrysler and  **SPSS**  (**Statistical Package for Social
    Science**). SEMMA was proposed by  **SAS** (**Statistical Analysis System**).
    Let's look at one of these two representations of the data-mining life cycle,
    CRISP-DM, and try to understand the place of unsupervised learning in the data-mining
    life cycle. Note that SEMMA has somewhat similar phases within its life cycle.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we look at the CRISP-DM life cycle, we can see that it consists of six distinct
    phases, which are shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ff5d5601-03c9-465c-92dd-b830b77cd130.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s understand each phase one by one:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Phase 1: Business Understanding**: This is about gathering the requirements
    and involves trying to fully understand the problem in depth from a business point
    of view. Defining the scope of the problem and properly rephrasing it according
    to  **machine learning**  (**ML**) is an important part of this phase—for example,
    for a binary classification problem, sometimes it is helpful to phrase the requirements
    in terms of a hypothesis that can be proved or rejected. This phase is also about
    documenting the expectations for the machine learning model that will be trained
    downstream in Phase 4—for example, for a classification problem, we need to document
    the minimum acceptable accuracy of the model that can be deployed in production.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is important to note that Phase 1 of the CRISP-DM life cycle is about business
    understanding. It focuses on what needs to be done, not on how it will be done.
  prefs: []
  type: TYPE_NORMAL
- en: '**Phase 2: Data Understanding**:This is about understanding the data that is
    available for data mining. In this phase, we will find out whether the right datasets
    are available for the problem we are trying to solve. After identifying the datasets,
    we need to understand the quality of the data and its structure. We need to find
    out what patterns can be extracted out of the data that can potentially lead us
    toward important insights. We will also try to find the right feature that can
    be used as the label (or the target variable) according to the requirements gathered
    in Phase 1\. Unsupervised learning algorithms can play a powerful role in achieving
    the objectives of Phase 2\. Unsupervised algorithms can be used for the following
    purposes:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To discover patterns in the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To understand the structure of the dataset by analyzing the discovered patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To identify or derive the target variable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Phase 3: Data Preparation**:This is about preparing the data for the ML model
    that we will train in Phase 4\. The available labeled data is divided into two
    unequal parts. The larger portion is called the  **training data**  and is used
    for training the model downstream in Phase 4\. The smaller portion is called the  **testing
    data**  and is used in Phase 5 for model evaluation. In this phase, the unsupervised
    machine learning algorithms can be used as a tool to prepare the data—for example,
    they can be used to convert unstructured data into structured data, providing
    additional dimensions that can be helpful in training the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Phase 4: Modeling**:  This is the phase where we use supervised learning
    to formulate the patterns that we have discovered. We are expected to successfully
    prepare the data according to the requirements of our chosen supervised learning
    algorithm. This is also the phase in which the particular feature that will be
    used as the label will be identified. In Phase 3, we divided the data into testing
    and training sets. In this phase, we form mathematical formulations to represent
    the relationships in our patterns of interest. This is done by training the model
    using the training data that was created in Phase 3\. As mentioned before, the
    resulting mathematical formulation will depend on our choice of algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Phase 5: Evaluation**:This phase is about testing the newly trained model
    using the test data from Phase 3\. If the evaluation matches the expectations
    set in Phase 1, then we need iterate through all the preceding phases again, starting
    with Phase 1\. This is illustrated in the preceding image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Phase 6: Deployment**:If the evaluation meets or exceeds the expectations
    described in Phase 5, then the trained model is deployed in production and starts
    generating a solution to the problem we defined in Phase 1\.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Phase 2 (Data Understanding) and Phase 3 (Data Preparation) of the CRISP-DM
    life cycle are all about understanding the data and preparing it for training
    the model. These phases involve data processing. Some organizations employ specialists
    for this data engineering phase.
  prefs: []
  type: TYPE_NORMAL
- en: It is obvious that the process of suggesting a solution to a problem is fully
    data driven. A combination of supervised and unsupervised machine learning is
    used to formulate a workable solution. This chapter focuses on the unsupervised
    learning part of the solution.
  prefs: []
  type: TYPE_NORMAL
- en: Data engineering comprises Phase 2 and Phase 3, and is the most time-consuming
    part of machine learning. It can take as much as 70% of the time and resources
    of a typical ML project. The unsupervised learning algorithms can play an important
    role in data engineering.
  prefs: []
  type: TYPE_NORMAL
- en: The following sections provide more details regarding unsupervised algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Current research trends in unsupervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For years, research into machine learning algorithms was more focused on supervised
    learning techniques. As supervised learning techniques can be directly used for
    inference, their benefits in terms of time, cost, and accuracy are relatively
    easily measurable. The power of unsupervised machine learning algorithms has been
    recognized more  recently. As unsupervised learning is not guided, it is less
    dependent on assumptions and can potentially converge the solution in any dimension.
    Although it is more difficult to control the scope and processing requirements
    of unsupervised learning algorithms, they have more potential to unearth the hidden
    patterns. Researchers are also working to combine unsupervised machine learning
    techniques with supervised learning techniques in order to design new powerful
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Practical examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Currently, unsupervised learning is used to get a better sense of the data and
    provide it with more structure—for example, it is used in marketing segmentation,
    fraud detection, and market basket analysis (which is discussed later in this
    chapter). Let's look at a couple of examples.
  prefs: []
  type: TYPE_NORMAL
- en: Voice categorization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unsupervised learning can be used to classify individual voices in a voice file.
    It uses the fact that each individual's voice has distinct characteristics, creating
    potentially separable audio patterns. These patterns can then be used for voice
    recognition—for example, Google uses this technique in their Google Home devices
    to train them to differentiate between different people's voices. Once trained,
    Google Home can personalize the response for each user individually.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s assume that we have a recorded conversation of three people
    talking to each other for half an hour. Using unsupervised learning algorithms,
    we can identify the voices of distinct people in this dataset. Note that through
    unsupervised learning, we are adding structure to the given set of unstructured
    data. This structure gives us additional useful dimensions in our problem space
    that can be used to gain insights and to prepare data for our chosen machine learning
    algorithm. The following diagram shows how unsupervised learning is used for voice
    recognition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/71ed2127-a466-4865-b70e-6511b810bb98.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that, in this case, unsupervised learning suggests that we add a new feature
    with three distinct levels.
  prefs: []
  type: TYPE_NORMAL
- en: Document categorization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Unsupervised machine learning algorithms can also be applied to a repository
    of unstructured textual data—for example, if we have a dataset of PDF documents,
    then unsupervised learning can be used to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Discover various topics in the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Associate each PDF document to one of the discovered topics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This use of unsupervised learning for document classification is shown in the
    following figure. This is another example in which we are adding more structure
    to unstructured data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/bafdb93e-edf1-4f72-a99e-dfd51fe936b8.png)Figure 6.4: Using unsupervised
    learning for document classification'
  prefs: []
  type: TYPE_NORMAL
- en: Note that, in this case, unsupervised learning suggests that we add a new feature
    with five distinct levels.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding clustering algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the simplest and most powerful techniques used in unsupervised learning
    is based on grouping similar patterns together through clustering algorithms.
    It is used to understand a particular aspect of the data that is related to the
    problem we are trying to solve. Clustering algorithms look for natural grouping
    in data items. As the group is not based on any target or assumptions, it is classified
    as an unsupervised learning technique.
  prefs: []
  type: TYPE_NORMAL
- en: Groupings created by various clustering algorithms are based on finding the
    similarities between various data points in the problem space. The best way to
    determine the similarities between data points will vary from problem to problem
    and will depend on the nature of the problem we are dealing with. Let's look at
    the various methods that can be used to calculate the similarities between various
    data points.
  prefs: []
  type: TYPE_NORMAL
- en: Quantifying similarities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The reliability of the grouping created by clustering algorithms is based on
    the assumption that we can accurately quantify the similarities or closeness between
    various data points in the problem space. This is done by using various distance  measures.
    The following are three of the most popular methods that are used to quantify
    similarities:'
  prefs: []
  type: TYPE_NORMAL
- en: Euclidean distance measure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manhattan distance measure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cosine distance measure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look at these distance measures in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Euclidean distance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The distance between different points can quantify the similarity between two
    data points and is extensively used in unsupervised machine learning techniques,
    such as clustering. Euclidean distance is the most common and simple distance
    measure used. It is calculated by measuring the shortest distance between two
    data points in multidimensional space. For example, let''s consider two points,
    **A(1,1)** and **B(4,4)**, in a two -dimensional space, as shown in the following
    plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/7a13e022-8b59-44d6-a2de-f009d99da231.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To calculate the distance between  **A**  and  **B**—that is *d(A,B)*, we can
    use the following Pythagorean formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/b816650c-9590-4145-850e-b428499fbb7c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that this calculation is for a two-dimensional problem space. For an *n*-dimensional
    problem space, we can calculate the distance between two points  **A**  and  **B**
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ab5364d1-0bdf-420c-bbfe-443875a74304.png)'
  prefs: []
  type: TYPE_IMG
- en: Manhattan distance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In many situations, measuring the shortest distance between two points using
    the Euclidean distance measure will not truly represent the similarity or closeness
    between two points—for example, if two data points represent locations on a map,
    then the actual distance from point A to point B using ground transportation,
    such as a car or taxi, will be more than the distance calculated by the Euclidean
    distance. For situations such as these, we use Manhattan distance, which marks
    the longest route between two points and is a better reflection of the closeness
    of two points in the context of source and destination points that can be traveled
    to in a busy city. The comparison between the Manhattan and Euclidean distance
    measures is shown in the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/2c74e092-8cbb-4082-84c4-f3adbdfc47e3.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the Manhattan distance will always be equal or larger than the corresponding
    Euclidean distance calculated.
  prefs: []
  type: TYPE_NORMAL
- en: Cosine distance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Euclidean and Manhattan distance measures do not perform well in high-dimensional
    space. In a high-dimensional problem space, cosine distance more accurately reflects
    the closeness between two data points in a multidimensional problem space. The
    cosine distance measure is calculated by measuring the cosine angle created by
    two points connected to a reference point. If the data points are close, then
    the angle will be narrow, irrespective of the dimensions they have. On the other
    hand, if they are far away, then the angle will be large:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/d5d492a0-e445-4361-80c4-7a92031aebd7.png)Textual data can almost
    be considered a highly dimensional space. As the cosine distance measure works
    very well with h-dimensional spaces, it is a good choice when dealing with textual
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that in the preceding figure, the cosine of the angle between  **A(2,5)**
    and **B(4.4)** is the cosine distance. The reference between these points is the
    origin—that is, **X(0,0)**. But in reality, any point in the problem space can
    act as the reference data point, and it does not have to be the origin.
  prefs: []
  type: TYPE_NORMAL
- en: K-means clustering algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The name of the k-means clustering algorithm comes from the fact that it tries
    to create a number of clusters, *k*,calculating the means to find the closeness
    between the data points. It uses a relatively simple clustering approach, but
    is still popular because of its scalability and speed. Algorithmically, k-means
    clustering uses an iterative logic that moves the centers of the clusters until
    they reflect the most representative data point of the grouping they belong to.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that k-means algorithms lack one of the very basic functionalities
    needed for clustering. That missing functionality is that for a given dataset,
    the k-means algorithm cannot determine the most appropriate number of clusters.
    The most appropriate number of clusters, *k*, is dependent on the number of natural
    groupings in a particular dataset. The philosophy behind this omission is to keep
    the algorithm as simple as possible, maximizing its performance. This lean-and-mean
    design makes k-means suitable for larger datasets. The assumption is that an external
    mechanism will be used to calculate *k*. The best way to determine *k* will depend
    on the problem we are trying to solve. In some cases, *k* is directly specified
    by the clustering problem's context—for example, if we want to divide a class
    of data-science students into two clusters, one consisting of the students with
    the data science skill and the other with programming skills, then *k* will be
    two. In some other problems, the value of *k* may not be obvious. In such cases,
    an iterative trial-and-error procedure or a heuristic-based algorithm will have
    to be used to estimate the most appropriate number of clusters for a given dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The logic of k-means clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section describes the logic of the k-means clustering algorithm. Let's
    look at them one by one.
  prefs: []
  type: TYPE_NORMAL
- en: Initialization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to group them, the k-means algorithm uses a distance measure to find
    the similarity or closeness between data points. Before using the k-means algorithm,
    the most appropriate distance measure needs to be selected. By default, the Euclidean
    distance measure will be used. Also, if the dataset has outliers, then a mechanism
    needs to be devised to determine the criteria that are to be identified and remove
    the outliers of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The steps of the k-means algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The steps involved in the k-means clustering algorithm are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Step 1 | We choose the number of clusters, *k*. |'
  prefs: []
  type: TYPE_TB
- en: '| Step 2 | Among the data points, we randomly choose *k* points as cluster
    centers. |'
  prefs: []
  type: TYPE_TB
- en: '| Step 3 | Based on the selected distance measure, we iteratively compute the
    distance from each point in the problem space to each of the *k* cluster centers.
    Based on the size of the dataset, this may be a time-consuming step—for example,
    if there are 10,000 points in the cluster and *k* = 3, this means that 30,000
    distances need to be calculated. |'
  prefs: []
  type: TYPE_TB
- en: '| Step 4 | We assign each data point in the problem space to the nearest cluster
    center. |'
  prefs: []
  type: TYPE_TB
- en: '| Step 5 | Now each data point in our problem space has an assigned cluster
    center. But we are not done, as the selection of the initial cluster centers was
    based on random selection. We need to verify that the current randomly selected
    cluster centers are actually the center of gravity of each cluster. We recalculate
    the cluster centers by computing the mean of the constituent data points of each
    of the *k* clusters. This step explains why this algorithm is called k-means.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Step 6 | If the cluster centers have shifted in step 5, this means that we
    need to recompute the cluster assignment for each data point. For this, we will
    go back to step  3 to repeat that compute-intensive step. If the cluster centers
    have not shifted or if our predetermined stop condition (for example, the number
    of maximum iterations) has been satisfied, then we are done. |'
  prefs: []
  type: TYPE_TB
- en: 'The following figure shows the result of running the k-means algorithm in a
    two-dimensional problem space:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/70ce3a57-73b1-4c76-97fe-fdbcc956e36d.png)(a) Data points before
    clustering; (b) Resultant clusters after running the k-means clustering algorithm'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the two resulting clusters created after running k-means are well
    differentiated in this case.
  prefs: []
  type: TYPE_NORMAL
- en: Stop condition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the k-means algorithm, the default stop condition is when there is no more
    shifting of cluster centers in step 5\. But as with many other algorithms, k-means
    algorithms may take lot of time to converge, especially while processing large
    datasets in a high-dimensional problem space. Instead of waiting for the algorithm
    to converge, we can also explicitly define the stop condition as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'By specifying the maximum execution time:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stop condition**:  *t>t[max]*, where  *t*  is the current execution time
    and  *t[max]*  is the  maximum execution time we have set for the algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'By specifying the maximum iterations:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stop condition**:  *if m>m[max]*, where *m* is the current iteration and  *m*[*max*]
    is the maximum number of iterations we have set for the algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coding the k-means algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at how we can code the k-means algorithm in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s import the packages that we will need to code for the k-means
    algorithm. Note that we are importing the `sklearn`  package for k-means clustering:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To use k-means clustering, let''s create 20 data points in a two-dimensional
    problem space that we will be using for k-means clustering:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s have two clusters (*k* = 2) and then create the cluster by calling the
    `fit` functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create a variable named `centroid` that is an array that holds the location
    of the center of the clusters formed. In our case, as *k* = 2, the array will
    have a size of 2\. Let''s also create another variable named `label` that represents
    the assignment of each data point to one of the two clusters. As there are 20
    data points, this array will have a size of 20:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s print these two arrays, `centroids` and `labels`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/1bd564b9-9201-40d0-8cb3-4c7109677fdd.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the first array shows the assignment of the cluster with each data
    point and the second one shows the two cluster centers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s plot and look at the clusters using `matplotlib`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/6cc48dd3-517d-4311-a93e-5f3e206d7f1f.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the bigger dots in the plot are the centroids as determined by the
    k-means algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Limitation of k-means clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The k-means algorithm is designed to be a simple and fast algorithm. Because
    of the intentional simplicity in its design, it comes with the following limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: The biggest limitation of k-means clustering is that the initial number of clusters
    has to be predetermined.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The initial assignment of cluster centers is random. This means that each time
    the algorithm is run, it may give slightly different clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each data point is assigned to only one cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: k-means clustering is sensitive to outliers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hierarchical clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: k-means clustering uses a top-down approach because we start the algorithm from
    the most important data points, which are the cluster centers. There is an alternative
    approach of clustering where, instead of starting from the top, we start the algorithm
    from the bottom. The bottom in this context is each of the individual data points
    in the problem space. The solution is to keep on grouping similar data points
    together as it progresses up toward the cluster centers. This alternative bottom-up
    approach is used by hierarchical clustering algorithms, and is discussed in this
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Steps of hierarchical clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following steps are involved in hierarchical clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: We create a separate cluster for each data point in our problem space. If our
    problem space consists of 100 data points, then it will start with 100 clusters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We group only those points that are closest to each other.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We check for the stop condition; if the stop condition is not yet satisfied,
    then we repeat step 2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The resulting clustered structure is called a **dendrogram**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a dendrogram, the height of the vertical lines determines how close the
    items are, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/5557743e-1874-453f-b274-47d2f3ca06c7.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the stopping condition is shown as a dotted line in the preceding
    figure.
  prefs: []
  type: TYPE_NORMAL
- en: Coding a hierarchical clustering algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s learn how we can code a hierarchical algorithm in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will first import `AgglomerativeClustering` from the `sklearn.cluster` library,
    along with the `pandas` and `numpy` packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we will create 20 data points in a two-dimensional problem space:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we create the hierarchical cluster by specifying the hyperparameters.
    We use the `fit_predict` function to actually process the algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s look at the association of each data point to the two clusters that
    were created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/055dc165-fddc-4091-b73f-259376a09a82.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see that the cluster assignment for both hierarchical and k-means algorithms
    are very similar.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The objective of good quality clustering is that the data points that belong
    to the separate clusters  should be differentiable. This implies the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The data points that belong to the same cluster should be as similar as possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data points that belong to separate clusters should be as different as possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Human intuition can be used to evaluate the clustering results by visualizing
    the clusters, but there are mathematical methods that can quantify the quality
    of the clusters.  Silhouette analysis is one such technique that compares the
    tightness and separation in the clusters created by the k-means algorithm. The
    silhouette draws a plot that displays the closeness each point in a particular
    cluster has with respect to the other points in the neighboring clusters. It associates
    a number in the range of [-0, 1] with each cluster. The following table shows
    what the figures in this range signify:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Range** | Meaning | Description |'
  prefs: []
  type: TYPE_TB
- en: '| 0.71–1.0 | Excellent | This means that the k-means clustering resulted in
    groups that are quite differentiable from each other. |'
  prefs: []
  type: TYPE_TB
- en: '| 0.51–0.70 | Reasonable | This means that the k-means clustering resulted
    in groups that are somewhat differentiable from each other. |'
  prefs: []
  type: TYPE_TB
- en: '| 0.26–0.50 | Weak | This means that the k-means clustering resulted in grouping,
    but the quality of the grouping should not be relied upon. |'
  prefs: []
  type: TYPE_TB
- en: '| <0.25 | No clustering has been found | Using the parameters selected and
    the data used, it was not possible to create grouping using k-means clustering.
    |'
  prefs: []
  type: TYPE_TB
- en: Note that each cluster in the problem space will get a separate score.
  prefs: []
  type: TYPE_NORMAL
- en: Application of clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering is used wherever we needed to discover the underlying patterns in
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'In government use cases, clustering can be used for the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Crime-hotspot analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Demographic social analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In market research, clustering can be used for the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Market segmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Targeted advertisements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customer categorization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Principal component analysis** (**PCA**) is also used for generally exploring
    the data and removing noise from real-time data, such as stock-market trading.'
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Each feature in our data corresponds to a dimension in our problem space. Minimizing
    the number of features to make our problem space simpler is called **dimensionality
    reduction**. It can be done in one of the following two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature selection**: Selecting a set of features that are important in the
    context of the problem we are trying to solve'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature aggregation**: Combining two or more features to reduce dimensions
    using one of the following algorithms:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PCA**: A linear unsupervised ML algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Linear discriminant analysis**  (**LDA**): A linear supervised ML algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kernel principal component analysis**: A nonlinear algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look deeper at one of the popular dimensionality reduction algorithms,
    namely PCA, in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Principal component analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'PCA is an unsupervised machine learning technique that can be used to  reduce
    dimensions using linear transformation. In the following figure, we can see two
    principle components,  **PC1**  and  **PC2**, which show the shape of the spread
    of the data points.  PC1 and  PC2 can be used to summarize the data points with
    appropriate coefficients:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/2a037f1f-58b8-4a7b-9a9b-8ad91808dfcb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s consider the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s print the coefficients of our PCA model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/07d52f65-eb9d-4f86-b9c0-01ab5f62c25b.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the original DataFrame has four features, `Sepal.Length`, `Sepal.Width`,
    `Petal.Length`, and `Petal.Width`. The preceding DataFrame specifies the coefficients
    of the four principal components, PC1, PC2, PC3, and PC4—for example, the first
    row specifies the coefficients of PC1 that can be used to replace the original
    four variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on these coefficients, we can calculate the PCA components for our input
    DataFrame X:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s print X after the calculation of the PCA components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/3d1712cf-5e6f-407c-81c9-b2c66a593b73.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now let''s print the variance ratio and try to understand the implications
    of using PCA:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/808e5134-ae27-41c7-82fb-59d557f5eccd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The variance ratio indicates the following:'
  prefs: []
  type: TYPE_NORMAL
- en: If we choose to replace the original four features with PC1, then we will be
    able to capture about 92.3% of the variance of the original variables. We will
    introduce some approximations by not capturing 100% of the variance of the original
    four features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we choose to replace the original four features with PC1 and PC2, then we
    will capture an additional 5.3 % of the variance of the original variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we choose to replace the original four features with PC1, PC2, and PC3, then
    we will now capture a further 0.017 % of the variance of the original variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we choose to replace the original four features with four principal components,
    then we will capture 100% of the variance of the original variables (92.4 + 0.053
    + 0.017 + 0.005), but replacing four original features with four principal components
    is meaningless as we did not reduce the dimensions at all and achieved nothing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limitations of PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the limitations of PCA:'
  prefs: []
  type: TYPE_NORMAL
- en: PCA can only be used for continuous variables and is not relevant for category
    variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While aggregating, PCA approximates the component variables; it simplifies the
    problem of dimensionality at the expense of accuracy. This trade-off should be
    carefully studied before using PCA.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Association rules mining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Patterns in a particular dataset are the treasure that needs to be discovered,
    understood, and mined for the information they contain. There is an important
    set of algorithms that try to focus on the pattern analysis in a given dataset.
    One of the more popular algorithms in this class of algorithm is called the **association
    rules mining** algorithm, which provides us with the following capabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: The ability to measure the frequency of a pattern
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to establish *cause*-and-*effect* relationship among the patterns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to quantify the usefulness of patterns by comparing their accuracy
    to random guessing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples of use
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Association rules mining  is used when we are trying to investigate the cause-and-effect
    relationships between different variables of a dataset. The following are example
    questions that it can help to answer:'
  prefs: []
  type: TYPE_NORMAL
- en: Which values of humidity, cloud cover, and temperature can lead to rain tomorrow?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What type of insurance claim can indicate fraud?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What combinations of medicine may lead to complications for patients?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Market basket analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this book, recommendation engines are discussed in  [Chapter 8](e1eec5e4-0365-4aeb-94d7-2e3ae02fc18c.xhtml),  *Neural
    Network Algorithms*. Basket analysis is a simpler way of learning recommendations.
    In basket analysis, our data contains only the information regarding what items
    were bought together. It does not have any information about the user or whether
    the user enjoyed individual items. Note that it is much easier to get this data
    than it is to get ratings data.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, this kind of data is generated when we shop at Walmart, and no
    special technique is required to get the data. This data, when collected over
    a period of time, is called  **transnational**  **data**. When association rules
    analysis is applied to transnational data sets of the shopping carts being used
    in convenience stores, supermarkets, and fast-food chains, it is called  **market
    basket analysis**. It measures the conditional probability of buying a set of
    items together, which helps to answer the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the optimal placement of items on the shelf?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How should the items appear in the marketing catalog?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What should be recommended, based on a user's buying patterns?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As market basket analysis can estimate how items are related to each other,
    it is often used for mass-market retail, such as supermarkets, convenience stores,
    drug stores, and fast-food chains. The advantage of market basket analysis is
    that the results are almost self-explanatory, which means that they are easily
    understood by the business users.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at a typical superstore. All the unique items that are available
    in the store can be represented by a set, ![](assets/ce31fde4-ba41-4fa9-bb71-4c63b428aec9.png)  =
    {item  [1]  , item  [2]  , . . . , item  [m]  }. So, if that superstore is selling
    500 distinct items, then ![](assets/cd637cd5-f4fc-4481-8df6-6dc079d692cb.png)
    will be a set of size 500.
  prefs: []
  type: TYPE_NORMAL
- en: People will buy items from this store. Each time someone buys an item and pays
    at the counter, it is added to a set of the items in a particular transaction,
    called an  **itemset**. In a given period of time, the transactions are grouped
    together in a set represented by ![](assets/ccfc36fd-6039-44f0-a488-2b03dc777d6a.png),
    where ![](assets/ccfc36fd-6039-44f0-a488-2b03dc777d6a.png)  = {t  [1]  ,t  [2]  ,
    . . . ,t  [n] }.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the following simple transaction data consisting of only four
    transactions. These transactions are summarized in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| t1 | Wickets, pads |'
  prefs: []
  type: TYPE_TB
- en: '| t2 | Bat, wickets, pads, helmet |'
  prefs: []
  type: TYPE_TB
- en: '| t3 | Helmet, ball |'
  prefs: []
  type: TYPE_TB
- en: '| t4 | Bat, pads, helmet |'
  prefs: []
  type: TYPE_TB
- en: 'Let''s look at this example in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ce31fde4-ba41-4fa9-bb71-4c63b428aec9.png)  = {bat  , wickets,  pads,
    helmet, ball  }, which represents all the unique items available at the store.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's consider one of the transactions, t3, from ![](assets/ccfc36fd-6039-44f0-a488-2b03dc777d6a.png).
    Note that items bought in t3 can be represented in the itemset[t3]= {helmet,ball},
    which indicates that a customer purchased two items. As there are two items in
    this itemset, the size of itemset[t5] is said to be two.
  prefs: []
  type: TYPE_NORMAL
- en: Association rules
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An association rule mathematically describes the relationship items involved
    in various transactions. It does this by investigating the relationship between
    two itemsets in the form *X* ⇒ *Y*, where *X* ⊂![](assets/4a0f1bae-4e77-4c84-86a7-6691729c3e57.png),
    *Y* ⊂![](assets/1735cf94-543b-47a9-b5b8-24cec3a6055c.png). In addition, *X* and
    *Y* are nonoverlapping itemsets; which means that ![](assets/25efda0c-53d6-46dc-b473-443ee7bc0dfa.png)  .
  prefs: []
  type: TYPE_NORMAL
- en: 'An association rule could be described in the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '{helmet,balls}⇒  {bike}'
  prefs: []
  type: TYPE_NORMAL
- en: Here, {helmet,ball} is *X* and {ball} is *Y*.
  prefs: []
  type: TYPE_NORMAL
- en: Types of rule
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Running associative analysis algorithms will typically result in the generation
    of a large number of rules from a transaction dataset. Most of them are useless.
    To pick rules that can result in useful information, we can classify them as one
    of the following three types:'
  prefs: []
  type: TYPE_NORMAL
- en: Trivial
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inexplicable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Actionable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look at each of these types in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Trivial rules
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Among the large numbers of rules generated, many that are derived will be  useless
    as they summarize common knowledge about the business. They are called trivial
    rules. Even if the confidence in the trivial rules is high, they remain useless
    and cannot be used for any data-driven decision making. We can safely ignore all
    trivial rules.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are examples of trivial rules:'
  prefs: []
  type: TYPE_NORMAL
- en: Anyone who jumps from a high-rise building is likely to die.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working harder leads to better scores in exams.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sales of heaters increase as the temperature drops
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Driving a car over the speed limit on a highway leads to a higher chance of
    an accident.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inexplicable rules
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Among the rules that are generated after running the association rules algorithm,
    the ones that have no obvious explanation are the trickiest to use. Note that
    a rule can only be useful if it can help us discover and understand a new pattern
    that is expected to eventually lead toward a certain course of action. If that
    is not the case, and we cannot explain why event *X* led to event *Y*, then it
    is an inexplicable rule, because it's just a mathematical formula that ends up
    exploring the pointless relationship between two events that are unrelated and
    independent.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are examples of inexplicable rules:'
  prefs: []
  type: TYPE_NORMAL
- en: People who wear red shirts tend to score better in exams.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Green bicycles are more likely to be stolen.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: People who buy pickles end up buying diapers as well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Actionable rules
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Actionable rules are the golden rules we are looking for. They are understood
    by the business and lead to insights. They can help us to discover the possible
    causes of an event when presented to an audience familiar with the business domain—for
    example, actionable rules may suggest the best placement in a store for a particular
    product based on current buying patterns. They may also suggest which items to
    place together to maximize their chances of selling as users tend to buy them
    together.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are examples of actionable rules and their corresponding actions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Rule 1:**  Displaying ads to users'' social media accounts results in a higher
    likelihood of sales.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Actionable item:** Suggests alternative ways of advertising a product'
  prefs: []
  type: TYPE_NORMAL
- en: '**Rule 2:**  Creating more price points increases the likelihood of sales.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Actionable item:** One item may be advertised in a sale, while the price
    of another item is raised.'
  prefs: []
  type: TYPE_NORMAL
- en: Ranking rules
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Association rules are measured in three ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Support (frequency) of items
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Confidence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lift
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look at them in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Support
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The support measure is a number that quantifies how frequent the pattern we
    are looking for is in our dataset. It is calculated by first counting the number
    of occurrences of our pattern of interest and then dividing it by the total number
    of all the transactions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the following formula for a particular *itemset[a]*:'
  prefs: []
  type: TYPE_NORMAL
- en: '*numItemset[a] = Number of transactions that contain itemset[a]*'
  prefs: []
  type: TYPE_NORMAL
- en: '*num[total]  = Total number of transactions*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/b03e04f2-449b-4843-ba17-698bb6ffe74c.png)By just looking at the
    support, we can get an idea of how rare the occurrence of a pattern is. A low
    support means that we are looking for a rare event.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, if *itemset[a] = {helmet, ball}* appears in two transactions out
    of six, then support (itemset[a]  ) = 2/6 = 0.33.
  prefs: []
  type: TYPE_NORMAL
- en: Confidence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The confidence is a number that quantifies how strongly we can associate the
    left side (*X*) with the right side (*Y*) by calculating the conditional probability.
    It calculates the probability that event *X* will lead toward the event *Y*, given
    that event *X* occurred.
  prefs: []
  type: TYPE_NORMAL
- en: Mathematically, consider the rule *X* ⇒ *Y*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The confidence of this rule is represented as confidence(*X* ⇒ *Y* ) and is
    measured as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/74b2952b-14dc-4f9e-b791-77fff35337da.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s look at an example. Consider the following rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '{helmet, ball} ⇒ {wickets}'
  prefs: []
  type: TYPE_NORMAL
- en: 'The confidence of this rule is calculated by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/1d0a3ece-4655-4222-b625-51cc092e2c38.png)'
  prefs: []
  type: TYPE_IMG
- en: This means that if someone has {helmet, balls} in the basket, then there is
    0.5 or 50 percent probability that they will also have wickets to go with it.
  prefs: []
  type: TYPE_NORMAL
- en: Lift
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another way to estimate the quality of a rule is by calculating the lift. The
    lift returns a number that quantifies how much improvement has been achieved by
    a rule at predicting the result compared to just assuming the result at the right-hand
    side of the equation. If the *X* and *Y* itemsets were independent, then the lift
    is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/01fae583-e035-4d40-b73d-32212d50c0e2.png)'
  prefs: []
  type: TYPE_IMG
- en: Algorithms for association analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will explore the following two algorithms that can be used
    for association analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Apriori algorithm**: Proposed by Agrawal, R. and Srikant in 1994.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**FP-growth algorithm**: An improvement suggested by Han et al. in 2001.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look at each of these algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Apriori Algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The apriori algorithm is an iterative and multiphase algorithm used to generate
    association rules. It is based on a generation-and-test approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before executing the apriori algorithm, we need to define two variables: support[threshold]
    and Confidence[threshold.]'
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm consists of the following two phases:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Candidate-generation phase**: It generates the candidate itemsets, which
    contain sets of all itemsets above support[threshold].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Filter phase**: It filters out all rules below the expected confidence[threshold].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After filtering, the resulting rules are the answer.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations of the apriori algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The major bottleneck in the apriori algorithm is the generation of candidate
    rules in Phase 1—for example, ![](assets/1486d41a-07b6-45b2-bab4-71873083bfd2.png)
    = {item  [1]  , item  [2]  , . . . , item  [m]  } can produce 2^m  possible itemsets.
    Because of its multiphase design, it first generates these itemsets and then works
    toward finding the frequent itemsets. This limitation is a huge performance bottleneck
    and makes the apriori algorithm unsuitable for larger items.
  prefs: []
  type: TYPE_NORMAL
- en: FP-growth algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **frequent pattern growth** (**FP-growth**) algorithm is an improvement
    on the apriori algorithm. It starts by showing the frequent transaction FP-tree,
    which is an ordered tree. It consists of two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Populating the FP-tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mining frequent patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look at these steps one by one.
  prefs: []
  type: TYPE_NORMAL
- en: Populating the FP-tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s consider the transaction data shown in the following table. Let''s first
    represent it as a sparse matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **ID** | **Bat** | **Wickets** | **Pads** | **Helmet** | **Ball** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 1 | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1 | 1 | 1 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0 | 0 | 0 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 1 | 0 | 1 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: 'Let''s calculate the frequency of each item and sort them in descending order
    by frequency:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Item** | **Frequency** |'
  prefs: []
  type: TYPE_TB
- en: '| pads | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| helmet | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| bat | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| wicket | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| ball | 1 |'
  prefs: []
  type: TYPE_TB
- en: 'Now let''s rearrange the transaction-based data based on the frequency:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **ID** | **Original Items** | **Reordered Items** |'
  prefs: []
  type: TYPE_TB
- en: '| t1 | Wickets, pads | Pads, wickets |'
  prefs: []
  type: TYPE_TB
- en: '| t2 | Bat, wickets, pads, helmet | Helmet, pads, wickets, bat |'
  prefs: []
  type: TYPE_TB
- en: '| t3 | Helmet, ball | Helmet, ball |'
  prefs: []
  type: TYPE_TB
- en: '| t4 | Bat, pads, helmet | Helmet, pads, bat |'
  prefs: []
  type: TYPE_TB
- en: 'To build the FP-tree, let''s start with the first branch of the FP-tree. The
    FP-tree starts with a **Null** as the root. To build the tree, we can represent
    each item with a node, as shown in the following diagram (the tree representation
    of t[1]  is shown here). Note that the label of each node is the name of the item
    and its frequency is appended after the colon. Also, note that the **pads**  item  has
    a frequency of 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/a1831fc4-21ae-4229-8d09-c125d2451a45.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Using the same pattern, let''s draw all four transactions, resulting in the
    full FP-tree. The FP-tree has four leaf nodes, each representing the itemset associated
    with the four transactions. Note that we need to count the frequencies of each
    item and need to increase it when used multiple times—for example, when adding
    t[2] to the FP-tree, the frequency of **helmet** was increased to two. Similarly,
    while adding t[4], it was increased again to three. The resulting tree is shown
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/81575675-cbb3-418b-b2ba-e58240c27b40.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the FP-tree generated in the preceding diagram is an ordered tree.
  prefs: []
  type: TYPE_NORMAL
- en: Mining Frequent Patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The second phase of the FP-growth tree involves mining the frequent patterns
    from the FP-tree. By creating an ordered tree, the intention is to create an efficient
    data structure that can be easily navigated to search for frequent patterns.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start from a leaf node (that is, the end node) and move upward—for example,
    let''s start from one of the leaf node items, **bat**. Then we need to calculate
    the conditional pattern base for **bat**. The conditional pattern base is calculated
    by specifying all the paths from the leaf item node to the top. The conditional
    pattern base for **bat** will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Wicket: 1 | Pads: 1 | Helmet: 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Pad: 1 | Helmet: 1 |  |'
  prefs: []
  type: TYPE_TB
- en: 'The  **frequent pattern**  for **bat** will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*{wicket, pads, helmet} : bat*'
  prefs: []
  type: TYPE_NORMAL
- en: '*{pad,helmet} : bat*'
  prefs: []
  type: TYPE_NORMAL
- en: Code for using FP-growth
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how we can generate association rules using the FP-growth algorithm
    in Python. For this, we will be using the `pyfpgrowth`  package. First, if we
    have never used  `pyfpgrowth`  before, let''s install it first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, let''s import the packages that we need to use to implement this algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will create the input data in the form of  `transactionSet`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the input data is generated, we will generate patterns that will be based
    on the parameters that we passed in the  `find_frequent_patterns()`. Note that
    the second parameter passed to this function is the minimum support, which is
    1 in this case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The patterns have been generated. Now let''s print the patterns. The patterns
    list the combinations of items with their supports:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/1c898244-8bbf-4f3f-acd7-0d71e554569c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now let''s generate the rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/a253eee2-98ff-43df-b71b-315b8921f0d6.png)'
  prefs: []
  type: TYPE_IMG
- en: Each rule has a left-hand side and a right-hand side, separated by a colon (:).
    It also gives us the support of each of the rules in our input dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Practical application– clustering similar tweets together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Unsupervised machine learning algorithms can also be applied in real time to
    cluster similar tweets together. They will do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'STEP 1- **Topic Modeling**: Discover various topics from a given set of tweets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: STEP 2- **Clustering:** Associate each of the tweets with one of the discovered
    topics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This use of unsupervised learning is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/8a9a0326-d16f-4c14-9707-a123d24850bb.png)Note that this example
    requires real-time processing of input data.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's look into these steps one by one.
  prefs: []
  type: TYPE_NORMAL
- en: Topic modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Topic Modeling is the process of discovering the concepts in a set of documents
    that can be used to differentiate them. In the context of tweets, it is about
    finding which are the most appropriate topics in which a set of tweets can be
    divided. Latent Dirichlet Allocation is a popular algorithm that is used for topic
    modeling. Because each of the tweet are short 144 character document usually about
    a very particular topic, we can write a simpler algorithm for topic modeling purposes.
    The algorithm is described as following:'
  prefs: []
  type: TYPE_NORMAL
- en: Tokenize tweets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Preprocess the data. Remove stopwords, numbers, symbols and perform stemming
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a Term-Document-Matrix (TDM) for the tweets. Choose the top 200 words
    that appear most frequently in unique tweets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose top 10 word that directly or indirectly represent a concept or a topic.
    For example Fashion, New York, Programming, Accident. These 10 words are now the
    topics that we have successfully discovered and will become the cluster centers
    for the tweets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's move to the next step that is clustering
  prefs: []
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once we have discovered the topics we will choose them as the center of the
    cluster. Then we can run k-means clustering algorithm that will assign each of
    the tweets to one of the cluster center.
  prefs: []
  type: TYPE_NORMAL
- en: So, this the practical example that how a set of tweets can be clustered into
    topics discovered.
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly-detection algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The dictionary definition of an *anomaly* is something that is different,  abnormal,  peculiar,
    or not  easily  classified. It is a deviation  from  the  common  rule. In the
    context of data science, an anomaly is a data point that deviates a lot from the
    expected pattern. Techniques to find such data points are called anomaly-detection
    techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s see some applications of anomaly-detection algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: Credit card fraud
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding a malignant tumor in a  **magnetic resonance imaging (MRI**) scan
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fault prevention in clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Impersonation in exams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accidents on a highway
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the upcoming sections, we will see various anomaly-detection techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Using clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering algorithms such as k-means can be used to group similar data points
    together. A threshold can be defined and any point beyond that threshold can be
    classified as an anomaly. The problem with this approach is that the grouping
    created by k-means clustering may itself be biased because of the presence of
    anomalous data points and may affect the usefulness and accuracy of the approach.
  prefs: []
  type: TYPE_NORMAL
- en: Using density-based anomaly detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A density-based approach tries to find dense neighborhoods. The **k-nearest
    neighbors** (**KNN**) algorithm can be used for this purpose. Abnormalities that
    are far away from the discovered dense neighborhoods are marked as anomalies.
  prefs: []
  type: TYPE_NORMAL
- en: Using support vector machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Support Vector Machine** (**SVM**) algorithm can be used to learn the
    boundaries of the data points. Any points beyond those discovered boundaries are
    identified as anomalies.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at the various unsupervised machine learning techniques.
    We looked at the circumstances in which it is a good idea to try to reduce the
    dimensionality of the problem we are trying to solve and the different methods
    of doing this. We also studied the  practical examples  where unsupervised machine
    learning techniques can be very helpful, including market basket analysis and
    anomaly detection.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at the various supervised learning techniques.
    We will start with linear regression and then we will look at more sophisticated
    supervised machine learning techniques, such as decision-tree-based algorithms,
    SVM, and XGBoast. We will also study the naive Bayes algorithm, which is best
    suited for unstructured textual data.
  prefs: []
  type: TYPE_NORMAL
