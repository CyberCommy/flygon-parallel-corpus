- en: '*Chapter 1*: Linux Kernel Concepts for Embedded Developers'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第1章*：嵌入式开发人员的Linux内核概念'
- en: As a standalone software, the Linux kernel implements a set of functions that
    help not to reinvent the wheel and ease device driver developments. The importance
    of these helpers is that it’s not a requirement to use these for code to be accepted
    upstream. This is the kernel core that drivers rely on. We’ll cover the most popular
    of these core functionalities in this book, though other ones also exist. We will
    begin by looking at the kernel locking API before discussing how to protect shared
    objects and avoid race conditions. Then, we will look at various work deferring
    mechanisms available, where you will learn what part of the code to defer in which
    execution context. Finally, you will learn how interrupts work and how to design
    interrupt handlers from within the Linux kernel.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个独立的软件，Linux内核实现了一组函数，有助于避免重复发明轮子，并简化设备驱动程序的开发。这些辅助程序的重要性在于，不需要使用这些代码才能被上游接受。这是驱动程序依赖的内核核心。我们将在本书中介绍这些核心功能中最受欢迎的功能，尽管还有其他功能存在。我们将首先查看内核锁定API，然后讨论如何保护共享对象并避免竞争条件。然后，我们将看看各种可用的工作推迟机制，您将了解在哪个执行上下文中推迟代码的部分。最后，您将学习中断的工作原理以及如何从Linux内核内部设计中断处理程序。
- en: 'This chapter will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: The kernel locking API and shared objects
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内核锁定API和共享对象
- en: Work deferring mechanisms
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作推迟机制
- en: Linux kernel interrupt management
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linux内核中断管理
- en: Let’s get started!
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'To understand and follow this chapter’s content, you will need the following:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解和遵循本章的内容，您将需要以下内容：
- en: Advanced computer architecture knowledge and C programming skills
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高级计算机体系结构知识和C编程技能
- en: Linux kernel 4.19 sources, available at [https://github.com/torvalds/linux](https://github.com/torvalds/linux)
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linux内核4.19源代码，可在[https://github.com/torvalds/linux](https://github.com/torvalds/linux)上获得
- en: The kernel locking API and shared objects
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内核锁定API和共享对象
- en: A resource is said to be shared when it can be accessed by several contenders,
    regardless of their exclusively. When they are exclusive, access must be synchronized
    so that only the allowed contender(s) may own the resource. Such resources might
    be memory locations or peripheral devices, while the contenders might be processors,
    processes, or threads. Operating systems perform mutual exclusion by atomically
    (that is, by means of an operation that can be interrupted) modifying a variable
    that holds the current state of the resource, making this visible to all contenders
    that might access the variable at the same time. This atomicity guarantees that
    the modification will either be successful, or not successful at all. Nowadays,
    modern operating systems rely on the hardware (which should allow atomic operations)
    used for implementing synchronization, though a simple system may ensure atomicity
    by disabling interrupts (and avoiding scheduling) around the critical code section.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当可以被多个竞争者访问时，资源被称为共享资源，而不管它们的独占性。当它们是独占的时，访问必须同步，以便只有允许的竞争者可以拥有资源。这样的资源可能是内存位置或外围设备，而竞争者可能是处理器、进程或线程。操作系统通过原子地（即通过可以被中断的操作）修改持有资源当前状态的变量来执行互斥排除，使这一点对所有可能同时访问变量的竞争者可见。这种原子性保证修改要么成功，要么根本不成功。如今，现代操作系统依赖于硬件（应该允许原子操作）用于实现同步，尽管一个简单的系统可以通过在关键代码部分周围禁用中断（并避免调度）来确保原子性。
- en: 'In this section, we’ll describe the following two synchronization mechanisms:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将描述以下两种同步机制：
- en: '**Locks**: Used for mutual exclusion. When one contender holds the lock, no
    other contender can hold it (others are excluded). The most well-known locking
    primitives in the kernel are spinlocks and mutexes.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**锁**：用于互斥。当一个竞争者持有锁时，其他竞争者无法持有它（其他被排除）。内核中最知名的锁定原语是自旋锁和互斥锁。'
- en: '**Conditional variables**: Mostly used to sense or wait for a state change.
    These are implemented differently in the kernel, as we will see later, mainly
    in the *Waiting, sensing, and blocking in the Linux kernel* section.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**条件变量**：主要用于感知或等待状态改变。这些在内核中实现方式不同，我们稍后将看到，主要在*Linux内核中的等待、感知和阻塞*部分中。'
- en: When it comes to locking, it is up to the hardware to allow such synchronizations
    by means of atomic operations. The kernel then uses these to implement locking
    facilities. Synchronization primitives are data structures that are used for coordinating
    access to shared resources. Because only one contender can hold the lock (and
    thus access the shared resource), it might perform an arbitrary operation on the
    resource associated with the lock that would appear to be atomic to others.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到锁定时，硬件允许通过原子操作进行同步。然后内核使用这些来实现锁定设施。同步原语是用于协调对共享资源访问的数据结构。因为只有一个竞争者可以持有锁（从而访问共享资源），它可能对与锁相关的资源执行任意操作，这对其他人来说似乎是原子的。
- en: 'Apart from dealing with the exclusive ownership of a given shared resource,
    there are situations where it is better to wait for the state of the resource
    to change; for example, waiting for a list to contain at least one object (its
    state then passes from empty to not empty) or for a task to complete (a DMA transaction,
    for example). The Linux kernel does not implement conditional variables. From
    our user space, we could think of using a conditional variable for both situations,
    but to achieve the same or even better, the kernel provides the following mechanisms:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 除了处理给定共享资源的独占所有权外，还存在一些情况更好地等待资源状态改变；例如，等待列表包含至少一个对象（其状态从空变为非空）或等待任务完成（例如DMA事务）。Linux内核不实现条件变量。从用户空间，我们可以考虑在这两种情况下使用条件变量，但为了实现相同或甚至更好的效果，内核提供了以下机制：
- en: '**Wait queue**: Mainly used to wait for a state change. It’s designed to work
    in concert with locks.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**等待队列**：主要用于等待状态改变。它被设计为与锁协同工作。'
- en: '**Completion queue**: Used to wait for a given computation to complete.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both mechanisms are supported by the Linux kernel and are exposed to drivers
    thanks to a reduced set of APIs (which significantly ease their use when used
    by a developer). We will discuss these in the upcoming sections.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Spinlocks
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A spinlock is a hardware-based locking primitive. It depends on the capabilities
    of the hardware at hand to provide atomic operations (such as `test_and_set`,
    which, in a non-atomic implementation, would result in read, modify, and write
    operations). Spinlocks are essentially used in an atomic context where sleeping
    is not allowed or simply not needed (in interrupts, for example, or when you want
    to disable preemption), but also as an inter-CPU locking primitive.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'It is the simplest locking primitive and also the base one. It works as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.1 – Spinlock contention flow'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_1.1_B10985.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.1 – Spinlock contention flow
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s explore the diagram by looking at the following scenario:When CPUB, which
    is running task B, wants to acquire the spinlock thanks to the spinlock’s locking
    function and this spinlock is already held by another CPU (let’s say CPUA, running
    task A, which has already called this spinlock’s locking function), then CPUB
    will simply spin around a while loop, thus blocking task B until the other CPU
    releases the lock (task A calls the spinlock’s release function). This spinning
    will only happen on multi-core machines, which is why the use case described previously,
    which involves more than one CPU since it’s on a single core machine, cannot happen:
    the task either holds a spinlock and proceeds or doesn’t run until the lock is
    released. I used to say that a spinlock is a lock held by a CPU, which is the
    opposite of a mutex (we will discuss this in the next section), which is a lock
    held by a task. A spinlock operates by disabling the scheduler on the local CPU
    (that is, the CPU running the task that called the spinlock’s locking API). This
    also means that the task currently running on that CPU cannot be preempted by
    another task, except for IRQs if they are not disabled (more on this later). In
    other words, spinlocks protect resources that only one CPU can take/access at
    a time. This makes spinlocks suitable for SMP safety and for executing atomic
    tasks.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Spinlocks are not the only implementation that take advantage of hardware’s
    atomic functions. In the Linux kernel, for example, the preemption status depends
    on a per-CPU variable that, if equal to 0, means preemption is enabled. However,
    if it’s greater than 0, this means preemption is disabled (`schedule()` becomes
    inoperative). Thus, disabling preemption (`preempt_disable(`)) consists of adding
    1 to the current per-CPU variable (`preempt_count` actually), while `preempt_enable()`
    subtracts 1 (one) from the variable, checks whether the new value is 0, and calls
    `schedule()`. These addition/subtraction operations should then be atomic, and
    thus rely on the CPU being able to provide atomic addition/subtraction functions.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two ways to create and initialize a spinlock: either statically using
    the `DEFINE_SPINLOCK` macro, which will declare and initialize the spinlock, or
    dynamically by calling `spin_lock_init()` on an uninitialized spinlock.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we’ll introduce how to use the `DEFINE_SPINLOCK` macro. To understand
    how this works, we must look at the definition of this macro in `include/linux/spinlock_types.h`,
    which is as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This can be used as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After this, the spinlock will be accessible through its name, `foo_lock`. Note
    that its address would be `&foo_lock`. However, for dynamic (runtime) allocation,
    you need to embed the spinlock into a bigger structure, allocate memory for this
    structure, and then call `spin_lock_init()` on the spinlock element:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'It is better to use `DEFINE_SPINLOCK` whenever possible. It offers compile-time
    initialization and requires less lines of code with no real drawback. At this
    stage, we can lock/unlock the spinlock using the `spin_lock()` and `spin_unlock()`
    inline functions, both of which are defined in `include/linux/spinlock.h`:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 尽可能使用`DEFINE_SPINLOCK`更好。它提供了编译时初始化，并且需要更少的代码行而没有真正的缺点。在这个阶段，我们可以使用`spin_lock()`和`spin_unlock()`内联函数来锁定/解锁自旋锁，这两个函数都在`include/linux/spinlock.h`中定义：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: That being said, there are some limitations to using spinlocks this way. Though
    a spinlock prevents preemption on the local CPU, it does not prevent this CPU
    from being hogged by an interrupt (thus, executing this interrupt’s handler).
    Imagine a situation where the CPU holds a “spinlock” in order to protect a given
    resource and an interrupt occurs. The CPU will stop its current task and branch
    out to this interrupt handler. So far, so good. Now, imagine that this IRQ handler
    needs to acquire this same spinlock (you’ve probably already guessed that the
    resource is shared with the interrupt handler). It will infinitely spin in place,
    trying to acquire a lock that’s already been locked by a task that it has preempted.
    This situation is known as a deadlock.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，使用自旋锁的这种方式存在一些限制。虽然自旋锁可以防止本地CPU的抢占，但它不能阻止这个CPU被中断占用（从而执行这个中断的处理程序）。想象一种情况，CPU持有一个“自旋锁”来保护一个给定的资源，然后发生了一个中断。CPU将停止当前的任务并转到这个中断处理程序。到目前为止，一切都好。现在，想象一下，这个IRQ处理程序需要获取相同的自旋锁（你可能已经猜到这个资源与中断处理程序共享）。它将无限地在原地旋转，试图获取一个已经被它抢占的任务锁。这种情况被称为死锁。
- en: 'To address this issue, the Linux kernel provides `_irq` variant functions for
    spinlocks, which, in addition to disabling/enabling the preemption, also disable/enable
    interrupts on the local CPU. These functions are `spin_lock_irq()` and `spin_unlock_irq()`,
    and they are defined as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，Linux内核为自旋锁提供了`_irq`变体函数，除了禁用/启用抢占之外，还在本地CPU上禁用/启用中断。这些函数是`spin_lock_irq()`和`spin_unlock_irq()`，定义如下：
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You might think that this solution is sufficient, but it is not. The `_irq`
    variant partially solves this problem. Imagine that interrupts are already disabled
    on the processor before your code starts locking. So, when you call `spin_unlock_irq()`,
    you will not just release the lock, but also enable interrupts. However, this
    will probably happen in an erroneous manner since there is no way for `spin_unlock_irq()`
    to know which interrupts were enabled before locking and which weren’t.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会认为这个解决方案已经足够了，但事实并非如此。`_irq`变体只能部分解决这个问题。想象一下，在处理器上已经禁用了中断，然后你的代码开始锁定。所以，当你调用`spin_unlock_irq()`时，你不仅会释放锁，还会启用中断。然而，这可能会以错误的方式发生，因为`spin_unlock_irq()`无法知道在锁定之前哪些中断被启用，哪些没有被启用。
- en: 'The following is a short example of this:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个简短的示例：
- en: Let’s say interrupts *x* and *y* were disabled before a spinlock was acquired,
    while *z* was not.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设在获取自旋锁之前，中断*x*和*y*已经被禁用，而*z*没有。
- en: '`spin_lock_irq()` will disable the interrupts (*x*, *y*, and *z* are now disabled)
    and take the lock.'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`spin_lock_irq()`将禁用中断（*x*、*y*和*z*现在都被禁用）并获取锁。'
- en: '`spin_unlock_irq()` will enable the interrupts. *x*, *y*, and *z* will all
    be enabled, which was not the case before the lock was acquired. This is where
    the problem arises.'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`spin_unlock_irq()`将启用中断。*x*、*y*和*z*都将被启用，这在获取锁之前并不是这样。这就是问题所在。'
- en: This makes `spin_lock_irq()` unsafe when it’s called from IRQs that are off-context
    as its counterpart, `spin_unlock_irq()`, will naively enable IRQs with the risk
    of enabling those that were not enabled while `spin_lock_irq()` was invoked. It
    only makes sense to use `spin_lock_irq()`when you know that interrupts are enabled;
    that is, you are sure nothing else might have disabled interrupts on the local
    CPU.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当从关闭上下文调用时，`spin_lock_irq()`是不安全的，因为它的对应函数`spin_unlock_irq()`会天真地启用中断，有可能启用那些在调用`spin_lock_irq()`时没有启用的中断。只有在你知道中断被启用时才有意义使用`spin_lock_irq()`；也就是说，你确定没有其他东西可能在本地CPU上禁用了中断。
- en: 'Now, imagine that you save the status of your interrupts in a variable before
    acquiring the lock and restoring them to how they were while they were releasing.
    In this situation, there would be no more issues. To achieve this, the kernel
    provides `_irqsave` variant functions. These behave just like the `_irq` ones,
    while also saving and restoring the interrupts status feature. These functions
    are `spin_lock_irqsave()` and `spin_lock_irqrestore()`, and they are defined as
    follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，想象一下，在获取锁之前将中断的状态保存在一个变量中，并在释放锁时将它们恢复到获取锁时的状态。在这种情况下，就不会再有问题了。为了实现这一点，内核提供了`_irqsave`变体函数。这些函数的行为就像`_irq`函数一样，同时保存和恢复中断状态。这些函数是`spin_lock_irqsave()`和`spin_lock_irqrestore()`，定义如下：
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Important note
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: '`spin_lock()` and all its variants automatically call `preempt_disable()`,
    which disables preemption on the local CPU. On the other hand, `spin_unlock()`
    and its variants call `preempt_enable()`, which try to enable (yes, try! – it
    depends on whether other spinlocks are locked, which would affect the value of
    the preemption counter) preemption, and which internally call `schedule()` if
    enabled (depending on the current value of the counter, which should be 0). `spin_unlock()`
    is then a preemption point and might reenable preemption.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`spin_lock()`及其所有变体会自动调用`preempt_disable()`，在本地CPU上禁用抢占。另一方面，`spin_unlock()`及其变体会调用`preempt_enable()`，尝试启用（是的，尝试！-
    这取决于其他自旋锁是否被锁定，这将影响抢占计数器的值）抢占，并在启用时（取决于计数器的当前值，应该为0）内部调用`schedule()`。`spin_unlock()`然后是一个抢占点，可能重新启用抢占。'
- en: Disabling interrupts versus only disabling preemption
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 禁用中断与仅禁用抢占
- en: Though disabling interrupts may prevent kernel preemption (a scheduler’s timer
    interrupts would be disabled), nothing prevents the protected section from invoking
    the scheduler (the `schedule()` function). Lots of kernel functions indirectly
    invoke the scheduler, such as those that deal with spinlocks. As a result, even
    a simple `printk()` function may invoke the scheduler since it deals with the
    spinlock that protects the kernel message buffer. The kernel disables or enables
    the scheduler (performs preemption) by increasing or decreasing a kernel-global
    and per-CPU variable (that defaults to 0, meaning “enabled”) called `preempt_count`.
    When this variable is greater than 0 (which is checked by the `schedule()` function),
    the scheduler simply returns and does nothing. Every time a spin_lock*-related
    helper gets invoked, this variable is increased by 1\. On the other hand, releasing
    a spinlock (any `spin_unlock*` family function) decreases it by 1, and whenever
    it reaches 0, the scheduler is invoked, meaning that your critical section would
    not be very atomic.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管禁用中断可以防止内核抢占（调度程序的定时器中断将被禁用），但没有什么可以阻止受保护的部分调用调度程序（`schedule()`函数）。许多内核函数间接调用调度程序，例如处理自旋锁的函数。因此，即使是一个简单的`printk()`函数也可能调用调度程序，因为它处理保护内核消息缓冲区的自旋锁。内核通过增加或减少一个称为`preempt_count`的全局和每个CPU变量（默认为0，表示“启用”）来禁用或启用调度程序（执行抢占）。当这个变量大于0时（由`schedule()`函数检查），调度程序简单地返回并不执行任何操作。每次调用与`spin_lock*`相关的帮助程序时，这个变量都会增加1。另一方面，释放自旋锁（任何`spin_unlock*`系列函数）会将其减少1，每当它达到0时，调度程序就会被调用，这意味着你的临界区不会是非常原子的。
- en: Thus, if your code does not trigger preemption itself, it can only be protected
    from preemption by disabling interrupts. That being said, code that’s locked a
    spinlock may not sleep as there would be no way to wake it up (remember, timer
    interrupts and schedulers are disabled on the local CPU).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果你的代码本身不触发抢占，它只能通过禁用中断来防止抢占。也就是说，锁定自旋锁的代码可能不会休眠，因为没有办法唤醒它（记住，定时器中断和调度程序在本地CPU上被禁用）。
- en: Now that we are familiar with the spinlock and its subtilities, let’s look at
    the mutex, which is our second locking primitive.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经熟悉了自旋锁及其细微差别，让我们来看看互斥锁，这是我们的第二个锁原语。
- en: Mutexes
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 互斥锁
- en: The mutex is the other locking primitive we will discuss in this chapter. It
    behaves just like the spinlock, with the only difference being that your code
    can sleep. If you try to lock a mutex that is already held by another task, your
    task will find itself suspended, and it will only be woken when the mutex is released.
    There’s no spinning this time, which means that the CPU can process something
    else while your task is waiting. As I mentioned previously, *a spinlock is a lock
    held by a CPU, while a mutex is a lock held by a task*.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 互斥锁是本章讨论的另一种锁原语。它的行为就像自旋锁一样，唯一的区别是你的代码可以休眠。如果你尝试锁定一个已经被另一个任务持有的互斥锁，你的任务将被挂起，只有在互斥锁被释放时才会被唤醒。这次没有自旋，这意味着CPU可以在你的任务等待时处理其他事情。正如我之前提到的，*自旋锁是由CPU持有的锁，而互斥锁是由任务持有的锁*。
- en: 'A mutex is a simple data structure that embeds a wait queue (to put contenders
    to sleep), while a spinlock protects access to this wait queue. The following
    is what `struct mutex` looks like:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 互斥锁是一个简单的数据结构，嵌入了一个等待队列（用于让竞争者休眠），而自旋锁则保护对这个等待队列的访问。以下是`struct mutex`的样子：
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In the preceding code, the elements that are only used in debugging mode have
    been removed for the sake of readability. However, as you can see, mutexes are
    built on top of spinlocks. `owner` represents the process that actually owns (hold)
    the lock. `wait_list` is the list in which the mutex’s contenders are put to sleep.
    `wait_lock` is the spinlock that protects `wait_list` while contenders are inserted
    and are put to sleep. This helps keep `wait_list` coherent on SMP systems.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中，为了可读性，仅在调试模式下使用的元素已被删除。但是，正如你所看到的，互斥锁是建立在自旋锁之上的。`owner`表示实际拥有（持有）锁的进程。`wait_list`是互斥锁的竞争者被放置在其中休眠的列表。`wait_lock`是保护`wait_list`的自旋锁，当竞争者被插入并休眠时，它有助于保持`wait_list`在SMP系统上的一致性。
- en: 'The mutex APIs can be found in the `include/linux/mutex.h` header file. Prior
    to acquiring and releasing a mutex, it must be initialized. As for other kernel
    core data structures, there may be a static initialization, as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 互斥锁的API可以在`include/linux/mutex.h`头文件中找到。在获取和释放互斥锁之前，必须对其进行初始化。与其他内核核心数据结构一样，可能存在静态初始化，如下所示：
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following is the definition of the `DEFINE_MUTEX()` macro:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是`DEFINE_MUTEX()`宏的定义：
- en: '[PRE8]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The second approach the kernel offers is dynamic initialization. This can be
    done by making a call to the low-level `__mutex_init()` function, which is actually
    wrapped by a much more user-friendly macro known as `mutex_init()`:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 内核提供的第二种方法是动态初始化。这可以通过调用底层的`__mutex_init()`函数来实现，实际上这个函数被一个更加用户友好的宏`mutex_init()`所包装：
- en: '[PRE9]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Acquiring (also known as locking) a mutex is as simple calling one of the following
    three functions:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 获取（也称为锁定）互斥锁就像调用以下三个函数之一那样简单：
- en: '[PRE10]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: If the mutex is free (unlocked), your task will immediately acquire it without
    going to sleep. Otherwise, your task will be put to sleep in a manner that depends
    on the locking function you use. With `mutex_lock()`, your task will be put in
    an uninterruptible sleep (`TASK_UNINTERRUPTIBLE`) while you wait for the mutex
    to be released (in case it is held by another task). `mutex_lock_interruptible()`
    will put your task in an interruptible sleep, in which the sleep can be interrupted
    by any signal. `mutex_lock_killable()` will allow your task’s sleep to be interrupted,
    but only by signals that actually kill the task. Both functions return zero if
    the lock has been acquired successfully. Moreover, interruptible variants return
    `-EINTR` when the locking attempt is interrupted by a signal.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果互斥锁是空闲的（未锁定），您的任务将立即获取它而不会进入睡眠状态。否则，您的任务将以取决于您使用的锁定函数的方式进入睡眠。使用`mutex_lock()`时，您的任务将进入不可中断的睡眠状态（`TASK_UNINTERRUPTIBLE`），直到等待互斥锁被释放（如果它被另一个任务持有）。`mutex_lock_interruptible()`将使您的任务进入可中断的睡眠状态，其中睡眠可以被任何信号中断。`mutex_lock_killable()`将允许您的任务的睡眠被中断，但只能被实际杀死任务的信号中断。这两个函数在成功获取锁时返回零。此外，可中断的变体在锁定尝试被信号中断时返回`-EINTR`。
- en: 'Whatever locking function is used, the mutex owner (and only the owner) should
    release the mutex using `mutex_unlock()`, which is defined as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 无论使用哪种锁定函数，互斥锁所有者（仅所有者）应使用`mutex_unlock()`释放互斥锁，其定义如下：
- en: '[PRE11]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'If you wish to check the status of your mutex, you can use `mutex_is_locked()`:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望检查互斥锁的状态，可以使用`mutex_is_locked()`：
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This function simply checks whether the mutex owner is `NULL` and returns true
    if it is, or false otherwise.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数只是检查互斥锁所有者是否为`NULL`，如果是，则返回true，否则返回false。
- en: Important note
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: It is only recommended to use `mutex_lock()`when you can guarantee that the
    mutex will not be held for a long time. Typically, you should use the interruptible
    variant instead.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 只有在可以保证互斥锁不会长时间持有时才建议使用`mutex_lock()`。通常情况下，应该使用可中断的变体。
- en: 'There are specific rules when using mutexes. The most important are enumerated
    in the kernel’s mutex API header file, `include/linux/mutex.h`. The following
    is an excerpt from it:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用互斥锁时有特定的规则。最重要的规则在内核的互斥锁API头文件`include/linux/mutex.h`中列出。以下是其中的一部分摘录：
- en: '[PRE13]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The full version can be found in the same file.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 完整版本可以在同一文件中找到。
- en: Now, let’s look at some cases where we can avoid putting the mutex to sleep
    while it is being held. This is known as the try-lock method.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一些情况，我们可以避免在持有锁时将互斥锁置于睡眠状态。这被称为尝试锁定方法。
- en: The try-lock method
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 尝试锁定方法
- en: There are cases where we may need to acquire the lock if it is not already held
    by another elsewhere. Such methods try to acquire the lock and immediately (without
    spinning if we are using a spinlock, nor sleeping if we are using a mutex) return
    a status value. This tells us whether the lock has been successfully locked. They
    can be used if we do not need to access the data that’s being protected by the
    lock when some other thread is holding the lock.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我们可能需要获取锁，如果它不是由其他地方持有的话。这样的方法尝试获取锁并立即返回一个状态值（如果我们使用自旋锁，则不会自旋，如果我们使用互斥锁，则不会休眠）。这告诉我们锁是否已成功锁定。如果我们不需要在其他线程持有锁时访问被保护的数据，可以使用它们。
- en: 'Both the spinlock and mutex APIs provide a try-lock method. They are called
    `spin_trylock()` and `mutex_trylock()`, respectively. Both methods return 0 on
    a failure (the lock is already locked) or 1 on a success (lock acquired). Thus,
    it makes sense to use these functions along with an statement:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 自旋锁和互斥锁API都提供了尝试锁定方法。它们分别称为`spin_trylock()`和`mutex_trylock()`。这两种方法在失败时（锁已被锁定）返回0，在成功时（锁已获取）返回1。因此，使用这些函数与语句是有意义的：
- en: '[PRE14]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`spin_trylock()` actually targets spinlocks. It will lock the spinlock if it
    is not already locked in the same way that the `spin_lock()` method is. However,
    it immediately returns `0` without spinning if the spinlock is already locked:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`spin_trylock()`实际上是针对自旋锁的。如果自旋锁尚未被锁定，它将锁定自旋锁，方式与`spin_lock()`方法相同。但是，如果自旋锁已经被锁定，它将立即返回`0`而不会自旋：'
- en: '[PRE15]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'On the other hand, `mutex_trylock()` targets mutexes. It will lock the mutex
    if it is not already locked in the same way that the `mutex_lock()` method is.
    However, it immediately returns `0` without sleeping if the mutex is already locked.
    The following is an example of this:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，`mutex_trylock()`是针对互斥锁的。如果互斥锁尚未被锁定，它将以与`mutex_lock()`方法相同的方式锁定互斥锁。但是，如果互斥锁已经被锁定，它将立即返回`0`而不会休眠。以下是一个示例：
- en: '[PRE16]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In the preceding code, the try-lock is being used along with the `if` statement
    so that the driver can adapt its behavior.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，使用了尝试锁定以及`if`语句，以便驱动程序可以调整其行为。
- en: Waiting, sensing, and blocking in the Linux kernel
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Linux内核中等待、感知和阻塞
- en: This section could have been named *kernel sleeping mechanism* as the mechanisms
    we will deal with involve putting the processes involved to sleep. A device driver,
    during its life cycle, can initiate completely separate tasks, some of which depend
    on the completion of others. The Linux kernel addresses such dependencies with
    `struct completion` items. On the other hand, it may be necessary to wait for
    a particular condition to become true or the state of an object to change. This
    time, the kernel provides work queues to address this situation.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分本来可以被命名为*内核睡眠机制*，因为我们将处理的机制涉及将涉及的进程置于睡眠状态。设备驱动程序在其生命周期中可以启动完全独立的任务，其中一些任务依赖于其他任务的完成。Linux内核使用`struct
    completion`项来解决这种依赖关系。另一方面，可能需要等待特定条件成为真或对象状态发生变化。这时，内核提供工作队列来解决这种情况。
- en: Waiting for completion or a state change
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 等待完成或状态改变
- en: 'You may not necessarily be waiting exclusively for a resource, but for the
    state of a given object (shared or not) to change or for a task to complete. In
    kernel programming practices, it is common to initiate an activity outside the
    current thread, and then wait for that activity to complete. Completion is a good
    alternative to `sleep()` when you’re waiting for a buffer to be used, for example.
    It is suitable for sensing data, as is the case with DMA transfers. Working with
    completions requires including the `<linux/completion.h>` header. Its structure
    looks as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能不一定是在专门等待资源，而是等待给定对象（共享或非共享）的状态改变或任务完成。在内核编程实践中，通常会在当前线程之外启动一个活动，然后等待该活动完成。当您等待缓冲区被使用时，完成是
    `sleep()` 的一个很好的替代方案，例如。它适用于传感数据，就像 DMA 传输一样。使用完成需要包括 `<linux/completion.h>` 头文件。其结构如下：
- en: '[PRE17]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'You can create instances of the struct completion structure either statically
    using the static `DECLARE_COMPLETION(my_comp)` function or dynamically by wrapping
    the completion structure into a dynamic (allocated on the heap, which will be
    alive for the lifetime of the function/driver) data structure and invoking `init_completion(&dynamic_object->my_comp)`.
    When the device driver performs some work (a DMA transaction, for example) and
    others (threads, for example) need to be notified of their completion, the waiter
    has to call `wait_for_completion()` on the previously initialized struct completion
    object in order to be notified of this:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用静态的 `DECLARE_COMPLETION(my_comp)` 函数静态地创建结构完成的实例，也可以通过将完成结构包装到动态数据结构中（在堆上分配，将在函数/驱动程序的生命周期内保持活动状态）并调用
    `init_completion(&dynamic_object->my_comp)` 来动态创建。当设备驱动程序执行一些工作（例如 DMA 事务）并且其他人（例如线程）需要被通知它们已经完成时，等待者必须在先前初始化的结构完成对象上调用
    `wait_for_completion()` 以便得到通知：
- en: '[PRE18]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'When the other part of the code has decided that the work has been completed
    (the transaction has been completed, in the case of DMA), it can wake up anyone
    (the code that needs to access the DMA buffer) who is waiting by either calling
    `complete()`, which will only wake one waiting process, or `complete_all()`, which
    will wake everyone waiting for this to complete:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当代码的另一部分已经决定工作已经完成（在 DMA 的情况下，事务已经完成），它可以通过调用 `complete()` 来唤醒任何等待的人（需要访问 DMA
    缓冲区的代码），这将只唤醒一个等待的进程，或者调用 `complete_all()`，这将唤醒所有等待完成的人：
- en: '[PRE19]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'A typical usage scenario is as follows (this excerpt has been taken from the
    kernel documentation):'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的使用场景如下（此摘录摘自内核文档）：
- en: '[PRE20]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The order in which `wait_for_completion()` and `complete()` are called does
    not matter. As semaphores, the completions API is designed so that they will work
    properly, even if `complete()` is called before `wait_for_completion()`. In such
    a case, the waiter will simply continue immediately once all the dependencies
    have been satisfied.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`wait_for_completion()` 和 `complete()` 被调用的顺序并不重要。与信号量一样，完成 API 被设计成，即使在 `complete()`
    被调用之前调用 `wait_for_completion()`，它们也能正常工作。在这种情况下，一旦所有依赖关系得到满足，等待者将立即继续执行。'
- en: Note that `wait_for_completion()` will invoke `spin_lock_irq()` and `spin_unlock_irq()`,
    which, according to the *Spinlocks* section, are not recommended to be used from
    within an interrupt handler or with disabled IRQs. This is because it would result
    in spurious interrupts being enabled, which are hard to detect. Additionally,
    by default, `wait_for_completion()` marks the task as uninterruptible (`TASK_UNINTERRUPTIBLE`),
    making it unresponsive to any external signal (even kill). This may block for
    a long time, depending on the nature of the activity it’s waiting for.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`wait_for_completion()` 将调用 `spin_lock_irq()` 和 `spin_unlock_irq()`，根据 *自旋锁*
    部分的说明，不建议在中断处理程序内部或禁用 IRQs 时使用它们。这是因为它会导致启用难以检测的虚假中断。另外，默认情况下，`wait_for_completion()`
    将任务标记为不可中断 (`TASK_UNINTERRUPTIBLE`)，使其对任何外部信号（甚至 kill 信号）都不响应。这可能会根据它等待的活动的性质而阻塞很长时间。
- en: 'You may need the *wait* not to be done in an uninterruptible state, or at least
    you may need the *wait* being able to be interrupted either by any signal or only
    by signals that kill the process. The kernel provides the following APIs:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能需要 *等待* 不是在不可中断状态下完成，或者至少您可能需要 *等待* 能够被任何信号中断，或者只能被杀死进程的信号中断。内核提供了以下 API：
- en: '`wait_for_completion_interruptible()`'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`wait_for_completion_interruptible()`'
- en: '`wait_for_completion_interruptible_timeout()`'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`wait_for_completion_interruptible_timeout()`'
- en: '`wait_for_completion_killable()`'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`wait_for_completion_killable()`'
- en: '`wait_for_completion_killable_timeout()`'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`wait_for_completion_killable_timeout()`'
- en: '`_killable` variants will mark the task as `TASK_KILLABLE`, thus only making
    it responsive to signals that actually kill it, while `_interruptible` variants
    mark the task as `TASK_INTERRUPTIBLE`, allowing it to be interrupted by any signal.
    `_timeout` variants will, at most, wait for the specified timeout:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`_killable` 变体将任务标记为 `TASK_KILLABLE`，因此只会对真正杀死它的信号做出响应，而 `_interruptible` 变体将任务标记为
    `TASK_INTERRUPTIBLE`，允许它被任何信号中断。 `_timeout` 变体将最多等待指定的超时时间：'
- en: '[PRE21]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Since `wait_for_completion*()` may sleep, it can only be used in this process
    context. Because the `interruptible`, `killable`, or `timeout` variant may return
    before the underlying job has run until completion, their return values should
    be checked carefully so that you can adopt the right behavior. The killable and
    interruptible variants return `-ERESTARTSYS` if they’re interrupted and `0` if
    they’ve been completed. On the other hand, the timeout variants will return `-ERESTARTSYS`
    if they’re interrupted, `0` if they’ve timed out, and the number of jiffies (at
    least 1) left until timeout if they’ve completed before timeout. Please refer
    to `kernel/sched/completion.c` in the kernel source for more on this, as well
    as more functions that will not be covered in this book.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`wait_for_completion*()`可能会休眠，因此只能在此进程上下文中使用。因为`interruptible`、`killable`或`timeout`变体可能在底层作业完成之前返回，所以应该仔细检查它们的返回值，以便采取正确的行为。可杀死和可中断的变体如果被中断则返回`-ERESTARTSYS`，如果已完成则返回`0`。另一方面，超时变体将在被中断时返回`-ERESTARTSYS`，在超时时返回`0`，在超时之前完成则返回剩余的jiffies数（至少为1）。有关更多信息，请参阅内核源代码中的`kernel/sched/completion.c`，以及本书中未涵盖的更多函数。
- en: On the other hand, `complete()` and `complete_all()` never sleep and internally
    call `spin_lock_irqsave()`/`spin_unlock_irqrestore()`, making completion signaling,
    from an IRQ context, completely safe.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，`complete()`和`complete_all()`永远不会休眠，并且在内部调用`spin_lock_irqsave()`/`spin_unlock_irqrestore()`，使得从IRQ上下文中进行完成信号完全安全。
- en: Linux kernel wait queues
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Linux内核等待队列
- en: 'Wait queues are high-level mechanisms that are used to process block I/O, wait
    for particular conditions to be true, wait for a given event to occur, or to sense
    data or resource availability. To understand how they work, let’s have a look
    at the structure in `include/linux/wait.h`:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 等待队列是用于处理块I/O、等待特定条件成立、等待特定事件发生或感知数据或资源可用性的高级机制。要了解它们的工作原理，让我们来看看`include/linux/wait.h`中的结构：
- en: '[PRE22]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'A `wait queue` is nothing but a list (in which processes are put to sleep so
    that they can be awaken if some conditions are met) where there’s a spinlock to
    protect access to this list. You can use a `wait queue` when more than one process
    wants to sleep and you’re waiting for one or more events to occur so that it can
    be woke up. The head member is the list of processes waiting for the event(s).
    Each process that wants to sleep while waiting for the event to occur puts itself
    in this list before going to sleep. When a process is in the list, it is called
    a `wait queue entry`. When the event occurs, one or more processes on the list
    are woken up and moved off the list. We can declare and initialize a `wait queue`
    in two ways. First, we can declare and initialize it statically using `DECLARE_WAIT_QUEUE_HEAD`,
    as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`等待队列`只是一个列表（其中进程被放入休眠，以便在满足某些条件时唤醒）的名称，其中有一个自旋锁来保护对该列表的访问。当多个进程希望休眠并且正在等待一个或多个事件发生时，可以使用`等待队列`。头成员是等待事件的进程列表。希望在等待事件发生时休眠的每个进程都会在进入休眠之前将自己放入此列表中。当进程在列表中时，它被称为`等待队列条目`。当事件发生时，列表中的一个或多个进程将被唤醒并移出列表。我们可以以两种方式声明和初始化`等待队列`。首先，我们可以使用`DECLARE_WAIT_QUEUE_HEAD`静态声明和初始化它，如下所示：'
- en: '[PRE23]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We can also do this dynamically using `init_waitqueue_head()`:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用`init_waitqueue_head()`来动态执行此操作：
- en: '[PRE24]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Any process that wants to sleep while waiting for `my_event` to occur can invoke
    either `wait_event_interruptible()` or `wait_event()`. Most of the time, the event
    is just the fact that a resource has become available. Thus, it only makes sense
    for a process to go to sleep after the availability of that resource has been
    checked. To make things easy for you, these functions both take an expression
    in place of the second argument so that the process is only put to sleep if the
    expression evaluates to false:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 任何希望在等待`my_event`发生时休眠的进程都可以调用`wait_event_interruptible()`或`wait_event()`。大多数情况下，事件只是资源已经可用的事实。因此，只有在检查资源的可用性之后，进程才会进入休眠状态。为了方便您，这些函数都接受一个表达式作为第二个参数，因此只有在表达式评估为假时，进程才会进入休眠状态：
- en: '[PRE25]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '`wait_event()` and `wait_event_interruptible()` simply evaluate the condition
    when it’s called. If the condition is false, the process is put into either a
    `TASK_UNINTERRUPTIBLE` or a `TASK_INTERRUPTIBLE` (for the `_interruptible` variant)
    state and removed from the running queue.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`wait_event()`和`wait_event_interruptible()`在调用时简单地评估条件。如果条件为假，则进程将被放入`TASK_UNINTERRUPTIBLE`或`TASK_INTERRUPTIBLE`（对于`_interruptible`变体）状态，并从运行队列中移除。'
- en: 'There may be cases where you need the condition to not only be true, but to
    time out after waiting a certain amount of time. You can address such cases using
    `wait_event_timeout()`, whose prototype is as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 可能存在这样的情况，您不仅需要条件为真，而且还需要在等待一定时间后超时。您可以使用`wait_event_timeout()`来处理这种情况，其原型如下：
- en: '[PRE26]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This function has two behaviors, depending on the timeout having elapsed or
    not:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数具有两种行为，取决于超时是否已经过去：
- en: '`timeout` has elapsed: The function returns 0 if the condition is evaluated
    to false or 1 if it is evaluated to true.'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`timeout`已经过去：如果条件评估为假，则函数返回0，如果评估为真，则返回1。'
- en: '`timeout` has not elapsed yet: The function returns the remaining time (in
    jiffies – must at least be 1) if the condition is evaluated to true.'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`timeout`尚未过去：如果条件评估为真，则函数返回剩余时间（以jiffies为单位，必须至少为1）。'
- en: 'The time unit for the timeout is `jiffies`. So that you don’t have to bother
    with seconds to `jiffies` conversion, you should use the `msecs_to_jiffies()`
    and `usecs_to_jiffies()` helpers, which convert milliseconds or microseconds into
    jiffies, respectively:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 超时的时间单位是`jiffies`。因此，您不必担心将秒转换为`jiffies`，您应该使用`msecs_to_jiffies()`和`usecs_to_jiffies()`辅助函数，分别将毫秒或微秒转换为`jiffies`：
- en: '[PRE27]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'After a change has been made to any variable that could mangle the result of
    the wait condition, you must call the appropriate `wake_up*` family function.
    That being said, in order to wake up a process sleeping on a wait queue, you should
    call either `wake_up()`, `wake_up_all()`, `wake_up_interruptible()`, or `wake_up_interruptible_all()`.
    Whenever you call any of these functions, the condition is reevaluated. If the
    condition is true at this time, then a process (or all the processes for the `_all()`
    variant) in `wait queue` will be awakened, and its (their) state will be set to
    `TASK_RUNNING`; otherwise (the condition is false), nothing will happen:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在更改可能破坏等待条件结果的任何变量后，必须调用适当的`wake_up*`系列函数。也就是说，为了唤醒在等待队列上休眠的进程，您应该调用`wake_up()`、`wake_up_all()`、`wake_up_interruptible()`或`wake_up_interruptible_all()`中的任何一个。每当调用这些函数时，条件都会被重新评估。如果此时条件为真，则等待队列中的进程（或`_all()`变体的所有进程）将被唤醒，并且其状态将被设置为`TASK_RUNNING`；否则（条件为假），什么也不会发生。
- en: '[PRE28]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Since they can be interrupted by signals, you should check the return values
    of `_interruptible` variants. A non-zero value means your sleep has been interrupted
    by some sort of signal, so the driver should return `ERESTARTSYS`:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它们可以被信号中断，您应该检查`_interruptible`变体的返回值。非零值意味着您的睡眠已被某种信号中断，因此驱动程序应返回`ERESTARTSYS`。
- en: '[PRE29]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'In the preceding example, the current process (actually, this is `insmod`)
    will be put to sleep in the wait queue for 5 seconds and woken up by the work
    handler. The output of `dmesg` is as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，当前进程（实际上，这是`insmod`）将在等待队列中休眠5秒，并由工作处理程序唤醒。`dmesg`的输出如下：
- en: '[PRE30]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'You may have noticed that I did not check the return value of `wait_event_interruptible()`.
    Sometimes (if not most of the time), this can lead to serious issues. The following
    is a true story: I’ve had to intervene in a company to fix a bug where killing
    (or sending a signal to) a user space task was making their kernel module crash
    the system (panic and reboot – of course, the system was configured so that it
    rebooted on panic). The reason this happened was because there was a thread in
    this user process that did an `ioctl()` on the `char` device exposed by their
    kernel module. This resulted in a call to `wait_event_interruptible()` in the
    kernel on a given flag, which meant there was some data that needed to be processed
    in the kernel (the `select()` system call could not be used).'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到，我没有检查`wait_event_interruptible()`的返回值。有时（如果不是大多数时候），这可能会导致严重问题。以下是一个真实的故事：我曾经不得不介入一家公司来修复一个bug，即杀死（或向）用户空间任务发送信号会使他们的内核模块使系统崩溃（恐慌和重启-当然，系统被配置为在恐慌时重新启动）。发生这种情况的原因是因为这个用户进程中有一个线程在其内核模块公开的`char`设备上执行`ioctl()`。这导致内核中对给定标志的`wait_event_interruptible()`的调用，这意味着内核中需要处理一些数据（不能使用`select()`系统调用）。
- en: So, what was their mistake? The signal that was sent to the process was making
    `wait_event_interruptible()` return without the flag being set (which meant data
    was still not available), and its code was not checking its return value, nor
    rechecking the flag or performing a sanity check on the data that was supposed
    to be available. The data was being accessed as if the flag had been set and it
    actually dereferenced an invalid pointer.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，他们的错误是什么？发送给进程的信号使`wait_event_interruptible()`返回而没有设置标志（这意味着数据仍然不可用），它的代码没有检查其返回值，也没有重新检查标志或对应该可用的数据进行合理性检查。数据被访问，就好像标志已经被设置，并且实际上对一个无效的指针进行了解引用。
- en: 'The solution could have been as simple as using the following code:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案可能只需使用以下代码：
- en: '[PRE31]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: However, for some reason (historical to their design), we had to make it uninterruptible,
    which resulted in us using `wait_event()`. However, note that this function puts
    the process into an exclusive wait (an uninterruptible sleep), which means it
    can’t be interrupted by signals. It should only be used for critical tasks. Interruptible
    functions are recommended in most situations.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于某种原因（与他们的设计有关），我们不得不使其不可中断，这导致我们使用了`wait_event()`。但是，请注意，此函数将进程置于独占等待状态（不可中断睡眠），这意味着它不能被信号中断。它应该只用于关键任务。在大多数情况下建议使用可中断函数。
- en: Now that we are familiar with the kernel locking APIs, we will look at various
    work deferring mechanisms, all of which are heavily used when writing Linux device
    drivers.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们熟悉了内核锁定API，我们将看一下各种工作推迟机制，这些机制在编写Linux设备驱动程序时被广泛使用。
- en: Work deferring mechanisms
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作推迟机制
- en: 'Work deferring is a mechanism the Linux kernel offers. It allows you to defer
    work/a task until the system’s workload allows it to run smoothly or after a given
    time has lapsed. Depending on the type of work, the deferred task can run either
    in a process context or in an atomic context. It is common to using work deferring
    to complement the interrupt handler in order to compensate for some of its limitations,
    some of which are as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 工作推迟是Linux内核提供的一种机制。它允许您推迟工作/任务，直到系统的工作负载允许它平稳运行或经过一定的时间。根据工作的类型，延迟任务可以在进程上下文或原子上下文中运行。通常使用工作推迟来补充中断处理程序，以弥补其中一些限制，其中一些如下：
- en: The interrupt handler must be as fast as possible, meaning that only a critical
    task should be performed in the handler so that the rest can be deferred later
    when the system is less busy.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中断处理程序必须尽可能快，这意味着处理程序中只能执行关键任务，以便其余任务在系统不太忙时稍后推迟。
- en: In the interrupt context, we cannot use blocking calls. The sleeping task should
    be scheduled in the process context.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在中断上下文中，我们不能使用阻塞调用。睡眠任务应该在进程上下文中被调度。
- en: 'The deferring work mechanism allows us to perform the minimum possible work
    in the interrupt handler (the so-called *top-half*, which runs in an interrupt
    context) and schedule an asynchronous action (the so-called *bottom-half*, which
    may – but not always – run in a user context) from the interrupt handler so that
    it can be run at a later time and execute the rest of the operations. Nowadays,
    the concept of bottom-half is mostly assimilated to deferred work running in a
    process context, since it was common to schedule work that might sleep (unlike
    rare work running in an interrupt context, which cannot happen). Linux now has
    three different implementations of this: **softIRQs**, **tasklets**, and **work
    queues**. Let’s take a look at these:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 延迟工作机制允许我们在中断处理程序中执行尽可能少的工作（所谓的*top-half*），并安排一个异步操作（所谓的*bottom-half*），以便稍后可以运行并执行其余的操作。现在，底半部分的概念大多被吸收到在进程上下文中运行的延迟工作中，因为常见的是安排可能休眠的工作（与在中断上下文中运行的罕见工作不同，后者不会发生）。Linux现在有三种不同的实现：**softIRQs**，**tasklets**和**work
    queues**。让我们来看看这些：
- en: '**SoftIRQs**: These are executed in an atomic context.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SoftIRQs**：这些在原子上下文中执行。'
- en: '**Tasklets**: These are also executed in an atomic context.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Tasklets**：这些也在原子上下文中执行。'
- en: '**Work queues**: These run in a process context.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Work queues**：这些在进程上下文中运行。'
- en: We will learn about each of them in detail in the next few sections.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的几节中详细了解每一个。
- en: SoftIRQs
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SoftIRQs
- en: As the name suggests, `kernel/softirq.c` in the kernel source tree, and any
    drivers that wish to use this API need to include `<linux/interrupt.h>`.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 顾名思义，`kernel/softirq.c`在内核源树中，任何希望使用此API的驱动程序都需要包含`<linux/interrupt.h>`。
- en: 'Note that you cannot dynamically register nor destroy softIRQs. They are statically
    allocated at compile time. Moreover, the usage of softIRQs is restricted to statically
    compiled kernel code; they cannot be used with dynamically loadable modules. SoftIRQs
    are represented by `struct softirq_action` structures defined in `<linux/interrupt.h>`,
    as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您不能动态注册或销毁softIRQ。它们在编译时静态分配。此外，softIRQ的使用受到静态编译内核代码的限制；它们不能与动态可加载模块一起使用。SoftIRQ由`<linux/interrupt.h>`中定义的`struct
    softirq_action`结构表示，如下所示：
- en: '[PRE32]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'This structure embeds a pointer to the function to be run when the `softirq`
    action is raised. Thus, the prototype of your softIRQ handler should look as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 此结构嵌入了指向`softirq`动作被触发时要运行的函数的指针。因此，您的softIRQ处理程序的原型应如下所示：
- en: '[PRE33]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Running a softIRQ handler results in this action function being executed. It
    only has one parameter: a pointer to the corresponding `softirq_action` structure.
    You can register the softIRQ handler at runtime by means of the `open_softirq()`
    function:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 运行softIRQ处理程序会导致执行此动作函数。它只有一个参数：指向相应的`softirq_action`结构的指针。您可以通过`open_softirq()`函数在运行时注册softIRQ处理程序：
- en: '[PRE34]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '`nr` represents the softIRQ’s index, which is also considered as the softIRQ’s
    priority (where `0` is the highest). `action` is a pointer to the softIRQ’s handler.
    Any possible indexes are enumerated in the following `enum`:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`nr`表示softIRQ的索引，也被视为softIRQ的优先级（其中`0`最高）。`action`是指向softIRQ处理程序的指针。任何可能的索引都在以下`enum`中列举：'
- en: '[PRE35]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'SoftIRQs with lower indexes (highest priority) run before those with higher
    indexes (lowest priority). The names of all the available softIRQs in the kernel
    are listed in the following array:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 具有较低索引（最高优先级）的SoftIRQ在具有较高索引（最低优先级）的SoftIRQ之前运行。内核中所有可用的softIRQ的名称都列在以下数组中：
- en: '[PRE36]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'It is easy to check the output of the `/proc/softirqs` virtual file, as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 可以轻松检查`/proc/softirqs`虚拟文件的输出，如下所示：
- en: '[PRE37]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'A `NR_SOFTIRQS` entry array of `struct softirq_action` is declared in `kernel/softirq.c`:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在`kernel/softirq.c`中声明了一个`struct softirq_action`的`NR_SOFTIRQS`条目数组：
- en: '[PRE38]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Each entry in this array may contain one and only one softIRQ. As a consequence
    of this, there can be a maximum of `NR_SOFTIRQS` (10 in v4.19, which is the last
    version at the time of writing this) for registered softIRQs:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 该数组中的每个条目可能只包含一个softIRQ。因此，最多可以有`NR_SOFTIRQS`（在撰写本文时的最后版本为v4.19，为10）个已注册的softIRQ：
- en: '[PRE39]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'A concrete example of this is the network subsystem, which registers softIRQs
    that it needs (in `net/core/dev.c`) as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个具体的例子是网络子系统，它在`net/core/dev.c`中注册所需的softIRQ，如下所示：
- en: '[PRE40]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Before a registered softIRQ gets a chance to run, it should be activated/scheduled.
    To do this, you must call `raise_softirq()` or `raise_softirq_irqoff()` (if interrupts
    are already off):'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在注册的softIRQ有机会运行之前，它应该被激活/安排。要做到这一点，您必须调用`raise_softirq()`或`raise_softirq_irqoff()`（如果中断已关闭）：
- en: '[PRE41]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The first function simply sets the appropriate bit in the per-CPU softIRQ bitmap
    (the `__softirq_pending` field in the `struct irq_cpustat_t` data structure, which
    is allocated per-CPU in `kernel/softirq.c`), as follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个函数只是在每个CPU的softIRQ位图中设置适当的位（`kernel/softirq.c`中为每个CPU分配的`struct irq_cpustat_t`数据结构中的`__softirq_pending`字段），如下所示：
- en: '[PRE42]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: This allows it to run when the flag is checked. This function has been described
    here for study purposes and should not be used directly.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这允许在检查标志时运行。此函数已在此处描述供学习目的，并且不应直接使用。
- en: '`raise_softirq_irqoff` needs be called with interrupts disabled. First, it
    internally calls `__raise_softirq_irqoff()`, as described previously, to activate
    the softIRQ. Then, it checks whether it has been called from within an interrupt
    (either hard or soft) context by means of the `in_interrupt()` macro (which simply
    returns the value of `current_thread_info( )->preempt_count`, where 0 means preemption
    is enabled. This states that we are not in an interrupt context. A value greater
    than 0 means we are in an interrupt context). If `in_interrupt() > 0`, this does
    nothing as we are in an interrupt context. This is because softIRQ flags are checked
    on the exit path of any I/O IRQ handler (`asm_do_IRQ()` for ARM or `do_IRQ()`
    for x86 platforms, which makes a call to `irq_exit()`). Here, softIRQs run in
    an interrupt context. However, if `in_interrupt() == 0`, then `wakeup_softirqd()`
    gets invoked. This is responsible for waking the local CPU `ksoftirqd` thread
    up (it schedules it) to ensure the softIRQ runs soon but in a process context
    this time.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '`raise_softirq_irqoff` 需要在中断被禁用时调用。首先，它内部调用 `__raise_softirq_irqoff()`，如前所述，来激活
    softIRQ。然后，它通过 `in_interrupt()` 宏检查是否从中断（硬件或软件）上下文中调用（该宏简单地返回 `current_thread_info(
    )->preempt_count` 的值，其中 0 表示启用了抢占。这表示我们不在中断上下文中。大于 0 的值表示我们在中断上下文中）。如果 `in_interrupt()
    > 0`，则不执行任何操作，因为我们在中断上下文中。这是因为 softIRQ 标志在任何 I/O IRQ 处理程序的退出路径上被检查（对于 ARM 是 `asm_do_IRQ()`，对于
    x86 平台是 `do_IRQ()`，它调用 `irq_exit()`）。在这里，softIRQ 在中断上下文中运行。但是，如果 `in_interrupt()
    == 0`，那么会调用 `wakeup_softirqd()`。这负责唤醒本地 CPU 的 `ksoftirqd` 线程（它调度它）以确保 softIRQ
    很快运行，但这次是在进程上下文中。'
- en: '`raise_softirq` first calls `local_irq_save()` (which disables interrupts on
    the local processor after saving its current interrupt flags). It then calls `raise_softirq_irqoff()`,
    as described previously, to schedule the softIRQ on the local CPU (remember, this
    function must be invoked with IRQs disabled on the local CPU). Finally, it calls
    `local_irq_restore()`to restore the previously saved interrupt flags.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '`raise_softirq` 首先调用 `local_irq_save()`（在保存当前中断标志后禁用本地处理器上的中断）。然后调用 `raise_softirq_irqoff()`，如前所述，在本地
    CPU 上调度 softIRQ（请记住，必须在本地 CPU 上禁用 IRQ 时调用此函数）。最后，它调用 `local_irq_restore()` 来恢复先前保存的中断标志。'
- en: 'There are a few things to remember about softIRQs:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 softIRQ，有一些需要记住的事情：
- en: A softIRQ can never preempt another softIRQ. Only hardware interrupts can. SoftIRQs
    are executed at a high priority with scheduler preemption disabled, but with IRQs
    enabled. This makes softIRQs suitable for the most time-critical and important
    deferred processing on the system.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: softIRQ 永远不会抢占另一个 softIRQ。只有硬件中断可以。SoftIRQ 以高优先级执行，禁用调度程序抢占，但启用 IRQ。这使得 softIRQ
    适用于系统上最关键和重要的延迟处理。
- en: While a handler runs on a CPU, other softIRQs on this CPU are disabled. SoftIRQs
    can run concurrently, however. While a softIRQ is running, another softIRQ (even
    the same one) can run on another processor. This is one of the main advantages
    of softIRQs over hardIRQs, and is the reason why they are used in the networking
    subsystem, which may require heavy CPU power.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当处理程序在 CPU 上运行时，该 CPU 上的其他 softIRQ 被禁用。但是 softIRQ 可以并发运行。在 softIRQ 运行时，另一个 softIRQ（甚至是相同的
    softIRQ）可以在另一个处理器上运行。这是 softIRQ 相对于 hardIRQ 的主要优势之一，也是它们被用于可能需要大量 CPU 力量的网络子系统的原因。
- en: For locking between softIRQs (or even the same softIRQ as it may be running
    on a different CPU), you should use `spin_lock()` and `spin_unlock()`.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 softIRQ 之间的锁定（甚至是同一个 softIRQ，因为它可能在不同的 CPU 上运行），应该使用 `spin_lock()` 和 `spin_unlock()`。
- en: SoftIRQs are mostly scheduled in the return paths of hardware interrupt handlers.
    `ksoftirqd``CONFIG_SMP` enabled). See `timer_tick()`, `update_process_times()`,
    and `run_local_timers()` for more.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: softIRQ 主要在硬件中断处理程序的返回路径上调度。`ksoftirqd``CONFIG_SMP` 已启用。更多信息请参见 `timer_tick()`、`update_process_times()`
    和 `run_local_timers()`。
- en: --By making a call to the `local_bh_enable()` function (mostly invoked by the
    network subsystem for handling packet receiving/transmitting softIRQs).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '- 通过调用 `local_bh_enable()` 函数（主要由网络子系统调用，用于处理软IRQ 的数据包接收/发送）。'
- en: --On the exit path of any I/O IRQ handler (see `do_IRQ`, which makes a call
    to `irq_exit()`, which in turn invokes `invoke_softirq()`).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '- 在任何 I/O IRQ 处理程序的退出路径上（参见 `do_IRQ`，它调用 `irq_exit()`，后者又调用 `invoke_softirq()`）。'
- en: --When the local `ksoftirqd` is given the CPU (that is, it’s been awakened).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '- 当本地的 `ksoftirqd` 获得 CPU 时（也就是说，它被唤醒）。'
- en: 'The actual kernel function responsible for walking through the softIRQ’s pending
    bitmap and running them is `__do_softirq()`, which is defined in `kernel/softirq.c`.
    This function is always invoked with interrupts disabled on the local CPU. It
    performs the following tasks:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 负责遍历 softIRQ 挂起位图并运行它们的实际内核函数是 `__do_softirq()`，它在 `kernel/softirq.c` 中定义。该函数始终在本地
    CPU 上禁用中断时调用。它执行以下任务：
- en: Once invoked, the function saves the current per-CPU pending softIRQ’s bitmap
    in a so-called pending variable and locally disables softIRQs by means of `__local_bh_disable_ip`.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦调用，该函数将当前每个 CPU 挂起的 softIRQ 位图保存在一个所谓的挂起变量中，并通过 `__local_bh_disable_ip` 本地禁用
    softIRQ。
- en: It then resets the current per-CPU pending bitmask (which has already been saved)
    and then reenables interrupts (softIRQs run with interrupts enabled).
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后重置当前每个 CPU 挂起位掩码（已保存），然后重新启用中断（softIRQ 在启用中断时运行）。
- en: After this, it enters a `while` loop, checking for pending softIRQs in the saved
    bitmap. If there is no softIRQ pending, nothing happens. Otherwise, it will execute
    the handlers of each pending softIRQ, taking care to increment their executions'
    statistics.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，它进入一个 `while` 循环，检查保存的位图中是否有挂起的 softIRQ。如果没有 softIRQ 挂起，则什么也不会发生。否则，它将执行每个挂起
    softIRQ 的处理程序，并注意增加它们的执行统计。
- en: After all the pending handlers have been executed (we are out of the `while`
    loop), `__do_softirq()` once again reads the per-CPU pending bitmask (required
    to disable IRQs and save them into the same pending variable) in order to check
    if any softIRQs were scheduled when it was in the `while` loop. If there are any
    pending softIRQs, the whole process will restart (based on a `goto` loop), starting
    from *step 2*. This helps with handling, for example, softIRQs that have rescheduled
    themselves.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在执行完所有挂起的处理程序之后（我们已经退出了`while`循环），`__do_softirq()`再次读取每个CPU的挂起位掩码（需要禁用IRQ并将它们保存到相同的挂起变量中），以检查在`while`循环中是否安排了任何softIRQs。如果有任何挂起的softIRQs，整个过程将重新启动（基于`goto`循环），从*步骤2*开始。这有助于处理例如重新安排自己的softIRQs。
- en: 'However, `__do_softirq()` will not repeat if one of the following conditions
    occurs:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果发生以下条件之一，`__do_softirq()`将不会重复：
- en: It has already repeated up to `MAX_SOFTIRQ_RESTART` times, which is set to `10`
    in `kernel/softirq.c`. This is actually the limit for the softIRQ processing loop,
    not the upper bound of the previously described `while` loop.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它已经重复了`MAX_SOFTIRQ_RESTART`次，该次数在`kernel/softirq.c`中设置为`10`。这实际上是softIRQ处理循环的限制，而不是先前描述的`while`循环的上限。
- en: It has hogged the CPU more than `MAX_SOFTIRQ_TIME`, which is set to 2 ms (`msecs_to_jiffies(2)`)
    in `kernel/softirq.c`, since this prevents the scheduler from being enabled.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它已经占用了CPU超过`MAX_SOFTIRQ_TIME`，在`kernel/softirq.c`中设置为2毫秒（`msecs_to_jiffies(2)`），因为这会阻止调度程序被启用。
- en: If one of the two situations occurs, `__do_softirq()` will break its loop and
    call `wakeup_softirqd()`to wake the local `ksoftirqd` thread, which will later
    execute the pending softIRQs in the process context. Since `do_softirq` is called
    at many points in the kernel, it is likely that another invocation of `__do_softirqs`
    will handle pending softIRQs before `ksoftirqd` has the chance to run.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 如果发生两种情况中的一种，`__do_softirq()`将中断其循环并调用`wakeup_softirqd()`来唤醒本地的`ksoftirqd`线程，后者将在进程上下文中执行挂起的softIRQs。由于`do_softirq`在内核中的许多点被调用，很可能在`ksoftirqd`有机会运行之前，另一个`__do_softirqs`的调用将处理挂起的softIRQs。
- en: Note that softIRQs do not always run in an atomic context, but in this case,
    this is quite specific. The next section explains how and why softIRQs may be
    executed in a process context.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，softIRQs并不总是在原子上下文中运行，但在这种情况下，这是非常特定的。下一节将解释softIRQs可能在进程上下文中执行的方式和原因。
- en: A word on ksoftirqd
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于ksoftirqd
- en: 'A `ksoftirqd` is a per-CPU kernel thread that’s raised in order to handle unserved
    software interrupts. It is spawned early on in the kernel boot process, as stated
    in `kernel/softirq.c`:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`ksoftirqd`是一个每个CPU的内核线程，用于处理未处理的软件中断。它在内核引导过程中早期生成，如`kernel/softirq.c`中所述：'
- en: '[PRE43]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: After running the `top` command, you will be able to see some `ksoftirqd/n`
    entries, where *n* is the logical CPU index of the CPU running the `ksoftirqd`
    thread. Since the `ksoftirqds` run in a process context, they are equal to classic
    processes/threads, and so are their competing claims for the CPU. `ksoftirqd`
    hogging CPUs for a long time may indicate a system under heavy load.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`top`命令后，您将能够看到一些`ksoftirqd/n`条目，其中*n*是运行`ksoftirqd`线程的CPU的逻辑CPU索引。由于`ksoftirqds`在进程上下文中运行，它们等同于经典的进程/线程，因此它们对CPU的竞争要求也是一样的。`ksoftirqd`长时间占用CPU可能表明系统负载很重。
- en: Now that we have finished looking at our first work deferring mechanism in the
    Linux kernel, we’ll discuss tasklets, which are an alternative (from an atomic
    context point of view) to softIRQs, though the former are built using the latter.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了对Linux内核中第一个工作延迟机制的讨论，我们将讨论tasklets，它们是一种替代（从原子上下文的角度来看）softIRQs的机制，尽管前者是使用后者构建的。
- en: Tasklets
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 任务
- en: Tasklets are *bottom halves* built on top of the `HI_SOFTIRQ` and `TASKLET_SOFTIRQ`
    softIRQs, with the only difference being that `HI_SOFTIRQ`-based tasklets run
    prior to the `TASKLET_SOFTIRQ`-based ones. This simply means tasklets are softIRQs,
    so they follow the same rules. *Unlike softIRQs however, two of the same tasklets
    never run concurrently*. The tasklet API is quite basic and intuitive.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: Tasklets是基于`HI_SOFTIRQ`和`TASKLET_SOFTIRQ` softIRQ构建的*底半部*，唯一的区别是基于`HI_SOFTIRQ`的tasklets在基于`TASKLET_SOFTIRQ`的tasklets之前运行。这意味着tasklets是softIRQs，因此它们遵循相同的规则。*但是，与softIRQs不同，两个相同的tasklets永远不会同时运行*。tasklet
    API非常基本和直观。
- en: 'Tasklets are represented by the `struct tasklet_struct` structure, which is
    defined in `<linux/interrupt.h>`. Each instance of this structure represents a
    unique tasklet:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 任务是由`struct tasklet_struct`结构表示，该结构在`<linux/interrupt.h>`中定义。该结构的每个实例表示一个唯一的任务：
- en: '[PRE44]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The `func` member is the handler of the tasklet that will be executed by the
    underlying softIRQ. It is the equivalent of what `action` is to a softIRQ, with
    the same prototype and the same argument meaning. `data` will be passed as its
    sole argument.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '`func`成员是tasklet的处理程序，将由底层softIRQ执行。它相当于softIRQ的`action`，具有相同的原型和相同的参数含义。`data`将作为其唯一参数传递。'
- en: 'You can use the `tasklet_init()` function to dynamically allocate and initialize
    tasklets at run-ime. For the static method, you can use the `DECLARE_TASKLET`
    macro. The option you choose will depend on your need (or requirement) to have
    a direct or indirect reference to the tasklet. Using `tasklet_init()` would require
    embedding the tasklet structure into a bigger and dynamically allocated object.
    An initialized tasklet can be scheduled by default – you could say it is enabled.
    `DECLARE_TASKLET_DISABLED` is an alternative to declaring default-disabled tasklets,
    and this will require the `tasklet_enable()` function to be invoked to make the
    tasklet schedulable. Tasklets are scheduled (similar to raising a softIRQ) via
    the `tasklet_schedule()` and `tasklet_hi_schedule()` functions. You can use `tasklet_disable()`
    to disable a tasklet. This function disables the tasklet and only returns when
    the tasklet has terminated its execution (assuming it was running). After this,
    the tasklet can still be scheduled, but it will not run on the CPU until it is
    enabled again. The asynchronous variant known as `tasklet_disable_nosync()` can
    be used too and returns immediately, even if termination has not occurred. Moreover,
    a tasklet that has been disabled several times should be enabled exactly the same
    number of times (this is allowed thanks to its `count` field):'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The kernel maintains normal priority and high priority tasklets in two different
    queues. Queues are actually singly linked lists, and each CPU has its own queue
    pair (low and high priority). Each processor has its own pair. `tasklet_schedule()`
    adds the tasklet to the normal priority list, thereby scheduling the associated
    softIRQ with a `TASKLET_SOFTIRQ` flag. With `tasklet_hi_schedule()`, the tasklet
    is added to the high priority list, thereby scheduling the associated softIRQ
    with a `HI_SOFTIRQ` flag. Once the tasklet has been scheduled, its `TASKLET_STATE_SCHED`
    flag is set, and the tasklet is added to a queue. At the time of execution, the
    `TASKLET_STATE_RUN` flag is set and the `TASKLET_STATE_SCHED` state is removed,
    thus allowing the tasklet to be rescheduled during its execution, either by the
    tasklet itself or from within an interrupt handler.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'High-priority tasklets are meant to be used for soft interrupt handlers with
    low latency requirements. Calling `tasklet_schedule()` on a tasklet that’s already
    been scheduled, but whose execution has not started, will do nothing, resulting
    in the tasklet being executed only once. A tasklet can reschedule itself, which
    means you can safely call `tasklet_schedule()` in a tasklet. High-priority tasklets
    are always executed before normal ones and should be used carefully; otherwise,
    you may increase system latency.Stopping a tasklet is as simple as calling `tasklet_kill()`,
    which will prevent the tasklet from running again or waiting for it to complete
    before killing it if the tasklet is currently scheduled to run. If the tasklet
    reschedules itself, you should prevent the tasklet from rescheduling itself prior
    to calling this function:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'That being said, let’s take a look at the following example of tasklet code
    usage:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: In the preceding code, we statically declared our `my_tasklet` tasklet and the
    function that’s supposed to be invoked when this tasklet is scheduled, along with
    the data that will be given as an argument to this function.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Because the same tasklet never runs concurrently, the locking case between a
    tasklet and itself doesn’t need to be addressed. However, any data that’s shared
    between two tasklets should be protected with `spin_lock()` and `spin_unlock()`.
    Remember, tasklets are implemented on top of softIRQs.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Workqueues
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we dealt with tasklets, which are atomically deferred
    mechanisms. Apart from atomic mechanisms, there are cases where we may want to
    sleep in the deferred task. Workqueues allow this.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'A workqueue is an asynchronous work deferring mechanism that is widely used
    across kernels, allowing them to run a dedicated function asynchronously in a
    process execution context. This makes them suitable for long-running and lengthy
    tasks or work that needs to sleep, thus improving the user experience. At the
    core of the workqueue subsystem, there are two data structures that can explain
    the concept behind this mechanism:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 工作队列是一种异步工作推迟机制，在内核中被广泛使用，允许它们在进程执行上下文中异步运行专用函数。这使它们适用于长时间运行和耗时的任务，或者需要休眠的工作，从而提高用户体验。在工作队列子系统的核心，有两个数据结构可以解释这种机制背后的概念：
- en: 'The work to be deferred (that is, the work item) is represented in the kernel
    by instances of `struct work_struct`, which indicates the handler function to
    be run. Typically, this structure is the first element of a user’s structure of
    the work definition. If you need a delay before the work can be run after it has
    been submitted to the workqueue, the kernel provides `struct delayed_work` instead.
    A work item is a basic structure that holds a pointer to the function that is
    to be executed asynchronously. To summarize, we can enumerate two types of work
    item structures:'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要延迟的工作（即工作项）在内核中由 `struct work_struct` 的实例表示，它指示要运行的处理函数。通常，这个结构是用户结构的工作定义的第一个元素。如果需要在提交给工作队列后延迟运行工作，内核提供了
    `struct delayed_work`。工作项是一个基本结构，它保存了要异步执行的函数的指针。总之，我们可以列举两种工作项结构：
- en: --The `struct work_struct` structure, which schedules a task to be run at a
    later time (as soon as possible when the system allows it).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: --`struct work_struct` 结构，安排一个任务在稍后的时间运行（尽快在系统允许的情况下）。
- en: --The `struct delayed_work` structure, which schedules a task to be run after
    at least a given time interval.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: --`struct delayed_work` 结构，安排一个任务在至少给定的时间间隔之后运行。
- en: The workqueue itself, which is represented by a `struct workqueue_struct`. This
    is the structure that work is placed on. It is a queue of work items.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作队列本身由 `struct workqueue_struct` 表示。这是工作被放置的结构。它是一个工作项队列。
- en: 'Apart from these data structures, there are two generic terms you should be
    familiar with:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些数据结构之外，还有两个通用术语你应该熟悉：
- en: '**Worker threads**, which are dedicated threads that execute the functions
    off the queue, one by one, one after the other.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工作线程**，这些是专用线程，按顺序执行队列中的函数。'
- en: '**Workerpools** are a collection of worker threads (a thread pool) that are
    used to manage worker threads.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Workerpools** 是一组用于管理工作线程的工作线程（线程池）。'
- en: 'The first step in using work queues consists of creating a work item, represented
    by `struct work_struct` or `struct delayed_work` for the delayed variant, that’s
    defined in `linux/workqueue.h`. The kernel provides either the `DECLARE_WORK`
    macro for statically declaring and initializing a work structure, or the `INIT_WORK`
    macro for doing the same by dynamically. If you need delayed work, you can use
    the `INIT_DELAYED_WORK` macro for dynamic allocation and initialization, or `DECLARE_DELAYED_WORK`
    for the static option:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 使用工作队列的第一步是创建一个工作项，由 `linux/workqueue.h` 中定义的 `struct work_struct` 或延迟变体的 `struct
    delayed_work` 表示。内核提供了 `DECLARE_WORK` 宏用于静态声明和初始化工作结构，或者 `INIT_WORK` 宏用于动态执行相同的操作。如果需要延迟工作，可以使用
    `INIT_DELAYED_WORK` 宏进行动态分配和初始化，或者使用 `DECLARE_DELAYED_WORK` 进行静态选项：
- en: '[PRE48]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The following code shows what our work item structure looks like:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了我们的工作项结构是什么样子的：
- en: '[PRE49]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The `func` field, which is of the `work_func_t` type, tells us a bit more about
    the header of a `work` function:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '`func` 字段是 `work_func_t` 类型，告诉我们有关 `work` 函数头的一些信息：'
- en: '[PRE50]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '`work` is an input parameter that corresponds to the work structure associated
    with your work. If you’ve submitted a delayed work, this would correspond to the
    `delayed_work.work` field. Here, it will be necessary to use the `to_delayed_work()`
    function to get the underlying delayed work structure:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '`work` 是一个输入参数，对应于与你的工作相关的工作结构。如果你提交了一个延迟的工作，这将对应于 `delayed_work.work` 字段。在这里，需要使用
    `to_delayed_work()` 函数来获取基础的延迟工作结构：'
- en: '[PRE51]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Workqueues let your driver create a kernel thread, called a worker thread,
    to handle deferred work. A new workqueue can be created with these functions:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 工作队列允许你的驱动程序创建一个内核线程，称为工作线程，来处理延迟的工作。可以使用以下函数创建一个新的工作队列：
- en: '[PRE52]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '`create_workqueue()` creates a dedicated thread (a worker) per CPU on the system,
    which is probably not a good idea. On an 8-core system, this will result in 8
    kernel threads being created to run work that’s been submitted to your workqueue.
    In most cases, a single system-wide kernel thread should be enough. In this case,
    you should use `create_singlethread_workqueue()` instead, which creates, as its
    name suggests, a single threaded workqueue; that is, with one worker thread system-wide.
    Either normal or delayed work can be enqueued on the same queue. To schedule works
    on your created workqueue, you can use either `queue_work()` or `queue_delayed_work()`,
    depending on the nature of the work:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '`create_workqueue()` 在系统的每个 CPU 上创建一个专用线程（工作线程），这可能不是一个好主意。在一个 8 核系统上，这将导致创建
    8 个内核线程来运行提交到你的工作队列的工作。在大多数情况下，一个全局的内核线程应该足够了。在这种情况下，你应该使用 `create_singlethread_workqueue()`，它创建一个单线程工作队列；也就是说，在整个系统中只有一个工作线程。可以在同一个队列上排定普通或延迟工作。要在创建的工作队列上安排工作，可以使用
    `queue_work()` 或 `queue_delayed_work()`，具体取决于工作的性质：'
- en: '[PRE53]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'These functions return false if the work was already on a queue and true otherwise.
    `queue_dalayed_work()` can be used to plan (delayed) work for execution with a
    given delay. The time unit for the delay is jiffies. If you don’t want to bother
    with seconds-to-jiffies conversion, you can use the `msecs_to_jiffies()` and `usecs_to_jiffies()`
    helpers, which convert milliseconds or microseconds into jiffies, respectively:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数如果工作已经在队列中则返回false，否则返回true。`queue_dalayed_work（）`可用于计划（延迟）工作以在给定延迟后执行。延迟的时间单位是jiffies。如果您不想麻烦秒到jiffies的转换，可以使用`msecs_to_jiffies（）`和`usecs_to_jiffies（）`辅助函数，分别将毫秒或微秒转换为jiffies：
- en: '[PRE54]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The following example uses 200 ms as a delay:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例使用200毫秒作为延迟：
- en: '[PRE55]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Submitted work items can be canceled by calling either `cancel_delayed_work()`,
    `cancel_delayed_work_sync()`, or `cancel_work_sync()`:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 提交的工作项可以通过调用`cancel_delayed_work（）`、`cancel_delayed_work_sync（）`或`cancel_work_sync（）`来取消：
- en: '[PRE56]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The following describes what these functions do:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 以下描述了这些函数的作用：
- en: '`cancel_work_sync()` synchronously cancels the given workqueue entry. In other
    words, it cancels `work` and waits for its execution to finish. The kernel guarantees
    that work won’t be pending or executing on any CPU when it’s return from this
    function, even if the work migrates to another workqueue or requeues itself. It
    returns `true` if `work` was pending, or `false` otherwise.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cancel_work_sync（）`同步取消给定的工作队列条目。换句话说，它取消`work`并等待其执行完成。内核保证当从此函数返回时，工作不会挂起或在任何CPU上执行，即使工作迁移到另一个工作队列或重新排队。如果`work`挂起，则返回`true`，否则返回`false`。'
- en: '`cancel_delayed_work()` asynchronously cancels a pending workqueue entry (a
    delayed one). It returns `true` (a non-zero value) if `dwork` was pending and
    canceled and `false` if it wasn’t pending, probably because it is actually running,
    and thus might still be running after `cancel_delayed_work()`. To ensure the work
    really ran to its end, you may want to use `flush_workqueue()`, which flushes
    every work item in the given queue, or `cancel_delayed_work_sync()`, which is
    the synchronous version of `cancel_delayed_work()`.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cancel_delayed_work（）`异步取消挂起的工作队列条目（延迟的）。如果`dwork`挂起并取消，则返回`true`（非零值），如果没有挂起，则返回`false`，可能是因为它实际上正在运行，因此在`cancel_delayed_work（）`之后可能仍在运行。为了确保工作确实运行到结束，您可能希望使用`flush_workqueue（）`，它会刷新给定队列中的每个工作项，或者使用`cancel_delayed_work_sync（）`，它是`cancel_delayed_work（）`的同步版本。'
- en: 'To wait for all the work items to finish, you can call `flush_workqueue()`.
    When you are done with a workqueue, you should destroy it with `destroy_workqueue()`.
    Both these options can be seen in the following code:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 要等待所有工作项完成，可以调用`flush_workqueue（）`。完成工作队列后，应使用`destroy_workqueue（）`销毁它。这两个选项可以在以下代码中看到：
- en: '[PRE57]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: While you’re waiting for any pending work to execute, the `_sync` variant functions
    sleep, which means they can only be called from a process context.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 当您等待任何挂起的工作执行时，`_sync`变体函数会休眠，这意味着它们只能从进程上下文中调用。
- en: The kernel shared queue
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内核共享队列
- en: In most situations, your code does not necessarily need to have the performance
    of its own dedicated set of threads, and because `create_workqueue()` creates
    one worker thread for each CPU, it may be a bad idea to use it on very large multi-CPU
    systems. In this situation, you may want to use the kernel shared queue, which
    has its own set of kernel threads preallocated (early during boot, via the `workqueue_init_early()`
    function) for running works.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，您的代码不一定需要拥有自己专用的线程集的性能，因为`create_workqueue（）`为每个CPU创建一个工作线程，所以在非常大的多CPU系统上使用它可能是一个坏主意。在这种情况下，您可能希望使用内核共享队列，它具有预先分配的一组内核线程（在引导期间通过`workqueue_init_early（）`函数提前分配）来运行工作。
- en: 'This global kernel workqueue is the so-called `system_wq`, and is defined in
    `kernel/workqueue.c`. There is one instance per CPU, with each backed by a dedicated
    thread named `events/n`, where `n` is the processor number that the thread is
    bound to. You can queue work to the system’s default workqueue using one of the
    following functions:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这个全局内核工作队列被称为`system_wq`，在`kernel/workqueue.c`中定义。每个CPU都有一个实例，每个实例都由一个名为`events/n`的专用线程支持，其中`n`是线程绑定的处理器编号。您可以使用以下函数之一将工作排队到系统的默认工作队列：
- en: '[PRE58]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '`schedule_work()` immediately schedules the work that will be executed as soon
    as possible after the worker thread on the current processor wakes up. With `schedule_delayed_work()`,
    the work will be put in the queue in the future, after the delay timer has ticked.
    The `_on` variants are used to schedule the work on a specific CPU (this does
    not need to be the current one). Each of these function queues work on the system’s
    shared workqueue, `system_wq`, which is defined in `kernel/workqueue.c`:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '`schedule_work（）`立即安排工作，该工作将在当前处理器上的工作线程唤醒后尽快执行。使用`schedule_delayed_work（）`，工作将在延迟计时器滴答后的未来放入队列中。`_on`变体用于在特定CPU上安排工作（这不需要是当前CPU）。这些函数中的每一个都在系统的共享工作队列`system_wq`上排队工作，该队列在`kernel/workqueue.c`中定义：'
- en: '[PRE59]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'To flush the kernel-global workqueue – that is, to ensure the given batch of
    work is completed – you can use `flush_scheduled_work()`:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 要刷新内核全局工作队列-也就是确保给定的工作批次已完成-可以使用`flush_scheduled_work（）`：
- en: '[PRE60]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '`flush_scheduled_work()` is a wrapper that calls `flush_workqueue()` on `system_wq`.
    Note that there may be work in `system_wq` that you have not submitted and have
    no control over. Due to this, flushing this workqueue entirely is overkill. It
    is recommended to use `cancel_delayed_work_sync()` or `cancel_work_sync()` instead.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '`flush_scheduled_work（）`是一个包装器，它在`system_wq`上调用`flush_workqueue（）`。请注意，`system_wq`中可能有您尚未提交且无法控制的工作。因此，完全刷新此工作队列是过度的。建议改用`cancel_delayed_work_sync（）`或`cancel_work_sync（）`。'
- en: Tip
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Unless you have a strong reason to create a dedicated thread, the default (kernel-global)
    thread is preferred.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 除非您有充分的理由创建专用线程，否则首选默认（内核全局）线程。
- en: Workqueues – a new generation
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作队列-新一代
- en: 'The original (now legacy) workqueue implementation used two kinds of workqueues:
    those with a **single thread system-wide**, and those with a **thread per-CPU**.
    However, due to the increasing number of CPUs, this led to some limitations:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 原始（现在是传统的）工作队列实现使用了两种工作队列：一种是**系统范围内单个线程**，另一种是**每个CPU一个线程**。然而，由于CPU数量的增加，这导致了一些限制：
- en: On very large systems, the kernel could run out of process IDs (defaulted to
    32k) just at boot, before the init was started.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在非常大的系统上，内核可能在启动时耗尽进程ID（默认为32k），甚至在init启动之前。
- en: Multi-threaded workqueues provided poor concurrency management as their threads
    competed for the CPU with other threads on the system. Since there were more CPU
    contenders, this introduced some overhead; that is, more context switches than
    necessary.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多线程工作队列提供了较差的并发管理，因为它们的线程与系统上的其他线程竞争CPU。由于有更多的CPU竞争者，这引入了一些开销；即比必要的更多的上下文切换。
- en: The consumption of much more resources than what was really needed.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消耗了比实际需要的更多资源。
- en: Moreover, subsystems that needed a dynamic or fine-grained level of concurrency
    had to implement their own thread pools. As a result of this, a new workqueue
    API has been designed and the legacy workqueue API (`create_workqueue()`, `create_singlethread_workqueue()`,
    and `create_freezable_workqueue()`) has been scheduled to be removed. However,
    these are actually wrappers around the new ones – the so-called concurrency-managed
    workqueues. This is done using per-CPU worker pools that are shared by all the
    workqueues in order to automatically provide a dynamic and flexible level of concurrency,
    thus abstracting such details for API users.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，需要动态或细粒度并发级别的子系统必须实现自己的线程池。因此，设计了一个新的工作队列API，并计划删除传统的工作队列API（`create_workqueue()`、`create_singlethread_workqueue()`和`create_freezable_workqueue()`）。然而，这些实际上是新API的包装器，即所谓的并发管理的工作队列。这是通过每个CPU的工作线程池来实现的，所有工作队列共享这些线程池，以自动提供动态和灵活的并发级别，从而为API用户抽象出这些细节。
- en: Concurrency-managed workqueues
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 并发管理的工作队列
- en: 'The concurrency-managed workqueue is an upgrade of the workqueue API. Using
    this new API implies that you must choose between two macros to create the workqueue:
    `alloc_workqueue()` and `alloc_ordered_workqueue()`. These macros both allocate
    a workqueue and return a pointer to it on success, and NULL on failure. The returned
    workqueue can be freed using the `destroy_workqueue()` function:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 并发管理的工作队列是工作队列API的升级。使用这个新API意味着你必须在`alloc_workqueue()`和`alloc_ordered_workqueue()`之间选择一个宏来创建工作队列。这些宏在成功时都分配一个工作队列并返回指针，失败时返回NULL。返回的工作队列可以使用`destroy_workqueue()`函数释放。
- en: '[PRE61]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '`fmt` is the `printf` format for the name of the workqueue, while `args...`
    are arguments for `fmt`. `destroy_workqueue()` is to be called on the workqueue
    once you are done with it. All work that’s currently pending will be completed
    first, before the kernel destroys the workqueue. `alloc_workqueue()` creates a
    workqueue based on `max_active`, which defines the concurrency level by limiting
    the number of work (tasks) that can be executing (workers in a runnable sate)
    simultaneously from this workqueue on any given CPU. For example, a `max_active`
    of 5 would mean that, at most, five work items on this workqueue can be executing
    at the same time per CPU. On the other hand, `alloc_ordered_workqueue()` creates
    a workqueue that processes each work item one by one in the queued order (that
    is, FIFO order).'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '`fmt`是工作队列名称的`printf`格式，而`args...`是`fmt`的参数。`destroy_workqueue()`在完成工作队列后调用。内核在销毁工作队列之前会先完成所有当前挂起的工作。`alloc_workqueue()`基于`max_active`创建一个工作队列，通过限制在任何给定CPU上同时执行（处于可运行状态的工作线程）的工作（任务）数量来定义并发级别。例如，`max_active`为5意味着在同一时间内每个CPU上最多可以执行五个工作队列的工作项。另一方面，`alloc_ordered_workqueue()`创建一个按队列顺序依次处理每个工作项的工作队列（即FIFO顺序）。'
- en: '`flags` controls how and when work items are queued, assigned execution resources,
    scheduled, and executed. Various flags are used in this new API. Let’s take a
    look at some of them:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '`flags`控制工作项如何排队、分配执行资源、调度和执行。在这个新API中使用了各种标志。让我们来看看其中一些：'
- en: '`WQ_UNBOUND`: Legacy workqueues had a worker thread per CPU and were designed
    to run tasks on the CPU where they were submitted. The kernel scheduler had no
    choice but to always schedule a worker on the CPU that it was defined on. With
    this approach, even a single workqueue could prevent a CPU from idling and being
    turned off, which leads to increased power consumption or poor scheduling policies.
    `WQ_UNBOUND` turns off this behavior. Work is not bound to a CPU anymore, hence
    the name unbound workqueues. There is no more locality, and the scheduler can
    reschedule the worker on any CPU as it sees fit. The scheduler has the last word
    now and can balance CPU load, especially for long and sometimes CPU-intensive
    work.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WQ_UNBOUND`：传统的工作队列每个CPU都有一个工作线程，并且设计为在提交任务的CPU上运行任务。内核调度程序只能在定义的CPU上调度工作线程。采用这种方法，即使是一个工作队列也可能阻止CPU空闲并关闭，从而导致增加功耗或者调度策略不佳。`WQ_UNBOUND`关闭了这种行为。工作不再绑定到CPU，因此被称为无绑定工作队列。不再有局部性，调度程序可以根据需要在任何CPU上重新调度工作线程。调度程序现在有最后的决定权，可以平衡CPU负载，特别是对于长时间且有时CPU密集的工作。'
- en: '`WQ_MEM_RECLAIM`: This flag is to be set for workqueues that need to guarantee
    forward progress during a memory reclaim path (when free memory is running dangerously
    low; here, the system is under memory pressure. In this case, `GFP_KERNEL` allocations
    may block and deadlock the entire workqueue). The workqueue is then guaranteed
    to have a ready-to-use worker thread, a so-called rescuer thread reserved for
    it, regardless of memory pressure, so that it can progress. One rescuer thread
    is allocated for each workqueue that has this flag set.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s consider a situation where we have three work items (*w1*, *w2*, and
    *w3*) in our workqueue, *W*. *w1* does some work and then waits for *w3* to complete
    (let’s say it depends on the computation result of *w3*). Afterward, *w2* (which
    is independent of the others) does some `kmalloc()` allocation (`GFP_KERNEL`).
    Now, it seems like there is not enough memory. While *w2* is blocked, it still
    occupies the workqueue of *W*. This results in *w3* not being able to run, despite
    the fact that there is no dependency between *w2* and *w3*. Since there is not
    enough memory available, there is no way to allocate a new thread to run *w3*.
    A pre-allocated thread would definitely solve this problem, not by magically allocating
    the memory for *w2*, but by running *w3* so that *w1* can continue its job, and
    so on. *w2* will continue its progression as soon as possible, when there is enough
    available memory to allocate. This pre-allocated thread is the so-called rescuer
    thread. You must set this `WQ_MEM_RECLAIM` flag if you think the workqueue might
    be used in the memory reclaim path. This flag replaces the old `WQ_RESCUER` flag
    as of the following commit: [https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=493008a8e475771a2126e0ce95a73e35b371d277](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=493008a8e475771a2126e0ce95a73e35b371d277).'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '`WQ_FREEZABLE`: This flag is used for power management purposes. A workqueue
    with this flag set will be frozen when the system is suspended or hibernates.
    On the freezing path, all current work(s) of the worker(s) will be processed.
    When the freeze is complete, no new work items will be executed until the system
    is unfrozen. Filesystem-related workqueue(s) may use this flag to ensure that
    modifications that are made to files are pushed to disk or create the hibernation
    image on the freezing path and that no modifications are made on-disk after the
    hibernation image has been created. In this situation, non-freezable items may
    do things differently that could lead to filesystem corruption. As an example,
    all of the XFS internal workqueues have this flag set (see `fs/xfs/xfs_super.c`)
    to ensure no further changes are made on disk once the freezer infrastructure
    freezes the kernel threads and creates the hibernation image. You should not set
    this flag if your workqueue can run tasks as part of the hibernation/suspend/resume
    process of the system. More information on this topic can be found in `Documentation/power/freezing-of-tasks.txt`,
    as well as by taking a look at the kernel’s internal `freeze_workqueues_begin()`
    and `thaw_workqueues()` functions.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WQ_HIGHPRI`: Tasks that have this flag set run immediately and do not wait
    for the CPU to become available. This flag is used for workqueues that queue work
    items that require high priority for execution. Such workqueues have worker threads
    with a high priority level (a lower `nice` value).'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the early days of the CMWQ, high-priority work items were just queued at
    the head of a global normal priority worklist so that they could immediately run.
    Nowadays, there is no interaction between normal priority and high-priority workqueues
    as each has its own worklist and its own worker pool. The work items of a high-priority
    workqueue are queued to the high-priority worker pool of the target CPU. Tasks
    in this workqueue should not block much. Use this flag if you do not want your
    work item competing for CPU with normal or lower-priority tasks. Crypto and Block
    subsystems use this, for example.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '`WQ_CPU_INTENSIVE`: Work items that are part of a CPU-intensive workqueue may
    burn a lot of CPU cycles and will not participate in the workqueue’s concurrency
    management. Instead, their execution is regulated by the system scheduler, just
    like any other task. This makes this flag useful for bound work items that may
    hog CPU cycles. Though their execution is regulated by the system scheduler, the
    start of their execution is still regulated by concurrency management, and runnable
    non-CPU-intensive work items can delay the execution of CPU-intensive work items.
    Actually, the crypto and dm-crypt subsystems use such workqueues. To prevent such
    tasks from delaying the execution of other non-CPU-intensive work items, they
    will not be taken into account when the workqueue code determines whether the
    CPU is available.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In order to be compliant with the old workqueue API, the following mappings
    are made to keep this API compatible with the original one:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '`create_workqueue(name)` is mapped to `alloc_workqueue(name,WQ_MEM_RECLAIM,
    1)`.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`create_singlethread_workqueue(name)` is mapped to `alloc_ordered_workqueue(name,
    WQ_MEM_RECLAIM)`.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`create_freezable_workqueue(name)` is mapped to `alloc_workqueue(name,WQ_FREEZABLE
    | WQ_UNBOUND|WQ_MEM_RECLAIM, 1)`.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To summarize, `alloc_ordered_workqueue()` actually replaces `create_freezable_workqueue()`
    and `create_singlethread_workqueue()` (as per the following commit: [https://git.kernel.org/pub/scm/linux/kernel/git/next/linux-next.git/commit/?id=81dcaf6516d8](https://git.kernel.org/pub/scm/linux/kernel/git/next/linux-next.git/commit/?id=81dcaf6516d8)).
    Workqueues allocated with `alloc_ordered_workqueue()` are unbound and have `max_active`
    set to `1`.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to scheduled items in a workqueue, the work items that have been
    queued to a specific CPU using `queue_work_on()` will execute on that CPU. Work
    items that have been queued via `queue_work()` will prefer the queueing CPU, though
    this locality is not guaranteed.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: Note that `schedule_work()` is a wrapper that calls `queue_work()` on the system
    workqueue (`system_wq`), while `schedule_work_on()` is a wrapper around `queue_work_on()`.
    Also, keep in mind that `system_wq = alloc_workqueue(“events”, 0, 0);`. Take a
    look at the `workqueue_init_early()` function in `kernel/workqueue.c` in the kernel
    sources to see how other system-wide workqueues are created.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: Memory reclaim is a Linux kernel mechanism on the memory allocation path. This
    consists of allocating memory after throwing the current content of that memory
    somewhere else.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have finished looking at workqueues and the concurrency-managed
    ones in particular. Next, we’ll introduce Linux kernel interrupt management, which
    is where most of the previous mechanisms will be solicited.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: Linux kernel interrupt management
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apart from servicing processes and user requests, another job of the Linux kernel
    is managing and speaking with hardware. This is either from the CPU to the device
    or from the device to the CPU. This is achieved by means of interrupts. An interrupt
    is a signal that’s sent to the processor by an external hardware device requesting
    immediate attention. Prior to an interrupt being visible to the CPU, this interrupt
    should be enabled by the interrupt controller, which is a device on its own, and
    whose main job consists of routing interrupts to CPUs.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: 'An interrupt may have five states:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '**Active**: An interrupt that has been acknowledged by a **processing element**
    (**PE**) and is being handled. While being handled, another assertion of the same
    interrupt is not presented as an interrupt to a processing element, until the
    initial interrupt is no longer active.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pending (asserted)**: An interrupt that is recognized as asserted in hardware,
    or generated by software, and is waiting to be handled by the target PE. It is
    a common behavior for most hardware devices not to generate other interrupts until
    their “interrupt pending” bit has been cleared. A disabled interrupt can’t be
    pending as it is never asserted, and it is immediately dropped by the interrupt
    controller.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Active and pending**: An interrupt that is active from one assertion of the
    interrupt and is pending from a subsequent assertion.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inactive**: An interrupt that is not active or pending. Deactivation clears
    the active state of the interrupt, and thereby allows the interrupt, when it is
    pending, to be taken again.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Disabled/Deactivated**: This is unknown to the CPU and not even seen by the
    interrupt controller. This will never be asserted. Disabled interrupts are lost.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: There are interrupt controllers where disabling an interrupt means masking that
    interrupt, or vice versa. In the remainder of this book, we will consider disabling
    to be the same as masking, though this is not always true.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: Upon reset, the processor disables all the interrupts until they are enabled
    again by the initialization code (this is the job of the Linux kernel in our case).
    The interrupts are enabled/disabled by setting/clearing the bits in the processor
    status/control registers. Upon an interrupt assertion (an interrupt occurred),
    the processor will check whether the interrupts are masked or not and will do
    nothing if they are masked. Once unmasked, the processor will pick one pending
    interrupt, if any (the order does not matter since it will do this for each pending
    interrupt until they are all serviced), and will execute a specially purposed
    function called the `kernel irq` core code) at a special location called the vector
    table. Right before the processor starts executing this ISR, it does some context
    saving (including the unmasked status of interrupts) and then masks the interrupts
    on the local CPU (interrupts can be asserted and will be serviced once unmasked).
    Once the ISR is running, we can say that the interrupt is being serviced.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the complete IRQ handling flow on ARM Linux. This happens
    when an interrupt occurs and the interrupts are enabled in the PSR:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: The ARM core will disable further interrupts occurring on the local CPU.
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The ARM core will then put the **Current Program Status Register** (**CPSR**)
    in the **Saved Program Status Register** (**SPSR**), put the current **Program
    Counter** (**PC**) in the **Link Register** (**LR**), and then switch to IRQ mode.
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, the ARM processor will refer to the vector table and jumps to the exception
    handler. In our case, it jumps to the exception handler of IRQ, which in the Linux
    kernel corresponds to the `vector_stub` macro defined in `arch/arm/kernel/entry-armv.S`.
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'These three steps are done by the ARM processor itself. Now, the kernel jumps
    into action:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: The `vector_stub` macro checks from what processor mode we used to get here
    – either kernel mode or user mode – and determines the macro to call accordingly;
    either `__irq_user` or `__irq_svc`.
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`__irq_svc()` will save the registers (from `r0` to `r12`) on the kernel stack
    and then call the `irq_handler()` macro, which either calls `handle_arch_irq()`
    (present in `arch/arm/include/asm/entry-macro-multi.S`) if `CONFIG_MULTI_IRQ_HANDLER`
    is defined, or `arch_irq_handler_default()` otherwise, with `handle_arch_irq`
    being a global pointer to the function that’s set in `arch/arm/kernel/setup.c`
    (from within the `setup_arch()` function).'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, we need to identify the hardware-IRQ number, which is what `asm_do_IRQ()`
    does. It then calls `handle_IRQ()` on that hardware-IRQ, which in turn calls `__handle_domain_irq()`,
    which will translate the hardware-irq into its corresponding Linux IRQ number
    (`irq = irq_find_mapping(domain, hwirq)`) and call `generic_handle_irq()` on the
    decoded Linux IRQ (`generic_handle_irq(irq)`).
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`generic_handle_irq()` will look for the IRQ descriptor structure (Linux’s
    view of an interrupt) that corresponds to the decoded Linux IRQ (`struct irq_desc
    *desc = irq_to_desc(irq)`) and calling `generic_handle_irq_desc()` on this descriptor),
    which will result in `desc->handle_irq(desc)`. `desc->handle_irq` corresponding
    to the high-level IRQ handler that was set using `irq_set_chip_and_handler()`
    during the mapping of this IRQ.'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`desc->handle_irq()` may result in a call to `handle_level_irq()`, `handle_simple_irq()`,
    `handle_edge_irq()`, and so on.'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The high-level IRQ handler calls our ISR.
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the ISR has been completed, `irq_svc` will return and restore the processor
    state by restoring registers (r0-r12), the PC, and the CSPR.
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Important note
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: 'Going back to *step 1*, during an interrupt, the ARM core disables further
    IRQs on the local CPU. It is worth mentioning that in the earlier Linux kernel
    days, there were two families of interrupt handlers: those running with interrupts
    disabled (that is, with the old `IRQF_DISABLED` flag set) and those running with
    interrupts enabled: they were then interruptible. The former were called **fast
    handlers**, while the latter were called **slow handlers**. For the latter, interrupts
    were actually reenabled by the kernel prior to invoking the handler.Since the
    interrupt context has a really small stack size compared to the process stack,
    it makes no sense that we may run into a stack overflow if we are in an interrupt
    context (running a given IRQ handler) while other interrupts keep occurring, even
    the one being serviced. This is confirmed by the commit at [https://git.kernel.org/pub/scm/linux/kernel/git/next/linux-next.git/commit/?id=e58aa3d2d0cc](https://git.kernel.org/pub/scm/linux/kernel/git/next/linux-next.git/commit/?id=e58aa3d2d0cc),
    which deprecated the fact of running interrupt handlers with IRQs enabled. As
    of this patch, IRQs remain disabled (left untouched after ARM core disabled them
    on the local CPU) during the execution of an IRQ handler. Additionally, the aforementioned
    flags have been entirely removed by the commit at [https://git.kernel.org/pub/scm/linux/kernel/git/next/linux-next.git/commit/?id=d8bf368d0631](https://git.kernel.org/pub/scm/linux/kernel/git/next/linux-next.git/commit/?id=d8bf368d0631),
    since Linux v4.1.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: Designing an interrupt handler
  id: totrans-313
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we’re familiar with the concept of bottom halves and deferring mechanisms,
    the time for us to implement interrupt handlers has come. In this section, we’ll
    take care of some specifics. Nowadays, the fact that interrupt handlers run with
    interrupts disabled (on the local CPU) means that we need to respect certain constraints
    in the ISR design:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '**Execution time:** Since IRQ handlers run with interrupts disabled on the
    local CPU, the code must be as short and as small as possible, as well as fast
    enough to ensure the previously disabled CPU-local interrupts are reenabled quickly
    in so that other IRQs are not missed. Time-consuming IRQ handlers may considerably
    alter the real-time properties of the system and slow it down.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Execution context**: Since interrupt handlers are executed in an atomic context,
    sleeping (or any other mechanism that may sleep, such as mutexes, copying data
    from kernel to user space or vice versa, and so on) is forbidden. Any part of
    the code that requires or involves sleeping must be deferred into another, safer
    context (that is, a process context).'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An IRQ handler needs to be given two arguments: the interrupt line to install
    the handler for, and a unique device identifier of the peripheral (mostly used
    as a context data structure; that is, the pointer to the per-device or private
    structure of the associated hardware device):'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The device driver that wants to enable a given interrupt and register an ISR
    for it should call `request_irq()`, which is declared in `<linux/interrupt.h>`.
    This must be included in the driver code:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'While the aforementioned API would require the caller to free the IRQ when
    it is no longer needed (that is, on driver detach), you can use the device managed
    variant, `devm_request_irq()`, which contains internal logic that allows it to
    take care of releasing the IRQ line automatically. It has the following prototype:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Except for the extra `dev` parameter (which is the device that requires the
    interrupt), both `devm_request_irq()` and `request_irq()` expect the following
    arguments:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '`irq`, which is the interrupt line (that is, the interrupt number of the issuing
    device). Prior to validating the request, the kernel will make sure the requested
    interrupt is valid and that it is not already assigned to another device, unless
    both devices request that this IRQ line needs to be shared (with the help of flags).'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`handler`, which is a function pointer to the interrupt handler.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`flags`, which represents the interrupt flags.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`name`, an ASCII string representing the name of the device generating or claiming
    this interrupt.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dev` should be unique to each registered handler. This cannot be `NULL` for
    shared IRQs since it is used to identify the device via the kernel IRQ core. The
    most common way of using it is to provide a pointer to the device structure or
    a pointer to any per-device (that’s potentially useful to the handler) data structure.
    This is because when an interrupt occurs, both the interrupt line (`irq`) and
    this parameter will be passed to the registered handler, which can use this data
    as context data for further processing.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`flags` mangle the state or behavior of the IRQ line or its handler by means
    of the following masks, which can be OR’ed to form the final desired bit mask
    according to your needs:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Note that flags can also be zero. Let’s take a look at some important flags.
    I’ll leave the rest for you to explore in `include/linux/interrupt.h`:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: '`IRQF_TRIGGER_HIGH` and `IRQF_TRIGGER_LOW` flags are to be used for level-sensitive
    interrupts. The former is for interrupts triggered at high level and the latter
    is for the low-level triggered interrupts. Level-sensitive interrupts are triggered
    as long as the physical interrupt signal is high. If the interrupt source is not
    cleared by the end of its interrupt handler in the kernel, the operating system
    will repeatedly call that kernel interrupt handler, which may lead platform to
    hang. In other words, when the handler services the interrupt and returns, if
    the IRQ line is still asserted, the CPU will signal the interrupt again immediately.
    To prevent such a situation, the interrupt must be acknowledged (that is, cleared
    or de-asserted) by the kernel interrupt handler immediately when it is received.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, those flags are safe with regard to interrupt sharing because if several
    devices pull the line active, an interrupt will be signaled (assuming the IRQ
    is enabled or as soon as it becomes so) until all drivers have serviced their
    devices. The only drawback is that it may lead to lockup if a driver fails to
    clear its interrupt source.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '`IRQF_TRIGGER_RISING` and `IRQF_TRIGGER_FALLING` concern edge-triggered interrupts,
    rising and falling edges respectively. Such interrupts are signaled when the line
    changes from inactive to active state, but only once. To get a new request the
    line must go back to inactive and then to active again. Most of the time, no special
    action is required in software in order to acknowledge this type of interrupt.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When using edge-triggered interrupts however, interrupts may be lost, especially
    in the context of a shared interrupt line: if one device pulls the line active
    for too long a time, when another device pulls the line active, no edge will be
    generated, the second request will not be seen by the processor and then will
    be ignored. With a shared edge-triggered interrupts, if a hardware does not de-assert
    the IRQ line, no other interrupt will be notified for either shared device.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: As a quick reminder, you can just remember that level triggered interrupts signal
    a state, while edge triggered ones signal an event.Moreover, when requesting an
    interrupt without specifying an `IRQF_TRIGGER` flag, the setting should be assumed
    to be *as already configured*, which may be as per machine or firmware initialization.
    In such cases, you can refer to the device tree (if specified in there) for example
    to see what this *assumed configuration* is.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '`IRQF_SHARED`: This allows the interrupt line to be shared among several devices.
    However, each device driver that needs to share the given interrupt line must
    set this flag; otherwise, the registration will fail.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IRQF_NOBALANCING`: This excludes the interrupt from *IRQ balancing*, which
    is a mechanism that consists of distributing/relocating interrupts across CPUs,
    with the goal of increasing performance. This prevents the CPU affinity of this
    IRQ from being changed. This flag can be used to provide a flexible setup for
    *clocksources* in order to prevent the event from being misattributed to the wrong
    core. This misattribution may result in the IRQ being disabled because if the
    CPU handling the interrupt is not the one that triggered it, the handler will
    return `IRQ_NONE`. This flag is only meaningful on multicore systems.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IRQF_IRQPOLL`: This flag allows the *irqpoll* mechanism to be used, which
    fixes interrupt problems. This means that this handler should be added to the
    list of known interrupt handlers that can be looked for when a given interrupt
    is not handled.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IRQF_ONESHOT`: Normally, the actual interrupt line being serviced is enabled
    after its hard-IRQ handler completes, whether it awakes a threaded handler or
    not. This flag keeps the interrupt line disabled after the hard-IRQ handler completes.
    This flag must be set on threaded interrupts (we will discuss this later) for
    which the interrupt line must remain disabled until the threaded handler has completed.
    After this, it will be enabled.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IRQF_NO_SUSPEND`: This does not disable the IRQ during system hibernation/suspension.
    This means that the interrupt is able to save the system from a suspended state.
    Such IRQs may be timer interrupts, which may trigger and need to be handled while
    the system is suspended. The whole IRQ line is affected by this flag in that if
    the IRQ is shared, every registered handler for this shared line will be executed,
    not just the one who installed this flag. You should avoid using `IRQF_NO_SUSPEND`
    and `IRQF_SHARED` at the same time as much as possible.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IRQF_FORCE_RESUME`: This enables the IRQ in the system resume path, even if
    `IRQF_NO_SUSPEND` is set.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IRQF_NO_THREAD`: This prevents the interrupt handler from being threaded.
    This flag overrides the `threadirqs` kernel (used on RT kernels, such as when
    applying the `PREEMPT_RT` patch) command-line option, which forces every interrupt
    to be threaded. This flag was introduced to address the non-threadability of some
    interrupts (for example, timers, which cannot be threaded even when all the interrupt
    handlers are forced to be threaded).'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IRQF_TIMER`: This marks this handler as being specific to the system timer
    interrupts. It helps not to disable the timer IRQ during system suspend to ensure
    that it resumes normally and does not thread them when full preemption (see `PREEMPT_RT`)
    is enabled. It is just an alias for `IRQF_NO_SUSPEND | IRQF_NO_THREAD`.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IRQF_EARLY_RESUME`: This resumes IRQ early at the resume time of system core
    (syscore) operations instead of at device resume time. Go to [https://lkml.org/lkml/2013/11/20/89](https://lkml.org/lkml/2013/11/20/89)
    to see the commit introducing its support.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We must also consider the return type, `irqreturn_t`, of interrupt handlers
    since they may involve further actions once the handler is returned:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: '`IRQ_NONE`: On a shared interrupt line, once the interrupt occurs, the kernel
    irqcore successively walks through the handlers that have been registered for
    this line and executes them in the order they have been registered. The driver
    then has the responsibility of checking whether it is their device that issued
    the interrupt. If the interrupt does not come from its device, it must return
    `IRQ_NONE` in order to instruct the kernel to call the next registered interrupt
    handler. This return value is mostly used on shared interrupt lines since it informs
    the kernel that the interrupt does not come from our device. However, if `__report_bad_irq()`
    function in the kernel source tree.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IRQ_HANDLED`: This value should be returned if the interrupt has been handled
    successfully. On a threaded IRQ, this value acknowledges the interrupt without
    waking the thread handler up.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IRQ_WAKE_THREAD`: On a thread IRQ handler, this value must be returned the
    by hard-IRQ handler in order to wake the handler thread. In this case, `IRQ_HANDLED`
    must only be returned by the threaded handler that was previously registered with
    `request_threaded_irq()`. We will discuss this later in the *Threaded IRQ handlers*
    section of this chapter.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: You must be very careful when reenabling interrupts in the handler. Actually,
    you must never reenable IRQs from within your IRQ handler as this would involve
    allowing “interrupts reentrancy”. In this case, it is your responsibility to address
    this.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: 'In the unloading path of your driver (or once you think you do not need the
    IRQ line anymore during your driver runtime life cycle, which is quite rare),
    you must release your IRQ resource by unregistering your interrupt handler and
    potentially disabling the interrupt line. The `free_irq()` interface does this
    for you:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'That being said, if an IRQ allocated with `devm_request_irq()` needs to be
    freed separately, `devm_free_irq()` must be used. It has the following prototype:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: This function has an extra `dev` argument, which is the device to free the IRQ
    for. This is usually the same as the one that the IRQ has been registered for.
    Except for `dev`, this function takes the same arguments and performs the same
    function as `free_irq()`. However, instead of `free_irq()`, it should be used
    to manually free IRQs that have been allocated with `devm_request_irq()`.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: Both `devm_request_irq()` and `free_irq()` remove the handler (identified by
    `dev_id` when it comes to shared interrupts) and disable the line. If the interrupt
    line is shared, the handler is simply removed from the list of handlers for this
    IRQ, and the interrupt line is disabled in the future when the last handler is
    removed. Moreover, if possible, your code must ensure the interrupt is really
    disabled on the card it drives before calling this function, since omitting this
    may leads to spurious IRQs.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: 'There are few things that are worth mentioning here about interrupts that you
    should never forget:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: Since interrupt handlers in Linux run with IRQs disabled on the local CPU and
    the current line is masked in all other cores, they don’t need to be reentrant,
    since the same interrupt will never be received until the current handler has
    completed. However, all other interrupts (on other cores) remain enabled (or should
    we say untouched), so other interrupts keep being serviced, even though the current
    line is always disabled, as well as further interrupts on the local CPU. Consequently,
    the same interrupt handler is never invoked concurrently to service a nested interrupt.
    This greatly simplifies writing your interrupt handler.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Critical regions that need to run with interrupts disabled should be limited
    as much as possible. To remember this, tell yourselves that your interrupt handler
    has interrupted other code and needs to give CPU back.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interrupt handlers cannot block as they do not run in a process context.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They may not transfer data to/from user space since this may block.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They may not sleep or rely on code that may lead to sleep, such as invoking
    `wait_event()`, memory allocation with anything other than `GFP_ATOMIC`, or using
    a mutex/semaphore. The threaded handler can handle this.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They may not trigger nor call `schedule()`.
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Only one interrupt on a given line can be pending (its interrupt flag bits get
    set when its interrupt condition occurs, regardless of the state of its corresponding
    enabled bit or the global enabled bit). Any further interrupt of this line is
    lost. For example, if you are processing an RX interrupt while five more packets
    are received at the same time, you should not expect five times more interrupts
    to appear sequentially. You’ll only be notified once. If the processor doesn’t
    service the ISR first, there’s no way to check how many RX interrupts will occur
    later. This means that if the device generates another interrupt before the handler
    function returns `IRQ_HANDLED`, the interrupt controller will be notified of the
    pending interrupt flag and the handler will get called again (only once), so you
    may miss some interrupts if you are not fast enough. Multiple interrupts will
    happen while you are still handling the first one.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: If an interrupt occurs while it is disabled (or masked), it will not be processed
    at all (masked in the flow handler), but will be recognized as asserted and will
    remain pending so that it will be processed when enabled (or unmasked).
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: The interrupt context has its own (fixed and quite low) stack size. Therefore,
    it totally makes sense to disable IRQs while running an ISR as reentrancy could
    cause stack overflow if too many preemptions happen.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: The concept of non-reentrancy for an interrupt means that if an interrupt is
    already in an active state, it cannot enter it again until the active status is
    cleared.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: The concept of top and bottom halves
  id: totrans-371
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: External devices send interrupt requests to the CPU either to signal a particular
    event or to request a service. As stated in the previous section, bad interrupt
    management may considerably increase system latency and decrease its real-time
    quality. We also stated that interrupt processing – that is, the hard-IRQ handler
    – must be very fast, not only to keep the system responsive, but also so that
    it doesn’t miss other interrupt events.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the following diagram:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.2 – Interrupt splitting flow'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_1.2_B10985.jpg)'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.2 – Interrupt splitting flow
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: The basic idea is that you split the interrupt handler into two parts. The first
    part is a function) that will run in a so-called hard-IRQ context, with interrupts
    disabled, and perform the minimum required work (such as doing some quick sanity
    checks, time-sensitive tasks, read/write hardware registers, and processing this
    data and acknowledging the interrupt on the device that raised it). This first
    part is the so-called top-half on Linux systems. The top-half then schedules a
    (sometimes threaded) handler, which then runs a so-called bottom-half function,
    with interrupts re-enabled. This is the second part of the interrupt. The bottom-half
    may then perform time-consuming tasks (such as buffer processing) – tasks that
    may sleep, depending on the deferring mechanism.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: This splitting would considerably increase the system’s responsiveness as the
    time spent with IRQs disabled is reduced to its minimum. When the bottom halves
    are run in kernel threads, they compete for the CPU with other processes on the
    runqueue. Moreover, they may have their real-time properties set. The top half
    is actually the handler that’s registered using `request_irq()`. When using `request_threaded_irq()`,
    as we will see in the next section, the top half is the first handler that’s given
    to the function.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: As we described previously, a bottom half represents any task (or work) that’s
    scheduled from within an interrupt handler. Bottom halves are designed using a
    work-deferring mechanism, which we have seen previously. Depending on which one
    you choose, it may run in a (software) interrupt context or in a process context.
    This includes *SoftIRQs*, *tasklets, workqueues*, and *threaded IRQs*.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: Tasklets and SoftIRQs do not actually fit into the so-called “thread interrupts”
    mechanism since they run in their own special contexts.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: Since softIRQ handlers run at a high priority with scheduler preemption disabled,
    they do not relinquish the CPU to processes/threads until they complete, so care
    must be taken while using them for bottom-half delegation. Nowadays, since the
    quantum that’s allocated for a particular process may vary, there is no strict
    rule regarding how long the softIRQ handler should take to complete so that it
    doesn’t slow the system down as the kernel would not be able to give CPU time
    to other processes. I would say that this should be no longer than a half of jiffy.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: 'The hard-IRQ handler (the top half) has to be as fast as possible, and most
    of time, it should just be reading and writing in I/O memory. Any other computation
    should be deferred to the bottom half, whose main goal is to perform any time-consuming
    and minimal interrupt-related work that’s not performed by the top half. There
    are no clear guidelines on repartitioning work between the top and bottom halves.
    The following is some advice:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: Hardware-related work and time-sensitive work should be performed in the top
    half.
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the work doesn’t need to be interrupted, perform it in the top half.
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From my point of view, everything else can be deferred – that is, performed
    in the bottom half – so that it runs with interrupts enabled and when the system
    is less busy.
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the hard-IRQ handler is fast enough to process and acknowledge interrupts
    consistently within a few microseconds, then there is absolutely no need to use
    bottom-half delegations at all.
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will look at threaded IRQ handlers.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: Threaded IRQ handlers
  id: totrans-389
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Threaded interrupt handlers were introduced to reduce the time spent in the
    interrupt handler and deferring the rest of the work (that is, processing) out
    to kernel threads. So, the top half (hard-IRQ handler) would consist of quick
    sanity checks such as ensuring that the interrupt comes from its device and waking
    the bottom half accordingly. A threaded interrupt handler runs in its own thread,
    either in the thread of their parent (if they have one) or in a separate kernel
    thread. Moreover, the dedicated kernel thread can have its real-time priority
    set, though it runs at normal real-time priority (that is, `MAX_USER_RT_PRIO/2`
    as shown in the `setup_irq_thread()` function in `kernel/irq/manage.c`).
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: 'The general rule behind threaded interrupts is simple: keep the hard-IRQ handler
    as minimal as possible and defer as much work to the kernel thread as possible
    (preferably all work). You should use `request_threaded_irq()` (defined in `kernel/irq/manage.c`)
    if you want to request a threaded interrupt handler:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'This function accepts two special parameters `handler` and `thread_fn`. The
    other parameters are the same as they are for `request_irq()`:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: '`handler` immediately runs when the interrupt occurs in the interrupt context,
    and acts as a hard-IRQ handler. Its job usually consists of reading the interrupt
    cause (in the device’s status register) to determine whether or how to handle
    the interrupt (this is frequent on MMIO devices). If the interrupt does not come
    from its device, this function should return `IRQ_NONE`. This return value usually
    only makes sense on shared interrupt lines. In the other case, if this hard-IRQ
    handler can finish interrupt processing fast enough (this is not a universal rule,
    but let’s say no longer than half a jiffy – that is, no longer than 500 µs if
    `CONFIG_HZ`, which defines the value of a jiffy, is set to 1,000) for a set of
    interrupt causes, it should return `IRQ_HANDLED` after processing in order to
    acknowledge the interrupts. Interrupt processing that does not fall into this
    time lapse should be deferred to the threaded IRQ handler. In this case, the hard-IRQ
    handler should return `IRQ_WAKE_T HREAD` in order to awake the threaded handler.
    Returning `IRQ_WAKE_THREAD` only makes sense when the `thread_fn` handler is also
    registered.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`thread_fn` is the threaded handler that’s added to the scheduler runqueue
    when the hard-IRQ handler function returns `IRQ_WAKE_THREAD`. If `thread_fn` is
    `NULL` while `handler` is set and it returns `IRQ_WAKE_THREAD`, nothing happens
    at the return path of the hard-IRQ handler except for a simple warning message
    being shown. Have a look at the `__irq_wake_thread()` function in the kernel sources
    for more information. As `thread_fn` competes for the CPU with other processes
    on the runqueue, it may be executed immediately or later in the future when the
    system has less load. This function should return `IRQ_HANDLED` when it has completed
    the interrupt handling process successfully. At this stage, the associated kthread
    will be taken off the runqueue and put in a blocked state until it’s woken up
    again by the hard-IRQ function.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A default hard-IRQ handler will be installed by the kernel if `handler` is
    `NULL` and `thread_fn != NULL`. This is the default primary handler. It is an
    almost empty handler that simply returns `IRQ_WAKE_THREAD` in order to wake up
    the associated kernel thread that will execute the `thread_fn` handler. This makes
    it possible to move the execution of interrupt handlers entirely to the process
    context, thus preventing buggy drivers (buggy IRQ handlers) from breaking the
    whole system and reducing interrupt latency. A dedicated handler’s kthreads will
    be visible in `ps ax`:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Important note
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: Nowadays, `request_irq()` is just a wrapper around `request_threaded_irq()`,
    with the `thread_fn` parameter set to `NULL`.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: Note that the interrupt is acknowledged at the interrupt controller level when
    you return from the hard-IRQ handler (whatever the return value is), thus allowing
    you to take other interrupts into account. In such a situation, if the interrupt
    hasn’t been acknowledged at the device level, the interrupt will fire again and
    again, resulting in stack overflows (or being stuck in the hard-IRQ handler forever)
    for level-triggered interrupts since the issuing device still has the interrupt
    line asserted. Before threaded IRQs were a thing, when you needed to run the bottom-half
    in a thread, you would instruct the top half to disable the IRQ at the device
    level, prior to waking the thread up. This way, even if the controller is ready
    to accept another interrupt, it is not raised again by the device.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: 'The `IRQF_ONESHOT` flag resolves this problem. It must be set when it comes
    to use a threaded interrupt (at the `request_threaded_irq()` call); otherwise,
    the request will fail with the following error:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: For more information on this, please have a look at the `__setup_irq()` function
    in the kernel source tree.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an excerpt from the message that introduced the `IRQF_ONESHOT`
    flag and explains what it does (the entire message can be found at [http://lkml.iu.edu/hypermail/linux/kernel/0908.1/02114.html](http://ebay.co.uk)):'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: “It allows drivers to request that the interrupt is not unmasked (at the controller
    level) after the hard interrupt context handler has been executed and the thread
    has been woken. The interrupt line is unmasked after the thread handler function
    has been executed.”
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: If you omit the `IRQF_ONESHOT` flag, you’ll have to provide a hard-IRQ handler
    (in which you should disable the interrupt line); otherwise, the request will
    fail.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of a thread-only IRQ is as follows:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: In the preceding example, our device sits on an I2C bus. Thus, accessing the
    available data may cause it to sleep, so this should not be performed in the hard-IRQ
    handler. This is why our handler parameter is `NULL`.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: If the IRQ line where you need threaded ISR handling to be shared among several
    devices (for example, some SoCs share the same interrupt among their internal
    ADCs and the touchscreen module), you must implement the hard-IRQ handler, which
    should check whether the interrupt has been raised by your device or not. If the
    interrupt does come from your device, you should disable the interrupt at the
    device level and return `IRQ_WAKE_THREAD` to wake the threaded handler. The interrupt
    should be enabled back at the device level in the return path of the threaded
    handler. If the interrupt does not come from your device, you should return `IRQ_NONE`
    directly from the hard-IRQ handler.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, if one driver has set either the `IRQF_SHARED` or `IRQF_ONESHOT` flag
    on the line, every other driver sharing the line must set the same flags. The
    `/proc/interrupts` file lists the IRQs and their processing per CPU, the IRQ name
    that was given during the requesting step, and a comma-separated list of drivers
    that registered an ISR for that interrupt.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: Threaded IRQs are the best choice for interrupt processing as they can hog too
    many CPU cycles (exceeding a jiffy in most cases), such as bulk data processing.
    Threading IRQs allow the priority and CPU affinity of their associated thread
    to be managed individually. Since this concept comes from the real-time kernel
    tree (from *Thomas Gleixner*), it fulfills many requirements of a real-time system,
    such as allowing a fine-grained priority model to be used and reducing interrupt
    latency in the kernel.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: Take a look at `/proc/irq/IRQ_NUMBER/smp_affinity`, which can be used to get
    or set the corresponding `IRQ_NUMBER` affinity. This file returns and accepts
    a bitmask that represents which processors can handle ISRs that have been registered
    for this IRQ. This way, you can, for example, decide to set the affinity of a
    hard-IRQ to one CPU while setting the affinity of the threaded handler to another
    CPU.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: Requesting a context IRQ
  id: totrans-416
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A driver requesting an IRQ must know the nature of the interrupt in advance
    and decide whether its handler can run in the hard-IRQ context in order to call
    `request_irq()` or `request_threaded_irq()` accordingly.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a problem when it comes to request IRQ lines provided by discrete
    and non-MMIO-based interrupt controllers, such as I2C/SPI gpio-expanders. Since
    accessing those buses may cause them to sleep, it would be a disaster to run the
    handler of such slow controllers in a hard-IRQ context. Since the driver does
    not contain any information about the nature of the interrupt line/controller,
    the IRQ core provides the `request_any_context_irq()` API. This function determines
    whether the interrupt controller/line can sleep and calls the appropriate requesting
    function:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '`request_any_context_irq()` and `request_irq()` have the same interface but
    different semantics. Depending on the underlying context (the hardware platform),
    `request_any_context_irq()` selects either a hardIRQ handling method using `request_irq()`
    or a threaded handling method using `request_threaded_irq()`. It returns a negative
    error value on failure, while on success, it returns either `IRQC_IS_HARDIRQ`
    (meaning hardI-RQ handling is used) or `IRQC_IS_NESTED` (meaning the threaded
    version is used). With this function, the behavior of the interrupt handler is
    decided at runtime. For more information, take a look at the comment introducing
    it in the kernel by following this link: [https://git.kernel.org/pub/scm/linux/kernel/git/next/linux-next.git/commit/?id=ae731f8d0785](https://git.kernel.org/pub/scm/linux/kernel/git/next/linux-next.git/commit/?id=ae731f8d0785).'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of using `request_any_context_irq()` is that you don’t need to
    care about what can be done in the IRQ handler. This is because the context in
    which the handler will run depends on the interrupt controller that provides the
    IRQ line. For example, for a gpio-IRQ-based device driver, if the gpio belongs
    to a controller that seats on an I2C or SPI bus (in which case gpio access may
    sleep), the handler will be threaded. Otherwise (the gpio access may not sleep
    and is memory mapped as it belongs to the SoC), the handler will run in the hardIRQ
    context.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, the device expects an IRQ line mapped to a gpio.
    The driver cannot assume that the given gpio line will be memory mapped since
    it’s coming from the SoC. It may come from a discrete I2C or SPI gpio controller
    as well. A good practice would be to use `request_any_context_irq()` here:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-423
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: The preceding code is simple enough but is quite safe thanks to `request_any_context_irq()`,
    which prevents us from mistaking the type of the underlying gpio.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: Using a workqueue to defer a bottom-half
  id: totrans-425
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since we have already discussed the workqueue API, we will provide an example
    of how to use it here. This example is not error-free and has not been tested.
    It is just a demonstration that highlights the concept of bottom-half deferring
    by means of a workqueue.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by defining the data structure that will hold the elements we need
    for further development:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'In the preceding data structure, our work structure is represented by the `my_work`
    element. We aren’t using the pointer here because we will need to use the `container_of()`
    macro to grab a pointer to the initial data structure. Next, we can define the
    method that will be invoked in the worker thread:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  id: totrans-430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'In the preceding code, we start data processing when enough data has been buffered.
    Now, we can provide our IRQ handler, which is responsible for scheduling our work,
    as follows:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'The comments in the IRQ handler code are meaningful enough. `schedule_work()`
    is the function that schedules our work. Finally, we can write our `probe` method,
    which will request our IRQ and register the previous handler:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: The structure of the preceding probe method shows without a doubt that we are
    facing a platform device driver. Generic IRQ and workqueue APIs have been used
    here to initialize our workqueue and register our handler.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: Locking from within an interrupt handler
  id: totrans-436
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If a resource is shared between two or more use contexts (kthread, work, threaded
    IRQ, and so on) and only with a threaded bottom-half (that is, they’re never accessed
    by the hard-IRQ), then mutex locking is the way to go, as shown in the following
    example:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: In the preceding code, both the user task (kthread, work, and so on) and the
    threaded bottom half must hold the mutex before accessing the resource.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding case is the simplest one to exemplify. The following are some
    rules that will help you lock between hard-IRQ contexts and others:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: '*If a resource is shared between a user context and a hard interrupt handler*,
    you will want to use the spinlock variant, which disables interrupts; that is,
    the simple `_irq` or `_irqsave`/`_irq_restore` variants. This ensures that the
    user context is never preempted by this IRQ when it’s accessing the resource.
    This can be seen in the following example:'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE79]'
  id: totrans-442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: In the preceding code, the hard-IRQ handler doesn’t need to hold the spinlock
    as it can never be preempted. Only the user context must be held. There is a case
    where protection may not be necessary between the hard-IRQ and its threaded counterpart;
    that is, when the `IRQF_ONESHOT` flag is set while requesting the IRQ line. This
    flag keeps the interrupt disabled after the hard-IRQ handler has finished. With
    this flag set, the IRQ line remains disabled until the threaded handler has been
    run until its completion. This way, the hard-IRQ handler and its threaded counterpart
    will never compete and a lock for a resource shared between the two might not
    be necessary.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: 'When the resource is shared between user context and softIRQ, there are two
    things you need to guard against: the fact the user context can be interrupted
    by the softIRQ (remember, softIRQs run on the return path of hard-IRQ handlers)
    and the fact that the critical region can be entered from another CPU (remember,
    the same softIRQ may run concurrently on another CPU). In this case, you should
    use spinlock API variants that will disable softIRQs; that is, `spin_lock_bh()`
    and `spin_unlock_bh()`. The `_bh` prefix means the bottom half. Because those
    APIs have not been discussed in detail in this chapter, you can use the `_irq`
    or even `_irqsave` variants, which disable hardware interrupts as well.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The same applies to tasklets (because tasklets are built on top of softIRQs),
    with the only difference that a tasklet never runs concurrently (it never runs
    on more than one CPU at once); a tasklet is exclusive by design.
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are two things to guard against when it comes to locking between hard
    IRQ and softIRQ: the softIRQ can be interrupted by the hard-IRQ and the critical
    region can be entered (`1` for either by another hard-IRQ if designed in this
    way, `2` by the same softIRQ, or `3` by another softIRQ) from another CPU. Because
    the softIRQ can never run when the hard-IRQ handler is running, hard-IRQ handlers
    only need to use the `spin_lock()` and `spin_unlock()` APIs, which prevent concurrent
    access by other hard handlers on another CPU. However, softIRQ needs to use the
    locking API that actually disables interrupts – that is, the `_irq()` or `irqsave()`
    variants – with a preference for the latter.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because softIRQs may run concurrently, *locking may be necessary between two
    different softIRQs, or even between a softIRQ and itself* (running on another
    CPU). In this case, `spinlock()`/`spin_unlock()` should be used. There’s no need
    to disable hardware interrupts.
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At this point, we are done looking at interrupt locking, which means we have
    come to the end of this chapter.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-449
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduced some core kernel functionalities that will be used in
    the next few chapters of this book. The concepts we covered concerned bit manipulation
    to Linux kernel interrupt design and implementation, through locking helpers and
    work deferring mechanisms. By now, you should be able to decide whether you should
    split your interrupt handler into two parts or not, as well as know what locking
    primitive suits your needs.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll cover Linux kernel managed resources, which is an
    interface that’s used to offload allocated resource management to the kernel core.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
