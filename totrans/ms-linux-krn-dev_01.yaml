- en: Comprehending Processes, Address Space, and Threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When kernel services are invoked in the current process context, its layout
    throws open the right path for exploring kernels in more detail. Our effort in
    this chapter is centered around comprehending processes and the underlying ecosystem
    the kernel provides for them. We will explore the following concepts in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Program to process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Process layout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Virtual address spaces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kernel and user space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Process APIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Process descriptors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kernel stack management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Threads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linux thread API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data structures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Namespace and cgroups
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Quintessentially, computing systems are designed, developed, and often tweaked
    for running user applications efficiently. Every element that goes into a computing
    platform is intended to enable effective and efficient ways for running applications.
    In other words, computing systems exist to run diverse application programs. Applications
    can run either as firmware in dedicated devices or as a "process" in systems driven
    by system software (operating systems).
  prefs: []
  type: TYPE_NORMAL
- en: At its core, a process is a running instance of a program in memory. The transformation
    from a program to a process happens when the program (on disk) is fetched into
    memory for execution.
  prefs: []
  type: TYPE_NORMAL
- en: A program’s binary image carries **code** (with all its binary instructions)
    and **data** (with all global data), which are mapped to distinct regions of memory
    with appropriate access permissions (read, write, and execute). Apart from code
    and data, a process is assigned additional memory regions called **stack** (for
    allocation of function call frames with auto variables and function arguments)
    and *heap* for dynamic allocations at runtime.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple instances of the same program can exist with their respective memory
    allocations. For instance, for a web browser with multiple open tabs (running
    simultaneous browsing sessions), each tab is considered a process instance by
    the kernel, with unique memory allocations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure represents the layout of processes in memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00005.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The illusion called address space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Modern-day computing platforms are expected to handle a plethora of processes
    efficiently. Operating systems thus must deal with allocating unique memory to
    all contending processes within the physical memory (often finite) and also ensure
    their reliable execution. With multiple processes contending and executing simultaneously
    (*multi-tasking*), the operating system must ensure that the memory allocation
    of every process is protected from accidental access by another process.
  prefs: []
  type: TYPE_NORMAL
- en: To address this issue, the kernel provides a level of abstraction between the
    process and the physical memory called *virtual* *address space*. Virtual address
    space is the process' view of memory; it is how the running program views the
    memory.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual address space creates an illusion that every process exclusively owns
    the whole memory while executing. This abstracted view of memory is called *virtual
    memory* and is achieved by the kernel's memory manager in coordination with the
    CPU's MMU. Each process is given a contiguous 32 or 64-bit address space, bound
    by the architecture and unique to that process. With each process caged into its
    virtual address space by the MMU, any attempt by a process to access an address
    region outside its boundaries will trigger a hardware fault, making it possible
    for the memory manger to detect and terminate violating processes, thus ensuring
    protection.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure depicts the illusion of address space created for every
    contending process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00006.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Kernel and user space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Modern operating systems not only prevent one process from accessing another
    but also prevent processes from accidentally accessing or manipulating kernel
    data and services (as the kernel is shared by all the processes).
  prefs: []
  type: TYPE_NORMAL
- en: Operating systems achieve this protection by segmenting the whole memory into
    two logical halves, the user and kernel space. This bifurcation ensures that all
    processes that are assigned address spaces are mapped to the user space section
    of memory and kernel data and services run in kernel space. The kernel achieves
    this protection in coordination with the hardware. While an application process
    is executing instructions from its code segment, the CPU is operating in user
    mode. When a process intends to invoke a kernel service, it needs to switch the
    CPU into privileged mode (kernel mode), which is achieved through special functions
    called APIs (application programming interfaces). These APIs enable user processes
    to switch into the kernel space using special CPU instructions and then execute
    the required services through *system calls*. On completion of the requested service,
    the kernel executes another mode switch, this time back from kernel mode to user
    mode, using another set of CPU instructions.
  prefs: []
  type: TYPE_NORMAL
- en: System calls are the kernel's interfaces to expose its services to application
    processes; they are also called *kernel entry points*. As system calls are implemented
    in kernel space, the respective handlers are provided through APIs in the user
    space. API abstraction also makes it easier and convenient to invoke related system
    calls.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure depicts a virtualized memory view:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00007.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Process context
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When a process requests a kernel service through a system call, the kernel will
    execute on behalf of the caller process. The kernel is now said to be executing
    in *process context*. Similarly, the kernel also responds to *interrupts* raised
    by other hardware entities; here, the kernel executes in *interrupt context*.
    When in interrupt context, the kernel is not running on behalf of any process.
  prefs: []
  type: TYPE_NORMAL
- en: Process descriptors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Right from the time a process is born until it exits, it’s the kernel's process
    management subsystem that carries out various operations, ranging from process
    creation, allocating CPU time, and event notifications to destruction of the process
    upon termination.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from the address space, a process in memory is also assigned a data structure
    called the *process descriptor*, which the kernel uses to identify, manage, and
    schedule the process. The following figure depicts process address spaces with
    their respective process descriptors in the kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00008.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In Linux, a process descriptor is an instance of type `struct task_struct` defined
    in `<linux/sched.h>`, it is one of the central data structures, and contains all
    the attributes, identification details, and resource allocation entries that a
    process holds. Looking at `struct task_struct` is like a peek into the window
    of what the kernel sees or works with to manage and schedule a process.
  prefs: []
  type: TYPE_NORMAL
- en: Since the task structure contains a wide set of data elements, which are related
    to the functionality of various kernel subsystems, it would be out of context
    to discuss the purpose and scope of all the elements in this chapter. We shall
    consider a few important elements that are related to process management.
  prefs: []
  type: TYPE_NORMAL
- en: Process attributes - key elements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Process attributes define all the key and fundamental characteristics of a process.
    These elements contain the process's state and identifications along with other
    key values of importance.
  prefs: []
  type: TYPE_NORMAL
- en: state
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A process right from the time it is spawned until it exits may exist in various
    states, referred to as *process states*--they define the process’s current state:'
  prefs: []
  type: TYPE_NORMAL
- en: '**TASK_RUNNING** (0): The task is either executing or contending for CPU in
    the scheduler run-queue.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TASK_INTERRUPTIBLE** (1): The task is in an interruptible wait state; it
    remains in wait until an awaited condition becomes true, such as the availability
    of mutual exclusion locks, device ready for I/O, lapse of sleep time, or an exclusive
    wake-up call. While in this wait state, any signals generated for the process
    are delivered, causing it to wake up before the wait condition is met.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TASK_KILLABLE**: This is similar to **TASK_INTERRUPTIBLE**, with the exception
    that interruptions can only occur on fatal signals, which makes it a better alternative
    to **TASK_INTERRUPTIBLE**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TASK_UNINTERRUTPIBLE** (2): The task is in uninterruptible wait state similar
    to **TASK_INTERRUPTIBLE**, except that generated signals to the sleeping process
    do not cause wake-up. When the event occurs for which it is waiting, the process
    transitions to **TASK_RUNNING**. This process state is rarely used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TASK_ STOPPED** (4): The task has received a STOP signal. It will be back
    to running on receiving the continue signal (SIGCONT).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TASK_TRACED** (8): A process is said to be in traced state when it is being
    combed, probably by a debugger.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**EXIT_ZOMBIE** (32): The process is terminated, but its resources are not
    yet reclaimed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**EXIT_DEAD** (16): The child is terminated and all the resources held by it
    freed, after the parent collects the exit status of the child using *wait*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following figure depicts process states:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00009.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: pid
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This field contains a unique process identifier referred to as **PID**. PIDs
    in Linux are of the type `pid_t` (integer). Though a PID is an integer, the default
    maximum number PIDs is 32,768 specified through the `/proc/sys/kernel/pid_max`
    interface. The value in this file can be set to any value up to 2^(22) (`PID_MAX_LIMIT`,
    approximately 4 million).
  prefs: []
  type: TYPE_NORMAL
- en: To manage PIDs, the kernel uses a bitmap. This bitmap allows the kernel to keep
    track of PIDs in use and assign a unique PID for new processes. Each PID is identified
    by a bit in the PID bitmap; the value of a PID is determined from the position
    of its corresponding bit. Bits with value 1 in the bitmap indicate that the corresponding
    PIDs are in *use*, and those with value 0 indicate free PIDs. Whenever the kernel
    needs to assign a unique PID, it looks for the first unset bit and sets it to
    1, and conversely to free a PID, it toggles the corresponding bit from 1 to 0.
  prefs: []
  type: TYPE_NORMAL
- en: tgid
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This field contains the thread group id. For easy understanding, let's say when
    a new process is created, its PID and TGID are the same, as the process happens
    to be the only thread. When the process spawns a new thread, the new child gets
    a unique PID but inherits the TGID from the parent, as it belongs to the same
    thread group. The TGID is primarily used to support multi-threaded process. We
    will delve into further details in the threads section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: thread info
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This field holds processor-specific state information, and is a critical element
    of the task structure. Later sections of this chapter contain details about the
    importance of `thread_info`.
  prefs: []
  type: TYPE_NORMAL
- en: flags
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The flags field records various attributes corresponding to a process. Each
    bit in the field corresponds to various stages in the lifetime of a process. Per-process
    flags are defined in `<linux/sched.h>`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: exit_code and exit_signal
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These fields contain the exit value of the task and details of the signal that
    caused the termination. These fields are to be accessed by the parent process
    through `wait()` on termination of the child.
  prefs: []
  type: TYPE_NORMAL
- en: comm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This field holds the name of the binary executable used to start the process.
  prefs: []
  type: TYPE_NORMAL
- en: ptrace
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This field is enabled and set when the process is put into trace mode using
    the `ptrace()` system call.
  prefs: []
  type: TYPE_NORMAL
- en: Process relations - key elements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Every process can be related to a parent process, establishing a parent-child
    relationship. Similarly, multiple processes spawned by the same process are called
    *siblings*. These fields establish how the current process relates to another
    process.
  prefs: []
  type: TYPE_NORMAL
- en: real_parent and parent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These are pointers to the parent's task structure. For a normal process, both
    these pointers refer to the same `task_struct`*;* they only differ for multi-thread
    processes, implemented using `posix` threads. For such cases, `real_parent` refers
    to the parent thread task structure and parent refers the process task structure
    to which SIGCHLD is delivered.
  prefs: []
  type: TYPE_NORMAL
- en: children
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is a pointer to a list of child task structures.
  prefs: []
  type: TYPE_NORMAL
- en: sibling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is a pointer to a list of sibling task structures.
  prefs: []
  type: TYPE_NORMAL
- en: group_leader
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is a pointer to the task structure of the process group leader.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduling attributes - key elements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All contending processes must be given fair CPU time, and this calls for scheduling
    based on time slices and process priorities. These attributes contain necessary
    information that the scheduler uses when deciding on which process gets priority
    when contending.
  prefs: []
  type: TYPE_NORMAL
- en: prio and static_prio
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`prio` helps determine the priority of the process for scheduling. This field
    holds static priority of the process within the range `1` to `99` (as specified
    by `sched_setscheduler()`) if the process is assigned a real-time scheduling policy.
    For normal processes, this field holds a dynamic priority derived from the nice
    value.'
  prefs: []
  type: TYPE_NORMAL
- en: se, rt, and dl
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Every task belongs to a scheduling entity (group of tasks), as scheduling is
    done at a per-entity level. `se` is for all normal processes, `rt` is for real-time
    processes, and `dl` is for deadline processes. We will discuss more on these attributes
    in the next chapter on scheduling.
  prefs: []
  type: TYPE_NORMAL
- en: policy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This field contains information about the scheduling policy of the process,
    which helps in determining its priority.
  prefs: []
  type: TYPE_NORMAL
- en: cpus_allowed
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This field specifies the CPU mask for the process, that is, on which CPU(s)
    the process is eligible to be scheduled in a multi-processor system.
  prefs: []
  type: TYPE_NORMAL
- en: rt_priority
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This field specifies the priority to be applied by real-time scheduling policies.
    For non-real-time processes, this field is unused.
  prefs: []
  type: TYPE_NORMAL
- en: Process limits - key elements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The kernel imposes resource limits to ensure fair allocation of system resources
    among contending processes. These limits guarantee that a random process does
    not monopolize ownership of resources. There are 16 different types of resource
    limits, and the `task structure` points to an array of type `struct rlimit`*,*
    in which each offset holds the current and maximum values for a specific resource.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: File descriptor table - key elements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: During the lifetime of a process, it may access various resource files to get
    its task done. This results in the process opening, closing, reading, and writing
    to these files. The system must keep track of these activities; file descriptor
    elements help the system know which files the process holds.
  prefs: []
  type: TYPE_NORMAL
- en: fs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Filesystem information is stored in this field.
  prefs: []
  type: TYPE_NORMAL
- en: files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The file descriptor table contains pointers to all the files that a process
    opens to perform various operations. The files field contains a pointer, which
    points to this file descriptor table.
  prefs: []
  type: TYPE_NORMAL
- en: Signal descriptor - key elements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For processes to handle signals, the *task structure* has various elements that
    determine how the signals must be handled.
  prefs: []
  type: TYPE_NORMAL
- en: signal
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is of type `struct signal_struct`*,* which contains information on all
    the signals associated with the process.
  prefs: []
  type: TYPE_NORMAL
- en: sighand
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is of type `struct sighand_struct`*,* which contains all signal handlers
    associated with the process.
  prefs: []
  type: TYPE_NORMAL
- en: sigset_t blocked, real_blocked
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These elements identify signals that are currently masked or blocked by the
    process.
  prefs: []
  type: TYPE_NORMAL
- en: pending
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is of type `struct sigpending`*,* which identifies signals which are generated
    but not yet delivered.
  prefs: []
  type: TYPE_NORMAL
- en: sas_ss_sp
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This field contains a pointer to an alternate stack, which facilitates signal
    handling.
  prefs: []
  type: TYPE_NORMAL
- en: sas_ss_size
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This filed shows the size of the alternate stack, used for signal handling.
  prefs: []
  type: TYPE_NORMAL
- en: Kernel stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With current-generation computing platforms powered by multi-core hardware capable
    of running simultaneous applications, the possibility of multiple processes concurrently
    initiating kernel mode switch when requesting for the same process is built in.
    To be able to handle such situations, kernel services are designed to be re-entrant,
    allowing multiple processes to step in and engage the required services. This
    mandated the requesting process to maintain its own private kernel stack to keep
    track of the kernel function call sequence, store local data of the kernel functions,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: The kernel stack is directly mapped to the physical memory, mandating the arrangement
    to be physically in a contiguous region. The kernel stack by default is 8kb for
    x86-32 and most other 32-bit systems (with an option of 4k kernel stack to be
    configured during kernel build), and 16kb on an x86-64 system.
  prefs: []
  type: TYPE_NORMAL
- en: 'When kernel services are invoked in the current process context, they need
    to validate the process’s prerogative before it commits to any relevant operations.
    To perform such validations, the kernel services must gain access to the task
    structure of the current process and look through the relevant fields. Similarly,
    kernel routines might need to have access to the current `task structure` for
    modifying various resource structures such as signal handler tables, looking for
    pending signals, file descriptor table, and memory descriptor among others. To
    enable accessing the `task structure` at runtime, the address of the current `task
    structure` is loaded into a processor register (register chosen is architecture
    specific) and made available through a kernel global macro called `current` (defined
    in architecture-specific kernel header `asm/current.h` ):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: However, in register-constricted architectures, where there are few registers
    to spare, reserving a register to hold the address of the current task structure
    is not viable. On such platforms, the `task structure` of the current process
    is directly made available at the top of the kernel stack that it owns. This approach
    renders a significant advantage with respect to locating the `task structure`,
    by just masking the least significant bits of the stack pointer.
  prefs: []
  type: TYPE_NORMAL
- en: With the evolution of the kernel, the `task structure` grew and became too large
    to be contained in the kernel stack, which is already restricted in physical memory
    (8Kb). As a result, the `task structure` was moved out of the kernel stack, barring
    a few key fields that define the process's CPU state and other low-level processor-specific
    information. These fields were then wrapped in a newly created structure called
    `struct thread_info`*.* This structure is contained on top of the kernel stack
    and provides a pointer that refers to the current `task structure`, which can
    be used by kernel services.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'With `thread_info` containing process-related information, apart from `task
    structure`, the kernel has multiple viewpoints to the current process structure:
    `struct task_struct`, an architecture-independent information block, and `thread_info`,
    an architecture-specific one. The following figure depicts **thread_info** and
    **task_struct**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00010.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'For architectures that engage `thread_info`*,* the current macro''s implementation
    is modified to look into the top of kernel stack to obtain a reference to the
    current `thread_info` and through it the `current task structure`. The following
    code snippet shows the implementation of current for an x86-64 platform:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '**[PRE6]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The issue of stack overflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unlike user mode, the kernel mode stack lives in directly mapped memory. When
    a process invokes a kernel service, which may internally be deeply nested, chances
    are that it may overrun into immediate memory range. The worst part of it is the
    kernel will be oblivious to such occurrences. Kernel programmers usually engage
    various debug options to track stack usage and detect overruns, but these methods
    are not handy to prevent stack breaches on production systems. Conventional protection
    through the use of *guard pages* is also ruled out here (as it wastes an actual
    memory page).
  prefs: []
  type: TYPE_NORMAL
- en: Kernel programmers tend to follow coding standards--minimizing the use of local
    data, avoiding recursion, and avoiding deep nesting among others--to cut down
    the probability of a stack breach. However, implementation of feature-rich and
    deeply layered kernel subsystems may pose various design challenges and complications,
    especially with the storage subsystem where filesystems, storage drivers, and
    networking code can be stacked up in several layers, resulting in deeply nested
    function calls.
  prefs: []
  type: TYPE_NORMAL
- en: The Linux kernel community has been pondering over preventing such breaches
    for quite long, and toward that end, the decision was made to expand the kernel
    stack to 16kb (x86-64, since kernel 3.15). Expansion of the kernel stack might
    prevent some breaches, but at the cost of engaging much of the directly mapped
    kernel memory for the per-process kernel stack. However, for reliable functioning
    of the system, it is expected of the kernel to elegantly handle stack breaches
    when they show up on production systems.
  prefs: []
  type: TYPE_NORMAL
- en: With the 4.9 release, the kernel has come with a new system to set up virtually
    mapped kernel stacks. Since virtual addresses are currently in use to map even
    a directly mapped page, principally the kernel stack does not actually require
    physically contiguous pages. The kernel reserves a separate range of addresses
    for virtually mapped memory, and addresses from this range are allocated when
    a call to `vmalloc()` is made. This range of memory is referred as the **vmalloc
    range**. Primarily this range is used when programs require huge chunks of memory
    which are virtually contiguous but physically scattered. Using this, the kernel
    stack can now be allotted as individual pages, mapped to the vmalloc range. Virtual
    mapping also enables protection from overruns as a no-access guard page can be
    allocated with a page table entry (without wasting an actual page). Guard pages
    would prompt the kernel to pop an oops message on memory overrun and initiate
    a kill against overrunning process.
  prefs: []
  type: TYPE_NORMAL
- en: Virtually mapped kernel stacks with guard pages are currently available only
    for the x86-64 architecture (support for other architectures seemingly to follow).
    This can be enabled by choosing the `HAVE_ARCH_VMAP_STACK` or `CONFIG_VMAP_STACK`
    build-time options.
  prefs: []
  type: TYPE_NORMAL
- en: Process creation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: During kernel boot, a kernel thread called `init is` spawned, which in turn
    is configured to initialize the first user-mode process (with the same name).
    The `init` (pid 1) process is then configured to carry out various initialization
    operations specified through configuration files, creating multiple processes.
    Every child process further created (which may in turn create its own child process(es))
    are all descendants of the *init* process. Processes thus created end up in a
    tree-like structure or a single hierarchy model. The `shell`, which is one such
    process, becomes the interface for users to create user processes, when programs
    are called for execution.
  prefs: []
  type: TYPE_NORMAL
- en: Fork, vfork, exec, clone, wait and exit are the core kernel interfaces for the
    creation and control of new process. These operations are invoked through corresponding
    user-mode APIs.
  prefs: []
  type: TYPE_NORMAL
- en: fork()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`Fork()` is one of the core "Unix thread APIs" available across *nix systems
    since the inception of legacy Unix releases. Aptly named, it forks a new process
    from a running process. When `fork()` succeeds, the new process is created (referred
    to as `child`) by duplicating the caller''s `address space` and `task structure`.
    On return from `fork()`, both caller (parent) and new process (child) resume executing
    instructions from the same code segment which was duplicated under copy-on-write.
    `Fork()` is perhaps the only API that enters kernel mode in the context of caller
    process, and on success returns to user mode in the context of both caller and
    child (new process).'
  prefs: []
  type: TYPE_NORMAL
- en: Most resource entries of the parent's `task structure` such as memory descriptor,
    file descriptor table, signal descriptors, and scheduling attributes are inherited
    by the child, except for a few attributes such as memory locks, pending signals,
    active timers, and file record locks (for the full list of exceptions, refer to
    the fork(2) man page). A child process is assigned a unique `pid` and will refer
    to its parent's `pid` through the `ppid` field of its `task structure`*;* the
    child’s resource utilization and processor usage entries are reset to zero.
  prefs: []
  type: TYPE_NORMAL
- en: The parent process updates itself about the child’s state using the `wait()`
    system call and normally waits for the termination of the child process. Failing
    to call `wait()`*,* the child may terminate and be pushed into a zombie state.
  prefs: []
  type: TYPE_NORMAL
- en: Copy-on-write (COW)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Duplication of parent process to create a child needs cloning of the user mode
    address space (`stack`, `data`, `code`, and `heap` segments) and task structure
    of the parent for the child; this would result in execution overhead that leads
    to un-deterministic process-creation time. To make matters worse, this process
    of cloning would be rendered useless if neither parent nor child did not initiate
    any state-change operations on cloned resources.
  prefs: []
  type: TYPE_NORMAL
- en: As per COW, when a child is created, it is allocated a unique `task structure`
    with all resource entries (including page tables) referring to the parent's `task
    structure`, with read-only access for both parent and child. Resources are truly
    duplicated when either of the processes initiates a state change operation, hence
    the name *copy-on-write* (`write` in COW implies a state change). COW does bring
    effectiveness and optimization to the fore, by deferring the need for duplicating
    process data until write, and in cases where only read happens, it avoids it altogether.
    This on-demand copying also reduces the number of swap pages needed, cuts down
    the time spent on swapping, and might help reduce demand paging.
  prefs: []
  type: TYPE_NORMAL
- en: exec
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At times creating a child process might not be useful, unless it runs a new
    program altogether: the `exec` family of calls serves precisely this purpose.
    `exec` replaces the existing program in a process with a new executable binary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The `execve` is the system call that executes the program binary file, passed
    as the first argument to it. The second and third arguments are null-terminated
    arrays of arguments and environment strings, to be passed to a new program as
    command-line arguments. This system call can also be invoked through various `glibc`
    (library) wrappers, which are found to be more convenient and flexible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Command-line user-interface programs such as `shell` use the `exec` interface
    to launch user-requested program binaries.
  prefs: []
  type: TYPE_NORMAL
- en: vfork()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unlike `fork()`, `vfork()` creates a child process and blocks the parent, which
    means that the child runs as a single thread and does not allow concurrency; in
    other words, the parent process is temporarily suspended until the child exits
    or call `exec()`. The child shares the data of the parent.
  prefs: []
  type: TYPE_NORMAL
- en: Linux support for threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The flow of execution in a process is referred to as a **thread**, which implies
    that every process will at least have one thread of execution. Multi-threaded
    means the existence of multiple flows of execution contexts in a process. With
    modern many-core architectures, multiple flows of execution in a process can be
    truly concurrent, achieving fair multitasking.
  prefs: []
  type: TYPE_NORMAL
- en: Threads are normally enumerated as pure user-level entities within a process
    that are scheduled for execution; they share parent's virtual address space and
    system resources. Each thread maintains its code, stack, and thread local storage.
    Threads are scheduled and managed by the thread library, which uses a structure
    referred to as a thread object to hold a unique thread identifier, for scheduling
    attributes and to save the thread context. User-level thread applications are
    generally lighter on memory, and are the preferred model of concurrency for event-driven
    applications. On the flip side, such user-level thread model is not suitable for
    parallel computing, since they are tied onto the same processor core to which
    their parent process is bound.
  prefs: []
  type: TYPE_NORMAL
- en: Linux doesn’t support user-level threads directly; it instead proposes an alternate
    API to enumerate a special process, called **l****ight weight process** (**LWP**),
    that can share a set of configured resources such as dynamic memory allocations,
    global data, open files, signal handlers, and other extensive resources with the
    parent process. Each LWP is identified by a unique PID and task structure, and
    is treated by the kernel as an independent execution context. In Linux, the term
    thread invariably refers to LWP, since each thread initialized by the thread library
    (`Pthreads`) is enumerated as an LWP by the kernel.
  prefs: []
  type: TYPE_NORMAL
- en: clone()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`clone()` is a Linux-specific system call to create a new process; it is considered
    a generic version of the `fork()` system call, offering finer controls to customize
    its functionality through the `flags` argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: It provides more than twenty different `CLONE_*` flags that control various
    aspects of the `clone` operation, including whether the parent and child process
    share resources such as virtual memory, open file descriptors, and signal dispositions.
    The child is created with the appropriate memory address (passed as the second
    argument) to be used as the `stack` (for storing the child's local data). The
    child process starts its execution with its start function (passed as the first
    argument to the clone call).
  prefs: []
  type: TYPE_NORMAL
- en: 'When a process attempts to create a thread through the `pthread` library, `clone()`
    is invoked with the following flags:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00011.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The `clone()` can also be used to create a regular child process that is normally
    spawned using `fork()` and `vfork()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Kernel threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To augment the need for running background operations, the kernel spawns threads
    (similar to processes). These kernel threads are similar to regular processes,
    in that they are represented by a task structure and assigned a PID. Unlike user
    processes, they do not have any address space mapped, and run exclusively in kernel
    mode, which makes them non-interactive. Various kernel subsystems use `kthreads`
    to run periodic and asynchronous operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'All kernel threads are descendants of `kthreadd (pid 2)`, which is spawned
    by the `kernel (pid 0)` during boot. The `kthreadd` enumerates other kernel threads;
    it provides interface routines through which other kernel threads can be dynamically
    spawned at runtime by kernel services. Kernel threads can be viewed from the command
    line with the `ps -ef` command--they are shown in [square brackets]:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The previous code shows the kernel boot routine `rest_init()` invoking the `kernel_thread()`
    routine with appropriate arguments to spawn both the `kernel_init` thread (which
    then goes on to start the user-mode `init` process) and `kthreadd`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `kthread` is a perpetually running thread that looks into a list called
    `kthread_create_list` for data on new `kthreads` to be created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '`kthread_create_on_node()` instantiates details (received as arguments) of
    `kthread` to be created into a structure of type `kthread_create_info` and queues
    it at the tail of `kthread_create_list`. It then wakes up `kthreadd` and waits
    for thread creation to complete:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Recall that `kthreadd` invokes the `create_thread()` routine to start kernel
    threads as per data queued into the list. This routine creates the thread and
    signals completion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: do_fork() and copy_process()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'All of the process/thread creation calls discussed so far invoke different
    system calls (except `create_thread`) to step into kernel mode. All of those system
    calls in turn converge into the common kernel `function _do_fork()`, which is
    invoked with distinct `CLONE_*` flags. `do_fork()` internally falls back on `copy_process()`
    to complete the task. The following figure sums up the call sequence for process
    creation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00012.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Process status and termination
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: During the lifetime of a process, it traverses through many states before it
    ultimately terminates. Users must have proper mechanisms to be updated with all
    that happens to a process during its lifetime. Linux provides a set of functions
    for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: wait
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For processes and threads created by a parent, it might be functionally useful
    for the parent to know the execution status of the child process/thread. This
    can be achieved using the `wait` family of system calls:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'These system calls update the calling process with the state change events
    of a child. The following state change events are notified:'
  prefs: []
  type: TYPE_NORMAL
- en: Termination of child
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stopped by a signal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resumed by a signal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to reporting the status, these APIs allow the parent process to
    reap a terminated child. A process on termination is put into zombie state until
    the immediate parent engages the `wait` call to reap it.
  prefs: []
  type: TYPE_NORMAL
- en: exit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Every process must end. Process termination is done either by the process calling
    `exit()` or when the main function returns. A process may also be terminated abruptly
    on receiving a signal or exception that forces it to terminate, such as the `KILL`
    command, which sends a signal to kill the process, or when an exception is raised.
    Upon termination, the process is put into exit state until the immediate parent
    reaps it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `exit` calls the `sys_exit` system call, which internally calls the `do_exit`
    routine. The `do_exit` primarily performs the following tasks (`do_exit` sets
    many values and makes multiple calls to related kernel routines to complete its
    task):'
  prefs: []
  type: TYPE_NORMAL
- en: Takes the exit code returned by the child to the parent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sets the `PF_EXITING` flag, indicating process exiting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cleans up and reclaims the resources held by the process. This includes releasing
    `mm_struct`, removal from the queue if it is waiting for an IPC semaphore, release
    of filesystem data and files, if any, and calling `schedule()` as the process
    is no longer executable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After `do_exit`, the process remains in zombie state and the process descriptor
    is still intact for the parent to collect the status, after which the resources
    are reclaimed by the system.
  prefs: []
  type: TYPE_NORMAL
- en: Namespaces and cgroups
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Users logged into a Linux system have a transparent view of various system entities
    such as global resources, processes, kernel, and users. For instance, a valid
    user can access PIDs of all running processes on the system (irrespective of the
    user to which they belong). Users can observe the presence of other users on the
    system, and they can run commands to view the state of global system global resources
    such as memory, filesystem mounts, and devices. Such operations are not deemed
    as intrusions or considered security breaches, as it is always guaranteed that
    one user/process can never intrude into other user/process.
  prefs: []
  type: TYPE_NORMAL
- en: However, such transparency is unwarranted on a few server platforms. For instance,
    consider cloud service providers offering **PaaS** (**platform as a service**).
    They offer an environment to host and deploy custom client applications. They
    manage runtime, storage, operating system, middleware, and networking services,
    leaving customers to manage their applications and data. PaaS services are used
    by various e-commerce, financial, online gaming, and other related enterprises.
  prefs: []
  type: TYPE_NORMAL
- en: For efficient and effective isolation and resource management for clients, PaaS
    service providers use various tools. They virtualize the system environment for
    each client to achieve security, reliability, and robustness. The Linux kernel
    provides low-level mechanisms in the form of cgroups and namespaces for building
    various lightweight tools that can virtualize the system environment. Docker is
    one such framework that builds on cgroups and namespaces.
  prefs: []
  type: TYPE_NORMAL
- en: Namespaces fundamentally are mechanisms to abstract, isolate, and limit the
    visibility that a group of processes has over various system entities such as
    process trees, network interfaces, user IDs, and filesystem mounts. Namespaces
    are categorized into several groups, which we will now see.
  prefs: []
  type: TYPE_NORMAL
- en: Mount namespaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Traditionally, mount and unmount operations will change the filesystem view
    as seen by all processes in the system; in other words, there is one global mount
    namespace seen by all processes. The mount namespaces confine the set of filesystem
    mount points visible within a process namespace, enabling one process group in
    a mount namespace to have an exclusive view of the filesystem list compared to
    another process.
  prefs: []
  type: TYPE_NORMAL
- en: UTS namespaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These enable isolating the system's host and domain name within a uts namespace.
    This makes initialization and configuration scripts able to be guided based on
    the respective namespaces.
  prefs: []
  type: TYPE_NORMAL
- en: IPC namespaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These demarcate processes from using System V and POSIX message queues. This
    prevents one process from an ipc namespace accessing the resources of another.
  prefs: []
  type: TYPE_NORMAL
- en: PID namespaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Traditionally, *nix kernels (including Linux) spawn the `init` process with
    PID 1 during system boot, which in turn starts other user-mode processes and is
    considered the root of the process tree (all the other processes start below this
    process in the tree). The PID namespace allows a process to spin off a new tree
    of processes under it with its own root process (PID 1 process). PID namespaces
    isolate process ID numbers, and allow duplication of PID numbers across different
    PID namespaces, which means that processes in different PID namespaces can have
    the same process ID. The process IDs within a PID namespace are unique, and are
    assigned sequentially starting with PID 1.
  prefs: []
  type: TYPE_NORMAL
- en: PID namespaces are used in containers (lightweight virtualization solution)
    to migrate a container with a process tree, onto a different host system without
    any changes to PIDs.
  prefs: []
  type: TYPE_NORMAL
- en: Network namespaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This type of namespace provides abstraction and virtualization of network protocol
    services and interfaces. Each network namespace will have its own network device
    instances that can be configured with individual network addresses. Isolation
    is enabled for other network services: routing table, port number, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: User namespaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: User namespaces allow a process to use unique user and group IDs within and
    outside a namespace. This means that a process can use privileged user and group
    IDs (zero) within a user namespace and continue with non-zero user and group IDs
    outside the namespace.
  prefs: []
  type: TYPE_NORMAL
- en: Cgroup namespaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A cgroup namespace virtualizes the contents of the `/proc/self/cgroup` file.
    Processes inside a cgroup namespace are only able to view paths relative to their
    namespace root.
  prefs: []
  type: TYPE_NORMAL
- en: Control groups (cgroups)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cgroups are kernel mechanisms to restrict and measure resource allocations to
    each process group. Using cgroups, you can allocate resources such as CPU time,
    network, and memory.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the process model in Linux, where each process is a child to a parent
    and relatively descends from the `init` process thus forming a single-tree like
    structure, cgroups are hierarchical, where child cgroups inherit the attributes
    of the parent, but what makes is different is that multiple cgroup hierarchies
    can exist within a single system, with each having distinct resource prerogatives.
  prefs: []
  type: TYPE_NORMAL
- en: Applying cgroups on namespaces results in isolation of processes into `containers`
    within a system, where resources are managed distinctly. Each *container* is a
    lightweight virtual machine, all of which run as individual entities and are oblivious
    of other entities within the same system.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are namespace APIs described in the Linux man page for `namespaces`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We understood one of the principal abstractions of Linux called the process*,*
    and the whole ecosystem that facilitates this abstraction. The challenge now remains
    in running the scores of processes by providing fair CPU time. With many-core
    systems imposing a multitude of processes with diverse policies and priorities,
    the need for deterministic scheduling is paramount.
  prefs: []
  type: TYPE_NORMAL
- en: In our next chapter, we will delve into process scheduling, another critical
    aspect of process management, and comprehend how the Linux scheduler is designed
    to handle this diversity.**
  prefs: []
  type: TYPE_NORMAL
