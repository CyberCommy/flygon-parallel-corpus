- en: GPU Parallelization in Go
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GPU accelerated programming is becoming more and more important in today's high-performance
    computing stacks. It is commonly used in fields such as **Artificial Intelligence**
    (**AI**) and **Machine Learning** (**ML**). GPUs are commonly used for these tasks
    because they tend to be excellent for parallel computation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn about Cgo, GPU accelerated programming, **CUDA**
    (short for **Compute Unified Device Architecture**), make commands, C style linking
    for Go programs, and executing a GPU enabled process within a Docker container.
    Learning all of these individual things will help us to use a GPU to power a Go
    backed CUDA program. Doing this will help us to determine how we can use the GPU
    effectively to help solve computational problems using Go:'
  prefs: []
  type: TYPE_NORMAL
- en: Cgo – writing C in Go
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPU-accelerated computing – utilizing the hardware
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CUDA on GCP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CUDA – powering the program
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cgo – writing C in Go
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cgo is a library that is built into the standard library of Go that allows users
    to invoke calls to underlying C programs in their Go code. Cgo is often used as
    a delegator for things that are currently coded in C but don't have equivalent
    Go code written.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cgo should be used sparingly and only when there isn''t an equivalent Go library
    available for a system. Cgo adds a few limitations to your Go programs:'
  prefs: []
  type: TYPE_NORMAL
- en: Needless complexity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Difficulty troubleshooting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added complexity of building and compiling C code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Much of Go's tooling is not available for use in Cgo programs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cross-compiling doesn't work as expected or at all
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The complexity of C code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Native Go calls are much faster than Cgo calls
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Slower build times
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you can (or must) live with all of these stipulations, Cgo may be a necessary
    resource for the project that you're working on.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few instances where it is appropriate to use Cgo. A couple of primary
    examples are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: When you must use a proprietary **Software Development Kit** (**SDK**) or proprietary
    library.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you have a legacy piece of software in C that would be difficult to port
    to Go because of business logic validation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You've exhausted the Go runtime to its limit and you need further optimization.
    It is very rare that we get to this particular case.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'More excellent cgo documentation can be found at the following URLs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://golang.org/cmd/cgo/](https://golang.org/cmd/cgo/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://blog.golang.org/c-go-cgo](https://blog.golang.org/c-go-cgo)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we are going to take a look at a simple cgo example in
    order to familiarize ourselves with how Cgo works, along with some of its highlights
    and shortcomings.
  prefs: []
  type: TYPE_NORMAL
- en: A simple Cgo example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s take a look at a relatively straightforward Cgo example. In this example,
    we will write a simple function to print "Hello Gophers" from a C binding and
    then we will call that C code from our Go program. In this function, we return
    a constant character string. We then call the `hello_gophers` C function within
    our Go program. We also use the `C.GoString` function to convert the C string
    type and the Go string type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Once this program has been executed, we can see a simple `Hello Gophers!` output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3678c8b4-c73c-4039-b628-2bc85cd3e795.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This example, while simple, shows us how we can bind C functions in our Go
    programs. To further emphasize the difference in execution time, we can look at
    a benchmark of our Cgo function and our Go function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then use these functions for benchmarking our bound C function in comparison
    to just a normal `GoPrint` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'After we execute this, we can see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/53f3c348-4a5b-43ea-842a-409ddd9d7a41.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the bound Cgo function takes about an order of magnitude longer than
    the native Go functionality. This is okay in some cases. This benchmark is just
    further verifying the fact that we should use Cgo bindings only when it makes
    sense. It's important to remember that there are specific times where we can justify
    using Cgo, such as when we have to perform actions that aren't available natively
    as Go functionality.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we are going to learn about GPU-accelerated programming
    and NVIDIA's CUDA platform.
  prefs: []
  type: TYPE_NORMAL
- en: GPU-accelerated computing – utilizing the hardware
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In today's modern computers, we have a couple of pieces of hardware that do
    most of the work for the system. The CPU performs most instructional operations
    from other parts of the computer and delivers the results of those operations.
    The memory is a fast, short-term location for data storage and manipulation. Hard
    disks are used for longer-term data storage and manipulation, and networking devices
    are used to send these bits of data between computing devices across a network.
    A device that is often also used in a modern computing system is a discrete GPU.
    Whether it is to display the latest computer games with high-fidelity graphics,
    decoding 4K video, or performing financial number-crunching, GPUs are becoming
    a more popular option for high-speed computing.
  prefs: []
  type: TYPE_NORMAL
- en: GPUs are designed for performing specific tasks in an efficient manner. Use
    of GPUs as **General-Purpose Graphics Processing Units** (**GPGPUs**) is becoming
    more commonplace as high-throughput computing is seeing wider adoption.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many different APIs available for GPU programming to use GPUs to
    their fullest extent, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenCL: [https://www.khronos.org/opencl/](https://www.khronos.org/opencl/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenMP: [https://www.openmp.org/](https://www.openmp.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NVIDIA''s CUDA platform: [https://developer.nvidia.com/cuda-zone](https://developer.nvidia.com/cuda-zone)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NVIDIA's CUDA library is mature, performant, and widely accepted. We are going
    to use the CUDA library in our examples in this chapter. Let's talk more about
    the CUDA platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'NVIDIA''s CUDA platform is an API written by the NVIDIA team that is used to
    increase parallelism and improve speed with a CUDA-enabled graphics card. Using
    a GPGPU for performing parallel algorithms on data structures can seriously improve
    compute time. Many of the current ML and AI toolsets use CUDA under the hood,
    including, but not limited to, the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow: [https://www.tensorflow.org/install/gpu](https://www.tensorflow.org/install/gpu)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Numba: [https://devblogs.nvidia.com/gpu-accelerated-graph-analytics-python-numba/](https://devblogs.nvidia.com/gpu-accelerated-graph-analytics-python-numba/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PyTorch: [https://pytorch.org/](https://pytorch.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CUDA provides an API for accessing these processing idioms in C++. It uses the
    concept of kernels, which are functions called from the C++ code that get executed
    on the GPU device. Kernels are the parts of the code that get executed in parallel. CUDA
    uses C++ syntax rules in order to process instructions.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many places you can use GPUs in the cloud to perform compute jobs,
    such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Google Cloud GPUs: [https://cloud.google.com/gpu/](https://cloud.google.com/gpu/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'AWS EC2 instances with GPU: [https://aws.amazon.com/nvidia/](https://aws.amazon.com/nvidia/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Paperspace: [https://www.paperspace.com/](https://www.paperspace.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'FloydHub: [https://www.floydhub.com/](https://www.floydhub.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can also run CUDA programs on your local workstation. The requirements
    for doing so are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A GPU that is CUDA capable (I used a NVIDIA GTX670 in my example)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An **Operating System** (**OS**) that has a GCC compiler and toolchain (I used
    Fedora 29 in my example)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the next section, we''ll run through how to get our workstation set up for
    CUDA processing:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll have to install the proper kernel development tools and kernel
    headers for our host. We can do this on our example Fedora host by executing the
    following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need to install `gcc` and the appropriate build tools. We can do so
    with the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'After we have the prerequisites installed, we can retrieve the local `.run`
    file installer that NVIDIA gives us for CUDA. At the time of writing, the `cuda_10.2.89_440.33.01_linux.run`
    package was the latest available. You can find the latest CUDA toolkit package
    for download from [https://developer.nvidia.com/cuda-downloads](https://developer.nvidia.com/cuda-downloads):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then install this package with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give us an installation prompt, as seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0e949b8a-966e-4ceb-b0d4-c9cb23c4e84b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After we accept the EULA, we can choose the necessary dependencies to install
    and select `Install`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/6b6958e4-be34-4ada-9069-a91439c6b6d8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After you accept the installation prompt, the CUDA installer should successfully
    complete the installation. If you have any errors during your installation, looking
    in the following locations may help you to sort out your installation:'
  prefs: []
  type: TYPE_NORMAL
- en: '`/var/log/cuda-installer.log`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`/var/log/nvidia-installer.log`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we are going to discuss how to use the host machine for
    CUDA processes.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA – utilizing host processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After you've successfully installed CUDA, you'll need to set some environment
    variables in order to add the installed bits to your execution path. This functionality
    works as expected if you don't have access to Docker on your host or if you'd
    rather use your bare machine to perform GPU-intensive operations. If you'd like
    to use a more reproducible build, you can use the Docker configuration defined
    in the following *Docker for GPU-enabled programming* section.
  prefs: []
  type: TYPE_NORMAL
- en: We'll need to update our `PATH` to include the CUDA binary paths that we just
    installed. We can do this by executing the following: `export PATH=$PATH:/usr/local/cuda-10.2/bin:/usr/local/cuda-10.2/NsightCompute-2019.1`.
  prefs: []
  type: TYPE_NORMAL
- en: We also need to update our `LD_LIBRARY_PATH` variable, which is an environment
    variable that your OS looks for when linking dynamic and shared libraries. We
    can add the CUDA libraries by executing `export LD_LIBRARY_PATH=:/usr/local/cuda-10.2/lib64`.
  prefs: []
  type: TYPE_NORMAL
- en: This will add the CUDA libraries to your library path. We will add these to
    our path programmatically with a GNU Makefile for our examples in the closing
    sections of this chapter. In the next section, we'll discuss how to utilize CUDA
    with Docker.
  prefs: []
  type: TYPE_NORMAL
- en: Docker for GPU-enabled programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you'd like to use Docker for your GPU-enabled programming in this chapter,
    you can perform the following steps, but to use this, you must have a compatible
    NVIDIA CUDA GPU in your computer. You can find a full list of enabled cards at [https://developer.nvidia.com/cuda-gpus](https://developer.nvidia.com/cuda-gpus).
  prefs: []
  type: TYPE_NORMAL
- en: We might not use Docker in this way in a production environment for GPU-accelerated
    computing, because you'd most likely want to be as close to the hardware as possible
    for GPU-accelerated programming, but I've chosen to use this methodology in this
    chapter in order to have a reproducible build for the consumer of this book to
    use.  Most of the time a reproducible build is an acceptable trade off for the
    slight performance penalty of using containerized methodologies.
  prefs: []
  type: TYPE_NORMAL
- en: If you're unsure of what your NVIDIA-enabled GPU supports, you can use the `cuda-z` utility to
    find more information about your graphics card. The executable for this program
    can be found at [http://cuda-z.sourceforge.net/](http://cuda-z.sourceforge.net/).
  prefs: []
  type: TYPE_NORMAL
- en: 'After you download the version for your particular OS, you should be able to
    execute the file like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'You''ll see an output that gives you all sorts of information about the card
    you''re currently using:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/967739f9-a0a3-46c0-8cf1-01bbc0876823.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once you''re certain your card supports the GPU processing required, we can
    use Docker to hook into your GPU for processing. To do so, we will go through
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Enable the NVIDIA container toolkit for your computer. With my Fedora test
    system, I had to make a small tweak to this by changing my distribution to ``centos7``—the
    installed RPMs still worked as expected:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The full instructions for installing this on other OSes can be found at [https://github.com/NVIDIA/nvidia-docker#quickstart](https://github.com/NVIDIA/nvidia-docker#quickstart%7C).
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the `nvidia-container-toolkit`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Restart Docker in order to pick up these new changes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Disable SELINUX so that your computer has the ability to use your GPU for these
    requests:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute a test `docker run` to ensure that you are able to perform GPU actions
    within Docker and inspect the information about your particular NVIDIA card:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we'll go through how to set up a CUDA GPU enabled machine
    in Google Cloud Platform.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA on GCP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you don't have the necessary hardware or you'd like to run your workloads
    for your GPU-enabled code in the cloud, you may decide that you'd rather use CUDA
    on a shared hosting environment. In the following example, we'll show you how
    to get set up using GPUs on GCP.
  prefs: []
  type: TYPE_NORMAL
- en: There are many other hosted GPU providers (you can see all of them listed in
    the *GPU-accelerated computing – utilizing the hardware* section of this chapter)—we
    are going to use GCP's GPU instances as an example here.
  prefs: []
  type: TYPE_NORMAL
- en: You can learn more about GCP's GPU offerings at [https://cloud.google.com/gpu](https://cloud.google.com/gpu).
  prefs: []
  type: TYPE_NORMAL
- en: Creating a VM with a GPU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We need to create a Google Compute Engine instance in order to be able to utilize
    GPUs on GCP.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may need to increase your GPU quota.  To do so, you can follow the steps
    at the following URL:'
  prefs: []
  type: TYPE_NORMAL
- en: https://cloud.google.com/compute/quotas#requesting_additional_quota
  prefs: []
  type: TYPE_NORMAL
- en: 'At the time of writing, the NVIDIA P4 GPU is the least expensive on the platform,
    and has ample power to demonstrate our work.  You can verify your quota by checking
    the NVIDIA P4 GPUs metric on the IAM Admin quotas page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8736ef6d-39c6-4752-97b7-4652f0ff2bee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To do this, we can visit the VM instances page on the Google Cloud console.
    A screenshot of this page follows. Click on the Create button in the center of
    the screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6a30a93c-431c-4daf-9ac0-95fffc36c1e8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We next create an Ubuntu 18.04 VM with a GPU attached. Our VM instance configuration
    for this example is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/953115ed-637a-4d16-be10-f2a23a328187.png)'
  prefs: []
  type: TYPE_IMG
- en: We are using Ubuntu 18.04 here as an example, rather than Fedora 29, to show
    how to set CUDA up for multiple architectures.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our OS and other configuration parameters are shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ab76ddc8-01ba-4415-b116-41b9acdbab27.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After we click the Create button, we are taken back to the VM instances page.
    Wait for your VM to be fully provisioned (it''ll have a green checkmark to the
    left of its name):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/23d21efe-e7d0-4608-bf27-8204a4517118.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we can SSH to the instance, as seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b333f11f-0d5f-4507-b644-59599f3cf668.png)'
  prefs: []
  type: TYPE_IMG
- en: In the next subsections, we will install all the necessary dependencies for
    running our GPU enabled CGo program.  I've also included a script that performs
    all these actions at the end of the explanation for your convenience.
  prefs: []
  type: TYPE_NORMAL
- en: Install the CUDA driver
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Follow the instructions from [https://cloud.google.com/compute/docs/gpus/install-drivers-gpu](https://cloud.google.com/compute/docs/gpus/install-drivers-gpu)
    to get the NVIDIA CUDA drivers installed:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Retrieve the CUDA repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Install the `.deb` package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the NVIDIA GPG key to the apt sources keyring:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Install the NVIDIA CUDA drivers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have a CUDA-enabled GPU on our GCP VM. We can validate this with the
    `nvidia-smi` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We will see the following output in the screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/94ef6bd0-d6a3-47e2-bcb3-a1afe673d7e0.png)'
  prefs: []
  type: TYPE_IMG
- en: Install Docker CE on GCP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We next need to install Docker CE on our CUDA enabled GCE VM. To install Docker
    CE on our VM, we can follow the instructions on this page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.docker.com/install/linux/docker-ce/ubuntu/](https://docs.docker.com/install/linux/docker-ce/ubuntu/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'At the time of writing this book, the following steps were necessary:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Validate there aren''t any other docker versions on the host:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Make sure our repositories are up to date:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Install the necessary dependencies to install docker CE:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the docker CE repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Run an update to ensure the docker CE repository is up to date:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Install the necessary docker dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We now have a working instance of Docker CE on our host.
  prefs: []
  type: TYPE_NORMAL
- en: Installing NVIDIA Docker on GCP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To install the NVIDIA docker driver on our VM, we can follow the instructions
    on this page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/NVIDIA/nvidia-docker#ubuntu-16041804-debian-jessiestretchbuster](https://github.com/NVIDIA/nvidia-docker#ubuntu-16041804-debian-jessiestretchbuster)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Set a distribution variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the `nvidia-docker` repo gpg key and apt repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Install the nvidia-container-toolkit:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Restart your VM for this driver to take effect.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scripting it all together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following bash script performs all of the previous actions together. First,
    we install the CUDA driver:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We then install Docker CE:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally we install the `nvidia-docker` driver:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This is included in the repo at [https://git/HighPerformanceWithGo/9-gpu-parallelization-in-go/gcp_scripts](https://github.com/bobstrecansky/HighPerformanceWithGo/blob/master/9-gpu-parallelization-in-go/gcp_scripts/nvidia-cuda-gcp-setup.sh)
    and can be executed by running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: within the directory.    In the next section, we'll go through an example CUDA
    program that is executed using Cgo.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA – powering the program
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After we have all of our CUDA dependencies installed and running, we can start
    out with a simple CUDA C++ program:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll include all of our necessary header files and define the number
    of elements we''d like to process. `1 << 20` is 1,048,576, which is more than
    enough elements to show an adequate GPU test. You can shift this if you''d like
    to see the difference in processing time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Our `multiply` function is wrapped in a `__global__` specifier. This allows
    `nvcc`, the CUDA-specific C++ compiler, to run a particular function on the GPU.
    This multiply function is relatively straightforward: it takes the `a` and `b`
    arrays, multiplies them together using some CUDA magic, and returns the value
    in the `c` array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'This CUDA magic is referencing the parallel-processing functionality of the
    GPU. The variables are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`gridDim.x`: The number of thread blocks available on the processor'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '`blockDim.x`: The number of threads in each block'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`blockIdx.x`: The index of the current block within the grid'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`threadId.x`: The index of the current thread within the block'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We then need to add an `extern "C"` call to have a C-style linkage for this
    particular function, so we can effectively call this function from our Go code.
    This `cuda_multiply` function creates three arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '`a` and `b`, which store random numbers between 1 and 10'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '`c`, which stores the result of the multiplication of `a` and `b`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We then create our arrays of random floats:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We then perform our multiply function (which we defined at the beginning of
    our file) based on a block size.  We calculate the number of blocks we''d like
    to use based on the number:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: After our multiplication is completed, we will wait for the GPU to finish before
    accessing our information on the host: `cudaDeviceSynchronize();`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then print the values of the multiplication that we performed to the
    screen in order to let the end user see the computations we are performing. This
    is commented out in the code, as printing to `stdout` doesn''t show a very good
    performance story for this particular code. You can uncomment it if you''d like
    to see the computation that is occurring:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We then free the GPU memory that we allocated for the multiply function with
    `cudaMallocManaged()` by calling `cudaFree` on each of our array pointers, followed
    by returning `0` to finish up our program:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then add our header file, `cuda_multiply.h`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Our Go program in this chapter is just a wrapper around the `cuda_multiply.cu`
    function that we've created with a little syntactical sugar.
  prefs: []
  type: TYPE_NORMAL
- en: 'We instantiate `main` and import the necessary packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We then add the necessary `CFLAGS` and `LDFLAGS` that we need in order to reference
    the libraries that we have created with our nvcc make, as well as the system libraries.
    It''s important to note here that these comments, referred to as *preambles* in
    cgo code, are used as the header while compiling the C parts of our package. We
    can include any C code that is necessary here in order to make the Go portion
    of our code more palatable. If you''re planning on using any of the following
    styles of flags, they must be preempted with a `#cgo` directive to tweak the behavior
    of the underlying compiler:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`CFLAGS`'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CPPFLAGS`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CXXFLAGS`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FFLAGS`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LDFLAGS`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We then import the pseudo-package `C`, which allows us to execute the C that
    we wrote (recall our `extern C` call in our `cuda_multiply.cu` file). We also
    add a timing wrapper around this function in order to see how long it takes to
    execute this function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'A Makefile is provided for the Docker container that we are going to build
    next. Our Makefile defines a method to build our nvcc library, run our Go code,
    and clean up our nvcc library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Our Dockerfile ties it all together for our demonstration to be very easily
    reproducible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will build and run our Docker container. The following is the output
    from a cached build to truncate the build steps for brevity:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then execute our Docker container with the following command (optionally
    with sudo depending on how your docker daemon is configured):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The follwing is the output of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eb5b521c-f3a6-443c-9d6e-2d3b125b8663.png)'
  prefs: []
  type: TYPE_IMG
- en: Pretty impressive for such a large multiplication calculation! With high computational
    workloads, GPU programming can often be a good solution for very quick calculations.
    An equivalent C++ program using just the CPU takes roughly 340 ms to run on the
    same machine.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about cgo, GPU-accelerated programming, CUDA, Make
    commands, C-style linking for Go programs, and executing a GPU-enabled process
    within a Docker container. Learning about all of these individual elements helped
    us to develop a performant GPU-driven application that can make some very large
    mathematical calculations. These steps could be repeated to do a lot of very large-scale
    computations in a performant manner.  We also learned how to set up a GPU enabled
    VM in GCP so that we can use cloud resources to perform our GPU computations.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll discuss runtime evaluations in Go.
  prefs: []
  type: TYPE_NORMAL
