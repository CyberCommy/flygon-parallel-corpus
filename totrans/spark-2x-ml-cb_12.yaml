- en: Implementing Text Analytics with Spark 2.0 ML Library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Doing term frequency with Spark - everything that counts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Displaying similar words with Spark using Word2Vec
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downloading a complete dump of Wikipedia for a real-life Spark ML project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Latent Semantic Analysis for text analytics with Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Topic modeling with Latent Dirichlet allocation in Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text analytics is at the intersection of machine learning, mathematics, linguistics,
    and natural language processing. Text analytics, referred to as text mining in
    older literature, attempts to extract information and infer higher level concepts,
    sentiment, and semantic details from unstructured and semi-structured data. It
    is important to note that the traditional keyword searches are insufficient to
    deal with noisy, ambiguous, and irrelevant tokens and concepts that need to be
    filtered out based on the actual context.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, what we are trying to do is for a given set of documents (text,
    tweets, web, and social media), is determine what the gist of the communication
    is and what concepts it is trying to convey (topics and concepts). These days,
    breaking down a document into its parts and taxonomy is too primitive to be considered
    text analytics. We can do better.
  prefs: []
  type: TYPE_NORMAL
- en: Spark provides a set of tools and facilities to make text analytics easier,
    but it is up to the users to combine the techniques to come up with a viable system
    (for example, KKN clustering and topic modelling).
  prefs: []
  type: TYPE_NORMAL
- en: 'It is worth mentioning that many of the commercially available systems use
    a combination of techniques to come up with the final answer. While Spark has
    a sufficient number of techniques that work very well at scale, it would not be
    hard to imagine that any text analytics system can benefit from a graphical model
    (that is, GraphFrame, GraphX). The following figure is a summary of the tools
    and facilities provided by Spark for text analytics:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00272.gif)'
  prefs: []
  type: TYPE_IMG
- en: Text analytics is an upcoming and important area due to its application to many
    fields such as security, customer engagement, sentiment analysis, social media,
    and online learning. Using text analytics techniques, one can combine traditional
    data stores (that is, structured data and database tables) with unstructured data
    (that is, customer reviews, sentiments, and social media interaction) to ascertain
    a higher order of understanding and a more complete view of the business unit,
    which was not possible before. This is especially important when dealing with
    millennials that have chosen social media and unstructured text as their primary
    means of communication.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00273.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The main challenge with unstructured text is that you cannot use the traditional
    data platforming tools such as ETL to extract and force order on the data. We
    need new data wrangling, ML, and statistical methods combined with NLP techniques
    that can extract information and insight. Social media and customer interactions,
    such as transcriptions of calls in a call center, contain valuable information
    that can no longer be ignored without losing one's competitive edge.
  prefs: []
  type: TYPE_NORMAL
- en: We not only need text analytics to be able to address big data at rest, but
    must also consider big data in motion, such as tweets and streams, to be effective.
  prefs: []
  type: TYPE_NORMAL
- en: There are several approaches to deal with unstructured data. The following figure
    given is a depiction of the techniques in today's toolkit. While the rule-based
    system can be a good fit for limited text and domains, it fails to generalize
    due to its specific decision boundaries designed to be effective in that particular
    domain. The newer systems use statistical and NLP techniques to achieve better
    accuracy and scale.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00274.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In this chapter, we cover four recipes and two real-life datasets to demonstrate
    Spark's facilities for handling unstructured text analytics at scale.
  prefs: []
  type: TYPE_NORMAL
- en: First, we start with a simple recipe to not only mimic the early days of web
    search (keyword frequency) but also to provide insight into TF-IDF in raw code
    format. This recipe attempts to find out how often a word or phrase occurs in
    a document. As unbelievable as it sounds, there was a US patent issued for this
    technique!
  prefs: []
  type: TYPE_NORMAL
- en: Second, we proceed with a well-known algorithm, Word2Vec, which attempts to
    answer the question, i*f I give you a word, can you tell me the surrounding words,
    or what is in its neighborhood?* This is a good way to ask for synonyms inside
    a document using statistical techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Third, we implement a **Latent Semantic Analysis** (**LSA**) which is a form
    of topic extraction. This method was invented at the University of Colorado Boulder
    and has been the workhorse in social sciences.
  prefs: []
  type: TYPE_NORMAL
- en: Fourth, we implement a **Latent Dirichlet Allocation** (**LDA**) to demonstrate
    topic modelling in which abstract concepts are extracted and associated with phrases
    or words (that is, less primitive constructs) in a scalable and meaningful way
    (for example, home, happiness, love, mother, family pet, children, shopping, and
    parties can be extracted into a single topic).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00275.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Doing term frequency with Spark - everything that counts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this recipe, we will download a book in text format from Project Gutenberg,
    from [http://www.gutenberg.org/cache/epub/62/pg62.txt](http://www.gutenberg.org/cache/epub/62/pg62.txt).
  prefs: []
  type: TYPE_NORMAL
- en: Project Gutenberg offers over 50,000 free eBooks in various formats for human
    consumption. Please read their terms of use; let us not use command-line tools
    to download any books.
  prefs: []
  type: TYPE_NORMAL
- en: When you look at the contents of the file, you will notice the title and author
    of the book is *The Project Gutenberg EBook of A Princess of Mars* by Edgar Rice
    Burroughs.
  prefs: []
  type: TYPE_NORMAL
- en: This eBook is for the use of anyone, anywhere, at no cost, and with almost no
    restrictions whatsoever. You may copy it, give it away, or reuse it under the
    terms of the Project Gutenberg License included with this eBook online at [http://www.gutenberg.org/](http://www.gutenberg.org/).
  prefs: []
  type: TYPE_NORMAL
- en: We then use the downloaded book to demonstrate the classic word count program
    with Scala and Spark. The example may seem somewhat simple at first, but we are
    beginning the process of feature extraction for text processing. Also, a general
    understanding of counting word occurrences in a document will go a long way to
    help us understand the concept of TF-IDF.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `package` statement for the recipe is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for Scala, Spark, and JFreeChart:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We will define a function to display our JFreeChart within a window:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us define the location of our book file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a Spark session with configurations using the factory builder pattern:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We should set the logging level to warning, otherwise output will be difficult
    to follow:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We read in the file of stop words which will be used as a filter later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The stop words file contains commonly used words which show no relevant value
    in matching or comparing documents, therefore they will be excluded from the pool
    of terms by a filter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We now load the book to tokenize, analyze, apply stop words, filter, count,
    and sort:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We take top the 25 words which have the highest frequency:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We loop through every element in the resulting RDD, generating a category dataset
    model to build our chart of word occurrences:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Display a bar chart of the word count:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The following chart displays the word count:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00276.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We close the program by stopping the SparkContext:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We began by loading the downloaded book and tokenizing it via a regular expression.
    The next step was to convert all tokens to lowercase and exclude stop words from
    our token list, followed by filtering out any words less than two characters long.
  prefs: []
  type: TYPE_NORMAL
- en: The removal of stop words and words of a certain length reduce the number of
    features we have to process. It may not seem obvious, but the removal of particular
    words based on various processing criteria reduce the number of dimensions our
    machine learning algorithms will later process.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we sorted the resulting word count in descending order, taking the
    top 25, which we displayed a bar chart for.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we have the base of what a keyword search would do. It is important
    to understand the difference between topic modelling and keyword search. In a
    keyword search, we try to associate a phrase with a given document based on the
    occurrences. In this case, we will point the user to a set of documents that has
    the most number of occurrences.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The next step in the evolution of this algorithm, that a developer can try as
    an extension, would be to add weights and come up with a weighted average, but
    then Spark provides a facility which we explore in the upcoming recipes.
  prefs: []
  type: TYPE_NORMAL
- en: Displaying similar words with Spark using Word2Vec
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will explore Word2Vec, which is Spark's tool for assessing
    word similarity. The Word2Vec algorithm is inspired by the *distributional hypothesis*
    in general linguistics. At the core, what it tries to say is that the tokens which
    occur in the same context (that is, distance from the target) tend to support
    the same primitive concept/meaning.
  prefs: []
  type: TYPE_NORMAL
- en: The Word2Vec algorithm was invented by a team of researchers at Google. Please
    refer to a white paper mentioned in the *There's more...* section of this recipe
    which describes Word2Vec in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `package` statement for the recipe is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for Scala and Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us define the location of our book file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a Spark session with configurations using the factory builder pattern:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We should set the logging level to warning, otherwise output will be difficult
    to follow:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We load in the book and convert it to a DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We now transform each line into a bag of words utilizing Spark''s regular expression
    tokenizer, converting each term into lowercase and filtering away any term which
    has a character length of less than four:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We remove stop words by using Spark''s `StopWordRemover` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We apply the Word2Vec machine learning algorithm to extract features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We find ten synonyms from the book for *martian*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Display the results of ten synonyms found by the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00277.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'We close the program by stopping the SparkContext:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Word2Vec in Spark uses skip-gram and not **continuous bag of words** (**CBOW**)
    which is more suitable for a **Neural Net** (**NN**). At its core, we are attempting
    to compute the representation of the words. It is highly recommended for the user
    to understand the difference between local representation versus distributed presentation,
    which is very different to the apparent meaning of the words themselves.
  prefs: []
  type: TYPE_NORMAL
- en: If we use distributed vector representation for words, it is natural that similar
    words will fall close together in the vector space, which is a desirable generalization
    technique for pattern abstraction and manipulation (that is, we reduce the problem
    to vector arithmetic).
  prefs: []
  type: TYPE_NORMAL
- en: What we want to do for a given set of words *{Word[1,] Word[2, .... ,]Word[n]}*
    that are cleaned and ready for processing, is define a maximum likelihood function
    (for example, log likelihood) for the sequence, and then proceed to maximize likelihood
    (that is, typical ML). For those familiar with NN, this is a simple multi class
    softmax model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00278.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We start off with loading the free book into the memory and tokenizing it into
    terms. The terms are then converted into lowercase and we filter out any words
    less than four. We finally apply the stop words followed by the Word2Vec computation.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How would you find similar words anyhow? How many algorithms are there that
    can solve this problem, and how do they vary? The Word2Vec algorithm has been
    around for a while and has a counterpart called CBOW. Please bear in mind that
    Spark provides the skip-gram method as the implementation technique.
  prefs: []
  type: TYPE_NORMAL
- en: 'The variations of the Word2Vec algorithm are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Continuous Bag of Words (CBOW)**: Given a central word, what are the surrounding
    words?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Skip-gram**: If we know the words surrounding, can we guess the missing word?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a variation of the algorithm that is called **skip-gram model with
    negative sampling** (**SGNS**), which seems to outperform other variants.
  prefs: []
  type: TYPE_NORMAL
- en: The co-occurrence is the fundamental concept underlying both CBOW and skip-gram.
    Even though the skip-gram does not directly use a co-occurrence matrix, it is
    using it indirectly.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we used the *stop words* techniques from NLP to have a cleaner
    corpus before running our algorithm. The stop words are English words such as
    "*the*" that need to be removed since they are not contributing to any improvement
    in the outcome.
  prefs: []
  type: TYPE_NORMAL
- en: Another important concept is* stemming*, which is not covered here, but will
    be demonstrated in later recipes. Stemming removes extra language artefacts and
    reduces the word to its root (for example, *Engineering*, *Engineer*, and *Engineers*
    become *Engin* which is the root).
  prefs: []
  type: TYPE_NORMAL
- en: 'The white paper found at the following URL should provide deeper explanation
    for Word2Vec:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://arxiv.org/pdf/1301.3781.pdf](http://arxiv.org/pdf/1301.3781.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Documentation for the Word2Vec recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Word2Vec()`: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.Word2Vec](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.Word2Vec)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Word2VecModel()`: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.Word2VecModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.Word2VecModel)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`StopWordsRemover()`: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.StopWordsRemover](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.StopWordsRemover)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downloading a complete dump of Wikipedia for a real-life Spark ML project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will be downloading and exploring a dump of Wikipedia so
    we can have a real-life example. The dataset that we will be downloading in this
    recipe is a dump of Wikipedia articles. You will either need the command-line
    tool **curl**, or a browser to retrieve a compressed file, which is about 13.6
    GB at this time. Due to the size, we recommend the curl command-line tool.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can start with downloading the dataset using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you want to decompress the ZIP file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This should create an uncompressed file which is named `enwiki-latest-pages-articles-multistream.xml`
    and is about 56 GB.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us take a look at the Wikipedia XML file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We recommend working with the XML file in chunks, and using sampling for your
    experiments until you are ready for a final job submit. It will save a tremendous
    amount of time and effort.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for Wiki download is available at [https://en.wikipedia.org/wiki/Wikipedia:Database_download](https://en.wikipedia.org/wiki/Wikipedia:Database_download).
  prefs: []
  type: TYPE_NORMAL
- en: Using Latent Semantic Analysis for text analytics with Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will explore LSA utilizing a data dump of articles from Wikipedia.
    LSA translates into analyzing a corpus of documents to find hidden meaning or
    concepts in those documents.
  prefs: []
  type: TYPE_NORMAL
- en: In the first recipe of this chapter, we covered the basics of the TF (that is,
    term frequency) technique. In this recipe, we use HashingTF for calculating TF
    and use IDF to fit a model into the calculated TF. At its core, LSA uses **singular
    value decomposition** (**SVD**) on the term frequency document to reduce dimensionality
    and therefore extract the most important concepts. There are other cleanup steps
    that we need to do (for example, stop words and stemming) that will clean up the
    bag of words before we start analyzing it.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The package statement for the recipe is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for Scala and Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The following two statements import the `Cloud9` library toolkit elements necessary
    for processing Wikipedia XML dumps/objects. `Cloud9` is a library toolkit that
    makes accessing, wrangling, and processing the Wikipedia XML dumps easier for
    developers. See the following lines of code for more detailed information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Wikipedia is a free body of knowledge that can be freely downloaded as a dump
    of XML chunks/objects via the following Wikipedia download link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/Wikipedia:Database_download](https://en.wikipedia.org/wiki/Wikipedia:Database_download)'
  prefs: []
  type: TYPE_NORMAL
- en: The complexity of text and its structure can be easily handled using the `Cloud9`
    toolkit which facilitates accessing and processing the text using the `import`
    statements listed previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following link provides some information regarding the `Cloud9` library:'
  prefs: []
  type: TYPE_NORMAL
- en: Main page is available at [https://lintool.github.io/Cloud9/docs/content/wikipedia.html](https://lintool.github.io/Cloud9/docs/content/wikipedia.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Source code is available at [http://grepcode.com/file/repo1.maven.org/maven2/edu.umd/cloud9/2.0.0/edu/umd/cloud9/collection/wikipedia/WikipediaPage.java](http://grepcode.com/file/repo1.maven.org/maven2/edu.umd/cloud9/2.0.0/edu/umd/cloud9/collection/wikipedia/WikipediaPage.java)
    and [http://grepcode.com/file/repo1.maven.org/maven2/edu.umd/cloud9/2.0.1/edu/umd/cloud9/collection/wikipedia/language/EnglishWikipediaPage.java](http://grepcode.com/file/repo1.maven.org/maven2/edu.umd/cloud9/2.0.1/edu/umd/cloud9/collection/wikipedia/language/EnglishWikipediaPage.java).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We define a function to parse a Wikipedia page and return the title and content
    text of the page:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We define a short function to apply the Porter stemming algorithm to terms:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We define a function to tokenize content text of a page into terms:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us define the location of the Wikipedia data dump:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a job configuration for Hadoop XML streaming:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We set up the data path for Hadoop XML streaming processing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `SparkSession` with configurations using the factory builder pattern:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We should set the logging level to warning, otherwise output will be difficult
    to follow:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We begin to process the huge Wikipedia data dump into article pages, taking
    a sample of the file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we process our sample data into an RDD containing a tuple of title and
    page context text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We now output the number of Wikipedia articles we will process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'We load into memory the stop words for filtering the page content text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We tokenize the page content text, turning it into terms for further processing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We use Spark''s `HashingTF` class to compute term frequency of our tokenized
    page context text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'We take term frequencies and compute the inverse document frequency utilizing
    Spark''s IDF class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We generate a `RowMatrix` using the inverse document frequency and compute
    singular value decomposition:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '**U**: The rows will be documents and the columns will be concepts.'
  prefs: []
  type: TYPE_NORMAL
- en: '**S**: The elements will be the amount variation from each concept.'
  prefs: []
  type: TYPE_NORMAL
- en: '**V**: The rows will be terms and the columns will be concepts.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We close the program by stopping the SparkContext:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The example starts off by loading a dump of Wikipedia XML using Cloud9 Hadoop
    XML streaming tools to process the enormous XML document. Once we have parsed
    out the page text, the tokenization phase invokes turning our stream of Wikipedia
    page text into tokens. We used the Porter stemmer during the tokenization phase
    to help reduce words to a common base form.
  prefs: []
  type: TYPE_NORMAL
- en: More details on stemming is available at [https://en.wikipedia.org/wiki/Stemming](https://en.wikipedia.org/wiki/Stemming).
  prefs: []
  type: TYPE_NORMAL
- en: The next step was to use Spark HashingTF on each page token to compute the term
    frequency. After this phase was completed, we utilized Spark's IDF to generate
    the inverse document frequency.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we took the TF-IDF API and applied a singular value decomposition to
    handle factorization and dimensionality reduction.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the steps and flow of the recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00279.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The Cloud9 Hadoop XML tools and several other required dependencies can be
    found at:'
  prefs: []
  type: TYPE_NORMAL
- en: '`bliki-core-3.0.19.jar`: [http://central.maven.org/maven2/info/bliki/wiki/bliki-core/3.0.19/bliki-core-3.0.19.jar](http://central.maven.org/maven2/info/bliki/wiki/bliki-core/3.0.19/bliki-core-3.0.19.jar)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cloud9-2.0.1.jar`: [http://central.maven.org/maven2/edu/umd/cloud9/2.0.1/cloud9-2.0.1.jar](http://central.maven.org/maven2/edu/umd/cloud9/2.0.1/cloud9-2.0.1.jar)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hadoop-streaming-2.7.4.jar`: [http://central.maven.org/maven2/org/apache/hadoop/hadoop-streaming/2.7.4/hadoop-streaming-2.7.4.jar](http://central.maven.org/maven2/org/apache/hadoop/hadoop-streaming/2.7.4/hadoop-streaming-2.7.4.jar)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lucene-snowball-3.0.3.jar`: [http://central.maven.org/maven2/org/apache/lucene/lucene-snowball/3.0.3/lucene-snowball-3.0.3.jar](http://central.maven.org/maven2/org/apache/lucene/lucene-snowball/3.0.3/lucene-snowball-3.0.3.jar)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It should be obvious by now that even though Spark does not provide a direct
    LSA implementation, the combination of TF-IDF and SVD will let us construct and
    then decompose the large corpus matrix into three matrices, which can help us
    interpret the results by applying the dimensionality reduction via SVD. We can
    concentrate on the most meaningful clusters (similar to a recommendation algorithm).
  prefs: []
  type: TYPE_NORMAL
- en: SVD will factor the term frequency document (that is, documents by attributes)
    to three distinct matrices that are much more efficient to extract to *N* concepts
    (that is, *N=27* in our example) from a large matrix that is hard and expensive
    to handle. In ML, we always prefer the tall and skinny matrices (that is, *U*
    matrix in this case) to other variations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the technique for SVD:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00280.gif)'
  prefs: []
  type: TYPE_IMG
- en: The primary goal of SVD is dimensionality reduction to cure desired (that is,
    top *N*) topics or abstract concepts. We will use the following input to get the
    output stated in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: As input, we'll take a large matrix of *m x n* (*m* is the number of documents,
    *n* is the number of terms or attributes).
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the output that we should get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00281.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'For a more detailed example and short tutorial on SVD, please see the following
    links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://home.iitk.ac.in/~crkrish/MLT/PreRequisites/linalgWithSVD.pdf](http://home.iitk.ac.in/~crkrish/MLT/PreRequisites/linalgWithSVD.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://davetang.org/file/Singular_Value_Decomposition_Tutorial.pdf](http://davetang.org/file/Singular_Value_Decomposition_Tutorial.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can also refer to a write up from RStudio, which is available at the following
    link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://rstudio-pubs-static.s3.amazonaws.com/222293_1c40c75d7faa42869cc59df879547c2b.html](http://rstudio-pubs-static.s3.amazonaws.com/222293_1c40c75d7faa42869cc59df879547c2b.html)'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SVD has been covered in detail in [Chapter 11](part0494.html#EN3LS0-4d291c9fed174a6992fd24938c2f9c77),
    *Curse of High**-Dimensionality in Big Data*.
  prefs: []
  type: TYPE_NORMAL
- en: For a pictorial representation of SVD, please see the recipe *Using Singular
    Value Decomposition (SVD) to address high-dimensionality* in [Chapter 11](part0494.html#EN3LS0-4d291c9fed174a6992fd24938c2f9c77), *Curse
    of High-Dimensionality in Big Data*.
  prefs: []
  type: TYPE_NORMAL
- en: More details on `SingularValueDecomposition()` can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.SingularValueDecomposition](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.SingularValueDecomposition).
  prefs: []
  type: TYPE_NORMAL
- en: Please refer to [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.distributed.RowMatrix](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.distributed.RowMatrix) for
    more details on `RowMatrix()`.
  prefs: []
  type: TYPE_NORMAL
- en: Topic modeling with Latent Dirichlet allocation in Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will be demonstrating topic model generation by utilizing
    Latent Dirichlet Allocation to infer topics from a collection of documents.
  prefs: []
  type: TYPE_NORMAL
- en: We have covered LDA in previous chapters as it applies to clustering and topic
    modelling, but in this chapter, we demonstrate a more elaborate example to show
    its application to text analytics using more real-life and complex datasets.
  prefs: []
  type: TYPE_NORMAL
- en: We also apply NLP techniques such as stemming and stop words to provide a more
    realistic approach to LDA problem-solving. What we are trying to do is to discover
    a set of latent factors (that is, different from the original) that can solve
    and describe the solution in a more efficient way in a reduced computational space.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first question that always comes up when using LDA and topic modelling
    is w*hat is Dirichlet?* Dirichlet is simply a type of distribution and nothing
    more. Please see the following link from the University of Minnesota for details:
    [http://www.tc.umn.edu/~horte005/docs/Dirichletdistribution.pdf](http://www.tc.umn.edu/~horte005/docs/Dirichletdistribution.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `package` statement for the recipe is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for Scala and Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'We define a function to parse a Wikipedia page and return the title and content
    text of the page:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us define the location of the Wikipedia data dump:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'We create a job configuration for Hadoop XML streaming:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'We set up the data path for Hadoop XML streaming processing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `SparkSession` with configurations using the factory builder pattern:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'We should set the logging level to warning, otherwise output will be difficult
    to follow:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'We begin to process the huge Wikipedia data dump into article pages taking
    a sample of the file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we process our sample data into an RDD containing a tuple of title and
    page context text to finally generate a DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'We now transform the text column of the DataFrame into raw words using Spark''s
    `RegexTokenizer` for each Wikipedia page:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to filter raw words by removing all stop words from the tokens:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'We generate term counts for the filtered tokens by using Spark''s `CountVectorizer`
    class, resulting in a new DataFrame containing the column features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: The "MinDF" specifies the minimum number of different document terms that must
    appear in order to be included in the vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now invoke Spark''s LDA class to generate topics and the distributions of
    tokens to topics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: The "K" refers to how many topics and "MaxIter" maximum iterations to execute.
  prefs: []
  type: TYPE_NORMAL
- en: 'We finally describe the top five generated topics and display:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00282.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now display, topics and terms associated with them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'The console output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00283.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'We close the program by stopping the SparkContext:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We began with loading the dump of Wikipedia articles and parsed the page text
    into tokens using Hadoop XML leveraging streaming facilities API. The feature
    extraction process utilized several classes to set up the final processing by
    the LDA class, letting the tokens flow from Spark's `RegexTokenize`, `StopwordsRemover`,
    and `HashingTF`. Once we had the term frequencies, the data was passed to the
    LDA class for clustering the articles together under several topics.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Hadoop XML tools and several other required dependencies can be found at:'
  prefs: []
  type: TYPE_NORMAL
- en: '`bliki-core-3.0.19.jar`: [http://central.maven.org/maven2/info/bliki/wiki/bliki-core/3.0.19/bliki-core-3.0.19.jar](http://central.maven.org/maven2/info/bliki/wiki/bliki-core/3.0.19/bliki-core-3.0.19.jar)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cloud9-2.0.1.jar`: [http://central.maven.org/maven2/edu/umd/cloud9/2.0.1/cloud9-2.0.1.jar](http://central.maven.org/maven2/edu/umd/cloud9/2.0.1/cloud9-2.0.1.jar)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hadoop-streaming-2.7.4.jar`: [http://central.maven.org/maven2/org/apache/hadoop/hadoop-streaming/2.7.4/hadoop-streaming-2.7.4.jar](http://central.maven.org/maven2/org/apache/hadoop/hadoop-streaming/2.7.4/hadoop-streaming-2.7.4.jar)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lucene-snowball-3.0.3.jar`: [http://central.maven.org/maven2/org/apache/lucene/lucene-snowball/3.0.3/lucene-snowball-3.0.3.jar](http://central.maven.org/maven2/org/apache/lucene/lucene-snowball/3.0.3/lucene-snowball-3.0.3.jar)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Please see the recipe LDA to classify documents and text into topics in [Chapter
    8](part0401.html#BUDHI0-4d291c9fed174a6992fd24938c2f9c77), *Unsupervised Clustering
    with Apache Spark 2.0* for a more detailed explanation of the LDA algorithm itself.
  prefs: []
  type: TYPE_NORMAL
- en: The following white paper from the *Journal of Machine Learning Research (JMLR)*
    provides a comprehensive treatment for those who would like to do an extensive
    analysis. It is a well written paper, and a person with a basic background in
    stat and math should be able to follow it without any problems.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the [http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf) link
    for more details of JMLR; an alternative link is [https://www.cs.colorado.edu/~mozer/Teaching/syllabi/ProbabilisticModels/readings/BleiNgJordan2003.pdf](https://www.cs.colorado.edu/~mozer/Teaching/syllabi/ProbabilisticModels/readings/BleiNgJordan2003.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for constructor is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDA](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDA)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for LDAModel is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDAModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDAModel)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'See also Spark''s Scala API documentation for the following:'
  prefs: []
  type: TYPE_NORMAL
- en: DistributedLDAModel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EMLDAOptimizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LDAOptimizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LocalLDAModel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OnlineLDAOptimizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
