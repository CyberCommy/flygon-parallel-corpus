- en: Chapter 13. Multiprocessing – When a Single CPU Core Is Not Enough
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we discussed factors that influence performance and
    some methods to increase performance. This chapter can actually be seen as an
    extension to the list of performance tips. In this chapter, we will discuss the
    multiprocessing module, a module that makes it very easy to make your code run
    on multiple CPU cores and even on multiple machines. This is an easy way to work
    around the **Global Interpreter Lock** (**GIL**) that was discussed in the previous
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize, this chapter will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Local multiprocessing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remote multiprocessing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data sharing and synchronization between processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multithreading versus multiprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Within this book we haven't really covered multithreading yet, but you have
    probably seen multithreaded code in the past. The big difference between multithreading
    and multiprocessing is that with multithreading everything is still executed within
    a single process. That effectively limits your performance to a single CPU core.
    It actually limits you even further because the code has to deal with the GIL
    limitations of CPython.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The GIL is the global lock that Python uses for safe memory access. It is discussed
    in more detail in [Chapter 12](ch12.html "Chapter 12. Performance – Tracking and
    Reducing Your Memory and CPU Usage"), *Performance – Tracking and Reducing Your
    Memory and CPU Usage*, about performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate that multithreading code doesn''t help performance in all cases
    and can actually be slightly slower than single threaded code, look at this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'With Python 3.5, which has the new and improved GIL implementation (introduced
    in Python 3.2), the performance is quite comparable but there is no improvement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'With Python 2.7, which still has the old GIL, the performance is a lot better
    in the single threaded variant:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: From this test we can conclude that Python 2 is faster in some cases while Python
    3 is faster in other cases. What you should take from this is that there is no
    performance reason to choose between Python 2 or Python 3 specifically. Just note
    that Python 3 is at least as fast as Python 2 in most cases and if that is not
    the case, it will be fixed soon.
  prefs: []
  type: TYPE_NORMAL
- en: Regardless, for CPU-bound operations, threading does not offer any performance
    benefit since it executes on a single processor core. For I/O bound operations
    however, the `threading` library does offer a clear benefit, but in that case
    I would recommend trying `asyncio` instead. The biggest problem with `threading`
    is that if one of the threads blocks, the main process blocks.
  prefs: []
  type: TYPE_NORMAL
- en: The `multiprocessing` library offers an API that is very similar to the `threading`
    library but utilizes multiple processes instead of multiple threads. The advantages
    are that the GIL is no longer an issue and that multiple processor cores and even
    multiple machines can be used for processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate the performance difference, let''s repeat the test while using
    the `multiprocessing` module instead of `threading`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'When running it, we see a huge improvement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Note that this was run on a quad core processor, which is why I chose four processes.
    The `multiprocessing` library defaults to `multiprocessing.cpu_count()` which
    counts the available CPU cores, but that method fails to take CPU hyper-threading
    into account. Which means it would return 8 in my case and that is why I hardcoded
    it to 4 instead.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It's important to note that because the `multiprocessing` library uses multiple
    processes, the code needs to be imported from the sub processes. The result is
    that the `multiprocessing` library does not work within the Python or IPython
    shells. As we will see later in this chapter, IPython has its own provisions for
    multiprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: Hyper-threading versus physical CPU cores
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In most cases, hyper-threading is very useful and improves performance, but
    when you truly maximize CPU usage it is generally better to only use the physical
    processor count. To demonstrate how this affects the performance, we will run
    the tests from the previous section again. This time with 1, 2, 4, 8, and 16 processes
    to demonstrate how it affects the performance. Luckily, the `multiprocessing`
    library has a nice `Pool` class to manage the processes for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The pool code makes starting a pool of workers and processing a queue a bit
    simpler as well. In this case we used `map` but there are several other options
    such as `imap`, `map_async`, `imap_unordered`, `apply`, `apply_async`, `starmap`,
    and `starmap_async`. Since these are very similar to how the similarly named `itertools`
    methods work, there won't be specific examples for all of them.
  prefs: []
  type: TYPE_NORMAL
- en: 'But now, the tests with varying amounts of processes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: You probably weren't expecting these results, but this is exactly the problem
    with hyper-threading. As soon as the single processes actually use 100 percent
    of a CPU core, the task switching between the processes actually reduces performance.
    Since there are only `4` physical cores, the other `4` have to fight to get something
    done on the processor cores. This fight takes time which is why the `4` process
    version is slightly faster than the `8` process version. Additionally, the scheduling
    effect can be seen in the runs using `1` and `2` cores as well. If we look at
    the single core version, we see that it took `5.3` seconds, which means that `4`
    cores should do it in `5.3 / 4 = 1.325` seconds instead of the `1.48` seconds
    it actually took. The `2` core version has a similar effect, `2.7 / 2 = 1.35`
    seconds which is still faster than `4` core version.
  prefs: []
  type: TYPE_NORMAL
- en: If you are truly pressed for performance with a CPU-bound problem then matching
    the physical CPU cores is the best solution. If you do not expect to maximize
    all cores all the time, then I recommend leaving it to the default as hyper-threading
    definitely has some performance benefits in other scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'It all depends on your use-case however and the only way to know for certain
    is to test for your specific scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: Disk I/O bound? A single process is most likely your best bet.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CPU bound? The amount of physical CPU cores is your best bet.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network I/O bound? Start with the defaults and tune if needed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No obvious bound but many parallel processes are needed? Perhaps you should
    try `asyncio` instead of `multiprocessing`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that the creation of multiple processes is not free in terms of memory
    and open files, whereas you could have a nearly unlimited amount of coroutines
    this is not the case for processes. Depending on your operating system configuration,
    it could max out long before you even reach a hundred, and even if you reach those
    numbers, CPU scheduling will be your bottleneck instead.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a pool of workers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Creating a processing pool of worker processes is generally a difficult task.
    You need to take care of scheduling jobs, processing the queue, handling the processes,
    and the most difficult part, handling synchronization between the processes without
    too much overhead.
  prefs: []
  type: TYPE_NORMAL
- en: 'With `multiprocessing` however, these problems have been solved already. You
    can simply create a process pool with a given number of processes and just add
    tasks to it whenever you need to. The following is an example of a multiprocessing
    version of the map operator and demonstrates that processing will not stall the
    application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The processing itself is pretty straightforward. The point is that the pool
    stays available and you are not required to wait for it. Just add jobs whenever
    you need to and use the asynchronous results as soon as they are available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Sharing data between processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is really the most difficult part about multiprocessing, multithreading,
    and distributed programming - which data to pass along and which data to skip.
    The theory is really simple, however: whenever possible don''t transfer any data,
    don''t share anything, and keep everything local. Essentially the functional programming
    paradigm, which is why functional programming mixes really well with multiprocessing.
    In practice, regrettably, this is simply not always possible. The `multiprocessing`
    library has several options to share data: `Pipe`, `Namespace`, `Queue`, and a
    few others. All these options might tempt you to share your data between the processes
    all the time. This is indeed possible, but the performance impact is, in many
    cases, more than what the distributed calculation will offer as extra power. All
    data sharing options come at the price of synchronization between all processing
    kernels, which takes a lot of time. Especially with distributed options, these
    synchronizations can take several milliseconds or, if executed globally, cause
    hundreds of milliseconds of latency.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The multiprocessing namespace behaves just as a regular object would work,
    with one small difference that all the actions are safe for multiprocessing. With
    all this power, namespaces are still very easy to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: A pipe is not that much more interesting either. It's just a bidirectional communication
    endpoint which allows both reading and writing. In this regard, it simply offers
    you a reader and a writer, and because of that, you can combine multiple processes/endpoints.
    The only thing you must always keep in mind when synchronizing data is that locking
    takes time. For a proper lock to be set, all the parties need to agree that the
    data is locked, which is a process that takes time. And that simple fact slows
    down execution much more than most people would expect.
  prefs: []
  type: TYPE_NORMAL
- en: On a regular hard disk setup, the database servers aren't able to handle more
    than about 10 transactions per second on the same row due to locking and disk
    latency. Using lazy file syncing, SSDs, and battery backed RAID cache, that performance
    can be increased to handle, perhaps, a 100 transactions per second on the same
    row. Those are simple hardware limitations, because you have multiple processes
    trying to write to a single target you need to synchronize the actions between
    the processes and that takes a lot of time.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The "database servers" statistic is a common statistic for all database servers
    that offer safe and consistent data storage.
  prefs: []
  type: TYPE_NORMAL
- en: Even with the fastest hardware available, synchronization can lock all the processes
    and produce enormous slowdowns, so if at all possible, try to avoid sharing data
    between multiple processes. Put simply, if all the processes are reading and writing
    from/to the same object, it is generally faster to use a single process instead.
  prefs: []
  type: TYPE_NORMAL
- en: Remote processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have only executed our scripts on multiple local processors, but
    we can actually expand this further. Using the `multiprocessing` library, it's
    actually very easy to execute jobs on remote servers, but the documentation is
    currently still a bit cryptic. There are actually a few ways of executing processes
    in a distributed way, but the most obvious one isn't the easiest one. The `multiprocessing.connection`
    module has both the `Client` and `Listener` classes, which facilitate secure communication
    between the clients and servers in a simple way. Communication is not the same
    as process management and queue management however, those features requires some
    extra effort. The multiprocessing library is still a bit bare in this regard,
    but it's most certainly possible given a few different processes.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed processing using multiprocessing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First of all, we will start with a module with containing a few constants which
    should be shared between all clients and the server, so the secret password and
    the hostname of the server are available to all. In addition to that, we will
    add our prime calculation functions, which we will be using later. The imports
    in the following modules will expect this file to be stored as `constants.py,`
    but feel free to call it anything you like as long as you modify the imports and
    references:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now it''s time to create the actual server which links the functions and the
    job queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'After creating the server, we need to have a script that sends the jobs, which
    will actually be a regular client. It''s simple enough really and a regular client
    can also function as a processor, but to keep things sensible we will use them
    as separate scripts. The following script will add 0 to 999 to the queue for processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, we need to create a client to actually process the queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding code you can see how we pass along functions; the manager
    allows registering of functions and classes which can be called from the clients
    as well. With that we pass along a queue from the multiprocessing class which
    is safe for both multithreading and multiprocessing. Now we need to start the
    processes themselves. First the server which keeps on running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, run the producer to generate the prime generation requests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'And now we can run multiple clients on multiple machines to get the first 1000
    primes. Since these clients now print the first 1000 primes, the output is a bit
    too lengthy to show here, but you can simply run this in parallel on multiple
    machines to generate your output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Instead of printing, you can obviously use queues or pipes to send the output
    to a different process if you'd like. As you can see verify though, it's still
    a bit of work to process things in parallel and it requires some code synchronization
    to work. There are a few alternatives available, such as **ØMQ**, **Celery**,
    and **IPyparallel**. Which of these is the best and most suitable depends on your
    use case. If you are simply looking for processing tasks on multiple CPUs, then
    multiprocessing and IPyparallel are probably your best choices. If you are looking
    for background processing and/or easy offloading to multiple machines, then ØMQ
    and Celery are better choices.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed processing using IPyparallel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The IPyparallel module (previously, IPython Parallel) is a module that makes
    it really easy to process code on multiple computers at the same time. The library
    supports more features than you are likely to need, but the basic usage is important
    to know just in case you need to do heavy calculations which can benefit from
    multiple computers. First let''s start with installing the latest IPyparallel
    package and all the IPython components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Especially on Windows, it might be easier to install IPython using Anaconda
    instead, as it includes binaries for many science, math, engineering, and data
    analysis packages. To get a consistent installation, the Anaconda installer is
    also available for OS X and Linux systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Secondly, we need a cluster configuration. Technically this is optional, but
    since we are going to create a distributed IPython cluster, it is much more convenient
    to configure everything using a specific profile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: These configuration files contain a huge amount of options so I recommend searching
    for a specific section instead of walking through them. A quick listing gave me
    about 2500 lines of configuration in total for these five files. The filenames
    already provide hint about the purpose of the configuration files, but we'll explain
    them in a little more detail since they are still a tad confusing.
  prefs: []
  type: TYPE_NORMAL
- en: ipython_config.py
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This is the generic IPython configuration file; you can customize pretty much
    everything about your IPython shell here. It defines how your shell should look,
    which modules should be loaded by default, whether or not to load a GUI, and quite
    a bit more. For the purpose of this chapter not all that important but it''s definitely
    worth a look if you''re going to use IPython more often. One of the things you
    can configure here is the automatic loading of extensions, such as `line_profiler`
    and `memory_profiler` discussed in the previous chapter. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: ipython_kernel_config.py
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This file configures your IPython kernel and allows you to overwrite/extend
    `ipython_config.py`. To understand its purpose, it's important to know what an
    IPython kernel is. The kernel, in this context, is the program that runs and introspects
    the code. By default this is `IPyKernel`, which is a regular Python interpreter,
    but there are also other options such as `IRuby` or `IJavascript` to run Ruby
    or JavaScript respectively.
  prefs: []
  type: TYPE_NORMAL
- en: One of the more useful options is the possibility to configure the listening
    port(s) and IP addresses for the kernel. By default the ports are all set to use
    a random number, but it is important to note that if someone else has access to
    the same machine while you are running your kernel, they will be able to connect
    to your IPython kernel which can be dangerous on shared machines.
  prefs: []
  type: TYPE_NORMAL
- en: ipcontroller_config.py
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`ipcontroller` is the master process of your IPython cluster. It controls the
    engines and the distribution of tasks, and takes care of tasks such as logging.'
  prefs: []
  type: TYPE_NORMAL
- en: The most important parameter in terms of performance is the `TaskScheduler`
    setting. By default, the `c.TaskScheduler.scheme_name` setting is set to use the
    Python LRU scheduler, but depending on your workload, others such as `leastload`
    and `weighted` might be better. And if you have to process so many tasks on such
    a large cluster that the scheduler becomes the bottleneck, there is also the `plainrandom`
    scheduler that works surprisingly well if all your machines have similar specs
    and the tasks have similar durations.
  prefs: []
  type: TYPE_NORMAL
- en: For the purpose of our test we will set the IP of the controller to *, which
    means that **all** IP addresses will be accepted and that every network connection
    will be accepted. If you are in an unsafe environment/network and/or don't have
    any firewalls which allow you to selectively enable certain IP addresses, then
    this method is **not** recommended! In such cases, I recommend launching through
    more secure options, such as `SSHEngineSetLauncher` or `WindowsHPCEngineSetLauncher`
    instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'But, assuming your network is indeed safe, set the factory IP to all the local
    addresses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now start the controller:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Pay attention to the files that were written to the security directory of the
    profile directory. They have the authentication information which is used by `ipengine`
    to find `ipcontroller`. It contains the ports, encryption keys, and IP address.
  prefs: []
  type: TYPE_NORMAL
- en: ipengine_config.py
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`ipengine` is the actual worker process. These processes run the actual calculations,
    so to speed up the processing you will need these on as many machines as you have
    available. You probably won''t need to change this file, but it can be useful
    if you want to configure centralized logging or need to change the working directory.
    Generally, you don''t want to start the `ipengine` process manually since you
    will most likely want to launch multiple processes per computer. That''s where
    our next command comes in, the `ipcluster` command.'
  prefs: []
  type: TYPE_NORMAL
- en: ipcluster_config.py
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `ipcluster` command is actually just an easy shorthand to start a combination
    of `ipcontroller` and `ipengine` at the same time. For a simple local processing
    cluster, I recommend using this, but when starting a distributed cluster, it can
    be useful to have the control that the separate use of `ipcontroller` and `ipengine`
    offers. In most cases the command offers enough options, so you might have no
    need for the separate commands.
  prefs: []
  type: TYPE_NORMAL
- en: The most important configuration option is `c.IPClusterEngines.engine_launcher_class`,
    as this controls the communication method between the engines and the controller.
    Along with that, it is also the most important component for secure communication
    between the processes. By default it's set to `ipyparallel.apps.launcher.LocalControllerLauncher`
    which is designed for local processes but `ipyparallel.apps.launcher.SSHEngineSetLauncher`
    is also an option if you want to use SSH to communicate with the clients. Or `ipyparallel.apps.launcher.WindowsHPCEngineSetLauncher`
    for Windows HPC.
  prefs: []
  type: TYPE_NORMAL
- en: Before we can create the cluster on all machines, we need to transfer the configuration
    files. Your options are to transfer all the files or to simply transfer the files
    in your IPython profile's `security` directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now it''s time to start the cluster, since we already started the `ipcontroller`
    separately, we only need to start the engines. On the local machine we simply
    need to start it, but the other machines don''t have the configuration yet. One
    option is copying the entire IPython profile directory, but the only file that
    really needs copying is `security/ipcontroller-engine.json`. After creating the
    profile using the profile creation command that is. So unless you are going to
    copy the entire IPython profile directory, you need to execute the profile creation
    command again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, simply copy the `ipcontroller-engine.json` file and you''re done.
    Now we can start the actual engines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Note that the `4` here was chosen for a quad-core processor, but any number
    would do. The default will use the amount of logical processor cores, but depending
    on the workload it might be better to match the amount of physical processor cores
    instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can run some parallel code from our IPython shell. To demonstrate the
    performance difference, we will use a simple sum of all the numbers from 0 to
    10,000,000\. Not an extremely heavy task, but when performed 10 times in succession,
    a regular Python interpreter takes a while:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This time however, to illustrate the difference, we will run it a 100 times
    to demonstrate how fast a distributed cluster is. Note that this is with only
    three machines cluster, but it''s still quite a bit faster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'More fun however is the definition of parallel functions in IPyParallel. With
    just a simple decorator, a function is marked as parallel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The IPyParallel library offers many more useful features, but that is outside
    the scope of this book. Even though IPyParallel is a separate entity from the
    rest of Jupyter/IPython, it does integrate well, which makes combining them easy
    enough.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the most convenient ways of using IPyParallel is through the Jupyter/IPython
    Notebooks. To demonstrate, we first have to make sure to enable the parallel processing
    in the Jupyter Notebook since IPython notebooks execute single threaded by default:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'After that we can start the `notebook` and see what it''s all about:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'With the Jupyter Notebook you can create scripts in your web browser which
    can easily be shared with others later. It is really very useful for sharing scripts
    and debugging your code, especially since web pages (as opposed to command line
    environments) can display images easily. This helps a lot with graphing data.
    Here''s a screenshot of our Notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ipcluster_config.py](images/4711_13_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter has shown us how multiprocessing works, how we can pool a lot of
    jobs, and how we should share data between multiple processes. But more interestingly,
    it has also shown how we can distribute processing across multiple machines which
    helps a lot in speeding up heavy calculations.
  prefs: []
  type: TYPE_NORMAL
- en: The most important lesson you can learn from this chapter is that you should
    always try to avoid data sharing and synchronisation between multiple processes
    or servers, as it is slow and will thus slow down your applications a lot. Whenever
    possible, keep your calculations and data local.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter we will learn about creating extensions in C/C++ to increase
    performance and allow low-level access to memory and other hardware resources.
    While Python will generally protect you from silly mistakes, C and C++ most certainly
    won't.
  prefs: []
  type: TYPE_NORMAL
- en: '|   | *"C makes it easy to shoot yourself in the foot; C++ makes it harder,
    but when you do, it blows away your whole leg."* |   |'
  prefs: []
  type: TYPE_TB
- en: '|   | --*Bjarne Stroustrup (the creator of C++)* |'
  prefs: []
  type: TYPE_TB
