- en: '*Chapter 3*: Working with Nodes'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Everyone familiar with Kubernetes knows that the cluster workload runs in nodes,
    where all Kubernetes pods get scheduled, deployed, redeployed, and destroyed.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes runs the workload by placing containers into pods and then schedules
    them to run on nodes. A node might be a virtual or physical machine, depending
    on the cluster setup. Each node has the services necessary to run pods, managed
    by the Kubernetes control plane.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main components of the node are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**kubelet**: An agent that registers/deregisters the node with the Kubernetes
    API.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Container runtime**: This runs containers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kube-proxy**: Network proxy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If the Kubernetes cluster supports nodes autoscaling, then nodes can come and
    go as specified by the autoscaling rules: by setting min and max node counts.
    If there is not much load running in the cluster, unnecessary nodes will be removed
    down to the minimum nodes set by the autoscaling rules. And when the load increases,
    the required amount of nodes will be deployed to accommodate the newly scheduled
    pods.'
  prefs: []
  type: TYPE_NORMAL
- en: There are times when you need to troubleshoot, get information about the nodes
    in the cluster, find out which pods they are running, see how much CPU and memory
    they are consuming, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: There are always going to be cases when you need to stop scheduling pods on
    some nodes, or rescheduling pods to different nodes, or temporally disabling the
    scheduling of any pods to some nodes, removing nodes, or any other reasons.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting a list of nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describing nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Displaying node resource usage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cordoning nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Draining nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removing nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to node pools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting a list of nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To start working with nodes, you need to get a list of them first. To get the
    nodes list, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following list of nodes using the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Nodes list'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B16411_03_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.1 – Nodes list
  prefs: []
  type: TYPE_NORMAL
- en: The preceding list shows we have three nodes in our Kubernetes cluster with
    a `Ready` status and Kubernetes version `1.17.5-gke.9`. However, if you have cloud-supported
    node pools with autoscaling, your nodes list could be different because nodes
    will be added/removed depending on the number of applications running in your
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Describing nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `kubectl describe` command allows us to get the state, metadata, and events
    of an object in a Kubernetes cluster. In this section, we will use it to describe
    the node.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have got a list of nodes, so let''s check out one of them:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To describe a node, run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As the command's output is quite big, we are going to show only some parts of
    it. You can check out the full output yourself.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following screenshot, we see the assigned `Labels` (which can be used
    to organize and select subsets of objects) and `Annotations` (extra information
    about the node is stored there) for the node, and `Unschedulable: false` means
    that the node accepts pods to be scheduled on to it. For example, `Labels` can
    be used for `Node Affinity` (which allows us to constrain which nodes the pod
    is eligible to be scheduled on, based on the labels on the node) to schedule pods
    on particular nodes:![Figure 3.2 – Node describe – check labels and annotations'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](image/B16411_03_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.2 – Node describe – check labels and annotations
  prefs: []
  type: TYPE_NORMAL
- en: In the following screenshot, we see the assigned internal and external IPs,
    the internal DNS name, and the hostname:![Figure 3.3 – Node describe – assigned
    internal and external IPs
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](image/B16411_03_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.3 – Node describe – assigned internal and external IPs
  prefs: []
  type: TYPE_NORMAL
- en: The following screenshot shows the running pods on the node with CPU/memory
    requests and limits per pod:![Figure 3.4 – Node describe – CPU/memory requests
    and limits per pod
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](image/B16411_03_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.4 – Node describe – CPU/memory requests and limits per pod
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the allocated resources for the node:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.5 – Node describe – allocated resources for the node'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B16411_03_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.5 – Node describe – allocated resources for the node
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the `$ kubectl describe node` command allows you to get various
    information about the node.
  prefs: []
  type: TYPE_NORMAL
- en: Displaying node resource usage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is handy to know what resources are consumed by nodes. To display the resources
    used by nodes, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following list of nodes using the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6 – Top nodes list with resources used'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B16411_03_006.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.6 – Top nodes list with resources used
  prefs: []
  type: TYPE_NORMAL
- en: The previous command shows node metrics such as CPU cores, memory (in bytes),
    and CPU and memory percentage usage.
  prefs: []
  type: TYPE_NORMAL
- en: Also, by using `$ watch kubectl top nodes`, you can watch and monitor nodes
    in real time when, for example, load testing your application or doing other node
    operations.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The `watch` command might not be present in your computer, you might need to
    install it. The `watch` command will run the specified command and refresh the
    screen every few seconds.
  prefs: []
  type: TYPE_NORMAL
- en: Cordoning nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's suppose we are going to run an app's load test and we want to keep a node
    away from the load test. In the node list that we saw in the *Getting a list of
    nodes* section, we have three nodes, and they are all in the `Ready` state. Let's
    pick one node, `gke-kubectl-lab-default-pool-b3c7050d-8jhj`, which we do not want
    new pods to be scheduled on.
  prefs: []
  type: TYPE_NORMAL
- en: '`kubectl` has a command called `cordon`, which allows us to make a node unschedulable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s cordon the `gke-kubectl-lab-default-pool-b3c7050d-8jhj` node and then
    print a nodes list. To cordon the node, run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output after running the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8 – Cordoning nodes'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B16411_03_007.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.8 – Cordoning nodes
  prefs: []
  type: TYPE_NORMAL
- en: We have cordoned the `gke-kubectl-lab-default-pool-b3c7050d-8jhj` node so from
    now on, no new pods will be scheduled onto that node, but whatever pods are running
    there will stay running on that node.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: If the cordoned node gets rebooted then all pods that were scheduled on it will
    get rescheduled to different nodes, as even when rebooting the node its readiness
    status doesn't change.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want the node to be scheduled on again, you just use `uncordon` command.
    To uncordon the node, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output after running the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9 – Uncordoning nodes'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B16411_03_008.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.9 – Uncordoning nodes
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the preceding screenshot, the `gke-kubectl-lab-default-pool-b3c7050d-8jhj`
    node is in the `Ready` state again and new pods will be scheduled on it from now
    on.
  prefs: []
  type: TYPE_NORMAL
- en: Draining nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You might want to remove/evict all pods from a node that is going to be deleted,
    upgraded, or rebooted, for example. There is a command, `drain`, for that. Its
    output is quite long, so only some of the output will be shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output from the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.10 – Partial kubectl drain – help output'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B16411_03_009.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.10 – Partial kubectl drain – help output
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see from the output, there are a few flags you need to pass to properly
    drain the node: `--ignore-daemonsets` and `–force`.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: A DaemonSet ensures that all specified Kubernetes nodes run a copy of the same
    pod specified in the DaemonSet. A DaemonSet cannot be deleted from the Kubernetes
    node, so the `--ignore-daemonsets` flag must be used to force draining the node.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s drain the `gke-kubectl-lab-default-pool-b3c7050d-8jhj` node using the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We drain the node using the preceding command. The output of this command is
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.11 – Drain node'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B16411_03_010.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.11 – Drain node
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: We have passed the `--ignore-daemonsets` flag so that if there are any DaemonSets
    running on the node the `drain` command will not fail.
  prefs: []
  type: TYPE_NORMAL
- en: So, we have drained the node. What else does `drain` do? It cordons the node
    as well, so no more pods can be scheduled on to the node.
  prefs: []
  type: TYPE_NORMAL
- en: Now we are ready to delete the node.
  prefs: []
  type: TYPE_NORMAL
- en: Removing nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `gke-kubectl-lab-default-pool-b3c7050d-8jhj` node got drained and is not
    running any deployments, pods, or StatefulSets, so it can be easily deleted now.
  prefs: []
  type: TYPE_NORMAL
- en: 'We do it using the `delete node` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We delete the node using the preceding command. The output of this command
    is as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.12 – Delete node'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B16411_03_011.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.12 – Delete node
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the `kubectl get nodes` output, the node was unregistered
    from the Kubernetes API and got deleted.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Actual node deletion depends on your Kubernetes setup. In cloud-hosted clusters,
    the node gets unregistered and deleted, but if you are running an on-premise self-hosted
    Kubernetes cluster, the actual node will not be deleted but only deregistered
    from the Kubernetes API.
  prefs: []
  type: TYPE_NORMAL
- en: Also, when you specify the cluster size in the cloud setup, the new node will
    replace the deleted one after some time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s run `kubectl get nodes` to check the nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.13 – Nodes list'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/B16411_03_012.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.13 – Nodes list
  prefs: []
  type: TYPE_NORMAL
- en: A few minutes later, we see the third node is back, even with the same name.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to node pools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cloud providers that have Kubernetes as a managed service support node pools.
    Let's learn what they are.
  prefs: []
  type: TYPE_NORMAL
- en: A node pool is just a group of Kubernetes nodes that have the same compute spec
    and the same Kubernetes node labels, nothing else too fancy.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we have two node pools:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The default pool with the `node-pool: default-pool` node label'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The web app pool with the `node-pool: web-app` node label'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes node labels can be used in node selectors and Node Affinity to control
    how workloads are scheduled to your nodes.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to learn how to use Kubernetes node pools with Node Affinity in
    [*Chapter 5*](B16411_05_Final_VK_ePub.xhtml#_idTextAnchor055), *Updating and Deleting
    Applications*.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned how to use `kubectl` to list nodes running
    in the cluster, get information about the nodes and their resources usage; we've
    seen how to cordon, drain, and remove nodes; and we had an introduction to node
    pools.
  prefs: []
  type: TYPE_NORMAL
- en: We have learned new skills that can be applied in real-world scenarios to conduct
    maintenance on Kubernetes nodes.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we're going to learn how to create and deploy applications
    to a Kubernetes cluster using `kubectl`.
  prefs: []
  type: TYPE_NORMAL
