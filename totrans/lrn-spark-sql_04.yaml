- en: Using Spark SQL for Data Munging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this code-intensive chapter, we will present key data munging techniques
    used to transform raw data to a usable format for analysis. We start with some
    general data munging steps that are applicable in a wide variety of scenarios.
    Then, we shift our focus to specific types of data including time-series data,
    text, and data preprocessing steps for Spark MLlib-based machine learning pipelines.
    We will use several Datasets to illustrate these techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we shall learn:'
  prefs: []
  type: TYPE_NORMAL
- en: What is data munging?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explore data munging techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combine data using joins
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Munging on textual data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Munging on time-series data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dealing with variable length records
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data preparation for machine learning pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing data munging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Raw data is typically messy and requires a series of transformations before
    it becomes useful for modeling and analysis work. Such Datasets can have missing
    data, duplicate records, corrupted data, incomplete records, and so on. In its
    simplest form, data munging, or data wrangling, is basically the transformation
    of raw data into a usable format. In most projects, this is the most challenging
    and time-consuming step.
  prefs: []
  type: TYPE_NORMAL
- en: However, without data munging your project can reduce to a garbage-in, garbage-out
    scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, you will execute a bunch of functions and processes such as subset,
    filter, aggregate, sort, merge, reshape, and so on. In addition, you will also
    do type conversions, add new fields/columns, rename fields/columns, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: A large project can comprise of several different kinds of data with varying
    degrees of data quality. There can be a mix of numerical, textual, time-series,
    structured, and unstructured data including audio and video data used together
    or separately for analysis. A substantial part of such projects consist of cleansing
    and transformation steps combined with some statistical analyses and visualization.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use several Datasets to demonstrate the key data munging techniques
    required for preparing the data for subsequent modeling and analyses. These Datasets
    and their sources are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Individual household electric power consumption Dataset**: The original source
    for the Dataset provided by Georges Hebrail, Senior Researcher, EDF R&D, Clamart,
    France and Alice Berard, TELECOM ParisTech Master of Engineering Internship at
    EDF R&D, Clamart, France. The Dataset consists of measurements of electric power
    consumption in one household at one-minute intervals for a period of nearly four
    years. This Dataset is available for download from the UCI Machine Learning Repository
    from the following URL:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption](https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Machine Learning based ZZAlpha Ltd Stock Recommendations 2012-2014 Dataset**:
    This Dataset contains recommendations made, for various US traded stock portfolios,
    the morning of each day during a three year period from Jan 1, 2012 to Dec 31,
    2014\. This Dataset can be downloaded from the following URL:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://archive.ics.uci.edu/ml/datasets/Machine+Learning+based+ZZAlpha+Ltd.+Stock+Recommendations+2012-2014](https://archive.ics.uci.edu/ml/datasets/Machine+Learning+based+ZZAlpha+Ltd.+Stock+Recommendations+2012-2014).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Paris weather history Dataset**: This Dataset contains the daily weather
    report for Paris. We downloaded historical data covering the same time period
    as in the household electric power consumption Dataset. This Dataset can downloaded
    from the following URL:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.wunderground.com/history/airport/LFPG](https://www.wunderground.com/history/airport/LFPG).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Original 20 newsgroups data**: This data set consists of 20,000 messages
    taken from 20 Usenet newsgroups. The original owner and donor of this Dataset
    was Tom Mitchell, School of Computer Science, Carnegie Mellon University. Approximately
    a thousand Usenet articles were taken from each of the 20 newsgroups. Each newsgroup
    is stored in a subdirectory and each article stored as a separate file. The Dataset
    can be downloaded from the following URL:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.html](http://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.html).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Yahoo finance data**: This Dataset comprises of historical daily stock prices
    for six stocks for one year duration (from 12/04/2015 to 12/04/2016). The data
    for each of the ticker symbols chosen can been downloaded from the following site:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ ](http://finance.yahoo.com/)[http://finance.yahoo.com/](http://finance.yahoo.com/).'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring data munging techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will introduce several data munging techniques using household
    electric consumption and weather Datasets. The best way to learn these techniques
    is to practice the various ways to manipulate the data contained in various publically
    available Datasets (in addition to the ones used here). The more you practice,
    the better you will get at it. In the process, you will probably evolve your own
    style, and develop several toolsets and techniques to achieve your munging objectives.
    At a minimum, you should get very comfortable working with and moving between
    RDDs, DataFrames, and Datasets, computing counts, distinct counts, and various
    aggregations to cross-check your results and match your intuitive understanding
    the Datasets. Additionally, it is also important to develop the ability to make
    decisions based on the pros and cons of executing any given munging step.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will attempt to accomplish the following objectives in this section:'
  prefs: []
  type: TYPE_NORMAL
- en: Pre-process the household electric consumption Dataset--read the input Dataset,
    define case class for the rows,  count the number of records, remove the header
    and rows with missing data values, and create a DataFrame.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute basic statistics and aggregations
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Augment the Dataset with new information relevant to the analysis
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Execute other miscellaneous processing steps, if required
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pre-process the weather Dataset--similar to step 1
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Analyze missing data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Combine the Datasets using JOIN and analyze the results
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start the Spark shell, at this time, and follow along as you read through this
    and the subsequent sections.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import all required classes used in this section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00085.gif)'
  prefs: []
  type: TYPE_IMG
- en: Pre-processing of the household electric consumption Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create a `case` class for household electric power consumption called `HouseholdEPC`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00086.gif)'
  prefs: []
  type: TYPE_IMG
- en: Read the input Dataset into a RDD and count the number of rows in it.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00087.gif)![](img/00088.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, remove the header and all other rows containing missing values, (represented
    as `?''s` in the input), as shown in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00089.gif)![](img/00090.gif)'
  prefs: []
  type: TYPE_IMG
- en: In the next step, convert the `RDD [String]` to a `RDD` with the `case` class,
    we defined earlier, and convert the RDD a DatFrame of `HouseholdEPC` objects.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00091.gif)'
  prefs: []
  type: TYPE_IMG
- en: Display a few sample records in the DataFrame, and count the number of rows
    in it to verify that the number of rows in the DataFrame matches the expected
    number of rows in your input Dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00092.gif)'
  prefs: []
  type: TYPE_IMG
- en: Computing basic statistics and aggregations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, compute and display some basic statistics for the numeric columns in the
    DataFrame to get a feel for the data, we will be working with.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00093.gif)'
  prefs: []
  type: TYPE_IMG
- en: We can also display the basic statistics for some or all of the columns rounded
    to four decimal places. We can also rename each of the columns by prefixing a `r`
    to the column names to differentiate them from the original column names.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00094.gif)![](img/00095.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'In addition, we count the distinct number of days, for which the data is contained
    in the DataFrame using an aggregation function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00096.gif)'
  prefs: []
  type: TYPE_IMG
- en: Augmenting the Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can augment the DataFrame with new columns for the day of the week, day of
    the month, month and year information. For example, we may be interested in studying
    power consumption on weekdays versus weekends. This can help achieve a better
    understanding of the data through visualization or pivoting based on these fields.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00097.gif)'
  prefs: []
  type: TYPE_IMG
- en: Executing other miscellaneous processing steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If required we can choose to execute a few more steps to help cleanse the data
    further, study more aggregations, or to convert to a typesafe data structure,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: We can drop the time column and aggregate the values in various columns using
    aggregation functions such as sum and average on the values of each day's readings.
    Here, we rename the columns with a `d` prefix to represent daily values.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00098.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'We display a few sample records from this DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00099.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, we group the readings by year and month, and then count the number of
    readings and display them for each of the months. The first month's number of
    readings is low as the data was captured in half a month.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00100.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also convert our DataFrame to a Dataset using a `case` class, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00101.gif)'
  prefs: []
  type: TYPE_IMG
- en: At this stage, we have completed all the steps for pre-processing the household
    electric consumption Dataset. We now shift our focus to processing the weather
    Dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Pre-processing of the weather Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we define a `case` class for the weather readings.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00102.gif)'
  prefs: []
  type: TYPE_IMG
- en: Next, we read in the four files of the daily weather readings (downloaded from
    the Paris Weather website) approximately matching the same duration as the household
    electric power consumption readings.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00103.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Remove the headers from each of the input files shown as follows. We have shown
    the output of header values so you get a sense of the various weather reading
    parameters captured in these Datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00104.gif)'
  prefs: []
  type: TYPE_IMG
- en: Analyzing missing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If we wanted to get a sense of the number of rows containing one or more missing
    fields in the RDD, we can create a RDD with these rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00105.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also do the same, if our data was available in a DataFrame as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00106.gif)'
  prefs: []
  type: TYPE_IMG
- en: A quick check of the Dataset reveals that most of the rows with missing data
    also have missing values for the Events and Max Gust Speed Km/h columns. Filtering
    on these two column values actually, captures all the rows with missing field
    values. It also matches the results for missing values in the RDD.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00107.gif)'
  prefs: []
  type: TYPE_IMG
- en: As there are many rows that contain one or more missing fields, we choose to
    retain these rows to ensure we do not lose valuable information. In the following
    function, we insert `0` in all the missing fields of an RDD.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00108.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'We can replace `0` inserted in the previous step with an `NA` in the string
    fields, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00109.gif)'
  prefs: []
  type: TYPE_IMG
- en: At this stage, we can combine the rows of the four Datasets into a single Dataset
    using the `union` operation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00110.gif)'
  prefs: []
  type: TYPE_IMG
- en: At this stage, the processing of our second Dataset containing weather data
    is complete. In the next section, we combine these pre-processed Datasets using
    a `join` operation.
  prefs: []
  type: TYPE_NORMAL
- en: Combining data using a JOIN operation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will introduce the JOIN operation, in which the daily household
    electric power consumption is combined with the weather data. We have assumed
    the locations of readings taken for the household electric power consumption and
    the weather readings are in close enough proximity to be relevant.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we use the join operation to combine the daily household electric power
    consumption Dataset with the weather Dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00111.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Verify the number of rows in the final DataFrame obtained with the number of
    rows expected subsequent to the `join` operation shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00112.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'You can compute a series of correlations between various columns in the newly
    joined Dataset containing columns from each of the two original Datasets to get
    a feel for the strength and direction of relationships between the columns, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00113.gif)'
  prefs: []
  type: TYPE_IMG
- en: Similarly, you can join the Datasets grouped by year and month to get a higher-level
    summarization of the data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00114.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'In order to visualize the summarized data, we can execute the preceding statements
    in an Apache Zeppelin notebook. For instance, we can plot the monthly **Global
    Reactive Power** (**GRP**) values by transforming `joinedMonthlyDF` into a table
    and then selecting the appropriate columns from it, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00115.jpeg)![](img/00116.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, if you want to analyze readings by the day of the week then follow,
    the steps as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00117.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, we print the schema of the joined Dataset (augmented with the day
    of the week column) so you can further explore the relationships between various
    fields of this DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00118.gif)'
  prefs: []
  type: TYPE_IMG
- en: In the next section, we shift our focus to munging textual data.
  prefs: []
  type: TYPE_NORMAL
- en: Munging textual data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we explore data munging techniques for typical text analysis
    situations. Many text-based analyses tasks require computing word counts, removing
    stop words, stemming, and so on. In addition, we will also explore how you can
    process multiple files, one at a time, from HDFS directories.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import all the classes that will be used in this section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00119.gif)'
  prefs: []
  type: TYPE_IMG
- en: Processing multiple input data files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the next few steps, we initialize a set of variables for defining the directory
    containing the input files, and an empty RDD. We also create a list of filenames
    from the input HDFS directory. In the following example, we will work with files
    contained in a single directory; however, the techniques can easily be extended
    across all 20 newsgroup sub-directories.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00120.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we write a function to compute the word counts for each file and collect
    the results in an `ArrayBuffer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00121.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'We have included a print statement to display the file names as they are picked
    up for processing, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00122.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'We add the rows into a single RDD using the `union` operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00123.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'We could have directly executed the union step as each file is processed, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00124.gif)'
  prefs: []
  type: TYPE_IMG
- en: However, using `RDD.union()` creates a new step in the lineage graph requiring
    an extra set of stack frames for each new RDD. This can easily lead to a Stack
    Overflow condition. Instead, we use `SparkContext.union()` which executes the
    `union` operation all at once without the extra memory overheads.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can cache and print sample rows from our output RDD as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00125.gif)'
  prefs: []
  type: TYPE_IMG
- en: In the next section, we show you ways of filtering out stop words. For simplicity,
    we focus only on well-formed words in the text. However, you can easily add conditions
    to filter out special characters and other anomalies in our data using String
    functions and regexes (for a detailed example, refer [Chapter 9](part0166.html#4U9TC0-e9cbc07f866e437b8aa14e841622275c), *Developing
    Applications with Spark SQL)*.
  prefs: []
  type: TYPE_NORMAL
- en: Removing stop words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our example, we create a set of stop words and filter them out from the words
    in contained in each file. Normally, a Spark operation executing on a remote node
    works on a separate copy of the variables used in the function. We can use a broadcast
    variable to maintain a read-only, cached copy of the set of stop words at each
    node in the cluster instead of shipping a copy of it with the tasks to be executed
    on the nodes. Spark attempts to distribute the broadcast variables efficiently
    to reduce the overall communication overheads. Further more, we also filter out
    empty lists returned by the function as a result of our filtering process and
    stop words removal.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00126.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'We can extract the words from each of the tuples in the RDD and create a DataFrame
    containing them, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00127.gif)'
  prefs: []
  type: TYPE_IMG
- en: In the following example, we show another method for filtering out the stop
    words from our list of words. In order to improve the word matches between the
    two lists, we process the stop words file in a similar manner to the words extracted
    from the input files. We read the file containing stop words, remove leading and
    trailing spaces, convert to lower case, replace special characters, filter out
    empty words, and, finally, create a DataFrame (containing the stop words).
  prefs: []
  type: TYPE_NORMAL
- en: We use the list of stop words available at [http://algs4.cs.princeton.edu/35applications/stopwords.txt](http://algs4.cs.princeton.edu/35applications/stopwords.txt)
    in our example.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00128.gif)'
  prefs: []
  type: TYPE_IMG
- en: Here, we use a `regex` to filter out special characters contained in the file.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00129.gif)'
  prefs: []
  type: TYPE_IMG
- en: Next, we compare the number of words in our list before and after the removal
    of the stop words from our original list of words. The final number of words remaining
    suggests that a majority of words in our input files were stop words.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00130.gif)'
  prefs: []
  type: TYPE_IMG
- en: For a more detailed coverage of text data processing (of an annual `10-K` financial
    filing document and other document corpuses) including building pre-processing
    data pipelines, identifying themes in document corpuses, using Naïve Bayes classifiers,
    and developing a machine learning application, refer [Chapter 9](part0166.html#4U9TC0-e9cbc07f866e437b8aa14e841622275c), *Developing
    Applications with Spark SQL.*
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we shift our focus to munging time-series data using the
    `spark-time-series` library from Cloudera.
  prefs: []
  type: TYPE_NORMAL
- en: Munging time series data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Time series data is a sequence of values linked to a timestamp. In this section,
    we use Cloudera's `spark-ts` package for analyzing time-series data.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to *Cloudera Engineering Blog*, *A New Library for Analyzing Time-Series
    Data with Apache Spark*, for more details on time-series data and its processing
    using `spark-ts`. This blog is available at: [https://github.com/sryza/spark-timeseries](https://github.com/sryza/spark-timeseries).
  prefs: []
  type: TYPE_NORMAL
- en: 'The `spark-ts` package can be downloaded and built using instructions available
    at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/sryza/spark-timeseries](https://github.com/sryza/spark-timeseries).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will attempt to accomplish the following objectives in the following sub-sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Pre-processing of the time-series Dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing date fields
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Persisting and loading data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining a date-time index
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the  `TimeSeriesRDD` object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling missing time-series data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing basic statistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For this section, specify inclusion of the `spark-ts.jar` file while starting
    the Spark shell as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00131.gif)'
  prefs: []
  type: TYPE_IMG
- en: We download Datasets containing pricing and volume data for six stocks over
    a one year period from the Yahoo Finance site. We will need to pre-process the
    data before we can use the `spark-ts` package for time-series data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Import the classes required in this section.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00132.gif)'
  prefs: []
  type: TYPE_IMG
- en: Pre-processing of the time-series Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Read the data from the input data files and define a `case` class Stock containing
    the fields in the Dataset plus a field to hold the ticker symbol.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00133.gif)'
  prefs: []
  type: TYPE_IMG
- en: Next, we remove the header from each of the files, map our RDD row using the
    `case` class, include a string for the ticker symbol, and convert the RDD to a
    DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00134.gif)'
  prefs: []
  type: TYPE_IMG
- en: Next, we combine the rows from each of our DataFrames using `union`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00135.gif)'
  prefs: []
  type: TYPE_IMG
- en: Processing date fields
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the next step, we separate out the date column into three separate fields
    containing the day, month, and the year information.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00136.gif)'
  prefs: []
  type: TYPE_IMG
- en: Persisting and loading data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At this stage, we can persist our DataFrame to a CSV file using the `DataFrameWriter`
    class. The overwrite mode lets you overwrite the file, if it is already present
    from a previous execution of the `write` operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00137.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'For loading the time series Dataset written to disk in the previous step, we
    define a function to load our observations from a file and return a DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00138.gif)![](img/00139.gif)'
  prefs: []
  type: TYPE_IMG
- en: Defining a date-time index
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We define a date-time index for the period for which we have the data so that
    each record (for a specific ticker symbol) includes a time series represented
    as an array of `366` positions for each of the days in the year (plus one extra
    day as we have downloaded the data from 12/04/2015 to 12/04/2016). The Business
    Day Frequency specifies that the data is available for the business days of the
    year only.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00140.gif)'
  prefs: []
  type: TYPE_IMG
- en: Using the  TimeSeriesRDD object
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main abstraction in the `spark-ts` library is a RDD called `TimeSeriesRDD`.
    The data is a set of observations represented as a tuple of (timestamp, key, value).
    The key is a label used to identify the time series. In the following example,
    our tuple is (timestamp, ticker, close). Each series in the RDD has the ticker
    symbol as the key and the daily closing price of the stock as the value.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We can cache and display the number of rows in the RDD which should be equal
    to the number of stocks in our example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00141.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Display a couple of rows from the RDD to see the data in each row:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00142.gif)'
  prefs: []
  type: TYPE_IMG
- en: Handling missing time-series data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, we check the RDD for missing data. The missing data is marked with `NaN`
    values. Computing basic statistics with `NaN` values present will give errors.
    Hence, we need to replace these missing values with approximations. Our example
    data does not contain any missing fields. However, as an exercise, we delete a
    few values from the input datasets to simulate these `NaN` values in the RDD,
    and then impute these values using linear interpolation. Other approximations
    available include next, previous, and nearest values.
  prefs: []
  type: TYPE_NORMAL
- en: 'We fill in the approximate values for the missing values, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00143.gif)'
  prefs: []
  type: TYPE_IMG
- en: Computing basic statistics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, we compute the mean, standard deviation, max and min values for each
    of our series, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00144.gif)'
  prefs: []
  type: TYPE_IMG
- en: There are many other useful functions available for exploratory data analysis
    and data munging using the `TimeSeriesRDD` object. These include collecting the
    RDD as a local time series, finding specific time series, various filters and
    slicing functionality, sorting and re-partitioning the data, writing out the time
    series to CSV files, and many more.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with variable length records
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will explore a way of dealing with variable length records.
    Our approach essentially converts each of the rows to a fixed length record equal
    to the maximum length record. In our example, as each row represents a portfolio
    and there is no unique identifier, this method is useful for manipulating data
    into the familiar fixed length records case. We will generate the requisite number
    of fields to equal the maximum number of stocks in the largest portfolio. This
    will lead to empty fields where the number of stocks is less than the maximum
    number of stocks in any portfolio. Another way to deal with variable length records
    is to use the `explode()` function to create new rows for each stock in a given
    portfolio (for an example of using the `explode()` function, refer [Chapter 9](part0166.html#4U9TC0-e9cbc07f866e437b8aa14e841622275c), *Developing
    Applications with Spark SQL).*
  prefs: []
  type: TYPE_NORMAL
- en: To avoid repeating all the steps from previous examples to read in all the files,
    we have combined the data into a single input file in this example.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the classes required and read the input file into an RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00145.gif)'
  prefs: []
  type: TYPE_IMG
- en: We count the total number of portfolios and print a few records from the RDD.
    You can see that while the first and the second portfolios contain one stock each,
    the third one contains two stocks.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00146.gif)'
  prefs: []
  type: TYPE_IMG
- en: Converting variable-length records to fixed-length records
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our example Dataset, there are no fields missing, hence, we can use the number
    of commas in each row to derive the varying number stock-related fields in each
    of the portfolios. Alternatively, this information can be extracted from the strings
    contained in last field of the RDD.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we create a UDF to count the number of stocks indirectly by counting the
    number of commas in each row. We use `describe` to find the maximum number of
    commas across all rows in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00147.gif)'
  prefs: []
  type: TYPE_IMG
- en: In the next step, we augment the DataFrame with a column containing the number
    of commas.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00148.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Then we write a function to insert the correct number of commas in each row
    at the appropriate location:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00149.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we drop the number of commas column, as it is not required in the subsequent
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00150.gif)![](img/00151.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'At this stage, if you want to get rid of duplicate rows in the DataFrame, then
    you can use the `dropDuplicates` method shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00152.gif)'
  prefs: []
  type: TYPE_IMG
- en: In the next step, we define a `case` class for the `Portfolio` with the maximum
    number of stocks in the largest portfolio.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00153.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we convert the RDD into a DataFrame. For convenience, we will demonstrate
    the operations using fewer stock-related columns; however, the same can be extended
    to fields for other stocks in the portfolio:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00154.gif)![](img/00155.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'We can replace empty fields for stocks in the smaller portfolios with `NA` , as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00156.gif)'
  prefs: []
  type: TYPE_IMG
- en: Extracting data from "messy" columns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we continue on from the previous section, however, we will
    work with a single stock to demonstrate the data manipulations required to modify
    the data fields to a state where we end up with cleaner and richer data than we
    started with.
  prefs: []
  type: TYPE_NORMAL
- en: 'As most of the fields contain several pieces of information, we will execute
    a series of statements to separate them out into their own independent columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00157.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'In the next step, we remove the first underscore with a space in the `datestr`
    column. This results in separating out the date field:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00158.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we separate out the information in the stock column, as it contains several
    pieces of useful information including the ticker symbol, ratio of the selling
    price and purchase price, and the selling price and purchase price. First, we
    get rid of the `=` in the stock column by replacing it with an empty string:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00159.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, the values in each column separated by spaces in each column are converted
    into an array of the values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00160.gif)![](img/00161.gif)'
  prefs: []
  type: TYPE_IMG
- en: Next, we use a `UDF` to pick certain elements from the arrays in each column
    into their own separate columns.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00162.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'The file column is not particularly useful for our analysis, except for extracting
    the information at the beginning of the filename that denotes the pool of stocks
    from which the stocks for any given portfolio were picked. We do that next, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00163.gif)'
  prefs: []
  type: TYPE_IMG
- en: The following is the final version of the DataFrame that is ready for further
    analysis. In this example, we have worked with a single stock however you can
    easily extend the same techniques to all stocks in any given portfolio to arrive
    at the final, clean and rich, DataFrame ready for querying, modeling, and analysis.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00164.gif)'
  prefs: []
  type: TYPE_IMG
- en: In the next section, we briefly introduce the steps required for preparing data
    for use with Spark MLlib machine learning algorithms for classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing data for machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we introduce the process of preparing the input data prior
    to applying Spark MLlib algorithms. Typically, we need to have two columns called
    label and features for using Spark MLlib classification algorithms. We will illustrate
    this with the following example described:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We import the required classes for this section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Pre-processing data for machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We define a set of `UDFs` used in this section. These include, for example,
    checking whether a string contains a specific substring or not, and returning
    a `0.0` or `1.0` value to create the label column. Another `UDF` is used to create
    a features vector from the numeric fields in the DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we can convert the day of week field to a numeric value by binning
    shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00165.gif)'
  prefs: []
  type: TYPE_IMG
- en: In our example, we create a `label` from the `Events` column of the household
    electric consumption Dataset based on whether it rained on given a day or not.
    For illustrative purposes, we use the columns from the household's electric power
    consumption readings in the joined DataFrame from before, even though readings
    from weather Dataset are probably a better predictor of rain.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00166.gif)'
  prefs: []
  type: TYPE_IMG
- en: Finally, we can also split our DataFrame to create training and test Datasets
    containing 70% and 30% of the readings, chosen randomly, respectively. These Datasets
    are used for training and testing machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00167.gif)'
  prefs: []
  type: TYPE_IMG
- en: Creating and running a machine learning pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we present an example of a machine learning pipeline that uses
    the indexers and the training data to train a Random Forest model. We will not
    present detailed explanations for the steps, as our primary purpose here is to
    only demonstrate how the preparatory steps in the previous section are actually
    used.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00168.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00169.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: More details on specific data structures and operations including vectors, processing
    categorical variables, and so on, for Spark MLlib processing, are covered in [Chapter
    6](part0103.html#3279U0-e9cbc07f866e437b8aa14e841622275c), **Using Spark SQL in
    Machine Learning Applications,** and [Chapter 9](part0166.html#4U9TC0-e9cbc07f866e437b8aa14e841622275c),
    *Developing Applications with Spark SQL*. Additionally, techniques for preparing
    data for graph applications are presented in [Chapter 7](part0134.html#3VPBC0-e9cbc07f866e437b8aa14e841622275c),
    **Using Spark SQL in Graph Applications**.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored using Spark SQL for performing some basic data
    munging/wrangling tasks. We covered munging textual data, working with variable
    length records, extracting data from "messy" columns, combining data using JOIN,
    and preparing data for machine learning applications. In addition, we used `spark-ts`
    library to work with time-series data.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will shift our focus to Spark Streaming applications.
    We will introduce you to using Spark SQL in such applications. We will also include
    extensive hands-on sessions for demonstrating the use of Spark SQL in implementing
    the common use cases in Spark Streaming applications.
  prefs: []
  type: TYPE_NORMAL
