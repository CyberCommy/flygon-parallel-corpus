- en: Using Spark SQL for Data Munging
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark SQL进行数据整理
- en: In this code-intensive chapter, we will present key data munging techniques
    used to transform raw data to a usable format for analysis. We start with some
    general data munging steps that are applicable in a wide variety of scenarios.
    Then, we shift our focus to specific types of data including time-series data,
    text, and data preprocessing steps for Spark MLlib-based machine learning pipelines.
    We will use several Datasets to illustrate these techniques.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码密集的章节中，我们将介绍用于将原始数据转换为可用格式进行分析的关键数据整理技术。我们首先介绍适用于各种场景的一些通用数据整理步骤。然后，我们将把重点转移到特定类型的数据，包括时间序列数据、文本和用于Spark
    MLlib机器学习流水线的数据预处理步骤。我们将使用几个数据集来说明这些技术。
- en: 'In this chapter, we shall learn:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习：
- en: What is data munging?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是数据整理？
- en: Explore data munging techniques
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索数据整理技术
- en: Combine data using joins
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用连接合并数据
- en: Munging on textual data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本数据整理
- en: Munging on time-series data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间序列数据整理
- en: Dealing with variable length records
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理可变长度记录
- en: Data preparation for machine learning pipelines
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为机器学习流水线准备数据
- en: Introducing data munging
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍数据整理
- en: Raw data is typically messy and requires a series of transformations before
    it becomes useful for modeling and analysis work. Such Datasets can have missing
    data, duplicate records, corrupted data, incomplete records, and so on. In its
    simplest form, data munging, or data wrangling, is basically the transformation
    of raw data into a usable format. In most projects, this is the most challenging
    and time-consuming step.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据通常混乱不堪，需要经过一系列转换才能变得有用，用于建模和分析工作。这样的数据集可能存在缺失数据、重复记录、损坏数据、不完整记录等问题。在其最简单的形式中，数据整理或数据整理基本上是将原始数据转换为可用格式。在大多数项目中，这是最具挑战性和耗时的步骤。
- en: However, without data munging your project can reduce to a garbage-in, garbage-out
    scenario.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果没有数据整理，您的项目可能会陷入垃圾进垃圾出的境地。
- en: Typically, you will execute a bunch of functions and processes such as subset,
    filter, aggregate, sort, merge, reshape, and so on. In addition, you will also
    do type conversions, add new fields/columns, rename fields/columns, and so on.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，您将执行一系列函数和过程，如子集、过滤、聚合、排序、合并、重塑等。此外，您还将进行类型转换、添加新字段/列、重命名字段/列等操作。
- en: A large project can comprise of several different kinds of data with varying
    degrees of data quality. There can be a mix of numerical, textual, time-series,
    structured, and unstructured data including audio and video data used together
    or separately for analysis. A substantial part of such projects consist of cleansing
    and transformation steps combined with some statistical analyses and visualization.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一个大型项目可能包含各种数据，数据质量不同。可能会混合使用数字、文本、时间序列、结构化和非结构化数据，包括音频和视频数据，一起或分开用于分析。这类项目的一个重要部分包括清洗和转换步骤，结合一些统计分析和可视化。
- en: 'We will use several Datasets to demonstrate the key data munging techniques
    required for preparing the data for subsequent modeling and analyses. These Datasets
    and their sources are listed as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用几个数据集来演示为准备数据进行后续建模和分析所需的关键数据整理技术。以下是这些数据集及其来源：
- en: '**Individual household electric power consumption Dataset**: The original source
    for the Dataset provided by Georges Hebrail, Senior Researcher, EDF R&D, Clamart,
    France and Alice Berard, TELECOM ParisTech Master of Engineering Internship at
    EDF R&D, Clamart, France. The Dataset consists of measurements of electric power
    consumption in one household at one-minute intervals for a period of nearly four
    years. This Dataset is available for download from the UCI Machine Learning Repository
    from the following URL:'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**个人家庭电力消耗数据集**：数据集的原始来源是法国EDF R&D的高级研究员Georges Hebrail和法国Clamart的TELECOM ParisTech工程师实习生Alice
    Berard。该数据集包括近四年内一个家庭每分钟的电力消耗测量。该数据集可以从以下网址下载：'
- en: '[https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption](https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption](https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption)。'
- en: '**Machine Learning based ZZAlpha Ltd Stock Recommendations 2012-2014 Dataset**:
    This Dataset contains recommendations made, for various US traded stock portfolios,
    the morning of each day during a three year period from Jan 1, 2012 to Dec 31,
    2014\. This Dataset can be downloaded from the following URL:'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于机器学习的ZZAlpha Ltd. 2012-2014股票推荐数据集**：该数据集包含了在2012年1月1日至2014年12月31日期间，每天早上针对各种美国交易的股票组合所做的推荐。该数据集可以从以下网址下载：'
- en: '[https://archive.ics.uci.edu/ml/datasets/Machine+Learning+based+ZZAlpha+Ltd.+Stock+Recommendations+2012-2014](https://archive.ics.uci.edu/ml/datasets/Machine+Learning+based+ZZAlpha+Ltd.+Stock+Recommendations+2012-2014).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://archive.ics.uci.edu/ml/datasets/Machine+Learning+based+ZZAlpha+Ltd.+Stock+Recommendations+2012-2014](https://archive.ics.uci.edu/ml/datasets/Machine+Learning+based+ZZAlpha+Ltd.+Stock+Recommendations+2012-2014)。'
- en: '**Paris weather history Dataset**: This Dataset contains the daily weather
    report for Paris. We downloaded historical data covering the same time period
    as in the household electric power consumption Dataset. This Dataset can downloaded
    from the following URL:'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**巴黎天气历史数据集**：该数据集包含了巴黎的每日天气报告。我们下载了与家庭电力消耗数据集相同时间段的历史数据。该数据集可以从以下网址下载：'
- en: '[https://www.wunderground.com/history/airport/LFPG](https://www.wunderground.com/history/airport/LFPG).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.wunderground.com/history/airport/LFPG](https://www.wunderground.com/history/airport/LFPG)。'
- en: '**Original 20 newsgroups data**: This data set consists of 20,000 messages
    taken from 20 Usenet newsgroups. The original owner and donor of this Dataset
    was Tom Mitchell, School of Computer Science, Carnegie Mellon University. Approximately
    a thousand Usenet articles were taken from each of the 20 newsgroups. Each newsgroup
    is stored in a subdirectory and each article stored as a separate file. The Dataset
    can be downloaded from the following URL:'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.html](http://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.html).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '**Yahoo finance data**: This Dataset comprises of historical daily stock prices
    for six stocks for one year duration (from 12/04/2015 to 12/04/2016). The data
    for each of the ticker symbols chosen can been downloaded from the following site:'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ ](http://finance.yahoo.com/)[http://finance.yahoo.com/](http://finance.yahoo.com/).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Exploring data munging techniques
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will introduce several data munging techniques using household
    electric consumption and weather Datasets. The best way to learn these techniques
    is to practice the various ways to manipulate the data contained in various publically
    available Datasets (in addition to the ones used here). The more you practice,
    the better you will get at it. In the process, you will probably evolve your own
    style, and develop several toolsets and techniques to achieve your munging objectives.
    At a minimum, you should get very comfortable working with and moving between
    RDDs, DataFrames, and Datasets, computing counts, distinct counts, and various
    aggregations to cross-check your results and match your intuitive understanding
    the Datasets. Additionally, it is also important to develop the ability to make
    decisions based on the pros and cons of executing any given munging step.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 'We will attempt to accomplish the following objectives in this section:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Pre-process the household electric consumption Dataset--read the input Dataset,
    define case class for the rows,  count the number of records, remove the header
    and rows with missing data values, and create a DataFrame.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute basic statistics and aggregations
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Augment the Dataset with new information relevant to the analysis
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Execute other miscellaneous processing steps, if required
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pre-process the weather Dataset--similar to step 1
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Analyze missing data
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Combine the Datasets using JOIN and analyze the results
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start the Spark shell, at this time, and follow along as you read through this
    and the subsequent sections.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 'Import all required classes used in this section:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00085.gif)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
- en: Pre-processing of the household electric consumption Dataset
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create a `case` class for household electric power consumption called `HouseholdEPC`:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00086.gif)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
- en: Read the input Dataset into a RDD and count the number of rows in it.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00087.gif)![](img/00088.gif)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
- en: 'Next, remove the header and all other rows containing missing values, (represented
    as `?''s` in the input), as shown in the following steps:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00089.gif)![](img/00090.gif)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
- en: In the next step, convert the `RDD [String]` to a `RDD` with the `case` class,
    we defined earlier, and convert the RDD a DatFrame of `HouseholdEPC` objects.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00091.gif)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
- en: Display a few sample records in the DataFrame, and count the number of rows
    in it to verify that the number of rows in the DataFrame matches the expected
    number of rows in your input Dataset.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00092.gif)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
- en: Computing basic statistics and aggregations
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, compute and display some basic statistics for the numeric columns in the
    DataFrame to get a feel for the data, we will be working with.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00093.gif)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
- en: We can also display the basic statistics for some or all of the columns rounded
    to four decimal places. We can also rename each of the columns by prefixing a `r`
    to the column names to differentiate them from the original column names.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00094.gif)![](img/00095.gif)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
- en: 'In addition, we count the distinct number of days, for which the data is contained
    in the DataFrame using an aggregation function:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们使用聚合函数计算包含在DataFrame中的数据的不同日期的数量：
- en: '![](img/00096.gif)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00096.gif)'
- en: Augmenting the Dataset
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 增强数据集
- en: We can augment the DataFrame with new columns for the day of the week, day of
    the month, month and year information. For example, we may be interested in studying
    power consumption on weekdays versus weekends. This can help achieve a better
    understanding of the data through visualization or pivoting based on these fields.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以为星期几、每月的日期、月份和年份信息在DataFrame中增加新的列。例如，我们可能对工作日和周末的用电量感兴趣。这可以通过可视化或基于这些字段的数据透视来更好地理解数据。
- en: '![](img/00097.gif)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00097.gif)'
- en: Executing other miscellaneous processing steps
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行其他杂项处理步骤
- en: If required we can choose to execute a few more steps to help cleanse the data
    further, study more aggregations, or to convert to a typesafe data structure,
    and so on.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要，我们可以选择执行更多步骤来帮助进一步清洗数据，研究更多的聚合，或者转换为类型安全的数据结构等。
- en: We can drop the time column and aggregate the values in various columns using
    aggregation functions such as sum and average on the values of each day's readings.
    Here, we rename the columns with a `d` prefix to represent daily values.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以删除时间列，并使用聚合函数（如sum和average）对各列的值进行聚合，以获取每天读数的值。在这里，我们使用`d`前缀来重命名列，以表示每日值。
- en: '![](img/00098.gif)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00098.gif)'
- en: 'We display a few sample records from this DataFrame:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从这个DataFrame中显示一些样本记录：
- en: '[PRE0]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](img/00099.jpeg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00099.jpeg)'
- en: Here, we group the readings by year and month, and then count the number of
    readings and display them for each of the months. The first month's number of
    readings is low as the data was captured in half a month.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们按年份和月份对读数进行分组，然后计算每个月的读数数量并显示出来。第一个月的读数数量较低，因为数据是在半个月内捕获的。
- en: '![](img/00100.gif)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00100.gif)'
- en: 'We can also convert our DataFrame to a Dataset using a `case` class, as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用`case`类将DataFrame转换为数据集，如下所示：
- en: '![](img/00101.gif)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00101.gif)'
- en: At this stage, we have completed all the steps for pre-processing the household
    electric consumption Dataset. We now shift our focus to processing the weather
    Dataset.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们已经完成了预处理家庭电力消耗数据集的所有步骤。现在我们将把重点转移到处理天气数据集上。
- en: Pre-processing of the weather Dataset
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 天气数据集的预处理
- en: First, we define a `case` class for the weather readings.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们为天气读数定义一个`case`类。
- en: '![](img/00102.gif)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00102.gif)'
- en: Next, we read in the four files of the daily weather readings (downloaded from
    the Paris Weather website) approximately matching the same duration as the household
    electric power consumption readings.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们读取了四个文件的每日天气读数（从巴黎天气网站下载），大致与家庭电力消耗读数的持续时间相匹配。
- en: '![](img/00103.gif)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00103.gif)'
- en: 'Remove the headers from each of the input files shown as follows. We have shown
    the output of header values so you get a sense of the various weather reading
    parameters captured in these Datasets:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 从以下显示的每个输入文件中删除标题。我们已经显示了标题值的输出，以便您了解这些数据集中捕获的各种天气读数参数：
- en: '![](img/00104.gif)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00104.gif)'
- en: Analyzing missing data
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析缺失数据
- en: 'If we wanted to get a sense of the number of rows containing one or more missing
    fields in the RDD, we can create a RDD with these rows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要了解RDD中包含一个或多个缺失字段的行数，我们可以创建一个包含这些行的RDD：
- en: '![](img/00105.gif)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00105.gif)'
- en: 'We can also do the same, if our data was available in a DataFrame as shown:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的数据以DataFrame的形式可用，我们也可以做同样的操作，如下所示：
- en: '![](img/00106.gif)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00106.gif)'
- en: A quick check of the Dataset reveals that most of the rows with missing data
    also have missing values for the Events and Max Gust Speed Km/h columns. Filtering
    on these two column values actually, captures all the rows with missing field
    values. It also matches the results for missing values in the RDD.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 快速检查数据集发现，大多数具有缺失数据的行也在“事件”和“最大阵风速度公里/小时”列中具有缺失值。根据这两列的值进行过滤实际上捕获了所有具有缺失字段值的行。这也与RDD中的缺失值的结果相匹配。
- en: '![](img/00107.gif)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00107.gif)'
- en: As there are many rows that contain one or more missing fields, we choose to
    retain these rows to ensure we do not lose valuable information. In the following
    function, we insert `0` in all the missing fields of an RDD.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 由于有许多行包含一个或多个缺失字段，我们选择保留这些行，以确保不丢失宝贵的信息。在下面的函数中，我们在RDD的所有缺失字段中插入`0`。
- en: '![](img/00108.gif)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00108.gif)'
- en: 'We can replace `0` inserted in the previous step with an `NA` in the string
    fields, as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用字符串字段中的`0`替换前一步骤中插入的`NA`，如下所示：
- en: '![](img/00109.gif)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00109.gif)'
- en: At this stage, we can combine the rows of the four Datasets into a single Dataset
    using the `union` operation.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们可以使用`union`操作将四个数据集的行合并成一个数据集。
- en: '![](img/00110.gif)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00110.gif)'
- en: At this stage, the processing of our second Dataset containing weather data
    is complete. In the next section, we combine these pre-processed Datasets using
    a `join` operation.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们第二个包含天气数据的数据集的处理已经完成。在接下来的部分，我们将使用`join`操作来合并这些预处理的数据集。
- en: Combining data using a JOIN operation
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用JOIN操作合并数据
- en: In this section, we will introduce the JOIN operation, in which the daily household
    electric power consumption is combined with the weather data. We have assumed
    the locations of readings taken for the household electric power consumption and
    the weather readings are in close enough proximity to be relevant.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将介绍JOIN操作，其中每日家庭电力消耗与天气数据进行了合并。我们假设家庭电力消耗的读数位置和天气读数的位置足够接近，以至于相关。
- en: Next, we use the join operation to combine the daily household electric power
    consumption Dataset with the weather Dataset.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用JOIN操作将每日家庭电力消耗数据集与天气数据集进行合并。
- en: '![](img/00111.gif)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00111.gif)'
- en: 'Verify the number of rows in the final DataFrame obtained with the number of
    rows expected subsequent to the `join` operation shown as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 验证最终DataFrame中的行数是否与`join`操作后预期的行数相匹配，如下所示：
- en: '![](img/00112.gif)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
- en: 'You can compute a series of correlations between various columns in the newly
    joined Dataset containing columns from each of the two original Datasets to get
    a feel for the strength and direction of relationships between the columns, as
    follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00113.gif)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
- en: Similarly, you can join the Datasets grouped by year and month to get a higher-level
    summarization of the data.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00114.gif)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
- en: 'In order to visualize the summarized data, we can execute the preceding statements
    in an Apache Zeppelin notebook. For instance, we can plot the monthly **Global
    Reactive Power** (**GRP**) values by transforming `joinedMonthlyDF` into a table
    and then selecting the appropriate columns from it, as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00115.jpeg)![](img/00116.jpeg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, if you want to analyze readings by the day of the week then follow,
    the steps as shown:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00117.gif)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
- en: 'Finally, we print the schema of the joined Dataset (augmented with the day
    of the week column) so you can further explore the relationships between various
    fields of this DataFrame:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00118.gif)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
- en: In the next section, we shift our focus to munging textual data.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Munging textual data
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we explore data munging techniques for typical text analysis
    situations. Many text-based analyses tasks require computing word counts, removing
    stop words, stemming, and so on. In addition, we will also explore how you can
    process multiple files, one at a time, from HDFS directories.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import all the classes that will be used in this section:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00119.gif)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
- en: Processing multiple input data files
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the next few steps, we initialize a set of variables for defining the directory
    containing the input files, and an empty RDD. We also create a list of filenames
    from the input HDFS directory. In the following example, we will work with files
    contained in a single directory; however, the techniques can easily be extended
    across all 20 newsgroup sub-directories.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00120.gif)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
- en: 'Next, we write a function to compute the word counts for each file and collect
    the results in an `ArrayBuffer`:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00121.gif)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
- en: 'We have included a print statement to display the file names as they are picked
    up for processing, as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00122.gif)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
- en: 'We add the rows into a single RDD using the `union` operation:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00123.gif)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
- en: 'We could have directly executed the union step as each file is processed, as
    follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00124.gif)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
- en: However, using `RDD.union()` creates a new step in the lineage graph requiring
    an extra set of stack frames for each new RDD. This can easily lead to a Stack
    Overflow condition. Instead, we use `SparkContext.union()` which executes the
    `union` operation all at once without the extra memory overheads.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 'We can cache and print sample rows from our output RDD as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00125.gif)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
- en: In the next section, we show you ways of filtering out stop words. For simplicity,
    we focus only on well-formed words in the text. However, you can easily add conditions
    to filter out special characters and other anomalies in our data using String
    functions and regexes (for a detailed example, refer [Chapter 9](part0166.html#4U9TC0-e9cbc07f866e437b8aa14e841622275c), *Developing
    Applications with Spark SQL)*.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Removing stop words
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our example, we create a set of stop words and filter them out from the words
    in contained in each file. Normally, a Spark operation executing on a remote node
    works on a separate copy of the variables used in the function. We can use a broadcast
    variable to maintain a read-only, cached copy of the set of stop words at each
    node in the cluster instead of shipping a copy of it with the tasks to be executed
    on the nodes. Spark attempts to distribute the broadcast variables efficiently
    to reduce the overall communication overheads. Further more, we also filter out
    empty lists returned by the function as a result of our filtering process and
    stop words removal.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00126.gif)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
- en: 'We can extract the words from each of the tuples in the RDD and create a DataFrame
    containing them, as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00127.gif)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
- en: In the following example, we show another method for filtering out the stop
    words from our list of words. In order to improve the word matches between the
    two lists, we process the stop words file in a similar manner to the words extracted
    from the input files. We read the file containing stop words, remove leading and
    trailing spaces, convert to lower case, replace special characters, filter out
    empty words, and, finally, create a DataFrame (containing the stop words).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: We use the list of stop words available at [http://algs4.cs.princeton.edu/35applications/stopwords.txt](http://algs4.cs.princeton.edu/35applications/stopwords.txt)
    in our example.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00128.gif)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
- en: Here, we use a `regex` to filter out special characters contained in the file.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00129.gif)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
- en: Next, we compare the number of words in our list before and after the removal
    of the stop words from our original list of words. The final number of words remaining
    suggests that a majority of words in our input files were stop words.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00130.gif)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
- en: For a more detailed coverage of text data processing (of an annual `10-K` financial
    filing document and other document corpuses) including building pre-processing
    data pipelines, identifying themes in document corpuses, using Naïve Bayes classifiers,
    and developing a machine learning application, refer [Chapter 9](part0166.html#4U9TC0-e9cbc07f866e437b8aa14e841622275c), *Developing
    Applications with Spark SQL.*
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we shift our focus to munging time-series data using the
    `spark-time-series` library from Cloudera.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Munging time series data
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Time series data is a sequence of values linked to a timestamp. In this section,
    we use Cloudera's `spark-ts` package for analyzing time-series data.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Refer to *Cloudera Engineering Blog*, *A New Library for Analyzing Time-Series
    Data with Apache Spark*, for more details on time-series data and its processing
    using `spark-ts`. This blog is available at: [https://github.com/sryza/spark-timeseries](https://github.com/sryza/spark-timeseries).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: 'The `spark-ts` package can be downloaded and built using instructions available
    at:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/sryza/spark-timeseries](https://github.com/sryza/spark-timeseries).'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 'We will attempt to accomplish the following objectives in the following sub-sections:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Pre-processing of the time-series Dataset
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing date fields
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Persisting and loading data
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining a date-time index
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the  `TimeSeriesRDD` object
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling missing time-series data
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing basic statistics
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For this section, specify inclusion of the `spark-ts.jar` file while starting
    the Spark shell as shown:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00131.gif)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
- en: We download Datasets containing pricing and volume data for six stocks over
    a one year period from the Yahoo Finance site. We will need to pre-process the
    data before we can use the `spark-ts` package for time-series data analysis.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Import the classes required in this section.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00132.gif)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
- en: Pre-processing of the time-series Dataset
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Read the data from the input data files and define a `case` class Stock containing
    the fields in the Dataset plus a field to hold the ticker symbol.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00133.gif)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
- en: Next, we remove the header from each of the files, map our RDD row using the
    `case` class, include a string for the ticker symbol, and convert the RDD to a
    DataFrame.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00134.gif)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
- en: Next, we combine the rows from each of our DataFrames using `union`.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00135.gif)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
- en: Processing date fields
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the next step, we separate out the date column into three separate fields
    containing the day, month, and the year information.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00136.gif)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
- en: Persisting and loading data
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At this stage, we can persist our DataFrame to a CSV file using the `DataFrameWriter`
    class. The overwrite mode lets you overwrite the file, if it is already present
    from a previous execution of the `write` operation:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00137.gif)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
- en: 'For loading the time series Dataset written to disk in the previous step, we
    define a function to load our observations from a file and return a DataFrame:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加载上一步写入磁盘的时间序列数据集，我们定义一个从文件加载观测值并返回DataFrame的函数：
- en: '![](img/00138.gif)![](img/00139.gif)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00138.gif)![](img/00139.gif)'
- en: Defining a date-time index
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义日期时间索引
- en: We define a date-time index for the period for which we have the data so that
    each record (for a specific ticker symbol) includes a time series represented
    as an array of `366` positions for each of the days in the year (plus one extra
    day as we have downloaded the data from 12/04/2015 to 12/04/2016). The Business
    Day Frequency specifies that the data is available for the business days of the
    year only.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为我们拥有数据的期间定义一个日期时间索引，以便每条记录（针对特定的股票代码）包括一个时间序列，表示为一年中每一天的`366`个位置的数组（加上额外的一天，因为我们已经从2015年12月4日下载了数据到2016年12月4日）。工作日频率指定数据仅适用于一年中的工作日。
- en: '![](img/00140.gif)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00140.gif)'
- en: Using the  TimeSeriesRDD object
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用`TimeSeriesRDD`对象
- en: The main abstraction in the `spark-ts` library is a RDD called `TimeSeriesRDD`.
    The data is a set of observations represented as a tuple of (timestamp, key, value).
    The key is a label used to identify the time series. In the following example,
    our tuple is (timestamp, ticker, close). Each series in the RDD has the ticker
    symbol as the key and the daily closing price of the stock as the value.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '`spark-ts`库中的主要抽象是称为`TimeSeriesRDD`的RDD。数据是一组以元组（时间戳、键、值）表示的观测值。键是用于标识时间序列的标签。在下面的示例中，我们的元组是（时间戳、股票代码、收盘价）。RDD中的每个系列都将股票代码作为键，将股票的每日收盘价作为值。'
- en: '[PRE1]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can cache and display the number of rows in the RDD which should be equal
    to the number of stocks in our example:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以缓存并显示RDD中的行数，这应该等于我们示例中的股票数量：
- en: '![](img/00141.gif)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00141.gif)'
- en: 'Display a couple of rows from the RDD to see the data in each row:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 显示RDD中的几行以查看每行中的数据：
- en: '![](img/00142.gif)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00142.gif)'
- en: Handling missing time-series data
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理缺失的时间序列数据
- en: Next, we check the RDD for missing data. The missing data is marked with `NaN`
    values. Computing basic statistics with `NaN` values present will give errors.
    Hence, we need to replace these missing values with approximations. Our example
    data does not contain any missing fields. However, as an exercise, we delete a
    few values from the input datasets to simulate these `NaN` values in the RDD,
    and then impute these values using linear interpolation. Other approximations
    available include next, previous, and nearest values.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们检查RDD中是否有缺失数据。缺失数据标记为`NaN`值。在存在`NaN`值的情况下计算基本统计数据会导致错误。因此，我们需要用近似值替换这些缺失值。我们的示例数据不包含任何缺失字段。但是，作为练习，我们从输入数据集中删除一些值，以模拟RDD中的这些`NaN`值，然后使用线性插值来填补这些值。其他可用的近似值包括下一个、上一个和最近的值。
- en: 'We fill in the approximate values for the missing values, as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们填写缺失值的近似值，如下所示：
- en: '![](img/00143.gif)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00143.gif)'
- en: Computing basic statistics
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算基本统计数据
- en: 'Finally, we compute the mean, standard deviation, max and min values for each
    of our series, as follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们计算每个系列的均值、标准差、最大值和最小值，如下所示：
- en: '![](img/00144.gif)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00144.gif)'
- en: There are many other useful functions available for exploratory data analysis
    and data munging using the `TimeSeriesRDD` object. These include collecting the
    RDD as a local time series, finding specific time series, various filters and
    slicing functionality, sorting and re-partitioning the data, writing out the time
    series to CSV files, and many more.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`TimeSeriesRDD`对象进行探索性数据分析和数据整理还有许多其他有用的函数。这些包括将RDD收集为本地时间序列、查找特定时间序列、各种过滤和切片功能、对数据进行排序和重新分区、将时间序列写入CSV文件等等。
- en: Dealing with variable length records
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理可变长度记录
- en: In this section, we will explore a way of dealing with variable length records.
    Our approach essentially converts each of the rows to a fixed length record equal
    to the maximum length record. In our example, as each row represents a portfolio
    and there is no unique identifier, this method is useful for manipulating data
    into the familiar fixed length records case. We will generate the requisite number
    of fields to equal the maximum number of stocks in the largest portfolio. This
    will lead to empty fields where the number of stocks is less than the maximum
    number of stocks in any portfolio. Another way to deal with variable length records
    is to use the `explode()` function to create new rows for each stock in a given
    portfolio (for an example of using the `explode()` function, refer [Chapter 9](part0166.html#4U9TC0-e9cbc07f866e437b8aa14e841622275c), *Developing
    Applications with Spark SQL).*
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将探讨处理可变长度记录的方法。我们的方法基本上将每一行转换为等于最大长度记录的固定长度记录。在我们的例子中，由于每行代表一个投资组合并且没有唯一标识符，这种方法对将数据转换为熟悉的固定长度记录情况非常有用。我们将生成所需数量的字段，使其等于最大投资组合中的股票数量。这将导致在股票数量少于任何投资组合中的最大股票数量时出现空字段。处理可变长度记录的另一种方法是使用`explode()`函数为给定投资组合中的每支股票创建新行（有关使用`explode()`函数的示例，请参阅[第9章](part0166.html#4U9TC0-e9cbc07f866e437b8aa14e841622275c)，*使用Spark
    SQL开发应用程序)。*
- en: To avoid repeating all the steps from previous examples to read in all the files,
    we have combined the data into a single input file in this example.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免重复之前示例中的所有步骤来读取所有文件，我们在本例中将数据合并为一个单独的输入文件。
- en: 'First, we import the classes required and read the input file into an RDD:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入所需的类并将输入文件读入RDD：
- en: '![](img/00145.gif)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00145.gif)'
- en: We count the total number of portfolios and print a few records from the RDD.
    You can see that while the first and the second portfolios contain one stock each,
    the third one contains two stocks.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算投资组合的总数，并打印RDD中的一些记录。您可以看到，第一个和第二个投资组合各包含一支股票，而第三个投资组合包含两支股票。
- en: '![](img/00146.gif)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00146.gif)'
- en: Converting variable-length records to fixed-length records
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将可变长度记录转换为固定长度记录
- en: In our example Dataset, there are no fields missing, hence, we can use the number
    of commas in each row to derive the varying number stock-related fields in each
    of the portfolios. Alternatively, this information can be extracted from the strings
    contained in last field of the RDD.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Next, we create a UDF to count the number of stocks indirectly by counting the
    number of commas in each row. We use `describe` to find the maximum number of
    commas across all rows in the dataset.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00147.gif)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
- en: In the next step, we augment the DataFrame with a column containing the number
    of commas.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00148.gif)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
- en: 'Then we write a function to insert the correct number of commas in each row
    at the appropriate location:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00149.gif)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
- en: 'Next, we drop the number of commas column, as it is not required in the subsequent
    steps:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00150.gif)![](img/00151.gif)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
- en: 'At this stage, if you want to get rid of duplicate rows in the DataFrame, then
    you can use the `dropDuplicates` method shown as follows:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00152.gif)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
- en: In the next step, we define a `case` class for the `Portfolio` with the maximum
    number of stocks in the largest portfolio.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00153.gif)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
- en: 'Next, we convert the RDD into a DataFrame. For convenience, we will demonstrate
    the operations using fewer stock-related columns; however, the same can be extended
    to fields for other stocks in the portfolio:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00154.gif)![](img/00155.gif)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
- en: 'We can replace empty fields for stocks in the smaller portfolios with `NA` , as
    follows:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00156.gif)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
- en: Extracting data from "messy" columns
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we continue on from the previous section, however, we will
    work with a single stock to demonstrate the data manipulations required to modify
    the data fields to a state where we end up with cleaner and richer data than we
    started with.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 'As most of the fields contain several pieces of information, we will execute
    a series of statements to separate them out into their own independent columns:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00157.gif)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
- en: 'In the next step, we remove the first underscore with a space in the `datestr`
    column. This results in separating out the date field:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00158.gif)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
- en: 'Next, we separate out the information in the stock column, as it contains several
    pieces of useful information including the ticker symbol, ratio of the selling
    price and purchase price, and the selling price and purchase price. First, we
    get rid of the `=` in the stock column by replacing it with an empty string:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00159.gif)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
- en: 'Next, the values in each column separated by spaces in each column are converted
    into an array of the values:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00160.gif)![](img/00161.gif)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
- en: Next, we use a `UDF` to pick certain elements from the arrays in each column
    into their own separate columns.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00162.gif)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
- en: 'The file column is not particularly useful for our analysis, except for extracting
    the information at the beginning of the filename that denotes the pool of stocks
    from which the stocks for any given portfolio were picked. We do that next, as
    follows:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00163.gif)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
- en: The following is the final version of the DataFrame that is ready for further
    analysis. In this example, we have worked with a single stock however you can
    easily extend the same techniques to all stocks in any given portfolio to arrive
    at the final, clean and rich, DataFrame ready for querying, modeling, and analysis.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00164.gif)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
- en: In the next section, we briefly introduce the steps required for preparing data
    for use with Spark MLlib machine learning algorithms for classification problems.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Preparing data for machine learning
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we introduce the process of preparing the input data prior
    to applying Spark MLlib algorithms. Typically, we need to have two columns called
    label and features for using Spark MLlib classification algorithms. We will illustrate
    this with the following example described:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 'We import the required classes for this section:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Pre-processing data for machine learning
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We define a set of `UDFs` used in this section. These include, for example,
    checking whether a string contains a specific substring or not, and returning
    a `0.0` or `1.0` value to create the label column. Another `UDF` is used to create
    a features vector from the numeric fields in the DataFrame.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we can convert the day of week field to a numeric value by binning
    shown as follows:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00165.gif)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
- en: In our example, we create a `label` from the `Events` column of the household
    electric consumption Dataset based on whether it rained on given a day or not.
    For illustrative purposes, we use the columns from the household's electric power
    consumption readings in the joined DataFrame from before, even though readings
    from weather Dataset are probably a better predictor of rain.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00166.gif)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
- en: Finally, we can also split our DataFrame to create training and test Datasets
    containing 70% and 30% of the readings, chosen randomly, respectively. These Datasets
    are used for training and testing machine learning algorithms.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00167.gif)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
- en: Creating and running a machine learning pipeline
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we present an example of a machine learning pipeline that uses
    the indexers and the training data to train a Random Forest model. We will not
    present detailed explanations for the steps, as our primary purpose here is to
    only demonstrate how the preparatory steps in the previous section are actually
    used.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](img/00168.jpeg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
- en: '[PRE4]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](img/00169.jpeg)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
- en: More details on specific data structures and operations including vectors, processing
    categorical variables, and so on, for Spark MLlib processing, are covered in [Chapter
    6](part0103.html#3279U0-e9cbc07f866e437b8aa14e841622275c), **Using Spark SQL in
    Machine Learning Applications,** and [Chapter 9](part0166.html#4U9TC0-e9cbc07f866e437b8aa14e841622275c),
    *Developing Applications with Spark SQL*. Additionally, techniques for preparing
    data for graph applications are presented in [Chapter 7](part0134.html#3VPBC0-e9cbc07f866e437b8aa14e841622275c),
    **Using Spark SQL in Graph Applications**.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored using Spark SQL for performing some basic data
    munging/wrangling tasks. We covered munging textual data, working with variable
    length records, extracting data from "messy" columns, combining data using JOIN,
    and preparing data for machine learning applications. In addition, we used `spark-ts`
    library to work with time-series data.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will shift our focus to Spark Streaming applications.
    We will introduce you to using Spark SQL in such applications. We will also include
    extensive hands-on sessions for demonstrating the use of Spark SQL in implementing
    the common use cases in Spark Streaming applications.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
