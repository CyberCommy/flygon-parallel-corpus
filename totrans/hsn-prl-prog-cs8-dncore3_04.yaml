- en: Implementing Data Parallelism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have learned about the basics of parallel programming, tasks, and
    task parallelism. In this chapter, we will cover another important aspect of parallel
    programming, which deals with the parallel execution of data: data parallelism.
    While task parallelism creates a separate unit of work for each participating
    thread, data parallelism creates a common task that is executed by every participating
    thread in a source collection. This source collection is partitioned so that multiple
    threads can work on it concurrently. Therefore, it is important to understand
    data parallelism to get the maximum performance out of loops/collections.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will discuss the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Handling exceptions in parallel loops
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating custom partitioning strategies in parallel loops
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Canceling loops
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding thread storage in parallel loops
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To complete this chapter, you should have a good understanding of the TPL and
    C#. The source code for this chapter is available on GitHub at [https://github.com/PacktPublishing/Hands-On-Parallel-Programming-with-C-8-and-.NET-Core-3/tree/master/Chapter03](https://github.com/PacktPublishing/Hands-On-Parallel-Programming-with-C-8-and-.NET-Core-3/tree/master/Chapter03).
  prefs: []
  type: TYPE_NORMAL
- en: Moving from sequential loops to parallel loops
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The TPL supports data parallelism through the `System.Threading.Tasks.Parallel`
    class, which provides parallel implementation of the `For` and `Foreach` loops.
    As a developer, you don't need to worry about synchronization or creating tasks
    as this is handled by the parallel class. This syntactic sugar allows you to easily
    write parallel loops in a way that's similar to how you have been writing sequential
    loops.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of a sequential `for` loop that books a trade by posting
    the trade object to the server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Since the loop is sequential, the total time that it takes to finish the loop
    is the time it takes to book one trade multiplied by the total number of trades.
    This means that the loop slows down as the number of trades increases, although
    the trade booking time remains the same. Here, we are dealing with large numbers.
    Since we are going to be booking trades on a server and all the servers support
    multiple requests, it makes sense to convert this loop from a sequential loop
    into a parallel loop as that will give us significant performance gains.
  prefs: []
  type: TYPE_NORMAL
- en: 'The previous code can be converted so that it''s parallel as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: While running a parallel loop, the TPL partitions the source collection so that
    the loop can execute on multiple parts concurrently. The partitioning of tasks
    is done by the `TaskScheduler` class, which takes the system resources and the
    load into consideration while creating partitions. We can also create a **custom
    partitioner** or **scheduler**, as we will see later in this chapter in the *Creating
    a custom partitioning strategy* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data parallelism performs better if the partitioning units are independent.
    With minimal performance overhead, we can also create dependency partitioning
    units using a technique called reduction, which reduces a series of operations
    to a scalar value. There are three ways to convert sequential code into parallel
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: Using the `Parallel.Invoke` method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the `Parallel.For` method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the `Parallel.ForEach` method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s try to understand the various ways in which the `Parallel` class can be
    utilized to exhibit data parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Parallel.Invoke method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is the most basic way of executing a set of operations in parallel and
    forms the basis for parallel `for` and `foreach` loops. The `Parallel.Invoke`
    method accepts an array of actions as a parameter and executes them, though it
    never guarantees that the actions will be executed in parallel. There are some
    important points to remember when using `Parallel.Invoke`:'
  prefs: []
  type: TYPE_NORMAL
- en: Parallelism is not guaranteed. Whether the actions are executed in parallel
    or in sequence will depend on the `TaskScheduler`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Parallel.Invoke` doesn''t guarantee the order of operations for passed actions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It blocks the calling thread until all the actions are completed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The syntax of `Parallel.Invoke` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can either pass an action or a lambda expression, as demonstrated in the
    following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Invoke` method behaves like an attached child task as it is blocked until
    all the actions are completed. All the exceptions are stacked together inside
    `System.AggregateException` and thrown to the caller. In the preceding code, since
    there is no exception, we will see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/66a69638-847b-4d6a-b800-164e267cc832.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can achieve a similar effect using the `Task` class, although this may look
    like very complex code in comparison to how `Parallel.Invoke` works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `Invoke` method behaves like an attached child task as it is blocked until
    all the actions are completed. All the exceptions are stacked together inside
    `System.AggregateException` and thrown to the caller.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Parallel.For method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`Parallel.For` is a variant of the sequential `for` loop, with the difference
    that the iterations run in parallel. `Parallel.For` returns an instance of the `ParallelLoopResult`
    class, which provides the loop competition status once the loop has finished execution.
    We can also check the `IsCompleted` and `LowestBreakIteration` properties of `ParallelLoopResult`
    to find out if the method has completed or canceled, or if break has been called
    by the users. Here are the possible scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: '| `IsCompleted` | `LowestBreakIteration` | **Reason** |'
  prefs: []
  type: TYPE_TB
- en: '| True | N/A | Run to completion |'
  prefs: []
  type: TYPE_TB
- en: '| False | Null | Loop stopped pre-matching |'
  prefs: []
  type: TYPE_TB
- en: '| False | Non-null integral value | Break called on the loop |'
  prefs: []
  type: TYPE_TB
- en: 'The basic syntax of the `Parallel.For` method is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'An example of this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This approach can be useful if you don''t want to cancel, break, or maintain
    any thread local state and the order of execution is not important. For example,
    imagine that we want to count the number of files in a directory that have been
    created today. The code for this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This code iterates all the files in the `C:` drive and counts all the files
    that were created today. The following is the output on my machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/92b7e1f0-42cb-4454-a9c3-934b0957916c.png)'
  prefs: []
  type: TYPE_IMG
- en: In the next section, we will try to understand the `Parallel.ForEach` method,
    which provides a parallel variant of the `ForEach` loop.
  prefs: []
  type: TYPE_NORMAL
- en: For some collections, sequential executions work faster, depending on the syntax
    of the loop and the type of work that's being done.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Parallel.ForEach method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is a variation of the `ForEach` loop wherein iterations may run in parallel.
    The source collection is partitioned and then the work is scheduled to run multiple
    threads. `Parallel.ForEach` works on generic collections and, just like the `for`
    loop, returns `ParallelLoopResult`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic syntax of the `Parallel.ForEach` loop is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'An example of this is as follows. We have a list of ports that we need to monitor.
    We also need to update their statuses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we used the `System.Net.NetworkInformation.Ping` class
    to ping a part and display a status to the console. Since the parts are independent,
    we can achieve great performance if the code is made parallel and the order is
    also not important.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2c68ea2f-feb9-4743-b703-c664e1ecca89.png)'
  prefs: []
  type: TYPE_IMG
- en: Parallelism can make applications slow on single-core processors. We can control
    how many cores can be utilized in a parallel operation by using the degree of
    parallelism, which we will cover next.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the degree of parallelism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have learned how data parallelism gives us the advantage of running
    loops in parallel on multiple cores of a system, thereby making efficient use
    of the available CPU resources. You should be aware that there is another important
    concept that you can use in order to control how many tasks you want to create
    in your loops. This concept is called the degree of parallelism. It''s a number
    that specifies the maximum number of tasks that can be created by your parallel
    loops. You can set the degree of parallelism via a property called `MaxDegreeOfParallelism`,
    which is part of the `ParallelOptions` class. The following is the syntax of `Parallel.For`,
    wherein you can pass the `ParallelOptions` instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the syntax of the `Parallel.For` and `Parallel.ForEach` methods,
    wherein you can pass the `ParallelOptions` instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The default value for the degree of parallelism is 64, which means that the
    parallel loops can utilize up to 64 processors in a system by creating that many
    tasks. We can modify this value to limit the number of tasks. Let's try to understand
    this concept with a few examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at an example of a `Parallel.For` loop with `MaxDegreeOfParallelism`
    set to `4`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/474f3ac2-6330-44fa-b6e1-4560c509c1de.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the loop was executed by four tasks denoted by the task IDs
    1, 2, 3, and 4.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of a `Parallel.ForEach` loop with `MaxDegreeOfParallelism`
    set to `4`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8dcc5f47-7a0e-481d-b2f2-c6247b60d8ba.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, this loop was executed by four tasks denoted by the task IDs
    1, 2, 3, and 4.
  prefs: []
  type: TYPE_NORMAL
- en: We should modify this setting for advanced scenarios where we are aware that
    a running algorithm cannot span more than a certain number of processors. We should
    also modify this setting if we are running multiple algorithms in parallel and
    we want to restrict each algorithm to only utilize a certain number of processors.
    Next, we will learn how to make custom partitions in collections by introducing
    the concept of partitioning strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a custom partitioning strategy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Partitioning is another important concept in data parallelism. To achieve parallelism
    in the source collection, it needs to be partitioned into smaller sections called
    ranges or chunks, which can be concurrently accessed by various threads. Without
    partitioning, the loop will execute serially. Partitioners can be classified into
    two categories and we can create custom partitioners as well. These categories
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Range partitioning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chunk partitioning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's discuss these in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Range partitioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This type of partitioning is primarily used with collections where the length
    is known in advance. As the name suggests, every thread gets a range of elements
    to process or the start and end index of a source collection. This is the simplest
    form of partitioning and very efficient in the sense that every thread executes
    its range without overwriting other threads. There is no synchronization overhead,
    though some bits of performance are lost initially while creating ranges. This
    type of partitioning works best in scenarios where the number of elements in each
    range is the same so that they will take a similar length of time to finish. With
    a different number of elements, some tasks may finish early and sit idle, whereas
    other tasks may have a lot of pending elements in the range to process.
  prefs: []
  type: TYPE_NORMAL
- en: Chunk partitioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This type of partitioning is primarily used with collections such as `LinkedList`,
    where the length isn't known in advance. Chunk partitioning provides more load
    balancing in case you have uneven collections. Every thread picks up a chunk of
    elements, processes them, and then comes back to pick up another chunk that hasn't
    been picked up by other threads yet. The size of the chunk depends on the partitioner's
    implementation and there is synchronization overhead to make sure that the chunks
    that are allocated to two threads don't contain duplicates.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can change the default partitioning strategy of the `Parallel.ForEach` loop
    to perform custom chunk partitioning, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we created chunked partitioners using the `OrderablePartitioner`
    class on a range of items (here, from `1` to `100`). We passed partitioners to
    the `ForEach` loop, where each chunk is passed to a thread and executed. The output
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/966e964a-5049-40d9-8b2e-8721b689bf52.png)'
  prefs: []
  type: TYPE_IMG
- en: So far, we have a good understanding of how parallel loops work. Now, we need
    to discuss some advanced concepts in order to find out more about how we can control
    loop execution; that is, how to stop a loop as needed.
  prefs: []
  type: TYPE_NORMAL
- en: Canceling loops
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have used constructs such as `break` and `continue` in sequential loops;
    `break` is used to break out of a loop by finishing the current iteration and
    skipping the rest, whereas `continue` skips the current iteration and moves to
    the rest of the iterations. These constructs can be used because the sequential
    loops are executed by a single thread. In the case of parallel loops, we cannot
    use the `break` and `continue` keywords since they run on multiple threads or
    tasks. To break a parallel loop, we need to make use of the `ParallelLoopState`
    class. To cancel a loop, we need to make use of the `CancellationToken` and `ParallelOptions`
    classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will discuss the options that you require to cancel loops:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Parallel.Break`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ParallelLoopState.Stop`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CancellationToken`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Using the Parallel.Break method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`Parallel.Break` tries to mimic the results of a sequential execution. Let''s
    have a look at how to `break` from a parallel loop. In the following code, we need
    to search a list of numbers for a specific number. We need to break the loop''s
    execution when a match is found:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown in the preceding code, the loop is supposed to run until the number
    `2` is found. With a sequential loop, it will break exactly on the second iteration.
    With parallel loops, since iterations run on multiple tasks, it will actually
    print values more than 2, as shown in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/91ed685a-4c48-476c-a30b-ec96de9ba817.png)'
  prefs: []
  type: TYPE_IMG
- en: To break out of the loop, we called `parallelLoopState.Break()`, which tries
    to mimic the behavior of the actual `break` keyword in a sequential loop. When
    the `Break()` method is encountered by any of the cores, it will set an iteration
    number in the `LowestBreakIteration` property of the **`ParallelLoopState`** object.
    This becomes the maximum number or the last iteration that can be executed. All
    the other tasks will continue iterating until this number is reached.
  prefs: []
  type: TYPE_NORMAL
- en: 'Subsequent calls to the `Break` method by running iterations in parallel further
    reduces `LowestBreakIteration`, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run the preceding code in Visual Studio, we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e418c044-fbe9-43ae-ac1a-6fff50149ec2.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we run the code on a multi-core processor. As you can see, a lot of iterations
    get a null value for `LowestBreakIteration` as the code is being executed on multiple
    cores. On iteration 17, one core calls the `Break()` method and sets the value
    of `LowestBreakIteration` to 17\. On iteration 10, another core calls `Break()`
    and further reduces the number to 10\. Later, on iteration 9, another core calls
    `Break()`, and further reduces the number to 9.
  prefs: []
  type: TYPE_NORMAL
- en: Using ParallelLoopState.Stop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you don''t want to mimic the results of sequential loops and want to exit
    the loop as soon as possible, you can call `ParallelLoopState.Stop`. Just like
    we did with the `Break()` method, all the iterations running in parallel finish
    before the loop exits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output when we run the preceding code in Visual Studio:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1b495524-49e8-4c5e-8298-8080775a6477.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, one core called `Stop` on iteration 4, another core called `Stop`
    on iteration 8, and a third core called `Stop` on iteration 12\. Iterations 3
    and 10 still execute since they were already scheduled for execution.
  prefs: []
  type: TYPE_NORMAL
- en: Using CancellationToken to cancel loops
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Just like normal tasks, we can use the `CancellationToken` class to cancel the
    `Parallel.For` and `Parallel.ForEach` loops. When we cancel the token, the loop
    will finish the current iterations that may be running in parallel but will not
    start new iterations. Once the existing iterations finish, the parallel loops
    throw `OperationCanceledException`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at this with an example. First, we''ll create a cancellation token
    source:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we''ll create a task that cancels the token after five seconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, we''ll create a parallel options object by passing the cancellation
    token:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll run the loop with an operation that will last for more than five
    seconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output when we run the preceding code in Visual Studio:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4ee931bb-5b0f-4c54-9af4-599b99d77aec.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the scheduled iterations are still executed, even after the
    canceling token has been called. I hope this gives you a good idea of how we can
    cancel loops based on program requirements. Another important aspect of parallel
    programming is the concept of storage. We'll discuss this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding thread storage in parallel loops
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By default, all parallel loops have access to a global variable. However, there
    is a synchronization overhead associated with accessing global variables, and
    because of this, it makes sense to use thread-scoped variables wherever possible.
    We can create either a **thread local** or a **partition local** variable to be
    used in parallel loops.
  prefs: []
  type: TYPE_NORMAL
- en: Thread local variable
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thread local variables are like global variables for a particular task. They
    have a lifetime that spans the number of iterations the loop is going to execute.
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, we are going to look at thread local variables using
    the `for` loop. In the case of the `Parallel.For` loop, multiple tasks are created
    to run the iterations. Let's say we need to find out the sum of 60 numbers via
    a parallel loop.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, say there are four tasks, each of which has 15 iterations. One
    way of achieving this is to create a global variable. After every iteration, the
    running task should update the global variable. This would require synchronization
    overhead. For four tasks, there would be four thread local variables that are
    private to each task. The variable will be updated by the task and the last updated
    value can be returned to the caller program, which can then be used to update
    the global variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the steps to be followed:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a collection of 60 numbers, with each item having a value equal to the
    index:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a finished action that will execute once the task has finished all its
    allocated iterations. The method will receive the final result of the thread local
    variable and add that to the global variable, that is, `sumOfNumbers`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Create a `For` loop. The first two parameters are `startIndex` and `endIndex`.
    The third parameter is a delegate that provides a seed value for the thread local
    variable. It is an action that needs to be performed by the task. In our case,
    we are just assigning the index to `subtotal`, which is our thread local variable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s say there is a task, *TaskA*, which gets the iterations with an index
    from 1 to 5\. *TaskA* will add up these iterations as  1+2+3+4+5\. This equals
    15, which will be returned as the task''s result and passed to `taskFinishedMethod`
    as a parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output when we run the preceding code in Visual Studio:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bb6fdaca-40ff-4b89-864b-d7d9465f0e1d.png)'
  prefs: []
  type: TYPE_IMG
- en: Remember that the output may be different on different machines, depending on
    the number of available cores.
  prefs: []
  type: TYPE_NORMAL
- en: Partition local variable
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is similar to the thread local variable but works with partitions. As you
    are aware, the `ForEach` loop divides the source collection into a number of partitions.
    Each partition will have its own copy of the partition local variable. With the
    thread local variable, there is a single copy of the variable per thread. Here,
    however, we can have multiple copies per thread since multiple partitions can
    be run on a single thread.
  prefs: []
  type: TYPE_NORMAL
- en: First, we need to create a `ForEach` loop. The first parameter is a source collection,
    which means numbers. The second parameter is a delegate that provides a seed value
    for the thread local variable. The third parameter is an action that needs to
    be performed by the task. In our case, we are just assigning the index to the
    `subtotal`, which is our thread local variable.
  prefs: []
  type: TYPE_NORMAL
- en: For the sake of understanding, let's say there is a task, *TaskA*, that gets
    iterations with indexes from 1 to 5\. *TaskA* will add up these iterations, which
    is 1+2+3+4+5\. This equals 15, which will be returned as the task's result and
    passed to `taskFinishedMethod` as a parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Again, in this case, the output will be different on different machines, depending
    on the number of available cores.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we elaborated on achieving task parallelism using TPL. We started
    by introducing how to move sequential loops to parallel using some built-in methods
    provided by TPL, such as `Parallel.Invoke`, `Parallel.For`, and `Parallel.ForEach`.
    Next, we discussed how to get maximum utilization out of the available CPU resources
    by understanding the degree of parallelism and partitioning strategies. Then,
    we discussed how to cancel and break out of parallel loops using built-in constructs
    such as cancellation tokens, `Parallel.Break`, and `ParallelLoopState.Stop`. At
    the end of this chapter, we discussed various thread storage options that are
    available in TPL.
  prefs: []
  type: TYPE_NORMAL
- en: The TPL provides a few very exciting options that we can use to achieve data
    parallelism through the parallel implementation of `For` and `ForEach` loops.
    Along with features such as `ParallelOptions` and `ParallelLoopState`, we can
    achieve significant performance benefits and control without losing a lot of synchronization
    overhead.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at another exciting feature of the parallel
    library called **PLINQ**.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Which of these is not the correct method to provide a `for` loop in TPL?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Parallel.Invoke`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Parallel.While`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Parallel.For`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Parallel.ForEach`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which is not a default partitioning strategy?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bulk partitioning
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Range partitioning
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chunk partitioning
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the default value for the degree of parallelism?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '1'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '64'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Parallel.Break` guarantees immediate returns as soon as it is executed.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'False'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can one thread see another thread's thread local or partition local value?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Yes'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'No'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
