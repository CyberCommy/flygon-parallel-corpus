- en: Chapter 7. If Ain't Tested, It Ain't Game, Bro!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Does the software you write have quality? How do you attest that?
  prefs: []
  type: TYPE_NORMAL
- en: Software is usually written according to certain requested needs, be it bug
    reports, feature and enhancement tickets, or whatever. To have quality, the software
    must satisfy these needs wholly and precisely; that is, it should do what is expected
    of it.
  prefs: []
  type: TYPE_NORMAL
- en: Just as you would push a button to know what it does (given you do not have
    a manual), you have to test your code to know what it does or to attest what it
    should do. That's how you assure **software quality**.
  prefs: []
  type: TYPE_NORMAL
- en: During the course of a software development, it is usual to have many features
    that share some code base or library. You could, for example, change a piece of
    code to fix a bug and create another bug in another point in your code. Software
    tests also help with that as they assure that your code does what it should do;
    if you change a piece of broken code and break another piece of code, you'll also
    be breaking a test. In this scenario, if you make use of **continuous integration**,
    the broken code will never reach your production environment.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Don't know what continuous integration is? Refer to [http://www.martinfowler.com/articles/continuousIntegration.html](http://www.martinfowler.com/articles/continuousIntegration.html)
    and [https://jenkins-ci.org/](https://jenkins-ci.org/).
  prefs: []
  type: TYPE_NORMAL
- en: Tests are so important that there is a software development process called **Test
    Driven Development** (**TDD**), which states that the test should be written before
    the actual code, and that the actual code is only *ready* when the test itself
    is satisfied. TDD is quite common among senior developers and beyond. Just for
    the fun of it, we'll be using TDD in this chapter, from top to toe.
  prefs: []
  type: TYPE_NORMAL
- en: What kinds of test are there?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We want tests, and we want them now; but what kind of test do we want?
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two major classifications for tests, based on how much access to
    the internal code you have: **black-box** and **white-box** tests.'
  prefs: []
  type: TYPE_NORMAL
- en: Black-box tests are where the testers do not have knowledge of, and/or access
    to, the actual code he/she is testing. In these cases, the test consists of checking
    whether the system states before and after the code execution are as expected
    or whether the given output corresponds to the given input.
  prefs: []
  type: TYPE_NORMAL
- en: White-box tests are a little different as you will have access to the actual
    code internals that you're testing as well as the system expected states before
    and after code execution and the expected output for a given input. This kind
    of test has a stronger subjective goal, usually related to performance and software
    quality.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will cover how to implement black-box tests as they are
    more accessible to others and easier to implement. On the other hand, we'll overview
    the tools for executing white-box tests.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many ways a code base may be tested. We''ll focus our efforts on
    two types of automated tests (we will not cover manual testing techniques), each
    with a different goal: **unit testing** and **behavior testing**. Each of these
    tests has a different purpose and complements the other. Let''s take a look at
    what these tests are, when to use them, and how to run them with Flask.'
  prefs: []
  type: TYPE_NORMAL
- en: Unit testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unit testing is a technique where you test the smallest piece of code that has
    meaningful functionality (called a **unit**) against an input and the expected
    output. You usually run unit tests against functions and methods in your code
    base that do not rely on other functions and methods that you've also written.
  prefs: []
  type: TYPE_NORMAL
- en: In a sense, testing is actually the art of stacking unit tests together (first
    test a function, then functions that interact with each other, then functions
    that interact with other systems) in a way that the whole system eventually becomes
    fully tested.
  prefs: []
  type: TYPE_NORMAL
- en: 'For unit testing with Python, we may use the `doctest` or `unittest` built-in
    modules. The `doctest` module is useful for running embedded interactive code
    examples from an object documentation as test cases. Doctests are a nice complement
    to Unittest, which is a more robust module focused on helping you write unit tests
    (as the name implies), and should, preferably, not be used alone. Let''s see an
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, we define a simple `sum_fnc` function, which receives
    two parameters and returns its sum. The `sum_fnc` function has a docstring explaining
    itself. In this docstring, we have an interactive code example of the function
    call and output. This code example is invoked by `doctest.testmod()`, which checks
    whether the given output is correct for the function called.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we have a `TestCase` called `TestSumFnc`, which defines three test methods
    (`test_<test_name>`) and does almost exactly what our docstring test does. The
    difference of this approach is that we are capable of discovering what is wrong
    without the test result, *if* something is wrong. If we wished, for both our docstring
    and test case, to do exactly the same, we would have used the `assert` Python
    keyword to compare the result with the expected result in the test methods. Instead,
    we used the `assertEqual` method, which not only tells us that something is wrong
    with the result if something is wrong, but also informs us that the problem is
    that both the result and the expected values are not equal.
  prefs: []
  type: TYPE_NORMAL
- en: If we wished to check whether our result is, for example, larger than a certain
    value, we would have used the method `assertGreater` or `assertGreaterEqual` so
    that an assertion error would have also told us what kind of error we had.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Good tests are independent from each other so that a failed test may never prevent
    another test from running. Importing the test dependencies from within the test
    and cleaning the database are common ways to do that.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding case is common when writing scripts or desktop applications. A
    web application has different needs regarding the tests. A web application code
    usually runs in response to user interaction through a browser request and returns
    a response as the output. To test in this kind of environment, we have to simulate
    requests and properly test the response content, which is usually not as straightforward
    as the output of our `sum_fnc`. A response may be any kind of document and it
    may have different sizes and content, and you even have to worry about the response
    HTTP code, which holds a lot of contextual meaning.
  prefs: []
  type: TYPE_NORMAL
- en: 'To help you test your views and simulate user interaction with your web application,
    Flask gives you a test client tool through which you can send requests in any
    valid HTTP method to your application. For example, You may consult a service
    through a `PUT` request, or a regular view through `GET`. Here''s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The preceding example is a complete one. We use the `app_factory` pattern to
    create our application, then we create an app and client inside `setUp`, which
    is run before every test method, and we create two tests, one for when the request
    receives a name parameter and another for when it doesn't. As we do not create
    any persistent resources, our `tearDown` method is empty. If we had we a database
    connection with fixtures of any kind, we would have to reset the database state
    inside `tearDown` or even drop the database.
  prefs: []
  type: TYPE_NORMAL
- en: Also, be aware of `test_request_context`, which is used to create a request
    context inside our tests. We create this context so that `url_for`, which requires
    a request context if `SERVER_NAME` config is not set, is able to return our view
    path.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Set the `SERVER_NAME` config if your website uses a subdomain.
  prefs: []
  type: TYPE_NORMAL
- en: Behavior testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In unit testing, we tested the output of functions against an expected result.
    If that result was not what we were waiting for, an assertion exception would
    be raised to notify a problem. It''s a simple black-box test. Now, some weird
    questions: did you notice your test is written in a way different from how a bug
    report or feature request is written? Did you notice that your test cannot be
    read by nontech people because it is, actually, code?'
  prefs: []
  type: TYPE_NORMAL
- en: I would like to introduce you to lettuce ([http://lettuce.it/](http://lettuce.it/)),
    a tool capable of converting the **Gherkin** language tests into actual tests.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For an overview on the Gherkin language, visit [https://github.com/cucumber/cucumber/wiki/Gherkin](https://github.com/cucumber/cucumber/wiki/Gherkin).
  prefs: []
  type: TYPE_NORMAL
- en: 'Lettuce helps you translate the actual user-written features into test method
    calls. This way, a feature request like:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature: compute sum'
  prefs: []
  type: TYPE_NORMAL
- en: In order to compute a sum
  prefs: []
  type: TYPE_NORMAL
- en: As student
  prefs: []
  type: TYPE_NORMAL
- en: Implement `sum_fnc`
  prefs: []
  type: TYPE_NORMAL
- en: '**Scenario**: Sum of positives'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Given** I have the numbers 10 and 20'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**When** I sum them'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Then** I see the result 30'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scenario**: Sum of negatives'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Given** I have the numbers -10 and -20'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**When** I sum them'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Then** I see the result -30'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scenario**: Sum with mixed signals'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Given** I have the numbers 10 and -20'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**When** I sum them'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Then** I see the result -10'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The feature could be translated into the actual code that will test our software.
    Make sure lettuce is properly installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `features` directory and place a `steps.py` (or any other Python filename
    you like) there with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: What did we just do now? We defined three test functions, have_the_numbers,
    compute_sum and check_number, where each receives as first argument a `step` instance
    and other parameters for the actual test. The step decorator, used to decorate
    our functions, is used to map a string pattern parsed from our Gherkin text into
    the function itself. Another responsibility for our decorator is to parse the
    arguments mapped from the step argument to the function as a parameter.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the step for `have_the_numbers` has a regular expression pattern
    (`\-?\d+`) and (`\-?\d+`), which maps two numbers to the `numbers` parameter of
    our function. These values are fetched from our Gherkin input text. For the given
    scenarios, these numbers would be [10, 20], [-10, -20], and [10, -20], respectively.
    At last, `world` is a global variable you may use to share values between the
    steps.
  prefs: []
  type: TYPE_NORMAL
- en: Using features to describe behavior is very healthy for the development process
    because it brings business people closer to what is being created, though it is
    quite verbose. Also, because it is verbose, its use is not advised for testing
    isolated functions as we did in our preceding example. As behavior should be written
    preferably by business people, it should also test behavior the person writing
    can visually attest. For example, "If I click on a button, I get the lowest price
    for something" or "Given I access a certain page, I see some message or some links".
  prefs: []
  type: TYPE_NORMAL
- en: '"Click here, and something happens there". Checking rendered request responses
    is kind of tricky, if you ask me. Why? In our second example, we verify if a given
    string value is inside our `resp.data`, and that was OK because our response is
    returned `complete`. We do not use JavaScript to render anything after the page
    is loaded or to show messages. If this had been the case, our verification would
    have probably returned a wrong result because the JavaScript code would not have
    been executed.'
  prefs: []
  type: TYPE_NORMAL
- en: To correctly render and verify a `view` response, we may use a headless browser
    such as **Selenium** or **PhantomJS** (refer to [https://pythonhosted.org/Flask-Testing/#testing-with-liveserver](https://pythonhosted.org/Flask-Testing/#testing-with-liveserver)).
    The **Flask-testing** extension will be of help too.
  prefs: []
  type: TYPE_NORMAL
- en: Flask-testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Like most Flask extensions, Flask-testing does not do much, but what it does,
    it does beautifully! We will discuss some very useful features that Flask-testing
    gives you out of the box: LiveServer setup, extra assertions, and the JSON response
    handle. Make sure it is installed before continuing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: LiveServer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LiveServer is a Flask-testing tool that allows you to connect to headless browsers,
    a browser that do not render the content visually (such as Firefox or Chrome)
    but executes all scripts and styling and simulates user interaction. Use LiveServer
    whenever you need to evaluate the page content after JavaScript interaction. We'll
    use PhantomJS as our headless browser. My advice to you is that you install the
    old browser, like our ancestors did, compiling it from source. Follow these instructions
    at [http://phantomjs.org/build.html](http://phantomjs.org/build.html) (you may
    have to install a few extra libraries in order to get full functionality from
    phantom). The `build.sh` file will advise you to install it when necessary).
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After compiling **PhantomJS**, make sure it is found in by your PATH by moving
    the binary `bin/phantomjs` to `/usr/local/bin`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Make sure Selenium is installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'And our code will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The `templates/js_index.html` file should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The preceding example is quite simple. We define our factory, which creates
    our app with the two views attached. One returns a `js_index.html` that has a
    script that consults our second view for a phrase and populates the `fillme` HTML
    element, and the second view returns a phrase in JSON format, chosen randomly
    from a predefined list.
  prefs: []
  type: TYPE_NORMAL
- en: We then define `IndexTest` that extends `LiveServerTestCase`, a special class
    we use to run our live server tests. We set our live server to run on a different
    port from the default, but that's not required.
  prefs: []
  type: TYPE_NORMAL
- en: Inside `setUp`, we create a `driver` with selenium WebDriver. The driver is
    something similar to a browser. We'll use it to access and inspect our application
    through the LiveServer. The `tearDown` makes sure our driver is closed after each
    test and resources are released.
  prefs: []
  type: TYPE_NORMAL
- en: '`test_server_is_up_and_running` is self explanatory and not actually necessary
    in real-world tests.'
  prefs: []
  type: TYPE_NORMAL
- en: Then we have `test_random_text_was_loaded`, which is a pretty busy test. We
    use `test_request_context` in order to create a request context to generate our
    URL paths with `url_open. get_server_url`, which will return us our live server
    URL; we join this with our view path and load it into our driver.
  prefs: []
  type: TYPE_NORMAL
- en: With the URL loaded (be aware that the URL was not only loaded, but the scripts
    were also executed), we use `find_element_by_id` to look for the element `fillme`
    and assert that its text context has one of the expected values. This is a simple
    example. You can, for example, test for whether a button is in the expected place;
    submit a form; and trigger a JavaScript function. Selenium plus PhantomJS is a
    powerful combination.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When your development is driven by feature testing, you're actually not using
    **TDD**, but **Behavior Driven Development** (**BDD**). A mix of both techniques
    is, usually, what you want.
  prefs: []
  type: TYPE_NORMAL
- en: Extra assertions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When testing your code, you''ll notice a few tests are kind of repetitive.
    To handle this scenario, one would create a custom TestCases with specific routines
    and extend the tests accordingly. With Flask-testing, you still have to do that,
    but will have to code a little less to test your Flask views as `flask.ext.testing.TestCase`
    is bundled with common assertions, many found in frameworks such as Django. Let''s
    see the most important (in my opinion, of course) assertions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`assert_context(name, value)`: This asserts that a variable is in the template
    context. Use it to verify that a given response context has the right values for
    a variable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`assert_redirects(response, location)`: This asserts that the response is a
    redirect and gives its location. It''s a good practice to redirect after writing
    to storage, like after a successful POST, which is a good use case for this assertion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`assert_template_used(name, tmpl_name_attribute=''name'')`: This asserts that
    a given template is used in the request (`tmpl_name_attribute` is only needed
    if you''re not using Jinja2; not in our case); use it whenever you render an HTML
    template, really!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`assert404(response, message=None)`: This asserts that the response has the
    404 HTTP code; it is useful for "rainy day" scenarios; that is, when someone is
    trying to access something that does not exist. It is very useful.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JSON handle
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here is a lovely trick Flask-testing has for you. Whenever you return a JSON
    response from your views, your response will have an extra attribute called `json`.
    That''s your JSON-converted response! Here is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Fixtures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Good tests are always executed considering a predefined, reproducible application
    state; that is, whenever you run a test in the chosen state, the result will always
    be equivalent. Usually, this is achieved by setting your database data yourself
    and clearing your cache and any temporary files (if you make use of external services,
    you should mock them) for each test. Clearing cache and temporary files is not
    hard, while setting your database data, on the other hand, is.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you''re using **Flask-SQLAlchemy** to hold your data, you would need to
    hardcode, somewhere in your tests as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This approach does not scale as it is not easily reusable (when you define
    this as a function and a method, define it for each test). There are two ways
    to populate your database for testing: **fixtures** and **pseudo-random data**.'
  prefs: []
  type: TYPE_NORMAL
- en: Using pseudo-random data is usually library-specific and produces better test
    data as the generated data is context-specific, not static, but it may require
    specific coding now and then, just like when you define your own fields or need
    a different value range for a field.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fixtures are the most straightforward way as you just have to define your data
    in a file and load it at each test. You can do that by exporting your database
    data, editing at your convenience, or writing it yourself. The JSON format is
    quite popular for this. Let''s take a look on how to implement both:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code is simple. We create a SQLAlchemy model, link it to our app,
    and, during the setup, we load our fixture. In `tearDow`n, we make sure our database
    and SQLAlchemy session are brand new for the next test. Our fixture is written
    using JSON format because it is fast enough and readable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Were we to use pseudo-random generators to create our users, (look up Google
    **fuzzy testing** for more on the subject), we could do it like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Be aware that our tests would also have to change as we are not testing against
    a static scenario. As a rule, fixtures will be enough in most cases, but pseudo-random
    test data is better in most cases as it forces your application to handle real
    scenarios, which are, usually left out.
  prefs: []
  type: TYPE_NORMAL
- en: Extra – integration testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Integration testing is a very widely used term/concept with a very narrow meaning.
    It is used to refer to the act of testing multiple modules together to test their
    integration. As testing multiple modules together from the same code base with
    Python is usually trivial and transparent (an import here, a call there, and some
    output checking), you'll usually hear people using the term **integration testing**
    while referring to testing their code against a different code base, an application
    they did not create or maintain, or when a new key functionality was added to
    the system.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whoa! We just survived a chapter about testing software! That's something to
    be proud of. We learned a few concepts such as TDD, white-box, and black-box testing.
    We also learned how to create unit tests; test our views; write features using
    the Gherkin language and test them using lettuce; use Flask-testing, Selenium
    with PhantomJS to test a HTML response from the user perspective; also how to
    use fixtures to control our application state for proper reproducible testing.
    Now you are capable of testing Flask applications in different ways using the
    correct techniques for different scenarios and needs.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, things are gonna go wild really fast as our subject of
    study will be tricks with Flask. Blueprints, sessions, logging, debugging, and
    so on, will be covered in the next chapter, allowing you to create even more robust
    software. See you there!
  prefs: []
  type: TYPE_NORMAL
