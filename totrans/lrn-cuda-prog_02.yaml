- en: CUDA Memory Management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we described in [Chapter 1](3be2aa92-ffec-4831-90b5-ebbfab302a71.xhtml), *Introduction
    t**o CUDA Programming*, the CPU and GPU architectures are fundamentally different
    and so is their memory hierarchy. They not only differ in terms of sizes and types
    but also in terms of their purpose and design. So far, we have studied how each
    thread accesses its own data with the help of indexing (`blockIdx` and `threadIdx`).
    We also made use of APIs such as `cudaMalloc` to allocate memory on the device.
    Many memory paths are available in a GPU, each with different performance characteristics.
    Launching the CUDA kernel can help us to achieve maximum performance, but only
    when the right type of memory hierarchy is used in an optimal way. It is the developer's
    responsibility to map datasets to the right memory type.
  prefs: []
  type: TYPE_NORMAL
- en: 'Empirically, if we were to plot a graph that outlines the top application performance
    constraints on the GPU, it would look something like the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ce9fca55-7032-4add-8f56-5a8d1af24a01.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding pie charts represent a rough breakdown of the performance problems
    that are seen in the majority of CUDA-based applications. It is clearly visible
    that, most of the time, the application's performance will be bottlenecked by
    memory-related constraints. Based on the application and which memory path is
    taken, the memory-related constraints are divided further.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a different view of this approach and understand the importance of
    using the right memory type efficiently. The latest NVIDIA GPU with Volta Architecture
    provides 7,000 GFLOP of peak performance and its device memory bandwidth is 900
    GB/s. The first observation you will have will be regarding the ratio of FLOP
    to memory bandwidth, which is approximately 7:1\. This is assuming that all of
    the threads are accessing 4 bytes (float) of data for performing an operation.
    The total required bandwidth that's required to perform this operation in one
    go is *4*7,000 = 28,000* GB/s, that is, to achieve peak performance. 900 GB/s
    limits the execution to 225 GFLOP. This bounds the execution rate to 3.2% ( 225
    GFLOP is 3.2% of the peak, which is 7,000 GFLOP) of the peak floating-point execution
    rate of the device. As you are aware by now, GPU is a latency hiding architecture
    that has many threads available for execution, which means it can, theoretically,
    tolerate long memory access latencies. Still, surplus calls to memory can prevent
    very few threads from stalling or waiting and will result in some of the SMs being
    idle. The CUDA architecture provides several other methods that we can use to
    access memory to solve this problem of memory bottlenecks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The path of data traversing from CPU memory until being utilized by the SM
    for processing is demonstrated in the following diagram. Here, we can see the
    journey of the data element before it reaches the SM core for computation. Each
    memory bandwidth is orders of magnitude different, and so is the latency to access
    them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/39dc4102-fded-4f2f-a522-3b7dbc01cb19.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, we can see the data path from the CPU until it reaches
    the registers where the final calculation is done by the ALU/cores.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the different types of memory hierarchies that
    are present in the latest GPU architecture. Each memory may have a different size,
    latency, throughput, and visibility for the application developer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/77cc060d-afe2-41f5-b4ea-842fe41e58fe.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram shows different types of memory that are present in the
    latest GPU architecture and their placement in the hardware.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn how to optimally utilize different types of
    GPU memories. We will also be looking at the latest features of GPU-like unified
    memory, which makes the life of a programmer much simpler. The following memory
    topics will be covered in detail in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Global memory/device memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shared memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Read-only data/cache
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pinned memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unified memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'But before we look at the memory hierarchy, we will follow the cycle of optimization,
    which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Analyze'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 2: Parallelize'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 3: Optimize'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analysis of the application requires us to not only understand the characteristics
    of our application but how effectively it runs on the GPU. For this purpose, we
    will introduce you to Visual Profiler first, before going into the GPU memory.
    Since we have used some of the latest features of CUDA here, please read the following
    section before proceeding with this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Linux PC with a modern NVIDIA GPU (Pascal architecture onward) is required
    for this chapter, along with all of the necessary GPU drivers and the CUDA Toolkit
    (10.0 onward) installed. If you are unsure of your GPU's architecture, please
    visit the NVIDIA GPU's site at [https://developer.nvidia.com/cuda-gpus](https://developer.nvidia.com/cuda-gpus) and
    confirm it. This chapter's code is also available on GitHub at [https://github.com/PacktPublishing/Learn-CUDA-Programming](https://github.com/PacktPublishing/Learn-CUDA-Programming).
  prefs: []
  type: TYPE_NORMAL
- en: The sample code examples for this chapter have been developed and tested with
    version 10.1 of CUDA Toolkit. However, it is recommended to use the latest CUDA
    version or higher.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will introduce you to the Visual Profiler, which will
    help us to analyze our applications. We will also look at how well it runs on
    the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: NVIDIA Visual Profiler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To understand the effective utilization of different memory hierarchies, it
    is important to analyze the characteristics of applications at runtime. Profilers
    are very handy tools that measure and show different metrics that help us to analyze
    the way memory, SM, cores, and other resources are used. NVIDIA made a decision
    to provide an API that developers of profiler tools can use to hook into a CUDA
    application, and a number of profiling tools have evolved over time, such as TAU
    Performance systems, Vampir Trace, and the HPC Toolkit. These all make use of
    the **CUDA Profiler Tools Interface** (**CUPTI**) to provide profiling information
    for CUDA applications.
  prefs: []
  type: TYPE_NORMAL
- en: NVIDIA itself develops and maintains profiling tools that are given as part
    of the CUDA Toolkit. This chapter makes use of these two profiling tools (NVPROF
    and NVVP) to demonstrate the efficient use of different memory types and is not
    a guide to profiling tools.
  prefs: []
  type: TYPE_NORMAL
- en: We will be demonstrating the characteristics of CUDA applications using either
    NVPROF or NVVP. NVPROF is a command-line tool, while `nvvp` has a visual interface.
    `nvvp` comes in two formats, one being a standalone version and another being
    an integrated version inside Nsight Eclipse.
  prefs: []
  type: TYPE_NORMAL
- en: 'The NVVP Profiler window that we will be using extensively looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/56803b05-bc2d-41d1-aba1-66bb38d85c30.png)'
  prefs: []
  type: TYPE_IMG
- en: This is an NVVP version 9.0 window snapshot that was taken on macOS.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are four views available in the window: Timeline, Guide, Analysis Results,
    and Summary. The Timeline view, as the name denotes, shows the CPU and GPU activity
    that occurred across time. The Visual Profiler shows a summary view of the memory
    hierarchy of the CUDA programming model. The Analysis view shows the analysis
    result. The Visual Profiler provides two modes of analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Guided analysis:** As the name suggests, it guides the developer by taking
    a step-by-step approach to understanding the key performance limiters. We would
    suggest this mode for beginners before moving on to the unguided mode once they
    become experts in understanding different metrics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unguided analysis:** The developer has to manually look at the results in
    this mode to understand the performance limiter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The CUDA Toolkit provides two GPU application profile tools, the **NVIDIA Profiler** (**NVPROF**) and
    the **NVIDIA Visual Profiler** (**NVVP**). To obtain performance limiter information,
    we need to have to types of profiling: timeline analysis and metric analysis.
    This code can be accessed at `02_memory_overview/04_sgemm`. The profiling command can be
    executed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Let's open the Visual Profiler. If you are using Linux or OSX, you can execute `nvvp` in
    Terminal. Or, you can find the `nvvp` binary from the CUDA Toolkit installed binary.
    If you are using Windows, you can execute this tool using the Windows search box
    with the `nvvp` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'To open two-profiled data, we will use the File | Import... menu, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/93fc3b11-ff9a-4c01-ab35-c4b52977a905.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, we''ll continue by clicking the Next button at the bottom:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fef3b906-3e8d-43ea-9945-8bee75de1527.png).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our CUDA application uses one process. So, let''s continue by clicking the Next button
    at the bottom:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3e64070c-e869-4d9a-aa83-e4915355f965.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s put the collected profiled data into the Visual Profiler. The following
    screenshot shows an example. Put the timeline data in the second textbox by using
    the Browse... button on the right. Then, place metric analysis data in the next
    textbox in the same way:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ccb86bb-9e2a-4062-b862-eedf5473da49.png)'
  prefs: []
  type: TYPE_IMG
- en: For detailed usage of profiling tools, please refer to the CUDA Profiling guide,
    which comes as part of the CUDA Toolkit (the respective web link is [https://docs.nvidia.com/cuda/profiler-users-guide/index.html](https://docs.nvidia.com/cuda/profiler-users-guide/index.html)).
  prefs: []
  type: TYPE_NORMAL
- en: 'In Windows-based systems, after the CUDA Toolkit''s installation, you can launch
    the Visual Profiler from the Start menu. On a Linux system with X11 forwarding,
    you can launch Visual Profiler by running the `nvvp` command, which stands for
    NVIDIA Visual Profiler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Since we now have a fair understanding of the analysis tool that we will use,
    let's jump into the first and definitely most critical GPU memory—global memory/device
    memory.
  prefs: []
  type: TYPE_NORMAL
- en: Global memory/device memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will provide details on how to make use of global memory, also
    referred to as device memory. In this section, we will also talk about how efficiently
    we can load/store data from global memory into the cache. Since global memory
    is a staging area where all of the data gets copied from CPU memory, the best
    utilization of this memory is essential. Global memory or device memory is visible
    to all of the threads in the kernel. This memory is also visible to the CPU.
  prefs: []
  type: TYPE_NORMAL
- en: The programmer explicitly manages allocation and deallocation with `cudaMalloc`
    and `cudaFree`, respectively. Data is allocated with `cudaMalloc` and declared
    as `__device__`. Global memory is the default staging area for all of the memory
    that's transferred from the CPU using the `cudaMemcpy` API.
  prefs: []
  type: TYPE_NORMAL
- en: Vector addition on global memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Vector addition example we used in the first chapter demonstrates the use
    of global memory. Let''s look at the code snippet again and try to understand
    how global memory is used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '`cudaMalloc` allocates the data on the device memory. The pointers in the arguments
    in the kernel (`a`, `b`, and `c`) point to this device memory. We free this memory
    using the `cudaFree` API. As you can see, all of the threads in the blocks have
    access to this memory inside the kernel.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This code can be accessed at `02_memory_overview/01_vector_addition`. In order
    to compile this code, you can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This is a simple example of making use of global memory. In the next section,
    we will look at how to access the data optimally.
  prefs: []
  type: TYPE_NORMAL
- en: Coalesced versus uncoalesced global memory access
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To effectively use global memory, it is important to understand the concept
    of warp in the CUDA programming model, which we have ignored so far. The warp
    is a unit of thread scheduling/execution in SMs. Once a block has been assigned
    to an SM, it is divided into a 32 -thread unit known as a **warp**. This is the
    basic execution unit in CUDA programming.
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate the concept of a warp, let''s look at an example. If two blocks
    get assigned to an SM and each block has 128 threads, then the number of warps
    within a block is *128/32 = 4* warps and the total number of warps on the SM is
    *4 * 2 = 8* warps. The following diagram shows how a CUDA block gets divided and
    scheduled on a GPU SM:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b1b16bf3-50bc-4e16-a5be-1bbc72b7e395.png)'
  prefs: []
  type: TYPE_IMG
- en: 'How the block and warps are scheduled on the SM and its core is more of architecture-specific
    and will be different for generations such as Kepler, Pascal, and the latest architecture,
    Volta. For now, we can ignore the integrities of scheduling. Among all of the
    available warps, the ones with operands that are ready for the next instruction
    become eligible for execution. Based on the scheduling policy of the GPU where
    the CUDA program is running, the warps are selected for execution. All of the
    threads in a warp execute the same instruction when selected. CUDA follows the
    **Single Instruction, Multiple Thread** (**SIMT**) model, that is, all threads
    in a warp fetch and execute the same instruction at one instance in time. To optimally
    utilize access from global memory, the access should coalesce. The difference
    between coalesced and uncoalesced is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Coalesced global memory access:** Sequential memory access is adjacent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Uncoalesced global memory access:** Sequential memory access is not adjacent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram shows an example of this access pattern in more detail.
    The left-hand side of the diagram shows coalesced access where threads from the
    warp access adjacent data and hence resulting in one 32-wide operation and 1 cache
    miss. The right-hand side of the diagram shows a scenario where access from threads
    within a warp is random and may result in calling 32 one wide operation and hence
    may have 32 cache misses, which is the worst-case scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b125e1a6-614e-48f2-b4d6-dd00e95b1cfe.jpg)'
  prefs: []
  type: TYPE_IMG
- en: To understand this concept further, we need to understand how data reaches from
    global memory via cache lines.
  prefs: []
  type: TYPE_NORMAL
- en: '**Scenario 1:** Warp request 32 aligned, 4 consecutive bytes'
  prefs: []
  type: TYPE_NORMAL
- en: 'The address falls within 1 cache line and one 32-wide operation. The bus utilization
    is 100%, that is, we are utilizing all of the data being fetched from the global
    memory into a cache and not wasting any bandwidth at all. This is shown in the
    following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1f19c553-c555-425a-a0c6-fe683f87a986.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram shows coalesced access, resulting in optimal utilization
    of the bus.
  prefs: []
  type: TYPE_NORMAL
- en: '**Scenario 2:** Warp request 32 scattered 4-byte words'
  prefs: []
  type: TYPE_NORMAL
- en: 'While the warp needs 128 bytes, there are 32 one wide fetches being executed,
    resulting in *32 * 128* bytes moving across the bus on a miss. Bus utilization
    is effectively less than 1%, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bd6f035c-56e5-4aa9-8062-310f4bbe5908.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram shows uncoalesced access, resulting in a waste of bus
    bandwidth**.**
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in the preceding diagram, it is important how threads within the
    warp access the data from global memory. To optimally utilize global memory, it
    is important to improve coalescing. There are multiple strategies that can be
    used. One such strategy is to change the data layout to improve locality. Let's
    look at an example. Computer vision algorithms that apply filters onto an image
    or apply masks onto an image requires the image to be stored onto a data structure.
    The developer has two choices when it comes to declaring an image type.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet makes use of the `Coefficients_SOA` data structure
    to store data in an array format. The `Coefficients_SOA` structure stores image-related
    data such as RGB, hue, and saturation values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram shows the data layout regarding how data is stored for
    `Coefficients_SOA` and accessed by different threads in a kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3742c385-2a7c-4ed3-be87-6f5c1bd9ee67.jpg)'
  prefs: []
  type: TYPE_IMG
- en: By doing this, we can see how the use of the AOS data structure resulted in uncoalesced
    global memory access.
  prefs: []
  type: TYPE_NORMAL
- en: 'The same image can be stored in an array structure format, as shown in the
    following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram shows the data layout regarding how data is stored for `Coefficients_AOS` and
    accessed by different threads in a kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6cba8cd2-87a2-4b90-8145-0f2ad79ace78.jpg)'
  prefs: []
  type: TYPE_IMG
- en: By doing this, we can see how using the SOA data structure results in uncoalesced
    global memory access.
  prefs: []
  type: TYPE_NORMAL
- en: While sequential code on CPU prefers AOS for cache efficiency, SOA is preferred
    in **Single Instruction Multiple Thread** (**SIMT**)models such as CUDA for execution
    and memory efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try to analyze this aspect by making use of a profiler. Configure your
    environment according to the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare your GPU application. As an example, we will use two pieces of code
    to demonstrate the efficient use of global memory. While the `aos_soa.cu` file
    contains the naive implementation that uses the AOS data structure, `aos_soa_solved.cu`
    makes use of the SOA data structure, which utilizes global memory efficiently.
    This code can be found in `02_memory_overview/02_aos_soa`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compile your application with the `nvcc` compiler and then profile it using
    the `nvprof` compiler. The following commands are an example of the `nvcc` command
    for this. We then use the `nvprof` command to profile the application. The `--analysis-metrics`
    flag is also passed so that we can get metrics for the kernels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The generated profiles, that is, `aos_soa.prof` and `aos_soa_solved.prof`,
    are then loaded into the NVIDIA Visual Profiler. The user needs to load the profiling
    output from the File | Open menu. Also, don''t forget to choose All Files as part
    of the file name options:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The profile output is shown in the following screenshot. It is a naive implementation
    that uses the AOS data structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3d6e2a1c-0926-460e-bd5d-d9b72094e20b.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram shows the output of the profiler in guided analysis mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing you will see is that the profiler clearly states that the application
    is memory bound. As you can see, the profilers don''t just show metrics but also
    the analysis of what those metrics mean. In this example, since we are using AOS,
    the profiler clearly states that the access pattern is not efficient. But how
    did the compiler come to this conclusion? Let''s take a look at the following
    screenshot, which gives more details about it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/90e14975-2add-439a-9772-65d594fed082.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, it clearly states that the ideal number transaction for accessing
    data is four, while the run is doing 32 transactions/accesses.
  prefs: []
  type: TYPE_NORMAL
- en: When we change the data structure from AOS to SOA, the bottlenecks are solved.
    When you run the `aos_soa_solved` executable, you will see that the kernel time
    reduces, which is an improvement for our timings. On a V100 16 GB card, the time
    reduces from 104 μs to 47 μs, which is a speedup factor of `2.2x`. The profiler
    output, `aos_soa_solved.prof`, will show that the kernel is still memory-bound,
    which is quite obvious since we are reading and writing more memory data compared
    to doing the computation.
  prefs: []
  type: TYPE_NORMAL
- en: Memory throughput analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It becomes important for an application developer to understand the memory
    throughput of an application. This can be defined in two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '**From an app point of view:** Counts the bytes that were requested by the
    application'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**From a hardware point of view:** Count the bytes that were moved by the hardware'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The two numbers are completely different. There are many reasons for this,
    including uncoalesced access resulting in not all of the transaction bytes being
    utilized, shared memory bank conflicts, and so on. The two aspects we should use
    to analyze the application from a memory point of view are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Address pattern:** Determining the access pattern in real code is quite difficult
    and hence the use of tools such as profilers becomes really important. The metrics
    that are shown by the profiler, such as global memory efficiency and L1/L2 transactions
    per access need to be carefully looked at.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The number of concurrent accesses in flight:** As a GPU is a latency-hiding
    architecture, it becomes important to saturate the memory bandwidth. But determining
    the number of concurrent accesses is generally insufficient. Also, the throughput from
    an HW point of view is much more different than the theoretical value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram demonstrates that ~6 KB of data in flight per SM can
    reach 90% of peak bandwidth for the Volta architecture. The same experiment, when
    done on a previous generation architecture, will yield a different graph. In general,
    it is recommended to understand the GPU memory characteristic for a particular
    architecture in order to get the best performance from that hardware:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/86177233-314d-430b-88d6-e81fedb8d916.png)'
  prefs: []
  type: TYPE_IMG
- en: This section provided us with sample uses of global memory and how we can utilize
    it in an optimal fashion. Sometimes, coalesced data access from global memory
    is difficult (for example, in CFD domains, in the case of unstructured grids,
    the data of neighboring cells may not reside next to each other in memory). To
    solve a problem like this or to reduce the impact on performance, we need to make
    use of another form of memory, known as shared memory.
  prefs: []
  type: TYPE_NORMAL
- en: Shared memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Shared memory has always had a vital role to play in the CUDA memory hierarchy
    known as the **User-Managed Cache**. This provides a mechanism for users so that
    they can read/write data in a coalesced fashion from global memory and store it
    in memory, which acts like a cache but can be controlled by the user. In this
    section, we will not only go through the steps we can take to make use of shared
    memory but also talk about how we can efficiently load/store data from shared
    memory and how it is internally arranged in banks. Shared memory is only visible
    to threads in the same block. All of the threads in a block see the same version
    of a shared variable.
  prefs: []
  type: TYPE_NORMAL
- en: Shared memory has similar benefits to a CPU cache; however, while a CPU cache
    cannot be explicitly managed, shared memory can. Shared memory has an order of
    magnitude lower latency than global memory and an order of magnitude higher bandwidth
    than global memory. But the key usage of shared memory comes from the fact that
    threads within a block can share memory access. CUDA programmers can use shared
    variables to hold the data that was reused many times during the execution phase
    of the kernel. Also, since threads within the same block can share results, this
    helps to avoid redundant calculations. The CUDA Toolkit, up until version 9.0,
    did not provide a reliable communication mechanism between threads in different
    blocks. We will be covering the CUDA 9.0 communication mechanism in more detail
    in subsequent chapters. For now, we will assume that communication between threads
    is only possible in CUDA by making use of shared memory.
  prefs: []
  type: TYPE_NORMAL
- en: Matrix transpose on shared memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the most primitive examples that''s used for demonstrating shared memory
    is that of the matrix transpose. Matrix transpose is a memory-bound operation.
    The following code snippet, which uses the `matrix_transpose_naive` kernel, shows
    a sample implementation of the matrix transpose kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code shows the naive implementation of matrix transpose using
    global memory. If this is implemented in a naive way, this will result in uncoalesced access
    either while reading the matrix or writing the matrix. The execution time of the
    kernel on a V100 PCIe 16 GB card is ~60 μs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Configure your environment according to the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare your GPU application. This code can be found in `02_memory_overview/02_matrix_transpose`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compile your application with the `nvcc` compiler and then profile it using
    the `nvprof` compiler. The following commands are an example of the `nvcc` command
    for this. Then, we use the `nvprof` command to profile the application. The `--analysis-metrics`
    flag is also passed to get metrics for the kernels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The generated profile, that is, `matrix_transpose.prof`, is then loaded in
    the NVIDIA Visual Profiler. The user needs to load the profiling output from the File
    | Open menu. Also, don''t forget to choose All Files as part of the filename options:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the output of profiling. The output clearly
    states that there is uncoalesced access to global memory, which is a key indicator
    that needs to be worked on so that we can improve performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/44f9e83a-c02c-424b-b703-4f58cdb5f3a8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'One way to solve this problem is to make use of high bandwidth and low latency
    memory, such as shared memory. The trick here is to read and write from global
    memory in a coalesced fashion. Here, the read or write to shared memory can be
    an uncoalesced pattern. The use of shared memory results in better performance
    and the time is reduced to 21 microseconds, which is a factor of 3x time speedup:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code snippet shows the implementation of matrix transpose using
    shared memory. Global memory reads/writes coalesce, while the transpose happens
    in the shared memory.
  prefs: []
  type: TYPE_NORMAL
- en: Bank conflicts and its effect on shared memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Good speedup compared to using global memory does not necessarily imply that
    we are using shared memory effectively. This becomes clearer if we look at the
    profiler metrics. If we shift from guided analysis to unguided analysis for the
    profiler output, that is, `matrix_transpose.prof`, we will see that the shared
    memory access pattern shows alignment problems, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/436d4870-5d3b-4251-8d60-cdf8e0fc4ad4.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see how the profiler shows nonoptimal usage of shared memory, which is
    a sign of a bank conflict.
  prefs: []
  type: TYPE_NORMAL
- en: 'To effectively understand this alignment problem, it is important to understand
    the concept of *banks*. Shared memory is organized into banks to achieve higher
    bandwidth. Each bank can service one address per cycle. Memory can serve as many
    simultaneous accesses as it has banks. The Volta GPU has 32 banks, each 4 bytes
    wide. When an array is stored in shared memory, the adjacent 4-byte words go to
    successive banks, as demonstrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/367a30e8-819e-4b7a-bda1-a1aa3212e9c4.png)'
  prefs: []
  type: TYPE_IMG
- en: The logical view in the preceding diagram shows how data is stored in shared
    memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiple simultaneous accesses by threads within a warp to a bank results in
    a bank conflict. In other words, a bank conflict occurs when, inside a warp, two
    or more threads access different 4-byte words in the same bank. Logically, this
    is when two or more threads access different *rows* in the same bank. The following
    diagrams show examples of different *n*-way bank conflicts. The worst case is
    a 32-way conflict | 31 replays – each replay adds a few cycles of latency:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7826d460-f142-4d1e-86bb-fdd80b74bcaa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding scenario shows threads from the same warp accessing the adjacent
    4-byte elements that reside in different banks, resulting in no bank conflict.
    Take a look at the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6b606f97-4c2f-46f1-8cdb-3f9f1b43d8f7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is another no bank conflict scenario where threads from the same warp
    access random 4-byte elements that reside in different banks, resulting in no
    bank conflict. Sequential access due to a 2-way bank conflict in shared memory
    is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b840222-c509-44e5-94df-5340ecdc9d65.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram shows a scenario where threads **T0** and **T1** from
    the same warp access 4-byte elements residing in the same bank and hence resulting
    in a 2-way bank conflict.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding example of the matrix transpose, we made use of shared memory
    to get better performance. However, we can see a 32-way bank conflict. To resolve
    this, a simple technique known as padding can be used. All this does is pad the
    shared memory with a dummy, that is, one additional column, which results in threads
    accessing different banks and hence resulting in better performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code snippet, where we used the `matrix_transpose_shared` kernel,
    shows this concept of padding, which results in removing bank conflicts and hence
    better utilization of shared memory bandwidth. As usual, run the code and verify
    this behavior with the help of the Visual Profiler. With these changes, you should
    see the time of the kernel reduce to 13 microseconds, which is a further speedup
    of 60%.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we saw how to optimally utilize shared memory, which provides
    both read and write access as a scratchpad. But sometimes, the data is just read-only
    input and does not require write access. In this scenario, GPU provides an optimal
    memory known as **texture** memory. We will take a look at this in the next chapter,
    along with other advantages that it provides to developers. We will cover read-only
    data in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Read-only data/cache
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you may have already guessed based on the memory name, a read-only cache
    is suitable for storing data that is read-only and does not change during the
    course of kernel execution. The cache is optimized for this purpose and, based
    on the GPU architecture, frees up and reduces the load on the other cache, resulting
    in better performance. In this section, we will provide details on how to make
    use of a read-only cache with the help of an image processing code sample that
    does image resizing.
  prefs: []
  type: TYPE_NORMAL
- en: Read-only data is visible to all of the threads in the grid in a GPU. The data
    is marked as read-only for the GPU, which means any changes to this data will
    result in unspecified behavior in the kernel. CPU, on the other hand, has both
    read and write access to this data.
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally, this cache is also referred to as the texture cache. While the
    user can explicitly call the texture API to make use of the read-only cache, with
    the latest GPU architecture, developers can take advantage of this cache without
    making use of the CUDA texture API explicitly. With the latest CUDA version and
    GPUs such as Volta, kernel pointer arguments marked as `const __restrict__` are
    qualified to be read-only data that traverses through the read-only cache data
    path. A developer can also force loading through this cache with the `__ldg` intrinsic.
  prefs: []
  type: TYPE_NORMAL
- en: Read-only data is ideally used when an algorithm demands the entire warp to
    read the same address/data, which primarily results in a broadcast to all of the
    threads requesting the data per clock cycle. The texture cache is optimized for
    2D and 3D locality. With threads being part of the same warp, read data from texture
    addresses that have 2D and 3D locality tend to achieve better performance. Textures
    have proven useful in applications that demand random memory access, especially
    prior to Volta architecture cards.
  prefs: []
  type: TYPE_NORMAL
- en: Texture provides support for bilinear and trilinear interpolation, which is
    particularly useful for image process algorithms such as scaling an image.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows an example of threads within a warp accessing elements
    spatially located in a 2D space. The texture is suitable for these kinds of workloads:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b5bd2c1c-54a6-44ad-9bf1-fd533f1da8bc.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, let's take a look at a small real-world algorithm about scaling to demonstrate
    the use of texture memory.
  prefs: []
  type: TYPE_NORMAL
- en: Computer vision – image scaling using texture memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will be using image scaling as an example to demonstrate the use of texture
    memory. An example of image scaling is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fea97149-f4e0-4cad-b8a1-dfec2573fabc.png)'
  prefs: []
  type: TYPE_IMG
- en: Image scaling requires interpolation of an image pixel in 2 dimensions. Texture
    provides both of these functionalities (interpolation and efficient access to
    2D locality) which, if accessed by global memory directly, would result in unconcealed
    memory access.
  prefs: []
  type: TYPE_NORMAL
- en: 'Configure your environment according to the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare your GPU application. This code can be found at `02_memory_overview/03_image_scaling`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compile your application with the `nvcc` compiler with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The `scrImagePgmPpmPackage.cpp` file contains the source code for reading and
    writing images with `.pgm` extensions. The texture code is present in `image_scaling.cu`.
  prefs: []
  type: TYPE_NORMAL
- en: For viewing the `pgm` files users can make use of viewers like IrfanView ([https://www.irfanview.com/main_download_engl.htm](https://www.irfanview.com/main_download_engl.htm))
    which are free to use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Primarily, there are four steps that are required so that we can make use of
    texture memory:'
  prefs: []
  type: TYPE_NORMAL
- en: Declare the texture memory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bind the texture memory to a texture reference.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read the texture memory using a texture reference in the CUDA kernel.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unbind the texture memory from your texture reference.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code snippet shows the four steps we can use to make use of texture
    memory. From the Kepler GPU architecture and CUDA 5.0 onward, a new feature called
    bindless textures was introduced. This exposes texture objects, which is basically
    a C++ object that can be passed to the CUDA kernel. They are referred to as bindless
    as they don''t require manual binding/unbinding, which was the case for earlier
    GPU and CUDA versions. Texture objects are declared using the `cudaTextureObject_t` class
    API. Let''s go through these steps now:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, declare the texture memory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a channel description, which will be used while we link to the texture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, specify the texture object parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, read the texture memory from your texture reference in the CUDA kernel:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, destroy the texture object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The important aspects of texture memory, which act like configurations and
    are set by the developer, are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Texture dimension:** This defines whether the texture is addressed as a 1D,
    2D, or 3D array. Elements within a texture are also referred to as texels. The
    depth, width, and height are also set to define each dimension. Note that each
    GPU architecture defines the maximum size for each dimension that is acceptable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Texture type:** This defines the size in terms of whether it is a basic integer
    or floating-point texel.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Texture read mode:** Read mode for texture defines how the elements are read.
    They can be either read in `NormalizedFloat` or `ModeElement` format. Normalized
    float mode expects the index within a range of [0.0 1.0] and [-1.0 1.0] for unsigned
    integer and signed integer types.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Texture addressing mode:** One of the unique features of texture is how it
    can address access that is out of range. This might sound unusual but, in fact,
    is pretty common in many imaging algorithms. As an example, if you are applying
    interpolation by averaging the neighboring pixels, what should be the behavior
    for the boundary pixels? Texture provides this as an option to the developer so
    that they can choose whether to treat out of range as clamped, wrapped, or mirrored.
    In the resizing example, we have set it to clamp mode, which basically means that
    out of range access is clamped to the boundary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Texture filtering mode:** Setting the mode defines how the return value is
    computed when fetching the texture. Two types of filtering modes are supported: `cudaFilterModePoint` and `cudaFilterModeLinear`.
    When set to linear mode, interpolation is possible (simple linear for 1D, bilinear
    for 2D, and trilinear for 3D). Linear mode only works when the return type is
    of the float type. `ModePoint`, on the other hand, does not perform interpolation
    but returns a texel of the nearest coordinate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The key intention of introducing texture memory in this section is to provide
    you with an example of its usage and to show you where texture memory is useful.
    It provides a good overview of the different configuration parameters. Please
    refer to the CUDA API guide ([https://docs.nvidia.com/cuda/cuda-runtime-api/index.html](https://docs.nvidia.com/cuda/cuda-runtime-api/index.html))
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we described the purpose of using texture memory by the use
    of an example. In the next section, we will look at the fastest (lowest latency)
    available GPU memory (registers). This is present in abundance in GPU compared
    to CPU.
  prefs: []
  type: TYPE_NORMAL
- en: Registers in GPU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the fundamental differences between the CPU and GPU architectures is
    the abundance of registers in GPU compared to CPU. This helps the threads to keep
    most of their data in registers and hence reducing the latency of context switching.
    Hence, it is also important to make this memory optimally.
  prefs: []
  type: TYPE_NORMAL
- en: Registers have a scope of a single thread. A private copy of the variable is
    created for all of the launched threads in the GRID. Each thread has access to
    its private copy of the variable, while other thread's private variables cannot
    be accessed. For example, if a kernel is launched with 1,000 threads, then a variable
    whose scope is a thread gets its own copy of the variable.
  prefs: []
  type: TYPE_NORMAL
- en: Local variables that are declared as part of the kernel are stored in the registers.
    Intermediate values are also stored in registers. Every SM has a fixed set of
    registers. During compilation, a compiler (`nvcc`) tries to find the best number
    of registers per thread. In case the number of registers falls short, which generally
    happens when the CUDA kernel is large and has a lot of local variables and intermediate
    calculations, the data gets pushed to local memory, which may reside either in
    an L1/L2 cache or even lower in the memory hierarchy, such as global memory. This
    is also referred to as register spills. The number of registers per thread plays
    an important role in how many blocks and threads can be active on an SM. This
    concept is covered in detail in the next chapter, which has a section dedicated
    to occupancy. In general, it is recommended to not declare lots of unnecessary
    local variables. If the registers are restricting the number of threads that can
    be scheduled on an SM, then the developer should look at restructuring the code
    by splitting the kernel into two—or more, if possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'A variable that''s declared as part of the `vecAdd` kernel is stored in register
    memory. The arguments that are passed to the kernel, that is, `A`, `B`, and `C`, point
    to global memory, but the variable themselves are stored either in the shared
    memory of registers based on the GPU architecture.The following diagram shows
    the UDA memory hierarchy and the default locations of the different variable types:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4063603e-0e98-42d7-9a61-5353c9dfb82d.png)'
  prefs: []
  type: TYPE_IMG
- en: So far, we have seen the purpose and optimal usage of the key memory hierarchy
    (global, texture, shared, and registers). In the next section, we will look at
    some of the optimizations and features of GPU memory that can improve the performance
    of the application and increase the productivity of developers while they are
    writing CUDA programs.
  prefs: []
  type: TYPE_NORMAL
- en: Pinned memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is time to recall the path that's taken by data, that is, from CPU memory
    to the GPU registers, which are finally consumed by the GPU cores for computation.
    Even though GPU has more compute performance and higher memory bandwidth, the
    overall benefit of the speedup gained by the application can become normalized
    due to the transfer between CPU memory and GPU memory. This transfer of data happens
    via bus/links/protocols such as PCIe (in the case of CPU architectures from Intel
    and AMD) or NVLink (for CPU architectures such as `power` from OpenPower Foundation).
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to overcome these bottlenecks, the following tricks/guidelines are
    recommended:'
  prefs: []
  type: TYPE_NORMAL
- en: First, it is recommended to minimize the amount of data that's transferred between
    the host and device when possible. This may even mean to run a portion of sequential
    code as a kernel on the GPU, thereby giving little or no speedup compared to running
    them sequentially on the host CPU.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Second, it is important to achieve higher bandwidth between the host and the
    device by making use of pinned memory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is advised to batch small transfers into one large transfer. This helps to
    reduce the latency involved in calling the data transfer CUDA API, which may range
    from a few microseconds to a few milliseconds based on the system's configuration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lastly, applications can make use of asynchronous data transfers to overlap
    the kernel execution with data transfers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will be covering pinned memory transfer in more detail in this section. Asynchronous
    transfers will be covered in more detail in [Chapter 4](449e8a0b-fb41-41b5-8b1d-9ab81fae16c5.xhtml), *Kernel
    Execution Model and Optimization Strategies*, where we will make use of a concept
    called CUDA streams.
  prefs: []
  type: TYPE_NORMAL
- en: Bandwidth test – pinned versus pageable
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By default, the memory allocation API known as `malloc()` allocates a memory
    type that's pageable. What this means is that, if needed, the memory that is mapped
    as pages can be swapped out by other applications or the OS itself. Hence, most
    devices, including GPUs and others such as InfiniBand, which also sit on the PCIe
    bus, expect the memory to be pinned before the transfer. By default, the GPU will
    not access the pageable memory. Hence, when a transfer of memory is invoked, the
    CUDA driver allocates the temporary pinned memory, copies the data from the default
    pageable memory to this temporary pinned memory, and then transfers it to the
    device via a **Device Memory Controller** (**DMA**).
  prefs: []
  type: TYPE_NORMAL
- en: This additional step not only adds latency but also has a chance to get the
    page that was requested transferred to the GPU memory, which has been swapped
    and needs to be brought back to GPU memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand the impact of making use of pinned memory, let''s try to compile
    and run a piece of sample code. This has been provided as part of the CUDA samples.
    Configure your environment according to the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare your GPU application. This code is present in`<CUDA_SAMPLES_DIR>/1_Utilities/bandwidthTest`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compile your application with the `make` command.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the executable in two modes, that is, `pageable` and `pinned`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Note that `CUDA_SAMPLES_DIR` is the path to the directory where the CUDA installation
    has been placed.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, the key change, compared to the previous pieces of code, is
    that we have written so far is a data allocation API. The following code snippet
    shows the allocation of memory using the `cudaMallocHost` API instead of `malloc`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The `cudaMallocHost` API makes the memory pinned memory instead of pageable
    memory. While the allocation API has changed, we can still use the same data transfers
    API, that is, `cudaMemcpy()`*. *Now, the important question is, *what is this
    pinned memory and why does it provide better bandwidth? *We will cover this in
    the next section.
  prefs: []
  type: TYPE_NORMAL
- en: The impact on performance can be seen from the output of the bandwidth test.
    We have plotted the results in a graph so that you can easily understand the impact. The
    *x* axis shows that data that was transferred in KB, while the *y* axis shows
    the achieved bandwidth in MB/sec.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first graph is for **Host to Device** transfer, while the second graph
    is for **Device to Host** transfers. The first thing you will see is that the
    maximum bandwidth that can be achieved is ~12 GB/sec. PCIe Gen3''s theoretical
    bandwidth is 16 GB/sec, but what''s achievable is in the range of 12 GB/sec. Achievable
    bandwidth highly depends on the system (motherboard, CPU, PCIe topology, and so
    on):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e309df71-3726-487e-a095-dcdda648b79b.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, for pinned memory, bandwidth is always higher for the lower
    transfer sizes while pageable memory bandwidth becomes equal at higher data size
    transfers since the driver and DMA engine start optimizing the transfers by applying
    concepts such as overlapping. As much as it is advised to make use of pinned memory,
    there is a downside to overdoing it as well. Allocating the whole system memory
    as pinned for the application(s) can reduce overall system performance. This happens
    because it takes away the pages that are available for other application and operating
    system tasks. The right size that should be pinned is very application and system-dependent
    and there is no-off-the-shelf formula available for this. The best thing we can
    do is test the application on the available system and choose the optimal performance
    parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Also, it is important to understand that new interconnects such as NVLink provide
    higher bandwidth and lower latency for applications that are bound by these data
    transfers. Currently, NVLink between CPU and GPU is only provided with the Power
    CPU.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we looked at how to improve the data transfer speed between
    CPU and GPU. We will now move on to making use of one of the new features of CUDA,
    called unified memory, which has helped to improve the productivity of developers
    writing CUDA programs.
  prefs: []
  type: TYPE_NORMAL
- en: Unified memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With every new CUDA and GPU architecture release, new features are added. These
    new features provide more performance and ease of programming or allow developers
    to implement new algorithms that otherwise weren't possible to port on GPUs using
    CUDA. One such important feature that was released from CUDA 6.0 onward and finds
    its implementation from the Kepler GPU architecture is unified memory. We will
    refer to unified memory as UM in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In simpler words, UM provides the user with a view of single memory space that''s
    accessible by all GPUs and CPUs in the system. This is illustrated in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/768c31b3-add3-4a60-a38e-34966eb35f1b.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this section, we will cover how to make use of UM, optimize it, and highlight
    the key advantages of making use of it. Like global memory access, if done in
    an uncoalesced fashion, results in bad performance, the UM feature, if not used
    in the right manner, will result in degradation in terms of the application's
    overall performance. We will take a step-wise approach, starting with a simple
    program, and build over it so that we can understand UM and its implication on
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try to compile and run some sample pieces of code. Configure your environment
    according to the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare your GPU application. This code can be found in `02_memory_overview/unified_memory`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compile your application with the following `nvcc` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Please note that the results that are shown in this section are for the Tesla
    P100 card. The same code, when run on other architectures such as Kepler, is expected
    to give different results. The concentration of this section is on the latest
    architectures, such as Pascal and Volta.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding unified memory page allocation and transfer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start with the naive implementation of UM. The first piece of code, `unified_memory.cu`,
    demonstrates the basic usage of this concept. The key change in the code is the
    usage of the `cudaMallocManaged()` API instead of allocating the memory using
    `malloc`, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'If we look at the source code carefully, we will see that the `x` and `y` variables
    are allocated only once and point to unified memory. The same pointer is being
    sent to both the GPU `add<<<>>>()` kernel and used for initialization in the CPU
    using the `for` loop. This makes things really simple for programmers as they
    don''t need to keep track of whether the pointer is pointing to CPU memory or
    GPU memory. But does it necessarily mean that we get good performance or transfer
    speeds out of it? Not necessarily, so let''s try to dig deeper by profiling this
    code, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d9df1195-6123-4c69-bc00-fd5ef9fc642e.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We used the following command to get the profiling output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'As expected, most of the time is spent in the `add<<<>>>` kernel. Let''s try
    to theoretically calculate the bandwidth. We will use the following formula to
    calculate the bandwidth:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Bandwidth = Bytes / Seconds = (3 * 4,194,304 bytes * 1e-9 bytes/GB) / 2.6205e-3s
    = 5 GB/s*'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, P100 provides a theoretical bandwidth of 720 GB/s, while we
    are able to achieve only 5 GB/s, which is really poor. You may be wondering why
    were are only calculating the memory bandwidth. The reason for this is that the
    application is memory-bound as it completes three memory operations and only one
    addition. Therefore, it makes sense to concentrate on this aspect only.
  prefs: []
  type: TYPE_NORMAL
- en: From Pascal cards onward, `cudaMallocManaged()` does not allocate physical memory
    but allocates memory based on a first-touch basis. If the GPU first touches the
    variable, the page will be allocated and mapped in the GPU page table; otherwise,
    if the CPU first touches the variable, it will be allocated and mapped to the
    CPU. In our code, the `x` and `y` variables get used in the CPU for initialization.
    Hence, the page is allocated to the CPU. In the `add<<<>>>` kernel, when these
    variables are accessed, there is a page fault which occurs and the time of page
    migration gets added to the kernel time. This is the fundamental reason why kernel
    time is high. Now, let's dive deep into the steps for page migration.
  prefs: []
  type: TYPE_NORMAL
- en: 'The sequence of operations that are completed in a page migration are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to allocate new pages on the GPU and CPU (first-touch basis).
    If the page is not present and mapped to another, a device page table page fault
    occurs. When ***x**, which resides in **page 2**, is accessed in the GPU that
    is currently mapped to CPU memory, it gets a page fault. Take a look at the following
    diagram:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/1870160e-d6f8-4d3a-92e4-ff861290d337.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the next step, the old page on the CPU is unmapped, as shown in the following
    diagram:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/8535b1f3-68ac-46a9-bc94-a6b16b9c50c1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, the data is copied from the CPU to the GPU, as shown in the following
    diagram:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/016530df-0a4d-4be0-a8b8-3fd2437925be.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, the new pages are mapped on the GPU, while the old pages are freed
    on the CPU, as shown in the following diagram:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c0655a9b-0339-4ca4-a6a2-f39c32608523.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The **Translation Lookaside Buffer** (**TLB**) in GPU, much like in the CPU,
    performs address translation from the physical address to the virtual address.
    When a page fault occurs, the TLB for the respective SM is locked. This basically
    means that the new instructions will be stalled until the time the preceding steps
    are performed and finally unlock the TLB. This is necessary to maintain coherency
    and maintain a consistent state of memory view within an SM. The driver is responsible
    for removing these duplicates, updating the mapping, and transferring page data.
    All of this time, as we mentioned earlier, is added to the overall kernel time.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we know the problem now. What is the solution, though? To solve this problem,
    we are going to make use of two approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we will create an initialization kernel on the GPU so that there are
    no page faults during the `add<<<>>>` kernel run. Then, we will optimize the page
    faults by making use of the warp per page concept.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will prefetch the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will cover these methods in the next sections.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing unified memory with warp per page
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start with the first approach, which is the initialization kernel. If
    you take a look at the source code in the `unified_memory_initialized.cu` file,
    we added a new kernel there named `init<<<>>>`, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'By adding a kernel to initialize the array in the GPU itself, the pages are
    allocated and mapped to the GPU memory as they are touched first in the `init<<<>>>`
    kernel. Let''s look at the output of the profiling results for this code, where
    profiling the output with the initialization kernel is shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/29bcc42e-cf74-4f62-98c5-f5c47a533c79.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We used the following command to get the profiling output
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the time of the `add<<<>>>` kernel was reduced to 18 μs. This
    effectively gives us the following kernel bandwidth:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Bandwidth = Bytes / Seconds = (3 * 4,194,304 bytes * 1e-9 bytes/GB) / 18.84e-6s
    = 670 GB/s*'
  prefs: []
  type: TYPE_NORMAL
- en: 'This bandwidth is what you would expect in a non-unified memory scenario. As
    we can see from the naive implementation in the preceding screenshot, there is
    no host to device row in the profiling output. However, you might have seen that
    even though the `add<<<>>>` kernel time has reduced, the `init<<<>>>` kernel has
    not become the hotspot taking maximum time. This is because we touch the memory
    for the first time in the `init<<<>>>` kernel. Also, you might be wondering what
    these GPU fault groups are. As we discussed earlier, individual page faults may
    be grouped together in groups to improve bandwidth based on heuristics, as well
    as the access pattern. To dive further into this, let''s reprofile the code with
    `--print-gpu-trace` so that we can see individual page faults. As you can see
    the following screenshot, the GPU trace shows the overall trace of faults and
    virtual addresses on which this fault happened:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8c5e2708-82dc-44ee-a680-74bbd6a305e1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We used the following command to get the profiling output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The second row shows 11 page faults for the same page. As we discussed earlier,
    the role of the driver is to filter these duplicate faults and transfer each page
    just once. In a complicated access pattern, generally, the driver doesn''t have
    enough information about what data can be migrated to the GPU. To improve this
    scenario, we will further implement the warp per page concept, which basically
    means that each warp will access elements that are in the same pages. This requires
    additional effort from the developer. Let''s reimplement the `init<<<>>>` kernel.
    You can see this implementation in the `unified_memory_64align.cu` file, which
    we compiled earlier. The snapshot of the kernel is shown in the following code
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The kernel shows that the indexing is based on `warp_id`. The warp size in
    the GPU is 32 and is responsible for populating the `x` and `y` variables within
    an index that has a range of 64 KB, that is, warp 1 is responsible for the first
    64 KB, while warp 2 is responsible for the elements in the next 64 KB. Each thread
    in a warp loops (the innermost `for` loop) to populate the index within the same
    64 KB. Let''s look at the profiling results of this code. As we can see from the
    profiling output in the following screenshot, the time for the `init<<<>>>` kernel
    has reduced, and the GPU fault group has also considerably reduced:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3566af1c-8117-4300-a9ca-f6881d8e16f8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can reconfirm this by running the profiler with `--print-gpu-trace`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot clearly shows that the GPU page faults per page have
    decreased:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4a798b4b-3c10-4012-917a-edf54eab358a.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Optimizing unified memory using data prefetching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s look at an easier method, called data prefetching. One key thing
    about CUDA is that it provides different methods to the developer, starting from
    the easiest ones to the ones that require ninja programming skills. **Data prefetching**
    are basically hints to the driver to prefetch the data that we believe will be
    used in the device prior to its use. CUDA provides a prefetching API called `cudaMemPrefetchAsync()`
    for this purpose. To see its implementation, let''s look at the `unified_memory_prefetch.cu` file,
    which we compiled earlier. A snapshot of this code is shown in the following code
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The code is quite simple and explains itself. The concept is fairly simple:
    in the case where it is known what memory will be used on a particular device,
    the memory can be prefetched. Let''s take a look at the profiling result, which
    is shown in the following screenshot.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, the `add<<<>>>` kernel provides the bandwidth that we expect
    it to provide:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5a07082c-4e58-4946-9ba6-764b41041553.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Unified memory is an evolving feature and changes with every CUDA version and
    GPU architecture release. It is expected that you keep yourself informed by accessing
    the latest CUDA programming guide ([https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-unified-memory-programming-hd](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-unified-memory-programming-hd)).
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have seen the usefulness of the UM concept, which not only provides
    ease of programming (not explicitly managing memory using the CUDA API) but is
    much more powerful and helpful when it comes to porting applications that were
    otherwise either not possible to be ported on GPU or were too difficult to port. One
    of the key advantages of using UM is over-subscription. GPU memory is quite limited
    compared to CPU memory. The latest GPU (Volta card V100) provides 32 GB max per
    GPU. With the help of UM, multiple pieces of GPU memory, along with CPU memory,
    can be seen as one big memory. For example, the NVIDIA DGX2 machine, which has
    a 16 Volta GPU of 323 GB, can be seen as a collection of GPU memory with a maximum
    size of 512 GB. The advantages of these are enormous for applications such as **Computational
    Fluid Dynamics** (**CFD**) and analytics. Previously, where it was difficult to
    fit the problem size in GPU memory, it is now possible. Moving pieces by hand
    is error-prone and requires tuning the memory size.
  prefs: []
  type: TYPE_NORMAL
- en: Also, the advent of high speed interconnects such as NVLink and NVSwitch allow
    for fast transfer between GPU with high bandwidth and low latency. You can actually
    get high performance with unified memory!
  prefs: []
  type: TYPE_NORMAL
- en: 'Data prefetching, combined with hints specifying where the data will actually
    reside, is helpful for multiple processors that need to simultaneously access
    the same data. The API name that''s used in this case is `cudaMemAdvice()`. Hence,
    by knowing your application inside out, you can optimize the access by making
    use of these hints. These are also useful if you wish to override some of the
    driver heuristics. Some of the advice that''s currently being taken by the API
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cudaMemAdviseSetReadMostly`: As the name suggests, this implies that the data
    is mostly read-only. The driver creates a read-only copy of the data, resulting
    in a reduction of the page fault. It is important to note that the data can still
    be written to. In that case, the page copies become invalidated, except for the
    device that wrote the memory:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '`cudaMemAdviseSetPreferredLocation`: This advice sets the preferred location
    for the data to be the memory belonging to the device. Setting the preferred location
    does not cause data to migrate to that location immediately. Like in the following
    code, `mykernel<<<>>>` will page fault and generate direct mapping to data on
    the CPU. The driver tries to *resist* migrating data away from the set preferred
    location using `cudaMemAdvise`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '`cudaMemAdviseSetAccessedBy`: This advice implies that the data will be accessed
    by the device. The device will create a direct mapping of input in the CPU memory
    and no page faults will be generated:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will use a holistic view to see how different memories
    in GPU have evolved with the newer architecture.
  prefs: []
  type: TYPE_NORMAL
- en: GPU memory evolution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'GPU architectures have evolved over time and memory architectures have changed
    considerably. If we take a look at the last four generations, there are some common
    patterns which emerge, some of which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The memory capacity, in general, has increased in levels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The memory bandwidth and capacity have increased with new generation architectures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following table shows the properties for the last four generations:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Memory type** | **Properties** | **Volta V100** | **Pascal P100** | **Maxwell
    M60** | **Kepler K80** |'
  prefs: []
  type: TYPE_TB
- en: '| **Register** | Size per SM | 256 KB | 256 KB | 256 KB | 256 KB |'
  prefs: []
  type: TYPE_TB
- en: '| **L1** | Size | 32...128 KiB | 24 KiB | 24 KiB | 16...48 KiB |'
  prefs: []
  type: TYPE_TB
- en: '| Line size | 32 | 32 B | 32 B | 128 B |'
  prefs: []
  type: TYPE_TB
- en: '| **L2** | Size | 6144 KiB | 4,096 KiB | 2,048 KiB | 1,536 Kib |'
  prefs: []
  type: TYPE_TB
- en: '| Line size | 64 B | 32B | 32B | 32B |'
  prefs: []
  type: TYPE_TB
- en: '| **Shared memory** | Size per SMX | Up to 96 KiB | 64 KiB | 64 KiB | 48 KiB
    |'
  prefs: []
  type: TYPE_TB
- en: '| Size per GPU | up to 7,689 KiB | 3,584 KiB | 1,536 KiB | 624 KiB |'
  prefs: []
  type: TYPE_TB
- en: '| Theoretical bandwidth | 13,800 GiB/s | 9,519 GiB/s | 2,410 GiB/s | 2,912
    GiB/s |'
  prefs: []
  type: TYPE_TB
- en: '| **Global memory** | Memory bus | HBM2 | HBM2 | GDDR5 | GDDR5 |'
  prefs: []
  type: TYPE_TB
- en: '| Size | 32,152 MiB | 16,276 MiB | 8,155 MiB | 12,237 MiB |'
  prefs: []
  type: TYPE_TB
- en: '| Theoretical bandwidth | 900 GiB/s | 732 GiB/s | 160 GiB/s | 240 GiB/s |'
  prefs: []
  type: TYPE_TB
- en: In general, the preceding observations have helped CUDA applications to run
    faster with the newer architectures. But in parallel, some fundamental changes
    were also brought to the CUDA programming model, as well as the memory architecture,
    to make life easy for CUDA programmers. One such change we observed was for texture
    memory where, prior to CUDA 5.0, the developer had to manually bind and unbind
    the textures and had to be declared globally. With CUDA 5.0, it was not necessary
    to do so. It also removed the restrictions on the number of texture references
    a developer could have in an application.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also looked at the Volta architecture and some of the fundamental changes
    that were made to simplify programming for developers. The total capacity in Volta
    is 128 KB/SM, which is seven times more than its previous generation card, Pascal
    P100, which makes larger caches available for developers. Also, since the L1 cache
    in the Volta architecture has much less latency due to unification, this makes
    it a high-bandwidth and low-latency access to frequently reused data. The key
    reason to do this is to allow L1 cache operations to attain the benefits of shared
    memory performance. The key problem with shared memory is that it needs to be
    explicitly controlled by developers. This becomes less necessary when working
    with newer architectures such as Volta. This does not mean that shared memory
    becomes redundant, however. Ninja programmers who want to extract every inch of
    performance still prefer to use shared memory, but many other applications do
    not require this expertise anymore. The difference between the Pascal and Volta
    L1 cache and shared memory is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/675c799b-6de1-445d-affd-9ddd781c90f8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram shows the unification of shared memory and the L1 cache
    compared to Pascal. It is important to understand that the CUDA programming model
    has remained almost constant from its inception. Even though the memory's capacity,
    bandwidth, or latency changes with every architecture, the same CUDA code will
    run on all architectures. What will definitely change, though, is the impact of
    performance in terms of these architectural changes. For example, an application
    that made use of shared memory before Volta and used to see performance gain compared
    to using global memory might not see such a speedup in Volta because of the unification
    of L1 and shared memory.
  prefs: []
  type: TYPE_NORMAL
- en: Why do GPUs have caches?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this evolution process, it is also important to understand that CPU and GPU
    caches are very different and serve a different purpose. As part of the CUDA architecture,
    we usually launch hundreds to thousands of threads per SM. Tens of thousands of
    threads share the L2 cache. So, L1 and L2 are small per thread. For example, at
    2,048 threads/SM with 80 SM, each thread gets only 64 bytes at L1 and 38 Bytes
    at L2 per thread. Caches in GPU cache common data that's accessed by many threads.
    This is sometimes referred to as spatial locality. A typical example of this is
    when accesses by threads are unaligned and irregular. The GPU cache can help to
    reduce the effect of register spills and local memory since the CPU cache is primarily
    for temporal locality.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started this chapter by providing an introduction to the different types
    of GPU memory. We went into detail about the global, texture, and shared memories,
    as well as registers. We also looked at what new features the GPU's memory evolution
    has provided, such as unified memory, which helps to improve the programmer's
    productivity. We saw how these features are implemented in the latest GPU architectures,
    such as Pascal and Volta.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will go into the details of CUDA thread programming
    and how to optimally launch different thread configurations to get the best performance
    out of GPU hardware. We will also be introducing new CUDA Toolkit features such
    as cooperative groups for flexible thread programming and multi-precision programming
    on GPUs.
  prefs: []
  type: TYPE_NORMAL
