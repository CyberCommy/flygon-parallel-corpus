- en: Web Scraping Using Scrapy and Beautiful Soup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have learned about web-development technologies, data-finding techniques,
    and accessing various Python libraries to scrape data from the web.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will be learning about and exploring two Python libraries
    that are popular for document parsing and scraping activities: Scrapy and Beautiful
    Soup.
  prefs: []
  type: TYPE_NORMAL
- en: Beautiful Soup deals with document parsing. Parsing a document is done for element
    traversing and extracting its content. Scrapy is a web crawling framework written
    in Python. It provides a project-oriented scope for web scraping. Scrapy provides
    plenty of built-in resources for email, selectors, items, and so on, and can be
    used from simple to API-based content extraction.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn about the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Web scraping using Beautiful Soup
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Web scraping using Scrapy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying a web crawler (learning how to deploy scraping code using [https://www.scrapinghub.com](https://www.scrapinghub.com)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A web browser (Google Chrome or Mozilla Firefox) is required and we will be
    using the application and Python libraries listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: Latest Python 3.7* or Python 3.0* (installed)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Python libraries required are the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lxml`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`requests`, ``urllib``'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bs4` or `beautifulsoup4`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scrapy`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For setting up or installation refer to [Chapter 2](b9919ebf-2d5c-4721-aa76-5c1378262473.xhtml),
    *Python and the Web – Using urllib and Requests*, *Setting things up* section.
  prefs: []
  type: TYPE_NORMAL
- en: Code files are available online at GitHub: [https://github.com/PacktPublishing/Hands-On-Web-Scraping-with-Python/tree/master/Chapter05](https://github.com/PacktPublishing/Hands-On-Web-Scraping-with-Python/tree/master/Chapter05).
  prefs: []
  type: TYPE_NORMAL
- en: Web scraping using Beautiful Soup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Web scraping is a procedure for extracting data from web documents. For data
    collection or extracting data from web documents, identifying and traversing through
    elements (of HTML, XML) is the basic requirement. Web documents are built with
    various types of elements that can exist either individually or nested together.
  prefs: []
  type: TYPE_NORMAL
- en: Parsing is an activity of breaking down, exposing, or identifying the components
    with contents from any given web content. Such activity enhances features such
    as searching and collecting content from the desired element or elements. Web
    documents obtained, parsed, and traversed through looking for required data or
    content is the basic scraping task.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 3](9e1ad029-726f-4ed3-897a-c68bcd61f71e.xhtml), *Using LXML, XPath,
    and CSS Selectors*, we explored lxml for a similar task and used XPath and CSS
    Selectors for data-extraction purposes. lxml is also used for scraping and parsing
    because of its memory-efficient features and extensible libraries.
  prefs: []
  type: TYPE_NORMAL
- en: In the next subsection, we will learn and explore features of the Python `bs4` library (for
    Beautiful Soup).
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Beautiful Soup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Beautiful Soup is generally identified as a parsing library, and is also known
    as an HTML parser that is used to parse web documents either in HTML or XML. It
    generates a parsed tree similar to lxml (ElementTree), which is used to identify
    and traverse through elements to extract data and perform web scraping.
  prefs: []
  type: TYPE_NORMAL
- en: Beautiful Soup provides complete parsing-related features that are available
    using `lxml` and `htmllib`. Collections of simple and easy-to-use methods, plus
    properties to deal with navigation, searching, and parsing-related activity, make
    Beautiful Soup a favorite among other Python libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Document encoding can be handled manually using the Beautiful Soup constructor,
    but Beautiful Soup handles encoding-related tasks automatically unless specified
    by the constructor.
  prefs: []
  type: TYPE_NORMAL
- en: One of the distinguishing features of Beautiful Soup, over other libraries and
    parsers, is that it can also be used to parse broken HTML or files with incomplete
    or missing tags. For more information on Beautiful Soup, please visit [https://www.crummy.com/software/BeautifulSoup](https://www.crummy.com/software/BeautifulSoup)*.*
  prefs: []
  type: TYPE_NORMAL
- en: Let's now explore and learn some of the major tools and methods relevant to
    the data-extraction process using Beautiful Soup.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring Beautiful Soup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Python `bs4` library contains a `BeautifulSoup` class, which is used for
    parsing. For more details on Beautiful Soup and installing the library, please
    refer to the official documentation on installing Beautiful Soup at [https://www.crummy.com/software/BeautifulSoup/](https://www.crummy.com/software/BeautifulSoup/).
    On successful installation of the library, we can obtain the details as shown
    in the following screenshot, using Python IDE:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/215ed3ea-bb33-426e-8a7f-e2859bbc5e07.png)'
  prefs: []
  type: TYPE_IMG
- en: Successful installation of bs4 with details
  prefs: []
  type: TYPE_NORMAL
- en: Also, the collection of simple (named) and explainable methods available as
    seen in the preceding screenshot and encoding support makes it more popular among
    developers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s import `BeautifulSoup` and `SoupStrainer` from `bs4`, as seen in the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We will be using the HTML as shown in the following snippet or `html_doc` as
    a sample to explore some of the fundamental features of Beautiful Soup. The response
    obtained for any chosen URL, using `requests` or `urllib`, can also be used for
    content in real scraping cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'To proceed with parsing and accessing Beautiful Soup methods and properties,
    a Beautiful Soup object, generally known as a soup object, must be created. Regarding
    the type of string or markup content provided in the constructor, a few examples
    of creating Beautiful Soup objects, along with the parameters mentioned earlier,
    are listed next:'
  prefs: []
  type: TYPE_NORMAL
- en: '`soup = Beautifulsoup(html_markup)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`soup = Beautifulsoup(html_markup, ''lxml'')`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`soup = Beautifulsoup(html_markup, ''lxml'', parse_from=SoupStrainer("a"))`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`soup = Beautifulsoup(html_markup, ''html.parser'')`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '``soup = Beautifulsoup(html_markup, ''html5lib'')``'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`soup = Beautifulsoup(xml_markup, ''xml'')`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`soup = Beautifulsoup(some_markup, from_encoding=''ISO-8859-8'')`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`soup = Beautifulsoup(some_markup, exclude_encodings=[''ISO-8859-7''])`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Beautiful Soup constructor plays an important part and we will explore
    some of the important parameters here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`markup`: The first parameter passed to the constructor accepts a string or
    objects to be parsed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`features`: The name of the parser or type of markup to be used for `markup`.
    The parser can be `lxml`, `lxml-xml`, `html.parser`, or `html5lib`. Similarly,
    markup types that can be used are `html`, `html5`, and `xml`. Different types
    of supported parsers can be used with Beautiful Soup. If we just want to parse
    some HTML, we can simply pass the markup to Beautiful Soup and it will use the
    appropriate parser installed accordingly. For more information on parsers and
    their installation, please visit installing a parser at [https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`parse_only`: Accepts a `bs4.SoupStrainer` object, that is, only parts of the
    document matching the `SoupStrainer` object will be used to parse. It''s pretty
    useful for scraping when only part of the document is to be parsed considering
    the effectiveness of the code and memory-related issues. For more information
    on `SoupStrainer`, please visit parsing only part of a document at [https://www.crummy.com/software/BeautifulSoup/bs4/doc/#parsing-only-part-of-a-document](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#parsing-only-part-of-a-document).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`from_encoding`: Strings indicating the proper encoding are used to parse the
    markup. This is usually provided if Beautiful Soup is using the wrong encoding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`exclude_encodings`: A list of strings indicating the wrong encodings if used
    by Beautiful Soup.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Response time is a considerable factor when using Beautiful Soup. As Beautiful
    Soup uses the parsers (`lxml`, `html.parser`, and `html5lib`), there is always
    a concern regarding the extra time consumption. Using a parser is always recommended
    to obtain similar results across platforms and systems. Also, for speeding up,
    it is recommended to use `lxml` as the parser with Beautiful Soup.
  prefs: []
  type: TYPE_NORMAL
- en: For this particular case, we will be creating the `soupA` object using `lxml` as
    a parser, along with the `SoupStrainer` object `tagsA` (parsing only `<a>`, that
    is, the elements or anchor tag of HTML). We can obtain partial content to parse
    using `SoupStrainer`, which is very useful when dealing with heavy content.
  prefs: []
  type: TYPE_NORMAL
- en: '`soupA`, an object of Beautiful Soup, presents all of the `<a>` elements found
    for the `SoupStrainer` object `tagsA`as used in the following code; as seen in
    the output, only the `<a>` tag has been collected, or the parsed document is the `SoupStrainer` object parsed
    using `lxml`*:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: HTML content, available from the website, might not always be formatted in a
    clean string. It would be difficult and time-consuming to read page content that
    is presented as paragraphs rather than as a line-by-line code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Beautiful Soup `prettify()` function returns a Unicode string, presents
    the string in a clean, formatted structure that is easy to read, and identifies
    the elements in a tree structure as seen in the following code; the `prettify()` function
    also accepts the parameter encoding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Document-based elements (such as HTML tags) in a parsed tree can have various
    attributes with predefined values. Element attributes are important resources
    as they provide identification and content together within the element. Verifying
    whether the element contains certain attributes can be handy when traversing through
    the tree.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, as seen in the following code, the HTML `<a>` element contains
    the `class`, `href`, and `id` attributes, each carrying predefined values, as
    seen in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The `has_attr()` function from Beautiful Soup returns a Boolean response to
    the searched attribute name for the chosen element, as seen in the following code
    element `a`:'
  prefs: []
  type: TYPE_NORMAL
- en: Returns `False` for the `name` attribute
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns `True` for the `class` attribute
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can use the `has_attr()` function to confirm the attribute keys by name,
    if it exists inside the parsed document as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: With a basic introduction to Beautiful Soup and a few methods explored in this
    section, we will now move forward for searching, traversing, and iterating through
    the parsed tree looking for elements and their content in the upcoming section.
  prefs: []
  type: TYPE_NORMAL
- en: Searching, traversing, and iterating
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Beautiful Soup provides a lot of methods and properties to traverse and search
    elements in the parsed tree. These methods are often named in a similar way to
    their implementation, describing the task they perform. There are also a number
    of properties and methods that can be linked together and used to obtain a similar
    result.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `find()` function returns the first child that is matched for the searched
    criteria or parsed element. It''s pretty useful in scraping context for finding
    elements and extracting details, but only for the single result. Additional parameters
    can also be passed to the `find()` function to identify the exact element, as
    listed:'
  prefs: []
  type: TYPE_NORMAL
- en: '`attrs`: A dictionary with a key-value pair'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text`: With element text'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`name`: HTML tag name'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s implement the `find()` function with different, allowed parameters in
    the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a list of short descriptions of codes implemented in the preceding
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '`find("a") or find(name="a")`: Search the HTML `<a>` element or tag name provided
    that `a` returns the first existence of `<a>` found in `soupA`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`find("a",attrs={''class'':''sister''})`: Search element `<``a>`, with attribute
    key as class and value as sister'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`find("a",attrs={''class'':''sister''}, text="Lacie")`: Search the `<a>` element with
    the `class` attribute key and the `sister` value and text with the `Lacie` value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`find("a",attrs={''id'':''link3''})`: Search the `<a>` element with the `id` attribute
    key and the `link3` value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`find("a",id="link2")`: Search the `<a>` element for the `id` attribute with
    the `link2` value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `find_all()` function works in a similar way to the `find()` function with
    the additional `attrs` and `text` as a parameters and returns a list of matched
    (multiple) elements for the provided criteria or `name` attribute as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The additional `limit` parameter, which accepts numeric values, controls the total
    count of the elements to be returned using the `find_all()` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The string, list of strings, regular expression objects, or any of these, can
    be provided to the `name` and `text` attributes as a value for `attrs` parameters,
    as seen in the code used in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The `find_all()` function has in-built support for global attributes such as
    class name along with a name as seen in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Multiple `name` and `attrs` values can also be passed through a list as shown
    in the following syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '`soup.find_all("p",attrs={''class'':["title","story"]})`: Finding all the `<p>`
    elements with the class attribute `title` and `story` values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`soup.find_all(["p","li"])`: Finding all the `<p>` and `<li>` elements from
    the soup object'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The preceding syntax can be observed in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also use element text to search and list the content. A `string` parameter,
    similar to a `text` parameter, is used for such cases; it can also be used with,
    or without, any tag names as in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Iteration through elements can also be achieved using the `find_all()` function.
    As can be seen in the following code, we are retrieving all of the `<li>` elements
    found inside the `<ul>` element and printing their tag name, attribute data, ID,
    and text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The elements `value` attribute can be retrieved using the `get()` function as
    seen in the preceding code. Also, the presence of attributes can be checked using
    the `has_attr()` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Element traversing can also be done with just a tag name, and with, or without,
    using the `find()` or `find_all()` functions as seen in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The `text` and `string` attributes or the `get_text()` method can be used with
    the elements to extract their text while traversing through the elements used
    in the following code. There''s also a parameter `text` and `string` in the `find()`
    or `find_all()` functions, which are used to search the content as shown in the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we explored searching and traversing using elements and by
    implementing important functions such as the `find()` and `find_all()` functions
    alongside their appropriate parameters and criteria.
  prefs: []
  type: TYPE_NORMAL
- en: In the next sections, we will explore elements based on their positions in the
    parsed tree.
  prefs: []
  type: TYPE_NORMAL
- en: Using children and parents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For parsed documents, traversing through children or child elements can be
    achieved using the `contents`, `children`, and `descendants` elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '`contents` collect children for the provided criteria in a list.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`children` are used for iteration that has direct children.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`descendants` work slightly differently to the `contents` and `children`elements.
    It allows iteration over all children, not just the direct ones, that is, the
    element tag and the contents inside the tag are actually two separate children.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The preceding list showed the features that can also be used for iteration.
    The following code illustrates the use of these features with output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Selected `children` and `descendants` tag names can be obtained using the `name` attribute.
    Parsed strings and the `\n` function (newline) are returned as `None`, which can
    be filtered out, as in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Similar to the `find()` and `find_all()` functions, we can also traverse child
    elements using the `findChild()` and `findChildren()` functions. The `findChild()` function
    is used to retrieve the single child and the `findChildren()` function retrieves
    a list of children as illustrated in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Similar to the `children` element, the`parent`element returns the parent object
    found for the searched criteria. The main difference here is that the `parent` element
    returns the single parent object from the tree as seen in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The limitation of the single parents returned can be overcome by using the `parents` element;
    this returns multiple existing parent elements and matches the searched criteria
    provided in the `find()` function as seen in code here, which is normally used
    for iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: As seen in the preceding output, `[document]` refers to the soup object and
    `html` refers to the complete HTML block found in the soup. The Beautiful Soup
    object that created itself is a parsed element.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the functions that exist for child traversing, parents can also be
    traversed and retrieved using the `findParent()` and `findParents()` search functions.
    The `findParent()` function traverses to the immediate parent, while the `findParents()` function
    returns all parents found for the criteria provided.
  prefs: []
  type: TYPE_NORMAL
- en: 'It must also be noted that the children and parent traversing functions are
    used with the `find()` function where necessary arguments and conditions are provided,
    as seen in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We explored traversing and searching with the children and parent element using
    a varied handful of functions. In the next section, we'll explore and use positional
    elements from the parsed tree.
  prefs: []
  type: TYPE_NORMAL
- en: Using next and previous
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to traversing through parsed children and parents in the tree, Beautiful
    Soup also has the support to traverse and iterate elements located previous to
    and next to the provided criteria.
  prefs: []
  type: TYPE_NORMAL
- en: 'The properties `next` and `next_element` return the immediately parsed content
    for the selected criteria. We can also append the `next` and `next_element` functions
    to create a chain of code for traversal, as seen in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Similar to the `next `and `next_elements` functions, there also exist properties
    with traversal result that returns results from prior or previous parsed elements,
    such as the `previous `and `previous_element`, which are opposite to work reversely
    when compared to the `next `and `next_element` functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'As seen in the following code, the `previous `and `previous_element` can also
    be appended to themselves to create a traversal series:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We now combine the `next `or `next_element` and `previous `or `previous_element` elements
    together to traverse as seen in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Iterating features for the `next_element` and `previous_element` are obtained
    using the `next_elements` and `previous_elements`, respectively. These iterators
    are used to move to the next or previous parsed content as seen in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The `find_next()` function implements the `next_elements` but returns only a
    single element that is found after the `next` or `next_element` element. There's
    also an advantage of using the `find_next()` function over the `next_elements` as
    we can implement additional search logic for elements.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code demonstrates the use of the `find_next()` function, with,
    and without, search conditions; it also displays the outputs from the `next` element
    and `next_elements` to compare the actual usage as shown in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The **`find_all_next()` **function works in a similar way to the `find_next()` function,
    but returns all of the next elements. It''s also used as an iterating version
    of the `find_next()` function. Additional search criteria and arguments such as `limit` can
    be used to search and control the results returned as used in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The `find_previous()` function implements `previous``_elements` but returns
    only the single element that was found before the `previous` or `previous_element`.
    It also has an advantage over the `previous``_elements` as we can implement additional
    search logic for elements. The following code demonstrates the use of the `find_previous()` function
    and the `previous` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The `find_all_previous()` function is an iterated version of the `find_previous()`;
    it returns all previous elements satisfied with the available criteria as seen
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '`next_sibling` and `previous_sibling` are yet another way of traversing along
    the parsed tree looking for next and previous siblings. A sibling or siblings
    are termed to the element that appears or is found on the same level, in the parsed
    tree or those elements that share the same parent. The following code illustrates
    the use of the `next_sibling` and `previous_sibling` elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Iteration is also possible with siblings, using the `next_siblings` and `previous_siblings` elements
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Similar to the `find_next()` and `find_all_next()` functions for the next elements,
    there's also functions available for siblings, that is,
  prefs: []
  type: TYPE_NORMAL
- en: 'the `find_next_sibling()` and `find_next_siblings()` functions. These functions
    implement the `next_siblings` function to iterate and search for available siblings.
    As seen in following code, the `find_next_sibling()` function returns a single
    element, whereas the `find_next_siblings()` function returns all matched siblings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The `find_previous_sibling()` and `find_previous_siblings()` functions work
    in a similar way to the `find_next_sibling()` and `find_next_siblings()` functions,
    but result in elements traced through the `previous_siblings` function. Additional
    search criteria and a result-controlling parameter `limit` can also be applied
    to the iterating version, such as the `find_previous_siblings()` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'As seen in the following code, the `find_previous_sibling()` function returns
    a single sibling element, whereas the `find_previous_siblings()` function returns
    all siblings available previously to the given criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: We have explored various ways of searching and traversing through the parsed
    tree with the functions and properties explored in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a list of tips that can be helpful in remembering and planning
    for search and traversing activities using Beautiful Soup:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A function name that starts with the `find` function is used to search and
    iterate for providing criteria and parameters:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A plural version of the `find` function works for iteration, such as the `findChildren()` and `findParents()` elements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A singular version of the `find` function returns a single element such as the `find()`,
    `findChild()`, or `findParent()` functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A function name that starts with the word `find_all` returns all matched elements and is
    used to search and iterate with provided criteria and parameters such as the `find_all()`,
    `find_all_next()`, and `find_all_previous()` functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Properties with a plural name are used for iteration purposes such as the `next_elements`,
    `previous_elements`, `parents`, `children`, `contents`, `descendants`, `next_siblings`,
    and `previous_siblings` elements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Properties with a singular name return single elements and can also be appended
    to form a chain of traversal code such as the `parent`, `next`, `previous`, `next_element`,
    `previous_element`, `next_sibling`, and `previous_sibling` functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using CSS Selectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have used plenty of properties and functions in the preceding sections, looking
    for desired elements and their content. Beautiful Soup also supports CSS Selectors
    (with library SoupSieve at [https://facelessuser.github.io/soupsieve/selectors/](https://facelessuser.github.io/soupsieve/selectors/)),
    which enhances its use and allows developers to write effective and efficient
    codes to traverse the parsed tree.
  prefs: []
  type: TYPE_NORMAL
- en: CSS Selectors (CSS query or CSS Selector query) are defined patterns used by
    CSS to select HTML elements, by element name or by using global attributes (`ID`,
    `Class`). For more information on CSS Selectors, please refer to [Chapter 3](9e1ad029-726f-4ed3-897a-c68bcd61f71e.xhtml),
    *Using LXML, XPath and CSS Selectors*, *Introduction to XPath and CSS Selector*
    section.
  prefs: []
  type: TYPE_NORMAL
- en: For Beautiful Soup, the `select()` function is used to execute the CSS Selectors.
    We can perform the searching, traversing, and iteration of elements by defining
    CSS Selectors. The `select()` function is implemented individually, that is, it
    is not extended with other functions and properties found in Beautiful Soup, creating
    a chain of codes. The `select()` function returns a list of elements matched to
    the CSS Selectors provided. It's also notable that code using CSS Selectors are
    quite short in length compared to the code used in the preceding sections for
    a similar purpose.
  prefs: []
  type: TYPE_NORMAL
- en: We will explore a few examples using `select()` to process CSS Selectors.
  prefs: []
  type: TYPE_NORMAL
- en: Example 1 – listing <li> elements with the data-id attribute
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following example, we will use the `select()` function to list the `<li>` element
    with the `data-id` attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'As seen in the preceding code, the `li[data-id]` selector queries the `<li>` element with
    the attribute key named as `data-id`. The Value for `data-id` is empty, which
    allows traversing through all `<li>` possessing `data-id`. The result is obtained
    as a list of objects, in which indexes can be applied to fetch the exact elements
    as seen in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'If we wish to extract the first match that has resulted the from CSS query,
    we can use either the list index, that is, `0` (zero) or the `select_one()` function in
    place of the `select()` function as seen in the following code. The `select_one()` function
    returns the string of objects, not the list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Example 2 – traversing through elements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'CSS Selectors have various combinators such as +, >, a space character, and
    so on, which show relationships between the elements. A few such combinators are
    used in the following example code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Example 3 – searching elements based on attribute values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are various ways of finding elements in Beautiful Soup, such as using
    functions starting with the word `find` or using attributes in CSS Selectors.
    Patterns can be searched for attributes keys using `*` in CSS Selectors as illustrated
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: We were searching for the `<a>` element with the text `example.com`, which might
    exist in the value of the `href` attribute. Also, we were searching for the `<a>` element,
    which contains an attribute ID with a text link.
  prefs: []
  type: TYPE_NORMAL
- en: With basic knowledge of CSS Selectors, we can deploy it with Beautiful Soup
    for various purposes. Using the `select()` function is quite effective when dealing
    with elements, but there are also limitations we might face, such as extracting
    text or content from the obtained element.
  prefs: []
  type: TYPE_NORMAL
- en: We have introduced and explored the elements of Beautiful Soup in the preceding
    sections. To wrap up the concept, we will create a crawler example in the upcoming
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Building a web crawler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will build a web crawler to demonstrate the real content-based
    scraping, targeting web content.
  prefs: []
  type: TYPE_NORMAL
- en: We will be scraping quotes from [http://toscrape.com/](http://toscrape.com/) and
    targeting quotes from authors found at [http://quotes.toscrape.com/](http://quotes.toscrape.com/).
    The crawler will collect the quote and author information from the first five
    listing pages and write the data into a CSV file. We will also explore the individual
    author page and extract information about the authors.
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin with the basic planning and identification of the fields that we are
    willing to collect information from, please refer to [Chapter 3](9e1ad029-726f-4ed3-897a-c68bcd61f71e.xhtml),
    *Using LXML, XPath, and CSS Selectors*, *Using web browser developer tools for
    accessing web content* section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code there are a few libraries and objects found as listed
    and described here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sourceUrl`: Represents the URL of the main page to be scraped for data for
    category web scraping'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keys`: The Python list contains the columns name that will be used while writing
    records to an external file'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`requests`: This library is imported to use for making an HTTP request to page
    URLs with quote listings and receiving a response'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`csv`: This library will be used to write scraped data to an external CSV file'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bs4`: Library for implementing and using Beautiful Soup'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first line in a CSV file contains column names. We need to write these columns
    before appending records with real content in the CSV file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `read_url()` function, as found in the following code, will be used to
    make a request and receive a response using the `requests` function. This function
    will accept a `url` argument for pages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '`dataSet `is a handle defined to manage the external file `quotes.csv`. `csv.writer()` file
    handle is use for accessing CSV-based properties. The `writerow()` function is
    passed with keys for writing a row containing the column names from the list keys to
    the external file as shown in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The implemented `get_details()` function is being coded for pagination and
    scraping logic. The `read_url()` function is supplied with a dynamically generated
    page URL to manage the pagination as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'As used in the following code, the `response` element from the `read_url()` function is
    parsed using `lxml` to obtain the `soup` element. The rows obtained using the
    soup list all of the quotes available in a single page (that is, the element block
    containing the single quote details) found inside the `<div class="quote">` function
    and will be iterated to scrape data for individual items such as `quote_tags`,
    `author_url`, and `author_name` traversing through the quote element:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/6768697a-286f-43c5-87e0-e4b42d3f5c83.png)'
  prefs: []
  type: TYPE_IMG
- en: Page source with quote element
  prefs: []
  type: TYPE_NORMAL
- en: The individual items received are scraped, cleaned, and collected in a list
    maintaining the order of their column names and are written to the file using
    the `writerow()` function (appends the list of values to the file) accessed through
    the `csv` library and file handle.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `quotes.csv` data file will contain scraped data as seen in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/211306fb-7fd4-4914-b588-d35f0694b630.png)'
  prefs: []
  type: TYPE_IMG
- en: Rows with scraped data from http://quotes.toscrape.com/
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we explored various ways to traverse and search using Beautiful
    Soup. In the upcoming section, we will be using Scrapy, a web crawling framework.
  prefs: []
  type: TYPE_NORMAL
- en: Web scraping using Scrapy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have used and explored various libraries and techniques for web scraping
    so far in this book. The latest libraries available adapt to new concepts and
    implement the techniques in a more effective, diverse, and easy way; Scrapy is
    among one of those libraries.
  prefs: []
  type: TYPE_NORMAL
- en: We will be introducing and using Scrapy (an open source web crawling framework
    written in Python) in this section. For more detailed information on Scrapy, please
    visit the official documentation at [http://docs.scrapy.org/en/latest/](http://docs.scrapy.org/en/latest/).
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will be implementing scraping features and building a project
    demonstrating useful concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Scrapy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scrapy is a web crawling framework written in Python used for crawling websites
    with effective and minimal coding. According to the official website of Scrapy
    ([https://scrapy.org/](https://scrapy.org/)), it is <q>"An open source and collaborative
    framework for extracting the data you need from websites. In a fast, simple, yet
    extensible way."</q>
  prefs: []
  type: TYPE_NORMAL
- en: Scrapy provides a complete framework that is required to deploy a crawler with
    built-in tools. Scrapy was originally designed for web scraping; with its popularity
    and development, it is also used to extract data from APIs. Scrapy-based web crawlers
    are also easy to manage and maintain because of their structure. In general, Scrapy provides
    a project-based scope for projects dealing with web scraping.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some of the features and distinguishable points that make
    Scrapy a favorite among developers:'
  prefs: []
  type: TYPE_NORMAL
- en: Scrapy provides built-in support for document parsing, traversing, and extracting
    data using XPath, CSS Selectors, and regular expressions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The crawler is scheduled and managed asynchronously allowing multiple links
    to be crawled at the same time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It automates HTTP methods and actions, that is, there's no need for importing
    libraries such as `requests` or `urllib` manually for code. Scrapy handles requests
    and responses using its built-in libraries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's built-in support for feed export, pipelines (items, files, images, and
    media), that is, exporting, downloading, and storing data in JSON, CSV, XML, and
    database.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The availability of the middleware and the large collection of built-in extensions
    can handle cookies, sessions, authentication, `robots.txt`, logs, usage statistics,
    email handling, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scrapy-driven projects are composed of easy-to-use distinguishable components
    and files, which can be handled with basic Python skills and many more.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Please refer to the official documentation of Scrapy at [https://docs.scrapy.org/en/latest/intro/overview.html](https://docs.scrapy.org/en/latest/intro/overview.html) for
    an in-depth and detailed overview.
  prefs: []
  type: TYPE_NORMAL
- en: With a basic introduction to Scrapy, we now begin setting up a project and exploring
    the framework in more detail in the next sections.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will require a Python library with `scrapy` successfully installed on the
    system before proceeding with the project setup. For setting up or installation
    refer to [Chapter 2](b9919ebf-2d5c-4721-aa76-5c1378262473.xhtml), *Python and
    the Web – Using urllib and Requests,* *Setting things up* section or, for more
    details on Scrapy installation, please refer to the official installation guide
    at [https://docs.scrapy.org/en/latest/intro/overview.html](https://docs.scrapy.org/en/latest/intro/overview.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Upon successful installation, we can obtain the details shown in the following
    screenshot, using Python IDE:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/37562e7b-756c-4844-b365-4768f849c1d7.png)'
  prefs: []
  type: TYPE_IMG
- en: Successful installation of Scrapy with details
  prefs: []
  type: TYPE_NORMAL
- en: With the successful installation of the `scrapy` library, there's also the availability
    of the `scrapy` command-line tool. This command-line tool contains a number of
    commands, which are used at various stages of a project from starting or creating
    a project through to it being fully up and running.
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin with creating a project, let''s follow the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Open Terminal or command-line interface
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a folder (`ScrapyProjects`) as shown in the following screenshot or select
    a folder in which to place Scrapy projects
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Inside the selected folder, run or execute the `scrapy` command
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A list of available commands and their brief details will appear, similar to
    the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/e8b9f1ab-2fb1-4bb5-9916-79eb18f96788.png)'
  prefs: []
  type: TYPE_IMG
- en: List of available commands for Scrapy
  prefs: []
  type: TYPE_NORMAL
- en: We will be creating a `Quotes` project to obtain author quotes related to web
    scraping from [http://toscrape.com/](http://toscrape.com/), accessing information
    from the first five pages or less which exists using the URL [http://quotes.toscrape.com/](http://quotes.toscrape.com/).
  prefs: []
  type: TYPE_NORMAL
- en: '[We are now going to start the `Quotes` project. From the Command Prompt, run
    or execute the `scrapy startproject Quotes` command as seen in the following screenshot:](http://quotes.toscrape.com/)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/20cbfc7a-2f6c-4d59-bf1b-b0d6ac50c8a5.png)'
  prefs: []
  type: TYPE_IMG
- en: Starting a project (using command: scrapy startproject Quotes)
  prefs: []
  type: TYPE_NORMAL
- en: 'If successful, the preceding command will be the creation of a new folder named
    `Quotes` (that is, the project root directory) with additional files and subfolders
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/2a93ab9e-a72e-4396-8970-1077983632c8.png)'
  prefs: []
  type: TYPE_IMG
- en: Contents for project folder ScrapyProjects\Quotes
  prefs: []
  type: TYPE_NORMAL
- en: 'With the project successfully created, let''s explore the individual components
    inside the project folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '`scrapy.cfg` is a configuration file in which default project-related settings
    for deployment are found and can be added.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Subfolder will find `Quotes` named same as project directory, which is actually
    a Python module. We will find additional Python files and other resources in this
    module as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](assets/b0dcc05f-a4e5-43b9-a55c-50a790da7441.png)'
  prefs: []
  type: TYPE_IMG
- en: Contents for project folder ScrapyProjects\Quotes\Quotes
  prefs: []
  type: TYPE_NORMAL
- en: 'As seen in the preceding screenshot, the module is contained in the `spiders` folder and
    the `items.py`, `pipelines.py`, and `settings.py` Python files. These content
    found inside the `Quotes` module has specific implementation regarding the project
    scope explored in the following list:'
  prefs: []
  type: TYPE_NORMAL
- en: '`spiders`: This folder will contain Spider classes or Spider writing in Python.
    Spiders are classes that contain code that is used for scraping. Each individual
    Spider class is designated to specific scraping activities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`items.py`: This Python file contains item containers, that is, Python class
    files inheriting `scrapy. Items` are used to collect the scraped data and use
    it inside spiders. Items are generally declared to carry values and receive built-in
    support from other resources in the main project. An item is like a Python dictionary
    object, where keys are fields or objects of `scrapy.item.Field`, which will hold
    certain values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although the default project creates the `items.py` for the item-related task,
    it's not compulsory to use it inside the spider. We can use any lists or collect
    data values and process them in our own way such as writing them into a file,
    appending them to a list, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '`pipelines.py`: This part is executed after the data is scraped. The scraped
    items are sent to the pipeline to perform certain actions. It also decides whether
    to process the received scraped items or drop them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`settings.py`: This is the most important file in which settings for the project
    can be adjusted. According to the preference of the project, we can adjust the
    settings. Please refer to the official documentation from Scrapy for settings
    at [https://scrapy2.readthedocs.io/en/latest/topics/settings.html](https://scrapy2.readthedocs.io/en/latest/topics/settings.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we have successfully created a project and the required files
    using Scrapy. These files will be used and updated as described in the following
    sections.
  prefs: []
  type: TYPE_NORMAL
- en: Generating a Spider
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We need to generate a Spider to collect the data. The Spider will perform the
    crawling activity. An empty default folder named `spiders` does exist inside the `ScrapyProjects\Quotes\Quotes` folder.
  prefs: []
  type: TYPE_NORMAL
- en: From the `ScrapyProjects\Quotes` project folder, run or execute the `scrapy
    genspider quotes quotes.toscrape.com` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Successful execution of the command will create a `quotes.py` file, that is,
    a Spider inside the `ScrapyProjects\Quotes\Quotes\spiders\` path. The generated
    Spider class `QuotesSpider` inherits Scrapy features from `scrapy.Spider`. There''s
    also a few required properties and functions found inside `QuotesSpider` as seen
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The `QuotesSpider` Spider class contains automatically generated properties
    that are assigned for specific tasks, as explored in the following list:'
  prefs: []
  type: TYPE_NORMAL
- en: '`name`: This variable holds value, that is, the name of the Spider quotes as
    seen in the preceding code. The name identifies the Spider and can be used to
    access it. The value of the name is provided through the command-line instructions
    while issuing `scrapy genspider quotes`, which is the first parameter after `genspider`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`allowed_domains`: The created Spiders are allowed to crawl within the listed
    domains found in the `allowed_domains`. The last parameter passed is the `quotes.toscrape.com` parameter,
    while generating a Spider is actually a domain name that will be listed inside
    an `allowed_domains` list.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A domain name passed to `allowed_domains` will generate URLs for `start_urls`.
    If there are any chances of URL redirection, such URL domain names need to be
    mentioned inside the `allowed_domains`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`start_urls`: These contain a list of URLs that are actually processed by Spider
    to crawl. The domain names found or provided to the `allowed_domains` are automatically
    added to this list and can be manually added or updated. Scrapy generates the
    URLs for `start_urls` adding HTTP protocols. On a few occasions, we might also
    need to change or fix the URLs manually, for example, `www` added to the domain
    name needs to be removed. `start_urls` after the update will be seen as in the
    following code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '`parse()`: This function is implemented with the logic relevant to data extraction
    or processing. `parse()` acts as a main controller and starting point for scraping
    activity. Spiders created for the main project will begin processing the provided
    URLs or `start_urls` from, or inside, the `parse()`. XPath-and CSS Selector-related
    expressions and codes are implemented, and extracted values are also added to
    the item (that is, the `QuotesItem` from the `item.py` file).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can also verify the successful creation of Spider by executing these commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '`scrapy list`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scrapy list spide*r*`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Both of these commands will list the Spider displaying its name, which is found
    inside the `spiders` folder as seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/af5972eb-6066-4dc7-9fb7-28298299376f.png)'
  prefs: []
  type: TYPE_IMG
- en: Listing Spiders from Command Prompt
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have generated a Spider named `quotes` for our scraping
    task. In the upcoming section, we will create Item fields that will work with
    Spider and help with collecting data.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an item
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Proceeding with the scraping task and the project folder, we will find a file
    named `item.py` or item, containing the Python class `QuotesItem`. The item is
    also automatically generated by Scrapy while issuing the `scrapy startproject
    Quotes` command. The `QuotesItem` class inherits the `scrapy.Item` for built-in
    properties and methods such as the `Field`. The `Item` or `QuotesItem` in Scrapy
    represents a container for collecting values and the `Fields` listed as shown
    in the following code, including quotes, tags, and so on, which will acts as the
    keys to the values which we will obtain using the `parse()` function. Values for
    the same fields will be extracted and collected across the found pages.
  prefs: []
  type: TYPE_NORMAL
- en: 'The item is accessed as a Python dictionary with the provided fields as keys
    with their values extracted. It''s effective to declare the fields in the item
    and use them in Spider but is not compulsory to use `item.py` as shown in the
    following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to import the `QuotesItem` when the item is required inside the Spider,
    as seen in the following code, and process it by creating an object and accessing
    the declared fields, that is, `quote`, `tags`, `author`, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we declared the `item` fields that we are willing to retrieve
    data from a website. In the upcoming section, we will explore different methods
    of data extraction and link them to the item fields.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With Spider generated and the item declared with the fields required, we will
    now proceed to extract the values or data required for specific item fields. Extraction-related
    logic can be applied using XPath, CSS Selectors, and regular expressions and we
    also can implement Python-related libraries such as `bs4` (Beautiful Soup), `pyquery`,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: With proper `start_urls` and item (`QuotesItem`) being set up for the Spider
    to crawl, we can now proceed with the extraction logic using `parse()` and using selectors
    at [https://docs.scrapy.org/en/latest/topics/selectors.html](https://docs.scrapy.org/en/latest/topics/selectors.html).
  prefs: []
  type: TYPE_NORMAL
- en: Using XPath
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `parse()` function inside Spider is the place to implement all logical processes
    for scraping data. As seen in the following code, we are using XPath expressions
    in this Spider to extract the values for the required fields in `QuotesItem`.
  prefs: []
  type: TYPE_NORMAL
- en: For more information on XPath and obtaining XPath Query, using browser-based
    developer tools, please refer to [Chapter 3](9e1ad029-726f-4ed3-897a-c68bcd61f71e.xhtml),
    *Using LXML, XPath and CSS Selectors*, *XPath and CSS Selectors using DevTools*
    section. Similarly, for more information on the `pyquery` Python library, please
    refer to [Chapter 4](30c30342-63a5-4452-9f61-a05a2e69e256.xhtml), *Scraping Using
    pyquery – a Python Library.*
  prefs: []
  type: TYPE_NORMAL
- en: As seen in the next code snippet an `item` object from `QuotesItem` is used
    to collect individual field-related data and it's finally being collected and
    iterated using the Python keyword `yield`. `parse()` is actually a generator that
    is returning object `item` from `QuotesItem`.
  prefs: []
  type: TYPE_NORMAL
- en: Python keyword `yield` is used to return a generator. Generators are functions
    that return an object that can be iterated. The Python function can be treated
    as a generator using the yield in place of the return.
  prefs: []
  type: TYPE_NORMAL
- en: '`parse()` has an additional argument `response`; this is an object of `scrapy.http.response.html.HtmlResponse` that
    is returned by Scrapy with the page content of the accessed or crawled URL. The
    response obtained can be used with XPath and CSS Selectors for further scraping
    activities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: As seen in the following code, the XPath expression is being applied to the
    response using the `xpath()` expression and is used as a `response.xpath()`. XPath
    expressions or queries provided to `response.xpath()` are parsed as rows, that
    is, an element block containing the desired elements for fields.
  prefs: []
  type: TYPE_NORMAL
- en: 'The obtained rows will be iterated for extracting individual element values
    by providing the XPath query and using additional functions as listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`extract()`: Extract all the elements matching the provided expression.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`extract_first()`: Extract only the first element that matches the provided
    expression.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`strip()`: Clears the whitespace characters from the beginning and the end
    of the string. We need to be careful using this function to the extracted content
    if they result in a type other than string such as `NoneType` or `List`, and so
    on as it can result in an error.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we have collected quotes listings details using XPath; in the
    next section, we will cover the same process but using CSS Selectors.
  prefs: []
  type: TYPE_NORMAL
- en: Using CSS Selectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will be using CSS Selectors with their extensions such
    as `::text` and `::attr` along with `extract()` and `strip()`. Similar to `response.xpath()`,
    available to run XPath expressions, CSS Selectors can be run using `response.css()`.
    The `css()` selector matches the elements using the provided expressions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: As seen in the preceding code, `rows` represent individual elements with the `post-item` class, iterated
    for obtaining the `Item` fields.
  prefs: []
  type: TYPE_NORMAL
- en: For more information on CSS Selectors and obtaining CSS Selectors using browser-based
    development tools, please refer to [Chapter 3](9e1ad029-726f-4ed3-897a-c68bcd61f71e.xhtml), *Using
    LXML, XPath, and CSS Selectors*, *CSS Selectors* section and *XPath and CSS Selectors
    using DevTools* section, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: For more detailed information on selectors and their properties, please refer
    to the Scrapy official documentation on selectors at [https://docs.scrapy.org/en/latest/topics/selectors.html](https://docs.scrapy.org/en/latest/topics/selectors.html).
    In the upcoming section, we will learn to scrape data from multiple pages.
  prefs: []
  type: TYPE_NORMAL
- en: Data from multiple pages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the preceding section, we tried scraping data for the URL in `start_urls`,
    that is, [http://quotes.toscrape.com/](http://quotes.toscrape.com/). It's also
    to be noted that this particular URL results in quotes listings for the first
    page only.
  prefs: []
  type: TYPE_NORMAL
- en: 'Quotes listings are found across multiple pages and we need to access each
    one of those pages to collect the information. A pattern for pagination links
    is found in the following list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://quotes.toscrape.com/](http://quotes.toscrape.com/) (first page)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://quotes.toscrape.com/page/2/](http://quotes.toscrape.com/page/2/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://quotes.toscrape.com/page/3/](http://quotes.toscrape.com/page/3/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XPath and CSS Selectors used inside the `parse()`, as found in codes from the
    preceding section, will be scraping data from the first page or page 1 only. Pagination
    links found across pages can be requested and extracted by passing the link to
    `parse()` inside Spider using the `callback` argument from a `scrapy.Request`.
  prefs: []
  type: TYPE_NORMAL
- en: 'As seen in the following code, a link to page 2 found on page 1 is extracted
    and passed to `scrapy.Request`, making a request to the `nextPage` processing
    plus yielding the item fields using `parse()`. Similarly, the iteration takes
    place until the link to the next page or `nextPage` exists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: We can also obtain the pagination-based result by making changes only to `start_urls` as
    seen in the code next. Using this process doesn't require the use of `nextPage` or `scrapy.Request` as
    used in the preceding code.
  prefs: []
  type: TYPE_NORMAL
- en: 'URLs to be crawled can be listed inside `start_url` and are recursively implemented
    by `parse()` as seen in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also obtain a list of URLs using the Python list comprehension technique.
    The `range()` function used in the following code accepts the start and end of
    the argument, that is, 1 and 4, and will result in the numbers 1, 2, and 3 as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: With extraction logic along with pagination and the item declared, in the next
    section, we will run the crawler quotes and export the item to the external files.
  prefs: []
  type: TYPE_NORMAL
- en: Running and exporting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We need to run a Spider and look for data for item fields in the provided URLs.
    We can start running the Spider from the command line by issuing the `scrapy crawl
    quotes` command or as seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/bf9bff49-7694-4fc4-b31c-44867857c418.png)'
  prefs: []
  type: TYPE_IMG
- en: Running a Spider (scrapy crawl quotes)
  prefs: []
  type: TYPE_NORMAL
- en: The Scrapy argument crawl is provided with a Spider name (`quotes`) in the command.
    A successful run of the command will result in information about Scrapy, bots,
    Spider, crawling stats, and HTTP methods, and will list the item data as a dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: 'While executing a Spider we will receive various forms of information, such
    as `INFO`/`DEBUG`/`scrapy` statistics and so on, as found in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The Scrapy statistics are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: We can also run the Spider and save the item found or data scraped to the external
    files. Data is exported or stored in files for easy access, usage, and convenience
    in sharing and managing.
  prefs: []
  type: TYPE_NORMAL
- en: 'With Scrapy, we can export scraped data to external files using crawl commands
    as seen in the following list:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To extract data to a CSV file we can use the `C:\ScrapyProjects\Quotes> scrapy
    crawl quotes -o quotes.csv` command as seen in the following screenshot:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](assets/4e5379b9-976f-4d5c-a890-a8e245b2c850.png)'
  prefs: []
  type: TYPE_IMG
- en: Contents from file quotes.csv
  prefs: []
  type: TYPE_NORMAL
- en: 'To extract data to JSON file format, we can use the `C:\ScrapyProjects\Quotes> scrapy
    crawl quotes -o quotes.json` command as seen in the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](assets/a62a3fae-a342-4d81-90a1-09cad3d9d2c2.png)'
  prefs: []
  type: TYPE_IMG
- en: Contents from file quotes.json
  prefs: []
  type: TYPE_NORMAL
- en: The `-o` parameter followed by a filename will be generated inside the main
    project folder. Please refer to the official Scrapy documentation about feed exports
    at [http://docs.scrapy.org/en/latest/topics/feed-exports.html](http://docs.scrapy.org/en/latest/topics/feed-exports.html)
    for more detailed information and file types that can be used to export data.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned about Scrapy and used it to create a Spider to scrape
    data and export the data scraped to external files. In the next section, we will
    deploy the crawler on the web.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a web crawler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deploying a web crawler online or on a live server will certainly improve the
    effectiveness of the crawling activity, with its speed, updated technology, web
    spaces, anytime usage, and so on. Local tests and confirmation are required before
    deploying online. We need to own or buy web spaces with web-hosting companies
    or the cloud server.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scrapy Cloud at [https://scrapinghub.com/scrapy-cloud](https://scrapinghub.com/scrapy-cloud) from
    Scrapinghub at [https://scrapinghub.com/](https://scrapinghub.com/) is one of
    the best platforms to deploy and manage the Scrapy Spider. The Scrapy Cloud provides
    an easy and interactive interface to deploy Scrapy and is free, with some of the
    additional features listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: Coding/managing and running Spider
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying Spider to cloud
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downloading and sharing data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: API access with resource management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are the steps performed to deploy projects using Scrapy Cloud:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the web browser and go to [https://scrapinghub.com/](https://scrapinghub.com/).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'From the navigation menu, select PRODUCTS and choose SCRAPY CLOUD as seen in
    the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/3e9545c1-6223-458e-8aea-ef367303a2ad.png)'
  prefs: []
  type: TYPE_IMG
- en: Scrapinghub products
  prefs: []
  type: TYPE_NORMAL
- en: 'Log in or register on the page loaded from [https://scrapinghub.com/scrapy-cloud](https://scrapinghub.com/scrapy-cloud)
    (or open the login page: [https://app.scrapinghub.com/account/login/](https://app.scrapinghub.com/account/login/)):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/d0711538-4f69-4af7-a9e6-d06cec4bee7e.png)'
  prefs: []
  type: TYPE_IMG
- en: Log in and register page from scraping hub
  prefs: []
  type: TYPE_NORMAL
- en: 'After completing registration and logging in, users are provided with an interactive
    dashboard and an option to CREATE A PROJECT, as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/1c2cec76-2f0d-4308-bb4d-cc2034660438.png)'
  prefs: []
  type: TYPE_IMG
- en: User dashboard
  prefs: []
  type: TYPE_NORMAL
- en: 'Clicking CREATE PROJECT will pop up a window, as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/2437a186-ba19-4040-b946-6b6329f26fd3.png)'
  prefs: []
  type: TYPE_IMG
- en: Create a new project from Scrapy Cloud
  prefs: []
  type: TYPE_NORMAL
- en: Create a project named as seen in the screenshot and choose technology SCRAPY to
    deploy the spiders; click CREATE.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A Dashboard with Scrapy Cloud Projects will be loaded, listing newly created
    projects as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/572b3ba0-d5a4-42ea-8efb-b2654391db66.png)'
  prefs: []
  type: TYPE_IMG
- en: Scrapy Cloud Projects listings with option CREATE PROJECT
  prefs: []
  type: TYPE_NORMAL
- en: To deploy the codes for the created project, select the project listed from
    the Scrapy Cloud Projects listings.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The project dashboard will be loaded with various options. Choose the option
    Code & Deploys:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/dba071c6-eb37-4947-810d-fb45cea3a322.png)'
  prefs: []
  type: TYPE_IMG
- en: Project dashboard with various options
  prefs: []
  type: TYPE_NORMAL
- en: Deploy the code using either the command line or the GitHub.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The successful deployment will list the Spider as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/725a4e3a-1b86-40e2-a11a-7e7b86adc66a.png)'
  prefs: []
  type: TYPE_IMG
- en: Listing of Spider after code deploy
  prefs: []
  type: TYPE_NORMAL
- en: 'Click the listed Spider, and detailed information and available options will
    be displayed as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/e0162ce0-62df-4762-a648-5764f8b670de.png)'
  prefs: []
  type: TYPE_IMG
- en: Spider details
  prefs: []
  type: TYPE_NORMAL
- en: 'Click RUN to start crawling the chosen Spider as seen here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/d64c0eaa-93ad-4307-b60f-c3a36ce4c6d0.png)'
  prefs: []
  type: TYPE_IMG
- en: Spider Run window
  prefs: []
  type: TYPE_NORMAL
- en: Click RUN with the default options.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Crawling jobs will be listed as seen in the following screenshot. We can browse
    through the Completed jobs for details on Items, Requests, Errors, Logs, and so
    on:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/879a0440-028a-4a14-b199-09ada3f8b84e.png)'
  prefs: []
  type: TYPE_IMG
- en: Jobs details for Spider
  prefs: []
  type: TYPE_NORMAL
- en: 'When exploring items for completed jobs, options such as filters, data export,
    and downloading with crawling job details for requests, logs, stats, and so on
    are available in the job details. More information can be loaded by clicking a
    particular Spider listed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/559fca95-9b82-41d3-9d1c-5cc3032783b6.png)'
  prefs: []
  type: TYPE_IMG
- en: Listing items from Spider
  prefs: []
  type: TYPE_NORMAL
- en: Using the actions listed previously, we can deploy Scrapy Spider successfully
    using the Scraping hub.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we used and explored the Scraping hub to deploy the Scrapy
    Spider.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Selecting the right libraries and frameworks does depend on the project scope.
    Users are free to choose libraries and experience the online process.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we have used and explored various aspects of traversing web
    documents using Beautiful Soup and have explored a framework built for crawling
    activities using Spiders: Scrapy. Scrapy provides a complete framework to develop
    a crawler and is effective using XPath and CSS Selectors with support for the
    data export. Scrapy projects can also be deployed using Scraping hub to experience
    the live performance of the deployed Spider and enjoy features provided by the
    Scrapings hub (Scrapy Cloud) at [https://scrapinghub.com/scrapy-cloud](https://scrapinghub.com/scrapy-cloud).'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore more information regarding scraping data
    from the web.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scrapy: [https://docs.scrapy.org/en/latest/intro/overview.html](https://docs.scrapy.org/en/latest/intro/overview.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn Scrapy: [https://learn.scrapinghub.com/scrapy/](https://learn.scrapinghub.com/scrapy/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beautiful Soup: [https://www.crummy.com/software/BeautifulSoup/bs4/doc/](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SoupSieve: [https://facelessuser.github.io/soupsieve/selectors/](https://facelessuser.github.io/soupsieve/selectors/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XPath tutorial: [https://doc.scrapy.org/en/xpath-tutorial/topics/xpath-tutorial.html](https://doc.scrapy.org/en/xpath-tutorial/topics/xpath-tutorial.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CSS Selector reference: [https://www.w3schools.com/cssref/css_selectors.asp](https://www.w3schools.com/cssref/css_selectors.asp)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feed exports: [http://docs.scrapy.org/en/latest/topics/feed-exports.html](http://docs.scrapy.org/en/latest/topics/feed-exports.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scraping hub (Scrapy Cloud): [https://scrapinghub.com/](https://scrapinghub.com/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
