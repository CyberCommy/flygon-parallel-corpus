- en: '*Chapter 3*: CPU Architecture, Resources, and Performance'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With this chapter, we begin the exploration of the computing hardware: we want
    to know how to use it optimally and squeeze the best performance from out of it.
    The first hardware component we have to learn about is the central processor.
    The CPU does all the computations, and if we are not using it efficiently, nothing
    is going to save our slow, poorly performing program. This chapter is dedicated
    to learning about CPU resources and capabilities, the optimal ways to use them,
    the more common reasons for not making the best use of CPU resources, and how
    to resolve them.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of modern CPUs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using internal concurrency of the CPUs for optimum performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CPU pipelines and speculative execution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Branch optimization and branchless computing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to evaluate whether a program uses CPU resources efficiently
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Again, you will need a C++ compiler and a micro-benchmarking tool, such as
    the Google Benchmark library we used in the previous chapter (found at [https://github.com/google/benchmark](https://github.com/google/benchmark)).
    We will also use the **LLVM Machine Code Analyzer** (**LLVM-MCA**), found at [https://llvm.org/docs/CommandGuide/llvm-mca.html](https://llvm.org/docs/CommandGuide/llvm-mca.html).
    If you want to use the MCA, your choice of compilers is more limited: you need
    an LLVM-based compiler such as Clang.'
  prefs: []
  type: TYPE_NORMAL
- en: The code accompanying this chapter can be found at [https://github.com/PacktPublishing/The-Art-of-Writing-Efficient-Programs/tree/master/Chapter03](https://github.com/PacktPublishing/The-Art-of-Writing-Efficient-Programs/tree/master/Chapter03).
  prefs: []
  type: TYPE_NORMAL
- en: The performance begins with the CPU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we have observed in the earlier chapters, an efficient program is one that
    makes full use of the available hardware resources and does not waste them for
    tasks that are not needed. A high-performing program cannot be described so simply
    because performance can be defined only with respect to specific targets. Nonetheless,
    in this book, and in particular, in this chapter, we are largely concerned with
    the computational performance or throughput: *how fast can we solve a given problem
    with the hardware resources we have?* This type of performance is closely related
    to efficiency: our program will deliver the result faster if every computation
    it executes brings us closer to the result, and, at every moment, we do as much
    computing as possible.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This brings us to the next question: *just how much computing can be done,
    say, in one second?* The answer, of course, will depend on what hardware you have,
    how much of it, and how efficiently your program can use it. Any program needs
    multiple hardware components: processors and memory, obviously, but also networking
    for any distributed program, storage, and other I/O channels for any program that
    manipulates large amounts of external data, possibly other hardware, depending
    on what the program does. But everything starts with the processor, and so, perforce,
    does our exploration of high-performance programming. Furthermore, in this chapter,
    we will limit ourselves to a single thread of execution; concurrency will come
    later.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With this narrower focus, we can define what this chapter is about: *how to
    make the best use of the CPU resources using a single thread*. To understand this,
    we first need to explore what are the resources that a CPU has. Of course, different
    generations and different models of processors will have a different assortment
    of hardware capabilities, but the goal of this book is two-fold: first, to give
    you a general understanding of the subject, and second, to equip you with the
    tools necessary to acquire more detailed and specific knowledge. The general overview
    of the computational resources available on any modern CPU can be summarized,
    unfortunately, as *it''s complicated*. To illustrate, consider this die image
    of an Intel CPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Die image of a Pentium CPU, with the markup of functional areas
    (source: Intel)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.1_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.1 – Die image of a Pentium CPU, with the markup of functional areas
    (source: Intel)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Overlaid on top of the image are the descriptions of the major functional areas.
    If this is the first time you have seen such an image, the most startling detail
    may be that the execution unit, that is, the part that does actual additions,
    multiplications, and other operations that we think of as the main function of
    the CPU, actually doesn''t take up even a quarter of all the silicon. The rest
    is *other stuff* whose purpose is, fundamentally, to enable the additions and
    multiplications to work and work efficiently. The second and more practically
    relevant observation is this: the processor has many components with different
    functions. Some of these components largely work by themselves, and there is little
    the programmer needs to do to make the best use of them. Some need a careful arrangement
    of the machine code that, thankfully, is mostly done by the compilers. But more
    than half of the silicon area is dedicated to the components that don''t just
    *optimize themselves*: to get the maximum performance out of this processor, the
    programmer needs to understand how they work, what they can and cannot do, and
    what affects the efficiency of their operations (both positively and negatively).
    Often even the parts that work OK by themselves can benefit from the programmer''s
    attention if truly exceptional performance is desired.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many books written on processor architecture, including all the hardware
    techniques the designers use to improve the performance of their creations. These
    books can be a source of valuable knowledge and understanding. This is not going
    to be yet another one of those books. What descriptions and explanations of the
    hardware it does have, serve a different goal: here, we will focus on the practical
    ways in which you can explore the performance of your hardware, starting with
    the CPUs. We start this exploration without delay in the next section.'
  prefs: []
  type: TYPE_NORMAL
- en: Probing performance with micro-benchmarks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The outcome of the previous section may leave you somewhat daunted: the processor
    is very complex and, apparently, needs a lot of hand-holding on the part of the
    programmer to operate at peak efficiency. Let us start small and see how fast
    a processor can do some basic operations. To that end, we will use the same **Google
    Benchmark** tool we have used in the last chapter. Here is a benchmark for the
    simple addition of two arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In this first example, we show the benchmark in all details, including input
    generation. While the speed of most operations does not depend on the values of
    the operands, we are going to use random input values, just so we don''t have
    to worry about it when we do get to the operations that are sensitive to the input
    values. Also note that, while we store the values in vectors, we don''t want to
    benchmark the speed of the vector indexing: the compiler will almost certainly
    optimize the expression `v1[i]` to produce the exact same code as `p1[i]`, but
    why take chances? We are excluding as many non-essential details as possible until
    we are left with the most basic problem: we have two arrays of values in memory,
    and we want to do some computations on each element of these arrays.'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, we have to be concerned with the possibility of undesired
    compiler optimizations: the compiler may figure out that the entire program is
    just a very long way of doing nothing at all (at least as far as the C++ standard
    is concerned), and come up with a much faster way to do the same by optimizing
    away big chunks of the code. The compiler directions to not optimize away the
    result of the computation and to assume that the state of the memory can change
    between benchmark iterations should prevent such optimizations. It is equally
    important not to get carried away in the other direction: for example, declaring
    the variable `a1` as `volatile` will certainly prevent most undesired optimizations.
    Unfortunately, it will also prevent the compiler from optimizing the loop itself,
    and this is not what we want: we want to see how efficiently the CPU can do the
    addition of the two arrays, which implies generating the most efficient code as
    well. We just don''t want the compiler to figure out that the first iteration
    of the benchmark loop is exactly the same as the second one.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that this is a somewhat unusual application of the micro-benchmark: usually,
    we have a fragment of code, and we want to find out how fast it is and how we
    can make it faster. Here, we are using the micro-benchmark to learn about the
    performance of the processor by tailoring the code in a way that will give us
    some insights.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The benchmark should be compiled with optimization turned on. Running this
    benchmark will produce the result that looks something like this (the exact numbers
    will depend on your CPU, of course):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.2_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.2
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we cannot conclude much from this experiment, other than the modern
    CPUs are *fast*: they can add two numbers in less than a nanosecond. If you''re
    curious, you can explore other operations at this point: subtraction and multiplication
    take exactly as much time as addition, while integer division is rather expensive
    (three to four times slower).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to analyze the performance of our code, we have to look at it the
    way the processor sees it, and there is a lot more going on here than the simple
    addition. The two input arrays are stored in memory, but the addition or multiplication
    operations are executed between values stored in registers (or, possibly, between
    a register and a memory location, for some operations). This is how the processor
    sees one iteration of our loop, step by step. At the start of the iteration, the
    index variable `i` is in one of the CPU registers, and the two corresponding array
    elements, `v1[i]` and `v2[i]`, are in memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – Processor state at the start of the i-th loop iteration'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.3_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.3 – Processor state at the start of the i-th loop iteration
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we can do anything, we have to move the input values into the registers.
    A register has to be allocated for each input, plus one register for the result.
    In a given loop iteration, the first instruction will load one of the inputs into
    the register:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – Processor state after the first instruction of the i-th iteration'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.4_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.4 – Processor state after the first instruction of the i-th iteration
  prefs: []
  type: TYPE_NORMAL
- en: 'The read (or load) instruction uses the register containing the index `i` and
    the location of the array `v1` in memory to access the value `v1[i]` and copy
    it into the register. The next instruction similarly loads the second input:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5 – Processor state after the second instruction of the i-th iteration'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.5_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.5 – Processor state after the second instruction of the i-th iteration
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we are finally ready to do the operation such as addition or multiplication:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6 – Processor state at the end of the i-th loop iteration'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Image88353.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.6 – Processor state at the end of the i-th loop iteration
  prefs: []
  type: TYPE_NORMAL
- en: 'This simple line of code produces all these steps after it is converted into
    hardware instructions (plus the operations necessary to advance to the next iteration
    of the loop):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'From the efficiency point of view, we want to focus on that last step: our
    CPU can add or multiply two numbers in under a nanosecond, not bad, but can it
    do more? A lot of transistors are dedicated to processing and executing instructions,
    so they have to be good for something more. Let us try to do two operations on
    the same values instead of just one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'If an addition takes one nanosecond and a multiplication takes one nanosecond,
    how long would both take? The benchmark gives us the answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7 – Benchmarks for a single instruction and two instructions'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.7_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.7 – Benchmarks for a single instruction and two instructions
  prefs: []
  type: TYPE_NORMAL
- en: 'Surprisingly, one plus one equals one here. We can add even more instructions
    to one iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The time per iteration is still the same (slight differences are within the
    accuracy of the benchmark measurement):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8 – Benchmarks for loops with up to four instructions per iteration'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.8_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.8 – Benchmarks for loops with up to four instructions per iteration
  prefs: []
  type: TYPE_NORMAL
- en: 'It appears that our view of the processor as executing one instruction at a
    time needs to be revised:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9 – Processor executing multiple operations in a single step'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.9_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.9 – Processor executing multiple operations in a single step
  prefs: []
  type: TYPE_NORMAL
- en: 'As long as the operands are already in the registers, the processor can execute
    several operations at once. This is known as **Instruction-Level Parallelism**
    (**ILP**). Of course, there is a limit to how many operations can be executed:
    the processor has only so many execution units capable of doing integer computations.
    Still, it is instructive to try to push the CPU to its limits by adding more and
    more instructions to one iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The exact number of instructions a processor can execute depends on the CPU
    and the instructions, of course, but the previous loop shows a noticeable slowdown
    compared to the single multiplication, at least on the machine I am using:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.10 – Benchmark of eight instructions per iteration'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.10_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.10 – Benchmark of eight instructions per iteration
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you can appreciate just how inefficient, in terms of the hardware utilization,
    our original code was: the CPU, apparently, can execute between five and seven
    different operations per iteration, so our single multiplication wasn''t taxing
    even a quarter of its capabilities. In truth, the modern processors are even more
    impressively capable: in addition to the integer computation units we have been
    experimenting with, they have separate floating-point hardware that can execute
    instructions on `double` or `float` values, and the vector processing units that
    execute MMX, SSE, AVX, and other specialized instructions, all at the same time!'
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing instruction-level parallelism
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, our conclusions about the CPU's ability to execute multiple instructions
    in parallel were based on strong but indirect evidence. It would be good to get
    a direct confirmation that this is indeed what's going on. We can get such confirmation
    from the **Machine Code Analyzer** (**MCA**), which is a part of the LLVM toolchain.
    The analyzer takes assembly code as the input and reports a lot of information
    on how the instructions are executed, what the delays and the bottlenecks are,
    and so on. We are not going to learn all the capabilities of this advanced tool
    here (refer to the project home page, [https://llvm.org/docs/CommandGuide/llvm-mca.html](https://llvm.org/docs/CommandGuide/llvm-mca.html),
    for details). However, we can use it now to see how the CPU executes our operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to annotate the code with the analyzer markup to select which
    part of the code to analyze:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: You don't have to use `#define` for the analyzer markup, but I find it easier
    to remember these commands than the exact assembly syntax (you can save the `#define`
    lines in a header file and include it as needed). Why did we mark for analysis
    just the body of the loop and not the whole loop? The analyzer actually assumes
    that the selected code fragment runs in a loop and repeats it for some number
    of iterations (ten by default). You can try to mark the entire loop for analysis,
    but, depending on the compiler optimizations, this may confuse the analyzer (it's
    a powerful tool, but not easy to use or, at the time of writing this, particularly
    robust).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can run the analyzer now:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.11'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.11_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.11
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we do not compile the code into an executable but rather generate
    assembly output (`-S`) in Intel syntax. The output is piped into the analyzer;
    of the many ways the analyzer can report the results, we selected the timeline
    output. The timeline view shows each instruction as it moves through the execution
    process. Let us analyze two code fragments, one with a single operation (addition
    or multiplication) and the other one with both operations. Here is the timeline
    for the iteration with just one multiplication (we have removed all the lines
    in the middle of the timeline):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.12'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.12_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.12
  prefs: []
  type: TYPE_NORMAL
- en: The horizontal axis is the time in cycles. The analyzer simulated running the
    selected code fragment for ten iterations; each instruction is identified by its
    sequential number in the code and the iteration index, so the first instruction
    of the first iteration has the index `[0,0]`, and the last instruction has the
    index `[9,2]`. This last instruction is also the third instruction of the tenth
    iteration (there are only three instructions per iteration). The entire sequence
    took 55 cycles, according to the timeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let us add another operation that uses the same values `p1[i]` and `p2[i]`
    that we already read from memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us look at the timeline for the code with two operations per iteration,
    one addition and one multiplication:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.13'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.13_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.13
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a lot more instructions executed now, six instructions per iteration
    (the last instruction has the index `[9,5]`). However, the duration of the timeline
    has increased by just one cycle: In *Figure 3.12*, the timeline ended on cycle
    54, whereas in *Figure 3.13*, it ends on cycle 55\. As we have suspected, the
    processor managed to execute twice as many instructions in the same length of
    time.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You may have also noticed that for all our benchmarks so far, we have increased
    the number of operations done on the same input values (add them, subtract them,
    multiply them, and so on). We have concluded that these extra operations are *free*
    as far as the runtime is concerned (up to a point). This is an important general
    lesson to learn: once you have some values in registers, adding a computation
    on the same values probably won''t cost you any performance unless your program
    was already extremely efficient and was stressing the hardware to the limit. Unfortunately,
    the experiment and the conclusions are of limited practical value. How often does
    it happen that all your computations are done just on a handful of inputs at a
    time, the next iteration uses its own inputs, and you can find some more useful
    computations you can do on the same inputs? Not quite never, but rarely. Any attempt
    to extend our simple demonstration of the CPU''s computational power is going
    to run into one or more complications. The first one is the data dependency: the
    sequential iterations of the loop are usually not independent; instead, each iteration
    needs some data from the previous iterations. We will explore this situation in
    the next section.'
  prefs: []
  type: TYPE_NORMAL
- en: Data dependencies and pipelining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our analysis of the CPU capabilities so far has shown that the processor can
    execute multiple operations at once as long as the operands are already in the
    registers: we can evaluate a fairly complex expression that depends on just two
    values in exactly as much time as it takes to add these values. The *depends on
    just two values* qualifier is, unfortunately, a very serious restriction. We now
    consider a more realistic code example, and we don''t have to make many changes
    to our code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Recall that the old code had the same loop with a simpler body: `a1 += (p1[i]
    + p2[i]);`. Also, `p1[i]` is just an alias for the vector element `v1[i]`, same
    for `p2` and `v2`. Why is this code more complex? We have already seen that the
    processor can do addition, subtraction, and multiplication in a single cycle,
    and the expression still depends on just two values, `v1[i]` and `v2[i]`. However,
    this expression cannot be evaluated in one cycle. To clarify this, we introduce
    two temporary variables that are really just names for the intermediate results
    during expression evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The results of the addition and the subtraction, `s[i]` and `d[i]`, can be
    evaluated at the same time, as we saw earlier. However, the last line cannot be
    executed until we have the values of `s[i]` and `d[i]`. It doesn''t matter how
    many additions and multiplications the CPU can do at once: you cannot compute
    the result of an operation whose inputs are unknown; therefore, the CPU has to
    wait for the inputs to the multiplication to become ready. The i-th iteration
    has to be executed in two steps: first, we have to add and subtract (we can do
    both at once), and second, we have to multiply the results. The iteration now
    takes two cycles instead of one because the second step of the computation depends
    on the **data** produced by the first step:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.14 – Data dependency in loop evaluation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.14_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.14 – Data dependency in loop evaluation
  prefs: []
  type: TYPE_NORMAL
- en: 'Even though the CPU has the resources to do all three operations at once, we
    cannot take advantage of this capability because of the data dependency inherent
    in our computations. This, of course, severely limits how efficiently we can use
    our processor. Data dependencies are very common in programs, but fortunately,
    the hardware designers came up with an effective antidote. Consider *Figure 3.14*
    carefully. We have the multiplication hardware unit standing by idly while we
    compute the values of `s[i]` and `d[i]`. We cannot start computing their product
    any earlier, but there is something else we can do: we can multiply the values
    `s[i-1]` and `d[i-1]` from the previous iteration at the same time. Now the two
    iterations of the loop are interleaved in time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.15 – Pipelining: the rows correspond to the successive iterations;
    all operations in the same row are executed simultaneously'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.15_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.15 – Pipelining: the rows correspond to the successive iterations;
    all operations in the same row are executed simultaneously'
  prefs: []
  type: TYPE_NORMAL
- en: 'This transformation of the code is known as **pipelining**: a complex expression
    is broken up into stages and executed in a pipeline where stage 2 of the previous
    expression runs at the same time as stage 1 of the next one (a more complex expression
    would have more stages and require a deeper pipeline). If we are correct in our
    expectations, the CPU will be able to compute our two-stage add-subtract-multiply
    expression just as fast as single multiplication as long as we have many iterations:
    the first iteration is going to take two cycles (add/subtract first, then multiply),
    there is no getting around that. Similarly, the last iteration will end with a
    single multiplication, and there is nothing else we can do at the same time. However,
    all the iterations in between will be executing three operations simultaneously.
    We already know that our CPU can add, subtract, and multiply at the same time.
    The fact that the multiplication belongs to a different iteration of the loop
    matters not at all.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can confirm our expectations with a direct benchmark, where we compare the
    time it takes to do one multiplication per loop iteration with the time it takes
    to do our two-step iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.16'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.16_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.16
  prefs: []
  type: TYPE_NORMAL
- en: 'As expected, both loops run at essentially the same speed. We can conclude
    that the pipelining has completely negated the performance penalty caused by the
    data dependency. Note that the pipelining does not eliminate the data dependency;
    each loop iteration still has to be executed in two stages, with the second stage
    depending on the results of the first one. However, by interleaving the computations
    from different stages, the pipelining does eliminate the inefficiency that would
    be otherwise caused by this dependency (at least in the ideal case, which is what
    we have so far). An even more direct confirmation can be seen in the results of
    the Machine Code Analyzer. Again, the timeline view is the most instructive:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.17 – A timeline view of the pipelined add-subtract-multiply loop
    (top) vs. a loop with a single multiplication (bottom)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.17_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.17 – A timeline view of the pipelined add-subtract-multiply loop (top)
    vs. a loop with a single multiplication (bottom)
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, it takes 56 cycles to execute ten iterations of either loop.
    The key step in the timeline is when an instruction is executed: `e` marks the
    beginning of the execution, and `E` is when the execution ends. The effect of
    the pipelining is clearly visible in the timeline: the first iteration of the
    loop starts to execute on the second cycle with the instruction `[0,0]`; the last
    instruction of the first iteration is done executing on cycle 18 (the horizontal
    axis is the cycle number). The second iteration begins executing on cycle 4, that
    is, there is a significant overlap of the two iterations. This is the pipelining
    in action, and you can see how it improves the efficiency of our program: at almost
    every cycle, the CPU is executing instructions from multiple iterations using
    its many computation units. It takes just as many cycles to execute a simple loop
    as it does to execute the more complex one, so the extra machine operations take
    no additional time.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter is not intended to be a manual for the Machine Code Analyzer:
    to understand better the timeline and other information it produces, you should
    study its documentation. There is, however, one issue that we must point out.
    Every iteration of our loop does not just have the same C++ code, it has exactly
    the same machine code as well. This makes sense: the pipelining is done by the
    hardware, not the compiler; the compiler simply generates the code for one iteration
    and the operations needed to advance to the next iteration (or exit the loop upon
    completion). The processor executes multiple instructions in parallel; we can
    see that in the timeline. But something does not make sense upon close examination:
    for example, consider the instruction `[0,4]` in *Figure 3.17*. It is executed
    during cycles 6 through 12, and it uses registers CPU `rax` and `rsi`. Now look
    at the instruction `[1,2]` that is executed during cycles 8 and 9: it also uses
    the same registers, it actually writes into the register `rsi`, which is still
    being used by other instructions at the same time. This cannot be: while the CPU
    can do multiple operations simultaneously using its many independent computing
    units, it cannot store two different values in the same register at the same time.
    This contradiction was actually present, although well-hidden, all the way back
    in *Figure 3.15*: assuming that the compiler generates only one copy of the code
    for all iterations, the register we are going to use to store the value of `s[i]`
    is exactly the same as the one we need to read the value of `s[i-1]`, and both
    actions happen at the same time.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important to understand that we are not running out of registers: the
    CPU has many more registers than we have seen named so far. The problem is that
    the code for one iteration looks exactly like the code for the next iteration,
    including the register names, but at each iteration, different values must be
    stored in the registers. It seems like the pipelining we have assumed and observed
    should not, in fact, be possible: the next iteration must wait for the previous
    iteration to stop using the registers it needs. This is not what really happens,
    and the solution to this apparent contradiction is the hardware technique called
    `rsi`, are not the *real* register names, they are mapped by the CPU to the actual
    physical registers. The same name, `rsi`, can be mapped to different registers
    that all have the same size and functionality.'
  prefs: []
  type: TYPE_NORMAL
- en: When the processor executes the code in a pipeline, the instructions from the
    first iteration that refer to `rsi` will, in fact, use an internal register that
    we shall call `rsi1` (this is not its real name, but the actual hardware names
    of registers are not something you would ever encounter unless you are designing
    a processor). The second iteration also has instructions that refer to `rsi` but
    needs to store a different value there, so the processor will use another register,
    `rsi2`. Unless the first iteration no longer needs the value stored in `rsi`,
    the third iteration will have to use yet another register, and so on. This register
    renaming is done by the hardware and is very different from the register assignment
    done by the compiler (in particular, it is entirely invisible to any tool that
    analyzes the object code, such as LLVM-MCA or a profiler). The end effect is that
    multiple iterations of the loop are now executed as a linear sequence of code
    as if `s[i]` and `s[i+1]` really did refer to different registers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Converting a loop into linear code is known as **loop unrolling**; it is a
    popular compiler optimization technique, but this time, it is done in hardware
    and is essential to be able to deal with data dependencies efficiently. The compiler''s
    point of view is closer to the way the source code is written: a single iteration,
    a group of machine instructions, executed over and over by jumping back to the
    beginning of the code fragment for the iteration. The processor''s point of view
    is more like what you see in the timeline, a linear sequence of instructions where
    each iteration has its own copy of the code and can use different registers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can make another important observation: the order in which the CPU executes
    our code is actually not the same order in which the instructions are written.
    This is called out-of-order execution, and it has important consequences for multi-threaded
    programs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have seen how the processor avoids the restrictions on the efficiency of
    execution that would be imposed by data dependencies: the antidote to the data
    dependency is the pipelining. However, the story does not end there, and the beautifully
    complex scheme we have devised so far to execute our very simple loop is missing
    something important: the loop must end at some point. In the next section, we
    will see how much that complicates things and what the solution is.'
  prefs: []
  type: TYPE_NORMAL
- en: Pipelining and branches
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is our understanding of the efficient use of a processor so far: first,
    the CPU can do multiple operations at once, such as add and multiply at the same
    time. Not taking advantage of this capability is like leaving free computing power
    on the table. Second, the factor that limits our ability to maximize efficiency
    is how fast we can produce the data to feed into these operations. Specifically,
    we are constrained by the data dependencies: if one operation computed the value
    that the next operation uses as an input, the two operations must be executed
    sequentially. The workaround to this dependency is pipelining: when executing
    loops or long sequences of code, the processor will interleave separate computations
    such as loop iterations, as long as they have at least some operations that can
    be executed independently.'
  prefs: []
  type: TYPE_NORMAL
- en: However, pipelining has an important precondition as well. Pipelining `if(condition)`
    statement, we will execute either the `true` branch or the `false` branch, but
    we won't know which until we evaluate the `condition`. Just like data dependency
    was the bane of instruction-level parallelism, the conditional execution, or branches,
    are the bane of pipelining.
  prefs: []
  type: TYPE_NORMAL
- en: 'With pipelining disrupted, we can expect a significant reduction in the efficiency
    of our program. It should be very easy to modify our earlier benchmark to observe
    this deleterious effect of conditionals. For example, instead of writing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We could write:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we have reintroduced the data dependency as a code dependency:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.18 – Effect of a branch instruction on the pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.18_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.18 – Effect of a branch instruction on the pipeline
  prefs: []
  type: TYPE_NORMAL
- en: There is no obvious way to convert this code into a linear stream of instructions
    to execute, and the conditional jump cannot be avoided.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reality is somewhat more complex: a benchmark such as we have just suggested
    may or may not show significant degradation in performance. The reason is that
    many processors have some sort of **conditional move** or even **conditional add**
    instructions, and the compiler may decide to use them. If this happens, our code
    becomes entirely sequential with no jumps or branches and can be pipelined perfectly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.19 – Conditional code pipelined with cmove'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.19_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.19 – Conditional code pipelined with cmove
  prefs: []
  type: TYPE_NORMAL
- en: The x86 CPUs have a conditional move instruction, `cmove` (although not all
    compilers will use it to implement the `?:` operator in the previous figure).
    The processors with AVX or AVX2 instruction sets have a powerful set of *masked*
    addition and multiplication instructions that can be used to implement some conditional
    code as well. That is why, when benchmarking and optimizing the code with branches,
    it is very important to examine the generated object code and confirm that the
    code indeed contains branches and that they are indeed affecting the performance.
    There are also profiler tools that can be used for this purpose, and we will see
    one such tool in a moment.
  prefs: []
  type: TYPE_NORMAL
- en: 'While branches and conditionals happen everywhere in most real-life programs,
    they can disappear when the program is reduced to just a few lines for a benchmark.
    One reason is that that the compiler may decide to use one of the conditional
    instructions we have mentioned earlier. Another reason that is common in poorly
    constructed benchmarks is that the compiler may be able to figure out, at compile-time,
    what the condition evaluates to. For example, most compilers will completely optimize
    away any code like `if (true)` or `if (false)`: there is no trace of this statement
    in the generated code, and any code that is never going to be executed is also
    eliminated. To observe the deleterious effect of the branches on the loop pipelining,
    we have to construct a test where the compiler cannot predict the outcome of the
    condition check. In your real-life benchmarks, you may have a data set extracted
    from your real program. For this next demonstration, we will use random values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, we have two input vectors `v1` and `v2`, plus a control vector `c1`
    that has random values of zero and one (avoid using `vector<bool>` here, it is
    not an array of bytes but a packed array of bits, so accessing it is considerably
    more expensive, and we are not interested in benchmarking bit manipulation instructions
    at this time). The compiler cannot predict whether the next random number is odd
    or even, thus, no optimizations are possible. Also, we have examined the generated
    machine code and confirmed that our compiler (Clang-11 on x86) implements this
    loop using a simple conditional jump. To have a baseline, we will compare the
    performance of this loop with one that does unconditional addition and multiplication
    on each iteration: `a1 += p1[i]*p2[i]`. This simpler loop does both an addition
    and a multiplication on each iteration; however, thanks to the pipelining, we
    get the addition *free*: it is executed simultaneously with the multiplication
    from the next iteration. The conditional branch, on the other hand, is anything
    but free:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.20'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.20_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.20
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the conditional code is about five times slower than the sequential
    one. This confirms our prediction that when the next instruction depends on the
    result of the previous one, the code cannot be effectively pipelined.
  prefs: []
  type: TYPE_NORMAL
- en: Branch prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'However, an astute reader may point out that the picture we have just described
    cannot possibly be complete, or even true: let us go back, for a moment, to the
    apparently linear code such as the loop we have used extensively in the last section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is what the body of this loop looks like from the processor''s point of
    view:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.21 – Loop executed in a pipeline of width w'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.21_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.21 – Loop executed in a pipeline of width w
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 3.21*, we have shown three interleaved iterations, but there could
    be even more, the total width of the pipeline is `w`, and ideally, `w` is large
    enough that at every cycle, the CPU is executing exactly as many instructions
    as it can execute simultaneously (such peak efficiency is rarely possible in practice).
    Note, however, that it may be impossible to access `v[i+2]` at the same time as
    we compute the sum `p1[i] + p2[i]`: there is no guarantee that the loop has two
    more iterations to go, and, if it doesn''t, the element `v[i+2]` does not exist
    and accessing it results in undefined behavior. There is a hidden conditional
    in the previous code: at every iteration, we must check if `i` is less than `N`,
    and only then can we execute the instructions of the i-th iteration.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, our comparison in *Figure 3.20* is a lie: we did not compare pipelined
    sequential execution versus an unpredictable conditional one. Both benchmarks
    are, in fact, examples of conditional code, they both have branches.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The full truth is somewhere in between. To understand it, we have to learn
    about the antidote to the conditional execution, which poisons the pipelining
    and is itself the antidote to the data dependency. The way to save the pipelining
    in the presence of branches is to attempt to convert the conditional code to the
    sequential one. Such conversion could be done if we knew in advance which path
    the branch is going to take: we would simply eliminate the branch and proceed
    to the next instruction to be executed. Of course, there would be no need even
    to write such code if we knew in advance what the condition is. Still, consider
    the loop termination condition. Assuming the loop is executed many times, it is
    a good bet that the condition `i < N` evaluates to `true` (we would lose this
    bet only one out of `N` times).'
  prefs: []
  type: TYPE_NORMAL
- en: The processor makes the same bet using the technique known as **branch prediction**.
    It analyzes the history of every branch in the code and assumes that the behavior
    will not change in the future. For the end of the loop condition, the processor
    will quickly learn that most of the time, it has to proceed to the next iteration.
    Therefore, the right thing to do is to pipeline the next iteration as if we are
    sure it's going to happen. Of course, we have to defer the actual writing of the
    results into memory until we evaluate the condition and confirm that the iteration
    does happen; the processor has a certain number of write buffers to hold such
    unconfirmed results *in limbo* before committing them to memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pipeline for the loop with just an addition, therefore, does look exactly
    as shown in *Figure 3.21*. The only catch is that, when starting to execute iteration
    `i+2` before the i-th iteration is complete, the processor is making a bet based
    on its prediction of whether the conditional branch will be taken or not. Such
    execution of the code before we know for sure that this code really exists is
    known as **speculative execution**. If the bet is won, we already have the results
    by the time we figure out that we needed the computation, and all is well. If
    the processor loses the bet, it has to discard some of the computations to avoid
    producing incorrect results: for example, writing into memory overwrites what
    was there before and cannot be undone on most hardware platforms, while computing
    the result and storing it in a register is entirely reversible, except for the
    time we wasted, of course.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We now have a more complete picture of how the pipelining really works: in
    order to find more instructions to execute in parallel, the processor looks at
    the code for the next iterations of the loop and starts to execute it simultaneously
    with the current iteration. If the code includes a conditional branch, which makes
    it impossible to know for sure which instruction will be executed, the processor
    makes an educated guess based on the past outcomes of checking the same condition
    and proceeds to execute the code speculatively. If the prediction proves to be
    correct, the pipelining can be as good as it was for the unconditional code. If
    the prediction is wrong, the processor has to discard the result of every instruction
    that should not have been evaluated, fetch the instructions that it previously
    assumed wouldn''t be needed, and evaluate them instead. This event is called a
    **pipeline flush**, and it is an expensive occurrence indeed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we have a better understanding of our previous benchmark in *Figure 3.20*:
    both loops have a condition for checking the end of the loop. However, it is predicted
    almost perfectly. The pipeline flush occurs only once at the end of the loop.
    The *conditional* benchmark also has a branch that is based on a random number:
    `if(b1[i])` where `b1[i]` is true 50% of the time, randomly. The processor is
    powerless to predict the outcome, and the pipeline is disrupted half the time
    (or worse, if we manage to confuse the CPU into actually making wrong predictions).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We should be able to verify our understanding with a direct experiment: all
    we need is to change the *random* condition to something that is always true.
    The only catch is that we have to do it in a way that the compiler cannot figure
    it out. One common way is to change the initialization of the condition vector
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The compiler doesn''t know that the function `rand()` always returns non-negative
    random numbers and will not eliminate the condition. The branch predictor circuit
    of the CPU will quickly learn that the condition `if(b1[i])` always evaluates
    to true and will execute the corresponding code speculatively. We can compare
    the performance of the well-predicted branch with that of the unpredictable one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.22'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.22_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.22
  prefs: []
  type: TYPE_NORMAL
- en: Here we can see that the cost of the well-predicted branch is minimal and that
    it is much faster than exactly the same code with a branch that is predicted poorly.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling for branch mispredictions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you have seen how badly a single mispredicted branch can impact the
    performance of the code, you may wonder, how would you ever find such code so
    you can optimize it? Sure, the function containing this code will take longer
    than you might expect, but how do you know whether it's because of the badly predicted
    branches or due to some other inefficiency? By now, you should know enough to
    avoid making guesses about performance in general; however, speculating about
    the effectiveness of the branch predictor is particularly futile. Fortunately,
    most profilers can profile not just execution time but also various factors determining
    the efficiency, including the branch prediction failures.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will once again use the `perf` profiler. As the first step,
    we can run this profiler to collect the overall performance metrics of the entire
    benchmark program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the `perf` results for the program running only the `BM_branch_not_predicted`
    benchmark (other benchmarks are commented out for this test):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.23 – Profile of a benchmark with a poorly predicted branch'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.23_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.23 – Profile of a benchmark with a poorly predicted branch
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, 11% of all branches were mispredicted (the last line of the
    report). Note that this number is cumulative for all branches, including the perfectly
    predictable end of the loop condition, so 11% total is quite bad. We should compare
    it with our other benchmark, `BM_branch_predicted`, which is identical to this
    one except the condition is always true:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.24 – Profile of a benchmark with a well-predicted branch'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.24_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.24 – Profile of a benchmark with a well-predicted branch
  prefs: []
  type: TYPE_NORMAL
- en: This time, less than 0.1% of all branches were not predicted correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The overall performance report is very useful, do not ignore its potential:
    it can be used to highlight or dismiss some possible causes of poor performance
    quickly. In our case, we can immediately conclude that our program suffers from
    one or more mispredicted branches. Now we just need to find which one, and the
    profiler can help with that as well. Just like in the previous chapter, where
    we have used the profiler to find out where in the code does our program spends
    the most time, we can generate a detailed line-by-line profile of branch predictions.
    We just need to specify the right performance counter to the profiler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In our case, we can copy the name of the counter from the output of `perf stat`,
    because it happens to be one of the counters it measures by default, but the complete
    list can be obtained by running `perf --list`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The profiler runs the program and collects the metrics. We can view them by
    generating a profile report:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The report analyzer is interactive and lets us navigate to the branch mispredictions
    counter for each function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.25 – Detailed profile report for mispredicted branches'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.25_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.25 – Detailed profile report for mispredicted branches
  prefs: []
  type: TYPE_NORMAL
- en: Over 99% of all mispredicted branches occur in just one function. Since the
    function is small, finding the responsible conditional operation should not be
    hard. In a larger function, we would have to look at the line-by-line profile.
  prefs: []
  type: TYPE_NORMAL
- en: 'The branch prediction hardware of a modern processor is fairly sophisticated.
    For example, if a function is called from two different places and, when called
    from the first place, a condition usually evaluates to true, while, when called
    from the second place, the same condition evaluates to false, the predictor will
    learn that pattern and predict the branch correctly based on the origin of the
    function call. Similarly, the predictor can detect fairly complex patterns in
    the data. For example, we can initialize our *random* condition variables so that
    the values always alternate, the first one is random, but the next one is the
    opposite of the first one, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The profiler confirms that the branch prediction rate for this data is excellent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.26 – Branch prediction rate for a "true-false" pattern'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.26_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.26 – Branch prediction rate for a "true-false" pattern
  prefs: []
  type: TYPE_NORMAL
- en: We are almost ready to apply our knowledge of how to use the processor efficiently.
    But first, I must admit that we have overlooked a major potential problem.
  prefs: []
  type: TYPE_NORMAL
- en: Speculative execution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We understand now how pipelining keeps the CPU busy and how, by predicting
    the results of conditional branches and executing the expected code speculatively,
    before we know for sure that it must be executed, we allow the conditional code
    to be pipelined. *Figure 3.21* illustrates this approach: by assuming that the
    end of the loop condition is not going to happen after the current iteration,
    we can interleave the instruction from the next iteration with those of the current
    one, so we have more instructions to execute in parallel.'
  prefs: []
  type: TYPE_NORMAL
- en: Sooner or later, our prediction will be wrong, but all we have to do is discard
    some results that should have never been computed in the first place and make
    it look like they were indeed never computed. This costs us some time, but we
    more than make up for it by speeding up the pipeline when the branch prediction
    is correct. But is this really all we have to do to cover up the fact that we
    tried to execute some code that doesn't really exist?
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider *Figure 3.21* again: if the i-th iteration is the last iteration in
    the loop, then the next iteration should not have happened. Sure, we can discard
    the value `a[i+1]` and not write it into memory. But, in order to do any pipelining,
    we have to read the value of `v1[i+1]`. There is no *discarding* the fact that
    we read it: we access `v1[i+1]` before we check whether the iteration `i` is the
    last iteration, and there is no way to deny that we did access it. But the element
    `v1[i+1]` is outside of the valid memory region allocated for the vector; even
    reading it results in undefined behavior.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An even more convincing example of the dangers hiding behind the innocent label
    of *speculative execution* is this very common code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us assume that the pointer `p` is rarely `NULL`, so the branch predictor
    learns that the `true` branch of the `if(p)` statement is usually taken. What
    happens when the function is finally called with `p == NULL`? The branch predictor
    is going to assume the opposite, as usual, and the `true` branch is executed speculatively.
    The first thing it does is dereference a `NULL` pointer. We all know what happens
    next: the program will crash. Later, we would have discovered that oops, very
    sorry, we should not have taken that branch in the first place, but how do you
    undo a crash?'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the fact that code like our function `f()` is very common and does not
    suffer from unexpected random crashes, we can conclude that either the speculative
    execution does not really exist, or there is a way to undo a crash, sort of. We
    have seen some evidence that speculative execution indeed happens and is very
    effective for improving performance. We will see more direct evidence in the next
    chapter. How, then, does it handle the situation when we speculatively attempt
    to do something impossible, like dereferencing a `NULL` pointer? The answer is,
    the catastrophic response to such a potential disaster must be held pending, neither
    discarded nor allowed to become a reality until the branch condition is actually
    evaluated, and the processor knows whether the speculative execution should be
    considered a real execution or not. In this regard, the faults and other invalid
    conditions are no different from the ordinary memory writes: any action that cannot
    be undone is treated as a potential action as long as the instruction that caused
    that action remains speculative. The CPU must have special hardware circuits,
    such as buffers, to store these events temporarily. The end result is, the processor
    really does dereference a `NULL` pointer or read the non-existing vector element
    `v[i+1]` during the speculative execution, then pretends that it never happened.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand how branch prediction and speculative execution allow
    the processor to operate efficiently despite the uncertainties created by the
    data and code dependencies, we can turn our attention to optimizing our programs.
  prefs: []
  type: TYPE_NORMAL
- en: Optimization of complex conditions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When it comes to a program with many conditional statements, usually `if()`
    statements, the effectiveness of the branch prediction often determines the overall
    performance. If the branch is predicted accurately, it has almost no cost. If
    the branch is mispredicted half the time, it can be as expensive as ten or more
    regular arithmetic instructions.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is very important to understand that the hardware branch prediction is based
    on the conditional instructions executed by the processor. As such, the processor''s
    understanding of what a *condition* is can be different from our understanding.
    The following example helps to drive this point home, with force:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Of interest here is the condition `if (b1[i] || b2[i])`: by construction, it
    always evaluates to `true`, so we can expect the perfect prediction rate from
    the processor. Of course, I would not be showing this example to you if it was
    as simple as that. What is, logically, a single condition to us is, to the CPU,
    two separate conditional branches: half the time, the overall result is true because
    of the first branch, and the other half the time, it is the second branch that
    makes it true. The overall result is always true, but it is impossible to predict
    which of the branches makes it so. The result is quite unfortunate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.27 – Branch prediction profile of a "fake" branch'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.27_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.27 – Branch prediction profile of a "fake" branch
  prefs: []
  type: TYPE_NORMAL
- en: 'The profiler shows the branch prediction rate that is just as poor as that
    of a truly random branch. The performance benchmark confirms our expectations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.28'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.28_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.28
  prefs: []
  type: TYPE_NORMAL
- en: The performance of the *fake* branch (that isn't really a branch at all) is
    just as bad as that of a truly random, unpredictable branch, and is far worse
    than that of a well-predicted branch.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a real program, you should not encounter such unnecessary conditional statements.
    What is very common, however, is a complex conditional expression that almost
    always evaluates to the same value but for different reasons. For example, we
    may have a condition that is very rarely false:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'However, nearly half the time, `c3` is true. When `c3` is false, `c1` and `c2`
    are usually both true. The overall condition should be easily predictable, and
    the true branch is taken. However, from the processor''s point of view, it is
    not a single condition but three separate conditional jumps: if `c1` is true,
    then `c2` must be checked. If `c2` is also true, then the execution jumps to the
    first instruction of the true branch. If one of `c1` or `c2` is false, then `c3`
    is checked, and, if it''s true, the execution again jumps to the true branch.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The reason this evaluation must be done step by step and in that specific order
    is that the C++ standard (and the C standard before it) dictates that the logical
    operations such as `&&` and `||` are *short-circuited*: as soon as the result
    of the entire expression is known, the evaluation of the rest of the expression
    should stop. This is particularly important when the conditions have side effects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Now the function `f2()` will be called only if `f1()` returns `false`. In the
    previous example, the conditions were simply boolean variables `c1`, `c2`, and
    `c3`. The compiler could have detected that there are no side effects and that
    evaluating the entire expression to the end will not change the observable behavior.
    Some compilers do this optimization; if our *fake branch* benchmark were compiled
    with such a compiler, it would have shown the performance of a well-predicted
    branch. Most compilers, unfortunately, do not recognize this as a potential problem
    (and, in fact, there is no way for the compiler to know that the entire expression
    usually evaluates to true, even if its parts do not). So, this is an optimization
    that the programmer must usually do manually.
  prefs: []
  type: TYPE_NORMAL
- en: Assume that the programmer knows that one of the two branches of the `if()`
    statement is taken much more often. For example, the `else` branch could correspond
    to an error situation or some other exceptional condition that must be handled
    properly but should not arise under normal operation. Let us also assume that
    we did things right and verified, using the profiler, that the individual conditional
    instructions that make up the complex boolean expression are not well-predicted.
    How do we optimize the code?
  prefs: []
  type: TYPE_NORMAL
- en: 'The first impulse may be to move the condition evaluation out of the `if()`
    statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: However, this is almost guaranteed not to work, for two reasons. First, the
    condition expression is still using the logical `&&` and `||` operations, so the
    evaluation still must be short-circuited and will require separate and unpredictable
    branches. Second, the compiler will likely optimize this code by removing the
    unnecessary temporary variable `c`, so the resulting object code probably will
    not change at all.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of a loop over an array of conditional variables, a similar transformation
    may be effective. For example, this code is likely to suffer from poor branch
    prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'However, if we pre-evaluate all the conditional expressions and store them
    in a new array, most compilers will not eliminate that temporary array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Of course, the boolean expression used to initialize `c[i]` now suffers from
    branch misprediction, so this transformation works only if the second loop is
    executed many more times than the initialization loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another optimization that is usually effective is to replace the logical `&&`
    and `||` operations with addition and multiplication or with bitwise `&` and `|`
    operations. Before doing this, you must be certain that the arguments to the `&&`
    and `||` operations are booleans (have values of zero or one) and not integers:
    even though a value of, say, `2` is interpreted as true, the result of the expression
    `2 & 1` is not the same as the result of `bool(2) & bool(1)`. The former evaluates
    to 0 (false) while the latter gives us the expected and correct answer, 1 (or
    true).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can compare the performance of all these optimizations in a benchmark:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.29'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.29_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.29
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the naïve attempt to optimize the *false branch* by introducing
    a temporary variable, `BM_false_branch_temp`, was utterly ineffective. Using a
    temporary vector gives us the expected performance of a perfectly predicted branch
    because all elements of the temporary vector are equal to true, and that is what
    the branch predictor learns (`BM_false_branch_vtemp`). Replacing the logical `||`
    with either arithmetic addition (`+`) or the bitwise `|` produces similar results.
  prefs: []
  type: TYPE_NORMAL
- en: 'You should keep in mind that the last two transformations, using arithmetic
    or bitwise operations instead of logic operations, change the meaning of your
    code: specifically, all arguments to the operations in the expression are always
    evaluated, including their side effects. It is up to you to decide whether this
    change will affect the correctness of your program. If these side effects are
    also expensive, then the overall performance change may end up being not in your
    favor. For example, if evaluating `f1()` and `f2()` is very time-consuming, replacing
    the logical `||` in the expression `f1() || f2()` by the equivalent arithmetic
    addition (`f1() + f2()`) may degrade the performance even as it improves the branch
    prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: Overall, there is no standard approach to optimizing the branch prediction in
    *false branches*, which is why it is so hard for the compiler to do any effective
    optimizations as well. The programmer must use problem-specific knowledge, such
    as whether a particular condition is likely to occur, and combine it with profiling
    measurements to arrive at the optimal solution.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we have learned how CPU operations affect performance, then
    progressed to a concrete and practically relevant example of applying this knowledge
    to code optimization. Before the end, we will learn about one more such optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Branchless computing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is what we have learned so far: to use the processor efficiently, we must
    give it enough code to execute many instructions in parallel. The main reason
    we may not have enough instructions to keep the CPU busy is the data dependencies:
    we have the code, but we cannot run it because the inputs aren''t ready. We solve
    this problem by pipelining the code, but in order to do so, we must know in advance
    which instructions are going to be executed. We cannot do this if we do not know
    in advance which path the execution will take. The way we deal with that is by
    making an educated guess about whether a conditional branch will be taken or not,
    based on the history of evaluating this condition. The more reliable the guess,
    the better the performance. Sometimes, there is no way to guess reliably, and
    performance suffers.'
  prefs: []
  type: TYPE_NORMAL
- en: The root of all of these performance problems is the conditional branches, where
    the next instruction to be executed is not known until runtime. A radical solution
    to the problem would be to rewrite our code to use no branches or at least much
    fewer of them. This is known as **branchless computing**.
  prefs: []
  type: TYPE_NORMAL
- en: Loop unrolling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In truth, the idea is not particularly novel. Now that you understand the mechanism
    by which the branches affect performance, you can recognize the well-known technique
    of loop unrolling as an early example of code transformation for the purpose of
    reducing the number of branches. Let us go all the way back to our original code
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We understand now that, while the body of the loop is perfectly pipelined,
    there is a hidden branch in this code: the end of the loop check. This check is
    performed once per loop iteration. If we had prior knowledge that, say, the number
    of iterations `N` is always even, then we don''t need to perform the check after
    odd iterations. We can explicitly omit this check as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We have unrolled this loop, converted two iterations into one larger one. In
    this and other similar examples, the manual unrolling is not likely to improve
    performance for several reasons: first of all, the end of the loop branch is predicted
    almost perfectly if `N` is large. Second, the compiler may do the unrolling anyway
    as an optimization; more likely, a vectorizing compiler will use SSE or AVX instructions
    to implement this loop, which, in effect, unrolls its body since the vector instructions
    process several array elements at once. All of these conclusions need to be confirmed
    by benchmarking or profiling; just don''t be surprised if you find out that manual
    loop unrolling had no effect on performance: this does not mean that what we learned
    about branches is not true; it means that our original code already had the benefit
    of loop unrolling, thanks to the compiler optimizations most likely.'
  prefs: []
  type: TYPE_NORMAL
- en: Branchless selection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Loop unrolling is a very specific optimization that the compilers were *taught*
    to do. Generalizing this idea into branchless computing is a recent advance that
    can yield spectacular performance gains. We will start with a very simple example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us assume that the conditional variable `b1[i]` cannot be predicted by
    the processor. As we have already seen, this code runs several times slower than
    a loop with a well-predicted branch. However, we can do even better; we can eliminate
    the branch entirely and replace it by indexing into an array of pointers to the
    two destination variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: In this transformation, we take advantage of the fact that a boolean variable
    can have only two values, 0 (`false`) or 1 (`true`), and is implicitly convertible
    to an integer (if we used some other type instead of `bool`, we would have to
    make sure that all `true` values are indeed represented by 1 since any non-zero
    value is considered `true` but only the value of 1 works in our branchless code).
  prefs: []
  type: TYPE_NORMAL
- en: 'This transformation replaces a conditional jump to one of two possible instructions
    by conditional access of one of two possible memory locations. Because such conditional
    memory accesses can be pipelined, the branchless version delivers significant
    performance improvement:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.30'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.30_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.30
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, the branchless version of the code is 3.5 times faster. It
    is worth noting that some compilers implement the `?:` operator using a lookup
    array instead of a conditional branch whenever possible. With such a compiler,
    we can gain the same performance benefit by rewriting our loop body as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: As usual, the only way to be certain whether this optimization works or how
    effective it is, is to measure.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding example covers all essential elements of branchless computing:
    instead of conditionally executing this code or that code, we transform the program
    so that the code is the same in all cases, and the conditional logic is implemented
    by an indexing operation. We will go through several more examples to highlight
    some noteworthy considerations and limitations.'
  prefs: []
  type: TYPE_NORMAL
- en: Branchless computing examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Most of the time, the code that depends on the condition is not as simple as
    where to write the result. Usually, we have to do the computations differently
    depending on some intermediate values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Here the condition affects what expression we compute and where the result is
    stored. The only thing common to both branches is the input, and even that doesn't
    have to be the case, in general.
  prefs: []
  type: TYPE_NORMAL
- en: 'To compute the same results without branches, we have to fetch the result of
    the correct expression from a memory location indexed by the condition variable.
    This implies that both expressions will be evaluated since we decided not to change
    which code we execute based on the condition. With this understanding, the transformation
    to the branchless form is straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Both expressions are evaluated, and their results are stored in an array. Another
    array is used to index the destination of the computation, that is, which variable
    is incremented. Overall, we have significantly increased the amount of computing
    the loop body must do; on the other hand, it''s all sequential code with no jumps,
    so, as long as the CPU has the resources to do few more operations without spending
    any extra cycles, we should come out ahead. The benchmark confirms that indeed
    this branchless transformation is effective:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.31'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.31_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.31
  prefs: []
  type: TYPE_NORMAL
- en: 'It must be stressed that there is a limit to how many extra computations you
    can do and still outperform the conditional code. There isn''t even a good general
    *rule of thumb* you could use to make an educated guess here (and you should never
    guess about performance anyway). The effectiveness of such optimizations must
    be measured: it is highly dependent on both the code and the data. For example,
    if the branch predictor were highly effective (predictable condition instead of
    a random one), the conditional code would outperform the branchless version:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.32'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.32_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.32
  prefs: []
  type: TYPE_NORMAL
- en: 'Perhaps the most remarkable conclusion we can learn from *Figure 3.31* and
    *Figure 3.32* is just how expensive a pipeline flush (a mispredicted branch) is
    and how much computing the CPU can do at once with instruction-level parallelism.
    The latter can be deduced from the relatively small difference in performance
    between the perfectly predicted branch (*Figure 3.32*) and the branchless implementation
    (*Figure 3.31*). This hidden and largely unused reserve of computing power is
    what branchless computing relies on, and we probably have not exhausted this reserve
    in our example. It is instructive to show another variant of the branchless transformation
    of the same code where, instead of using an array to select the right result variable,
    we always increment both by zero if we don''t want to actually change the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead of an array of destinations, we now have two arrays of intermediate
    values. This version does even more computations unconditionally and yet, provides
    the same performance as the previous branchless code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.33 – The results of Figure 3.31, with the alternative branchless
    implementation added as "BM_branchless1"'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_3.33_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.33 – The results of Figure 3.31, with the alternative branchless implementation
    added as "BM_branchless1"
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important to understand the limitations of the branchless transformations
    and not get carried away. We have already seen the first limitation: branchless
    code usually executes more instructions; therefore, if the branch predictor ends
    up working well, the small number of pipeline flushes may not be enough to justify
    the *optimization*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second reason for the branchless transformation to not perform as expected
    has to do with the compiler: in some cases, the compiler can do an equivalent
    or even better optimization. For example, consider what is known as a **clamp
    loop**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This loop *clamps* the values in an array `c` of `unsigned char` to the limit
    of `128`. Assuming the initial values were random, the condition in the body of
    the loop cannot be predicted with any degree of certainty, and we can expect a
    very high branch misprediction rate. The alternative, branchless, implementation
    uses a `256` elements, one for each possible value of unsigned `char`. The table
    entries `LUT[i]` for indices `i` from 0 to 127 contain the index value itself,
    and the entries `LUT[i]` for the higher indices all contain 128:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'With most modern compilers, this is not optimization at all: the compiler will
    do better with the original code, most likely using SSE or AVX vector instructions
    to copy and clamp multiple characters at once and without any branches at all.
    If we profiled the original code instead of assuming that the branch must be mispredicted,
    we would have discovered that the program does not suffer from a poor branch prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There is one more scenario where a branchless transformation may not pay off,
    and that is the case when the body of the loop is significantly more expensive
    than the branch, even a mispredicted one. This case is notable because it often
    describes loops that make function calls:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we call one of the two functions, `f1()` or `f2()`, depending on the condition
    `b1`. The `if-else` statement can be eliminated, and the code can be made branchless
    if we use an array of function pointers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Is this an optimization worth doing? Often, it isn't. First of all, if the functions
    `f1()` or `f2()` can be inlined, the function pointer call will prevent that.
    **Inlining** is usually a major optimization; giving up inlining to get rid of
    a branch is almost never justified. When the functions are not inlined, the function
    call by itself disrupts the pipeline (this is one reason inlining is such an effective
    optimization). Compared to the cost of the function call, even a mispredicted
    branch is usually not that much.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nonetheless, sometimes this function lookup table is a worthwhile optimization:
    it almost never pays off for just two alternatives, but if we had to choose from
    many functions based on a single condition, the function pointer table is more
    efficient than a chained `if-else` statement. It is worth noting that this example
    is very similar to the implementation used by all modern compilers to implement
    virtual function calls; such calls are also dispatched using an array of function
    pointers instead of a chain of comparisons. When faced with a need to optimize
    code that calls one of several functions based on a runtime condition, you should
    consider whether a redesign using polymorphic objects is worthwhile.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You should also keep in mind the effect of the branchless transformations on
    the readability of your code: a lookup table of function pointers is not as easy
    to read and can be much harder to debug than a `switch` or `if-else` statement.
    Given the many factors contributing to the final outcome (compiler optimizations,
    hardware resource availability, the nature of the data the program operates on),
    any optimizations must be verified by measurements such as benchmarks and profiles
    and weighed against the additional cost imposed on the programmer in terms of
    time, readability, and complexity.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we have learned about the computing capabilities of the main
    processor and how to use them effectively. The key to high performance is to make
    maximum use of all available computing resources: a program that computes two
    results at the same time is faster than the one that computes the second result
    later (assuming the computing power is available). As we have learned, the CPU
    has a lot of computing units for various types of computations, most of which
    are idle at any given moment unless the program is very highly optimized.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have seen that the main restriction on efficient use of the CPU''s instruction-level
    parallelism is usually the data dependencies: there simply isn''t enough work
    that can be done in parallel to keep the CPU busy. The hardware solution to this
    problem is pipelining: the CPU doesn''t just execute the code at the current point
    in the program but takes some computations from the future that have no unsatisfied
    data dependencies and executes them in parallel. This works well as long as the
    future is well known: the CPU cannot execute the computations from the future
    if it cannot determine what these computations are. Whenever the CPU must wait
    to determine what machine instructions are to be executed next, the pipeline stalls.
    To reduce the frequency of such stalls, the CPU has special hardware that predicts
    the most probable future, the path through the conditional code that is likely
    to be taken, and executes that code speculatively. The performance of the program,
    thus, depends critically on how well this prediction works.'
  prefs: []
  type: TYPE_NORMAL
- en: We have learned the use of special tools that can help measure the efficiency
    of the code and identify the bottlenecks that limit the performance. Guided by
    the measurements, we have studied several optimization techniques that can make
    the program utilize more of the CPU resources, wait less and compute more, and,
    in the end, help to improve performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Throughout this chapter, we have persistently ignored one step every computation
    must do eventually: access the memory. The inputs for any expression reside in
    memory and must be brought into the registers before the rest of the computation
    takes place. The intermediate results can be stored in the registers, but eventually,
    something has to be written back into memory, or the entire code has no lasting
    effect. As it turns out, memory operations (reads and writes) have a significant
    effect on performance and, in many programs, are the limiting factor that prevents
    further optimizations. The next chapter is dedicated to studying the CPU-memory
    interactions.'
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is the key to using the CPU resources efficiently?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can we use instruction-level parallelism to improve performance?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can the CPU execute computations in parallel if the latter one needs the
    results of the former one?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why are conditional branches much more expensive than simply the cost of evaluating
    a conditional expression?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is speculative execution?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What optimization techniques are available to improve the effectiveness of pipelining
    in code with conditional computations?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
