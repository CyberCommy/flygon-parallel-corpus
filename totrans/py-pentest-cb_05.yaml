- en: Web Scraping with Scrapy and BeautifulSoup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Web spiders with Scrapy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scrapy shell
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linking the extractor with Scrapy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scraping after logging into websites using Scrapy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Scrapy** is one of the most powerful Python web-crawling frameworks, and
    it can help with a lot of basic functionalities for efficiently scraping web pages.'
  prefs: []
  type: TYPE_NORMAL
- en: Web spiders with Scrapy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Web spidering starts with a URL or a list of URLs to visit, and when the spider
    gets a new page, it analyzes the page to identify all the hyperlinks, adding these
    links to the list of URLs to be crawled. This action continues recursively for
    as long as new data is found.
  prefs: []
  type: TYPE_NORMAL
- en: A web spider can find new URLs and index them for crawling or download useful
    data from them. In the following recipe, we will use Scrapy to create a web spider.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can start by installing Scrapy. It can be installed from Python''s `pip`
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Make sure that you have the required permission for installing Scrapy. If any
    errors occur with the permission, use the `sudo` command.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s create a simple spider with the Scrapy:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For creating a new spider project, open the Terminal and go to the folder for
    our spider:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Then run the following command to create a new spider project with `scrapy`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This will create a project with the name `books` and some useful files for
    creating the crawler. Now you have a folder structure, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00011.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we can create a crawler with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This will generate the code for the spider with the name `home`, as we are
    planning to spider the home page of `books.toscrape.com`. Now the folder structure
    inside the `spiders` folder will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00012.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, there is a file named `home.py` inside the `spiders` folder.
    We can open the `home.py` and start editing it. The `home.py` files will have
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '`HomeSpider` is a subclass of `scrapy.spider`. The name is set as `home`, which
    we provided while generating the spider. The `allowed_domains` property defines
    the authorized domains for this crawler and `start_urls` defines the URLs for
    the crawler to start with.'
  prefs: []
  type: TYPE_NORMAL
- en: As the name suggests, the `parse` method parses the content of the accessed
    URLs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Try running the spider with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can rewrite the spider to navigate through the pagination links:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: To navigate through many pages, we can use a subclass of `CrawlSpider`. Import
    the `CrawlSpider` and `Rule` module from `scrapy.spider`. For extracting links,
    we can use `LinkExtractor` from `scrapy.linkextractors`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we have to set the `rules` variable, which is used to set the rule for
    navigating through the pages. Here, we used the `restrict_css` parameter to set
    the `css` class to get to the next page. The `css` class for the next page''s
    URL can be found by inspecting the web page from the browser, as shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00013.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now check the crawler by running it with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This will print all the URLs that the spider parsed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s rewrite the script to get the book `title` and the `price`. For this,
    we have to create a class for our item, so inside the `book` project we will create
    another file with the name `item.py` and define our item to extract:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we define a new class with the details we expect to extract with our
    spider. Now the folder structure will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00014.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, update the `spider/home.py` file to extract the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Update the `parse_page` method to extract the `title` and `price` details from
    each page. To extract the data from the page, we have to use a selector. Here,
    we used the `xpath` selector. XPath is a common syntax or language that is used
    to navigate through XML and HTML documents.
  prefs: []
  type: TYPE_NORMAL
- en: In the `parse_page` method, initially, we selected all the article tags in which
    the book details are placed on the website and iterated through each article tag
    to parse the titles and prices of the books.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get the `xpath` selector for a tag, we can use the Google Chrome browser''s
    XPath tool as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00015.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can use Firefox Inspector as following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00016.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we can run the spider to extract the data to a `.csv` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This will create a file named `book-data.csv` in the current directory containing
    the extracted details.
  prefs: []
  type: TYPE_NORMAL
- en: You can learn more about selectors such as XPath and how to select details from
    a page at [https://doc.scrapy.org/en/latest/topics/selectors.html](https://doc.scrapy.org/en/latest/topics/selectors.html).
  prefs: []
  type: TYPE_NORMAL
- en: Scrapy shell
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scrapy shell is a command-line interface that helps to debug scripts without
    running the entire crawler. We have to provide a URL, and Scrapy shell will open
    up an interface to interact with objects that the spider handles in its callbacks,
    such as a response object.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can go through some simple usage of Scrapy''s interactive shell. The steps
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open up a Terminal window and type the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'After loading the Scrapy shell, it will open up an interface to interact with
    the response object as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00017.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can use this interface to debug the selectors for the `response` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This will print the selector output. With this, we can create and test the extraction
    rules for spiders.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also open the Scrapy shell from the code for debugging errors in extraction
    rules. For that, we can use the `inspect_response` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This will open up a shell interface if the condition fails. Here, we have imported
    `inspect_response` and used it to debug the spider from the code.
  prefs: []
  type: TYPE_NORMAL
- en: Link extractor with Scrapy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As their name indicates, link extractors are the objects that are used to extract
    links from the Scrapy response object. Scrapy has built-in link extractors, such
    as `scrapy.linkextractors`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s build a simple link extractor with Scrapy:'
  prefs: []
  type: TYPE_NORMAL
- en: As we did for the previous recipe, we have to create another spider for getting
    all the links.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the new `spider` file, import the required modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a new `spider` class and initialize the variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we have to initialize the rule for crawling the URL:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This rule orders the extraction of all unique and canonicalized links, and also
    instructs the program to follow those links and parse them using the `parse_page`
    method
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can start the spider using the list of URLs listed in the `start_urls`
    variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The `start_requests()` method is called once when the spider is opened for scraping
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can write the method to parse the URLs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This method extracts all canonicalized and unique links with respect to the
    current response. It also verifies that the domain of the URL of the link is in
    one of the authorized domains.
  prefs: []
  type: TYPE_NORMAL
- en: Scraping after logging into websites using Scrapy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are situations where we have to log into websites to access the data we
    are planning to extract. With Scrapy, we can handle the login forms and cookies
    easily. We can make use of Scrapy's `FormRequest` object; it will deal with the
    login form and try to log in with the credentials provided.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we visit a website that has authentication, we need a username and password.
    In Scrapy, we need the same credentials to log in. So we need to get an account
    for the website that we plan to scrape.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is how we can use Scrapy to crawl websites which require logging in:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To use the `FormRequest` object, we can update the `parse_page` method as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Here, the response object is the HTTP response of the page where we have to
    fill in the login form. The `FormRequest` method includes the credentials that
    we need to log in and the `callback` method that is used to parse the page after
    login.
  prefs: []
  type: TYPE_NORMAL
- en: To paginate after logging in while preserving the logged-in session, we can
    use the method we used in the previous recipe.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
