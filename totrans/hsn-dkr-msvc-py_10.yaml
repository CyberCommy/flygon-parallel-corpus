- en: Configuring and Securing the Production System
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Production (from production environment) is a common name to describe the main
    system—the one that does the work for the real customers. This is the main environment
    available in the company. It can also be called **l****ive**. This system needs
    to be openly available on the internet to be useful, which also makes security
    and reliability a big priority. In this chapter, we'll see how to deploy a Kubernetes
    cluster for production.
  prefs: []
  type: TYPE_NORMAL
- en: We'll see how to set one up using a third-party offering **Amazon Web Services**
    (**AWS**), and will cover why it's a bad idea to create your own. We will deploy
    our system in this new deployment, and will check how to set up a load balancer
    to move traffic from the old monolith to the new system in an ordered fashion.
  prefs: []
  type: TYPE_NORMAL
- en: We will also see how to scale automatically both the pods inside the Kubernetes
    cluster and the nodes to adapt the resources to the needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Using Kubernetes in the wild
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up the Docker registry
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating the cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using HTTPS and TLS to secure external access
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Being ready for migration to microservices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autoscaling the cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying a new Docker image smoothly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will also cover some good practices to ensure that our deployments get deployed
    as smoothly and reliably as possible. By the end of the chapter, you'll have the
    system deployed in a publicly available Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use AWS as our cloud vendor for the examples. We need to install some
    utilities to interact from the command line. Check how to install the AWS CLI
    utility in this documentation ([https://aws.amazon.com/cli/](https://aws.amazon.com/cli/)).
    This utility allows performing AWS tasks from the command line.
  prefs: []
  type: TYPE_NORMAL
- en: To operate the Kubernetes cluster, we will use `eksctl`. Check this documentation
    ([https://eksctl.io/introduction/installation/](https://eksctl.io/introduction/installation/))
    for installation instructions.
  prefs: []
  type: TYPE_NORMAL
- en: You'll need also to install `aws-iam-authenticator`. You can check the installation
    instructions here ([https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html](https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for this chapter can be found on GitHub at this link: [https://github.com/PacktPublishing/Hands-On-Docker-for-Microservices-with-Python/tree/master/Chapter07](https://github.com/PacktPublishing/Hands-On-Docker-for-Microservices-with-Python/tree/master/Chapter07).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Be sure you have `ab` (Apache Bench) installed on your computer. It is bundled
    with Apache, and it''s installed by default in macOS and some Linux distributions.
    You can check this article: [https://www.petefreitag.com/item/689.cfm](https://www.petefreitag.com/item/689.cfm).'
  prefs: []
  type: TYPE_NORMAL
- en: Using Kubernetes in the wild
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When deploying a cluster to be used as production, the best possible advice
    is to use a commercial service. All the main cloud providers (AWS EKS, **Google
    Kubernetes Engine** (**GKE**), and **Azure Kubernetes Service** (**AKS**)) allow
    you to create a managed Kubernetes cluster, meaning that the only required parameter
    is to choose the number and type of physical nodes and then access it through
    `kubectl`.
  prefs: []
  type: TYPE_NORMAL
- en: We will use AWS for the examples in this book, but take a look at the documentation
    of other providers in case they work better for your use case.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes is an abstraction layer, so this way of operation is very convenient.
    The pricing is similar to paying for raw instances to act as node servers and
    removes the need to install and manage the Kubernetes Control Plane so the instances
    act as Kubernetes nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s worth saying it again: unless you have a very good reason, *do not deploy
    your own Kubernetes cluster*; instead, use a cloud provider offering. It will
    be easier and will save you from a lot of maintenance costs. Configuring a Kubernetes
    node in a way that''s performant and implements good practices to avoid security
    problems is not trivial.'
  prefs: []
  type: TYPE_NORMAL
- en: Creating your own Kubernetes cluster may be unavoidable if you have your own
    internal data center, but in any other case it makes much more sense to directly
    use one managed by a known cloud provider. Probably your current provider already
    has an offering for managed Kubernetes!
  prefs: []
  type: TYPE_NORMAL
- en: Creating an IAM user
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AWS works using different users to grant them several roles. They carry different
    permissions that enable the users to perform actions. This system is called **Identity
    and Access Management** (**IAM**) in AWS nomenclature.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a proper IAM user could be quite complicated, depending on your settings
    and how AWS is used in your organization. Check the documentation ([https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html))
    and find the people responsible for dealing with AWS in your organization and
    check with them to see what the required steps are.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the steps to create an IAM user:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to create an AWS user if it is not created with proper permissions.
    Be sure that it will be able to access the API by activating the Programmatic
    access as seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/cedad3a8-ea90-4541-9f22-c5605c90b77e.png)'
  prefs: []
  type: TYPE_IMG
- en: This will show its Access Key, Secret Key, and Password. Be sure to store them
    securely.
  prefs: []
  type: TYPE_NORMAL
- en: 'To access through the command line, you need to use the AWS CLI. With the AWS
    CLI and the access information, configure your command line to use `aws`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You should be able to get the identity to check that the configuration is successful
    using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You can now access the command-line AWS actions.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that the IAM user can create more keys if necessary, revoke the
    existing ones, and so on. This normally is handled by an admin user in charge
    of AWS security. You can read more in the Amazon documentation ([https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html#Using_CreateAccessKey_API](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html#Using_CreateAccessKey_API)).
    Key rotation is a good idea to ensure that old keys are deprecated. You can do
    it through the `aws` client interface.
  prefs: []
  type: TYPE_NORMAL
- en: We will use the web console for some operations, but others require the usage
    of `aws`.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the Docker registry
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We need to be able to reach the Docker registry where the images to be deployed
    are stored. The easiest way of ensuring that the Docker registry is reachable
    is to use the Docker registry in the same service.
  prefs: []
  type: TYPE_NORMAL
- en: You can still use the Docker Hub registry, but using a registry in the same
    cloud provider is typically easier as it's better integrated. It will also help
    in terms of authentication.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to configure an **Elastic Container Registry** (**ECR**), using the
    following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Log into the AWS console and search for Kubernetes or ECR:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/29d9a38c-01d7-4df5-b22d-f0df643270b2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Create a new registry called `frontend`. It will create a full URL, which you
    will need to copy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/17c47ce3-fa05-48a7-aa03-48efcdc28818.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We need to make our local `docker` log in the registry. Note that `aws ecr
    get-login` will return a `docker` command that will log you in, so copy it and
    paste:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can tag the image that we want to push with the full registry name,
    and push it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The image is pushed! You can check it by opening the AWS console in the browser:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/bed774ba-ff0f-45bc-aefa-38ed3337de56.png)'
  prefs: []
  type: TYPE_IMG
- en: We need to repeat the process to also push the Users Backend and Thoughts Backend.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We use the setting of two containers for the deployment of the Users Backend
    and Thoughts Backend, which includes one for the service and another for a volatile
    database. This is done for demonstration purposes, but won't be the configuration
    for a production system, as the data will need to be persistent.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of the chapter, there's a question about how to deal with this situation.
    Be sure to check it!
  prefs: []
  type: TYPE_NORMAL
- en: 'All the different registries will be added. You can check them in the browser
    AWS console:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/cb2a0221-97f4-427d-beab-1bcc17c237e1.png)'
  prefs: []
  type: TYPE_IMG
- en: Our pipelines will need to be adapted to push to this repositories.
  prefs: []
  type: TYPE_NORMAL
- en: A good practice in deployment is to make a specific step called **promotion**,
    where the images ready to use in production are copied to an specific registry,
    lowering the chance that a bad image gets deployed by mistake in production.
  prefs: []
  type: TYPE_NORMAL
- en: This process may be done several times to promote the images in different environments.
    For example, deploy a version in an staging environment. Run some tests, and if
    they are correct, promote the version, copying it into the production registry
    and labelling it as good to deploy on the production environment.
  prefs: []
  type: TYPE_NORMAL
- en: This process can be done with different registries in different providers.
  prefs: []
  type: TYPE_NORMAL
- en: We need to use the name of the full URL in our deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To make our code available in the cloud and publicly accessible, we need to
    set up a working production cluster, which requires two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create the EKS cluster in AWS cloud (this enables you to run the `kubectl` commands
    that operate in this cloud cluster).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy your services, using a set of `.yaml` files, as we've seen in previous
    chapters. The files require minimal changes to adapt them to the cloud.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's check the first step.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the Kubernetes cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The best way to create a cluster is to use the `eksctl` utility. This automates
    most of the work for us, and allows us to scale it later if we have to.
  prefs: []
  type: TYPE_NORMAL
- en: Be aware that EKS is available only in some regions, not all. Check the AWS
    regional table ([https://aws.amazon.com/about-aws/global-infrastructure/regional-product-services/](https://aws.amazon.com/about-aws/global-infrastructure/regional-product-services/)) to
    see the available zones. We will use the Oregon (`us-west-2`) region.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create the Kubernetes cluster, let''s take the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, check that `eksctl` is properly installed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a new cluster. It will take around 10 minutes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This creates the cluster. Checking the AWS web interface will show the newly
    configured elements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The  `--arg-access` option needs to be added for a cluster capable of autoscaling.
    This will be described in more detail in the *Autoscaling the cluster* section.
  prefs: []
  type: TYPE_NORMAL
- en: The `eksctl create` command also adds a new context with the information about
    the remote Kubernetes cluster and activates it, so `kubectl` will now point to
    this new cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that `kubectl` has the concept of contexts as different clusters it can
    connect. You can see all the available contexts running `kubectl config get-contexts`
    and `kubectl config use-context <context-name>` to change them. Check the Kubernetes
    documentation ([https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/](https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/))
    on how to create new contexts manually.
  prefs: []
  type: TYPE_NORMAL
- en: 'This command sets `kubectl` with the proper context to run commands. By default,
    it generates a cluster with two nodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can scale the number of nodes. To reduce the usage of resources and save
    money. We need to retrieve the name of the nodegroup, which controls the number
    of nodes, and then downscale it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'You can contact the cluster through `kubectl` and carry the operations normally:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The cluster is set up, and we can run commands on it from our command line.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an EKS cluster can be tweaked in a lot of ways, but AWS can be temperamental
    in terms of access, users, and permissions. For example, the cluster likes to
    have a CloudFormation rule to handle the cluster, and all the elements should
    be created with the same IAM user. Check with anyone that works with the infrastructure
    definition in your organization to check what's the proper configuration. Don't
    be afraid of running tests, a cluster can be quickly removed through the `eksctl`
    configuration or the AWS console.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, `eksctl` creates the cluster with the nodes in different availability
    zones (AWS isolated locations within the same geographical region) wherever possible,
    which minimizes the risk of getting the whole cluster down because of problems
    in AWS data centers.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the cloud Kubernetes cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The next stage is to run our services on the EKS cluster, so it's available
    in the cloud. We will use the `.yaml` files as a base, but we need to make a few
    changes.
  prefs: []
  type: TYPE_NORMAL
- en: Take a look at the files in the GitHub `Chapter07` ([https://github.com/PacktPublishing/Hands-On-Docker-for-Microservices-with-Python/tree/master/Chapter07](https://github.com/PacktPublishing/Hands-On-Docker-for-Microservices-with-Python/tree/master/Chapter07))
    subdirectory.
  prefs: []
  type: TYPE_NORMAL
- en: We will see the differences from the Kubernetes configuration files in the previous
    chapter, and then we will deploy them in the *Deploying the system* section.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the AWS image registry
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first difference is that we need to change the image to the full registry,
    so the cluster uses the images available in the ECS registry.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, you need to specify the registry inside AWS so the AWS cluster can
    properly access it.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in the `frontend/deployment.yaml` file, we need to define them
    in this way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The image should pull from the AWS registry. The pull policy should be changed
    to force pulling from the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can deploy in the remote server by applying the file, after creating the `example` namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'After a bit, the deployment creates the pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Now we need to change the rest of the elements. All the deployments need to
    be adapted to include the proper registry.
  prefs: []
  type: TYPE_NORMAL
- en: Check the code on GitHub to check all the `deployment.yaml` files.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the usage of an externally accessible load balancer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The second difference is to make the frontend service available externally,
    so internet traffic can access the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is easily done by changing the service from `NodePort` to `LoadBalancer`.
    Check the  `frontend/service.yaml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This creates a new **Elastic Load Balancer** (**ELB**) that can be externally
    accessed. Now, let's start deploying.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The whole system can be deployed, from the `Chapter07` subdirectory, with the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This commands go iteratively through the subdirectories and applies any `.yaml`
    file.
  prefs: []
  type: TYPE_NORMAL
- en: 'After a few minutes, you should see everything up and running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'To obtain the public access point, you need to check the services:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Note that the frontend service has an external ELB DNS available.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you put that DNS in a browser, you can access the service as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/b27d1c74-f06d-4be2-b017-73f59bb4fa8d.png)'
  prefs: []
  type: TYPE_IMG
- en: Congratulations, you have your own cloud Kubernetes service. The DNS name the
    service is accessible under is not great, so we will see how to add a registered
    DNS name and expose it under an HTTPS endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Using HTTPS and TLS to secure external access
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To provide a good service to your customers, your external endpoint should be
    served through HTTPS. This means that the communication between you and your customers
    is private, and it can't be sniffed throughout the network route.
  prefs: []
  type: TYPE_NORMAL
- en: The way HTTPS works is that the server and client encrypt the communication.
    To be sure that the server is who they say they are, there needs to be an SSL
    certificate issued by an authority that grants that the DNS is verified.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, the point of HTTPS is *not* that the server is inherently trustworthy,
    but that the communication is private between the client and the server. The server
    can still be malicious. That's why verifying that a particular DNS does not contain
    misspellings is important.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can get more information on how HTTPS works in this fantastic comic: [https://howhttps.works/](https://howhttps.works/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Obtaining a certificate for your external endpoint requires two stages:'
  prefs: []
  type: TYPE_NORMAL
- en: You own a particular DNS name, normally by buying it from a name registrar.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You obtain a unique certificate for the DNS name by a recognized **Certificate
    Authority** (**CA**). The CA has to validate that you control the DNS name.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To help in promoting the usage of HTTPS, the non-profit *Let's Encrypt* ([https://letsencrypt.org](https://letsencrypt.org))
    supplies free certificates valid for 60 days. This will be more work than obtaining
    one through your cloud provider, but could be an option if money is tight.
  prefs: []
  type: TYPE_NORMAL
- en: These days, this process is very easy to do with cloud providers as they can
    act as both, simplifying the process.
  prefs: []
  type: TYPE_NORMAL
- en: The important element that needs to communicate through HTTPS is the edge of
    our network. The internal network where our own microservices are communicating
    doesn't require to be HTTPS, and HTTP will suffice. It needs to be a private network
    out of public interference, though.
  prefs: []
  type: TYPE_NORMAL
- en: Following our example, AWS allows us to create and associate a certificate with
    an ELB, serving traffic in HTTP.
  prefs: []
  type: TYPE_NORMAL
- en: Having AWS to serve HTTPS traffic ensures that we are using the latest and safest
    security protocols, such as **Transport Layer Security** (**TLS**) v1.3 (the latest
    at the time of writing), but also that it keeps backward compatibility with older
    protocols, such as SSL.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, it is the best option to use the most secure environment by
    default.
  prefs: []
  type: TYPE_NORMAL
- en: The first step of setting HTTPS is either to buy a DNS domain name directly
    from AWS or to transfer  the control to AWS. This can be done through their service
    Route 53\. You can check the documentation at [https://aws.amazon.com/route53/](https://aws.amazon.com/route53/).
  prefs: []
  type: TYPE_NORMAL
- en: It is not strictly required to transfer your DNS to Amazon, as long as you can
    point it toward the externally facing ELB, but it helps with the integration and
    obtaining of certificates. You'll need to prove that you own the DNS record when
    creating a certificate, and using AWS makes it simple as they create a certificate
    to a DNS record they control. Check the documentation at [https://docs.aws.amazon.com/acm/latest/userguide/gs-acm-validate-dns.html](https://docs.aws.amazon.com/acm/latest/userguide/gs-acm-validate-dns.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'To enable HTTPS support on your ELB, let''s check the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to Listeners in the AWS console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/fa6eb258-65a0-4953-a49c-4f0ac559524f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Click on Edit and add a new rule for HTTPS support:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/9bceff02-8ad6-4194-8315-75ea59238415.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, it will require an SSL certificate. Click on Change to go to
    management:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/c0378dc3-fd4b-4c3a-ab55-11a07af6a74d.png)'
  prefs: []
  type: TYPE_IMG
- en: From here, you can either add an existing certificate or buy one from Amazon.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Be sure to check the documentation about the load balancer in Amazon. There
    are several kinds of ELBs that can be used, and some have different features than
    others depending on your use case. For example, some of the new ELBs are able
    to redirect toward HTTPS even if your customer requests the data in HTTP. See
    the documentation at [https://aws.amazon.com/elasticloadbalancing/](https://aws.amazon.com/elasticloadbalancing/).
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations, now your external endpoint supports HTTPS, ensuring that your
    communications with your customers are private.
  prefs: []
  type: TYPE_NORMAL
- en: Being ready for migration to microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To operate smoothly while making the migration, you need to deploy a load balancer
    that allows you to quickly swap between backends and keeps the service up.
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed in [Chapter 1](ddb0a00a-6c5b-4ffe-b403-0f5f9f7a7df2.xhtml),
    *Making the Move – Design, Plan, and Execute*, HAProxy is an excellent choice
    because it is very versatile and has a good UI that allows you to make operations
    quickly just by clicking on a web page. It also has an excellent stats page that
    allows you to monitor the status of the service.
  prefs: []
  type: TYPE_NORMAL
- en: AWS has an alternative to HAProxy called **Application Load Balancer** (**ALB**).
    This works as a feature-rich update on the ELB, which allows you to route different
    HTTP paths into different backend services.
  prefs: []
  type: TYPE_NORMAL
- en: HAProxy has a richer set of features and a better dashboard to interact with
    it. It can also be changed through a configuration file, which helps in controlling
    changes, as we will see in [Chapter 8](9a5c53a2-9131-4233-9e4f-992af51d8321.xhtml),
    *Using GitOps Principles*. It is, obviously, only available if all the services
    are available in AWS, but it can be a good solution in that case, as it will be
    simpler and more aligned with the rest of the technical stack. Take a look at
    the documentation at [https://aws.amazon.com/blogs/aws/new-aws-application-load-balancer/](https://aws.amazon.com/blogs/aws/new-aws-application-load-balancer/).
  prefs: []
  type: TYPE_NORMAL
- en: To deploy a load balancer in front of your service, I recommend not deploying
    it on Kubernetes, but running it in the same way as your traditional services.
    This kind of load balancer will be a critical part of the system, and removing
    uncertainty is important for a successful run. It's also a relatively simple service.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that a load balancer needs to be properly replicated, or it becomes
    a single point of failure. Amazon and other cloud providers allow you to set up
    an ELB or other kinds of load balancer toward your own deployment of load balancers,
    enabling the traffic to be balanced among them.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, we've created an example configuration and the `docker-compose`
    file to quickly run it, but the configuration can be set up in whatever way your
    team is most comfortable with.
  prefs: []
  type: TYPE_NORMAL
- en: Running the example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code is available on GitHub ([https://github.com/PacktPublishing/Hands-On-Docker-for-Microservices-with-Python/tree/master/Chapter07/haproxy](https://github.com/PacktPublishing/Hands-On-Docker-for-Microservices-with-Python/tree/master/Chapter07/haproxy)).
    We inherit from the HAProxy Docker image in Docker Hub ([https://hub.docker.com/_/haproxy/](https://hub.docker.com/_/haproxy/)),
    adding our own config file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the main elements in the config file, `haproxy.cfg`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We define a frontend that accepts any requests into port `80` and sends the
    requests toward the backend. The backend balances the requests to two servers,
    `example` and `aws`. Basically, `example` points to `www.example.com` (a placeholder
    for your old service) and `aws` to the previously created load balancer.
  prefs: []
  type: TYPE_NORMAL
- en: We enable the stats server in port `8001` and allow admin access.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `docker-compose` config starts the server and forwards the localhost ports
    towards the container ports `8000` (load balancer) and `8001` (stats). Start it
    with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Now we can access `localhost:8000`, which will alternate between the `thoughts`
    service and a 404 error.
  prefs: []
  type: TYPE_NORMAL
- en: When calling `example.com` this way, we are forwarding the host request. This
    means we send a request requesting `Host:localhost` to `example.com`, which returns
    a 404 error. Be sure to check on your service that the same host information is
    accepted by all the backends.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the stats page to check the setup:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/50260323-0ec3-4f15-a91e-3e66ece92b0e.png)'
  prefs: []
  type: TYPE_IMG
- en: Check the entries for `aws` and `example` in the backend nodes. There is also
    a lot of interesting information, such as the number of requests, the last connection,
    data, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can perform actions when checking the `example` backend, and then set state
    to MAINT in the drop-down menu. Once applied, the `example` backend is in maintenance
    mode and removed from the load balancer. The stats page is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/c4ee4c3e-ae10-4597-b4ff-f3ec513ce6ee.png)'
  prefs: []
  type: TYPE_IMG
- en: Accessing the load balancer in `localhost:8000` now will only return the **thoughts**
    frontend. You can re-enable the backend setting it to the READY state.
  prefs: []
  type: TYPE_NORMAL
- en: There's a state called DRAIN that will stop new sessions going to the selected
    server, but existing sessions will keep going. This may be interesting in some
    configurations, but if the backend is truly stateless, moving directly to the MAINT
    state should be enough.
  prefs: []
  type: TYPE_NORMAL
- en: 'HAProxy can also be configured to use checks to ensure that the backend is
    available. We added in the example a commented one, which sends an HTTP command
    to check a return:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The check will be the same to both backends, so it needs to be returned successfully.
    By default, it will be run every couple of seconds.
  prefs: []
  type: TYPE_NORMAL
- en: You can check the full HAProxy documentation at [http://www.haproxy.org/](http://www.haproxy.org/).
    There are a lot of details that can be configured. Follow up with your team to
    be sure that the configuration of areas like timeouts, forwarding headers, and
    so on are correct.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of the health check is also used in Kubernetes to ensure that pods
    and containers are ready to accept requests and are stable. We'll see how to ensure
    that a new image is deployed correctly in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a new Docker image smoothly
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When deploying a service in a production environment, it is critically important
    to ensure that it is going to work smoothly to avoid interrupting the service.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes and HAProxy are able to detect when a service is running correctly,
    and take action when it's not, but we need to give an endpoint that acts as a
    health check and configure it to be pinged at regular intervals, for early detection
    of problems.
  prefs: []
  type: TYPE_NORMAL
- en: For simplicity, we will use the root URL as a health check, but we can design
    specific endpoints to be tested. A good health checkup checks that the service
    is working as expected, but is light and quick. Avoid the temptation of over testing
    or performing an external verification that could make the endpoint take a long
    time.
  prefs: []
  type: TYPE_NORMAL
- en: An API endpoint that returns an empty response is a great example, as it checks
    that the whole piping system works, but it's very fast to answer.
  prefs: []
  type: TYPE_NORMAL
- en: In Kubernetes, there are two tests to ensure that a pod is working correctly,
    the readiness probe and the liveness probe.
  prefs: []
  type: TYPE_NORMAL
- en: The liveness probe
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The liveness probe checks that a container is working properly. It is a process
    started in the container that returns correctly. If it returns an error (or more,
    depending on the config), Kubernetes will kill the container and restart it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The liveness probe will be executed inside the container, so it needs to be
    valid. For a web service, adding a `curl` command is a good idea:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: While there are options such as checking if a TCP port is open or sending an
    HTTP request, running a command is the most versatile option. It can also be checked
    for debugging purposes. See the documentation for more options.
  prefs: []
  type: TYPE_NORMAL
- en: Be careful of being very aggressive on liveness probes. Each check puts some
    load on the container, so depending on load multiple probes can end up killing
    more containers than they should.
  prefs: []
  type: TYPE_NORMAL
- en: If your services are restarted often by the liveness probe, either the probe
    is too aggressive or the load is high for the number of containers, or a combination
    of both.
  prefs: []
  type: TYPE_NORMAL
- en: The probe is configured to wait for five seconds, and then run once every 30
    seconds. By default, after three failed checks, it will restart the container.
  prefs: []
  type: TYPE_NORMAL
- en: The readiness probe
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The readiness probe checks if the container is ready to accept more requests.
    It's a less aggressive version. If the tests return an error or timeout, the container
    won't be restarted, but it will be just labeled as unavailable.
  prefs: []
  type: TYPE_NORMAL
- en: The readiness probe is normally used to avoid accepting requests too early,
    but it will run after startup. A smart readiness probe can label when a container
    is at full capacity and can't accept more requests, but normally a probe configured
    in a similar way to the liveness prove will be enough.
  prefs: []
  type: TYPE_NORMAL
- en: 'The readiness probe is defined in the deployment configuration, in the same
    fashion as the liveness probe. Let''s take a look:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The readiness probe should be more aggressive than the liveness probe, as the
    result is safer. That's why `periodSeconds` is shorter. You may require both or
    not, depending on your particular use case, but readiness probes are required
    to enable rolling updates, as we'll see next.
  prefs: []
  type: TYPE_NORMAL
- en: The `frontend/deployment.yaml` deployment in the example code includes both
    probes. Check the Kubernetes documentation ([https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/))
    for more details and options.
  prefs: []
  type: TYPE_NORMAL
- en: Be aware that the two probes are used for different objectives. The readiness
    probe delays the input of requests until the pod is ready, while the liveness
    probe helps with stuck containers.
  prefs: []
  type: TYPE_NORMAL
- en: A delay in the liveness probe getting back will restart the pod, so an increase
    in load could produce a cascade effect of restarting pods. Adjust accordingly,
    and remember that both probes don't need to repeat the same command.
  prefs: []
  type: TYPE_NORMAL
- en: Both readiness and liveness probes help Kubernetes to control how pods are created,
    which affects the update of a deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Rolling updates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By default, each time that we update an image for deployment, the Kubernetes
    deployment will recreate the containers.
  prefs: []
  type: TYPE_NORMAL
- en: Notifying Kubernetes that a new version is available is not enough to push a
    new image to the registry, even if the tag is the same. You'll need to change
    the tag described in the `image` field in the deployment `.yaml` file.
  prefs: []
  type: TYPE_NORMAL
- en: We need to control how the images are being changed. To not interrupt the service,
    we need to perform a rolling update. This kind of update adds new containers,
    waits until they are ready, adds them to the pool, and removes the old containers.
    This deployment is a bit slower than removing all containers and restarting them,
    but it allows the service to be uninterrupted.
  prefs: []
  type: TYPE_NORMAL
- en: 'How this process is performed can be configured by tweaking the `strategy` section
    in the deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s understand this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '`strategy` and  `type` can be either `RollingUpdate` (the default) or `Recreate`,
    which stops existing pods and creates new ones.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxUnavailable` defines the maximum number of unavailable pods during a change.
    This defines how quick o new containers will be added and old ones removed. It
    can be described as a percentage, like our example, or a fixed number.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxSurge` defines the number of extra pods that can be created over the limit
    of desired pods. This can be a specific number or a percentage of the total.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we set `replicas` to `4`, in both cases the result is one pod. This means
    that during a change, up to one pod may be unavailable and that we will create
    the new pods one by one.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Higher numbers will perform the update faster but will consume more resources
    (`maxSurge`) or reduce more aggressively the available resources during the update
    (`maxUnavailable`).
  prefs: []
  type: TYPE_NORMAL
- en: For a small number of replicas, be conservative and grow the numbers when you
    are more comfortable with the process and have more resources.
  prefs: []
  type: TYPE_NORMAL
- en: Initially, scaling the pods manually will be easiest and best option. If the
    traffic is highly variable, with high peaks and low valleys, it may be worth autoscaling
    the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Autoscaling the cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've seen before how to change the number of pods for a service, and how to
    add and remove nodes. This can be automated to describe some rules, allowing the
    cluster to change its resources elastically.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that autoscaling requires tweaking to adjust to your specific use
    case. This is a technique to use if the resource utilization changes greatly over
    time; for example, if there's a daily pattern where some hours present way more
    activity than others, or if there's a viral element that means the service multiplies
    the requests by 10 unexpectedly. If your usage of servers is small and the utilization
    stays relatively constant, there's probably no need to add autoscaling.
  prefs: []
  type: TYPE_NORMAL
- en: 'The cluster can be scaled automatically up or down on two different fronts:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of pods can be set to increase or decrease automatically in a Kubernetes
    configuration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of nodes can be set to increase or decrease automatically in AWS.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both the number of pods and the number of nodes need to be in line with each
    other to allow natural growth.
  prefs: []
  type: TYPE_NORMAL
- en: If the number of pods increases without adding more hardware (nodes), the Kubernetes
    cluster won't have much more capacity, only the same resources allocated in a
    different distribution.
  prefs: []
  type: TYPE_NORMAL
- en: If the number of nodes increases without more pods created, at some point the
    extra nodes won't have pods to allocate, producing underutilization of resources.
    On the other hand, any new node added will have a cost associated, so we want
    to be properly using it.
  prefs: []
  type: TYPE_NORMAL
- en: To be able to automatically scale a pod, be sure that it is scalable. To ensure
    the pod is scalable check that it is an stateless web service and obtain all the
    information from an external source.
  prefs: []
  type: TYPE_NORMAL
- en: Note that, in our code example, the frontend pod is scalable, while the Thoughts
    and Users Backend is not, as they include their own database container the application
    connects to.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a new pod creates a new empty database, which is not the expected behavior.
    This has been done on purpose to simplify the example code. The intended production
    deployment is, as described before, to connect to an external database instead.
  prefs: []
  type: TYPE_NORMAL
- en: Both Kubernetes configuration and EKS have features that allow changing the
    number of pods and nodes based on rules.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Kubernetes Horizontal Pod Autoscaler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Kubernetes nomenclature, the service to scale pods up and down is called
    a **Horizontal Pod Autoscaler** (**H****PA**).
  prefs: []
  type: TYPE_NORMAL
- en: This is because it requires a way of checking the measurement to scale. To enable
    these metrics, we need to deploy the Kubernetes metric server.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the Kubernetes metrics server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Kubernetes metrics server captures internal low-level metrics such as CPU
    usage, memory, and so on. The HPA will capture these metrics and use them to scale
    the resources.
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes metrics server is not the only available server for feeding metrics
    to the HPA, and other metrics systems can be defined. The list of the currently
    available adaptors is available in the Kubernetes metrics project ([https://github.com/kubernetes/metrics/blob/master/IMPLEMENTATIONS.md#custom-metrics-api](https://github.com/kubernetes/metrics/blob/master/IMPLEMENTATIONS.md#custom-metrics-api)).
  prefs: []
  type: TYPE_NORMAL
- en: This allows for custom metrics to be defined as a target. Start first with default
    ones, though, and only move to custom ones if there are real limitations for your
    specific deployment.
  prefs: []
  type: TYPE_NORMAL
- en: To deploy the Kubernetes metrics server, download the latest version from the
    official project page ([https://github.com/kubernetes-incubator/metrics-server/releases](https://github.com/kubernetes-incubator/metrics-server/releases)).
    At the time of writing, it was `0.3.3`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the `tar.gz` file, which at the time of writing was `metrics-server-0.3.3.tar.gz`.
    Uncompress it and apply the version to the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see the new pod in the `kube-system` namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'You can use the `kubectl top` command to get basic information about the nodes
    and pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: To properly control what the limit of usage is, we need to configure in the
    deployment what is allocated and limit resources for it.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the resources in deployments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the configuration for a container, we can specify what the requested resources
    are and the maximum resources for them.
  prefs: []
  type: TYPE_NORMAL
- en: They both inform Kubernetes about the expected memory and CPU usage for a container.
    When creating a new container, Kubernetes will automatically deploy it on a node
    that has enough resources to cover it.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `frontend/deployment.yaml` file, we include the following `resources`
    instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The initially requested memory is 64 MB, and 0.06 of a CPU core.
  prefs: []
  type: TYPE_NORMAL
- en: The resources for memory can also use Mi to the power of 2, which is equivalent
    to a megabyte (*1000²* bytes), which is called a mebibyte (*2^(20)* bytes). The
    difference is small in any case. You can use also G or T for bigger amounts.
  prefs: []
  type: TYPE_NORMAL
- en: The CPU resources are measured fractionally where 1 being a core in whatever
    system the node is running (for example, AWS vCPU). Note that 1000m, meaning 1000
    milli CPUs is equivalent to a whole core.
  prefs: []
  type: TYPE_NORMAL
- en: The limits are 128 MB and 0.07 of a CPU core. The container won't be able to
    use more memory or CPU than the limit.
  prefs: []
  type: TYPE_NORMAL
- en: Aim at round simple numbers to understand the limits and requested resources.
    Don't expect to have them perfect the first time; the applications will change
    their consumption.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring the metrics in an aggregated way, as we will talk about in [Chapter
    11](06d0c451-77f1-4e4a-8d38-3abf112f79fa.xhtml), *Handling Change, Dependencies,
    and Secrets in the System*, will help you to see the evolution of the system and
    tweak it accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: The limit creates the benchmark for the autoscaler, as it will be measured in
    a percentage of the resource.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an HPA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To create a new HPA, we can use the `kubectl autoscale` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: This creates a new HPA that targets the `frontend` deployment in the `example` namespace,
    and sets the number of pods to be between `2` and `8`. The parameter to scale
    is the CPU, which we set to 10% of the available CPU, averaged across all the
    pods. If it's higher, it will create new pods, and if it's lower, it will reduce
    them.
  prefs: []
  type: TYPE_NORMAL
- en: The 10% limit is used to be able to trigger the autoscaler and to demonstrate
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The autoscaler works as a special kind of Kubernetes object, and it can be
    queried as such:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Note how the target says it is currently at around 2%, near the limit. This
    was designed with the small available CPU that will have a relatively high baseline.
  prefs: []
  type: TYPE_NORMAL
- en: After some minutes, the number of replicas will go down until the minimum is
    reached, `2`.
  prefs: []
  type: TYPE_NORMAL
- en: The downscaling may take a few minutes. This generally is the expected behavior,
    upscaling being more aggressive than downscaling.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create some load, let''s use the application Apache Bench (`ab`) in combination
    with a specially created endpoint in the frontend that uses a lot of CPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Note that `ab` is a convenient test application that produces HTTP requests
    concurrently. If you prefer, you can hit the URL from a browser multiple times
    in quick succession.
  prefs: []
  type: TYPE_NORMAL
- en: Remember to add the load balancer DNS, as retrieved in the *Creating the cluster*
    section.
  prefs: []
  type: TYPE_NORMAL
- en: 'This will generate an extra CPU load in the cluster and will make the deployment
    scale up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: After the requests are completed, and after some minutes, the number of pods will
    slowly scale down until hitting the two pods again.
  prefs: []
  type: TYPE_NORMAL
- en: But we need a way of scaling the nodes as well, or we won't be able to grow
    the total number of resources in the system.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling the number of nodes in the cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The number of AWS instances that work as nodes in the EKS cluster can also be
    increased. This adds extra resources to the cluster and makes it possible to start
    more pods.
  prefs: []
  type: TYPE_NORMAL
- en: The underlying AWS service that allows that is an Auto Scaling group. This is
    a group of EC2 instances that share the same image and have a defined size, both
    for the minimum and maximum instances.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the core of any EKS cluster, there''s an Auto Scaling group that controls
    the nodes of the cluster. Note that `eksctl` creates and exposes the Auto Scaling
    group as a nodegroup:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'With `eksctl`, we can scale the cluster up or down manually as we described
    when creating the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'This nodegroup is also visible in the AWS console, under EC2 | Auto Scaling
    Groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/1a35c1f7-06fa-4243-b336-1db202e9b03b.png)'
  prefs: []
  type: TYPE_IMG
- en: In the web interface, we have some interesting information that we can use to
    collect information about the Auto Scaling group. The Activity History tab allows
    you to see any scaling up or down event, and the Monitoring tab allows you to
    check metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the handling has been created automatically with `eksctl`, such as the
    Instance Type and the AMI-ID (the initial software on the instance, containing
    the operating system). They should be mainly controlled by `eksctl`.
  prefs: []
  type: TYPE_NORMAL
- en: If you need to change the Instance Type, `eksctl` requires you to create a new
    nodegroup, move all the pods, and then delete the old. You can learn more about
    the process in the `eksctl` documentation ([https://eksctl.io/usage/managing-nodegroups/](https://eksctl.io/usage/managing-nodegroups/)).
  prefs: []
  type: TYPE_NORMAL
- en: But from the web interface, it is easy to edit the scale parameters and to add
    policies for autoscaling.
  prefs: []
  type: TYPE_NORMAL
- en: Changing the parameters through the web interface may confuse the data retrieved
    in `eksctl`, as it's independently set up.
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to install a Kubernetes autoscaler for AWS, but it requires a `secrets`
    configuration file to include a proper AMI in the autoscaler pod, with AWS permissions
    to add instances.
  prefs: []
  type: TYPE_NORMAL
- en: Describing the autoscale policy in AWS terms in code can also be quite confusing.
    The web interface makes it a bit easier. The pro is that you can describe everything
    in config files that can be under source control.
  prefs: []
  type: TYPE_NORMAL
- en: We will go with the web interface configuration, here, but you can follow the
    instructions at [https://eksctl.io/usage/autoscaling/](https://eksctl.io/usage/autoscaling/).
  prefs: []
  type: TYPE_NORMAL
- en: 'For scaling policies, there are two main components that can be created:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scheduled actions**: They are scale up and down events that happen at defined
    times. The action can change the number of nodes through a combination of the
    desired number and the minimum and maximum number, for example, increasing the
    cluster during the weekend. The actions can be repeated periodically, such as
    each day or each hour. The action can also have an ending time, which will revert
    the values to the ones previously defined. This can be used to give a boost for
    a few hours if we expect extra load in the system, or to reduce costs during night
    hours.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scaling policies**: These are policies that look for demand at a particular
    time and scale up or down the instances, between the described numbers. There
    are three types of policies: target tracking, step scaling, and simple scaling.
    Target tracking is the simplest, as it monitors the target (typically CPU usage)
    and scales up and down to keep close to the number. The other two policies require
    you to generate alerts using the AWS CloudWatch metrics system, which is more
    powerful but also requires using CloudWatch and a more complex configuration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of nodes can change not only going up but also down, which implies
    deleting nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Deleting nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When deleting a node, the pods running need to move to another node. This is
    handled automatically by Kubernetes, and EKS will do the operation in a safe way.
  prefs: []
  type: TYPE_NORMAL
- en: This can also happen if a node is down for any reason, such as an unexpected
    hardware problem. As we've seen before, the cluster is created in multiple availability
    zones to minimize risks, but some nodes may have problems if there's a problem
    in an Amazon availability zone.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes was designed for this kind of problem, so it's good at moving pods
    from one node to another in unforeseen circumstances.
  prefs: []
  type: TYPE_NORMAL
- en: Moving a pod from one node to another is done by destroying the pod and restarting
    it in the new node. As pods are controlled by deployments, they will keep the
    proper number of pods, as described by the replicas or autoscale values.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that pods are inherently volatile and should be designed so they can
    be destroyed and recreated.
  prefs: []
  type: TYPE_NORMAL
- en: Upscaling can also result in existing pods moving to other nodes to better utilize
    resources, though this is less common. An increase in the number of nodes is normally
    done at the same time as growing the number of pods.
  prefs: []
  type: TYPE_NORMAL
- en: Controlling the number of nodes requires thinking about the strategy to follow
    to achieve the best result, depending on the requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Designing a winning autoscaling strategy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we've seen, both kinds of autoscaling, pods and nodes, need to be related
    to each other. Keeping the number of nodes down reduces costs but limits the available
    resources that could be used for growing the number of pods.
  prefs: []
  type: TYPE_NORMAL
- en: Always keep in mind that autoscaling is a big numbers game. Unless you have
    enough load variation to justify it, tweaking it will produce cost savings that
    are not comparable to the cost of developing and maintaining the process. Run
    a cost analysis of expected gains and maintenance costs.
  prefs: []
  type: TYPE_NORMAL
- en: Prioritize simplicity when dealing with changing the size of a cluster. Scaling
    down during nights and weekends could save a lot of money, and it's much easier
    to handle than generating a complex CPU algorithm to detect highs and lows.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that autoscaling is not the only way of reducing costs with cloud
    providers, and can be used combined with other strategies.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in AWS, reserving EC2 instances for a year or more allows you
    to greatly reduce the bill. They can be used for the cluster baseline and combined
    with more expensive on-demand instances for autoscaling, which yields an extra
    reduction in costs: [https://aws.amazon.com/ec2/pricing/reserved-instances/](https://aws.amazon.com/ec2/pricing/reserved-instances/).'
  prefs: []
  type: TYPE_NORMAL
- en: As a general rule, you should aim to have an extra bit of hardware available
    to allow scaling pods, as this is faster. This is allowed in cases where different
    pods are scaled at different rates. It is possible, depending on the application,
    that when the usage of one service goes up, another goes down, which will keep
    the utilization in similar numbers.
  prefs: []
  type: TYPE_NORMAL
- en: This is not the use case that comes to mind, but for example, scheduled tasks
    during the night may make use of available resources that at daytime are being
    used by external requests.
  prefs: []
  type: TYPE_NORMAL
- en: They can work in different services, balancing automatically as the load changes
    from one service to the other.
  prefs: []
  type: TYPE_NORMAL
- en: Once the headroom is reduced, start scaling nodes. Always leave a security margin
    to avoid getting stuck in a situation where the nodes are not scaling fast enough
    and no more pods can be started because of a lack of resources.
  prefs: []
  type: TYPE_NORMAL
- en: The pod autoscaler can try to create new pods, and if there are no resources
    available, they won't be started. In the same fashion, if a node is removed, any
    Pod that is not deleted may not start because of a lack of resources.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that we describe to Kubernetes the requirements for a new pod in the
    `resources` section of the deployment. Be sure that the numbers there are indicative
    of the required ones for the pod.
  prefs: []
  type: TYPE_NORMAL
- en: To ensure that the pods are adequately distributed across different nodes, you
    can use the Kubernetes affinity and anti-affinity rules. These rules allow defining
    whether pods of a certain kind should run in the same node or not.
  prefs: []
  type: TYPE_NORMAL
- en: This is useful, for example, to make sure that all kinds of pods are evenly
    distributed across zones, or for ensuring that two services are always deployed
    in the same node to reduce latency.
  prefs: []
  type: TYPE_NORMAL
- en: You can learn more about affinity and how to configure in this blog post: [https://supergiant.io/blog/learn-how-to-assign-pods-to-nodes-in-kubernetes-using-nodeselector-and-affinity-features/](https://supergiant.io/blog/learn-how-to-assign-pods-to-nodes-in-kubernetes-using-nodeselector-and-affinity-features/), and
    in the Kubernetes official configuration ([https://kubernetes.io/docs/concepts/configuration/assign-pod-node/](https://kubernetes.io/docs/concepts/configuration/assign-pod-node/)).
  prefs: []
  type: TYPE_NORMAL
- en: In general, Kubernetes and `eksctl` defaults work fine for most applications.
    Use this advice only for advanced configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've seen how to apply a Kubernetes cluster into a production
    environment and create a Kubernetes cluster in a cloud provider, in this case,
    AWS. We've seen how to set up our Docker registries, create a cluster using EKS,
    and adapt our existing YAML files so they are ready for the environment.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that, though we used AWS as an example, all of the elements we discussed
    are available in other cloud providers. Check their documentation to see if they
    work better for you.
  prefs: []
  type: TYPE_NORMAL
- en: We also saw how to deploy an ELB so the cluster is available to the public interface,
    and how to enable HTTPS support on it.
  prefs: []
  type: TYPE_NORMAL
- en: We discussed the different elements of deployments to make the cluster more
    resilient and to deploy new versions smoothly, not interrupting the service—both
    by using HAProxy to be able to quickly enable or disable services and by making
    sure that changing the container image is done in an orderly fashion.
  prefs: []
  type: TYPE_NORMAL
- en: We also covered how autoscaling can help rationalize the use of resources and
    allow you to cover peaks in load in the system, both by creating more pods and
    by adding more AWS instances to add resources to the cluster when needed and remove
    them to avoid needless costs.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will see how to control the state of the Kubernetes
    cluster using GitOps principles to be sure that any changes on it are properly
    reviewed and captured.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are the main disadvantages of managing your own Kubernetes cluster?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you name some commercial cloud providers that have a managed Kubernetes
    solution?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is there any action you need to do to be able to push to an AWS Docker registry?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What tool do we use to set up an EKS cluster?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the main changes we did in this chapter to adapt the YAML files from
    previous chapters?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Are there any Kubernetes elements that are not required in the cluster from
    this chapter?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we need to control the DNS associated with an SSL certificate?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between the liveness and readiness probes?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why are rolling updates important in production environments?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between autoscaling pods and nodes?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this chapter, we deployed our own database containers. In production this
    will change, as it's required to connect to an already existing external database.
    How would you change the configuration to do so?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about how to use AWS, networking capabilities, which are vast,
    you can check out the book *AWS Networking Cookbook* ([https://www.packtpub.com/eu/virtualization-and-cloud/aws-networking-cookbook](https://www.packtpub.com/eu/virtualization-and-cloud/aws-networking-cookbook)).
    To learn how to ensure that you set up a secure system in AWS, read *AWS: Security
    Best Practices on AWS* ([https://www.packtpub.com/eu/virtualization-and-cloud/aws-security-best-practices-aws](https://www.packtpub.com/eu/virtualization-and-cloud/aws-security-best-practices-aws)).'
  prefs: []
  type: TYPE_NORMAL
