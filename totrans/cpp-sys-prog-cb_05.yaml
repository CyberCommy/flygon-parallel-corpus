- en: Using Mutexes, Semaphores, and Condition Variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will focus on the most common mechanisms you can use to synchronize
    access to a shared resource. The synchronization mechanisms we will look at prevent
    a critical section (the program segment responsible for a resource) from being
    executed concurrently from two or more processes or threads. In this chapter,
    you'll learn how to use both POSIX and C++ standard library synchronization building
    blocks such as mutexes, `std::condition_variable`, `std::promise`, and `std::future`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Using POSIX mutexes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using POSIX semaphores
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: POSIX semaphores advanced usage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synchronization building blocks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning inter-thread communication with simple events
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning inter-thread communication with condition variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So that you can try out all the programs in this chapter immediately, we've
    set up a Docker image that contains all the tools and libraries we'll need throughout
    this book. It is based on Ubuntu 19.04.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to set it up, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Download and install the Docker Engine from [www.docker.com](http://www.docker.com).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pull the image from Docker Hub: `docker pull kasperondocker/system_programming_cookbook:latest`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The image should now be available. Type in the `docker images` command to view
    the image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You should have the following image: `kasperondocker/system_programming_cookbook`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the Docker image with an interactive shell using the `docker run -it --cap-add
    sys_ptrace kasperondocker/system_programming_cookbook:latest /bin/bash` command.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The shell on the running container is now available. Use `root@39a5a8934370/#
    cd /BOOK/` to get all the programs that will be developed in this book.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `--cap-add sys_ptrace` argument is needed to allow GDB to set breakpoints.
    Docker doesn't allow this by default.
  prefs: []
  type: TYPE_NORMAL
- en: Using POSIX mutexes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe will teach you how to use POSIX mutexes to synchronize access to
    a resource from multiple threads. We'll do this by developing a program that contains
    a method (the critical section) that will perform a task that cannot run concurrently.
    We'll use the `pthread_mutex_lock`, `pthread_mutex_unlock`, and `pthread_mutex_init`
    POSIX methods to synchronize the threads' access to it.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we''ll create a multi-threaded program just to increment an
    integer to `200000`. To do this, we''ll develop the critical section that''s responsible
    for incrementing the counter, which must be protected. Then, we''ll develop the
    main section, which will create the two threads and manage the coordination between
    them. Let''s proceed:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a new file called `posixMutex.cpp` and develop its structure and critical
    section method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, in the `main` section, add the `init` method for the lock that''s needed
    for synchronization between threads:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the method that will execute the `increment` (that is, the
    critical section to protect) and the lock that will manage the synchronization
    between threads, let''s create the threads:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have to wait for the threads to complete the tasks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This program (available in the Docker image under the `/BOOK/Chapter05/` folder)
    showed us how to use the POSIX mutex interfaces to synchronize the use of a shared
    resource – a counter, in this case – between threads. We will explain this process
    in detail in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the first step, we created the `struct` that was needed to pass the parameters
    to the threads: `struct ThreadInfo`. In this `struct`, we put the lock that''s
    needed to protect the resource `counter` and the counter itself. Then, we developed
    the `increment` feature. `increment`, logically, needs to lock the `pthread_mutex_lock(&info->lock);` resource,
    increment the counter (or any other action needed by the critical section), and
    unlock the `pthread_mutex_unlock(&info->lock);` resource to let the other threads
    do the same.'
  prefs: []
  type: TYPE_NORMAL
- en: In the second step, we started developing the `main` method. The first thing
    we did is initialize the lock mutex with `pthread_mutex_init`. Here, we need to
    pass a pointer to the locally allocated resource.
  prefs: []
  type: TYPE_NORMAL
- en: In the third step, we created two threads, `th1` and `th2`. These are responsible
    for running the `increment` method concurrently. The two threads are created with
    the `pthread_create` POSIX API by passing the address of `thInfo` that was allocated
    in *step 2*. If the thread is created successfully, it starts the elaboration
    immediately.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the fourth and last step, we waited for both `th1` and `th2` to finish printing
    the value of the counter to the standard output, which we expect to be `200000`.
    By compiling `g++ posixMutex.cpp -lpthread` and running the `./a.out` program,
    we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5918fefd-5ad2-4e38-80ca-a057d0b440b7.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, the two threads never overlap the execution. Thus, the counter
    resource in the critical section is managed properly and the output is what we
    expected.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we used `pthread_create` for the sake of completeness. The exact
    same goal could have been achieved by using `std::thread` and `std::async` from
    the C++ standard library.
  prefs: []
  type: TYPE_NORMAL
- en: The `pthread_mutex_lock()` function locks the mutex. If the mutex is already
    locked, the calling thread will be blocked until the mutex becomes available.
    The `pthread_mutex_unlock` function unlocks the mutex if the current thread holds
    the lock on a mutex; otherwise, it results in undefined behavior.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You are invited to modify this program and use `pthread_mutex_lock` and `pthread_mutex_unlock`
    in conjunction with `std::thread` or `std::async` from the C++ standard library.
    See [Chapter 2](1bf083f4-9d12-4b2e-bf5c-35a2e3d99c36.xhtml), *Revisiting C++*, to
    refresh yourself on this topic.
  prefs: []
  type: TYPE_NORMAL
- en: Using POSIX semaphores
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: POSIX mutexes are clearly not the only mechanism you can use to synchronize
    access to a shared resource. This recipe will show you how to use another POSIX
    tool to achieve the same result. Semaphores are different from mutexes, and this
    recipe will teach you their basic usage, while the next will show you more advanced
    ones. A semaphore is a notification mechanism between threads and/or processes.
    As a rule of the thumb, try to use a mutex as a synchronization mechanism and
    semaphores as a notification mechanism. In this recipe, we'll develop a program
    that's similar to the one we built in the *Using POSIX mutexes* recipe, but this
    time, we'll protect the critical section with semaphores.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we''ll create a multi-threaded program to increment an integer
    until it reaches `200000`. Again, the code section that takes care of the increments
    must be protected and we''ll use POSIX semaphores. The `main` method will create
    the two threads and ensure that the resources are destroyed correctly. Let''s
    get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s open a new file called `posixSemaphore.cpp` and develop the structure
    and the critical section method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, in the `main` section, add the `init` method for the lock that''s needed
    for the synchronization between threads:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the `init` section is complete, let''s write the code that will start
    the two threads:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, here''s the closing part:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The same program we used for POSIX mutexes now runs with POSIX semaphores. As
    you can see, the program's design doesn't change – what really changes is the
    APIs we used to protect the critical section.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first section contains the structure that''s used to communicate with the
    `increment` method and the definition of the method itself. The main difference,
    compared to the mutex version of the program, is that we now include the `#include
    <semaphore.h>` headers so that we can use the POSIX semaphores APIs. Then, in
    the structure, we use the `sem_t` type, which is the actual semaphore that is
    going to protect the critical section. The `increment` method has two barriers
    to protect the actual logic: `sem_wait(&info->sem);` and `sem_post(&info->sem);`.
    All these two methods do is atomically decrement and increment the `sem` counter,
    respectively. `sem_wait(&info->sem);` acquires the lock by decrementing the counter
    by `1`. If the value of the counter is greater than 0, then the lock is acquired
    and the thread can enter the critical region. `sem_post(&info->sem);` just increments
    the counter by one while exiting the critical region.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the second step, we initialize the semaphore by calling the `sem_init` API.
    Here, we passed three parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: The semaphore to initialize.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `pshared` argument. This indicates whether the semaphore is to be shared
    between the threads of a process or between processes. `0` indicates the first
    option.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last parameter indicates the initial value of the semaphore. By passing
    `1` to `sem_init`, we are asking the semaphore to protect one resource. The semaphore,
    through `sem_wait` and `sem_post`, will internally increase and decrease that
    counter automatically, letting each thread enter the critical section one at a
    time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the third step, we created the two threads that use the `increment` method.
  prefs: []
  type: TYPE_NORMAL
- en: In the last step, we waited for the two threads to finish the elaboration with
    `pthread_join` and, most relevant in this section, we destroyed the semaphore
    structure with `sem_destroy` by passing the semaphore structure we've used so
    far.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s compile and execute the program: `g++ posixSemaphore.cpp -lpthread`.
    Even in this case, we need to link the program with the `libpthread.a` by passing
    the `-lpthread` option to g++ as we use `pthreads`. The output of doing this is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6b0aabdc-b066-4e25-b9fc-5300752d41a4.png)'
  prefs: []
  type: TYPE_IMG
- en: As expected, the output shows the counter at `200000`. It also shows that the
    two threads are not overlapping.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We used `sem_t` as a binary semaphore by passing the value `1` to the `sem_init`
    method. Semaphores can be used as *counting semaphores*, which means passing a
    value greater than 1 to the `init` method. In this case, it means that the critical
    section will be accessed concurrently by *N* threads.
  prefs: []
  type: TYPE_NORMAL
- en: For more information on the GNU/Linux man pages, type `man sem_init` in a shell.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can find out more about *counting semaphores* in the next recipe, where
    we'll learn about the difference between mutexes and semaphores.
  prefs: []
  type: TYPE_NORMAL
- en: You are invited to modify this program and use `pthread_mutex_lock` and `pthread_mutex_unlock` in
    conjunction with `std::thread` or `std::async` from the C++ standard library.
  prefs: []
  type: TYPE_NORMAL
- en: POSIX semaphores advanced usage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *Using POSIX semaphores *recipe showed us how to use POSIX semaphores to
    protect a critical region. In this recipe, you'll learn how to use it as a counting
    semaphore and notification mechanism. We'll do this by developing a classical
    publish-subscriber program where there is one publisher thread and one consumer
    thread. The challenge here is that we want to limit the maximum number of items
    in the queue to a defined value.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we''ll write a program representing a typical use case for
    a counting semaphore – a producer-consumer problem in which we want to limit the
    number of items in the queue to a certain number. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s open a new file called `producerConsumer.cpp` and code the structure
    we''ll need in the two threads:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s write the code for `producer`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We do the same for `consumer`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we need to code the `main` method in order to initialize the resources
    (for example, semaphores):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we need to code the section that will release the resources:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This program, which is the typical implementation of a consumer-producer problem
    based on semaphores, shows how to limit the use of a resource to *N* (in our case, `MAX_ITEM_IN_QUEUE`).
    This concept can be applied to other problems, including how to limit the number
    of connections to a database, and so on. What would happen if, instead of one
    producer, we started two producer threads?
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the first step of the program, we defined `struct` that''s needed to let
    the two threads communicate. It contains the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A `full` semaphore (counting semaphore): This semaphore is set to `MAX_ITEM_IN_QUEUE`.
    This limits the number of the item on the queue.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An `empty` semaphore (counting semaphore): This semaphore notifies the process
    when the queue is empty.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A `mutex` semaphore (binary semaphore): This is a mutex that''s implemented
    with semaphores and is needed to provide mutual exclusion on the queue''s access.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Queue: Implemented with `std::vector`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the second step, we implemented the `producer` method. The core part of the
    method is the `for` loop implementation. The producer goal is to push items into
    the queue with no more than `MAX_ITEM_IN_QUEUE` items at the same time so that
    the producer tries to enter the critical region by decrementing the `full` semaphore
    (which we initialized to `MAX_ITEM_IN_QUEUE` in `sem_init`), then push the item
    into the queue and increment the empty semaphore (this gives the consumer permission
    to go on and read from the queue). Why do we need to notify that the consumer
    can read an item? In other words, why do we need to call `sem_post(&info->empty);` in
    the producer? If we didn't, the consumer thread would read items continuously
    and would keep incrementing the `full` semaphore to values greater than `MAX_ITEM_IN_QUEUE` with
    the effect of more than `MAX_ITEM_IN_QUEUE` item in the queue.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the third step, we implemented the `consumer` method. This is specular to `producer`.
    What the consumer does is wait for the notification to read an item from the queue
    with `sem_wait(&info->empty);`, reads from the queue, and then increments the
    `full` semaphore. This last step can be read like so: I''ve just consumed one
    item from the queue.'
  prefs: []
  type: TYPE_NORMAL
- en: The fourth step is where we started the two threads and initialized the three
    semaphores.
  prefs: []
  type: TYPE_NORMAL
- en: The fifth step is the closing section.
  prefs: []
  type: TYPE_NORMAL
- en: If we start more producers, the code would still work as the `full` and `empty`
    semaphores would ensure the behavior we described previously and the `mutex` on
    the queue ensures that just one item at a time writes/read on it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Both POSIX mutexes and semaphores can be used among threads and processes.
    To make a semaphore working among processes, we just need to pass a value different
    from 0 in the second parameter of `sem_init`. For mutexes, we need to pass the
    `PTHREAD_PROCESS_SHARED` flag when calling `pthread_mutexattr_setpshared`. By
    building and running the program we''d have output like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/df663e70-e7d1-4142-85b6-54910957bcbb.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's see something more about this recipe in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It''s worth highlighting that a semaphore can be initialized (the third parameter
    of the `sem_init` method) to three possible values:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To `1`: In this case, we''re using the semaphore as a mutex.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To `N`: In this case, we''re using the semaphore as a *counting semaphore*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To `0`: We''re using the semaphore like a notification mechanism (see the `empty` semaphore
    example previously).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In general, semaphores must be seen as a notification mechanism between threads
    or processes.
  prefs: []
  type: TYPE_NORMAL
- en: When should we use POSIX semaphores and POSIX mutexes? Try to use a mutex as
    a synchronization mechanism and semaphores as a notification mechanism. Furthermore,
    consider that POSIX mutexes are generally faster than POSIX semaphores in Linux
    kernels.
  prefs: []
  type: TYPE_NORMAL
- en: 'One last thing: remember that both POSIX mutexes and semaphores make the tasks
    go to sleep, as opposed to spinlocks, which don''t. Indeed, when a mutex or semaphore
    is locked, the Linux scheduler puts the task in the waiting queue.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Please have look at the following list for further information:'
  prefs: []
  type: TYPE_NORMAL
- en: The *Using POSIX mutexes* recipe in this chapter to learn how to program POSIX
    mutexes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Using POSIX semaphores* recipe in this chapter to learn how to program
    POSIX mutexes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Linux Kernel Development*, by Robert Love'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synchronization building blocks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From this recipe and the next two, we'll be back in the C++ world. In this recipe,
    we'll learn about the C++ synchronization building blocks. Specifically, we'll
    look at using `std::lock_guard` and `std::unique_lock` in combination with **Resource
    Acquisition Is Initialization** (**RAII**), an object-oriented programming idiom
    that makes the code more robust and readable. `std::lock_guard` and `std::unique_lock` wrap the
    C++ concept of mutexes around two classes with the RAII concept. `std::lock_guard`
    is the simplest and smallest guard, while `std::unique_lock` adds some functionality
    on top of it.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we''ll develop two programs in order to learn how to use `std::unique_lock`
    and `std::lock_guard`. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'From a shell, create a new file called `lock_guard.cpp`. Then, write the code
    for the `ThreadInfo` structure and the `increment` (thread) method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, write the code for the `main` method, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s write the same program for `std::unique_lock`. From a shell, create
    a new file called `unique_lock.cpp` and write the code for the `ThreadInfo` structure
    and the `increment` (thread) method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Regarding the `main` method, there are no differences here to what we saw in
    the *Using POSIX mutexes* recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: These two programs are the C++ versions of the one we wrote in the *Using POSIX
    mutexes* recipe. Note the conciseness of the code.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Step 1* of the `lock_guard.cpp` program defines the `ThreadInfo` struct and
    the `increment` method that''s needed. The first thing we can see is the use of `std::mutex`
    as a protection mechanism for the critical section. The `increment` method is
    now simplified with fewer headaches for the developer. Note that we have the `std::lock_guard<std::mutex> lock(info.mutex);`
    variable definition. As we can see in the method, there is no `unlock()` call
    at the end – why is this? Let''s see how `std::lock_guard` works: its constructor
    locks the mutex. Since `std::lock_guard` is a class, when the object goes out
    of scope (at the end of the method, in this case), the destructor is called. The
    unlock of the `std::mutex` object is called in the `std::lock_guard` destructor.
    This means that whatever happens to the `increment` method, the constructor is
    called so there''s no risk of a deadlock and the developer doesn''t have to take
    care of the `unlock()`. What we described here is the RAII C++ technique, which
    binds the life cycle of the `info.mutex` object with the lifetime of the `lock`
    variable.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 2* contains the main code that''s used to manage the two threads. In
    this case, C++ has a much cleaner and simpler interface. A thread is created with
    `std::thread t1 (increment, std::ref(thInfo));`. Here, `std::thread` accepts two
    parameters: the first is the method that the thread will call, while the second
    is the `ThreadInfo` that''s passed to the increment method.'
  prefs: []
  type: TYPE_NORMAL
- en: The `unique_lock.cpp` program is the version of the `lock_guard` we've described
    so far. The main difference is that `std::unique_lock` gives the developer more
    freedom. In this case, we've modified the `increment` method to simulate the needs
    of the mutex unlock for the `if (info.counter < 0)` case. With the use of `std::unique_lock`,
    we are able to `unlock()` the mutex and return from the method. We wouldn't be
    able to do the same on the `std::lock_guard` class. Of course, the `lock_guard`
    would unlock at the end of the scope no matter what, but what we want to highlight
    here is that with `std::unique_lock`, the developer has the freedom to unlock
    the mutex manually, at any time.
  prefs: []
  type: TYPE_NORMAL
- en: 'By compiling `lock_guard.cpp`: `g++ lock_guard.cpp -lpthread` and running the
    generated executable, we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bd008ce4-2587-419a-b206-6f98ee10173c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The same happens for `unique_lock.cpp`: `g++ unique_lock.cpp -lpthread`, the
    output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/113fd27c-e7a5-4481-9a8f-49d066070a57.png)'
  prefs: []
  type: TYPE_IMG
- en: As expected, both outputs are exactly the same, with the advantage that the
    code that uses `lock_guard` looks cleaner and definitely more safe from the developer's
    point of view.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we''ve seen in this recipe, `std::lock_guard` and `std::unique_lock` are
    template classes that we used with the `std::mutex` `object.lock_guard`. `unique_lock`
    can be defined with other mutex objects, such as **`std::timed_mutex`**, which
    allows us to get a lock for a specific amount of time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The `lock` object will try to acquire the lock for `5` milliseconds. We have
    to be careful when adding `std::defer_lock`, which will not lock the mutex automatically
    on construction. This will only happen when `try_lock_for` succeeds.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is a list of references you may refer to:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Linux Kernel Development*, by Robert Love'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Using POSIX mutexes* recipe in this chapter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Using POSIX semaphores* recipe in this chapter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Chapter 2](1bf083f4-9d12-4b2e-bf5c-35a2e3d99c36.xhtml), *Revisiting C++*,
    for a refresher on C++'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning inter-thread communication with simple events
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we know how to use both POSIX and C++ standard library mechanisms to
    synchronize a critical section. There are use cases where we don't need to explicitly
    use locks; instead, we can use more simple communication mechanisms. `std::promise`
    and `std::future` can be used to allow two threads to communicate without the
    hassle of the synchronization.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we will write a program that splits a problem into two parts:
    thread 1 will run a highly intensive computation and will send the result to thread
    2, which is the consumer of the results. We''ll do this by using `std::promise`
    and `std::future`. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a new file called `promiseFuture.cpp` and type the following code into
    it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Write the `main` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The consumer is responsible for getting the result through `std::future` and
    using it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The producer performs an elaboration to get the item and sends it to the waiting
    consumer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This program shows the typical use case for `std::promise` and `std::future`,
    where a mutex or semaphore is not needed for a one-shot form of communication.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *step 1*, we defined the `struct Item` to use between the producer and the
    consumer and declared the two method's prototypes.
  prefs: []
  type: TYPE_NORMAL
- en: In *step 2*, we defined two tasks using `std::async` by passing the defined
    promise and future.
  prefs: []
  type: TYPE_NORMAL
- en: In *step 3*, the `asyncConsumer` method waits for the result of the elaboration
    with the `fut.get()` method, which is a blocking call.
  prefs: []
  type: TYPE_NORMAL
- en: In *step 4*, we implemented the `asyncProducer` method. This method is simple –
    it just returns a canned answer. In a real scenario, the producer performs a highly
    intensive elaboration.
  prefs: []
  type: TYPE_NORMAL
- en: This simple program showed us how to simply decouple a problem from the producer
    of the information (promise) and the consumer of the information without taking
    care of the synchronization between threads. This solution of using `std::promise`
    and `std::future` only works for a one-shot type of communication (that is, we
    cannot have loops in the two threads sending and getting items).
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`std::promise` and `std::future` are just concurrency tools offered by the
    C++ standard library. The C++ standard library also provides `std::shared_future`
    in addition to `std::future`. In this recipe, we had one information producer
    and one information consumer, but what if we have more consumers? `std::shared_future` allows
    multiple threads to wait for the same information (coming from `std::promise`).'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The books *Effective Modern C++* by Scott Meyers and *The C++ Programming Language* by Bjarne
    Stroustrup cover these topics in great detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'You''re also invited to read more about concurrency through the C++ Core Guideline
    in the *CP: Concurrency and parallelism* ([https://github.com/isocpp/CppCoreGuidelines/blob/master/CppCoreGuidelines.md#cp-concurrency-and-parallelism](https://github.com/isocpp/CppCoreGuidelines/blob/master/CppCoreGuidelines.md#cp-concurrency-and-parallelism)) section.'
  prefs: []
  type: TYPE_NORMAL
- en: Learning inter-thread communication with condition variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, you'll learn about another C++ tool that's available in the
    standard library that allows multiple threads to communicate. We'll be using `std::condition_variable`
    and `std::mutex` to develop a producer-consumer program.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The program in this recipe will use `std::mutex` to protect the queue from
    concurrent access and `std::condition_variable` to notify the consumer that an
    item has been pushed to the queue. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a new file called `conditionVariable.cpp` and type the following code
    into it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s write the `main` method, which creates the threads for the consumer
    and the producer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define the `consumer` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let''s define the `producer` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Although the program we've developed solves the typical producer-consumer problem
    we saw in the previous recipe, the code is more idiomatic, easy to read, and less
    error-prone.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the first step, we defined `struct Item` that we need to pass from the producer
    to the consumer. The interesting point in this step is the definition of the `std::queue`
    variable; it uses a mutex that synchronizes access to the queue and `std::condition_variable`
    to communicate an event to the consumer from the producer.
  prefs: []
  type: TYPE_NORMAL
- en: In the second step, we defined the producer and consumer threads and called
    the `join()` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the third step, the consumer method does essentially four things: acquires
    the lock to read the item from the queue, waits for a notification from the producer
    with the condition variable, `cond`, pops an item from the queue, and then releases
    the lock. Interestingly, the condition variable uses `std::unique_lock` and not
    `std::lock_guard` for one simple reason: as soon as the `wait()` method on the
    condition variable is called, the lock is (internally) released so that the producer
    isn''t blocked. When the producer calls the `notify_one` method, the `cond` variable
    on the consumer gets woken up and locks the mutex again. This allows it to safely
    pop an item from the queue and release the lock again at the end with `lck.unlock()`.
    Immediately after `cond.wait()` (the commented out code), there is an alternative
    way of calling `wait()` by passing a second parameter, a predicate, which will
    wait further if the second parameter returns false. In our case, the consumer will
    not wait if the queue isn''t empty.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The last step is quite simple: we create an item, lock it with `lock_guard`
    on a mutex, and push it onto the queue. Note that by using `std::lock_guard`,
    we don''t need to call unlock; the destructor of the `lock` variable will take
    care of that. The last thing we need to do before ending the current loop is notify
    the consumer with the `notify_one` method.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `g++ conditionVariable.cpp -lpthread` compilation and the execution of
    the `./a.out` program will produce the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/726a088f-c4c0-4c67-8d11-ddca1550ff4b.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that since the producer is way faster than the consumer due to the `condition_variable`,
    which is asynchronous, there is a latency to pay. As you may have noticed, the
    producer and the consumer run infinitely, so you have to stop the process manually
    (*Ctrl* + *C*).
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we used the `notify_one` method on the `condition_variable`
    in the producer. An alternative method is to use `notify_all`, which notifies
    all the waiting threads.
  prefs: []
  type: TYPE_NORMAL
- en: Another important aspect to highlight is that condition variables are best used
    when the producer wants to notify one of the waiting thread about an event occurred
    in the computation so that the consumer can take action. For example, let's say
    that the producer notifies the consumer that a special item has been pushed or
    that the producer notifies a queue manager that the queue is full so that another
    consumer has to be spawned.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *Creating a new thread* recipe in [Chapter 2](1bf083f4-9d12-4b2e-bf5c-35a2e3d99c36.xhtml), *Revisiting
    C++*, to find out more or refresh yourself on threading in C++.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*C++ Programming Language*, by Bjarne Stroustrup, covers these topics in great
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
