- en: Chapter 8. Logging and Real-time Analytics with MongoDB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this book, many concepts you already knew were presented to you.
    You have learned how to use them in conjunction with the techniques and tools
    that MongoDB offers to us. The goal of this chapter is to apply these techniques
    in a real-life example.
  prefs: []
  type: TYPE_NORMAL
- en: The real-life example we will develop in this chapter explains how to use MongoDB
    as a persistent storage for a web server's log data—to be more specific, the data
    from an Nginx web server. By doing this, we will be able to analyze the traffic
    data of a web application.
  prefs: []
  type: TYPE_NORMAL
- en: We will start this chapter by analyzing the Nginx log format in order to define
    the information that will be useful for our experiment. After this, we will define
    the type of analysis we want to perform in MongoDB. Finally, we will design our
    database schema and implement, by using code, reading and writing data in MongoDB.
  prefs: []
  type: TYPE_NORMAL
- en: For this chapter, we will take into consideration that each host that generates
    this event consumes this information and sends it to MongoDB. Our focus will not
    be on the application's architecture or on the code we will produce in our example.
    So, kind reader, if you do not agree with the code snippets shown here, please,
    feel free to modify them or create a new one yourself.
  prefs: []
  type: TYPE_NORMAL
- en: 'That said, this chapter will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Log data analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What we are looking for
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing the schema
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Log data analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The access log is often ignored by developers, system administrators, or anyone
    who keeps services on the web. But it is a powerful tool when we need prompt feedback
    on what is happening for each request on our web server.
  prefs: []
  type: TYPE_NORMAL
- en: 'The access log keeps information about the server''s activities and performance
    and also tells us about eventual problems. The most common web servers nowadays
    are Apache HTTPD and Nginx. These web servers have by default two log types: error
    logs and access logs.'
  prefs: []
  type: TYPE_NORMAL
- en: Error logs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the name suggests, the error log is where the web server will store errors
    found during the processing of a received request. In general, this type of log
    is configurable and will write the messages according to the predefined severity
    level.
  prefs: []
  type: TYPE_NORMAL
- en: Access logs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The access log is where all the received and processed requests are stored.
    This will be the main object of our study.
  prefs: []
  type: TYPE_NORMAL
- en: 'The events written in the file are recorded in a predefined layout that can
    be formatted according to the wishes of those who are managing the server. By
    default, both Apache HTTPD and Nginx have a format known as **combined**. An example
    of a log generated in this format is presented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: At first sight, it can be a little frightening to see too much information in
    only one log line. However, if we take a look at the pattern that is being applied
    in order to generate this log and try to examine it, we will see that it is not
    so difficult to understand.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pattern that generates this line on the Nginx web server is presented as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We will describe each part of this pattern so you get a better understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: '`$remote_addr`: This is the IP address of the client that performed the request
    on the web server. In our example, this value corresponds to 191.32.254.162.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`$remote_user`: This is the authenticated user, if it exists. When an authenticated
    user is not identified, this field will be filled with a hyphen. In our example,
    the value is `-`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`[$time_local]`: This is the time that the request was received on the web
    server in the format `[day/month/year:hour:minute:second zone]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"$request"`: This is the client request itself. Also known as a request line.
    To get a better understanding, we will analyze our request line in the example:
    `"GET /admin HTTP/1.1"`. First, we have the HTTP verb used by the client. In this
    case, it was a `GET` HTTP verb. In the sequence, we have the resource accessed
    by the client. In this case, the resource accessed was `/admin`. And last, we
    have the protocol used by the client. In this case, `HTTP/1.1`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`$status`: This is the HTTP status code replied to the client by the web server.
    The possible values for this field are defined in RFC 2616\. In our example, the
    web server returns the status code `200` to the client.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To learn more about RFC 2616, you can visit [http://www.w3.org/Protocols/rfc2616/rfc2616.txt](http://www.w3.org/Protocols/rfc2616/rfc2616.txt).
  prefs: []
  type: TYPE_NORMAL
- en: '`$body_bytes_sent`: This is the length in bytes of the response body sent to
    the client. When you do not have a body, the value will be a hyphen. We must observe
    that this value does not include the request headers. In our example, the value
    is 2,529 bytes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"$http_referer"`: This is the contained value in the header "Referer" of the
    client''s request. This value represents where the requested resource is referenced
    from. When the resource access is performed directly, this field is filled with
    a hyphen. In our example, the value is `-`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"$http_user_agent"`: This is the information that the client sends in the
    User-Agent header. Normally, it is in this header that we can identify the web
    browser used on the request. In our example, the value for this field is `"Mozilla/5.0
    (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2272.104
    Safari/537.36"`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Besides these, we have more variables available to create new log formats.
    Among these, we highlight:'
  prefs: []
  type: TYPE_NORMAL
- en: '`$request_time`: This indicates the total time for the request processing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`$request_length`: This indicates the total length for the client response,
    including the headers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we are familiar with the web server access log, we will define what
    we want to analyze in order to know what the information needs to be logged.
  prefs: []
  type: TYPE_NORMAL
- en: What we are looking for
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The information extracted from a web server access log is very rich and give
    us good material for infinite possibilities of study. Being simple and direct,
    it is possible to count the number of requests that our web server receives just
    by counting the number of lines that the access log has. But we can expand our
    analysis and try to measure the average of the data traffic in bytes over the
    time, for example.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, one of the most widely used services is the application performance
    management system, also known as **APMs**. Nowadays, these services are commonly
    offered as software-as-a-service and the main goal is to give us a view of an
    application's performance and health.
  prefs: []
  type: TYPE_NORMAL
- en: APMs are a good example of what can be analyzed based on the information extracted
    from the access log, due to the fact that a good part of the information that
    APMs generate is based on the access logs.
  prefs: []
  type: TYPE_NORMAL
- en: Attention! I am not saying that an APM works based only on the access log, but
    a good part of the information generated by APMs can be extracted from the access
    log. Okay?
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To learn more, visit [https://en.wikipedia.org/wiki/Application_performance_management](https://en.wikipedia.org/wiki/Application_performance_management).
  prefs: []
  type: TYPE_NORMAL
- en: As said at the beginning of this chapter, we do not have any intention of coding
    or creating an entire system, but we will show in practice how we can keep the
    access log information for an eventual analysis using MongoDB.
  prefs: []
  type: TYPE_NORMAL
- en: Based on APMs, we will structure our example on an analysis of web server resource
    throughput. It is possible to perform this analysis only with the information
    contained on the web server access log. To do so, what data do we need in our
    access log? And should we use the combined format?
  prefs: []
  type: TYPE_NORMAL
- en: Measuring the traffic on the web server
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The throughput in our web server will be estimated based on the number of requests
    for a given period of time, that is, requests in a day, in an hour, in a minute,
    or in a second. The number of requests per minute is a very reasonable measure
    for a real-time monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: The throughput is calculated by counting the requests processed in our web server.
    Because of this, it is not necessary to work with specific data from the access
    log. Nevertheless, in order to make possible a richer further analysis of our
    data, we will create a specific log format that will collect request information
    such as the HTTP status code, the request time, and length.
  prefs: []
  type: TYPE_NORMAL
- en: Both Apache HTTP and Nginx allow us to customize the access log or to create
    a new file with a custom format. The second option seems to be perfect. Before
    we start to configure our web server, we will create our log format using the
    variables previously explained. Just remember that we are working on an Nginx
    web server.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As we defined our log format, we can configure our Nginx web server. To do
    so, let''s perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, to define this new format in Nginx, we need to edit the `nginx.conf`
    file, adding a new entry in the HTTP element with the new log format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we need to add another entry in `nginx.conf` file that defines in which
    file the new custom log will be written:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'To apply our changes, reload the Nginx web server executing the following command
    in a terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'After reloading the Nginx server, we can look at our new log file, `/var/log/nginx/custom_access.log`,
    and check whether the lines are like the following lines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Log format configured, web server set up; it is time to design our schema.
  prefs: []
  type: TYPE_NORMAL
- en: Designing the schema
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Something that has been repeated several times during this book is the importance
    of knowing our data and what it will be used for. And now, more than ever, in
    this practical exercise we will design our schema step by step, considering every
    detail involved in this.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous section, we defined the data set that we want to extract from
    the web server access logs. The next step is to list the requirements that are
    associated with the clients of the database. As said previously, one process will
    be responsible for capturing the information from the log files and writing them
    in MongoDB, and another process will read the information already persisted in
    the database.
  prefs: []
  type: TYPE_NORMAL
- en: A point of concern is the performance during the document writing in MongoDB
    because it is important to ensure that the information will be generated almost
    in realtime. Since we do not have a previous estimation of the data volume per
    second, we will be optimistic. Let's consider that we will have a huge volume
    of data all the time coming from the web server to our MongoDB instance.
  prefs: []
  type: TYPE_NORMAL
- en: Taking into account this requirement, we will be worried about the data dimensions
    that will increase over time. More events imply that more documents will be inserted.
    These are the main requirements for our system to operate well.
  prefs: []
  type: TYPE_NORMAL
- en: At first sight, we can imagine that it is all about defining the document format
    and persisting it into a collection. But thinking in this way is to ignore the
    well-known MongoDB schema flexibility. So we will analyze the problem of throughput
    in order to define where and how to persist the information.
  prefs: []
  type: TYPE_NORMAL
- en: Capturing an event request
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The web server throughput analysis is maybe the simplest task. In a simple way,
    the measure of the number of events will give us a number that represents the
    web server's throughput.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, if for each generated event a write document is performed stating the time
    of this operation, does it mean that we can easily get the throughput? Yes! Thus,
    the easiest way to represent a MongoDB document where we can analyze the throughput
    is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'When executing the `count` method in the document collection, we will get the
    web server''s throughput value. Assuming that we have a collection called `events`,
    in order to find out the throughput, we must execute the following command in
    the mongod shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This command returns the total of generated events until now on our web server.
    But, is this the number that we want? No. There is no point in having the total
    number of events without placing it in a given period of time. Would it be of
    any use to have 10,000 events processed by the web server until now without knowing
    when we started recording these events or even when the last event was generated?
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to count the events in a given period of time, the easiest way to
    do so is by including a field that will represent the event''s creation date.
    An example for this document is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, we can check the total number of requests in the web server in
    a given period of time by executing a query. The simplest way to perform this
    query is to use the aggregation framework. The execution of the following command
    on the mongod shell will return the total of requests per minute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The aggregation pipeline has its limits. If the command result returns a single
    document that exceeds the BSON document size, an error is produced. Since MongoDB's
    2.6 release, the `aggregate` command returns a cursor, so it can return result
    sets of any size.
  prefs: []
  type: TYPE_NORMAL
- en: You can find more about aggregation pipeline limits in the MongoDB reference
    manual at [http://docs.mongodb.org/manual/core/aggregation-pipeline-limits/](http://docs.mongodb.org/manual/core/aggregation-pipeline-limits/).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the command pipeline, we defined the `$group` stage to group the documents
    per day, month, year, hour, and minute. And we count everything using the `$sum`
    operator. From this `aggregate` command''s execution, we will have, as an example,
    documents such as these:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In this output, it is possible to know how many requests the web server received
    in a certain time period. This happens due to the `$group` operator behavior,
    which takes documents that match a query and then collects groups of documents
    based on one or more fields. We took each part of our `$date_created` field, such
    as the month, day, year, hour, and minute, to the group stage of the aggregation
    pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to know which resource is accessed the most often in your web server
    with the higher throughput, none of these options fit this request. However, a
    fast solution for this problem is easily reachable. At first sight, the fastest
    way is to deconstruct the event and create a more complex document, as you can
    see in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'By using this document design, it is possible to know the resource throughput
    per minute with the help of an aggregation framework:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding pipeline, the first step is to group by resource and to count
    how many times a request on the resource occurred during an entire day. The next
    step is to use the operator `$project` and, together with the operator `$divide`,
    use the number of hits in a given resource and calculate the average per minute
    by dividing by 1,440 minutes, that is, the total of minutes in a day or 24 hours.
    Finally, we order the results in descending order to view which resources have
    the higher throughput.
  prefs: []
  type: TYPE_NORMAL
- en: 'To keep things clear, we will execute the pipeline step by step and explain
    the results for each step. In the execution of the first phase, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This execution groups the event collection documents by field resource and
    counts the number of occurrences of field hits when we use the operator `$sum`
    with the value `1`. The returned result is demonstrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'In the second phase of the pipeline, we use the operator `$project`, which
    will give us the value of hits per minute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the result of this phase:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The last phase of the pipeline is to order the results by throughput in descending
    order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The output produced is like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: It looks like we succeeded in obtaining a good design for our document. Now
    we can extract the desired analysis and other analyses, as we will see further,
    and for that reason we can stop for now. Wrong! We will review our requirements,
    compare them to the model that we designed, and try to figure out whether it was
    the best solution.
  prefs: []
  type: TYPE_NORMAL
- en: Our desire is to know the measure of throughput per minute for all web server
    resources. In the model we designed, one document is created per event in our
    web server, and by using the aggregation framework we can calculate the information
    that we need for our analysis.
  prefs: []
  type: TYPE_NORMAL
- en: What can be wrong in this solution? Well, if you think it's the number of documents
    in the collection, you are right. One document per event can generate huge collections
    depending on the web server traffic. Obviously, we can adopt the strategy of using
    shards, and distribute the collection through many hosts. But first, we will see
    how we can take advantage of the schema flexibility in MongoDB in order to reduce
    the collection size and optimize the queries.
  prefs: []
  type: TYPE_NORMAL
- en: A one-document solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having one document per event may be advantageous if we consider that we will
    have a huge amount of information to create an analysis. But in the example that
    we are trying to solve, it is expensive to persist one document for each HTTP
    request.
  prefs: []
  type: TYPE_NORMAL
- en: We will take advantage of the schema flexibility in MongoDB that will help us
    to grow documents over time. The following proposal has the main goal of reducing
    the number of persisted documents, also optimizing the queries for read and write
    operations in our collection.
  prefs: []
  type: TYPE_NORMAL
- en: 'The document we are looking for should be able to provide us all the information
    needed in order to know the resource throughput in requests per minute; thus we
    can have a document with this structure:'
  prefs: []
  type: TYPE_NORMAL
- en: A field with the resource
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A field with the event date
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A field with the minute that the event happened, and the total hits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following document implements all the requirements described in the preceding
    list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: With this document design, we can retrieve the number of events happening in
    a certain resource every minute. We can also know, by the daily field, the total
    requests during the day and use this to calculate whatever we want, such as requests
    per minute, or requests per hour, for instance.
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrate the write and read operations we could make on this collection,
    we will make use of JavaScript code running on the Node.js platform. So, before
    continuing, we must make sure we have Node.js installed on our machine.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you need help, you will find more information at [http://nodejs.org](http://nodejs.org).
  prefs: []
  type: TYPE_NORMAL
- en: 'The firs thing we should do is to create a directory where our application
    will live. In a terminal, execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we navigate to the directory we created and initiate the project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Answer all the questions asked by the wizard to create the initial structure
    of our new project. At the moment, we will have a `package.json` file based on
    the answers we gave.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to set up the MongoDB driver for our project. We can do this
    by editing the `package.json` file, including the driver reference for its dependencies,
    or by executing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command will install the MongoDB driver for our project and save
    the reference in the `package.json` file. Our file should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The last step is to create the `app.js` file with our sample code. The following
    is sample code that shows us how to count an event on our web server and record
    it in our collection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The preceding sample code is quite simple. In it, we have the `logDailyHit`
    function, which is responsible for logging an event and incrementing one unit
    in the document `daily` field. The second function is the `logMinuteHit` function,
    which is responsible for logging the occurrence of an event and incrementing the
    document `minute` field that represents the current minute in the day. Both functions
    have an update query that has an `upsert` option with the value `true` if the
    document does not exist, in which case it will be created.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we execute the following command, we will record an event on the resource
    `"/"`. To run the code, just navigate to the project directory and execute the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'If everything is fine, we should see the following output after running the
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'To get a feel for this, we will execute a `findOne` command on the mongod shell
    and watch the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: In addition to everything that the previous models can give us, this one has
    some advantages over them. The first thing we notice is that, every time we register
    a new event happening on the web server, we will manipulate only one document.
    The next advantage also lies in how easy we can find the information we are looking
    for, given a specific resource, since we have the information for an entire day
    in one document, which will lead to us to manipulating fewer documents in each
    query.
  prefs: []
  type: TYPE_NORMAL
- en: The way this schema design deals with time will give us many benefits when we
    think about reports. Both textual and graphical representations can be easily
    extracted from this collection for historical or real-time analysis.
  prefs: []
  type: TYPE_NORMAL
- en: However, as well as the previous approaches, we will have to deal with a few
    limitations too. As we have seen, we increment both the `daily` field and a `minute`
    field in an event document as they occur on the web server. When no event in a
    resource was reported for that day, then a new document will be created since
    we are using the `upsert` option on the update query. The same thing will happen
    if an event occurs in a resource for the first time in a given minute—the `$inc`
    operator will create the new `minute` field and set `"1"` as the value. This means
    that our document will grow over time, and will exceed the size MongoDB allocated
    initially for it. MongoDB automatically performs a reallocation operation every
    time the space allocated to the document is full. This reallocation operation
    that happens through the entire day has a direct impact on the database's performance.
  prefs: []
  type: TYPE_NORMAL
- en: What we should do? Live with it? No. We could reduce the impact of a reallocation
    operation by adding a process that preallocates space for our document. In summary,
    we will give the application the responsibility for creating a document with all
    the minutes we can have in a day, and initializing every field with the value
    `0`. By doing this, we will avoid too many reallocation operations by MongoDB
    during the day.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To learn more about record allocation strategies, visit the MongoDB reference
    user manual at [http://docs.mongodb.org/manual/core/storage/#record-allocation-strategies](http://docs.mongodb.org/manual/core/storage/#record-allocation-strategies).
  prefs: []
  type: TYPE_NORMAL
- en: 'To give an example of how we can preallocate the document space, we can create
    a new function in our `app.js` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Downloading the example code**'
  prefs: []
  type: TYPE_NORMAL
- en: You can download the example code files from your account at [http://www.packtpub.com](http://www.packtpub.com)
    for all the Packt Publishing books you have purchased. If you purchased this book
    elsewhere, you can visit [http://www.packtpub.com/support](http://www.packtpub.com/support)
    and register to have the files e-mailed directly to you.
  prefs: []
  type: TYPE_NORMAL
- en: 'To preallocate space in the current date to the `"/"` resource, just run the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the execution is something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We can run a `findOne` command on the mongod shell to check the new document.
    The document created is very long, so we will show just a piece of it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: It is recommended that we preallocate the document before midnight to ensure
    smooth functioning of the application. If we schedule this creation with an appropriate
    safety margin, we are not running any risk of creating the document for an event
    occurrence after midnight.
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, with the reallocation problem solved, we can go back to the issue that
    initiated our document redesign: the growth of our data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Even reducing the number of documents in our collection to one document per
    event per day, we can still run into a problem with storage space. This can happen
    when we have too many resources receiving events on our web server, and we cannot
    predict how many new resources we will have in our application''s lifecycle. In
    order to solve this issue, we will use two different techniques: TTL indexes and
    sharding.'
  prefs: []
  type: TYPE_NORMAL
- en: TTL indexes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is not always the case that we need to have all log information stored on
    our servers forever. It has become a standard practice by operations people to
    limit the number of files stored on disk.
  prefs: []
  type: TYPE_NORMAL
- en: By the same reasoning, we can limit the number of documents we need living in
    our collection. To make this happen, we could create a TTL index on the `date`
    field, specifying how long one document will exist in the collection. Just remember
    that, once we create a TTL index, MongoDB will automatically remove the expired
    documents from the collection.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that the event hit information is useful just during one year. We will
    create an index on the `date` field with the property `expireAfterSeconds` with
    `31556926` as the value, which corresponds to one year in seconds.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command, executed on the mongod shell, creates the index on our
    events collection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'If the index does not exist, the output should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Once this is done, our documents will live in our collection for one year, based
    on the date field, and after this MongoDB will remove them automatically.
  prefs: []
  type: TYPE_NORMAL
- en: Sharding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you are one of those people who have infinite resources and would like to
    have a lot of information stored on disk, then one solution to mitigate the storage
    space problem is to distribute the data by sharding your collection.
  prefs: []
  type: TYPE_NORMAL
- en: And, as we stated before, we should increase our efforts when we choose the
    shard key, since it is through the shard key that we will guarantee that our read
    and write operations will be equally distributed by the shards, that is, one query
    will target a single shard or a few shards on the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have full control over how many resources (or pages) we have on our
    web server and how this number will grow or decrease, the resource name becomes
    a good choice for a shard key. However, if we have a resource that has more requests
    (or events) than others, then we will have a shard that will be overloaded. To
    avoid this, we will include the date field to compose the shard key, which will
    also give us better performance on query executions that include this field in
    the criteria.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember: our goal is not to explain the setup of a sharded cluster. We will
    present to you the command that shards our collection, taking into account that
    you previously created your sharded cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To shard the events collection with the shard key we choose, we will execute
    the following command on the mongos shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The expected output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If our events collection has any document in it, we will need to create an
    index where the shard key is a prefix before sharding the collection. To create
    the index, execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: With the collection with the shard enabled, we will have more capacity to store
    data in the events collection, and a potential gain in performance as the data
    grows.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've designed our document and prepared our collection to receive
    a huge amount of data, let's perform some queries!
  prefs: []
  type: TYPE_NORMAL
- en: Querying for reports
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Until now, we have focused our efforts on storing the data in our database.
    This does not mean that we are not concerned about read operations. Everything
    we did was made possible by outlining the profile of our application, and trying
    to cover all the requirements to prepare our database for whatever comes our way.
  prefs: []
  type: TYPE_NORMAL
- en: So, we will now illustrate some of the possibilities that we have to query our
    collection, in order to build reports based on the stored data.
  prefs: []
  type: TYPE_NORMAL
- en: If what we need is real-time information about the total hits on a resource,
    we can use our daily field to query the data. With this field, we can determine
    the total hits on a resource at a particular time of day, or even the average
    requests per minute on the resource based on the minute of the day.
  prefs: []
  type: TYPE_NORMAL
- en: 'To query the total hits based on the current time of the day, we will create
    a new function called `getCurrentDayhits` and, to query the average request per
    minute in a day, we will create the `getCurrentMinuteStats` function in the `app.js`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'To see the magic happening, we should run the following command in the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'If everything is fine, the output should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Another possibility is to retrieve daily information to calculate the average
    requests per minute of a resource, or to get the set of data between two dates
    to build a graph or a table.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code has two new functions, `getAverageRequestPerMinuteStats`,
    which calculates the average number of requests per minute of a resource, and
    `getBetweenDatesDailyStats`, which shows how to retrieve the set of data between
    two dates. Let''s see what the `app.js` file looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, there are many ways to query the data in the `events` collection.
    These were some very simple examples of how to extract the data, but they were
    functional and reliable ones.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter showed you an example of the process of designing a schema from
    scratch, in order to solve a real-life problem. We began with a detailed problem
    and its requirements and evolved the schema design to have better use of available
    resources. The sample code based on the problem is very simple, but will serve
    as a basis for your life-long learning. Great!
  prefs: []
  type: TYPE_NORMAL
- en: In this last chapter, we had the opportunity to make, in just a few pages, a
    journey back to the first chapters of this book and apply the concepts that were
    introduced along the way. But, as you must have realized by now, MongoDB is a
    young, full of database that is full of possibilities. Its adoption by the community
    around it—and that includes your own—gets bigger with each new release. Thus,
    if you find yourself faced with a new challenge that you realize that has more
    than one single solution, carry out any test deem necessary or useful. Colleagues
    can also help, so talk to them. And always keep in mind that a good design is
    the one that fits your needs.
  prefs: []
  type: TYPE_NORMAL
