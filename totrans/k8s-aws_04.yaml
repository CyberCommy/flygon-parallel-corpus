- en: Managing Change in Your Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 2](87aab5e7-ff37-4a46-831b-8ff62708b7d8.xhtml), *Start Your Engines*,
    we took a first look at running an application on Kubernetes using deployments.
    In this chapter, we are going to go into depth with tools that Kubernetes provides
    to manage the pods that we run on your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: We will learn how to ensure that batch tasks are successfully completed by using
    the `Job` resource
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will learn how to run jobs at scheduled intervals with the `CronJob` resource
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we will learn how to use deployments to keep long-running applications
    running indefinitely, and to update them or their configuration when changes need
    to be made
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will look at how we can launch pods in different ways with Kubernetes, depending
    on the workloads we are running.
  prefs: []
  type: TYPE_NORMAL
- en: You will learn a lot more about how to use the deployment resource to control
    the way Kubernetes rolls out changes to long-running applications. You will discover
    the ways you can use Kubernetes to perform common deploy patterns, such as blue-green
    and canary deployments.
  prefs: []
  type: TYPE_NORMAL
- en: 'By design, pods are not intended to be durable in any way. As we have discussed
    previously, there is a whole raft of conditions that can cause the life of a pod
    to be terminated. They include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The failure of an underlying node**: Perhaps caused by some unexpected event,
    such as a hardware failure. Or perhaps by design; for example in a cluster utilizing
    spot priced instances nodes can be terminated without warning if demand for instances
    increases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pod evictions initiated by the scheduler**: The scheduler can initiate pod
    evictions when it needs to in order to optimize the usage of resources on the
    cluster. This could be because some processes have a higher priority than others,
    or just to optimize bin packing on the cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pods manually removed by the user.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pods removed due to planned maintenance; for example, by using the `kubectl
    drain` command.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The node is no longer visible to the cluster due to a network partition.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pods removed from the node in preparation of a scaling down action.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, if the design of Kubernetes expects pods to ephemeral, how can we deploy
    reliable applications? Surely, we need some way to run our programs without fail?
    Thankfully, this is not exactly the case. The important part of this design is
    that it accurately models the wide range of issues that can occur in the system
    due to the underlying hardware and software, and as a result of management processes.
    Rather than trying to make the primitive building block (the pod) resilient to
    failures itself, Kubernetes provides a number of controllers that we, as users,
    can interact with directly to build resilient services. These controllers handle
    creating replacements for pods that have been lost for any number of reasons.
  prefs: []
  type: TYPE_NORMAL
- en: 'These controllers fall into four groups, and our choice really depends on the
    type of workload we want to run:'
  prefs: []
  type: TYPE_NORMAL
- en: For processes that we expect to end, such as batch jobs or other finite processes,
    Kubernetes provides the job abstraction. Jobs ensure that a pod runs to completion
    at least once.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For pods that we expect to be long-running, such as a web server or a background
    processing worker, Kubernetes provides deployments and the lower level ReplicationController
    or ReplicaSet.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For pods that we want to run on all machines (or a subset of them), Kubernetes
    provides DaemonSet. DaemonSet are typically used to provide machine-specific services
    that form part of your platform, such as log management or monitoring agents,
    and commonly to deploy per node components of an overlay network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For groups of pods where each pod requires a stable identity or to access persistent
    storage, Kubernetes provides `StatefulSets`. (We will cover `StatefulSets` in
    [Chapter 9](c2a3f846-f6cc-4fd1-af93-d325afeeffb6.xhtml), *Storing State*.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you think back to what we learned about the architecture of Kubernetes in
    [Chapter 1](19821a2b-bb32-408d-9f21-256dce5d644e.xhtml), *Google's Infrastructure
    for the Rest of Us*, it is important to remember that the controller manager (the
    Kubernetes micro service that runs all these controllers) is a separate and distinct
    process from the scheduler. The core lower-level parts of Kubernetes, such as
    the scheduler and the kubelet, only know about pods, whereas the higher-level
    controllers don't need to understand any of the details of actually scheduling
    and running pods on nodes. They just make a request to the API server for a pod
    to be created and the lower-level machinery ensures that they are scheduled and
    run correctly.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are going to walk through the important features and configuration
    options that jobs, deployments, and DaemonSet provide us. By working through some
    examples, you will start to get a feel for when to use each resource to deploy
    your applications. You should take your time to understand what each controller
    is doing and why you would want to use it.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying software to a distributed environment can be a little bit unusual
    at first, because a lot of assumptions you might have made about the way that
    your software runs when deploying it to a single machine might not work in a distributed
    system.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes does a great job of making it possible to deploy most software without
    any modifications at all. I like to think that Kubernetes lets us trade a little
    simplicity for a lot of reliability.
  prefs: []
  type: TYPE_NORMAL
- en: Running pods directly
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes doesn't really intend for users to directly submit and launch pods
    on the cluster. As we discussed previously, pods are designed to be ephemeral,
    so are not suitable for running workloads where we want to ensure that execution
    has completed or where we want to ensure that a process remains up and running.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we will start from first principles, launching pods, before moving on
    to use a controller to help us manage them. Bear in mind that this is a learning
    exercise; you shouldn''t submit pods in this way if you need them to run reliably:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This pod launches an infinite loop that prints `hello world`Â every 2 seconds.
    Start by submitting the pod to the cluster with `kubectl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: It might take a few moments for the pod be created while the container runtime
    downloads the image. While this is happening, you could check on the status of
    the pod by running `kubectl describe pod/hello-loop` or by using the dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: The fact that Kubernetes makes it possible to control even the lowest-level
    abstractions, such as pods, through the API makes it easy to extend Kubernetes
    with additional functionality using or building add-on tools that can be just
    as powerful as the built-in controllers.
  prefs: []
  type: TYPE_NORMAL
- en: Once the pod is up and running, you can follow the output with `kubectl logs
    -f hello-loop` and you should see `hello world` output every 2 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: '`kubectl logs` allows us to display logs from pods that have run on the cluster.
    If you know the name of the pod you want logs from, you can just pass the name
    as an argument. But if you are using a controller to launch a pod, you can use
    the name of a job or deployment in place of the pod name just by prefixing the
    name with the resource type.'
  prefs: []
  type: TYPE_NORMAL
- en: If you have a label selector for the pod or pods you are interested in, they
    can be passed with the `-l` flag. With the `-c` flag, you can target a specific
    named container in a pod with more than one container; if the pod only has one
    container, this can be omitted.
  prefs: []
  type: TYPE_NORMAL
- en: Try running `kubectl`. It helps logs to discover some more of the options you
    can use to view just the logs you are interested in, including limiting them to
    a particular time period.
  prefs: []
  type: TYPE_NORMAL
- en: Jobs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The simplest use case for a job is to launch a single pod and ensure that it
    successfully runs to completion.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our next example, we are going to use the Ruby programming language to compute
    and print out the first 100 Fibonacci numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the contents of `spec` and `template` are very similar to the specification
    we used to launch a pod directly. When we define a pod template for use in a job,
    we need to choose a `restartPolicy` of `Never` or `OnFailure`.
  prefs: []
  type: TYPE_NORMAL
- en: The reason for this is that the end goal of a job is to run the pod until it
    exits successfully. If the underlying pod is restarted when it exits successfully,
    the pod would continue to be restarted and the job would never complete.
  prefs: []
  type: TYPE_NORMAL
- en: 'Save the definition to a file and then submit it to the cluster using `kubectl
    create`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you have submitted a job to Kubernetes, you can check on its status with
    the `kubectl describe` command. It might take a little while for the Docker image
    to download and Kubernetes to launch the pod. Once the pod is running, you should
    see first `1 Running` and then `1 Succeeded` in the `Pods Statues` field:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'When waiting for Kubernetes to take some action, repeatedly running `kubectl`
    to find out what is happening can get tedious. I like to use the `watch` command
    in conjunction with `kubectl`. To watch Kubernetes launch this job, I could run:'
  prefs: []
  type: TYPE_NORMAL
- en: '`**$ watch kubectl describe jobs/fib**`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Most Linux distributions will include the watch command by default, or make
    it simple to install with a package manager. If you are on macOS, it''s very simple
    to install with Homebrew:'
  prefs: []
  type: TYPE_NORMAL
- en: '`**$ brew install watch**`'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use `kubectl logs` to view the output from our job. Notice how we don''t
    need to know the name of the underlying pod(s); we can just refer to the job by
    name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also look at the underlying pod that was created by this job with `kubectl
    get` by using the `job-name` label that Kubernetes adds to the pods for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `--show-all` flag means that all pods are shown (even those that no longer
    have a running status).
  prefs: []
  type: TYPE_NORMAL
- en: Notice how Kubernetes created a unique name for our pod based on the job name.
    This is important because if the first pod to have been created failed in some
    way, Kubernetes would need to launch another pod based on the same pod specification.
  prefs: []
  type: TYPE_NORMAL
- en: One of the key advantages jobs have over launching a pod directly is that a
    job is able to handle not only errors caused by the underlying infrastructure
    that might cause a pod to be lost before it has completed, but also errors that
    occur at runtime.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate how this works, this job simulates a process that (mostly) fails
    with a non-zero exit status, but sometimes exits with a (successful) zero exit
    status. This Ruby program chooses a random integer from 0 to 10 and exits with
    it. So, on average, Kubernetes will have to run the pod 10 times before it exits
    successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'As before, submit the job to your cluster with `kubectl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Unless you are very lucky, when you inspect the job, you should see that Kubernetes
    has to launch a number of pods before one exited with 0 status:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/84ee4ac9-b016-4699-ba25-d666146e64aa.png)'
  prefs: []
  type: TYPE_IMG
- en: Inspecting the pods launched by the luck job using the Kubernetes dashboard
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the pod spec has a `restartPolicy` of `Never`. This means that
    when the pod exits with a non-zero exit status, the pod is marked as terminated
    and the job controller launches another pod. It is also possible to run jobs with
    a `restartPolicy` of `OnFailure`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Try editing `luck.yaml` to make this change. Remove the first version of the
    `luck` job and submit your new version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This time, you should notice that instead of quickly launching new pods until
    one exits successfully, Kubernetes restarts one pod until it is successful. You
    will notice that this takes quite a bit longer, because when Kubernetes restarts
    a pod locally with an exponential back-off, this behavior is useful if a failure
    was caused by an underlying resource that is overloaded or unavailable. You might
    notice the pod in a status of `CrashLoopBackoff` while Kubernetes is waiting to
    restart the pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Allowing the job controller to recreate a new pod each time it terminates in
    error ensures that the new pod is run in a new pristine environment and causes
    the job resource to retain a record of each execution attempt. For this reason,
    it is usually best not to utilize a pod restart policy in conjunction with a job,
    unless you have to deal with pods that regularly fail or if you want to retain
    the execution environment between attempts.
  prefs: []
  type: TYPE_NORMAL
- en: CronJob
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now you have learned how to run one-off or batch tasks with jobs, it is simple
    to extend the concept in order to run scheduled jobs. In Kubernetes, a `CronJob`
    is a controller that creates new jobs from a template on a given schedule.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s begin with a simple example. The following example will launch a job
    every minute. This job will output the current date and time and then exit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Push the CronJob to Kubernetes with `kubectl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'After some time (less than a minute), you should see the first job created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The label we added to the pod template spec allows us to use `kubectl logs`
    to see the output of all the pods created by the CronJob:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Cron syntax
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The syntax of the schedule field follows the standard Cron format, which should
    be familiar if you have ever set up CronJobs on a Unix-like system. Kubernetes
    supports standard cron strings with a few common extensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'A standard cron string consists of five fields that each represent different
    units of time. Each can be set to an expression representing a particular time,
    or a wildcard (*) that would match every time. For example, a wildcard in the
    **Months** column would match every month:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Minutes | Hours | Day of Month | Month | Day of Week |'
  prefs: []
  type: TYPE_TB
- en: Order of cron fields
  prefs: []
  type: TYPE_NORMAL
- en: 'The cron format is simplest to understand if it is read from left to right.
    Here are some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '`0 * * * *`: On the hour, every hour'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`15 * * * *`: 15 minutes past every hour'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`0 0 * * *`: At midnight, every day'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`30 5 1 * *`: 5:30 a.m. on the first day of the month, every month'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`30 17 * * 1`: 15:30 p.m., every Monday'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As well as the wildcard, there are a few other characters with special meaning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Slashes are used to indicate steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '`0/15 * * * *`: Every 15 minutes, starting at 0; for example, 12:00, 12:15,
    12:30, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`15/15 * * * *`: Every 15 minutes, starting at 15; for example, 12:15, 12:30,
    12:45, 13:15, 13:30, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`0 0 0/10 * *`: Every 10 days at midnight'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hyphens indicate ranges:'
  prefs: []
  type: TYPE_NORMAL
- en: '`0 9-17 * * *`: Once an hour during office hours (9 a.m. till 5 p.m.)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`0 0 1-15/2 * *`: Every other day for the first 15 days of every month'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Commas indicate lists:'
  prefs: []
  type: TYPE_NORMAL
- en: '`0 0 * * 6,0`: Midnight on Saturday and Sunday'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`0 9,12,17 * * 1-5`: At 9:00 a.m., 12 noon, and 5:00 p.m., Monday to Friday'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As an aid to readability, names can be used in the month and day of the week
    fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '`0 0 * * SUN`: Midnight on Sunday'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`0 6 * MAR-MAY *`: 6 a.m. every day in Spring'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you don''t mind when exactly a job is run, you can specify a fixed interval
    and Kubernetes will create jobs at a fixed interval:'
  prefs: []
  type: TYPE_NORMAL
- en: '`@every 15m`: Every 15 minutes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`@every 1h30m`: Every 1-and-a half hours'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`@every 12h`: Every 12 hours'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bear in mind that the interval doesn't take the time that the job takes to run
    into account; it just ensures that the time that each job is scheduled is separated
    by the given interval.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, there are several predefined schedules that can be used as a shortcut
    in place of a cron string:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Shortcut** | **Equivalent cron** |  |'
  prefs: []
  type: TYPE_TB
- en: '| `@hourly` | `0 0 * * * *` | Every hour, on the hour |'
  prefs: []
  type: TYPE_TB
- en: '| `@daily` | `0 0 0 * * *` | Every day at midnight |'
  prefs: []
  type: TYPE_TB
- en: '| `@weekly` | `0 0 0 * * 0` | Every week midnight on Sunday |'
  prefs: []
  type: TYPE_TB
- en: '| `@monthly` | `0 0 0 1 * *` | Monthly, at midnight on the 1st |'
  prefs: []
  type: TYPE_TB
- en: '| `@yearly` | `0 0 0 1 1 *` | Midnight, every New Year''s Eve |'
  prefs: []
  type: TYPE_TB
- en: Concurrency policy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kubernetes CronJob, in contrast to the traditional CronJob, allows us to decide
    what happens when a job overruns and we reach the scheduled time while the previous
    job is still running. We can control this behavior by setting the `spec.concurrencyPolicy`
    field on the CronJob. There are three possible policies that we can choose:'
  prefs: []
  type: TYPE_NORMAL
- en: By default, if the field is unset then we get the `Allow` policy. This behaves
    just like a traditional CronJob and allows multiple instances of a job to run
    at the same time. If you stick with this, you should be sure that your jobs indeed
    get completed at some point, or your cluster could end up overwhelmed with many
    jobs running at the same time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Forbid` policy prevents any new jobs from starting while an existing job
    is still running. This means that if a job overruns, Kubernetes will skip the
    next run. This is a good choice if having two or more instances of a job running
    could cause conflicts or use up shared resources. Your job, of course, does need
    to be able to account for missing runs in this case.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the `Replace` policy also prevents more than one job from running at
    once, but rather than skipping a run, it first kills the existing job and then
    launches a new job.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: History limits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By default, when you use a CronJob, the jobs that it creates will stick around,
    so you can check on what happened to a particular run of a job for debugging or
    reporting. You might, however, find that the number of jobs in the successful
    or failed state starts to pile up quite quickly when using CronJob. This is simple
    to manage with the `spec.successfulJobsHistoryLimit` and `spec.failedJobsHistoryLimit`
    fields. Once the successful, or failed jobs reach the number specified in the
    limit, the oldest job is deleted each time a new job is created. If you set a
    limit to 0, the jobs are removed as soon as they complete.
  prefs: []
  type: TYPE_NORMAL
- en: Managing long running processes with deployments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Updating batch processes, such as jobs and CronJobs, is relatively easy. Since
    they have a limited lifetime, the simplest strategy of updating code or configurations
    is just to update the resources in question before they are used again.
  prefs: []
  type: TYPE_NORMAL
- en: Long-running processes are a little harder to deal with, and even harder to
    manage if you are exposing a service to the network. Kubernetes provides us with
    the deployment resource to make deploying and, more importantly, updating long-running
    processes simpler.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 2](87aab5e7-ff37-4a46-831b-8ff62708b7d8.xhtml), *Start Your Engines*,
    we took a first look at the deployment resource, both creating deployments with
    `kubectl run` and by defining a deployment object in a YAML file. In this chapter,
    we will recap the process that the deployment controller uses to roll out changes,
    and then look in to some of the more advanced options for controlling exactly
    how new versions of the pods are made available. We will cover how we can use
    deployments in conjunction with services to make changes to services provided
    on the network without downtime.
  prefs: []
  type: TYPE_NORMAL
- en: Much like CronJob is a controller for jobs, a deployment is a controller for
    ReplicaSets. A ReplicaSet makes sure that the required number of pods for a particular
    configuration is up and running. In order to manage a change to this configuration,
    the deployment controller creates a new ReplicaSet with the new configuration,
    and then scales the old ReplicaSet down and the new one up, according to a particular
    strategy. A deployment will maintain a reference to the old ReplicaSet even after
    the deployment of the new configuration is complete. This allows the deployment
    to also orchestrate a rollback to a previous version if required.
  prefs: []
  type: TYPE_NORMAL
- en: Let's begin with an example application that will allow you to quickly understand
    how the different options offered by deployments allow you to manipulate the behavior
    of your application during an update to your code or configuration.
  prefs: []
  type: TYPE_NORMAL
- en: We will be deploying an application that I created to make it simple to illustrate
    deploying new versions of software with Kubernetes. It is a simple Ruby web application
    in a Docker repository that has many version tags. Each version displays a unique
    name and color scheme when the homepage is opened in a browser.
  prefs: []
  type: TYPE_NORMAL
- en: When we deploy a long-running process to Kubernetes, we can roll out access
    to the application in a controlled manner using labels.
  prefs: []
  type: TYPE_NORMAL
- en: The simplest strategy to implement is to use a single deployment to roll out
    changes to a new version of your applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement this, we need to start by creating a service with a label selector
    that will match every version of the application that we might deploy now, or
    in the future:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, we have achieved this by matching any pod that has a label matching
    the `selector` asÂ `app: ver`.'
  prefs: []
  type: TYPE_NORMAL
- en: When running a more complicated application that has several different processes
    managed by multiple deployments, your labels and selectors will need to be more
    complicated. A common pattern is to distinguish between the component parts of
    an application with a `component` label.
  prefs: []
  type: TYPE_NORMAL
- en: It makes sense to submit the service definition before you start any pods. This
    is because the scheduler will, if possible, try to spread the pods used by a particular
    service across multiple nodes for greater reliability.
  prefs: []
  type: TYPE_NORMAL
- en: Submit the service definition to your cluster using `kubectl apply -f service.yaml`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the service has been submitted to the cluster, we can prepare the initial
    deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'To access the running service, the simplest way is to use `kubectl` to open
    a proxy to the Kubernetes API running on your cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Once you have done, you should be able to view the app using your browser at
    `http://localhost:8001/api/v1/namespaces/default/services/ver/proxy`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/43c3e4f1-135f-4c5a-b3e5-a02ef4f97e85.png)'
  prefs: []
  type: TYPE_IMG
- en: Version 0.0.1 running in our cluster
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of ways that we can now make changes to our deployment.
  prefs: []
  type: TYPE_NORMAL
- en: kubectl patch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To upgrade to version 0.0.2, we will execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Because containers is a list, we need to specify the merge key `name` for Kubernetes
    to understand which container we want to update the image field on.
  prefs: []
  type: TYPE_NORMAL
- en: With the `patch` command, Kubernetes performs a merge, merging the JSON provided
    with the current definition of the `deployment/versions` object.
  prefs: []
  type: TYPE_NORMAL
- en: Go ahead and reload the app in your browser, and then you should notice (after
    a few seconds) that the new version of the app becomes available.
  prefs: []
  type: TYPE_NORMAL
- en: kubectl edit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To upgrade to version 0.0.3, we are going to use the `kubectl edit` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '`kubectl edit` uses your system''s *standard* editor to edit Kubernetes resources.
    This is often vi, vim, or even ed, but if you have another text editor you prefer
    you should set up the `EDITOR` environment variable to point at your preferred
    choice.'
  prefs: []
  type: TYPE_NORMAL
- en: This should open your editor, so you can make changes to the deployment. Once
    this has happened, edit the image field to use version 0.0.3 and save the file.
  prefs: []
  type: TYPE_NORMAL
- en: You might notice that there are more fields in the object opened in your editor
    than the original file you submitted to Kubernetes. This is because Kubernetes
    is storing metadata about the current status of the deployment in this object.
  prefs: []
  type: TYPE_NORMAL
- en: kubectl apply
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To update to version 0.0.4, we are going to use the `apply` command. This allows
    us to submit the full resource to Kubernetes just like when we made the initial
    deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by editing your deployment YAML file, and then update the image field
    to use version 0.0.4\. Save the file and then use `kubectl` to submit it to Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: If you use `kubectl apply` for a resource that doesn't yet exist, it will be
    created for you. This can be useful if you are using it in a scripted deployment.
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of using `kubectl apply` rather than edit or patch is that you
    can keep a file checked into version control to represent the state of your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes dashboard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Kubernetes dashboard includes a tree-based editor that allows you to edit
    resources right in the browser. On Minikube, you can run the Minikube dashboard
    to open the dashboard in your browser. You can then choose your deployment and
    click on the edit button at the top of the page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ed270944-d34b-428c-861b-769b43553495.png)'
  prefs: []
  type: TYPE_IMG
- en: You should be able to find the container image field by scrolling or with the
    search function. It is simple to click on a value to edit it and then press **UPDATE**.
  prefs: []
  type: TYPE_NORMAL
- en: While you are learning about Kubernetes and experimenting with different configurations,
    the method you use for updating your configuration should be your own personal
    preference. Using the Kubernetes dashboard or tools such as `kubectl edit` are
    great for learning and debugging. But when you move forward to a production environment,
    you will want to move toward checking your configuration into version control,
    or using a tool such as Helm (which we will discuss in [Chapter 5](90166a73-b677-467e-a198-c00b7c078bd0.xhtml),
    *Managing Complex Applications with Helm*).
  prefs: []
  type: TYPE_NORMAL
- en: Greater control of your deployments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By now we have covered a number of ways that we can update resources in Kubernetes.
    As we have observed, when we update a deployment in Kubernetes, eventually the
    pods in the cluster are updated to reflect the new configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes achieves this by managing ReplicaSets behind the scenes.
  prefs: []
  type: TYPE_NORMAL
- en: The ReplicaSet is purely concerned with managing a set of pods to ensure that
    the desired number of replicas are running on the cluster. During an update, the
    pod spec of the existing ReplicaSet is never changed. The deployment controller
    creates a new ReplicaSet with the new pod configuration. The roll-out of this
    new configuration is orchestrated by altering the desired number of replicas for
    each ReplicaSet.
  prefs: []
  type: TYPE_NORMAL
- en: This separation of concerns is typical of the way that resources are designed
    in Kubernetes. More complex behavior is achieved by orchestrating simpler objects,
    whose controllers implement simpler behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: This design also makes it quite simple for us (the cluster operator) to decide
    exactly what behavior we want when we update our configuration. The `spec.stratergy`
    field is used to configure the behavior that is used when changes are rolled out.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `.spec.strategy.type` field defines the strategy that is used to replace
    the old pods with new ones. Currently, there are two strategies: `Recreate` and
    `RollingUpdate`. `RollingUpdate` is the default strategy, so normally you won''t
    need to specify it in your configuration.'
  prefs: []
  type: TYPE_NORMAL
- en: RollingUpdate deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`.spec.strategy.type=RollingUpdate is the default strategy`. This is the strategy
    that we have been using in our examples so far.'
  prefs: []
  type: TYPE_NORMAL
- en: You would specifically choose a rolling update whenever you want to update without
    interruption to service. Conversely, your application has to work correctly when
    multiple versions are running at the same time if you use this strategy.
  prefs: []
  type: TYPE_NORMAL
- en: 'When using the `RollingUpdate` strategy, there are two settings that allow
    us to specify how quickly the new ReplicaSet is scaled up and how quickly the
    old ReplicaSet is scaled down:'
  prefs: []
  type: TYPE_NORMAL
- en: '`.spec.strategy.rollingUpdate.maxUnavailable`: It specifies the number of pods
    that can be unavailable (out of the desired total) during the deployment process'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.spec.strategy.rollingUpdate.maxSurge`: It specifies the number of pods that
    can be created over and above the desired total during the deployment process'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These settings accept either absolute values, such as 1 or 0, or a percentage
    of the total desired number of pods on the deployment. A percentage value is useful
    if you intend for this configuration to be reusable across different deployments
    that are scaled to different levels, or if you intend to control the desired number
    of pods with an auto-scaling mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: By setting `maxUnavailable` to `0`, Kubernetes will wait till replacement pod(s)
    have been scheduled and are running before killing any pods managed by the old
    ReplicationSet. If `maxUnavailable` is used in this way, then during the deployment
    process, Kubernetes will run more than the desired number of pods so `maxSurge`
    cannot be `0`, and you must have the required resources (in a cluster, and for
    backing services) to support temporarily running the extra instances during the
    deployment phase.
  prefs: []
  type: TYPE_NORMAL
- en: Once Kubernetes has launched all the instances, it it must wait until the new
    pods are in service and in the `Ready` state. This means that if you have set
    up health checks for your pod, the deployment will pause if these are failing.
  prefs: []
  type: TYPE_NORMAL
- en: If `maxSurge` and/or `maxUnavailable` are set to low values, your deployments
    will take longer as the deployment will pause and wait for the new pod(s) to become
    available before moving forward. This can be useful, as it provides you a degree
    of protection against deploying broken code or configurations.
  prefs: []
  type: TYPE_NORMAL
- en: Setting `maxSurge` to a bigger value will decrease the number of scaling steps
    the deployment takes to update the application. If, for example, you were to set
    `maxSurge` to 100% and `maxUnavailable` to 0 then Kubernetes would create all
    the replacement pods as soon as the deployment starts and kill the existing pods
    as the new ones enter the Ready state.
  prefs: []
  type: TYPE_NORMAL
- en: Exactly how you want to configure your deployments will depend on the requirements
    of your application and the resources available to your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: You should bear in mind that setting `maxSurge` to lower values will give you
    slower deployments that take longer to complete, but may be more resilient to
    errors, whereas, with higher `maxSurge` values, your deployments will progress
    faster. But your cluster will need to have enough capacity to support the additional
    running instances. If your application accesses other services, you should also
    be aware of the additional load that might be placed on them. For example, databases
    can be configured to have a limit to the number of connections that they accept.
  prefs: []
  type: TYPE_NORMAL
- en: Recreate deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`.spec.strategy.type=Recreate` takes a much simpler approach to rolling out
    changes to your application. First, all the pods with the previous configuration
    are terminated by scaling down the active ReplicaSet, and then a new ReplicaSet
    is created that starts replacement pods.'
  prefs: []
  type: TYPE_NORMAL
- en: This strategy is particularly appropriate when you don't mind short periods
    of downtime. For example, with background processing, when workers or other tasks
    don't need to provide services that are accessed over the network. The advantages
    in these use cases are twofold. Firstly, you don't have to worry about any incompatibilities
    caused by two versions of your code running at the same time. Secondly, of course,
    with this strategy the process of updating your pods uses no more resources that
    your application would normally need.
  prefs: []
  type: TYPE_NORMAL
- en: DaemonSet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you want a single instance of a particular pod to be running on every node
    of your cluster (or a subset of your nodes), then you need to use a DaemonSet.
    When you schedule a DaemonSet to your cluster, an instance of your pod will be
    scheduled to every node, and when you add new nodes, the pod is scheduled there
    too. DaemonSet are very useful for providing ubiquitous services that need to
    be available everywhere on your cluster. You might use DaemonSet to provide services
    such as:'
  prefs: []
  type: TYPE_NORMAL
- en: An agent to ingest and ship logs, such as Fluentd or Logstash
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A monitoring agent, such as collectd, Prometheus Node Exporter, datadog, NewRelic
    or SysDig, and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A daemon for a distributed storage system, such as Gluster or Ceph
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Components for an overlay network, such as Calico or Flannel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Per node components, a virtualization tool, such as OpenStack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before Kubernetes, these sorts of services would require you to configure an
    init system, such as `systemd` or SysVnit, on every server in your infrastructure.
    When you came to update the service or its configuration, you would have to update
    that configuration and restart services across all your servers, which is not
    a problem when you are managing a few servers, but with tens, hundreds, or even
    thousands of servers, things quickly become much harder to manage.
  prefs: []
  type: TYPE_NORMAL
- en: DaemonSetÂ lets you use exactly the same configuration and containerization we
    have been applying to the applications that run on your infrastructure to manage
    the infrastructure itself.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at a simple example to understand how we can create a DaemonSet for
    a useful purpose. We will be deploying the Prometheus Node Exporter. The purpose
    of this application is to expose an HTTP endpoint that includes metrics about
    the Linux system it is running on.
  prefs: []
  type: TYPE_NORMAL
- en: If you decide to monitor your cluster, PrometheusNode Exporter is a very useful
    tool. If you do decide to run it in your own cluster, I would recommend that you
    look at the extensive documentation available on the GitHub page atÂ [https://github.com/prometheus/node_exporter](https://github.com/prometheus/node_exporter).
  prefs: []
  type: TYPE_NORMAL
- en: 'This manifest causes the pod specified in the template section to be scheduled
    to every node in your cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Once you have prepared the manifest file for the Node Exporter, submit it to
    Kubernetes by running theÂ `kubectl apply -f node-exporter.yaml` command.
  prefs: []
  type: TYPE_NORMAL
- en: You can check if the DaemonSet controller has scheduled our pod to the nodes
    in your cluster correctly by running theÂ `kubectl describe ds/node-exporter` command.
    Assuming that the pod is successfully running, you should be able to make an HTTP
    request to port `9100` on one of your nodes to see the metrics that it exposes.
  prefs: []
  type: TYPE_NORMAL
- en: If you are trying this example on Minikube, you can discover the IP address
    of the (only) node in your cluster by running `minikube ip`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then you can use a tool such as `curl` to make a request:'
  prefs: []
  type: TYPE_NORMAL
- en: '`**curl 192.168.99.100:9100/metrics**`'
  prefs: []
  type: TYPE_NORMAL
- en: One of the key advantages to using DaemonSetÂ to manage infrastructure tools
    and components, rather than relying on static configuration on your nodes to manage
    them, is that they can be updated just as easily as any other application you
    are running on your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: By default, DaemonSet have an `updateStrategy` of `RollingUpdate`. This means
    if you edit the pod template in a DaemonSet, the existing pods currently running
    on the cluster are killed and replaced one by one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try using this functionality to upgrade to a newer version of the Prometheus
    Node Exporter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'You can check on the progress of replacing the old pods with the new version
    by running: `kubectl rollout status ds/node-exporter` command. Once the update
    is completed, you should see the following message: `daemon set "node-exporter"
    successfully rolled out`.'
  prefs: []
  type: TYPE_NORMAL
- en: You might be wondering what other `updateStrategys` are available for DaemonSet.
    The only other option is `OnDelete`. With this option, when a DaemonSet is updated,
    no changes are made to the running pods running on the cluster, and it is left
    up to you to manually delete the running pods before the new version is launched.
    This mainly exists to provide compatibility with the behavior in previous versions
    of Kubernetes and is not, in practice, very useful.
  prefs: []
  type: TYPE_NORMAL
- en: It is worth bearing in mind that in order to roll out a new version of a pod
    with a DaemonSet, there will be a short period between the old pod being killed
    and the new one being launched, during which the service you are running will
    be unavailable.
  prefs: []
  type: TYPE_NORMAL
- en: 'DaemonSet can also be used to run pods on a subset of the nodes in your cluster.
    This is achieved by labeling the nodes in your cluster and adding a `nodeSelector`
    to the pod spec of your DaemonSet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you have edited your manifest to add the `nodeSelector`, submit the new
    configuration to Kubernetes with: `kubectl apply -f node-exporter.yaml`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You should notice that the running node exporter pods are terminated and removed
    from your cluster. This is because no nodes in your cluster match the label selector
    that we added to the DaemonSet. Nodes can be labeled on the fly by using `kubectl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Once a node is correctly labeled, you should notice that the DaemonSet controller
    schedules a pod to it.
  prefs: []
  type: TYPE_NORMAL
- en: On AWS, nodes are automatically labeled with information including region, availability
    zone, instance type, and hostname. You might wish to use these labels to deploy
    services to certain nodes in your cluster, or to provide differently configured
    versions of tools for different types of node in your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to add additional labels, you can pass them as arguments to the
    kubelet using the `--node-labels` flag.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned how to use Kubernetes to run our applications
    and, importantly, how to roll out new versions of our applications and their configurations.
  prefs: []
  type: TYPE_NORMAL
- en: 'We built on our basic knowledge of pods and deployments from the previous chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: Pods are the lowest-level abstraction that Kubernetes provides us
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the other resources that deal with running containers, such as jobs, ScheduledJobs,
    deployments, and even DaemonSet, work by creating pods in specific ways.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normally, we don't want to create pods directly because if the node a pod is
    running on stops working, then so will the pod. Using one of the higher-level
    controllers ensures that a new pod will be created to replace failed pods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The higher-level resources, such as deployments and DaemonSet, provide a mechanism
    to replace one version of a pod with a different one in a controlled way. We learned
    about the different strategies that are available to do this.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before you move on to the next chapter, take some time to get a feel for how
    each of the deployment strategies work by observing how they behave during the
    deployment process. With a little experience, you will develop an understanding
    of which options to choose for a given application.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to look at using a tool that builds upon these
    concepts to provide even more powerful ways to deploy and update your applications.
  prefs: []
  type: TYPE_NORMAL
