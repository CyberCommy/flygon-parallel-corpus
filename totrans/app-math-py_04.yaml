- en: Working with Randomness and Probability
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss randomness and probability. We will start by
    briefly exploring the fundamentals of probability by selecting elements from a
    set of data. Then, we will learn how to generate (pseudo) random numbers using
    Python and NumPy, and how to generate samples according to a specific probability
    distribution. We will conclude the chapter by looking at a number of advanced
    topics covering random processes and Bayesian techniques, and using Markov chain
    Monte Carlo methods to estimate parameters on a simple model.
  prefs: []
  type: TYPE_NORMAL
- en: Probability is a quantification of the likelihood of a specific event occurring.
    We use probabilities intuitively all of the time, although sometimes the formal
    theory can be quite counterintuitive. Probability theory aims to describe the
    behavior of *random variables*, whose value is not known, but where the probabilities
    of the value of this random variable taking some (range of) values is known. These
    probabilities are usually in the form of one of several probability distributions.
    Arguably, the most famous such probability distribution is normal distribution
    which, for example, can describe the spread of a certain characteristic over a
    large population.
  prefs: []
  type: TYPE_NORMAL
- en: We will see probability again in a more applied setting in [Chapter6](87b0f91d-3086-41a9-995d-27fe7d364e8b.xhtml),
    *Working with Data and Statistics*, where we discuss statistics. Here, we will
    put probability theory to use to quantify errors and build a systematic theory
    of analyzing data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Selecting items at random
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating random data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Changing the random number generator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating normally distributed random numbers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with random processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing conversion rates with Bayesian techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimating parameters with Monte Carlo simulations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, we require the standard scientific Python packages, NumPy,
    Matplotlib, and SciPy. We will also require the PyMC3 package for the final recipe.
    You can install this using your favorite package manager, such as `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This command will install the most recent version of PyMC3, which, at the time
    of writing, was 3.9.2\. This package provides facilities for probabilistic programming,
    which involves performing many calculations driven by randomly generated data
    to understand the likely distribution of a solution to a problem.
  prefs: []
  type: TYPE_NORMAL
- en: The code for this chapter can be found in the `Chapter 04` folder of the GitHub
    repository at [https://github.com/PacktPublishing/Applying-Math-with-Python/tree/master/Chapter%2004](https://github.com/PacktPublishing/Applying-Math-with-Python/tree/master/Chapter%2004).
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the Code in Action: [https://bit.ly/2OP3FAo](https://bit.ly/2OP3FAo).'
  prefs: []
  type: TYPE_NORMAL
- en: Selecting items at random
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the core of probability and randomness is the idea of selecting an item from
    some kind of collection. As we know, the probability of selecting an item from
    a collection quantifies the likelihood of that item being selected. Randomness
    describes the selection of items from a collection according to the probabilities
    without any additional bias. The opposite of a random selection might be described
    as a *deterministic* selection. In general, it is very difficult to replicate
    a purely random process using a computer, because computers and their processing
    are inherently deterministic. However, we can generate sequences of pseudo-random
    numbers that, when properly constructed, demonstrate a reasonable approximation
    of randomness.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will select items from a collection and learn some of the
    key terminology associated with probability and randomness that we will need throughout
    this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Python Standard Library contains a module for generating (pseudo) random
    numbers called `random`, but in this recipe, and throughout this chapter, we will
    instead use the NumPy `random` module. The routines in the NumPy `random` module
    can be used to generate arrays of random numbers and are slightly more flexible
    than their standard library counterparts. As usual, we import NumPy under the
    alias `np`.
  prefs: []
  type: TYPE_NORMAL
- en: Before we can proceed, we need to fix some terminology. A *sample space* is
    a set (a collection with no repeated elements), and an *event* is a subset of
    the sample space. The *probability* that an event *A* occurs is denoted as *P*(*A*),
    and is a number between 0 and 1\. A probability of 0 indicates that the event
    can never occur, while a probability of 1 indicates that an event will certainly
    occur. The probability of the whole sample space must be 1.
  prefs: []
  type: TYPE_NORMAL
- en: When the sample space is discrete, then probabilities are just numbers between
    0 and 1 associated with each of the elements, where the sum of all these numbers
    is 1\. This gives meaning to the probability of selecting a single item (an event
    consisting of a single element) from a collection. We will consider methods for
    selecting items from a discrete collection here and deal with the *continuous*
    case in the *Generating normally distributed random numbers* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to select items at random from a container:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to set up the random number generator. For the moment, we
    will use the default random number generator for NumPy, which is recommended in
    most cases. We can do this by calling the `default_rng`routine from the NumPy
    `random`module, which will return an instance of a random number generator. We
    will usually call this function without a seed, but for this recipe, we will add
    the seed `12345` so that our results are repeatable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to create the data and probabilities that we will select from.
    This step can be skipped if you already have the data stored or if you want to
    select elements with equal probabilities:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As a quick sanity test, we can use an assertion to check that these probabilities
    do indeed sum to 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can use the `choice`method on the random number generator, `rng`, to
    select the samples from `data`according to the probabilities just created. For
    this selection, we want to turn the replacement on, so calling the method multiple
    times can select from the whole of `data`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'To select multiple items from `data`, we can also supply the `size`argument,
    which specifies the shape of the array to be selected. This plays the same role
    as the `shape`keyword argument to many of the other NumPy array creation routines.
    The argument given to `size`can be either an integer or a tuple of integers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `default_rng` routine creates a new **pseudo random number generator** (**PRNG**)
    instance (with or without a seed) that can be used to generate random numbers
    or, as we saw in the recipe, select items at random from predefined data. NumPy
    also has an **implicit state**-based interface for generating random numbers using
    routines directly from the `random` module. However, it is generally advisable
    to create the generator explicitly, using `default_rng` or create a `Generator`
    instance yourself. Being more explicit in this way is more Pythonic, and should
    lead to more reproducible results (in some sense).
  prefs: []
  type: TYPE_NORMAL
- en: A **seed** is a value that is passed to a random number generator in order to
    generate the values. The generator generates a sequence of numbers in a completely
    deterministic way based only on the seed. This means that two instances of the
    same PRNGs provided with the same seed will generate the same sequence of random
    numbers. If no seed is provided, the generators typically produce a seed that
    depends on the user's system.
  prefs: []
  type: TYPE_NORMAL
- en: The `Generator` class from NumPy is a wrapper around a low-level pseudo random
    bit generator, which is where the random numbers are actually generated. In recent
    versions of NumPy, the default PRNG algorithm is the 128-bit *permuted congruential
    generator.* By contrast, Python's built-in `random` module uses a Mersenne Twister
    PRNG. More information about the different options for PRNG algorithms is given
    in the *Changing the random number generator* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: The `choice` method on a `Generator` instance performs selections according
    to random numbers generated by the underlying `BitGenerator`. The optional `p`
    keyword argument specifies the probability associated with each item from the
    data provided. If this argument isn't provided, then a *uniform probability* is
    assumed, where each item has equal probability of being selected. The `replace`
    keyword argument specifies whether selections should be made with or without a
    replacement. We turned replacement on so that the same element can be selected
    more than once. The `choice` method uses the random numbers given by the generator
    to make the selections, which means that two PRNGs of the same type using the
    same seed will select the same items when using the `choice` method.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `choice` method can also be used to create random samples of a given size
    by passing `replace=False` as an argument. This guarantees the selection of distinct
    items from the data, which is good for generating a random sample. This might
    be used, for example, to select users to test a new version of an interface from
    the whole group of users; most sample statistical techniques rely on randomly
    selected samples.
  prefs: []
  type: TYPE_NORMAL
- en: Generating random data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many tasks involve generating large quantities of random numbers, which, in
    their most basic form, are either integers or floating-point numbers (double precision)
    lying in the range 0 ≤ *x* < 1\. Ideally, these numbers should be selected uniformly,
    so that if we draw a large quantity of such numbers, they should be distributed
    roughly evenly across the range 0 ≤ *x* < 1.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will see how to generate large quantities of random integers
    and floating-point numbers using NumPy, and show the distribution of these numbers
    using a histogram.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we start, we need to import the `default_rng` routine from the NumPy
    `random` module and create an instance of the default random number generator
    to use in the recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We have discussed this process in the *Selecting items at random* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: We also import the Matplotlib `pyplot` module under the alias `plt`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to generate uniform random data and plot a histogram
    to understand its distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To generate random floating-point numbers between 0 and 1, including 0 but
    not 1, we use the `random` method on the `rng` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'To generate random integers, we use the `integers` method on the `rng` object.
    This will return integers in the specified range:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'To examine the distribution of the random floating-point numbers, we first
    need to generate a large array of random numbers, just as we did in *Step 1*.
    While this is not strictly necessary, a larger sample will be able to show the
    distribution more clearly. We generate these numbers as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'To show the distribution of the numbers we have generated, we plot a *histogram*
    of the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting plot is shown in *Figure 4.1*. As we can see, the data is roughly
    evenly distributed across the whole range:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ec4fd1ac-8e09-462c-90f1-62d20e668fa6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.1: Histogram of randomly generated random numbers between 0 and 1'
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `Generator` interface provides three simple methods for generating basic
    random numbers, not including the `choice` method that we discussed in the *Selecting
    items at random* recipe. In addition to the `random` method, for generating random
    floating-point numbers, and the `integers` method, for generating random integers,
    there is also a `bytes` method for generating raw random bytes. Each of these
    methods calls a relevant method on the underlying `BitGenerator` instance. Each
    of these methods also enables the data type of the generated numbers to be changed,
    for example, from double to single precision floating-point numbers.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `integers` method on the `Generator` class combines the functionality of
    the `randint` and `random_integers` methods on the old `RandomState` interface
    through the addition of the `endpoint` optional argument. (In the old interface,
    the `randint` method excluded the upper end point, whereas the `random_integers`
    method included the upper end point.) All of the random data generating methods
    on `Generator` allow the data type of the data they generate to be customized,
    which was not possible using the old interface. (This interface was introduced
    in NumPy 1.17.)
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 4.1*, we can see that the histogram of the data that we generated
    is approximately uniform over the range 0 ≤ *x* < 1\. That is, all of the bars
    are approximately level. (They are not completely level due to the random nature
    of the data.) This is what we expect from uniformly distributed random numbers,
    such as those generated by the `random` method. We will explain distributions
    of random numbers in greater detail in the *Generating normally distributed random
    numbers* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Changing the random number generator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `random` module in NumPy provides several alternatives to the default PRNG,
    which uses a 128-bit permutation congruential generator. While this is a good
    general-purpose random number generator, it might not be sufficient for your particular
    needs. For example, this algorithm is very different from the one used in Python’s
    internal random number generator. We will follow the guidelines for best practice
    set out in the NumPy documentation for running repeatable, but suitably random,
    simulations.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will show you how to change to an alternative pseudo random
    number generator, and how to use seeds effectively in your programs.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As usual, we import NumPy under the alias `np`. Since we will be using multiple
    items from the `random` package, we import that module from NumPy, too, using
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: You will need to select one of the alternative random number generators that
    are provided by NumPy (or define your own; refer to the *There's more...* section
    in this recipe). For this recipe, we will use the MT19937 random number generator,
    which uses a Mersenne Twister-based algorithm like the one used in Python's internal
    random number generator.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following steps show how to generate seeds and different random number
    generators in a reproducible way:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will generate a `SeedSequence` object that can reproducibly generate new
    seeds from a given source of entropy. We can either provide our own entropy as
    an integer, very much like how we provide the seed to `default_rng`, or we can
    let Python gather entropy from the operating system. We will use the latter case
    here, to demonstrate its use. For this, we do not provide any additional arguments
    to create the `SeedSequence` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have a means to generate the seeds for random number generators
    for the rest of the session, we next log the entropy so that we can reproduce
    this session later, if necessary. The following is an example of what the entropy
    should look like; your results will inevitably differ somewhat:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can create the underlying `BitGenerator` instance that will provide
    the random numbers for the wrapping `Generator` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create the wrapping `Generator` object around this `BitGenerator`
    instance to create a usable random number generator:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As mentioned in the *Selecting items at random* recipe, the `Generator` class
    is a wrapper around an underlying `BitGenerator` that implements a given pseudo
    random number algorithm. NumPy provides several implementations of pseudo random
    number algorithms through the various subclasses of the `BitGenerator` class:
    `PCG64` (default); `MT19937` (as seen in this recipe); `Philox`; and `SFC64`.
    These bit generators are implemented in Cython.'
  prefs: []
  type: TYPE_NORMAL
- en: The `PCG64` generator should provide high-performance random number generation
    with good statistical quality. (This might not be the case on 32 bit systems.)
    The `MT19937` generator is slower than more modern PRNGs and does not produce
    random numbers with good statistical properties. However, this is the random number
    generator algorithm that is used by the Python Standard Library `random` module.
    The `Philox` generator is relatively slow, but produces random numbers of very
    high quality, and the `SFC64` generator is fast and of good quality, but lacks
    some features available in other generators.
  prefs: []
  type: TYPE_NORMAL
- en: The `SeedSequence` object created in this recipe is a means to create seeds
    for random number generators in an independent and reproducible manner. In particular,
    this is useful if you need to create independent random number generators for
    several parallel processes, but still need to be able to reconstruct each session
    later to debug or inspect results. The entropy stored on this object is a 128-bit
    integer that was gathered from the operating system, and serves as a source of
    random seeds.
  prefs: []
  type: TYPE_NORMAL
- en: The `SeedSequence` object allows us to create a separate random number generator
    for each process/thread that are independent of one another, which eliminates
    any data race problems that might make results unpredictable. It also generates
    seed values that are very different from one another, which can help avoid problems
    with some PRNGs (such as MT19937, which can produce very similar streams with
    two similar 32-bit integer seed values). Obviously having two independent random
    number generators producing the same or very similar values will be problematic
    when we are depending on the independence of these values.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `BitGenerator` class serves as a common interface for generators of raw
    random integers. The classes mentioned previously are those that are implemented
    in NumPy with the `BitGenerator` interface. You can also create your own `BitGenerator`
    subclasses, although this needs to be implemented in Cython.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the NumPy documentation at [https://numpy.org/devdocs/reference/random/extending.html#new-bit-generators](https://numpy.org/devdocs/reference/random/extending.html#new-bit-generators)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: Generating normally distributed random numbers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the *Generating random data* recipe, we generated random floating-point numbers
    following a uniform distribution between 0 and 1, but not including 1\. However,
    in most cases where we require random data, we need to instead follow one of several
    different **distributions**. Roughly speaking, a **distribution function** is
    a function *f*(*x*) that describes the probability that a random variable has
    a value that is below *x*. In practical terms, the distribution describes the
    spread of the random data over a range. In particular, if we create a histogram
    of data that follows a particular distribution, then it should roughly resemble
    the graph of the distribution function. This is best seen by example.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most common distributions is **normal distribution**, which appears
    frequently in statistics and forms the basis for many statistical methods that
    we will see in [Chapter 6](87b0f91d-3086-41a9-995d-27fe7d364e8b.xhtml), *Working
    with Data and Statistics*. In this recipe, we will demonstrate how to generate
    data following the normal distribution, and plot a histogram of this data to see
    the shape of the distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As in the *Generating random data* recipe, we import the `default_rng` routine
    from the NumPy `random` module and create a `Generator` instance with a seeded
    generator for demonstration purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: As usual, we have the Matplotlib `pyplot` module imported as `plt`, and NumPy
    imported as `np`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the following steps, we generate random data that follows a normal distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the `normal` method on our `Generator` instance to generate the random
    data according to the `normal` distribution. The normal distribution has two *parameters*,
    *location* and *scale.* There is also an optional `size` argument that specifies
    the shape of the generated data. (See the *Generating random data* recipe for
    more information on the `size` argument.) We generate an array of 10,000 values
    to get a reasonably sized sample:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we plot a histogram of this data. We have increased the number of `bins`
    in the histogram. This isn''t strictly necessary as the default number (10) is
    perfectly adequate, but it does show the distribution slightly better:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create a function that will generate the expected density for a range
    of values. This is given by multiplying the probability density function for normal
    distribution by the number of samples (10,000):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we plot our expected distribution over the histogram of our data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is shown in *Figure 4.2*. We can see here that the distribution
    of our sampled data closely follows the expected distribution from the normal
    distribution curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/9499ac0a-e1ba-40bd-a8a1-039e42ed3d24.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.2: Histogram of data drawn from a normal distribution centered at
    5 with a scale of 3, with the expected density overlaid'
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Normal distribution has a probability density function defined by the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/6198a9a0-658b-480f-a42a-abcb58c27969.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is related to the normal distribution function *F*(*x*) according to the
    following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/c351a1d8-481f-48c8-a1ba-6f8c1c5128ab.png)'
  prefs: []
  type: TYPE_IMG
- en: This probability density function peaks at the mean value, which coincides with
    the location parameter, and the width of the "bell shape" is determined by the
    scale parameter. We can see in *Figure 4.2* that the histogram of the data generated
    by the `normal` method on the `Generator` object fits the expected distribution
    very closely.
  prefs: []
  type: TYPE_NORMAL
- en: The `Generator` class uses a 256-step Ziggurat method to generate normally distributed
    random data, which is fast compared to the Box-Muller or inverse CDF implementations
    that are also available in NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The normal distribution is one example of a *continuous* probability distribution,
    in that it is defined for real numbers and the distribution function is defined
    by an integral (rather than a sum). An interesting feature of normal distribution
    (and other continuous probability distributions) is that the probability of selecting
    any given real number is 0\. This is reasonable, because it only makes sense to
    measure the probability that a value selected in this distribution lies within
    a given range. (It shouldn't make sense that the probability of selecting a specific
    value should be not zero.)
  prefs: []
  type: TYPE_NORMAL
- en: The normal distribution is important in statistics, mostly due to the *central
    limit theorem.* Roughly speaking, this theorem states that sums of **independent
    and identically distributed** (**IID**) random variables, with a common mean and
    variance, are eventually like normal distribution with the common mean and variance.
    This holds, regardless of the actual distribution of these random variables. This
    allows us to use statistical tests based on normal distribution in many cases
    even if the actual distribution of the variables is not necessarily normal. (We
    do, however, need to be extremely cautious when appealing to the central limit
    theorem.)
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many other continuous probability distributions aside from normal
    distribution. We have already encountered *uniform* distribution over the range
    0 to 1\. More generally, uniform distribution over the range *a* *≤ x**≤ b* has
    a probability density function given by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/b90f2536-cab6-4685-947a-581c0281f2d4.png)'
  prefs: []
  type: TYPE_IMG
- en: Other common examples of continuous probability density functions include *exponential*
    distribution, *beta* distribution, and *gamma* distribution*.* Each of these distributions
    has a corresponding method on the `Generator` class that generates random data
    from that distribution. These are typically named according to the name of the
    distribution, all in lowercase letters. So, for the aforementioned distributions,
    the corresponding methods are `exponential`, `beta`, and `gamma`. These distributions
    each have one or more *parameters*, like location and scale for normal distribution,
    that determine the final shape of the distribution. You may need to consult the
    NumPy documentation ([https://numpy.org/doc/1.18/reference/random/generator.html#numpy.random.Generator](https://numpy.org/doc/1.18/reference/random/generator.html#numpy.random.Generator))
    or other sources to see what parameters are required for each distribution. The
    NumPy documentation also lists the probability distributions from which random
    data can be generated.
  prefs: []
  type: TYPE_NORMAL
- en: Working with random processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Random processes exist everywhere. Roughly speaking, a random process is a system
    of related random variables, usually indexed with respect to time *t ≥ 0*, for
    a continuous random process, or by natural numbers *n = 1, 2, …*, for a discrete
    random process. Many (discrete) random processes satisfy the **Markov property**,
    which makes them a **Markov chain***.* The Markov property is the statement that
    the process is *memoryless*, in that only the current value is important for the
    probabilities of the next value.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we will examine a simple example of a random process that models
    the number of bus arrivals at a stop over time. This process is called a **Poisson
    process**. A Poisson process *N*(*t*) has a single parameter, *λ*, which is usually
    called the *intensity* or *rate*, and the probability that *N*(*t*) takes the
    value *n* at a given time *t* is given by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/106772d7-94ec-4fa7-b32a-d60d705df5cc.png)'
  prefs: []
  type: TYPE_IMG
- en: This equation describes the probability that *n* buses have arrived by time
    *t*. Mathematically, this equation means that *N*(*t*) has a Poisson distribution
    with the parameter *λt*. There is, however, an easy way to construct a Poisson
    process by taking sums of inter-arrival times that follow an exponential distribution.
    For instance, let *X[i]* be the time between the (*i-1*)-st*arrival and the *i*-th
    arrival, which are exponentially distributed with parameter *λ*. Now, we take
    the following equation:*
  prefs: []
  type: TYPE_NORMAL
- en: '*![](assets/8c051194-080f-40e5-82a4-cad043e6dcd6.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Here, the number *N(t)* is the maximum *n* such that *T_n <= t*. This is the
    construction that we will work through in this recipe. We will also estimate the
    intensity of the process by taking the mean of the inter-arrival times.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we start, we import the `default_rng` routine from NumPy''s `random`
    module and create a new random number generator with a seed for the purpose of
    demonstration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: In addition to the random number generator, we also import NumPy as `np` and
    the Matplotlib `pyplot` module as `plt`. We also need to have the SciPy package
    available.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following steps show how to model the arrival of buses using a Poisson
    process:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our first task is to create the sample inter-arrival times by sampling data
    from an exponential distribution. The `exponential` method on the NumPy `Generator`
    class requires a `scale` parameter, which is *1/λ*, where *λ* is the rate. We
    choose a rate of 4, and create 50 sample inter-arrival times:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we compute the actual arrival times by using the `accumulate` method
    of the NumPy `add` universal function. We also create an array containing the
    integers 0 to 49, representing the number of arrivals at each point:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we plot the arrivals over time using the `step` plotting method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is shown in *Figure 4.3*, where the length of each horizontal line
    represents the inter-arrival times:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/1ff070ea-f4f3-49fd-b260-be6acd827068.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.3: Arrivals over time, where inter-arrival times are exponentially
    distributed, which makes the number of arrivals at a time a Poisson process'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we define a function that will evaluate the probability distribution
    of the counts at a time, which we will take as `1` here. This uses the formula
    for the Poisson distribution that we gave in the introduction to this recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we plot the probability distribution over the count per unit of time,
    since we chose `time=1` in the previous step. We will add to this plot later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we move on to estimate the rate from our sample data. We do this by computing
    the mean of the inter-arrival times, which, for exponential distribution, is an
    estimator of the scale *1/λ*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we plot the probability distribution with this estimated rate for the
    counts per unit of time. We plot this on top of the true probability distribution
    that we produced in *Step 5:*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting plot is given in *Figure 4.4*, where we can see that, apart from
    a small discrepancy, the estimated distribution is very close to the true distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/59f7bc51-399f-4b12-93d9-6193c357c47d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.4: Poisson distribution of the number of arrivals per time unit, the
    true distribution, and the distribution estimated from the sampled data'
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A Poisson process is a counting process that counts the number of events (bus
    arrivals) that occur in an amount of time if the events are randomly spaced (in
    time) with an exponential distribution with a fixed parameter. We constructed
    the Poisson process by sampling inter-arrival times from exponential distribution,
    following the construction we described in the introduction. However, it turns
    out that this fact (that the inter-arrival times are exponentially distributed)
    is a property of all Poisson processes when they are given their formal definition
    in terms of probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we sampled 50 points from an exponential distribution with a
    given `rate` parameter. We had to do a small conversion because the NumPy `Generator`
    method for sampling from an exponential distribution uses a related `scale` parameter,
    which is `1` over the `rate`. Once we have these points, we create an array that
    contains cumulative sums of these exponentially distributed numbers. This creates
    our arrival times. The actual Poisson process is that displayed in *Figure 4.3*,
    and is a combination of the arrival times with the corresponding number of events
    that had occurred at that time.
  prefs: []
  type: TYPE_NORMAL
- en: The mean (expected value) of an exponential distribution coincides with the
    scale parameter, so the mean of a sample drawn from an exponential distribution
    is one way to estimate the scale (rate) parameter. This estimate will not be perfect,
    since our sample is relatively small. This is why there is a small discrepancy
    between the two plots in *Figure 4.4*.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are many types of random processes describing a wide variety of real-world
    scenarios. In this recipe, we modeled arrival times using a Poisson process. A
    Poisson process is a continuous random process, meaning that it is parameterized
    by a continuous variable, *t* ≥ 0, rather than a discrete variable, *n*=1,2,….
    Poisson processes are actually Markov chains, under a suitably generalized definition
    of a Markov chain, and also an example of a *renewal process*. A renewal process
    is a process that describes the number of events that occur within a period of
    time. The Poisson process described here is an example of a renewal process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many Markov chains also satisfy some properties in addition to their defining
    Markov property. For example, a Markov chain is *homogeneous* if the following
    equality holds for all *n*, *i*, and *j* values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/2d2f34e4-b2c4-4759-8708-732d78597133.png)'
  prefs: []
  type: TYPE_IMG
- en: In simple terms, this means that the probabilities of moving from one state
    to another over a single step does not change as we increase the number of steps.
    This is extremely useful for examining the long-term behavior of a Markov chain.
  prefs: []
  type: TYPE_NORMAL
- en: It is very easy to construct simple examples of homogeneous Markov chains. Suppose
    that we have two states, *A* and *B*. At any given step, we could be either at
    state *A* or at state *B.* We move between states according to a probability.
    For instance, let's say that the probability of transitioning from state *A*to
    state *A* is 0.4, and that theprobability of transitioning from *A*to *B*is 0.6.*Similarly,
    let's say that the probability of transitioning from *B* to *B*is 0.2, and transitioning
    from *B*to *A* is 0.8\. Notice that both the probability of switching plus the
    probability of staying the same sum to 1 in both cases. We can represent the probability
    of transitioning from each state in matrix form given, in this case, by the following
    equation:*
  prefs: []
  type: TYPE_NORMAL
- en: '*![](assets/7b1faeb6-1ed9-4eac-98c4-98e68a4d9bac.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'This matrix is called the *transition matrix*. The idea here is that the probability
    of being in a particular state after a step is given by multiplying the vector
    containing the probability of being in state *A* and *B* (position 0 and 1, respectively).
    For example, if we start in state *A* then the probability vector will contain
    a 1 at index 0 and 0 at index 1\. Then, the probability of being in state *A*
    after 1 step is given by 0.4, and the probability of being in state *B* is 0.6\.
    This is what we expect, given the probabilities we outlined previously. However,
    we could also write this calculation using the matrix formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/0c516c06-936a-46cc-a55c-ea87ff1cdfa9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To get the probability of being in either state after two steps, we multiply
    the right-hand side vector again by the transition matrix, *T*, to obtain the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ee656e40-e42a-4549-91ee-498b80325245.png)'
  prefs: []
  type: TYPE_IMG
- en: We can continue this process *ad infinitum* to obtain a sequence of state vectors,
    which constitute our Markov chain. This construction can be applied, with more
    states if necessary, to model many simple, real-world problems.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing conversion rates with Bayesian techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bayesian probability allows us to systematically update our understanding (in
    a probabilistic sense) of a situation by considering data. In more technical language,
    we update the *prior* distribution (our current understanding) using data to obtain
    a *posterior* distribution. This is particularly useful, for example, when examining
    the proportion of users who go on to buy a product after viewing a website. We
    start with our prior belief distribution. For this we will use the *beta* distribution,
    which models the probability of success given numbers of successes (completed
    purchases) against failures (no purchases). For this recipe, we will assume that
    our prior belief is that we expect 25 successes from 100 views (75 fails). This
    means that our prior belief follows a beta (25, 75) distribution. Let's say that
    we wish to calculate the probability that the true rate of success is at least
    33%.
  prefs: []
  type: TYPE_NORMAL
- en: Our method is roughly divided into three steps. We first need to understand
    our prior belief for the conversion rate, which we have decided follows a beta
    (25, 75) distribution. We compute the probability that the conversion rate is
    at least 33% by integrating (numerically) the probability density function for
    the prior distribution from 0.33 to 1\. The next step is to apply the Bayesian
    reasoning to update our prior belief with new information. Then, we can perform
    the same integration with the posterior belief to examine the probability that
    the conversion rate is at least 33% given this new information.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will see how to use Bayesian techniques to update a prior
    belief based on new information for our hypothetical website.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As usual, we will need the NumPy and Matplotlib packages imported as `np` and
    `plt`, respectively. We will also require the SciPy package, imported as `sp`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following steps show how to estimate and update conversion rate estimations
    using Bayesian reasoning:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to set up the prior distribution. For this we use the `beta`
    distribution object from the SciPy `stats` module, which has various methods for
    working with the beta distribution. We import the `beta` distribution from the
    `stats` module under the alias `beta_dist` and then create a convenience function
    for the probability density function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to compute the probability, under the prior belief distribution,
    that the success rate is at least 33%. To do this, we use the `quad` routine from
    the SciPy `integrate` module, which performs numerical integration of a function.
    We use this to integrate the probability density function for the beta distribution,
    imported in *Step 1*, with our prior parameters. We print the probability according
    to our prior distribution to the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, suppose we have received some information about successes and failures
    over a new period of time. For example, we observed 122 successes and 257 failures
    over this period. We create new variables to reflect these values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'To obtain the parameter values for the posterior distribution with a beta distribution,
    we simply add the observed successes and failures to the `prior_alpha` and `prior_beta`
    parameters, respectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we repeat our numerical integration to compute the probability that the
    success rate is now above 33% using the posterior distribution (with our new parameters
    computed earlier). Again, we print this probability in the terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see here that the new probability, given the updated posterior distribution,
    is 13% as opposed to the prior 3%. This is a significant difference, although
    we are still not confident that the conversion rate is above 33% given these values.
    Now, we plot the prior and posterior distribution to visualize this increase in
    probability. To start with, we create an array of values and evaluate our probability
    density function based on these values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we plot the two probability density functions computed in *Step 6*
    onto a new plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting plot is shown in *Figure 4.5*, where we can see that the posterior
    distribution is much more narrow and centered to the right of the prior:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/6f4a1bb6-f47b-4b3a-b93e-73c8dd97dfe9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.5: Prior and posterior distributions of a success rate following a
    beta distribution'
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bayesian techniques work by taking a prior belief (probability distribution)
    and using *Bayes' theorem* to combine the prior belief with the likelihood of
    our data given this prior belief to form a posterior belief. This is actually
    similar to how we might understand things in real life. For example, when you
    wake up on a given day, you might have the belief (from a forecast or otherwise)
    that there is a 40% chance of rain outside. Upon opening the blinds, you see that
    it is very cloudy outside, which might indicate that rain is more likely, so we
    update our belief according to this new data, to say a 70% chance of rain.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand how this works, we need to understand *conditional probability*.
    Conditional probability deals with the probability that one event will occur *given
    that* another event has already occurred. In symbols, the probability of event
    *A* given that event *B* has occurred is written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/5a3c0d70-ca6f-414e-bb4f-e1f05232bd59.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Bayes'' theorem is a powerful tool that can be written (symbolically) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/66cca2d1-4176-43e5-aa37-302a06dd800f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The probability *P*(*A*) represents our prior belief. The event *B* represents
    the data that we have gathered, so that *P*(*B* | *A*) is the likelihood that
    our data arose given our prior belief. The probability *P*(*B*) represents the
    probability that our data arose, and *P*(*A* | *B*) represents our posterior belief
    given the data. In practice, the probability *P*(*B*) can be difficult to calculate
    or otherwise estimate, so it is quite common to replace the strong equality above
    with a proportional version of Bayes'' theorem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ebdc2354-2595-40dc-b038-949ee0ca5fe0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the recipe, we assumed that our prior was beta distributed. The beta distribution
    has a probability density function given by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/a78a4ebb-bebe-49fe-89cb-908ef5f74e07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *Γ*(*α*) is the gamma function. The likelihood is binomially distributed,
    which has a probability density function given by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/8535466c-882c-4283-a838-58a01e26e23c.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *k* is the number of observations, and *j* is one of those that was successful.
    In the recipe, we observed *m = 122* successes and *n* = 257 failures, which gives
    *k = m + n = 379* and *j = m = 122*. To calculate the posterior distribution,
    we can use the fact that the beta distribution is a conjugate prior for the binomial
    distribution to see that the right-hand side of the proportional form of Bayes'
    theorem is beta distributed with parameters *α + m**and *β +* *n**.**This is what
    we used in the recipe. The fact that the beta distribution is a conjugate prior
    for binomial random variables makes them useful in Bayesian statistics.**
  prefs: []
  type: TYPE_NORMAL
- en: '**The method we demonstrated in this recipe is a rather basic example of using
    a Bayesian method, but it is still useful for updating our prior beliefs given
    new data in a systematic way.'
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bayesian methods can be used for a wide variety of tasks, making it a powerful
    tool. In this recipe, we used a Bayesian approach to model the success rate of
    a website based on our prior belief of how it performs and additional data gathered
    from users. This is a rather complex example since we modeled our prior belief
    on a beta distribution. Here is another example of using Bayes' theorem to examine
    two competing hypotheses using only simple probabilities (numbers between 0 and
    1).
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose you place your keys in the same place every day when you return home,
    but one morning you wake up to find that they are not in this place. After searching
    for a short time, you cannot find them and so conclude that they must have vanished
    from existence. Let''s call this hypothesis *H[1]*. Now, *H[1]* certainly explains
    the data, *D*, that you cannot find your keys, hence the likelihood *P*(*D* |
    *H[1]*) = 1\. (If your keys vanished from existence, then you could not possibly
    find them.) An alternative hypothesis is that you simply placed them somewhere
    else when you got home the night before. Let''s call this hypothesis *H[2]*. Now
    this hypothesis also explains the data, so *P*(*D* | *H[2]*) = 1, but in reality,
    *H[2]* is far more plausible than *H[1]*. Let''s say that the probability that
    your keys completely vanished from existence is 1 in 1 million – this is a huge
    overestimation, but we need to keep the numbers reasonable – while you estimate
    that the probability that you placed them elsewhere the night before is 1 in 100\.
    Computing the posterior probabilities, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/2fd79ac1-310f-49a9-b24b-a1c5643b1e75.png)'
  prefs: []
  type: TYPE_IMG
- en: This highlights the reality that it is 10,000 times more likely that you simply
    misplaced your keys as opposed to the fact that they simply vanished. Sure enough,
    you soon find your keys already in your pocket, because you had picked them up
    earlier that morning.
  prefs: []
  type: TYPE_NORMAL
- en: Estimating parameters with Monte Carlo simulations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Monte Carlo methods broadly describe techniques that use random sampling to
    solve problems. These techniques are especially powerful when the underlying problem
    involves some kind of uncertainty. The general method involves performing large
    numbers of simulations, each sampling different inputs according to a given probability
    distribution, and then aggregating the results to give a better approximation
    of the true solution than any individual sample solution.
  prefs: []
  type: TYPE_NORMAL
- en: '**Markov Chain Monte Carlo** (**MCMC**) is a specific kind of Monte Carlo simulation
    in which we construct a Markov chain of successively better approximations of
    the true distribution that we seek. This works by accepting or rejecting a proposed
    state, sampled at random, based on carefully selected *acceptance probabilities*
    at each stage, with the aim of constructing a Markov chain whose unique stationary
    distribution is precisely the unknown distribution that we wish to find.'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will use the PyMC3 package and MCMC methods to estimate the
    parameters of a simple model. The package will deal with most of the technical
    details of running simulations, so we don't need to go any further into the details
    of how the different MCMC algorithms actually work.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As usual, we import the NumPy package and Matplotlib `pyplot` module as `np`
    and `plt`, respectively. We also import and create a default random number generator,
    with a seed for the purpose of demonstration, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: We will also need a module from the SciPy package for this recipe as well as
    the PyMC3 package, which is a package for probabilistic programming.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to use Markov chain Monte Carlo simulations to
    estimate the parameters of a simple model using sample data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our first task is to create a function that represents the underlying structure
    that we wish to identify. In this case, we will be estimating the coefficients
    of a quadratic (a polynomial of degree 2). This function takes two arguments,
    which are the points in the range, which is fixed, and the variable parameters
    that we wish to estimate:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we set up the `true` parameters and a `size` parameter that will determine
    how many points are in the sample that we generate:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We generate the sample that we will use to estimate the parameters. This will
    consist of the underlying data, generated by the `underlying` function we defined
    in *Step 1*, plus some random noise that follows a normal distribution. We first
    generate a range of *x* values, which will stay constant throughout the recipe,
    and then use the `underlying` function and the `normal` method on our random number
    generator to generate the sample data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'It is a good idea to plot the sample data, with the underlying data overlaid,
    before we begin the analysis. We use the `scatter` plotting method to plot only
    the data points (without connecting lines), and then plot the underlying quadratic
    structure using a dashed line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is *Figure 4.6*, where we can see that the shape of the underlying
    model is still visible even with the noise, although the exact parameters of this
    model are no longer obvious:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/c8c9b89d-be0d-4861-b5ce-7bb06bef685d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.6: Sampled data with the underlying model overlaid'
  prefs: []
  type: TYPE_NORMAL
- en: 'We are ready to start our analysis, so we now import the PyMC3 package under
    the alias `pm` as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The basic object of PyMC3 programming is the `Model` class, which is usually
    created using the context manager interface. We also create our prior distributions
    for the parameters. In this case, we will assume that our prior parameters are
    normally distributed with a mean of 1 and a standard deviation of 1\. We need
    3 parameters, so we provide the `shape` argument. The `Normal` class creates random
    variables that will be used in the Monte Carlo simulations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We create a model for the underlying data, which can be done by passing the
    random variable, `param`, that we created in *Step 6* into the `underlying` function
    that we defined in *Step 1*. We also create a variable that handles our observations.
    For this we use the `Normal` class, since we know that our noise is normally distributed
    around the underlying data, `y`. We set a standard deviation of `2`, and pass
    our observed `sample` data into the `observed` keyword argument (this is also
    inside the `Model` context):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'To run the simulations, we need only call the `sample` routine inside the `Model`
    context. We pass the `cores` argument to speed up the calculations, but leave
    all of the other arguments at the default values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: These simulations should take a short time to execute.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we plot the posterior distributions that use the `plot_posterior` routine
    from PyMC3\. This routine takes the `trace` result from the sampling step that
    performed the simulations. We create our own figure and axes using the `plt.subplots`
    routine in advance, but this isn''t strictly necessary. We are using three subplots
    on a single figure, and we pass the `axs2`tuple of `Axes` to the plotting routing
    under the `ax` keyword argument:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting plot is shown in *Figure 4.7*, where you can see that each of
    these distributions is approximately normal, with a mean that is similar to the
    true parameter values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/f1707e92-3bf1-4e86-8ba7-42786c9002b8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.7: Posterior distributions of estimated parameters'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now retrieve the mean of each of the estimated parameters from the trace by
    using the `mean` method on the `params` item from the trace, which is simply a
    NumPy array. We pass the `axis=0` argument because we want the mean of each of
    the rows of the matrix of parameter estimates. We print these estimated parameters
    in the terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we use our estimated parameters to generate our estimated underlying
    data by passing the *x* values and the estimated parameters to the `underlying`
    function defined in *Step 1*. We then plot this estimated underlying data together
    with the true underlying data on the same axes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting plot is in *Figure 4.8*, where there is only a small difference
    between these two models on this range:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/e6a65709-18f5-4568-8c9f-42cb68ac02c1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.8: True model and estimated model plotted on the same axes. There
    is a small discrepancy between the estimated parameters and the true parameters'
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The interesting part of the code in this recipe can be found in the `Model`
    context manager. This object keeps track of the random variables, orchestrates
    the simulations, and keeps track of the state. The context manager gives us a
    convenient way to separate the probabilistic variables from the surrounding code.
  prefs: []
  type: TYPE_NORMAL
- en: We start by proposing a prior distribution for the distribution of the random
    variables representing our parameters, of which there are three. We proposed a
    normal distribution since we know that the parameters cannot stray too far from
    the value 1\. (We can tell this by looking at the plot that we generated in *Step
    4*, for example.) Using a normal distribution will give a higher probability to
    the values that are close to the current values. Next, we add the details relating
    to the observed data, which is used to calculate the acceptance probabilities
    that are used to either accept or reject a state. Finally, we start the sampler
    using the `sample` routine. This constructs the Markov chain and generates all
    of the step data.
  prefs: []
  type: TYPE_NORMAL
- en: The `sample` routine sets up the sampler based on the types of variables that
    will be simulated. Since the normal distribution is a continuous variable, the
    `sample` routine selected the **No U-turn sampler** (**NUTS**). This is a reasonable
    general-purpose sampler for continuous variables. A common alternative to NUTS
    is the Metropolis sampler, which is less reliable but faster than NUTS in some
    cases. The PyMC3 documentation recommends using NUTS whenever possible.
  prefs: []
  type: TYPE_NORMAL
- en: Once the sampling is complete, we plotted the posterior distribution of the
    trace (the states given by the Markov chain) to see the final shape of the approximations
    we generated. We can see here that all three of our random variables (parameters)
    are normally distributed around approximately the correct value.
  prefs: []
  type: TYPE_NORMAL
- en: Under the hood, PyMC3 uses Theano to speed up its calculations. This makes it
    possible for PyMC3 to perform computations on a **Graphics Processing Unit** (**GPU**)
    rather than on the **Central Processing Unit** (**CPU**) for a considerable boost
    to computation speed. Theano also supports the dynamic generation of C code to
    improve computation speeds further.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Monte Carlo method is very flexible, and the example we gave here is one
    particular case where it can be used. A more typical basic example of where the
    Monte Carlo method is applied is in estimating the value of integrals, commonly,
    Monte Carlo integration. A really interesting case of Monte Carlo integration
    is estimating the value of π ≈ 3.1415\. Let's briefly look at how this works.
  prefs: []
  type: TYPE_NORMAL
- en: First, we take the unit disk, whose radius is 1 and therefore has an area, π.
    We can enclose this disk inside a square with vertices at the points (1, 1), (-1,
    1), (1, -1), and (-1, -1). This square has an area 4, since the edge length is
    2\. Now we can generate random points uniformly over this square. When we do this,
    the probability that any one of these random points lies inside a given region
    is proportional to the area of that region. Thus, the area of a region can be
    estimated by multiplying the proportion of randomly generated points that lie
    within the region by the total area of the square. In particular, we can estimate
    the area of the disk by simply multiplying the number of randomly generate points
    that lie within the disk by 4, and dividing by the total number of points we generated.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can easily write a function in Python that performs this calculation, which
    might be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this function just once will give a reasonable approximation of π:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'We can improve the accuracy of our estimation by using more points, but we
    could also run this a number of times and average the results. Let''s run this
    simulation 100 times and average the results (we''ll use concurrent futures to
    parallelize this so that we can run larger numbers of samples if we want):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Running this code once prints the estimated value of π as 3.1415752, which is
    an even better estimate of the true value.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The PyMC3 package has many features that are documented by numerous examples
    ([https://docs.pymc.io/](https://docs.pymc.io/)). There is also another probabilistic
    programming library based on TensorFlow ([https://www.tensorflow.org/probability](https://www.tensorflow.org/probability)).
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A good, comprehensive reference for probability and random processes is the
    following book:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Grimmett, G. and Stirzaker, D. (2009). Probability and random processes*.
    3rd ed. Oxford: Oxford Univ. Press*.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An easy introduction to Bayes'' theorem and Bayesian statistics is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Kurt, W. (2019).Bayesian statistics the fun way*. San Francisco, CA: No Starch
    Press, Inc*.*****'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
