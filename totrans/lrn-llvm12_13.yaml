- en: '*Chapter 10*: JIT Compilation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The LLVM core libraries come with the **ExecutionEngine** component, which allows
    the compilation and execution of IR code in memory. Using this component, we can
    build **just in time** (**JIT**) compilers, which allow the direct execution of
    IR code. A JIT compiler works more like an interpreter, in the sense that no object
    code needs to be stored on secondary storage.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn about applications for JIT compilers, and how
    the LLVM JIT compiler works in principle. You will explore the LLVM dynamic compiler
    and interpreter, and you will also learn how to implement a JIT compiler tool
    on your own. You will also see how to make use of a JIT compiler as part of a
    static compiler, and the challenges associated with it.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting an overview of LLVM's JIT implementation and use cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using JIT compilation for direct execution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilizing a JIT compiler for code evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of the chapter, you will know how to develop a JIT compiler, either
    using a preconfigured class, or a customized version fitting your needs. You will
    also acquire the knowledge to make use of a JIT compiler inside a traditional
    static compiler.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code files for the chapter can be found at [https://github.com/PacktPublishing/Learn-LLVM-12/tree/master/Chapter10](https://github.com/PacktPublishing/Learn-LLVM-12/tree/master/Chapter10)
  prefs: []
  type: TYPE_NORMAL
- en: You can find the code in action videos at [https://bit.ly/3nllhED](https://bit.ly/3nllhED)
  prefs: []
  type: TYPE_NORMAL
- en: Getting an overview of LLVM's JIT implementation and use cases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have only looked at **ahead of time** (**AOT**) compilers. These
    compilers compile the whole application. Only once the compilation is finished
    can the application run. If the compilation is performed at the runtime of the
    application, then the compiler is a JIT compiler. A JIT compiler has interesting
    use cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Implementation of a virtual machine**: A programming language can be translated
    to byte code with an AOT compiler. At runtime, a JIT compiler is used to compile
    the byte code to machine code. The advantage of this approach is that the byte
    code is hardware-independent, and thanks to the JIT compiler, there is no performance
    penalty compared to an AOT compiler. Java and C# use this model today, but the
    idea is really old: the USCD Pascal compiler from 1977 already used a similar
    approach.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Expression evaluation**: A spreadsheet application can compile often-executed
    expressions with a JIT compiler. This can speed up the financial simulations,
    for example. The LLVM debugger LLDB uses the approach to evaluate a source expression
    at debug time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Database queries**: A database creates an execution plan from a database
    query. The execution plan describes the operations on tables and columns, which
    leads to the query answer when executed. A JIT compiler can be used to translate
    the execution plan into machine code, thereby speeding up the execution of the
    query.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The static compilation model of LLVM is not as far away from the JIT model as
    you may think. The LLVM static compiler, `llc`, compiles LLVM IR into machine
    code and saves the result as an object file on disk. If the object file is not
    stored on disk but in memory, would the code be executable? Not directly, because
    references to global functions and global data use relocations instead of absolute
    addresses.
  prefs: []
  type: TYPE_NORMAL
- en: Conceptually, a relocation describes how to calculate the address, for example,
    as an offset to a known address. If we resolve the relocations into addresses,
    like the linker and dynamic loader do, then we can execute the object code. Running
    the static compiler to compile IR code into an object file in memory, performing
    a link step on the in-memory object file, and then running the code gives us a
    JIT compiler. The JIT implementation in the LLVM core libraries is based on this
    idea.
  prefs: []
  type: TYPE_NORMAL
- en: 'During the development history of LLVM, there were several JIT implementations,
    with different feature sets. The latest JIT API is the **on request compilation**
    (**ORC**) engine. In case you were wondering about the acronym: it was the lead
    developer''s intention to invent yet another acronym based on Tolkien''s universe,
    after the **ELF** (**Executable and Linking Format**) and the **DWARF** (**Debugging
    Standard**) were already there.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The ORC engine builds on, and extends, the idea of using the static compiler
    and a dynamic linker on the in-memory object file. The implementation uses a *layered*
    approach. The two basic levels are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Compile layer
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Link layer
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On top of the compile layer can sit a layer providing support for *lazy compilation*.
    A **transformation layer** can be stacked on top or below the lazy compilation
    layer, allowing the developer to add arbitrary transformation, or simply be notified
    of certain events. This layered approach has the advantage that the JIT engine
    is *customizable for diverse requirements*. For example, a high-performance virtual
    machine may choose to compile everything upfront and make no use of the lazy compilation
    layer. Other virtual machines will emphasize start up time and responsiveness
    to the user, and achieve this with the help of the lazy compilation layer.
  prefs: []
  type: TYPE_NORMAL
- en: The older MCJIT engine is still available. The API is derived from an even older,
    already removed, JIT engine. Over time, the API became a bit bloated, and it lacks
    the flexibility of the ORC API. The goal is to remove this implementation, as
    the ORC engine now provides all the functionality of the MCJIT engine. New developments
    should use the ORC API.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we look at `lli`, the LLVM interpreter and dynamic compiler,
    before we dive into implementing a JIT compiler.
  prefs: []
  type: TYPE_NORMAL
- en: Using JIT compilation for direct execution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Running LLVM IR directly is the first idea that comes to mind when thinking
    about a JIT compiler. This is what the `lli` tool, the LLVM interpreter, and the
    dynamic compiler do. We will explore the `lli` tool in the next section, and subsequently
    implement a similar tool on our own.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the lli tool
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s try the `lli` tool with a very simple example. Store the following source
    as a `hello.ll` file. It is the equivalent of a C hello world application. It
    declares the prototype for the `printf()` function from the C library. The `hellostr`
    constant contains the message to be printed. Inside the `main()` function, a pointer
    to the first character of the message is calculated via the `getelementptr` instruction,
    and this value is passed to the `printf()` function. The application always returns
    `0`. The complete source code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This LLVM IR file is generic enough that it is valid for all platforms. We
    can directly execute the IR with the `lli` tool with the help of the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The interesting point here is how the `printf()` function is found. The IR code
    is compiled to machine code, and a lookup for the `printf` symbol is triggered.
    This symbol is not found in the IR, so the current process is searched for it.
    The `lli` tool dynamically links against the C library, and the symbol is found
    there.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, the `lli` tool does not link against libraries you created. To enable
    the use of such functions, the `lli` tool supports the loading of shared libraries
    and objects. The following C source just prints a friendly message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Stored in the `greetings.c` file, we use this to explore the loading of objects
    with the `lli` tool. Compile this source into a shared library. The `–fPIC` option
    instructs clang to generate position-independent code, which is required for shared
    libraries. With the `–shared` option given, the compiler creates the `greetings.so`
    shared library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We also compile the file into a `greetings.o` object file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We now have two files, the `greetings.so` shared library and the `greetings.o`
    object file, which we will load into the `lli` tool.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also need an LLVM IR file, which calls the `greetings()` function. For this,
    create the `main.ll` file, which contains a single call to the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'If you try to execute the IR as before, then the `lli` tool is not able to
    locate the greetings symbol and will simply crash:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The `greetings()` function is defined in an external file, and to fix the crash,
    we have to tell the `lli` tool which additional file needs to be loaded. In order
    to use the shared library, you have to use the `–load` option, which takes the
    path to the shared library as an argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: It is important to specify the path to the shared library, if the directory
    containing the shared library is not in the search path for the dynamic loader.
    If omitted, then the library will not be found.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, we can instruct the `lli` tool to load the object file with
    the `–extra-object` option:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Other supported options are `–extra-archive`, which loads an archive, and `–extra-module`,
    which loads another bitcode file. Both options require the path to the file as
    an argument.
  prefs: []
  type: TYPE_NORMAL
- en: You now know how you can use the `lli` tool to directly execute LLVM IR. In
    the next section, we will implement our own JIT tool.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing our own JIT compiler with LLJIT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `lli` tool is nothing more than a thin wrapper around LLVM APIs. In the
    first section, we learned that the ORC engine uses a layered approach. The `ExecutionSession`
    class represents a running JIT program. Besides other items, this class holds
    the used `JITDylib` instances. A `JITDylib` instance is a symbol table, which
    maps symbol names to addresses. For example, this can be the symbols defined in
    an LLVM IR file, or the symbols of a loaded shared library.
  prefs: []
  type: TYPE_NORMAL
- en: To execute LLVM IR, we do not need to create a JIT stack on our own. The utility
    `LLJIT` class provides this functionality. You can also make use of this class
    when migrating from the older MCJIT implementation. This class essentially provides
    the same functionality. We begin the implementation with the initialization of
    the JIT engine in the next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Initializing the JIT engine for compiling LLVM IR
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We first implement the function that sets up the JIT engine, compiles an LLVM
    IR module, and executes the `main()` function in this module. Later, we use this
    core functionality to build a small JIT tool. This is the `jitmain()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The function needs the LLVM module with the IR to execute. Also needed is the
    LLVM context class used for this module, because the context class holds important
    type information. The goal is to call the `main()` function, so we also pass the
    usual `argc` and `argv` parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the `LLJITBuilder` class to create an `LLJIT` instance. If an error
    occurs, then we return the error. A possible source for an error is that the platform
    does not yet support JIT compilation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we add the module to the main `JITDylib` instance. If configured, then
    JIT compilation utilizes multiple threads. Therefore, we need to wrap the module
    and the context in a `ThreadSafeModule` instance. If an error occurs, then we
    return the error:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Like the `lli` tool, we also support the symbols from the C library. The `DefinitionGenerator`
    class exposes symbols, and the `DynamicLibrarySearchGenerator` subclass exposes
    the names found in the shared library. The class provides two factory methods.
    The `Load()` method can be used to load a shared library, while the `GetForCurrentProcess()`
    method exposes the symbols of the current process. We use the latter function.
    The symbol names can have a prefix, depending on the platform. We retrieve the
    data layout and pass the prefix to the `GetForCurrentprocess()` function. The
    symbol names are then treated in the right way, and we do not need to care about
    it. As usual, we return from the function in case an error occurs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We then add the generator to the main `JITDylib` instance. In case a symbol
    needs to be looked up, the symbols from the loaded shared library are also searched:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we look up the `main` symbol. This symbol must be in the IR module given
    on the command line. The lookup triggers compilation of that IR module. If other
    symbols are referenced inside the IR module, then they are resolved using the
    generator added in the previous step. The result is of the `JITEvaluatedSymbol`
    class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We ask the returned JIT symbol for the address of the function. We cast this
    address to the prototype of the C `main()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can call the `main()` function in the IR module, and pass the `argc`
    and `argv` parameters, which the function expects. We ignore the return value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We report success following execution of the function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This demonstrates how easy it is to use JIT compilation. There is a bunch of
    other possibilities to expose names, besides exposing the symbols for the current
    process or from a shared library. The `StaticLibraryDefinitionGenerator` class
    exposes the symbols found in a static archive, and can be used in the same way
    as the `DynamicLibrarySearchGenerator` class. The `LLJIT` class also has an `addObjectFile()`
    method to expose the symbols of an object file. You can also provide your own
    `DefinitionGenerator` implementation if the existing implementations do not fit
    your needs. In the next subsection, you extend the implementation into a JIT compiler.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the JIT compiler utility
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `jitmain()` function is easily extended into a small tool, which we do
    next. The source is saved in a `JIT.cpp` file and is a simple JIT compiler:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We must include several header files. The `LLJIT.h` header defines the `LLJIT`
    class, and the core classes of the ORC API. We include the `IRReader.h` header
    because it defines a function to read LLVM IR files. The `CommandLine.h` header
    allows us to parse the command-line options in the LLVM style. Finally, the `InitLLVM.h`
    header is required for basic initialization of the tool, and the `TargetSelect.h`
    header for the initialization of the native target:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We add the `llvm` namespace to the current scope:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Our JIT tool expects exactly one input file on the command line, which we declare
    with the `cl::opt<>` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'To read the IR file, we call the `parseIRFile()` function. The file can be
    the textual IR representation, or a bitcode file. The function returns a pointer
    to the created module. Error handling is a bit different because a textual IR
    file can be parsed, which is not necessarily syntactical correct. The `SMDiagnostic`
    instance holds the error information in case of a syntax error. The error message
    is printed, and the application is exited:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The `jitmain()` function is placed here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we add the `main()` function, which initializes the tool and the native
    target, and parses the command line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, the LLVM context class is initialized:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we load the IR module named on the command line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we can call the `jitmain()` function. To handle errors, we use the `ExitOnError`
    utility class. This class prints an error message and exits the application when
    an error occurred. We also set a banner with the name of the application, which
    is printed before the error message:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'If the control flow reaches this point, then the IR was successfully executed.
    We return `0` to indicate success:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This is already the complete implementation! We only need to add the build description,
    which is the topic of the next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Adding the CMake build description
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In order to compile this source file, we also need to create a `CMakeLists.txt`
    file with the build description, saved besides the `JIT.cpp` file:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We set the minimal required CMake version to the number required by LLVM and
    give the project the name `jit`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The LLVM package needs to be loaded, and we add the directory of the CMake
    modules provided by LLVM to the search path. Then we include the `ChooseMSVCCRT`
    module, which makes sure that the same C runtime is used as by LLVM:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need to add the definitions and the include path from LLVM. The LLVM
    components used are mapped to the library names with a function call:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, we define the name of the executable, the source files to compile,
    and the library to link against:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'That is everything that is required for the JIT tool. Create and change into
    a build directory, and then run the following command to create and compile the
    application:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'This compiles the `JIT` tool. You can check the functionality with the `hello.ll`
    file from the beginning of the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Creating a JIT compiler is surprisingly easy!
  prefs: []
  type: TYPE_NORMAL
- en: The example used LLVM IR as input, but this is not a requirement. The `LLJIT`
    class uses the `IRCompileLayer` class, which is responsible for compiling IR to
    machine code. You can define your own layer, which accepts the input you need,
    for example, Java byte code.
  prefs: []
  type: TYPE_NORMAL
- en: Using the predefined LLJIT class is handy, but limits our flexibility. In the
    next section, we will look at how to implement a JIT compiler using the layers
    provided by the ORC API.
  prefs: []
  type: TYPE_NORMAL
- en: Building a JIT compiler class from scratch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using the layered approach of ORC, it is very easy to build a JIT compiler customized
    for the requirements. There is no one-size-fits-all JIT compiler, and the first
    section of this chapter gave some examples. Let's have a look at how to set up
    a JIT compiler.
  prefs: []
  type: TYPE_NORMAL
- en: The ORC API uses layers, which are stacked together. The lowest level is the
    object linking layer, represented by the `llvm::orc::RTDyldObjectLinkingLayer`
    class. It is responsible for linking in-memory objects and turning them into executable
    code. The memory required for this task is managed by an instance of the `MemoryManager`
    interface. There is a default implementation, but we can also use a custom version
    if we need to.
  prefs: []
  type: TYPE_NORMAL
- en: Above the object linking layer is the compile layer, which is responsible for
    creating an in-memory object file. The `llvm::orc::IRCompileLayer` class takes
    an IR module as input, and compiles it to an object file. The `IRCompileLayer`
    class is a subclass of the `IRLayer` class, which is a generic class for layer
    implementations accepting LLVM IR.
  prefs: []
  type: TYPE_NORMAL
- en: These two layers already form the core of a JIT compiler. They add an LLVM IR
    module as input, which is compiled and linked in-memory. To add more functionality,
    we can add more layers on top of these both. For example, the `CompileOnDemandLayer`
    class splits a module, so that only the requested functions are compiled. This
    can be used to implement lazy compilation. The `CompileOnDemandLayer` class is
    also a subclass of the `IRLayer` class. In a very generic way, the `IRTransformLayer`
    class, also a subclass of the `IRLayer` class, allows us to apply a transformation
    to the module.
  prefs: []
  type: TYPE_NORMAL
- en: Another important class is the `ExecutionSession` class. This class represents
    a running JIT program. Basically, this means that the class manages the `JITDylib`
    symbol tables, provides lookup functionality for symbols, and keeps track of the
    resource managers used.
  prefs: []
  type: TYPE_NORMAL
- en: 'The generic recipe for a JIT compiler is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize an instance of the `ExecutionSession` class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the layer, at least consisting of the `RTDyldObjectLinkingLayer`
    class and the `IRCompileLayer` class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the first `JITDylib` symbol table, usually with `main` or a similar name.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The usage is very similar to the `LLJIT` class from the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: Add an IR module to the symbol table.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Look up a symbol, the triggered compilation of the associated function, and
    possibly the whole module.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Execute the function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the next subsection, we will implement a JIT compiler class based on the
    generic recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a JIT compiler class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To keep the implementation of the JIT compiler class simple, we put everything
    into the `JIT.h` header file. The initialization of the class is a bit more complex.
    Due to the handling of possible errors, we need a factory method to create some
    objects upfront before we can call the constructor. The steps to create the class
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin by guarding the header file against multiple inclusion with the `JIT_H`
    preprocessor definition:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'A bunch of include files is required. Most of them provide a class with the
    same name as the header file. The `Core.h` header provides a couple of basic classes,
    including the `ExecutionSession` class. The `ExecutionUtils.h` header provides
    the `DynamicLibrarySearchGenerator` class to search libraries for symbols, which
    we already used in the *Implementing our own JIT compiler with LLJIT* section.
    The `CompileUtils.h` header provides the `ConcurrentIRCompiler` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Our new class is the `JIT` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The private data members reflect the ORC layers and a helper class. The `ExecutionSession`,
    `ObjectLinkingLayer`, `CompileLayer`, `OptIRLayer`, and `MainJITDylib` instances
    represent the running JIT program, the layers, and the symbol table, as already
    described. The `TargetProcessControl` instance is used for interaction with the
    JIT target process. This can be the same process, another process on the same
    machine, or a remote process on a different machine, possible with a different
    architecture. The `DataLayout` and `MangleAndInterner` classes are required to
    mangle the symbols names in the correct way. The symbol names are internalized,
    which means that all equal names have the same address. To check whether two symbol
    names are equal, it is then sufficient to compare the addresses, which is a very
    fast operation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The initialization is split into three parts. In C++, a constructor cannot return
    an error. The simple and recommended solution is to create a static factory method,
    which can do the error handling prior to constructing the object. The initialization
    of the layers is more complex, so we introduce factory methods for them, too.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the `create()` factory method, we first create a `SymbolStringPool` instance,
    which is used to implement string internalization and is shared by several classes.
    To take control of the current process, we create a `SelfTargetProcessControl`
    instance. If we want to target a different process, then we need to change this
    instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we construct a `JITTargetMachineBuilder` instance, for which we need to
    know the target triple of the JIT process. Next, we query the target machine builder
    for the data layout. This step can fail if the builder is not able to instantiate
    the target machine based on the triple provided, for example, because support
    for this target is not compiled into the LLVM libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we have handled all the calls that could potentially fail. We
    are now able to initialize the `ExecutionSession` instance. Finally, the constructor
    of the `JIT` class is called with all instantiated objects, and the result is
    returned to the caller:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The constructor of the `JIT` class moves the passed parameters to the private
    data members. The layer objects are constructed with a call to a static factory
    name with the `create` prefix. Each `layer` factory method requires a reference
    to the `ExecutionSession` instance, connecting the layer to the running JIT session.
    Except for the object linking layer, which is at the bottom of the layer stack,
    each layer also requires a reference to the previous layer, illustrating the stacking
    order:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'In the body of the constructor, we add the generator to search the current
    process for symbols. The `GetForCurrentProcess()` method is special, because the
    return value is wrapped in an `Expected<>` template, indicating that an `Error`
    object can also be returned. But we know that no error can occur – the current
    process will eventually run! Therefore, we unwrap the result with the `cantFail()`
    function, which terminates the application if an error occurred anyway:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'To create the object linking layer, we need to provide a memory manager. We
    stick here to the default `SectionMemoryManager` class, but we could also provide
    a different implementation if needed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'A slight complication exists for the COFF object file format, which is used
    on Windows. This file format does not allow functions to be marked as exported.
    This subsequently leads to failures in checks inside the object linking layer:
    the flags stored in the symbol are compared with the flags from IR, which leads
    to a mismatch because of the missing export marker. The solution is to override
    the flags only for this file format. This finishes construction of the object
    layer, and the object is returned to the caller:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'To initialize the compiler layer, an `IRCompiler` instance is needed. The `IRCompiler`
    instance is responsible for compiling an IR module into an object file. If our
    JIT compiler does not use threads, then we can use the `SimpleCompiler` class,
    which compiles the IR module using a given target machine. The `TargetMachine`
    class is not thread-safe, likewise the `SimpleCompiler` class, too. To support
    compilation with multiple threads, we use the `ConcurrentIRCompiler` class, which
    creates a new `TargetMachine` instance for each module to compile. This approach
    solves the problem with multiple threads:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead of compiling the IR module directly to machine code, we install a layer
    that optimizes the IR first. This is a deliberate design decision: We turn our
    JIT compiler into an optimizing JIT compiler, which produces faster code that
    takes longer to produce, meaning a delay for the user. We do not add lazy compilation,
    so entire modules are compiled when just a symbol is looked up. This can add up
    to a significant time before the user sees the code executing.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The `optimizeModule()` function is an example of a transformation on an IR
    module. The function gets the module to transform as parameter, and returns the
    transformed one. Because the JIT can potentially run with multiple threads, the
    IR module is wrapped in a `ThreadSafeModule` instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'To optimize the IR, we recall some information from [*Chapter 8*](B15647_08_ePub_RK.xhtml#_idTextAnchor126),
    *Optimizing IR*, in the *Adding an optimization pipeline to your compiler* section.
    We require a `PassBuilder` instance to create an optimization pipeline. First,
    we define a couple of analysis managers, and register them afterward at the pass
    builder. Then we populate a `ModulePassManager` instance with the default optimization
    pipeline for the `O2` level. This is again a design decision: the `O2` level produces
    fast machine code already, but does this faster still than the `O3` level. Afterward,
    we run the pipeline on the module. Finally, the optimized module is returned to
    the caller:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The client of the `JIT` class needs a way to add an IR module, which we provide
    with the `addIRModule()` function. Remember the layer stack we created: we must
    add the IR module to the top layer, otherwise we would accidently bypass some
    layers. This would be a programming error that is not easily spotted: if the `OptIRLayer`
    member is replaced by a `CompileLayer` member, then our `JIT` class still works,
    but not as an optimizing JIT because we have bypassed this layer. This is no cause
    for concern as regards this small implementation, but in a large JIT optimization,
    we would introduce a function to return the top-level layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Likewise, a client of our JIT class needs a way to look up a symbol. We delegate
    this to the `ExecutionSession` instance, passing in a reference to the main symbol
    table and the mangled and internalized name of the requested symbol:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Putting the JIT compiler together was quite easy. Initializing the class is
    a bit tricky, as it involves a factory method and a constructor call for the `JIT`
    class, and factory methods for each layer. This distribution is caused by limitations
    in C++, although the code itself is simple.
  prefs: []
  type: TYPE_NORMAL
- en: In the next subsection, we are using our new JIT compiler class to implement
    a command-line utility.
  prefs: []
  type: TYPE_NORMAL
- en: Using our new JIT compiler class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The interface of our new JIT compiler class resembles the `LLJIT` class used
    in the *Implementing our own JIT compiler with LLJIT* section. To test our new
    implementation, we copy the `LIT.cpp` class from the previous section and make
    the following changes:'
  prefs: []
  type: TYPE_NORMAL
- en: To be able to use our new class, we include the `JIT.h` header file. This replaces
    the `llvm/ExecutionEngine/Orc/LLJIT.h` header file, which is no longer required
    because we are no longer using the LLJIT class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Inside the `jitmain()` function, we replace the call to `orc::LLJITBuilder().create()`
    with a call to our new `JIT::create()` method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Again, in the `jitmain()` function, we remove the code to add the `DynamicLibrarySearchGenerator`
    class. Precisely this generator is integrated in the JIT class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is already everything that needs to be changed! We can compile and run
    the changed application as in the previous section, with the same result. Under
    the hood, the new class uses a fixed optimization level, so with sufficiently
    large modules, we can note the differences in startup and runtime.
  prefs: []
  type: TYPE_NORMAL
- en: Having a JIT compiler at hand can stimulate new ideas. In the next section,
    we will look at how we can use the JIT compiler as part of a static compiler to
    evaluate code at compile time.
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing a JIT compiler for code evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Compiler writers make a great effort to produce optimal code. A simple, yet
    effective, optimization is to replace an arithmetic operation on two constants
    by the result value of this operation. To be able to perform the computation,
    an interpreter for constant expressions is embedded. And to arrive at the same
    result, the interpreter has to implement the same rules as the generated machine
    code! Of course, this can be the source of subtle errors.
  prefs: []
  type: TYPE_NORMAL
- en: A different approach would be to compile the constant expression to IR using
    the same code generations methods, and then have JIT compile and execute the IR.
    This idea can even be taken a step further. In mathematics, a function always
    produces the same result for the same input. For functions in computer languages,
    this is not true. A good example is the `rand()` function, which returns a random
    value for each call. A function in computer languages, which has the same characteristic
    as a function in mathematics, is called a **pure function**. During the optimization
    of expressions, we could JIT-compile and execute pure functions, which only have
    constant parameters, and replace the call to the function with the result returned
    from JIT execution. Effectively, we move the execution of the function from runtime
    to compile time!
  prefs: []
  type: TYPE_NORMAL
- en: Think about cross-compilation
  prefs: []
  type: TYPE_NORMAL
- en: Using a JIT compiler as part of a static compiler is an interesting option.
    However, if the compiler were to support cross-compilation, then this approach
    should be well thought-out. The usual candidates causing trouble are floating-point
    types. The precision of the `long double` type in C often depends on the hardware
    and the operation system. Some systems use 128-bit floating points, while others
    only use 64-bit floating points. The 80-bit floating point type is only available
    on the x86 platform, and usually only used on Windows. Performing the same floating-point
    operation with different precision can result in huge differences. Evaluation
    through JIT compilation cannot be used in such cases.
  prefs: []
  type: TYPE_NORMAL
- en: It cannot easily be decided whether a function is pure. The common solution
    is to apply a heuristic. If a function does not read or write into heap memory,
    either through pointers or indirectly with the use of aggregate types, and only
    calls other pure functions, then it is a pure function. The developer can aid
    the compiler, and mark pure functions, for example, with a special keyword or
    symbol. In the semantic analysis phase, the compiler can then check for violations.
  prefs: []
  type: TYPE_NORMAL
- en: In the next subsection, we will take a closer look at the implications for language
    semantics when trying to JIT-execute a function at compile time.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying the language semantics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The difficult part is indeed to decide at the language semantics level which
    parts of the language are suitable for evaluation at compile time. Excluding access
    to heap memory is very restrictive. In general terms, it rules out string handling,
    for example. Using heap memory becomes problematic when the allocated memory survives
    the lifetime of the JIT-executed function. This is a program state, which can
    influence other results, and is therefore dangerous. On the other hand, if there
    are matched calls to `malloc()` and `free()` functions, then the memory is only
    used for internal calculation. In this case, the use of heap memory would be safe.
    But precisely this condition is not easy to proof.
  prefs: []
  type: TYPE_NORMAL
- en: At a similar level, an infinite loop inside the JIT-executed function can freeze
    the compiler. Alan Turing showed in 1936 that no machine can decide whether a
    function will produce a result or whether it is stuck in an endless loop. Some
    precautions must be taken to avoid this situation, for example, a runtime limit
    after which the JIT-executed function is terminated.
  prefs: []
  type: TYPE_NORMAL
- en: 'And last, the more that functionality is allowed, the more thoughts must be
    put into security, because the compiler now executes code written by someone else.
    Just imagine that this code downloads and runs files from the internet or tries
    to erase the hard disk: with too much state allowed for JIT-executed functions,
    we also need to think about such scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: The idea is not new. The D programming language has a feature called **compile-time
    function execution**. The reference compiler **dmd** implements this feature by
    interpretation of the functions at the AST level. The LLVM-based LDC compiler
    has an experimental feature to use the LLVM JIT engine for it. You can find out
    more about the language and the compilers at https://dlang.org/.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ignoring the semantic challenges, the implementation is not that difficult.
    In the *Building a JIT compiler class from scratch* section, we developed a JIT
    compiler with the `JIT` class. We feed an IR module in the class, and we can look
    up and execute a function from this module. Looking at the `tinylang` compiler
    implementation, we can clearly identify access to constants, because there is
    a `ConstantAccess` node in the AST. For example, there is code like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead of interpreting the operations in the expression to derive the value
    of the constant, we can do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a new IR module.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create an IR function in the module, returning a value of the expected type.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the existing `emitExpr()` function to create the IR for the expression and
    return the calculated value with the last instruction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: JIT-execute the function to calculate the value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is this worth implementing? LLVM performs constant propagation and function
    inlining as part of the optimization pipeline. A simple expression such as 4 +
    5 is already replaced during IR construction with the result. Small functions
    such as calculation of the greatest common divisor are inlined. If all parameters
    are constant values, then the inlined code gets replaced by the result of the
    calculation through constant propagation.
  prefs: []
  type: TYPE_NORMAL
- en: Based on this observation, an implementation of this approach is only useful
    if enough language features are available for execution at compile time. If this
    is the case, then it is fairly easily implemented using the given sketch.
  prefs: []
  type: TYPE_NORMAL
- en: Knowing how to utilize the JIT compiler component of LLVM enables you to use
    LLVM in whole new ways. Besides implementing a JIT compiler like the Java VM,
    the JIT compiler can also be embedded in other applications. This allows creative
    approaches, such as its use inside a static compiler, which you looked at in this
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to develop a JIT compiler. You began with possible
    applications of JIT compilers, and you explored `lli`, the LLVM dynamic compiler
    and interpreter. Using the predefined `LLJIT` class, you built a tool similar
    to `lli` on your own. To be able to take advantage of the layered structure of
    the ORC API, you implemented an optimizing `JIT` class. Having acquired all this
    knowledge, you explored the possibility of using a JIT compiler inside a static
    compiler, a feature from which some languages can benefit.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will examine how to add a backend for a new CPU architecture
    to LLVM.
  prefs: []
  type: TYPE_NORMAL
