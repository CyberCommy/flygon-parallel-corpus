- en: Interprocess Communication
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A complex application-programming model might include a number of processes,
    each implemented to handle a specific job, which contribute to the end functionality
    of the application as a whole. Depending on the objective, design, and environment
    in which such applications are hosted, processes involved might be related (parent-child,
    siblings) or unrelated. Often, such processes need various resources to communicate,
    share data, and synchronize their execution to achieve desired results. These
    are provided by the operating system's kernel as services called **interprocess
    communication** (**IPC**). We have already discussed the usage of signals as an
    IPC mechanism; in this chapter, we shall begin to explore various other resources
    available for process communication and data sharing.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Pipes and FIFOs as messaging resources
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SysV IPC resources
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: POSX IPC mechanisms
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipes and FIFOs
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Pipes form a basic unidirectional, self-synchronous means of communication
    between processes. As the name suggests, they have two ends: one where a process
    writes and the opposite end from where another process reads the data. Presumably
    what goes in first will be read out first in this kind of a setup. Pipes innately
    result in communication synchronization due to their limited capacity: if the
    writing process writes much faster than the reading process reads, the pipe’s
    capacity will fail to hold excess data and invariably block the writing process
    until the reader reads and frees up data. Similarly, if the reader reads data
    faster than the writer, it will be left with no data to read, thus being blocked
    until data becomes available.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 'Pipes can be used as a messaging resource for both cases of communication:
    between related processes and between unrelated processes. When applied between
    related processes, pipes are referred to as **unnamed pipes**, since they are
    not enumerated as files under the `rootfs` tree. An unnamed pipe can be allocated
    through the `pipe()` API.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: API invokes a corresponding system call, which allocates appropriate data structures
    and sets up pipe buffers. It maps a pair of file descriptors, one for reading
    on the pipe buffer and another for writing on the pipe buffer. These descriptors
    are returned to the caller. The caller process normally forks the child process,
    which inherits the pipe file descriptors that can be used for messaging.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code excerpt shows the pipe system call implementation:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Communication between unrelated processes requires the pipe file to be enumerated
    into **rootfs***.* Such pipes are often called **named pipes***,* and can be created
    either from the command line (`mkfifo`) or from a process using the `mkfifo` API*.*
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: A named pipe is created with the name specified and with appropriate permissions
    as specified by the mode argument. The `mknod` system call is invoked for creating
    a FIFO, which internally invokes VFS routines to set up the named pipe. Processes
    with access permissions can initiate operations on FIFOs through common VFS file
    APIs `open`, `read`, `write`, and `close`.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: pipefs
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Pipes and FIFOs are created and managed by a special filesystem called `pipefs`.
    It registers with VFS as a special filesystem. The following is a code excerpt
    from `fs/pipe.c`:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'It integrates pipe files into VFS by enumerating an `inode` instance representing
    each pipe; this allows applications to engage common file APIs `read` and `write`.
    The `inode` structure contains a union of pointers that are relevant for special
    files such as pipes and device files. For pipe file `inodes`, one of the pointers,
    `i_pipe`, is initialized to `pipefs`, defined as an instance of type `pipe_inode_info`:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`struct pipe_inode_info` contains all pipe-related metadata as defined by `pipefs`,
    which includes information of the pipe buffer and other important management data.
    This structure is defined in `<linux/pipe_fs_i.h>`:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The `bufs` pointer refers to the pipe buffer; each pipe is by default assigned
    a total buffer of 65,535 bytes (64k) arranged as a circular array of 16 pages.
    User processes can alter the total size of the pipe buffer via a `fcntl()` operation
    on the pipe descriptor. The default maximum limit for the pipe buffer is 1,048,576
    bytes, which can be changed by a privileged process via the `/proc/sys/fs/pipe-max-size`
    file interface. Following is a summarized table that describes the rest of the
    important elements:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '| **Name** | **Description** |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
- en: '| `mutex` | Exclusion lock protecting the pipe |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
- en: '| `wait` | Wait queue for readers and writers |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
- en: '| `nrbufs` | Count of non-empty pipe buffers for this pipe |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
- en: '| `curbuf` | Current pipe buffer |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
- en: '| `buffers` | Total number of buffers |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
- en: '| `readers` | Number of current readers |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
- en: '| `writers` | Number of current writers |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
- en: '| `files` | Number of struct file instances currently referring to this pipe
    |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
- en: '| `waiting_writers` | Number of writers currently blocked on the pipe |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
- en: '| `r_coutner` | Reader counter (relevant for FIFO) |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
- en: '| `w_counter` | Writer counter (relevant for FIFO) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
- en: '| `*fasync_readers` | Reader side fasync |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
- en: '| `*fasync_writers` | Writer side fasync |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
- en: '| `*bufs` | Pointer to circular array of pipe buffers |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
- en: '| `*user` | Pointer to the `user_struct` instance that represents the user
    who created this pipe |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
- en: 'Reference to each page of the pipe buffer is wrapped into a circular array
    of instances of *type* `struct pipe_buffer`. This structure is defined in `<linux/pipe_fs_i.h>`:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`*page` is a pointer to the page descriptor of the page buffer, and the `offset`
    and `len` fields contain the offset to the data contained in the page buffer and
    its length. `*ops` is a pointer to a structure of type `pipe_buf_operations`,
    which encapsulates pipe buffer operations implemented by `pipefs`. It also implements
    file operations that are bound to pipe and FIFO inodes:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](img/00039.jpeg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
- en: Message queues
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Message queues** are lists of message buffers through which an arbitrary
    number of processes can communicate. Unlike pipes, the writer does not have to
    wait for the reader to open the pipe and listen for data. Similar to a mailbox,
    writers can drop a fixed-length message wrapped in a buffer into the queue, which
    the reader can pick whenever it is ready. The message queue does not retain the
    message packet after it is picked by the reader, which means that each message
    packet is assured to be process persistent. Linux supports two distinct implementations
    of message queues: classic Unix SYSV message queues and contemporary POSIX message
    queues.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: System V message queues
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is the classic AT&T message queue implementation suitable for messaging
    between an arbitrary number of unrelated processes. Sender processes wrap each
    message into a packet containing message data and a message number. The message
    queue implementation does not define the meaning of the message number, and it
    is left to the application designers to define appropriate meanings for message
    numbers and program readers and writers to interpret the same. This mechanism
    provides flexibility for programmers to use message numbers as message IDs or
    receiver IDs. It enables reader processes to selectively read messages that match
    specific IDs. However, messages with the same ID are always read in FIFO order
    (first in, first out).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'Processes can create and open a SysV message queue with:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The `key` parameter is a unique constant that serves as a magic number to identify
    the message queue. All programs that are required to access this message queue
    will need to use the same magic number; this number is usually hard-coded into
    relevant processes at compile time. However, applications need to ensure that
    the key value is unique for each message queue, and there are alternate library
    functions available through which unique keys can be dynamically generated.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 'The unique key and `msgflag` parameter values, if set to `IPC_CREATE`, will
    cause a new message queue to be set up. Valid processes that have access to the
    queue can read or write messages into the queue using `msgsnd` and `msgrcv` routines
    (we will not discuss them in detail here; refer to Linux system programming manuals):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Data structures
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Each message queue is created by enumerating a set of data structures by the
    underlying SysV IPC subsystem. `struct msg_queue` is the core data structure,
    and an instance of this is enumerated for each message queue:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The `q_messages` field represents the head node of a double-linked circular
    list that contains all messages currently in the queue. Each message begins with
    a header followed by message data; each message can consume one of more pages
    depending on length of message data. The message header is always at the start
    of the first page and is represented by an instance of `struct msg_msg`:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The `m_list` field contains pointers to previous and next messages in the queue.
    The `*next` pointer refers to an instance of type `struct msg_msgseg`, which contains
    the address of the next page of message data. This pointer is relevant only when
    message data exceeds the first page. The second page frame starts with a descriptor
    `msg_msgseg`, which further contains a pointer to a subsequent page, and this
    order continues until the last page of the message data is reached:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](img/00040.jpeg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
- en: POSIX message queues
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: POSIX message queues implement priority-ordered messages. Each message written
    by a sender process is associated with an integer number which is interpreted
    as message priority; messages with a higher number are considered higher in priority.
    The message queue orders current messages as per priority and delivers them to
    the reader process in descending order (highest priority first). This implementation
    also supports a wider API interface with facilities of bounded wait send and receive
    operations and asynchronous message arrival notifications for receivers through
    signals or threads.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'This implementation provides a distinct API interface to `create`, `open`,
    `read`, `write`, and `destroy` message queues. Following is a summarized description
    of APIs (we will not discuss usage semantics here, refer to system programming
    manuals for more details):'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '| **API interface** | **Description** |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
- en: '| `mq_open()` | Create or open a POSIX message queue |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
- en: '| `mq_send()` | Write a message to the queue |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
- en: '| `mq_timedsend()` | Similar to `mq_send`, but with a timeout parameter for
    bounded operations |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
- en: '| `mq_receive()` | Fetch a message from the queue; this operation is possible
    on unbounded blocking calls |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
- en: '| `mq_timedreceive()` | Similar to `mq_receive()` but with a timeout parameter
    that limits possible blocking for bounded time |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
- en: '| `mq_close()` | Close a message queue |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
- en: '| `mq_unlink()` | Destroy message queue |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
- en: '| `mq_notify()` | Customize and set up message arrival notifications |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
- en: '| `mq_getattr()` | Get attributes associated with a message queue |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
- en: '| `mq_setattr()` | Set attributes specified on a message queue |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
- en: 'POSIX message queues are managed by a special filesystem called `mqueue`. Each
    message queue is identified by a filename. Metadata for each queue is described
    by an instance of struct `mqueue_inode_info`, which symbolizes the inode object
    associated with the message queue file in the `mqueue` filesystem:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The `*node_cache` pointer refers to the `posix_msg_tree_node` descriptor that
    contains the header to a linked list of message nodes, in which each message is
    represented by a descriptor of type `msg_msg`:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Shared memory
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unlike message queues, which offer a process-persistent messaging infrastructure,
    the shared memory service of IPC provides kernel-persistent memory that can be
    attached by an arbitrary number of processes that share common data. A shared
    memory infrastructure provides operation interfaces to allocate, attach, detach,
    and destroy shared memory regions. A process that needs access to shared data
    will *attach* or *map* a shared memory region into its address space; it can then
    access data in shared memory through the address returned by the mapping routine.
    This makes shared memory one of the fastest means of IPC since from a process's
    perspective it is akin to accessing local memory, which does not involve switch
    into kernel mode.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: System V shared memory
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linux supports legacy SysV shared memory implementation under the IPC subsystem.
    Similar to SysV message queues, each shared memory region is identified by a unique
    IPC identifier.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Operation interfaces
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The kernel provides distinct system call interfaces for initiating shared memory
    operations as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Allocating shared memory
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`shmget()` system call is invoked by a process to get an IPC identifier for
    a shared memory region; if the region does not exists, it creates one:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This function returns the identifier of the shared memory segment corresponding
    to the value contained in the *key* parameter. If other processes intend to use
    an existing segment, they can use the segment's *key* value when looking for its
    identifier. A new segment is however created if the *key* parameter is unique
    or has the value `IPC_PRIVATE`.`size` indicates the number of bytes that needs
    to be allocated, as segments are allocated as memory pages. The number of pages
    to be allocated is obtained by rounding off the *size* value to the nearest multiple
    of a page size.\
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: 'The `shmflg` flag specifies how the segment needs to be created. It can contain
    two values:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '`IPC_CREATE`: This indicates creating a new segment. If this flag is unused,
    the segment associated with the key value is found, and if the user has the access
    permissions, the segment''s identifier is returned.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IPC_EXCL`: This flag is always used with `IPC_CREAT`, to ensure that the call
    fails if the *key* value exists.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attaching a shared memory
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The shared memory region must be attached to its address space for a process
    to access it. `shmat()` is invoked to attach the shared memory to the address
    space of the calling process:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The segment indicated by `shmid` is attached by this function. `shmaddr` specifies
    a pointer indicating the location in the process''s address space where the segment
    is to be mapped. The third argument `shmflg` is a flag, which can be one of the
    following:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '`SHM_RND`: This is specified when `shmaddr` isn''t a NULL value, indicating
    the function to attach the segment at the address, computed by rounding off the
    `shmaddr` value to the nearest multiple of page size; otherwise, the user must
    take care that `shmaddr` be page-aligned so that the segment gets attached correctly.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SHM_RDONLY`: This is to specify that the segment will only be read if the
    user has the necessary read permissions. Otherwise, both read and write access
    for the segment is given (the process must have the respective permissions).'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SHM_REMAP`: This is a Linux-specific flag that indicates that any existing
    mapping at the address specified by `shmaddr` be replaced with the new mapping.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detaching shared memory
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Likewise, to detach the shared memory from the process address space, `shmdt()`
    is invoked. As IPC shared memory regions are persistent in the kernel, they continue
    to exist even after the processes detach:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The segment at the address specified by `shmaddr` is detached from the address
    space of the calling process.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Each of these interface operations invoke relevant system calls implemented
    in the `<ipc/shm.c>` source file.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Data structures
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Each shared memory segment is represented by a `struct shmid_kernel` descriptor.
    This structure contains all metadata relevant to the management of SysV shared
    memory:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'For reliability and ease of management, the kernel''s IPC subsystem manages
    shared memory segments through a special file system called `shmfs`*.* This filesystem
    is not mounted on to the rootfs tree; its operations are only accessible through
    SysV shared memory system calls. The `*shm_file` pointer refers to the `struct
    file` object of `shmfs` that represents a shared memory block. When a process
    initiates an attach operation, the underlying system call invokes `do_mmap()`
    to create relevant mapping into the caller''s address space (through `struct vm_area_struct`)
    and steps into the `*shmfs-*`defined `shm_mmap()` operation to map corresponding
    shared memory:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00041.jpeg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
- en: POSIX shared memory
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Linux kernel supports POSIX shared memory through a special filesystem called
    `tmpfs`*,* which is mounted on to `/dev/shm` of the `rootfs`*.* This implementation
    offers a distinct API which is consistent with the Unix file model, resulting
    in each shared memory allocation to be represented by a unique filename and inode.
    This interface is considered more flexible by application programmers since it
    allows standard POSIX file-mapping routines `mmap()` and `unmap()` for attaching
    and detaching memory segments into the caller process address space.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is a summarized description of interface routines:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '| **API** | **Description** |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
- en: '| `shm_open()` | Create and open a shared memory segment identified by a filename
    |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
- en: '| `mmap()` | POSIX standard file mapping interface for attaching shared memory
    to caller''s address space |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
- en: '| `sh_unlink()` | Destroy specified shared memory block |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
- en: '| `unmap()` | Detach specified shared memory map from caller address space
    |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
- en: The underlying implementation is similar to that of SysV shared memory with
    the difference that the mapping implementation is handled by the `tmpfs` filesystem.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Although shared memory is the easiest way of sharing common data or resources,
    it dumps the burden of implementing synchronization on the processes, as a shared
    memory infrastructure does not provide any synchronization or protection mechanism
    for the data or resources in the shared memory region. An application designer
    must consider synchronization of shared memory access between contending processes
    to ensure reliability and validity of shared data, for instance, preventing a
    possible write by two processes on the same region at a time, restricting a reading
    process to wait until a write is completed by another process, and so on. Often,
    to synchronize such race conditions another IPC resource called semaphores is
    used.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Semaphores
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Semaphores** are synchronization primitives provided by the IPC subsystem.
    They deliver a protective mechanism for shared data structures or resources against
    concurrent access by processes in a multithreaded environment. At its core, each
    semaphore is composed of an integer counter that can be atomically accessed by
    a caller process. Semaphore implementations provide two operations, one for waiting
    on a semaphore variable and another to signal the semaphore variable. In other
    words, waiting on the semaphore decreases the counter by 1 and signaling the semaphore
    increases the counter by 1\. Typically, when a process wants to access a shared
    resource, it tries to decrease the semaphore counter. This attempt is however
    handled by the kernel as it blocks the attempting process until the counter yields
    a positive value. Similarly, when a process relinquishes the resource, it increases
    the semaphore counter, which wakes up any process that is waiting for the resource.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '**Semaphore versions**'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally all `*nix` systems implement the System V semaphore mechanism;
    however, POSIX has its own implementation of semaphores aiming at portability
    and leveling a few clumsy issues which the System V version carries. Let’s begin
    by looking at System V semaphores.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: System V semaphores
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Semaphores in System V are not just a single counter as you might think, but
    rather a set of counters. This implies that a semaphore set can contain single
    or multiple counters (0 to n) with an identical semaphore ID. Each counter in
    the set can protect a shared resource, and a single semaphore set can protect
    multiple resources. The system call that helps create this kind of semaphore is
    as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '`key` is used to identify the semaphore. If the key value is `IPC_PRIVATE`,
    a new set of semaphores is created.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nsems` indicates the semaphore set with the number of counters needed in the
    set'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`semflg` dictates how the semaphore should be created. It can contain two values:'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IPC_CREATE:` If the key does not exist, it creates a new semaphore'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IPC_EXCL`: If the key exists, it throws an error and fails'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On success, the call returns the semaphore set identifier (a positive value).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'A semaphore thus created contains uninitialized values and requires the initialization
    to be carried out using the `semctl()` function. After initialization, the semaphore
    set can be used by the processes:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The `Semop()` function lets the process initiate operations on the semaphore
    set. This function offers a facility unique to the SysV semaphore implementation
    called **undoable operations** through a special flag called `SEM_UNDO`. When
    this flag is set, the kernel allows a semaphore to be restored to a consistent
    state if a process aborts before completing the relevant shared data access operation.
    For instance, consider a case where one of the processes locks the semaphore and
    begins its access operations on shared data; during this time if the process aborts
    before completion of shared data access, the semaphore will be left in an inconsistent
    state, making it unavailable for other contending processes. However, if the process
    had acquired a lock on the semaphore by setting the `SEM_UNDO` flag with `semop()`,
    its termination would allow the kernel to revert the semaphore to a consistent
    state (unlocked state) making it available for other contending processes in wait.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Data structures
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Each SysV semaphore set is represented in the kernel by a descriptor of type
    `struct sem_array`:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Each semaphore in the array is enumerated as an instance of `struct sem` defined
    in `<ipc/sem.c>`; the `*sem_base` pointer refers to the first semaphore object
    in the set. ;Each semaphore set contains a list of pending queue per process waiting;
    `pending_alter` is the head node for this pending queue of type `struct sem_queue`.
    Each semaphore set also contains per-semaphore undoable operations. `list_id`
    is a head node to a list of `struct sem_undo` instances; there is one instance
    in the list for each semaphore in the set. The following diagram sums up the semaphore
    set data structure and its lists:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00042.jpeg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
- en: POSIX semaphores
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: POSIX semaphore semantics are rather simple when compared to System V. Each
    semaphore is a simple counter that can never be less than zero. The implementation
    provides function interfaces for initialization, increment, and decrement operations.
    They can be used for synchronizing threads by allocating the semaphore instance
    in memory accessible to all the threads. They can also be used for synchronizing
    processes by placing the semaphore in shared memory. Linux implementation of POSIX
    semaphores is optimized to deliver better performance for non-contending synchronization
    scenarios.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 'POSIX semaphores are available in two variants: named semaphores and unnamed
    semaphores. A named semaphore is identified by a filename and is suitable for
    use between unrelated processes. An unnamed semaphore is just a global instance
    of type `sem_t`; this form is generally preferred for use between threads. POSIX
    semaphore interface operations are part of the POSIX threads library implementation.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: POSIX信号量有两种变体：命名信号量和无名信号量。命名信号量由文件名标识，适用于不相关进程之间的使用。无名信号量只是`sem_t`类型的全局实例；一般情况下，这种形式更适合在线程之间使用。POSIX信号量接口操作是POSIX线程库实现的一部分。
- en: '| **Function interfaces** | **Description** |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '**函数接口** | **描述**'
- en: '| `sem_open()` | Opens an existing named semaphore file or creates a new named
    semaphore and returns its descriptor |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '`sem_open()` | 打开现有的命名信号量文件或创建一个新的命名信号量并返回其描述符'
- en: '| `sem_init()` | Initializer routine for an unnamed semaphore |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '`sem_init()` | 无名信号量的初始化程序'
- en: '| `sem_post()` | Operation to increment semaphore |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '`sem_post()` | 增加信号量的操作'
- en: '| `sem_wait()` | Operation to decrement semaphore, blocks if invoked when semaphore
    value is zero |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '`sem_wait()` | 减少信号量的操作，如果在信号量值为零时调用，则会阻塞'
- en: '| `sem_timedwait()` | Extends `sem_wait()` with a timeout parameter for bounded
    wait |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '`sem_timedwait()` | 用有界等待的超时参数扩展`sem_wait()`'
- en: '| `sem_getvalue()` | Returns the current value of the semaphore counter |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '`sem_getvalue()` | 返回信号量计数器的当前值'
- en: '| `sem_unlink()` | Removes a named semaphore identified with a file |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '`sem_unlink()` | 通过文件标识符移除命名信号量'
- en: Summary
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we touched on various IPC mechanisms offered by the kernel.
    We explored the layout and relationship between various data structures for each
    mechanism, and also looked at both SysV and POSIX IPC mechanisms.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涉及了内核提供的各种IPC机制。我们探讨了每种机制的各种数据结构的布局和关系，并且还研究了SysV和POSIX IPC机制。
- en: In the next chapter, we will take this discussion further into locking and kernel-synchronization
    mechanisms.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将进一步讨论锁定和内核同步机制。
