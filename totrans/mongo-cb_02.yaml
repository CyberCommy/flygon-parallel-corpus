- en: Chapter 2. Command-line Operations and Indexes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating test data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing simple querying, projections, and pagination from the Mongo shell
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Updating and deleting data from the shell
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating an index and viewing plans of queries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a background and foreground index in the shell
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating and understanding sparse indexes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Expiring documents after a fixed interval using the TTL index
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Expiring documents at a given time using the TTL index
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we will be performing simple queries using the mongo shell.
    Later in the chapter, we will have a detailed look at commonly used MongoDB indexes.
  prefs: []
  type: TYPE_NORMAL
- en: Creating test data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe is about creating test data for some of the recipes in this chapter
    and also for the later chapters in this book. We will demonstrate how to load
    a CSV file in a mongo database using the mongo import utility. This is a basic
    recipe, and if the reader is aware of the data import utility; they can just download
    the CSV file from the Packt website (`pincodes.csv`), load it in the collection
    by themselves, and skip the rest of the recipe. We will use the default database,
    `test`, and the collection will be named `postalCodes`.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The data used here is for postcodes in India. Download the `pincodes.csv` file
    from the Packt website. The file is a CSV file with 39,732 records; it should
    create 39,732 documents on successful import. We need to have the Mongo server
    up and running. Refer to the *Installing single node MongoDB* recipe from [Chapter
    1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing and
    Starting the Server* for instructions on how to start the server. The server should
    begin listening for connections on the default port, `27017`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Execute the following command from the shell with the file to be imported in
    the current directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Start the mongo shell by typing in `mongo` on the command prompt.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the shell, execute the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Assuming that the server is up and running, the CSV file has been downloaded
    and is kept in a local directory where we execute the import utility with the
    file in the current directory. Let''s look at the options given in the `mongoimport`
    utility and their meanings:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Command-line option | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `--type` | This specifies that the type of the input file is CSV. It defaults
    to JSON; another possible value being TSV. |'
  prefs: []
  type: TYPE_TB
- en: '| `-d` | This is the target database in which the data will be loaded. |'
  prefs: []
  type: TYPE_TB
- en: '| `-c` | This is the collection in the previously mentioned database in which
    the data will be loaded. |'
  prefs: []
  type: TYPE_TB
- en: '| `--headerline` | This is relevant only in case of TSV or CSV files. It indicates
    that the first line of the file is the header. The same names would be used as
    the name of the fields in the document. |'
  prefs: []
  type: TYPE_TB
- en: '| `--drop` | Drop the collection before importing data. |'
  prefs: []
  type: TYPE_TB
- en: The final value on the command prompt after all the options are given is the
    name of the file, `pincodes.csv`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the import goes through successfully, you should see something similar to
    the following printed to the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we start the mongo shell and find the count of the documents in the
    collection; it should indeed be 39,732 as seen in the preceding import log.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The postal code data has been taken from [https://github.com/kishorek/India-Codes/](https://github.com/kishorek/India-Codes/).
    This data is not taken from an official source and might not be accurate as it
    is being compiled manually for free public use.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Performing simple querying, projections, and pagination from Mongo shell*
    recipe is about executing some basic queries on the data imported.
  prefs: []
  type: TYPE_NORMAL
- en: Performing simple querying, projections, and pagination from Mongo shell
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will get our hands dirty with a bit of querying to select
    documents from the test data that we set up in our previous recipe, *Creating
    test data*. There is nothing extravagant in this recipe and someone who is well
    versed with the query language basics can skip this recipe. Others who aren't
    too comfortable with basic querying or those who want to get a small refresher
    can continue to read the next section of the recipe. Additionally, this recipe
    is intended to get a feel of the test data setup from the previous recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To execute simple queries, we need to have a server up and running. A simple
    single node is what we need. Refer to the *Installing single node MongoDB* recipe
    from [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing
    and Starting the Server* for instructions on how to start the server. The data
    that we would be operating on needs to be imported in the database. The steps
    to import the data are given in the previous recipe, *Creating test data*. You
    also need to start the mongo shell and connect to the server running on the localhost.
    Once these prerequisites are complete, we are good to go.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s first find a count of the documents in the collection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s find just one document from the `postalCodes` collection as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we find multiple documents in the collection as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding query retrieves all the keys of the first 20 documents and displays
    them on the shell. At the end of the result, you will notice a line that says
    `Type "it" for more`. By typing `"it"`, the mongo shell will iterate over the
    resulting cursor. Let''s do a couple of things now; we will just display the `city`,
    `state`, and `pincode` fields. Additionally, we want to display the documents
    numbered 91 to 100 in the collection. Let''s see how we do this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s move a step ahead and write a slightly complex query where we find the
    top 10 cities in the state of Gujarat sorted by the name of the city, and, similar
    to the last query, we just select `city`, `state`, and the `pincode` field:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe is pretty simple and allows us to get a feel for the test data that
    we set up in the previous recipe. Nevertheless, as with other recipes, I do owe
    you all some explanation for what we did here.
  prefs: []
  type: TYPE_NORMAL
- en: We first found the count of the documents in the collection using `db.postalCodes.count()`
    and it should give us 39,732 documents. This should be in sync with the logs that
    we saw while importing the data in the postal codes collection. We next queried
    for one document from the collection using `findOne`. This method returns the
    first document in the result set of the query. In absence of a query or sort order,
    as in this case, it will be the first document in the collection sorted by its
    natural order.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we perform `find` rather than `findOne`. The difference between both of
    them is that the `find` operation returns an iterator for the result set, which
    we can use to traverse through the results of the find operation, whereas `findOne`
    returns a document. Adding a pretty method call to the `find` operation will print
    the result in a pretty or formatted way.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that the `pretty` method makes sense and works only with `find` and not
    with `findOne`. This is because the return value of `findOne` is a document and
    there is no `pretty` operation on the returned document.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now execute the following query on the mongo shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we pass two parameters to the `find` method:'
  prefs: []
  type: TYPE_NORMAL
- en: The first one is `{}`, which is the query to select the documents, and, in this
    case, we ask mongo to select all the documents.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second parameter is the set of fields that we want in the result documents
    also known as **projection**. Remember that the `_id` field is present by default
    unless we explicitly say `_id:0`. For all the other fields, we need to say `<field_name>:1`
    or `<field_name>:true`. The find portion with projections is the same as saying
    `select field1``, field2 from table` in a relational world, and not specifying
    the fields to be selected in the find is saying `select * from table` in a relational
    world.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Moving on, we just need to look at what `skip` and `limit` do:'
  prefs: []
  type: TYPE_NORMAL
- en: The `skip` function skips the given number of documents from the result set
    all the way up to the end document
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `limit` function then limits the result to the given number of documents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's see what this all means with an example. By doing .`skip(90).limit(10)`,
    we say that we want to skip the first `90` documents from the result set and start
    returning from the 91st document. The limit, however, says that we will be returning
    only `10` documents from the 91st document.
  prefs: []
  type: TYPE_NORMAL
- en: Now, there are some border conditions that we need to know here. What if skip
    is being provided with a value more than the total number of documents in the
    collection? Well, in this case, no documents will be returned. Additionally, if
    the number provided to the limit function is more than the actual number of documents
    remaining in the collection, then the number of documents returned will be the
    same as the remaining documents in the collection and no exception will be thrown
    in either cases.
  prefs: []
  type: TYPE_NORMAL
- en: Updating and deleting data from the shell
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This again will be a simple recipe that will be looking at executing deletes
    and updates on a test collection. We won't be dealing with the same test data
    that we imported as we don't want to update/delete any of that, but instead, we
    will work on a test collection created for this recipe only.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this recipe, we will create a collection called `updAndDelTest`. We will
    require the server to be up and running. Refer to the *Installing single node
    MongoDB* recipe from [Chapter 1](ch01.html "Chapter 1. Installing and Starting
    the Server"), *Installing and Starting the Server* for instructions on how to
    start the server. Start the shell with the `UpdAndDelTest.js` script loaded. This
    script will be available on the Packt website for download. To know how to start
    the shell with a script preloaded, refer to the *Connecting to a single node in
    the Mongo shell with JavaScript* recipe in [Chapter 1](ch01.html "Chapter 1. Installing
    and Starting the Server"), *Installing and Starting the Server*.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start the MongoDB shell and preload the script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'With the shell started and script loaded, execute the following in the shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'If all goes well, you should see `Inserted 20 documents in updAndDelTest` printed
    to the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To get a feel of the collection, let''s query it as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We should see that for each value of `x` as `1` and `2`, we have `y` incrementing
    from 1 to 10 for each value of `x`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will first update some documents and observe the results. Execute the following
    update:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Execute the following `find` command and observe the results; we should get
    10 documents. For each of them, note the value of `y`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We shall now execute the following update:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Executing the query given in step 6 again to view the updated documents. It
    will show the same documents that we saw earlier. Take a note of the values of
    `y` again and compare them to the results that we saw when we executed this query
    last time before executing the update given in step 7.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will now see how delete works. We will again choose the documents where
    `x` is `1` for the deletion test. Let''s delete all the documents where `x` is
    `1` from the collection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Execute the following `find` command and observe the results. We will not get
    any results. It seems that the `remove` operation has removed all the documents
    with `x` as `1`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When you are in the mongo shell and you want to see the source code of a function,
    simply type in the function name without the parenthesis. For example, in this
    recipe, we can view the code of our custom function by typing the function name,
    `prepareTestData`, without the parenthesis, and press *Enter*.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we set up the data that we will use for the updating and deleting `test`.
    We have already seen the data and know what it is. An interesting thing to observe
    is that when we execute an update such as `db.updAndDelTest.update({x:1}, {$set:{y:0}})`,
    it only updates the first document that matches the query provided as the first
    parameter. This is something we will observe when we query the collection after
    this update. The update function has the following format `db.<collection name>.update(query,
    update object, {upsert: <boolean>, multi:<boolean>})`.'
  prefs: []
  type: TYPE_NORMAL
- en: We will see what upsert is in the later recipes. The multi parameter is set
    to `false` by default. This means that multiple documents will not be updated
    by the `update` method; only the first matching document will be updated. However,
    when we do `db.updAndDelTest.update({x:1}, {$set:{y:0}}, {multi:true})` with multi
    set to `true`, all the documents in the collection that match the given query
    are updated. This is something that we can verify after querying the collection.
  prefs: []
  type: TYPE_NORMAL
- en: Removals, on the other hand, behave differently. By default, the `remove` operation
    deletes all the documents that match the provided query. However, if we want to
    delete only one document, we explicitly pass the second parameter as `true`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The default behavior of update and remove is different. An `update` call, by
    default, updates only the *first* matching document, whereas `remove` deletes
    *all* the documents matching the query.
  prefs: []
  type: TYPE_NORMAL
- en: Creating index and viewing plans of queries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will look at querying the data, analyzing its performance
    by explaining the query plan, and then optimizing it by creating indexes.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the creation of indexes, we need to have a server up and running. A simple
    single node is what we need. Refer to the *Installing single node MongoDB* recipe
    from [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing
    and Starting the Server* for instructions on how to start the server. The data
    that we will operate on needs to be imported in the database. The steps to import
    the data are given in the previous recipe, *Creating test data*. Once this prerequisite
    is complete, we are good to go.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are trying to write a query that would find us all the zip codes in a given
    state.
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the following query to view the plan of this query:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a note of the following fields: `stage`, `nReturned`, `totalDocsExamined`,
    `docsExamined`, and `executionTimeMillis` in the result of the explain plan operation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s again execute the same query, but this time, we limit the results to
    100 results only:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a note of the following fields: `nReturned`, `totalDocsExamined`, `docsExamined`,
    and `executionTimeMillis` in the result.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We now create an index on the `state` and `pincode` fields as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following query:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a note of the following fields: `stage`, `nReturned`, `totalDocsExamined`,
    `docsExamined`, and `executionTimeMillis` in the result.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we want the pincodes only, we modify the query as follows and view its plan:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a note of the following fields: `stage`, `nReturned`, `totalDocsExamined`,
    `docsExamined`, and `executionTimeMillis` in the result.'
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is a lot to explain here. We will first discuss what we just did and how
    to analyze the stats. Next, we will discuss some points to be kept in mind for
    the index creation and some caveats.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the plan
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Okay, let''s look at the first step and analyze the output that we executed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The output on my machine is as follows: ( I am skipping the nonrelevant fields
    for now.)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The value of the `stage` field in the result is `COLLSCAN`, which means that
    a full collection scan (all the documents scanned one after another) has happened
    in order to search the matching documents in the entire collection. The `nReturned`
    value is `6446`, which is the number of results that matched the query. The `totalDocsExamined`
    and `docsExamined` field have values of `39,732`, which is the number of documents
    in the collection scanned to retrieve the results. This is the also the total
    number of documents present in the collection and all were scanned for the result.
    Finally, `executionTimeMillis` is the number of milliseconds taken to retrieve
    the result.
  prefs: []
  type: TYPE_NORMAL
- en: Improving the query execution time
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'So far, the query doesn''t look too good in terms of performance and there
    is great scope for improvement. To demonstrate how the limit applied to the query
    affects the query plan, we can find the query plan again without the index but
    with the limit clause as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The query plan this time around is interesting. Though we still haven't created
    an index, we do see an improvement in the time that the query took for execution
    and the number of objects scanned to retrieve the results. This is due to the
    fact that mongo ignores the scanning of the remaining documents once the number
    of documents specified in the `limit` function has been reached. We can thus conclude
    that it is recommended that you use the `limit` function to limit your number
    of results where the maximum number of documents accessed is known up front. This
    may give better query performance. The word `may` is important as, in the absence
    of an index, the collection might still be completely scanned if the number of
    matches is not met.
  prefs: []
  type: TYPE_NORMAL
- en: Improvement using indexes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Moving on, we then create a compound index on the state and pincode field.
    The order of the index is ascending in this case (as the value is one) and is
    not significant unless we plan to execute a multi-key sorting. This is a deciding
    factor as to whether the result can be sorted using an index only or mongo needs
    to sort it in memory later on before returning the results. As far as the plan
    of the query is concerned, we can see that there is a significant improvement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The `inputStage` field now has the `IXSCAN` value, which shows that the index
    is indeed used now. The number of results stays, as expected, the same at `6446`.
    The number of objects scanned in the index and the documents scanned in the collection
    has now reduced to the same number of documents as in the result. This is because
    we have now used an index that gives us the starting document to scan, and only
    then, the required number of documents are scanned. This is similar to using the
    book's index to find a word or scanning the entire book to search for the word.
    As expected, the time in `executionTimeMillis` has reduced as well.
  prefs: []
  type: TYPE_NORMAL
- en: Improvement using covered indexes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This leaves us with one field, `executionStages`, which is `FETCH`, and we will
    see what this means. To know what this value is, we need to look briefly at how
    indexes operate.
  prefs: []
  type: TYPE_NORMAL
- en: Indexes store a subset of fields of the original document in the collection.
    The fields present in the index are the same as those that the index is created
    on. The fields, however, are kept sorted in the index in an order specified during
    the index creation. Apart from the fields, there is an additional value stored
    in the index that acts as a pointer to the original document in the collection.
    Thus, whenever the user executes a query, the index is consulted to get a set
    of matches if the query contains fields that an index is present on. The pointer,
    stored with the index entries matching the query, is then used to make another
    IO operation to fetch the complete document from the collection, which is then
    returned to the user.
  prefs: []
  type: TYPE_NORMAL
- en: The value of `executionStages`, which is `FETCH`, indicates that the data requested
    by the user in the query is not entirely present in the index, but an additional
    IO operation is needed to retrieve the entire document from the collection following
    the pointer from the index. If the value is present in the index itself, an additional
    operation to retrieve the document from the collection would not be necessary
    and the data from the index would be returned. This is called a covered index,
    and the value of `executionStages`, in this case, would be `IXSCAN`.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, we just needed the pincodes. So, why not use projection in our
    queries to retrieve just what we need? This would also make the index covered
    as the index entry just has the state's name and pincode, and the required data
    can be served completely without retrieving the original document from the collection.
    The plan of the query in this case is interesting too.
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following plan:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The value of the `totalDocsExamined` and `executionStage: PROJECTION` fields
    is something to observe. As expected, the data that we requested in the projection
    can be served from the index alone. In this case, we scanned 6446 entries in the
    index and thus, the `totalKeysExamined` value is `6446`.'
  prefs: []
  type: TYPE_NORMAL
- en: As the entire result was fetched from the index, our query did not fetch any
    documents from the collection. Hence, the value of `totalDocsExamined` is `0`.
  prefs: []
  type: TYPE_NORMAL
- en: As this collection is small, we do not see a significant difference in the execution
    time of the query. This will be more evident on larger collections. Making use
    of indexes is great and gives us a good performance. Making use of covered index
    gives us an even better performance.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The explain results feature of MongoDB has had a major overhaul in version 3.0\.
    I would suggest spending a few minutes going through its documentation at [http://docs.mongodb.org/manual/reference/explain-results/](http://docs.mongodb.org/manual/reference/explain-results/).
  prefs: []
  type: TYPE_NORMAL
- en: Another thing to remember is that if your document has a lot of fields, try
    and use projection to retrieve only the number of fields we need. The `_id` field
    is retrieved every time by default. Unless we plan to use it, set `_id:0` to not
    retrieve it if it is not a part of the index. Executing a covered query is the
    most efficient way to query a collection.
  prefs: []
  type: TYPE_NORMAL
- en: Some caveats of index creations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will now see some pitfalls in index creation and some facts when an array
    field is used in the index.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the operators that do not use the index efficiently are the `$where`,
    `$nin`, and `$exists` operators. Whenever these operators are used in the query,
    one should bear in mind that a possible performance bottleneck might occur when
    the data size increases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, the `$in` operator must be preferred over the `$or` operator as
    both can be used to achieve more or less the same result. As an exercise, try
    to find the pincodes in the state of Maharashtra and Gujarat in the `postalCodes`
    collection. Write two queries: one using `$or` and one using the `$in` operator.
    Explain the plan for both these queries.'
  prefs: []
  type: TYPE_NORMAL
- en: What happens when an array field is used in the index?
  prefs: []
  type: TYPE_NORMAL
- en: Mongo creates an index entry for each element present in the array field of
    a document. So, if there are 10 elements in an array in a document, there will
    be 10 index entries, one for each element in the array. However, there is a constraint
    while creating indexes containing array fields. When creating indexes using multiple
    fields, no more than one field can be of a type array, and this is done to prevent
    a possible explosion in the number of indexes on adding even a single element
    to the array used in the index. If we think of it carefully, an index entry is
    created for each element in the array. If multiple fields of the type array were
    allowed to be a part of an index, then we would have a large number of entries
    in the index, which would be a product of the length of these array fields. For
    example, a document added with two array fields, each of length 10, would add
    100 entries to the index if it is allowed to create one index using these two
    array fields.
  prefs: []
  type: TYPE_NORMAL
- en: This should be good enough, for now, to scratch the surfaces of a plain, vanilla
    index. We will see more options and types in the following few recipes.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a background and foreground index in the shell
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our previous recipe, we looked at how to analyze the queries, how to decide
    what index needs to be created, and how to create indexes. This, by itself, is
    straightforward and looks reasonably simple. However, for large collections, things
    start getting worse as the index creation time is large. The objective of this
    recipe is to throw some light on these concepts and avoid these pitfalls while
    creating indexes, especially on large collections.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the creation of indexes, we need to have a server up and running. A simple
    single node is what we need. Refer to the *Installing single node MongoDB* recipe
    from [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing
    and Starting the Server* for instructions on how to start the server.
  prefs: []
  type: TYPE_NORMAL
- en: Start connecting two shells to the server by just typing `mongo` from the operating
    system shell. Both of them will, by default, connect to the `test` database.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our test data for zip codes is too small to demonstrate the problem faced in
    index creation on large collections. We need to have more data and thus, we will
    start by creating some data to simulate the problems during index creation. The
    data has no practical meaning but is good enough to test the concepts. Copy the
    following piece in one of the started shells and execute: (It is a pretty easy
    snippet to type out.)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'A document in this collection will look something as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The execution will take quite a lot of time, so we need to be patient. Once
    the execution is over, we are all set for the action.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you are keen to know what the current number of documents loaded in the
    collection is, keep evaluating the following from the second shell periodically:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Create an index on the `value` field of the document as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'While the index creation is in progress, which should take quite some time,
    switch over to the second console and execute the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Both the index creation shell and the one where we executed `findOne` will be
    blocked and the prompt will not be shown on both of them until the index creation
    is complete.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, this was foreground index creation by default. We want to see the behavior
    in background index creation. Drop the created index as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the index again, but this time in background, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'In the second mongo shell, execute `findOne` as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: This should return one document, which is unlike the first instance, where the
    operation was blocked until the index creation completed in the foreground.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the second shell, repeatedly execute the following explain operation with
    a four-to-five second interval between each explain plan invocation until the
    index creation process is complete:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's now analyze what we just did. We created about five million documents
    with no practical importance, but we are just looking to get some data that will
    take a significant amount of time to build the index.
  prefs: []
  type: TYPE_NORMAL
- en: An index can be built in two ways, in the foreground and background. In either
    case, the shell doesn't show the prompt until the `createIndex` operation has
    been completed and will block all operations until the index is created. To illustrate
    the difference between a foreground and background index creation, we executed
    a second mongo shell.
  prefs: []
  type: TYPE_NORMAL
- en: We first created the index in the foreground, which is the default behavior.
    This index building didn't allow us to query the collection (from the second shell)
    until the index was constructed. The `findOne` operation is blocked until the
    entire index was built (from the first shell) before returning the result. On
    other hand, the index that was built in the background didn't block the `findOne`
    operation. If you want to try inserting new documents into the collection while
    the index building is on, this should work very well. Feel free to drop the index
    and recreate it in the background, while simultaneously inserting a document into
    the `indexTest` collection, and you will notice that it works smoothly.
  prefs: []
  type: TYPE_NORMAL
- en: Well, what is the difference between the two approaches and why not always build
    the index in the background? Apart from an extra parameter, `{background:true}`
    (which can also be`{background:1}`) passed as a second parameter to the `createIndex`
    call, there are few differences. The index creation process in the background
    will be slightly slower than the index created in the foreground. Furthermore,
    internally—though not relevant to the end user—the index created in the foreground
    will be more compact than the one created in the background.
  prefs: []
  type: TYPE_NORMAL
- en: Other than this, there will be no significant difference. In fact, if a system
    is running and an index needs to be created while it is serving the end users
    (not recommended, but a situation can come up at times that demands index creation
    in a live system), then creating an index in the background is the only way you
    can do it. There are other strategies to perform such administrative activities,
    which we will see in some recipes in the administration section.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make things worse for foreground index creation, the lock acquired by mongo
    during index creation is not at the collection level but is at the database level.
    To explain what this means, we will have to drop the index on the `indexTest`
    collection and perform the following small exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by creating the index in the foreground from the shell by executing the
    following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, insert a document into the person collection, which may or may not exist
    at this point in the test database, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: We will see that this insert operation in the person collection will be blocked
    while index creation on the `indexTest` collection is in process. However, if
    this insert operation was done on a collection in a different database during
    the index building, it would execute normally without blocking. (You can try this
    out as well.) This clearly shows that the lock is acquired at the database level
    and not at the collection or global level.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Prior to version 2.2 of mongo, locks were at the global level, which is at the
    mongod process level, and not at the database level as we saw previously. You
    need to remember this fact when dealing with a distribution of mongo older than
    version 2.2.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and understanding sparse indexes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Schemaless design is one of the fundamental features of Mongo. This allows documents
    in a collection to have disparate fields, with some fields present in some documents
    and absent in others. In other words, these fields might be sparse, which might
    have already given you a clue on what sparse indexes are. In this recipe, we will
    create some random test data and see how sparse indexes behave against a normal
    index. We shall see the advantages and one major pitfall of using a sparse index.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this recipe, we need to create a collection called `sparseTest`. We will
    require a server to be up and running. Refer to the *Installing single node MongoDB*
    recipe from [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"),
    *Installing and Starting the Server* for instructions on how to start the server.
    Start the shell with the `SparseIndexData.js` script loaded. This script is available
    on the Packt website for download. To know how to start the shell with a script
    preloaded, refer to the *Connecting to a single node in the Mongo shell with JavaScript*
    recipe in [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"),
    *Installing and Starting the Server*.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Load the data in the collection by invoking the following. This should import
    100 documents in the `sparseTest` collection.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, take a look at the data by executing the following query, taking note
    of the `y` field in the top few results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the `y` field is absent, or it is unique if it is present.
    Let''s then execute the following query:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Take a note of the result; it contains both the documents that match the condition
    as well as fields that do not contain the given field, `y`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As the value of `y` seems unique, let''s create a new unique index on the `y`
    field as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: This throws an error complaining that the value is not unique and the offending
    value is the null value.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will fix this by making this index sparse as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'This should fix our problem. To confirm that the index got created, execute
    the following on the shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: This should show two indexes, the default one on `_id` and the one that we just
    created in the preceding step.
  prefs: []
  type: TYPE_NORMAL
- en: Now, execute the query that we executed earlier in step 3 again and see the
    result.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Look at the result and compare it with what we saw before the index was created.
    Re-execute the query but with the following hint forcing a full collection scan:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Observe the result.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: These were a lot of steps that we just performed. We will now dig deeper and
    explain the internals and reasoning for the weird behavior that we see while querying
    the collection that used sparse indexes.
  prefs: []
  type: TYPE_NORMAL
- en: The test data that we created using the JavaScript method just created documents
    with a key, `x`, whose value is a number starting from one, all the way up to
    100\. The value of `y` is set only when `x` is a multiple of three—its value is
    a running number as well, starting from one and should go up to a maximum of 33
    when `x` is `99`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then execute a query and see the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: The value where `y` is `2` is missing in the result and this is what we intended.
    Note that the documents where `y` isn't present are still seen in the result.
    We now plan to create an index on the `y` field. As the field is either not present
    or has a value that is unique, it seems natural that a unique index should work.
  prefs: []
  type: TYPE_NORMAL
- en: Internally, indexes add an entry in the index by default, even if the field
    is absent in the original document in the collection. The value going in the index
    will, however, be null. This means that there will be the same number of entries
    in the index as the number of documents in the collection. For a unique index,
    the value (including the null values) should be unique across the collection,
    which explains why we got an exception during index creation where the field is
    sparse (not present in all the documents).
  prefs: []
  type: TYPE_NORMAL
- en: A solution for this problem is making the index sparse, and all we did was add
    `sparse:1` to the options along with `unique:1`. This does not put an entry in
    the index if the field doesn't exist in the document. Thus, the index will now
    contain fewer entries. It will only contain those entries where the field is present
    in the document. This not only makes the index smaller, making it easy to fit
    in the memory, but also solves our problem of adding a unique constraint. The
    last thing that we want is an index of a collection with millions of documents
    to have millions of entries, where only a few hundred have some values defined.
  prefs: []
  type: TYPE_NORMAL
- en: 'Though we can see that creating a sparse index did make the index efficient,
    it introduced a new problem where some query results were not consistent. The
    same query that we executed earlier yields different results. See the following
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Why did this happen? The answer lies in the query plan for this query. Execute
    the following to view the plan of this query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: This plan shows that it used the index to fetch the matching results. As this
    is a sparse index, all the documents that didn't have the `y` field are not present
    in it and they didn't show up in the result, though they should have. This is
    a pitfall that we need to be careful of when querying a collection with a sparse
    index and the query happens to use the index. It will yield unexpected results.
    One solution is to force a full collection scan, where we provide the query analyzer
    a hint using the `hint` function. Hints are used to force query analyzers to use
    a user-specified index. Though this is not recommended usually as you really need
    to know what you are doing, this is one of the scenarios where this is really
    needed. So, how do we force a full table scan? All we do is provide `{$natural:1}`
    in the `hint` function. A natural ordering of a collection is the order that it
    is stored in on the disk for a particular collection. This `hint` forces a full
    table scan and now we get the results as before. The query performance will, however,
    degrade for large collections as it is now using a full table scan.
  prefs: []
  type: TYPE_NORMAL
- en: If the field is present in a lot of documents (There is no formal cutoff for
    what is a *lot*; it can be 50% for some or 75% for others.) and not really sparse,
    then making the index sparse doesn't make much sense apart from when we want to
    make it unique.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If two documents have a null value for the same field, unique index creation
    will fail, and creating it as a sparse index will not help either.
  prefs: []
  type: TYPE_NORMAL
- en: Expiring documents after a fixed interval using the TTL index
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the interesting features in Mongo is automatically expiring data in the
    collection after a predetermined amount of time. This is a very useful tool when
    we want to purge some data older than a particular timeframe. For a relational
    database, it is not common for folks to set up a batch job that runs every night
    to perform this operation.
  prefs: []
  type: TYPE_NORMAL
- en: With the TTL feature of Mongo, you need not worry about this as the database
    takes care of it out of the box. Let's see how we can achieve this.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's create data in Mongo that we want to play with using the TTL indexes.
    We will create a collection called `ttlTest` for this purpose. We will require
    a server to be up and running. Refer to the *Installing single node MongoDB* recipe
    from [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing
    and Starting the Server* for instructions on how to start the server. Start the
    shell with the `TTLData.js` script loaded. This script is available on the Packt
    website for download. To know how to start the shell with a script preloaded,
    refer to the *Connecting to a single node in the Mongo shell with JavaScript*
    recipe from [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"),
    *Installing and Starting the Server*.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Assuming that the server has started and the script provided is loaded on the
    shell, invoke the following method from the mongo shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a TTL index on the `createDate` field as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, query the collection as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: This should give us three documents. Repeat the process and execute the `find`
    query in approximately 30-40 seconds repeatedly to see the three documents getting
    deleted until the entire collection has zero documents left in it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's start by opening the `TTLData.js` file and see what is going on inside
    it. The code is pretty simple and it just gets the current date using new `Date()`.
    It then creates three documents with `createDate` that were four, three, and two
    minutes behind the current time for the three documents. So, on the execution
    of the `addTTLTestData()` method in this script, we have three documents in the
    `ttlTest` collection with each having a difference of one minute in their creation
    time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is the core of the TTL feature: the creation of the TTL index.
    It is similar to the creation of any other index using the `createIndex` method,
    except that it also accepts a second parameter that is a JSON object. These two
    parameters are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The first parameter is `{createDate:1}`; this will tell mongo to create an index
    on the `createDate` field, and the order of the index is ascending as the value
    is `1` (`-1` would have been descending).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second parameter, `{expireAfterSeconds:300}`, is what makes this index a
    TTL index, and it tells Mongo to automatically expire the documents after 300
    seconds (five minutes).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Okay, but five minutes since when? Is it the time they were inserted in the
    collection or some other timestamp? In this case, it considers the `createTime`
    field as the base because this was the field that we created the index on.
  prefs: []
  type: TYPE_NORMAL
- en: 'This now raises a question: if a field is being used as a base for the computation
    of time, there has to be some restriction on its type. It just doesn''t make sense
    to create a TTL index, as we created previously, on a `char` field holding, say,
    the name of a person.'
  prefs: []
  type: TYPE_NORMAL
- en: Yes; as we guessed, the type of the field can be of a BSON type date or an array
    of dates. What will happen in the case where an array has multiple dates? What
    will be considered in that case?
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that Mongo uses a minimum of dates available in the array. Try
    this scenario out as an exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Put two dates separated by about five minutes from each other in a document
    against the field name, `updateField`, and then create a TTL index on this field
    to expire the document after 10 minutes (600 seconds). Query the collection and
    see when the document gets deleted from the collection. It should get deleted
    after roughly 10 minutes have elapsed after the minimum time value present in
    the `updateField` array.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from the constraint for the type of field, there are a few more constraints:'
  prefs: []
  type: TYPE_NORMAL
- en: If a field already has an index on it, you cannot create a TTL index. As the
    `_id` field of the collection already has an index by default, it effectively
    means that you cannot create a TTL index on the `_id` field.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A TTL index cannot be a compound index involving multiple fields.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a field doesn't exist, it will never expire. (That's pretty logical, I guess.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It cannot be created on capped collections. In case you are not aware of capped
    collections, they are special collections in Mongo with a size limit on them with
    a FIFO insertion order and delete old documents to make place for new documents,
    if needed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: TTL indexes are supported only on the Mongo version 2.2 and above. Note that
    the document will not be deleted at exactly the given time in the field. The cycle
    will be of a granularity of one minute, which will delete all the documents eligible
    for deletion since the last time the cycle was run.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A use case might not demand deleting all the documents after a fixed interval
    has elapsed. What if we want to customize the point until a document stays in
    the collection? This too can be achieved, which is what will be demonstrated in
    the next recipe, *Expiring documents at a given time using the TTL index*.
  prefs: []
  type: TYPE_NORMAL
- en: Expiring documents at a given time using the TTL index
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, *Expiring documents after a fixed interval using the
    TTL index*, we have seen how documents can expire after a fixed time period. However,
    there can be some cases where we might want to have documents expiring at different
    times. This is not what we saw in the previous recipe. In this recipe, we will
    see how we can specify the time that the document can expire and it might be different
    for different documents.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this recipe, we will create a collection called `ttlTest2`. We will require
    a server to be up and running. Refer to the *Installing single node MongoDB* recipe
    from [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"), *Installing
    and Starting the Server* for instructions on how to start the server. Start the
    shell with the `TTLData.js` script loaded. This script is available on the Packt
    website for download. To know how to start the shell with a script preloaded,
    refer to the *Connecting to a single node in the Mongo shell with JavaScript*
    recipe in [Chapter 1](ch01.html "Chapter 1. Installing and Starting the Server"),
    *Installing and Starting the Server*.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Load the required data in the collection using the `addTTLTestData2` method.
    Execute the following on the mongo shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, create the TTL index on the `ttlTest2` collection as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following `find` query to view the three documents in the collection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Now, after approximately four, five, and seven minutes, see that the documents
    with the IDs two, one, and three get deleted, respectively.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's start by opening the `TTLData.js` file and see what is going on inside
    it. Our method of interest for this recipe is `addTTLTestData2`. This method simply
    creates three documents in the `tllTest2` collection with `_id` of `1`, `2`, and
    `3` with their `exipryDate` fields set to `5`, `4`, and `7` minutes after the
    current time, respectively. Note that this field has a future date, unlike the
    date given in the previous recipe, where it was a creation date.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we create an index: `db.ttlTest2.createIndex({expiryDate :1}, {expireAfterSeconds:0})`.
    This is different from the way we created the index for the previous recipe, where
    the `expireAfterSeconds` field of the object was set to a non-zero value. This
    is how the value of the `expireAfterSeconds` attribute is interpreted. If the
    value is non-zero, then this is the time in seconds that has elapsed after a base
    time when the document will be deleted from the collection by Mongo. This base
    time is the value held in the field that the index is created on (`createTime`,
    as in the previous recipe). If this value is zero, then the date value that the
    index is created on (`expiryDate`, in this case) will be the time when the document
    will expire.'
  prefs: []
  type: TYPE_NORMAL
- en: To conclude, TTL indexes work well if you want to delete the document upon expiry.
    There are quite a lot of cases where we might want to move the document to an
    archive collection, where the archived collection might be created based on, say,
    the year and month. In any such scenarios, a TTL index is not helpful and we might
    have to write an external job ourselves that does this work. Such a job could
    also read the collection for a range of documents, add them to the target collection,
    and delete them from the source collection. The folks at MongoDB have already
    planned to release a feature that addresses this issue.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this and the previous recipe, we looked at TTL indexes and how to use them.
    However, what if, after creating a TTL index, we want to modify the TTL value?
    This is possible using the `collMod` option. See more on this option in the administration
    section.
  prefs: []
  type: TYPE_NORMAL
