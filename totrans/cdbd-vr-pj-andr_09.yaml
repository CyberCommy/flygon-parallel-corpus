- en: Chapter 9. Music Visualizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*"See the music, hear the dance," said George Balanchine, famed Russian-born
    choreographer and father of the American ballet.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We won't attempt to raise the level of the art form, but still, maybe it'd be
    fun to visualize the playlists on our phones. In this project, we will create
    3D animated abstract graphics that dance to the beat of your music. You might
    be familiar with music visualizations in 2D, but what would it look like in VR?
    To get inspired, try Googling for images using the phrase *geometry wars*, the
    classic game for XBox, for example!
  prefs: []
  type: TYPE_NORMAL
- en: A visualizer app takes input from the Android audio system and displays visualizations.
    In this project, we will take advantage of the Android `Visualizer` class, which
    lets an app capture part of the currently playing audio, not the full fidelity
    music details but a lower quality audio content sufficient for visualizations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this project, we will:'
  prefs: []
  type: TYPE_NORMAL
- en: Set up the new project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build a Java class architecture named VisualizerBox
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Capture waveform data from the phone's audio player
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build a geometric visualization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build a texture-based visualization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Capture the FFT data and build an FFT visualization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add a trippy trails mode
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support multiple concurrent visualizations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The source code for this project can be found on the Packt Publishing website
    and on GitHub at [https://github.com/cardbookvr/visualizevr](https://github.com/cardbookvr/visualizevr)
    (with each topic as a separate commit).
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a new project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To build this project, we''re going to use our RenderBox library created in
    [Chapter 5](ch05.html "Chapter 5. RenderBox Engine"), *RenderBox Engine*. You
    can use yours, or grab a copy from the downloadable files provided with this book
    or our GitHub repo (use the commit tagged `after-ch8`—[https://github.com/cardbookvr/renderboxlib/releases/tag/after-ch8](https://github.com/cardbookvr/renderboxlib/releases/tag/after-ch8)).
    For a more detailed description of how to import the `RenderBox` library, refer
    to the final section, *Using RenderBox in future projects*, of [Chapter 5](ch05.html
    "Chapter 5. RenderBox Engine"), *RenderBox Engine*. To create a new project, perform
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: With Android Studio opened, create a new project. Let's name it `VisualizeVR`
    and target **Android 4.4 KitKat (API 19)** with an **Empty Activity**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create new modules for each of `renderbox`, `common`, and `core` packages, using
    **File** | **New Module** | **Import .JAR/.AAR Package**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the modules as dependencies for the app, using **File** | **Project Structure**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Edit the `build.gradle` file as explained in [Chapter 2](ch02.html "Chapter 2. The
    Skeleton Cardboard Project"), *The Skeleton Cardboard Project*, to compile against
    SDK 22.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update `/res/layout/activity_main.xml` and `AndroidManifest.xml`, as explained
    in the previous chapters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Edit `MainActivity` as `class MainActivity extends CardboardActivity implements
    IRenderBox`, and implement the interface method stubs (*Ctrl* + *I*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can go ahead and define the `onCreate` method in `MainActivity`. The class
    now has the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You can add a cube to the scene, temporarily, to ensure that everything is
    set up properly. Add it to the `setup` method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: If you remember, a `Cube` is a `Component` that's added to a `Transform`. The
    `Cube` defines its geometry (for example, vertices). The `Transform` defines its
    position, rotation, and scale in 3D space.
  prefs: []
  type: TYPE_NORMAL
- en: You should be able to click on **Run 'app'** with no compile errors, and see
    the cube and Cardboard split screen view on your Android device.
  prefs: []
  type: TYPE_NORMAL
- en: Capturing audio data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using the Android `Visualizer` class ([http://developer.android.com/reference/android/media/audiofx/Visualizer.html](http://developer.android.com/reference/android/media/audiofx/Visualizer.html)),
    we can retrieve part of the audio data that is currently playing, at a specified
    sample rate. You can choose to capture data as waveform and/or frequency data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Waveform**: This is an array of mono audio waveform bytes, or **pulse code
    modulation** (**PCM**) data, representing a sample series of audio amplitudes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Frequency**: This is an array of **Fast Fourier Transform** (**FFT**) bytes,
    representing a sampling of audio frequencies'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data is limited to 8 bits, so it's not useful for playback but is sufficient
    for visualizations. You can specify the sampling rate, although it must be a power
    of two.
  prefs: []
  type: TYPE_NORMAL
- en: Armed with this knowledge, we'll now go ahead and begin implementing an architecture
    that captures audio data and makes it available to visualization renderers that
    you can build.
  prefs: []
  type: TYPE_NORMAL
- en: A VisualizerBox architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Music visualizers often look really cool, especially at first. But after a time
    they may seem too repetitive, even boring. Therefore, in our design, we'll build
    the ability to queue up a number of different visualizations, and then, after
    a period of time, transition from one to the next.
  prefs: []
  type: TYPE_NORMAL
- en: To begin our implementation, we'll define an architecture structure that will
    be expandable and let us develop new visualizations as we go along.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, even before that, we must ensure that the app has permission to use
    the Android audio features we need. Add the following directives to `AndroidManifest.xml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Remember that the `RenderBox` library, first developed in [Chapter 5](ch05.html
    "Chapter 5. RenderBox Engine"), *RenderBox Engine,* allows `MainActivity` to delegate
    much of the graphics and Cardboard VR work to the `RenderBox` class and associated
    classes (`Component`, `Material`, and so on). We will follow a similar design
    pattern here, built on top of `RenderBox`. `MainActivity` can instantiate specific
    visualizations and then delegate the work to the `VisualizerBox` class.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `VisualizerBox` class will provide the callback functions to the Android
    `Visualizer` class. Let''s define a skeletal implementation of this first. Create
    a `VisualizerBox` Java class, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Integrate `VisualizerBox` into `MainActivity`, adding a `visualizerBox` variable
    at the top of the class. In `MainActivity`, add the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize it in `onCreate`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, in `MainActivity`, call the corresponding version of each of the `IRenderBox`
    interface methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Good. Now we''ll set up `VisualizerBox` to let you build and use one or more
    visualizations. So, first let''s define the abstract `Visualization` class in
    the `Visualization.java` file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we have a mechanism to create a variety of visualization implementations
    for the app. Before we go ahead and start writing one of those, let''s also provide
    the integration with `VisualizerBox`. At the top of the `VisualizerBox` class,
    add a variable to the current `activeViz` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, call it from the interface methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Of course, we're not even using the Android `Visualizer` class yet and not rendering
    anything on the screen. That'll come next.
  prefs: []
  type: TYPE_NORMAL
- en: 'For now, let''s create a placeholder for a visualization. Create a new folder
    in your project named `visualizations`. Right-click on your Java code folder (for
    example, `java/com/cardbookvr/visualizevr/`), go to **New** | **Package**, and
    name it `visualizations`. Then, right click on the new `visualizations` folder,
    go to **New** | **Java Class**, and name it `BlankVisualization`. Then, define
    it as `extends Visualization` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll be able to use this as a template for specific visualizers. The purpose
    of each method is pretty self-explanatory:'
  prefs: []
  type: TYPE_NORMAL
- en: '`setup`: This initializes variables, transforms, and materials for the visualization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`preDraw`: This code is executed at the beginning of each frame; for example,
    using the current captured audio data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`postDraw`: This code is executed at the end of each frame'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let's add some meat to this skeleton.
  prefs: []
  type: TYPE_NORMAL
- en: Waveform data capture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As mentioned earlier, the Android `Visualizer` class lets us define callbacks
    to capture audio data. This data comes in two formats: waveform and FFT. We''ll
    add just the waveform data to the `VisualizerBox` class now.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, define the variables that we''ll use for the captured audio data, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Using the API, we can determine the minimum capture size available, and then
    use that as our capture sample size.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, initialize them in the constructor as follows. First, instantiate an
    Android `Visualizer`. Then set the capture size to use, and allocate our buffers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We want to use the minimum size for a variety of reasons. Firstly, it will be
    faster, and in VR, speed is paramount. Secondly, it organizes our FFT samples
    (as discussed later) into fewer buckets. This is helpful because each bucket catches
    more activity over a broader range of frequencies.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that we left a comment where we'll define the capture listener, and then
    set it in the visualizer. Make sure that you enable the visualizer as always listening.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first write the `captureListener` object for waveform data only. We
    define and instantiate a new anonymous class that implements `Visualizer.OnDataCaptureListener`,
    and provide it with a function named `onWaveFormDataCapture`, which receives the
    wave form bytes and stores them for our `Visualization` code (forthcoming):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The interface still requires that we provide an `onFftDataCapture` method, but
    we're leaving it empty for the time being.
  prefs: []
  type: TYPE_NORMAL
- en: Now we're ready to add some graphics to this baby.
  prefs: []
  type: TYPE_NORMAL
- en: A basic geometric visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For our first visualization, we'll create a basic equalizer wave graphic. It'll
    be a rectangular block consisting of a series of cubes that are scaled according
    to the audio waveform data. We'll use the built-in `Cube` component already in
    the `RenderBox` library and its basic vertex color lighting material.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `visualizations/` folder, create a new Java class named `GeometricVisualization`
    and begin as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'At the top of the class, declare a `Transform` array of cube transforms and
    the corresponding array for `RenderObjects`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, initialize them in the `setup` method. We''ll allocate the array of cubes,
    aligned and scaled as an adjacent set of blocks, creating a 3D representation
    of a wavy block. The setup method can be implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now on each frame, we just need to modify the height of each cube based on
    the current waveform data from the audio source (as obtained in `VisualizerBox`).
    Implement the `preDraw` method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need to add a stub for the `postDraw` implementation. Then, instantiate
    the visualization and make it the active one. In `MainActivity`, at the end of
    `onCreate`, add the following line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: That's all we need for now.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start playing some music on your phone. Then, run the app. You will see something
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A basic geometric visualization](img/B05144_09_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, we kept the unit cube in the scene, as it helps clarify what's
    going on. Each audio datum is a thin "slice" (or a flattened cube) the height
    of which varies with the audio value. If you're looking at a colored version of
    the preceding screen image, you will notice that the colored faces of the visualization
    cubes are like solitary cubes since they use the same object and material to render.
  prefs: []
  type: TYPE_NORMAL
- en: This visualization is a very basic example of using audio waveform data to dynamically
    modify 3D geometry. Let your imagination run wild to create your own. The audio
    bytes can control any transform parameters, including scale, position, and rotation.
    Remember that we're in a 3D virtual reality space, and you can use all of it—move
    your stuff all round, up and down, and even behind you. We have a few basic primitive
    geometric shapes (a cube, sphere, plane, triangle, and so on). But you can also
    use the audio data to parametrically generate new shapes and models. Plus, you
    can even integrate the `ModelObject` class from the previous chapter to load interesting
    3D models!
  prefs: []
  type: TYPE_NORMAL
- en: In the next topic, we'll take a look at how to use the audio waveform data in
    texture-based material shaders.
  prefs: []
  type: TYPE_NORMAL
- en: 2D texture-based visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The second visualization will also be a basic oscilloscope-type display of waveform
    data. However, previously, we used audio data to scale 3D slice cubes; this time,
    we'll render them all on a 2D plane using a shader that uses audio data as input.
  prefs: []
  type: TYPE_NORMAL
- en: Our `RenderBox` library allows us to define new materials and shaders. In the
    previous projects, we built materials that use bitmap images for texture mapping
    onto the geometry as it's rendered. In this project, we'll paint the quad using
    the audio bytes array, using the byte value to control the position where we set
    a brighter color. (Note that the `Plane` class was added to `RenderBox` lib in
    [Chapter 7](ch07.html "Chapter 7. 360-Degree Gallery"), *360-Degree Gallery*.)
  prefs: []
  type: TYPE_NORMAL
- en: Texture generator and loader
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, let''s generate a texture structure to hold our texture data. In the
    `VisualizerBox` class, add the following method to set up the texture in GLES.
    We can''t use our normal texture pipeline, since it is designed to allocate a
    texture directly from image data. Our data is one-dimensional, so it may seem
    odd to use a `Texture2D` resource, but we''ll set the height to one pixel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Then add the call to `setup`, including a static variable to hold the generated
    texture handle:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can populate the texture from audio byte data. In the Android `Visualizer`
    listener, add a call to `loadTexture` in the `onWaveFormDataCapture` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Let's define `loadTexture` as follows. It copies the audio bytes into a new
    array buffer and hands it off to OpenGL ES with the `glBindTexture` and `glTexImage2D`
    calls.
  prefs: []
  type: TYPE_NORMAL
- en: '(Refer to [http://stackoverflow.com/questions/14290096/how-to-create-a-opengl-texture-from-byte-array-in-android](http://stackoverflow.com/questions/14290096/how-to-create-a-opengl-texture-from-byte-array-in-android).):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Waveform shaders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now it's time to write the shader programs that, among other things, will dictate
    the parameters and attributes that need to be set in the `Material` class.
  prefs: []
  type: TYPE_NORMAL
- en: If necessary, create a resources directory for the shaders, `res/raw/`. Then,
    create the `waveform_vertex.shader` and `waveform_fragment.shader` files. Define
    them as follows.
  prefs: []
  type: TYPE_NORMAL
- en: The `waveform_vertex.shader` file is identical to the `unlit_tex_vertex` shader
    we were using. Strictly speaking, we can just reuse this file and specify its
    resource in the `createProgram` function, but it is good practice to define individual
    shader files unless you are explicitly following some sort of a pattern where
    you are using a number of variants on a given shader.
  prefs: []
  type: TYPE_NORMAL
- en: 'File: `res/raw/waveform_vertex.shader`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: For the `waveform_fragment` shader, we add variables for a solid color (`u_Color`)
    and threshold width (`u_Width`). And then, add a bit of logic to decide whether
    the *y* coordinate of the current pixel being rendered is within `u_Width` of
    the sample.
  prefs: []
  type: TYPE_NORMAL
- en: 'File: `res/raw/waveform_fragment.shader`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Basic waveform material
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now we define the `Material` class for the shaders. Create a new Java class
    named `WaveformMaterial` and define it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Add material variables for the texture ID, border, width, and color. Then,
    add variables for the shader program reference and buffers, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can add a constructor. As we saw earlier, it calls a `setupProgram`
    helper method that creates the shader program and obtains references to its parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Likewise, we add a `setBuffers` method to be called by the `RenderObject` component
    (`Plane`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the `draw` code, which will be called from the `Camera` component, to render
    the geometry prepared in the buffers (via `setBuffers`). The `draw` method looks
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'One more thing; let''s provide a method to destroy an existing material:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Waveform visualization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now we can create a new visualization object. Under the `visualizations/` folder,
    create a new Java class named `WaveformVisualization` and define it as `extends
    Visualization`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Declare a variable for the `Plane` component we will create:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Create it in the `setup` method as follows. Set the material to a new `WaveformMaterial`,
    and position it over towards the left:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Now in `onCreate` of `MainActivity`, replace the previous visualization with
    this one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'When you run the project, you get a visualization like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Waveform visualization](img/B05144_09_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FFT visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the next visualization, we'll introduce the use of FFT data (instead of
    waveform data). As in the previous example, we'll dynamically generate a texture
    from the data and write a material and shaders to render it.
  prefs: []
  type: TYPE_NORMAL
- en: Capture the FFT audio data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To begin with, we need to add that data capture to our `VisualizerBox` class.
    We will start by adding the variables we''ll need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to allocate the FFT data arrays, and to do that we need to know their
    size. We can ask the Android `Visualizer` API how much data it''s capable of giving
    us. For now, we''ll choose the minimum size and then allocate the arrays as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Capturing FFT data is similar to capturing waveform data. But we''ll do some
    preprocessing on it before saving it. According to the Android `Visualizer` API
    documentation, ([http://developer.android.com/reference/android/media/audiofx/Visualizer.html#getFft(byte[]](http://developer.android.com/reference/android/media/audiofx/Visualizer.html#getFft(byte[]))
    the `getFfT` function provides data specified as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The capture is an 8-bit magnitude FFT; the frequency range covered being 0 (DC)
    to half of the sampling rate returned by `getSamplingRate()`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The capture returns the real and imaginary parts of a number of frequency points
    equal to half of the capture size plus one
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that only the real part is returned for the first point (DC) and the last
    point (*sampling frequency/2*).
  prefs: []
  type: TYPE_NORMAL
- en: 'The layout in the returned byte array is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*n* is the capture size returned by `getCaptureSize()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Rfk` and `Ifk` are the real and imaginary parts of the *kth* frequency component,
    respectively'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If `Fs` is the sampling frequency returned by `getSamplingRate()`, the *kth*
    frequency is: *(k*Fs)/(n/2)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Likewise, we''ll prepare the incoming captured data into a normalized array
    of values between 0 and 255\. Our implementation is as follows. Add the `onFftDataCapture`
    declaration immediately after the `onWaveFormDataCapture` method (within the `OnDataCaptureListener`
    instance):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that our algorithm uses a `MIN_THRESHOLD` value of 1.5 to filter out insignificant
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Now in `setup()`, initialize `fftTexture` with a generated texture, as we do
    for the `audioTexture` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: FFT shaders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we need to write the shader programs.
  prefs: []
  type: TYPE_NORMAL
- en: If necessary, create a resources directory for the shaders, `res/raw/`. The
    `fft_vertex.shader` is identical to the `waveform_vertext.shader` created earlier,
    so you can just duplicate it.
  prefs: []
  type: TYPE_NORMAL
- en: For the `fft_fragment` shader, we add a bit of logic to decide whether the current
    coordinate is being rendered. In this case, we are not specifying a width and
    just rendering all pixels below the value. One way to look at the difference is
    that our waveform shader is a line graph (well, actually a scatterplot), and our
    FFT shader is a bar graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'File: `res/raw/fft_fragment.shader`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Basic FFT material
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The code for the `FFTMaterial` class is very similar to what we did for the
    `WaveformMaterial` class. So for brevity, just duplicate that file into a new
    one named `FFTMaterial.java`. And then, modify it as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ensure that the class name and constructor method name now read as `FFTMaterial`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We decided to change the `borderColor` array to a different hue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'In `setupProgram`, ensure that you''re referencing the `R.raw.fft_vertex` and
    `R.raw.fft_fragment` shaders:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, make sure that the appropriate shader-specific parameters are getting
    set. These shaders use `u_Color` (but not a `u_Width` variable):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, in the `draw` method, we''re going to draw with the `VisualizerBox.fftTexture`
    value (instead of `VisualizerBox.audioTexture`), so change the call to `GLES20.glBindTexture`
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Ensure that the `colorParam` parameter is set (but unlike the `WaveformMaterial`
    class, there is no width parameter here):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: FFT visualization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can now add the visualization for the FFT data. In the `visualizations/`
    folder, duplicate the `WaveformVisualization.java` file into a new file named
    `FFTVisualization.java`. Ensure that it''s defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'In its `setup` method, we''ll create a `Plane` component and texture it with
    the `FFTMaterial` class like this, (also note modifying the position and rotation
    values):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Now in `onCreate` of `MainActivity`, replace the previous visualization with
    this one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'When you run the project, we get a visualization like this, rotated and positioned
    over to the right:'
  prefs: []
  type: TYPE_NORMAL
- en: '![FFT visualization](img/B05144_09_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This simple example illustrates that FFT data separates spatial frequencies
    of the audio into discrete data values. Even without understanding the underlying
    mathematics (which is nontrivial), it's often sufficient to know that the data
    changes and flows in sync with the music. We used it here to drive a texture map.
    FFT can also be used like we used waveform data in the first example to drive
    attributes of 3D objects in the scene, including position, scale, and rotation,
    as well as parametrically defined geometry. In fact, it is generally a better
    data channel for such purposes. Each bar corresponds to an individual frequency
    range, so you can specify certain objects to respond to high frequencies versus
    low frequencies.
  prefs: []
  type: TYPE_NORMAL
- en: Trippy trails mode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you are craving hallucinogenic simulations, we''ll introduce a "trippy trails
    mode" to our visualizations! The implementation is added to the `RenderBox` library
    itself. If you''re using the completed `RenderBox` library, then just toggle on
    the mode in your app. For example, in `setup()` of `MainActivity`, add the following
    line of code at the end:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'To implement it in your copy of `RenderBox` library, open that project (in
    Android Studio). In the `Camera` class (the `components/Camera.java` file), add
    `public boolean trailsMode`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, in `onDrawEye`, instead of erasing the screen for the new frame, we''ll
    draw a full screen quad over the entire frame, with alpha transparency, thus leaving
    behind a ghostly faded image of the last frame. Every subsequent frame is overdrawn
    by more semi-transparent black, causing them to fade out over time. Define a color
    value as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, modify `onDrawEye`, so it reads as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: The `customClear` method skips the clear call, leaving behind the colors from
    the previous frame. Instead, it just draws a semitransparent full-screen black
    quad with transparency, slightly darkening the "old" image each frame. Before
    we can do this, the camera needs a shader program to draw the full screen solid
    color.
  prefs: []
  type: TYPE_NORMAL
- en: '`fullscreen_solid_color_vertex.shader` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '`fullscreen_solid_color_fragment.shader` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Now back to the `Camera` component. We set up the program and define a full
    screen quad mesh, buffers, and other variables. First, we define the variables
    we''ll need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, define a method to set up the program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a method to allocate the buffers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, call these from the `Camera` initializer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can implement the `customClear` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Rebuild the `RenderBox` module and copy the library file back to this `VisualizeVR`
    project. Don’t forget to set `trailsMode` to `true`!
  prefs: []
  type: TYPE_NORMAL
- en: Now when you run the app, it looks trippy and cool!
  prefs: []
  type: TYPE_NORMAL
- en: '![Trippy trails mode](img/B05144_09_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Multiple simultaneous visualizations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have a collection of visualizations, we can enhance the app to run
    more than one at a time and switch between them.
  prefs: []
  type: TYPE_NORMAL
- en: 'To support multiple concurrent visualizations, replace the `activeViz` variable
    in `VisualizerBox` with a list of `visualizations`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, cycle through the list in each of the `VisualizerBox` method that use
    it. We always want to set up all of them, but then only draw (`preDraw`, `postDraw`)
    the active ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'We can control the scene in `MainActivity`. Modify the `MainActivity` class''s
    `onCreate` method to populate the `visualizations` list, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Run the project and we have a 3D scene full of visualizations!
  prefs: []
  type: TYPE_NORMAL
- en: '![Multiple simultaneous visualizations](img/B05144_09_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Random visualizations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can switch between visualizations by adding and removing them over time.
    In the following example, we start with one active visualization and then every
    few seconds, toggle a random visualization on or off.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, add an `activate` method to the abstract `Visualization` class, which
    takes a Boolean enabled parameter. The Boolean active variable is read-only:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Its implementation will depend on the specific visualization. `RenderBox` library
    provides an `enabled` flag that''s used when we render objects. The ones that
    instantiate a single `Plane` component are the easiest, such as `WaveformVisualization`
    and `FFTVisualization`. To each of these, add the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'For the `GeometricVisualization` class, we can enable (and disable) each of
    the component cubes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Now we can control this within the `MainActivity` class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start with each of `visualizations` that are inactive. Add this initialization
    to `setup()` of `MainActivity`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'In `preDraw` of `MainActivity`, we''ll check the current time (using the `Time`
    class of `RenderBox` library) and toggle a random visualization after every 3
    seconds. First, add a few variables to the top of the class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can modify `preDraw` to check the time and modify the list of `visualizations`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: A similar kind of time control structure (or delta time) can be used to implement
    many kinds of animation, such as changing the visualization object's position,
    rotation, and/or scale, or evolving the geometry itself over time.
  prefs: []
  type: TYPE_NORMAL
- en: Further enhancements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We hope that we've given you some tools to get you going with your own music
    visualizations. As we've suggested throughout this chapter, the options are infinite.
    Unfortunately, space prohibits us from having too much fun coding more and more
    stuff here.
  prefs: []
  type: TYPE_NORMAL
- en: '**Animations**: We have applied the simplest transformations to each of our
    visualizations: a simple position, scale, and perhaps 90-degree rotations. Naturally,
    the position, rotation, and scale can be animated, that is, updated for each frame
    in coordination with the music, or independent of the music using `Time.deltaTime`.
    Stuff can be virtually flying all around you!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Advanced textures and shaders**: Our shaders and data-driven textures are
    the most basic: fundamentally rendering a single color pixel corresponding to
    the audio byte value. The audio data can be fed into much more complex and interesting
    algorithms to generate new patterns and color and/or be used to morph preloaded
    textures.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Texture mapping**: The texture materials in the project are simply mapped
    onto a flat plane. Hey man, this is VR! Map the textures onto a photosphere or
    other geometry and totally immerse your users in it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Render to texture**: Our trails mode looks alright for these visualizations,
    but will probably become a mess for anything sufficiently complex. Instead, you
    could use it exclusively within the surface of your textured planes. Setting up
    RTs is complex and beyond the scope of this book. Essentially, you introduce another
    camera to your scene, direct OpenGL to render subsequent draw calls to a new surface
    that you''ve created, and use that surface as the texture buffer for the objects
    you want to render it onto. RT is a powerful concept, enabling techniques such
    as reflection and in-game security cameras. Furthermore, you can apply transformations
    to the surface to make the trails appear to fly off into the distance, which is
    a popular effect among traditional visualizers such as MilkDrop ([https://en.wikipedia.org/wiki/MilkDrop](https://en.wikipedia.org/wiki/MilkDrop)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parametric geometry**: Audio data can be used to drive the definition and
    rendering of 3D geometric models of varying complexity. Think of fractals, crystals,
    and 3D polyhedra. Take a look at Goldberg polyhedra (refer to [http://schoengeometry.com/](http://schoengeometry.com/))
    and Sacred geometry (refer to [http://www.geometrycode.com/sacred-geometry/](http://www.geometrycode.com/sacred-geometry/))
    for inspiration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A community invite
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We invite you to share your own visualizations with other readers of this book
    and the Cardboard community at large. One way to do this is via our GitHub repository.
    If you create a new visualization, submit it as a pull request to the project
    at [https://github.com/cardbookvr/visualizevr](https://github.com/cardbookvr/visualizevr),
    or create your own fork of the entire project!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we built a music visualizer that runs as a Cardboard VR application.
    We designed a general architecture that lets you define multiple visualizations,
    plug them into the app, and transition between them. The app uses the Android
    `Visualization` API to capture the waveform and FFT data from the phone's current
    audio player.
  prefs: []
  type: TYPE_NORMAL
- en: First, we defined the `VisualizerBox` class responsible for the activity and
    callback functions to the Android `Visualizer` API. Then, we defined an abstract
    `Visualization` class to implement a variety of visualizations. We then added
    waveform audio data capture to `VisualizerBox` and used it to parametrically animate
    a series of cubes to make a 3D wavy box. Next, we wrote a second visualizer; this
    time using waveform data to dynamically generate a texture that is rendered with
    material shader programs. And lastly, we captured the FFT audio data and used
    it for a third visualization. Then, we added more fun with a trippy trails mode
    and multiple concurrent visualizations that transition in and out randomly.
  prefs: []
  type: TYPE_NORMAL
- en: We acknowledge that the visual examples are pretty simplistic, but hopefully
    they'll fuel your imagination. We challenge you to build your own 3D virtual reality
    music visualizations that perhaps utilize a combination of the techniques in this
    project as well as other things from this book.
  prefs: []
  type: TYPE_NORMAL
- en: Onward to the future
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We hope you've enjoyed this introduction to and journey through Cardboard virtual
    reality development for Android. Throughout this book, we have explored the Google
    Cardboard Java SDK, OpenGL ES 2.0 graphics, and Android development in general.
    We touched on a number of VR best practices and saw the limitations of low-level
    graphics development on a mobile platform. Still, if you followed along, you've
    succeeded in implementing a reasonable general purpose library for 3D graphics
    and VR development. You created a wide variety of VR applications, including an
    app launcher, a Solar System simulation, a 360-degree media gallery, a 3D model
    viewer, and music visualizers.
  prefs: []
  type: TYPE_NORMAL
- en: Naturally, we expect the Cardboard Java SDK to change, evolve, and mature from
    this point forward. No one really knows what the future holds, perhaps not even
    Google. Yet here we are, at the precipice of a bold new future. The best way to
    predict the future is to help invent it. Now it's your turn!
  prefs: []
  type: TYPE_NORMAL
