- en: Using Elasticsearch, Logstash, and Kibana to Manage Logs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deploying **Elasticsearch**, **Logstash**, and **Kibana** (**ELK Stack**) is
    relatively straightforward, but there are several considerations that need to
    be taken into account when installing these components. While this will not be
    an in-depth guide for an Elastic Stack, the main takeaways will be the implementation
    aspects, the decisions that are made through the process, and how you, as an architect,
    should think when making these decisions.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will help you, as an architect, define the aspects that are needed
    to deploy an ELK Stack, and what configurations to use when working with the components
    that make up the Elastic Stack.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will go through the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Installing and configuring Elasticsearch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing and configuring Logstash and Kibana
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing and explaining Beats
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring Kibana dashboards
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following tools and installations will be used in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Elasticsearch installation guide**: [https://www.elastic.co/guide/en/elasticsearch/reference/current/_installation.html](https://www.elastic.co/guide/en/elasticsearch/reference/current/_installation.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**XFS stripe size and Stripe unit "how to"**: [http://xfs.org/index.php/XFS_FAQ#Q:_How_to_calculate_the_correct_sunit.2Cswidth_values_for_optimal_performance](http://xfs.org/index.php/XFS_FAQ#Q:_How_to_calculate_the_correct_sunit.2Cswidth_values_for_optimal_performance)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**XFS write barriers**: [https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/storage_administration_guide/writebarrieronoff](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/storage_administration_guide/writebarrieronoff)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Elasticsearch configuration details**: [https://www.elastic.co/guide/en/elasticsearch/reference/current/settings.html](https://www.elastic.co/guide/en/elasticsearch/reference/current/settings.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Avoiding a split brain in Elasticsearch**: [https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html#split-brain](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html#split-brain)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Elasticsearch cluster state API**: [https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-state.html](https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-state.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Logstash installation guide**: [https://www.elastic.co/guide/en/logstash/current/installing-logstash.html](https://www.elastic.co/guide/en/logstash/current/installing-logstash.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kibana user guide and how to install**: [https://www.elastic.co/guide/en/kibana/current/rpm.html](https://www.elastic.co/guide/en/kibana/current/rpm.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Logstash filter example for Beats modules**: [https://www.elastic.co/guide/en/logstash/current/logstash-config-for-filebeat-modules.html](https://www.elastic.co/guide/en/logstash/current/logstash-config-for-filebeat-modules.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Structure of a Logstash config file**: [https://www.elastic.co/guide/en/logstash/current/configuration-file-structure.html](https://www.elastic.co/guide/en/logstash/current/configuration-file-structure.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Filebeat installation process**: [https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-installation.html](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-installation.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metricbeat installation overview and details**: [https://www.elastic.co/guide/en/beats/metricbeat/current/metricbeat-installation.html](https://www.elastic.co/guide/en/beats/metricbeat/current/metricbeat-installation.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this deployment, we will be using Elasticsearch version 6.5 (which is the
    latest version at the time of writing). This means that all subsequent components
    must be the same version. The base OS will be CentOS 7.6\. While this specific
    deployment will be implemented on a local **virtual machine** (**VM**) setup,
    the concepts can still be applied to the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch will be deployed using 2 nodes on 2 vCPU VMs with 4 GB of RAM
    each (in [Chapter 11](2bc754cd-e146-4fbf-b874-d2a80bf471ba.xhtml), *Designing
    an ELK Stack*, we established that the minimum RAM required is about 2.5 GB).
    The underlying storage for the VMs is **non-volatile memory express** (**NVMe**),
    so some considerations need to be taken when replicating the setup somewhere else.
    In terms of space, the Elasticsearch nodes will have 64 GB of disk space each;
    the nodes will have the 64 GB disk mounted to the `/var/lib/elasticsearch` directory.
  prefs: []
  type: TYPE_NORMAL
- en: Logstash and Kibana will be deployed on the same VM using 2 vCPUs and 4 GB of
    RAM. As seen in [Chapter 11](2bc754cd-e146-4fbf-b874-d2a80bf471ba.xhtml), *Designing
    an ELK Stack*, Logstash has a requirement for persistent storage for queues. So,
    for this, we will be using a 32 GB dedicated disk. This disk will be mounted on
    the `/var/lib/logstash` directory for persistent queuing.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can summarize what will be used for the deployment as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The base OS is CentOS 7.6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elasticsearch v6.5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logstash v6.5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kibana v6.5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elasticsearch using 2 nodes on 2 vCPU VMs with 4 GB of RAM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logstash and Kibana on a single VM using 2 vCPUs with 4 GB of RAM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 64 GB disks for the Elasticsearch nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 32 GB disk for the Logstash persistent queue
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the entire implementation and will give you
    an idea of how things are connected:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a387ec77-f6d5-4e5f-9632-2a9751668a69.png)'
  prefs: []
  type: TYPE_IMG
- en: Installing Elasticsearch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Going from nothing to a functional Elasticsearch setup requires the software
    to be installed; this can be done in several ways and on different platforms.
    Some of these installation options are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Installing from the source
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing `deb` for Debian-based Linux Distributions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing `rpm` for **Red Hat Enterprise Linux** (**RHEL**), CentOS, **sound
    library for embedded systems** (**SLES**), OpenSLES, and RPM-based distributions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing `msi` for Windows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying Docker images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For this setup, we will be using the RPM repository for consistency across versions,
    and for simplification purposes when updates are available.
  prefs: []
  type: TYPE_NORMAL
- en: The RPM repository
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To install the RPM repository for RHEL and CentOS, we need to create a file
    in the `/etc/yum.repos.d` directory. Here, the name of the file doesn't matter
    but, in reality, it needs to be meaningful. The contents of the file indicate
    how `yum` will go and search for software.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a file named `/etc/yum.repos.d/elastic.repo` with the following code
    details:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the repository file has been created, simply run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This will refresh the metadata of all of the configured repositories. Before
    installing Elasticsearch, we need to install the OpenJDK version, `1.8.0`; for
    this, we can run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, confirm that `java` is installed, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, you should see something similar to the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then proceed to install `elasticsearch`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Before starting Elasticsearch, some configuration needs to be done.
  prefs: []
  type: TYPE_NORMAL
- en: The Elasticsearch data directory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The default configuration for Elasticsearch has the data directory set to the
    `/var/lib/elasticsearch` path. This is controlled through the `path.data` configuration
    option in the `/etc/elasticsearch/elasticsearch.yml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In this setup, a 64 GB disk will be mounted to this location.
  prefs: []
  type: TYPE_NORMAL
- en: When deploying in Azure, make sure that the `path.data` option is configured
    to use a data disk rather than the OS disk.
  prefs: []
  type: TYPE_NORMAL
- en: Partitioning the disk
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before creating a filesystem, the disk needs to be partitioned. To do this,
    we can use the `parted` utility.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to initialize the disk as `gpt`; for this, we can use the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we create the partition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Here, we're telling `parted` to create a partition from `0GB` to `64GB`, or
    from the beginning of the disk to the end. Additionally, we're using an `xfs`
    signature, since that is the filesystem that is going to be used for the data
    directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we verify that the partition has been successfully created with the
    correct boundaries by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should be similar to the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Formatting the filesystem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To be able to store data on the newly created partition, we first need to create
    a filesystem. For this setup, we will be using the XFS filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: 'To format the disk, run the `mkfs.xfs` command, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: By default, XFS uses a 4K block size that matches the memory page size; this
    is also ideal for relatively small files.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the partition of the device file is specified rather than the entire
    disk. While it is possible to use the disk itself, it is recommended that you
    create filesystems on partitions. Additionally, if the filesystem is going to
    be used on a RAID setup, then changing the stripe unit and stripe size generally
    helps with performance.
  prefs: []
  type: TYPE_NORMAL
- en: Persistent mounting using fstab
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that the filesystem has been created, we need to make sure that it is mounted
    after every reboot at the correct location.
  prefs: []
  type: TYPE_NORMAL
- en: As a general rule, mounting the filesystem using the device file is not advised,
    especially in the cloud. This is because the disk order might change, causing
    the device file of the disks to be mixed up. To work around this problem, we can
    use the UUID of the disk, which is a unique identifier that will persist even
    when the disk is moved to another system.
  prefs: []
  type: TYPE_NORMAL
- en: 'To obtain the UUID of the disk, run the `blkid` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, `/dev/sdb1` is the 64 GB disk that we will be using for Elasticsearch.
    With the UUID, we can add it to the `/etc/fstab` file, which controls the filesystems
    that will be mounted during boot time. Simply edit the file and add the following
    entries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are some important details to take note of from the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '`nobarrier`: This helps with write performance as it disables the mechanism
    used by XFS to acknowledge writes once they hit persistent storage. This is usually
    used on physical storage systems where there is no form of battery backup write
    cache.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`noatime`: This disables the recording mechanism when a file is accessed or
    modified. When `atime` is enabled, every read will result in a small set of writes,
    since the access times will need to be updated. Disabling can help with reads
    as it doesn''t generate any unnecessary writes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nofail`: This allows the system to boot normally in the event of the disk
    that is backing the mount point going missing. This is particularly helpful when
    deploying on the cloud were no access to the console exists.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, verify that the disk has been mounted to the correct location before
    starting the Elasticsearch service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, make sure that the correct ownership of the `/var/lib/elasticsearch` directory
    is configured:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Configuring Elasticsearch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before starting the Elasticsearch service, we need to define several parameters
    that control how Elasticsearch behaves. The configuration file is in the YAML
    format and is located on `/etc/elasticsearch/elasticsearch.yml`. Let's explore
    which main parameters need to be changed.
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch YAML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The central control for Elasticsearch is done through the `/etc/elasticsearch/elasticsearch.yml` file,
    which is in the YAML format. The default configuration file is reasonably well-documented
    and explains what each parameter controls, but there are some entries that should
    be changed as part of the configuration process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main parameters to look for are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Cluster name
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discovery settings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Node name
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network host
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Path settings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cluster name
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Elasticsearch nodes will only be able to join a cluster when they have the
    same cluster name specified in their configuration. This is handled through the
    `cluster.name` parameter; for this setup, we will use `elastic-cluster`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This setting should be configured on both nodes so that they have the same value.
    Otherwise, the second node will not be able to join the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Discovery settings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The discovery parameters control how Elasticsearch manages intra-node communication
    that is used for clustering and master election.
  prefs: []
  type: TYPE_NORMAL
- en: The two main parameters regarding discovery are `discovery.zen.ping.unicast.hosts`
    and `discovery.zen.minimum_master_nodes`.
  prefs: []
  type: TYPE_NORMAL
- en: The `discovery.zen.ping.unicast.hosts` setting controls which nodes are going
    to be used for clustering. Since two nodes will be used in our setup, the configuration
    for `node1` should have the DNS name of `node2`, while `node2` should have the
    DNS of `node1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `discovery.zen.minimum_master_nodes` setting controls the minimum number
    of master nodes in the cluster; this is used to avoid split-brain scenarios where
    there''s more than one master node that is active in the cluster. The number for
    this parameter can be calculated based on a simple equation, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6f0843de-eade-4bc1-ae57-eac32164ff77.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *N* is the number of nodes in the cluster. For this setup, since only
    `2` nodes are to be configured, the setting should be `2`. Both parameters should
    be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'For `node2`, change `discovery.zen.ping.unicast.hosts: ["elastic2"]` to `discovery.zen.ping.unicast.hosts:
    ["elastic1"]`.'
  prefs: []
  type: TYPE_NORMAL
- en: Node name
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By default, Elasticsearch uses a randomly-generated UUID for its node name,
    which is not very user-friendly. This parameter is relatively simple as it controls
    the name for the specific node. For this setup, we''ll be using `elasticX`, where
    `X` is the node number; `node1` should be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Change `node2` to match the naming convention, so it is `elastic2`.
  prefs: []
  type: TYPE_NORMAL
- en: Network host
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This controls which IP address Elasticsearch will bind to and listen to requests.
    By default, it binds to the loopback IP address; this setting needs to be changed
    to allow other nodes from a cluster or allow Kibana and Logstash on other servers
    to send requests. This setting also accepts special parameters, such as the network
    interface. For this setup, we'll have Elasticsearch listen to all the addresses
    by setting the `network.host` parameter to `0.0.0.0`.
  prefs: []
  type: TYPE_NORMAL
- en: 'On both nodes, make sure that the setting is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Path settings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, the path parameters control where Elasticsearch stores its data and
    its logs.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, it is configured to store data under `/var/lib/elasticsearch`,
    and logs under `/var/log/elasticsearch`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: One crucial aspect of this parameter is that, under the `path.data` setting,
    multiple paths can be specified. Elasticsearch will use all the paths specified
    here to store data, thus increasing the overall performance and available space.
    For this setup, we'll leave the defaults as they were in the preceding steps,
    where we mounted a data disk under the `/var/lib/elasticsearch` directory.
  prefs: []
  type: TYPE_NORMAL
- en: Starting Elasticsearch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we've configured Elasticsearch, we need to make sure that the service
    starts automatically and correctly during boot.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start and enable the Elasticsearch service, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, verify that Elasticsearch started correctly by running the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should be similar to the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Adding an Elasticsearch node
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, we can add the second node to the Elasticsearch cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The same configuration should be applied to the previous steps, making sure
    that the settings are changed to reflect the DNS name for `node2`.
  prefs: []
  type: TYPE_NORMAL
- en: To add the node to the cluster, all we need to do is simply start the Elasticsearch
    service.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the service starts, messages are logged to `/var/log/elasticsearch`, which
    indicates that the node was successfully added to the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'You can use the following code to confirm that the cluster is up and running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should be similar to the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: For any subsequent nodes that need to be added to the cluster, the previous
    steps should be followed, making sure that the `cluster.name` parameter is set
    to the correct value.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Logstash and Kibana
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the Elasticsearch cluster up and running, we can now go ahead and install
    Logstash and Kibana.
  prefs: []
  type: TYPE_NORMAL
- en: The repository that was used in the previous steps is the same for the remaining
    components. So, the same process that was used before to add the repository should
    be applied to the Logstash and Kibana node.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a summary, the same process has been explored before:'
  prefs: []
  type: TYPE_NORMAL
- en: Add the repository to `/etc/yum.repos.d/elastic.repo`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the `yum` cache to `sudo yum makecache`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install Logstash and Kibana using `sudo yum install logstash kibana`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the disk for `/var/lib/logstash` and `sudo parted /dev/sdX mklabel
    gpt`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the `sudo parted /dev/sdX mkpart xfs 0GB 32GB` partition (note that this
    is a 32 GB disk)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the `sudo mkfs.xfs /dev/sdX1` filesystem
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update `fstab`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Update the `sudo chown logstash: /var/lib/logstash` directory permissions'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The Logstash `systemd` unit is not added by default; to do so, run the script
    provided by Logstash:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, one specific component that is required is a coordinating Elasticsearch
    node. This will serve as a load balancer for the Elasticsearch cluster that is
    used by Kibana to install Elasticsearch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: More information on the coordinating node configuration is provided in the *Configuring
    Kibana* section.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Logstash
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to Elasticsearch, the main configuration file for Logstash is located
    under `/etc/logstash/logstash.yml`, and some settings will need to be changed
    to achieve the desired functionality.
  prefs: []
  type: TYPE_NORMAL
- en: Logstash YAML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, the `node.name` parameter should be adjusted so that it identifies the
    Logstash node correctly. By default, it uses the machine's hostname as the `node.name`
    parameter. However, since we are running both Logstash and Kibana on the same
    system, it is worth changing this setting to avoid confusion.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we need to consider the queuing settings; these control how Logstash manages
    the type of queues and where it stores queue data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first setting is `queue.type`, which defines the type of queue that is
    used by Logstash. For this setup, we are using persistent queuing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Since queuing is set to persistent, the events need to be stored in a temporary
    location before being sent to Elasticsearch; this is controlled by the `path.queue`
    parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: If left by default, Logstash will use the `path.data/queue` directory to store
    events in the queue. The `path.data` directory defaults to `/var/lib/logstash`,
    which is where we configured the 32 GB disk; this is the desired configuration.
    If another location needs to be specified for queuing, this setting should be
    adjusted to match the correct path.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last setting to be changed in the `logstash.yml` file is the `queue.max_bytes`
    setting, which controls the maximum space that is allowed for the queue. For this
    setup, since we added a dedicated 32 GB disk for only this purpose, the setting
    can be changed to 25 GB to allow for a buffer if more space is needed. The setting
    should look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: As an option, the `xpack.monitoring.enabled` setting can be set to true to enable
    monitoring through Kibana.
  prefs: []
  type: TYPE_NORMAL
- en: Make sure that the parameters in the `yaml` file don't have a space at the beginning
    of the line or it might fail to load the configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Logstash pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Logstash outputs are controlled by the pipelines that are configured through
    files placed under `/etc/logstash/conf.d/`; these files control how Logstash ingests
    data, processes it, and then returns it as an output for Elasticsearch. A pipeline
    configuration is similar to the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Here, the `input` section defines which data to accept and from which source;
    in this setup, we will be using `beats` as an input. The filter section controls
    how data is transformed before being sent to the output, and the output section
    defines where the data is sent. In this case, we will be sending data to the Elasticsearch
    nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a configuration file for `syslog` messages to be filtered by
    Logstash, and then be sent to the Elasticsearch cluster. The file needs to be
    placed in `/etc/logstash/conf.d`, since the input will be from the `beats` module;
    let''s call it the `beats-syslog.conf` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The file''s contents is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Make sure that the `output` section has the DNS names or IPs of the Elasticsearch
    nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: In this pipeline configuration, the `beats` module sends logs to the Logstash
    node. Then Logstash will process the data and load balance the output between
    the Elasticsearch nodes. We can now go ahead and configure Kibana.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Kibana
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last piece of the Elastic Stack is Kibana; the configuration is handled
    by `/etc/kibana/kibana.yml` in a similar way to Elasticsearch and Logstash.
  prefs: []
  type: TYPE_NORMAL
- en: Kibana YAML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By default, Kibana listens on port `5601`; this is controlled by the `server.port`
    parameter, which can be changed if there's a need to access Kibana on a different
    port. For this setup, the default will be used.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `server.host` setting controls which addresses Kibana will listen to for
    requests. Since access is needed from external sources (that is, other than `localhost`),
    we can use the following setting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The `server.name` parameter defaults to the hostname where Kibana runs, but
    since Logstash is running alongside Kibana, we can change this to identify the
    Kibana part:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Finally, `elasticsearch.url` specifies which Elasticsearch node Kibana will
    connect to. As we mentioned previously, we will be using an Elasticsearch coordinate
    node to act as a load balancer between the other two nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the URL of the Elasticsearch instance to use for all your queries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The coordinating node
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A coordinating node is an Elasticsearch node that does not accept inputs, does
    not store data, nor does it take part in master or slave elections.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of this node is to load balance requests for Kibana between the different
    Elasticsearch nodes on the cluster. The process of installing is the same as the
    one we used before, that is making sure that Java (open JDK) is also installed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The configuration will be different as we want to achieve a number of things:'
  prefs: []
  type: TYPE_NORMAL
- en: Disable the master node role
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disable the ingest node role
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disable the data node role
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disable cross-cluster search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To do this, we need the following settings on the `/etc/elasticsearch/elasticsearch.yml`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Starting Logstash and Kibana
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With all of the components already configured, we can start Logstash, Kibana,
    and the coordinating Elasticsearch node.
  prefs: []
  type: TYPE_NORMAL
- en: 'Logstash can be started first as it doesn''t require any of the other components
    to be up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can start and enable the `elasticsearch` coordinating node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Last but not least, `kibana` can go through the same procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'To verify it all started correctly, point your browser to the `kibana` address
    on port `5601` `http://kibana:5601`. Click on Monitoring, and then click on Enable
    monitoring; after a couple of seconds, you will see something similar to the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2668b7d3-4572-4ca6-89db-46168cb206fd.png)'
  prefs: []
  type: TYPE_IMG
- en: You should see all the components online; the **yellow** status is due to system
    indexes that are not replicated, but this is normal.
  prefs: []
  type: TYPE_NORMAL
- en: With this, the cluster is up and running and ready to accept incoming data from
    logs and metrics. We will be feeding data to the cluster using Beats, which we'll
    explore in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: What are Beats?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Beats are the lightweight data shippers from Elastic.co (the company behind
    Elasticsearch). Beats are designed to be easy to configure and run.
  prefs: []
  type: TYPE_NORMAL
- en: Beats are the client part of the equation, living on the systems that are to
    be monitored. Beats capture metrics, logs, and more from servers across the environment
    and ship them to either Logstash for further processing or Elasticsearch for indexing
    and analysis.
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple official Beats (which are developed and maintained by Elastic),
    and a multitude of open source Beats have been developed by the community.
  prefs: []
  type: TYPE_NORMAL
- en: The main Beats that we'll be using for this setup are **Filebeat** and **Metricbeat**.
  prefs: []
  type: TYPE_NORMAL
- en: Filebeat
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Filebeat function collects logs from sources (such as syslog, Apache, and
    Nginx), and then ships these to Elasticsearch or Logstash.
  prefs: []
  type: TYPE_NORMAL
- en: The Filebeat client needs to be installed in each of the servers that require
    data collection in order to be enabled. This component allows the logs to be sent
    to a centralized location for seamless search and indexing.
  prefs: []
  type: TYPE_NORMAL
- en: Metricbeat
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Metricbeat collects metrics, such as CPU usage, memory usage, disk IO statistics,
    and network statistics, and then ships them to either Elasticsearch or Logstash.
  prefs: []
  type: TYPE_NORMAL
- en: There's really no need to transform metric data further, so feeding data directly
    to Elasticsearch makes more sense.
  prefs: []
  type: TYPE_NORMAL
- en: Metricbeat should be installed in all systems that require monitoring of resource
    usage; having Metricbeat installed on the Elasticsearch nodes allows you to keep
    a closer control on resource usage to avoid problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other Beats exist, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Packetbeat**: For network traffic monitoring'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Journalbeat**: For `systemd` journal logs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Auditbeat**: For audit data such as logins'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, Beats can be further adapted to suit a specific need through the
    use of modules. As an example, Metricbeat has a module to collect MySQL performance
    statistics.
  prefs: []
  type: TYPE_NORMAL
- en: Let's not skip a beat – installing Beats
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The installation of the Beats provided by Elasticsearch can be done through
    the Elastic repository that was previously used to install Elasticsearch, Logstash,
    and Kibana.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s install Filebeat on one of the Elasticsearch nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Once installed, confirm that it has completed by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should be similar to the following command block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'To install `metricbeat`, the process is the same as it lives in the same repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: To install Beats on other clients, simply add the Elastic repository as we previously
    explained and install it through `yum`. Beats are also provided as standalone
    packages in case there is no repository available for the distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Beats clients
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With both Filebeat and Metricbeat installed on one of the Elasticsearch nodes,
    we can go ahead and configure them to feed data to both Logstash and Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: Filebeat YAML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, it is no surprise that most of the Elastic components are configured through
    YAML files. Filebeat is no exception to that norm, and its configuration is handled
    by the `/etc/filebeat/filebeat.yml` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to tell `filebeat` where to look for the log files that are
    to be shipped to Logstash. In the `yaml` file, this is in the `filebeat.inputs`
    section; change `enabled: false` to `enabled: true`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Filebeat comes embedded with Kibana dashboards for easy visualization of the
    data that''s sent. This allows Filebeat to load the dashboards and then add the
    Kibana address to the `setup.kibana` section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the `dashboards`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: This configuration needs to be done only once for each new Beat installation;
    there is no need to change this setting on further Filebeat installations as the
    dashboards are already loaded.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we are going to be sending data to Logstash, comment out the `output.elasticsearch`
    section; then, uncomment the `output.logstash` section and add Logstash''s details:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll be using the system module for Filebeat to send the output to
    Logstash; to enable this, simply run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, load the index template into `elasticsearch`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, start and enable `filebeat`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'To verify that data is being sent, we can use one of the provided dashboards
    to visualize `syslog` events. On Kibana, go to Dashboard and type `Syslog Dashboard` into
    the search bar; you will see something similar to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9496cac8-0164-4963-aa9f-e1db2d85c59a.png)'
  prefs: []
  type: TYPE_IMG
- en: Kibana Dashboard showing search results for Syslog Dashboard
  prefs: []
  type: TYPE_NORMAL
- en: Metricbeat YAML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Metricbeat follows a similar process to Filebeat, where the `/etc/metricbeat/metricbeat.yml`
    file needs to edited to send output to Elasticsearch, and the Kibana dashboards
    need to be loaded (that is, they need to be run once).
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, edit the `metricbeat.yml` file to allow Metricbeat to load the
    Kibana dashboards:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, specify the `Elasticsearch` cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the Kibana `dashboards`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: By default, `metricbeat` has the system module enabled, which will capture statistics
    for CPU, system load, memory, and network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start and enable the `metricbeat` service, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'To confirm that data is being sent to the cluster, go to Discover on the kibana
    screen; then, select the metricbeat-* index pattern and verify that events are
    being sent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/845a23fb-4fe8-4423-a682-6e72b2404c92.png)'
  prefs: []
  type: TYPE_IMG
- en: Events filtered with the metricbeat-* index pattern
  prefs: []
  type: TYPE_NORMAL
- en: Next steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, the cluster is now fully functional. All that is left is to install
    Metricbeat and Filebeat onto the other nodes of the cluster to ensure full visibility
    of the cluster's health and resource usage.
  prefs: []
  type: TYPE_NORMAL
- en: Adding more clients to the cluster is a matter of installing the appropriate
    Beat, depending on what needs to be monitored and which logs need to be indexed.
  prefs: []
  type: TYPE_NORMAL
- en: If load increases on the cluster, a number of options are available—either adding
    more nodes to the cluster to load balance requests or increasing the number of
    available resources for each of the nodes. In certain scenarios, simply adding
    more resources is a more cost-effective solution as it doesn't require a new node
    to be configured.
  prefs: []
  type: TYPE_NORMAL
- en: An implementation such as this one can be used to monitor the performance and
    events of a Kubernetes setup (such as the one described in [Chapter 11](2bc754cd-e146-4fbf-b874-d2a80bf471ba.xhtml), *Designing
    an ELK Stack*). Some of the Beats have specific modules that are used to extract
    data from Kubernetes clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, one enhancement that can be made to this setup to ease configuration
    and maintenance is to have the Beat clients point to the coordinating Elasticsearch
    node to act as a load balancer between the nodes; this avoids having to hardcode
    each of the Elasticsearch nodes in the output configuration for the Beats—only
    a single address is needed.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we went through many steps to configure an Elastic Stack, which
    is a collection of four main components—Elasticsearch, Logstash, Kibana, and Beats.
    For the setup, we used three VMs; we hosted two Elasticsearch nodes, and then,
    on a single system, we installed Logstash and Kibana, using version 6.5 for each
    of the components. We installed Elasticsearch using the RPM repository provided
    by Elastic Stack; `yum` was used to install the required packages. Elasticsearch
    configuration was done using the `elasticsearch.yml` file, which controls how
    `elasticsearch` behaves. We defined a number of settings that are required for
    a functional cluster, such as the `cluster.name` parameter and `discovery.zen.minimum_master_nodes`.
  prefs: []
  type: TYPE_NORMAL
- en: We added a new Elasticsearch node by configuring the cluster name and the discovery
    settings, which allows the node to join the cluster automatically. Then, we moved
    onto installing Kibana and Logstash, which are provided on the same RPM repository
    that was used for Elasticsearch; configuring Logstash and Kibana was done through
    their respective `.yml` files.
  prefs: []
  type: TYPE_NORMAL
- en: Once all three main components were up, and the operation was ready to accept
    incoming data, we moved onto installing Beats, which are the data shippers that
    are used by Elasticsearch and Logstash to ingest data. For logs and events, we
    used Filebeat, and for system metrics such as memory usage and CPU, we used Metricbeat.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about the challenges of systems management
    and Salt's architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How can Elasticsearch be installed?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you partition a disk?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can you persistently mount a filesystem?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which file controls Elasticsearch configuration?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does the `cluster.name` setting do?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the recommended number of nodes in an Elasticsearch cluster?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can an Elasticsearch node be added to an existing cluster?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What process is needed to install Logstash and Kibana?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is persistent queuing?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a coordinating node?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are Beats?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is Filebeat used for?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '***Fundamentals of Linux* by Oliver Pelz**: [https://www.packtpub.com/networking-and-servers/fundamentals-linux](https://www.packtpub.com/networking-and-servers/fundamentals-linux)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
