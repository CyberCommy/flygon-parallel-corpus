- en: Building for Large-Scale Request Handling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In an enterprise environment, as the number of users grow, it is normal for
    the number of users who try to access the web application at the same time to
    also grow. This presents us with the interesting problem of how to scale the web
    application to handle a large number of concurrent requests by the users.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling up a web application to handle a large number of users is a task that
    can be achieved in multiple ways where one of the simplest ways can be adding
    more infrastructure and running more instances of the application. However, this
    technique, though simple, is highly burdensome on the economics of application
    scalability, since the infrastructure costs associated with running the application
    at scale can be huge. We certainly need to craft our application in such a way
    that it is easily able to handle a lot of concurrent requests without really requiring
    frequent infrastructure scaling.
  prefs: []
  type: TYPE_NORMAL
- en: Building on the foundation laid out in the previous chapter, we will see how
    we can apply these techniques to build a scalable application that can handle
    a large number of concurrent requests, while also learning a few other techniques
    that will help us scale the application in an effortless manner.
  prefs: []
  type: TYPE_NORMAL
- en: 'Over the course of the chapter, we will be taking a look at the following techniques
    to scale our web application for large-scale request handling:'
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing reverse proxies in web application deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using thread pools to scale up request processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the concept of single-threaded concurrent code with Python AsyncIO
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code listings in this book can be found under `chapter05` directory at [https://github.com/PacktPublishing/Hands-On-Enterprise-Application-Development-with-Python.](https://github.com/PacktPublishing/Hands-On-Enterprise-Application-Development-with-Python)
  prefs: []
  type: TYPE_NORMAL
- en: 'The code samples can be cloned by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: For successful execution of the code sample, the python-`virtualenv` package
    needs to be present.
  prefs: []
  type: TYPE_NORMAL
- en: The problems of accommodating increased concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Over the years, in the time the internet has been around, one of the most common
    problems that web application architects have commonly faced is how to deal with
    the increasing concurrency. As more and more users are coming online and utilizing
    web applications, there is a huge need to scale up infrastructures to manage all
    these requests.
  prefs: []
  type: TYPE_NORMAL
- en: This stands true even for our enterprise web applications. Even though we can
    make an estimate of how many users could be concurrently accessing these web applications
    inside an enterprise, there is no hard and fast rule that will be true for the
    time to come. As the enterprise grows, the number of clients accessing the application
    will also increase, putting more stress upon the infrastructure and increasing
    the need to scale it out. But what options do we have, while trying to scale out
    the application to accommodate the increasing number of clients? Let's take a
    look.
  prefs: []
  type: TYPE_NORMAL
- en: The multiple options to scale up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The world of technology provides a lot of options to scale up the application
    to accommodate the ever increasing user base; some of these options simply ask
    for increasing the hardware resources whereas the other options require the application
    to be built around dealing with multiple requests internally itself. Most of the
    time, the options of scaling fall into two major categories, vertical scaling,
    and horizontal scaling:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/093dd96f-4d4a-4e41-984c-d15d16a27c21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s take a look at both of them and figure out their pros and cons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Vertical scaling**: The whole concept of vertical scaling is based upon the
    fact of adding more resources to the existing ...'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Engineering the application for scalability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At a time when most of the enterprise projects resort to using one framework
    or another, which usually decides how the application will be served during the
    production phase, it is still a good idea to take a look beneath the surface and
    understand how to develop the application while keeping the scalability of the
    application in perspective.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will take a look at the different techniques that can help
    us build a scalable application, even when we are not using some per-built framework
    which can do it for us. During the course of this section, we will see how we
    can use thread/process pooling to handle multiple clients at the same time, and
    why the pooling of resources is necessary and what prevents us from starting a
    separate thread or process for dealing with every other incoming request.
  prefs: []
  type: TYPE_NORMAL
- en: But before we dive into the concepts of how we can utilize the thread pooling
    or process pooling in the application development, let's first take a look at
    a simple way through which we can hand-off the processing of the incoming requests
    to a background thread.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code implements a simple socket server which first accepts an
    incoming connection and then hands it off to a background thread for reads and
    writes, hence freeing the main thread to accept the other incoming connections:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In this code, we have implemented a simple `Server` class which initializes
    a TCP-based server on the machine, ready to accept the incoming connections. Without
    diverting too much, let's try to focus on the important aspect of this code, where
    we start the listening loop of the server under the `listen()` method.
  prefs: []
  type: TYPE_NORMAL
- en: Under the `listen()` method, we first call the `listen()` method of the socket
    and tell it that it can queue up, at most, 10 connections which have not been
    accepted. Once this limit is reached, any further client connection will be rejected
    by the server. Now, moving on from here, we start an infinite loop where the first
    call is made to the `accept()` method of the socket. The call to the `accept()` method
    blocks until a client attempts to make a connection. On a successful attempt,
    the `accept()` call returns the client connection socket and the client address.
    The client connection socket can be used to perform I/O operations with the client.
  prefs: []
  type: TYPE_NORMAL
- en: 'The fun part happens next: as soon as the client connection is accepted, we
    launch a daemon thread responsible for handling the communication with the client
    and hand-off the client connection socket to the thread. This essentially frees
    up our main thread from dealing with the I/O of the client socket, and hence,
    our main thread can now accept more clients. This process continues for every
    other client that connects to our server.'
  prefs: []
  type: TYPE_NORMAL
- en: So far so good; we have a nice way through which we can handle the incoming
    clients and our service can scale up gradually as the number of clients increases.
    That was an easy solution, wasn't it? Well, apparently during the course of coming
    up with this solution, we have ignored a major flaw in the process. The flaw lies
    in the fact that we have not implemented any kind of control related to how many
    threads can be launched by the application for dealing with the incoming clients.
    Imagine what will happen if a million clients try to connect to our server? Will
    we be really running a million threads at the same time? The answer is a big NO.
  prefs: []
  type: TYPE_NORMAL
- en: But why isn't it possible? Let's take a look.
  prefs: []
  type: TYPE_NORMAL
- en: Controlling the concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous example, we came up with a problem of why can''t we have a
    million threads, each dealing with an individual client? That should provide us
    with a lot of concurrency and scalability. But, there are a number of reasons
    that really prevent us from running a million threads at the same time. Let''s
    try to take a look at the possible reasons preventing us from scaling our application
    infinitely:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Resource limitations**: Every single client connection that is being handled
    by the server doesn''t come free of cost. With every new connected client, we
    are expending some of the resources of the machine. These may include file descriptors
    that map to a socket, some amount of memory that is used to hold the information
    ...'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using thread pools for handling incoming connections
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we saw in the previous section, we do not need an infinite number of threads
    to handle the incoming clients. We can manage with a limited number of threads
    to handle a large number of clients. But, how do we implement this thread pooling
    in our application. As it turns out, it is quite easy to implement the thread
    pool functionality with Python 3 and the `concurrent.futures` module.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code sample modifies our existing TCP server example to use a
    thread pool, instead of arbitrarily launching an infinite number of threads to
    handle the incoming client connections:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we modified our TCP server code to utilize a thread pool instead
    of launching an arbitrary number of threads. Let's take a look at how we made
    it possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, to utilize the thread pool, we need to initialize an instance of the
    thread pool executor. Under the `__init__` method of the `Server` class, we first
    initialize the thread pool executor by calling its constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The `ThreadPoolExecutor` constructor takes a `max_workers` parameter that defines
    how many concurrent threads are possible inside the `ThreadPool`. But, what will
    be an optimal value for the `max_workers` parameter?
  prefs: []
  type: TYPE_NORMAL
- en: A general rule of thumb will be to have `max_workers` = *(5 x Total number of
    CPU cores)*. The reasoning behind this formula is that inside a web application,
    most of the threads are generally waiting for the I/O to complete, whereas a few
    threads are busy doing CPU-bound operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next thing after we have created a `ThreadPoolExecutor` is to submit jobs
    to it so that they can be processed by the threads inside the Executor Pool. This
    can be achieved through the use of the submit method of the `ThreadPoolExecutor`
    class. This can be seen under the `listen()` method of the `Server` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `submit()` method of the `ThreadPoolExecutor` takes in, as the first parameter,
    the name of the method to execute inside a thread and the parameters that need
    to be passed to the executing method.
  prefs: []
  type: TYPE_NORMAL
- en: 'That was quite simple to implement and provides us with lots of benefits, such
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: Optimal usage of resources provided by the underlying infrastructure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ability to handle multiple requests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increased scalability and reduced wait times for the clients
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One important thing to take a note of here is, since the `ThreadPoolExecutor`
    utilizes the threads, the CPython implementation might not provide the maximum
    performance due to the presence of GIL, which doesn't allow the execution of more
    than one thread at a time. Hence, the performance of the application may vary
    depending upon the underlying Python implementation being used.
  prefs: []
  type: TYPE_NORMAL
- en: Now, the question that arises is, what if we wanted to sidestep the Global Interpreter
    Lock? Is there some mechanism while still using the CPython implementation of
    Python? We discussed this scenario in the previous chapter and settled with the
    use of Python's multiprocessing module in place of the threading library.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, as it turns out, using a `ProcessPoolExecutor` is quite a simple feat
    to achieve. The underlying implementation inside the concurrent.futures package
    takes care of most of the necessities and provides the programmer with a simple-to-use
    abstraction. To see this in action, let''s modify our previous example to swap
    in `ProcessPoolExecutor` in place of the `ThreadPoolExecutor`. To do this, all
    we need to do is first import the correct implementation from the concurrent.futures
    package as described by the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The next thing we need to do is to modify our `__init__` method to create a
    process pool instead of a thread pool. The following implementation of the `__init__`
    method shows how we can achieve this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Indeed, that was a simple process to carry out and now our application can use
    the multiprocess model instead of the multithread model.
  prefs: []
  type: TYPE_NORMAL
- en: But, can we keep the pool size the same or does it also need to change?
  prefs: []
  type: TYPE_NORMAL
- en: Every process has its own memory space and internal pointers that it needs to
    maintain, which makes the process heavier in comparison to the use of threads
    for achieving concurrency. This provides a reason to reduce the pool size so as
    to allow for the heavier usage of the underlying system resources. As a general
    rule, for a `ProcessPoolExecutor`, the `max_workers` can be calculated by the
    formula `max_workers` = *(2 x Number of CPU Cores + 1)*.
  prefs: []
  type: TYPE_NORMAL
- en: The reasoning behind this formula can be attributed to the fact that, at any
    given time, we can assume that half of the processes will be busy in performing
    network I/O while the others might be busy doing CPU-intensive tasks.
  prefs: []
  type: TYPE_NORMAL
- en: So, now we have a fair enough idea about how we can use a resource pool and
    why it is a better approach in comparison to launching an arbitrary number of
    threads. But, this approach still requires a lot of context switches and is also
    highly dependent upon the underlying Python implementation being used. But there
    should be something better than this, for sure.
  prefs: []
  type: TYPE_NORMAL
- en: With this in mind, let's try to venture into another territory in the kingdom
    of Python, the territory of asynchronous programming.
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous programming with AsyncIO
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we dive into this unknown territory of asynchronous programming, let's
    first try to recall why we used threads or multiple processes.
  prefs: []
  type: TYPE_NORMAL
- en: One of the main reasons to use threads or multiple processes was to increase
    the concurrency and, as a result, the ability of the application to handle a higher
    number of concurrent requests. But this came at a cost of increased resource utilization,
    and the limited ability to run multiple threads or the launching of heavier processes
    to accommodate higher concurrency with complex mechanisms of implementing locks
    between the shared data structures.
  prefs: []
  type: TYPE_NORMAL
- en: Now, in the context of building a scalable web application, we also have a few
    major differences from a general purpose ...
  prefs: []
  type: TYPE_NORMAL
- en: AsyncIO terminology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we recently discussed, the support for asynchronous programming in Python
    is implemented through the use of an event loop and co-routines. But what exactly
    are they? Let''s take a look:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/da23e7db-4f0e-46af-b4c4-77f78570c732.png)'
  prefs: []
  type: TYPE_IMG
- en: Event loop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An event loop, as its name implies, is a loop. What this loop does is, when
    a new task is supposed to be executed, the event loop queues this task. Now from
    here, the control shifts to the event loop. When the event loop runs, it checks
    whether there is some task in its queue or not. If there is a task present, the
    control switches to the task.
  prefs: []
  type: TYPE_NORMAL
- en: Now, here is the interesting part in the context of the asynchronous execution
    of tasks. Suppose there are two tasks, namely Task A and Task B, in the queue
    of the event loop. When the event loop starts executing, it checks the status
    of the task queue it has. The event queue finds out that there are tasks in its
    queue. So, the event queue picks up Task A. Now a context switch happens ...
  prefs: []
  type: TYPE_NORMAL
- en: Co-routines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Co-routines in Python AsyncIO provide a lightweight mechanism of executing multiple
    simultaneous operations. The co-routines are implemented as a special use case
    of generators in Python. So, before we dive into understanding what co-routines
    are, let's spend a little time on understanding the generators.
  prefs: []
  type: TYPE_NORMAL
- en: In general terms, generators are those functions which generate some value.
    However, that is what every other function does, so how does a generator differ
    from a regular function. The difference lies in how the life cycle of a general
    function differs from a generator. When we call a function, it produces some value,
    returns it, and the scope of the function is destroyed once the call moves out
    of the function body. When we call the function again, a new scope is generated
    and executed.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to this, when we call a generator, the generator can return a value
    and then goes into a paused state and the control transfers back to the caller.
    At this time, the scope of the generator is not destroyed and it can pick up the
    generation of values from where it previously left. This basically provides us
    with a function through which we can pull or yield some values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code sample shows how to write a simple generator function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The interesting part here is that a generator won't continue to provide you
    with the next result by simply calling the generator again and again. For yielding
    new results, we need to use the `next()` method on the generator. This allows
    us to yield new results from the generator.
  prefs: []
  type: TYPE_NORMAL
- en: Now, co-routines implement a special use case of generator in which they can
    not only yield new results, but can also take in some data. This is made possible
    with a combination of yield and the `send()` method of the generators.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code sample shows the implementation of a simple co-routine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Since co-routines allow for the pausing and resuming of functions, and hence
    the lazy generation of the results, that makes it a good option for the use case
    of asynchronous programming, where the tasks are frequently sent into the blocking
    state and are then resumed from there once their operation completes.
  prefs: []
  type: TYPE_NORMAL
- en: Tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A task in Python AsyncIO is a mechanism to wrap the co-routines. Every task
    has a result associated with it, that may be generated immediately or may be deferred
    depending upon the kind of task. This result is known as the Future.
  prefs: []
  type: TYPE_NORMAL
- en: In AsyncIO, a task is a subclass of the Future which wraps around a co-routine.
    When a co-routine has finished generating the values, the task returns and is
    marked as complete by the event loop and is hence removed from the task queue
    of the event queue.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have a fair enough idea of the terminology associated with the use of
    Python AsyncIO. Let's now dive into some action and write a simple program to
    understand how the Python AsyncIO really works.
  prefs: []
  type: TYPE_NORMAL
- en: Writing a simple Python AsyncIO program
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It's time to buckle up and start taking a dive into the world of asynchronous
    programming with Python and to understand how the AsyncIO really works.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code implements a simple URL fetcher using the Python requests
    library and AsyncIO:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: That was a small and a nice asynchronous program implementing the Python AsyncIO
    library. Now, let's spend some time understanding what we did here.
  prefs: []
  type: TYPE_NORMAL
- en: Starting from the top, we have imported the Python requests library to make
    web requests from our Python code and have also imported the Python's AsyncIO
    library.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we define a co-routine named `fetch_url`. The general syntax of defining
    a co-routine for AsyncIO requires the use of the `async` keyword:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The next in line is the definition of another co-routine named `get_url`. What
    we do inside the `get_url` routine is make a call to our other co-routine, `fetch_url`,
    which does the actual fetch of the URL.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since `fetch_url` is a blocking co-routine, we proceed the call to `fetch_url`
    with the `await` keyword. This signifies that this method can be suspended until
    the results are obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Next in the program is the definition of the `process_results` method. We use
    this method as a callback to process the results from the `get_url` method once
    they arrive. This method takes a single parameter, a `future` object, which will
    contain the results of the function call to the `get_url`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inside the method, the results of the future can be accessed through the use
    of the `results()` method of the `future` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: With this, we have all the basic machinery set up for the execution of the AsyncIO
    event loop. Now, it's time to implement a real event loop and submit a few tasks
    to it.
  prefs: []
  type: TYPE_NORMAL
- en: We start this by first fetching an AsyncIO event loop by making a call to the `get_event_loop()`
    method. The `get_event_loop()` method returns the optimal event loop implementation
    of AsyncIO for the platform on which the code is running.
  prefs: []
  type: TYPE_NORMAL
- en: AsyncIO implements multiple event loops which a programmer can use. Usually
    a simple call to `get_event_loop()` will return the best event loop implementation
    for the system the interpreter is running on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have the loop created, we now submit a few tasks to the event loop
    through the use of the `create_task()` method. This adds the tasks to the queue
    of the event loop to execute. Now, since these tasks are asynchronous and we don''t
    have a clue about which task will produce the results first, we need to provide
    a callback to handle the results of the task. To achieve this, we add a callback
    to the tasks with the help of the tasks `add_done_callback()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Once everything here is set, we start the event loop into a `run_forever` mode
    so that the event loop keeps on running and dealing with the new tasks.
  prefs: []
  type: TYPE_NORMAL
- en: With this, we have completed the implementation of a simple AsyncIO program.
    But hey, we are trying to build a enterprise scale application. What if I wanted
    to build an enterprise web application with AsyncIO?
  prefs: []
  type: TYPE_NORMAL
- en: So, now let's take a look at how we can use AsyncIO to implement a simple asynchronous
    socket server.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a simple socket server with AsyncIO
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The AsyncIO library provided by the Python implementation provides a lot of
    powerful functionality. One of these many functionalities is the ability to interface
    and manage socket communication. This provides the programmer with the ability
    to implement asynchronous socket handling and, hence, allows for a higher number
    of clients to connect to the server.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code sample builds a simple socket handler with the callback-based
    mechanism to handle the communication with the clients:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Boosting the application concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of the time when we are building some web application through a framework,
    the frameworks usually provide a small and easy to run web server. Although these
    servers are good for use in the development environment to quickly realize the
    changes and debug through the issues inside the application during the development
    stage, these servers are not capable of handling the production workloads.
  prefs: []
  type: TYPE_NORMAL
- en: 'Even in the case when the whole application has been developed from scratch,
    it is generally a good idea to proxy the communication to the web application
    through use of a reverse proxy. But the question arises is, why do we need to
    do so? Why shouldn''t we just run the web application directly and let it handle
    the incoming requests. Let''s quickly go through all the responsibilities the
    web application serves:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Handling of incoming requests**: When a new request arrives at the web application,
    the web application might need to decide what to do with that request. If the
    web application has workers that can process the request, the application will
    accept the request, hand it over to a worker, and return the response for the
    request, once the worker finishes processing. If there is no worker, then the
    web application has to queue this request for later processing. In the worst case,
    when the queue backlog has exceeded the threshold of the maximum number of queued
    clients, then the web application has to reject the request.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Serving static resources**: If the web application needs to generate dynamic
    HTML pages, it may also double up as a server to send across the static resources
    such as CSS, Javascript, images, and so on, hence increasing the load.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Handling encryption**: Most of the web applications now come up with the
    encryption turned on. In this case, our web application will also require us to
    manage the parsing of the encrypted data and provide a secure connection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Those are quite some responsibilities to be handled by a simple web application
    server. What we rather need is a mechanism through which we can offload quite
    a lot of these responsibilities from the web application server and let it handle
    only the essential work that it is supposed to do and where it really shines.
  prefs: []
  type: TYPE_NORMAL
- en: Running behind a reverse proxy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So, our first line of action to improve the ability of our web application
    to handle a lot of clients, is to first take a few responsibilities off its shoulders.
    A simple option that comes to mind in order to achieve this, is to first start
    running the web application behind a **Reverse Proxy**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/86e65d54-88df-4f22-a6f3-ae0ed9b7e2a1.png)'
  prefs: []
  type: TYPE_IMG
- en: So, what essentially does a **Reverse Proxy** do? The way the reverse proxy
    works is, when a **Client** request arrives at the **Web Application Server**,
    the **Reverse Proxy** intercepts the request. Based on the rules defined to match
    the request to the appropriate backend application, the **Reverse Proxy** then
    forwards this request to the ...
  prefs: []
  type: TYPE_NORMAL
- en: Improved security
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the first advantages that comes to the mind when considering the use
    of a reverse proxy is the improved security. This happens because now we can run
    our web application behind the firewall so that it cannot be accessed directly.
    The reverse proxy intercepts the request and forwards it to the application without
    letting the user know what is going on behind the scenes with the request that
    they made.
  prefs: []
  type: TYPE_NORMAL
- en: This restricted access to the web application helps in reducing the attack surface
    that can be utilized by a malicious user to break into the web application, and
    access or modify the critical records.
  prefs: []
  type: TYPE_NORMAL
- en: Improved connection handling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A reverse proxy server can also be used to improve the connection handling capability
    of the web application. Nowadays, to speed up the fetching of the remote content,
    the web browsers open multiple connections to a web server to increase the parallel
    download of the resources. The reverse proxy can queue up and serve the connection
    requests as the web application is processing the pending requests, hence improving
    the connection acceptance and reducing the load on the application to manage the
    connection states.
  prefs: []
  type: TYPE_NORMAL
- en: Resource caching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When the web application generates a response to a particular client request,
    there is a chance that the same kind of request may arrive again, or the same
    resource may be requested again. For every similar request, using the web application
    to generate the response again and again may turn out to be a not so elegant solution.
  prefs: []
  type: TYPE_NORMAL
- en: The reverse proxies can at times help understand the request and response patterns
    and implement caching for them. When caching is enabled, when the similar request
    arrives again or the same resource is requested again, the reverse proxy, instead
    of forwarding the request to the web application can send back the cached response
    directly, hence offloading a lot of overhead from the web application. This results
    in the improved performance of the web application and a shorter response time
    for the clients.
  prefs: []
  type: TYPE_NORMAL
- en: Serving static resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of the web applications have two kind of resources that they serve. One
    is the dynamic responses generated in accordance to the external input and static
    content that remains the same, such as CSS files, Javascript files, images, and
    so on.
  prefs: []
  type: TYPE_NORMAL
- en: It provides a lot of performance gain as well as improved scalability if we
    can offload either one of these responsibilities from the web application.
  prefs: []
  type: TYPE_NORMAL
- en: The best possibility that we have here is to offload the serving of static resources
    to the clients. A reverse proxy can also double up as a server which can serve
    the static resources to the clients without forwarding these requests to the web
    application server, which dramatically reduces the number of requests waiting
    to ...
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Through the course of this chapter, we got to learn about the different ways
    through which we can build our web application to handle a large number of concurrent
    requests. We started off by understanding and learning about the different scaling
    techniques, such as vertical scaling and horizontal scaling, and learned about
    the different pros and cons of each technique. We then further dived into the
    topics to help us improve the ability of the web application itself to process
    a higher number of requests. This led us to a journey into the use of resource
    pools and why it is a good idea to use resource pooling instead of arbitrarily
    allocating the resources for every new request that arrives at the web application.
    Further on in the journey, we got to know about the asynchronous way of dealing
    with the incoming requests and why the asynchronous mechanism is better suited
    for higher scalability in the case of web applications which are more I/O bound.
    We ended our discussion on scaling the applications for large numbers of clients
    by looking into the use of reverse proxies and what advantages a reverse proxy
    provides to help us scale our web application up.
  prefs: []
  type: TYPE_NORMAL
- en: Now with the understanding of how we can make our application handle a large
    number of concurrent requests, the next chapter will take us through the process
    of building a demo application taking advantage of the different concepts we have
    learned so far in the book so far.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How can we use multiple instances of the same application to serve the incoming
    requests?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we implement process pools and distribute the client requests over them?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can we implement an application which utilizes both process pooling and thread
    pooling? What are the issues we may face while implementing the same?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we implement a basic web server with AsyncIO?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
