- en: Chapter 7. Spring with Hadoop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Processing large chunks of data has been a major challenge in architecting modern
    day web applications. Hadoop is an open source framework from Apache that provides
    libraries to process and store large chunks of data. It offers a scalable, cost-effective,
    and fault-tolerant solution to store and process large chunks of data. In this
    chapter, let us demonstrate how the Spring Framework supports Hadoop. Map and
    Reduce, Hive, and HDFS are some of the Hadoop key terminology used with cloud-based
    technologies. Google has also come with its own Map and Reduce and distributed
    file system framework, apart from Apache Hadoop.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Hadoop modules
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Apache Hadoop consists of the following modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hadoop Common**: This is a common module used by other modules of Hadoop.
    It is like a utility package.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hadoop Distributed File System**: Hadoop Distributed File System can be considered
    when we have to store large amounts of data across various machines or machine
    clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hadoop Yarn**: Think of a scenario where we have many servers on the cloud
    that need to be scheduled to restart or reboot at a particular time by sending
    an e-mail intimation to the tenants. Hadoop Yarn can be used for scheduling resources
    across computers or clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hadoop Map and Reduce**: If we have to process a large set of data, we can
    break it into small clusters and process them as units and merge them back later.
    This can be done with the libraries provided in Apache map and reduce.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spring namespace for Hadoop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Following is the namespace that needs to be used to integrate the Hadoop framework
    with Spring. [http://www.springframework.org/schema/hadoop/spring-hadoop.xsd](http://www.springframework.org/schema/hadoop/spring-hadoop.xsd)
    defines the XSD for Spring-Hadoop, which is normally used in the `application-context.xml`
    file. The XSD details how to configure Hadoop as jobs with Spring Framework.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Hadoop Distributed Files System
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Hadoop Distributed File System** (**HDFS**) is used for storing large
    amounts of data on a distributed file system. HDFS stores metadata and application
    data separately on different servers. The servers used to store metadata are called
    `NameNode` servers. The servers used to store application data are called `DataNode`
    servers. The `NameNode` and `DataNodes` behave in a master-slave architecture.
    Usually, one `NameNode` will have many `DataNodes`. `NameNodes` stores the file's
    namespace, and the file will spilt it into many small chunks across `DataNodes`.
    `DataNodes` usually function as per the instruction from `NameNode` and per functions
    such as block creation, replication, and deletion. So, the major tasks with Hadoop
    will involve interacting with the file system. This may include creating files,
    parsing file for processes, or deleting files.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Hadoop file system can be accessed in a number of ways. We have listed
    a few here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`hdfs`: It uses RPC for communication, and the protocol used is `hdfs://`.
    It requires the client, server, and clusters to have the same versions, else a
    serialization error will occur.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hftp` and `hsftp`: These are HTTP-based, version-independent protocols with
    the prefix `hftp://`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`webhdfs`: This is based on HTTP with REST API and is also version independent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The abstract class `org.apache.hadoop.fs.FileSystem` behaves like an entry point
    into the Hadoop File System implementation. This class has been extended by Spring
    Framework with the subclass `SimplerFileSystem`. This subclass contains all the
    methods that serve file operations, such as copying from one location to another.
  prefs: []
  type: TYPE_NORMAL
- en: Spring Framework comes with a package in Hadoop to handle Hadoop Distributed
    File Systems. The package `org.springframework.data.hadoop.fs` has classes to
    handle the file resources.
  prefs: []
  type: TYPE_NORMAL
- en: '`HdfsResourceLoader` is a class found in Sping''s Hadoop File System package
    and is used to load the resources in Hadoop File System. It has constructors that
    take the configuration object as the input. The `HdfsResourceLoader` constructors
    are shown in the following code snippet. It also has methods to get the resource
    from a path specified and to close the file stream after use.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the following command to configure Spring to use `webhdfs`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'To manually configure the URI and File System ID, the following configuration
    can be given:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Languages such as **Rhino** and **Groovy** have provided Java scripting or use
    Python to do the HDFS configuration. A sample one is shown in the following code.
    The scripts can be configured to run on start up or conditional start up. Two
    script variables that can be used for this configuration are `run-at-start-up`
    and `evaluate`. Scripts can also be configured to get started as tasklets (which
    means as a batch job).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Some of the implicit variables and classes associated with the implicit variables
    are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`hdfsRL-org.springframework.data.hadoop.io.HdfsResourceLoader`: a HDFS resource
    loader (relies on `hadoop-resource-loader` or singleton type match, falls back
    to creating one automatically based on ''`cfg`'').'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`distcp-org.springframework.data.hadoop.fs.DistributedCopyUtil`: Programmatic
    access to `DistCp`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fs-org.apache.hadoop.fs.FileSystem`: a Hadoop File System (relies on ''`hadoop-fs`''
    bean or singleton type match, falls back to creating one based on ''cfg'').'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fsh-org.springframework.data.hadoop.fs.FsShell`: a File System shell, exposing
    hadoop `fs` commands as an API.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HBase
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Apache HBase is mainly the key value storage for Hadoop. It's actually a database
    that is easily scalable and can accommodate millions of rows and columns. It can
    be scaled across hardware and is similar to a NoSQL database. It integrates with
    Map and Reduce and works best with the RESTFUL API. HBase was derived from Google's
    bigdata. It has been used by Netflix, Yahoo, and Facebook. It is also memory intensive,
    since it's meant to handle large amounts of data and has to scale against hardware.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a simple employee table using Eclipse and Hadoop HBase. In Eclipse,
    just add the following JAR files, or if you are using Maven, ensure that the following
    JAR files are updated in Maven''s `pom.xml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '`hbase-0.94.8.jar`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`commons-logging-1.1.1.jar`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`log4j-1.2.16.jar`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`zookeeper-3.4.5.jar`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hadoop-core-1.1.2.jar`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`commons-configuration-1.6.jar`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`common-lang-2.5.jar`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`protobuf-java-2.4.0a.jar`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`slf4j-api-1.4.3.jar`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`slf4j-log4j12-1.4.3.jar`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a `Main` class to with the following code. This class will create an
    employee table with ID and Name as its two columns using the `HbaseAdmin` class.
    This class has methods for creating, modifying, and deleting tables in Hadoop.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: HBase is supported by the Spring Framework, and `factoryBean` is also created
    in a Spring Hadoop package to support it. The `HbaseConfigurationFactoryBean`
    bean is available in the package `org.springframework.data.hadoop.hbase`. The
    `HBaseAccessor` class is an abstract class and has been extended by two subclasses,
    `HbaseTemplate` and `HbaseInterceptors`.
  prefs: []
  type: TYPE_NORMAL
- en: '![HBase](img/B02116_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Spring offers a core class called `HBaseTemplate`. This class is the first point
    of contact for the application when HBase is implemented. This class has all the
    methods to access tables, such as `execute`, `find`, `find all`, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'This class has the following constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the HBase template configuration that can be used in the application''s
    `context.xml` or `Hbasecontext.xml` files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s also look at how `HBaseTemplate` is used for retrieving table information
    with a sample code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Spring also supports AOP integration with Hadoop HBase and has a package to
    handle all the AOP events using `HBaseInterceptors`. This class implements the
    following interfaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '`org.aopalliance.aop.Advice`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`org.aopalliance.intercept.Interceptor`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`org.aopalliance.intercept.MethodInterceptor`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`InitializingBean`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HBaseInterceptors` with `HBaseSynchronizationManager` can be used to bind
    an HBase table to a thread before a method call, or detach from it after a method
    call.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is Spring''s Hadoop HBase configuration for creating an HBase configuration
    object to manage HBase configuration connections:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This is Spring''s Hadoop HBase configuration to manage proxies and connections
    when the application context is null or not available for some reason:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the configuration for a high performance coordination server called
    `ZooKeeper` which is used in Hadoop distributed systems:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also load the properties from the file as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Map and Reduce
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Map and Reduce** is a programming approach that allows a lot of scalability.
    The term "Map and Reduce" implies that we will be using maps to process the data.
    We can see two steps here. The first one is map creation (a map is created with
    a key-value pair), and the second one is reduction, which reads the map created
    in the first step and breaks it into many smaller maps.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s think of a scenario that can be related to Map and Reduce—let''s say
    that we need to get the population of tigers in India and do some work to enhance
    their living conditions so that they don''t go extinct. We may have an average
    figure of the population of tigers. Say that we dispatch people to different states
    and they collect information like this: Karnataka (100), TamilNadu (150), and
    so on. We would then combine these figures into a single figure to get the total
    population of tigers. The mapping of population can be seen as a parallel process
    (mapping job), and combining the result can be seen as a reducing job.'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a configuration object in Spring for Map and Reduce
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The configuration object holds information about the Map and Reduce job. The
    configuration object itself is a bean definition mapped to the class `ConfigurationFactoryBean`,
    with the default name `hadoopConfiguration`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The configuration object can be simply configured as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is another variation of configuring the configuration object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Another variation is to configure Hadoop resources using `java.properties`
    directly in the `configuration` tag, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also use Spring''s property placeholder to externalize the properties,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Map and Reduce jobs with Spring
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Map and Reduce can be scheduled as a job using Spring Framework. Spring Framework
    comes with `spring-data-hadoop` package which supports Map and Reduce. With this,
    we need to ensure that we have the Apache Hadoop core package.
  prefs: []
  type: TYPE_NORMAL
- en: Let us implement a simple scenario of counting the occurrence of each word in
    the input file. Create a simple Maven Java project with the following mentioned
    dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: Dependencies for Maven project
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We need to add these dependencies in the `pom.xml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Apache Hadoop Map and Reduce comes with a mapper class that can be used to create
    maps to solve the problem of reading the contents and storing the occurrence of
    the word with key-value pairs. Each line in the file will be broken into words
    to be stored in maps.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create a custom mapper by extending the `ApacheMapper` class and overriding
    the map method, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The `CustomWordMapper` class does the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Creates an `myword` instance of the `Text()` class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Overrides the `map` method of the super class `Mapper` and implements these
    steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Text object is converted to string and assigned to string `line`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Line is a string object that is passed to string tokenizer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: String tokenizer is looped using `while` and calls the `removeNonLettersNonNumbers`
    method. The returning string is assigned to a `myword` text instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `context.write(myword,newIntwritable(1))` method is called.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Has a method to remove non-letters and non-number that uses the `string.replaceAll()`
    method. It finally returns a string object which has only number and letters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We shall next create a reducer component. The reducer component will do the
    following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Extend the `reducer` class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a string attribute for the reducer class which accepts the string that
    needs to be searched and whose occurrence needs to be found.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Override the `reduce` method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove unwanted key-value pairs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Keep key-value pairs that are required.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check whether the input key is already present. If it is present, it will get
    the occurrence and the latest value will be stored.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Configure the `application.properties` file with HDFS ports and input and output
    file paths.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here is the sample `application.properties` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Once the properties are configured, it should be available in the Spring context.
    So, configure the properties file in Spring's `application-context.xml` file using
    `property-placeholder`. This is the configuration snippet that needs to be added
    in the `application-conext.xml` file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'You can directly configure Apache Hadoop in the `application-context.xml` file
    or use the properties file and read the key-value pair from the properties file.
    Since we have used the properties file, we shall read the values from the properties
    file. The following code snippet shows that `${mapred.job.tracker}` is a key in
    the properties file. You can see that the default name is also configured from
    the properties file using the key `${fs.default.name}`. Configure Apache Hadoop
    in the `application-context.xml` file as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to configure Hadoop as a job in Spring:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Provide a job ID.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Specify the input path; it will be read from the properties file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Specify the output path; it will be read from the properties file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Jar-by class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Mapper class-reference to the custom mapper class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reducer class-reference to the custom reducer class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This is the configuration snippet that needs to be available in the `application-xccontext.xml`
    file. Configure the Hadoop job in the `application-context.xml` file, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Lastly, we need to configure the job runner in the `application-context.xml`
    file. The job runner configuration tells Spring Framework when to start the job.
    Here we have configured the job runner to start `wordcountjob` on start up.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Here is the configuration snippet for job runner. Configure the `application-context.xml`
    file to run the Hadoop job.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Since this is a standalone Spring application, we do not have a web module that
    will invoke the application context. The context needs to be loaded in a class
    file. So, let's create a `Main` class with a `static` method to load the `application-context.xml`
    file.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create a class that loads the `application-context.xml` file on start
    up, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create a file named `myinput.txt` with content, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to give an input file to HDFS by executing this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Run the `Main` class to see the output.
  prefs: []
  type: TYPE_NORMAL
- en: Map and Reduce jobs using Hadoop streaming and Spring DataApache Hadoop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we shall demonstrate Map and Reduce data streaming with Unix
    shell commands. Since this is related to Hadoop streaming, we shall set up a Hadoop
    instance on the Unix system. A Hadoop instance is always is run on a Unix machine
    for production mode, while for development, a Windows Hadoop instance is used.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the requirements to set up the requirement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: JAVA 1.7.x
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SSH must be installed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Download the latest Apache Hadoop Distribution Binary Package.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unzip and extract the package into a folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the following environment variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`JAVA_HOME`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HADOOP_HOME`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HADOOP_LOG_DIR`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PATH`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We also need to configure the files that are present in the `conf` folder of
    the Hadoop installation directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Core-site.xml`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Hdfs-site.xml`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Mapred-site.xml`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We need to set a default Hadoop file system.
  prefs: []
  type: TYPE_NORMAL
- en: To configure a default Hadoop file system, provide setting information in the
    `core-site.xml` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Also configure the replication factor. Replication factor configuration ensures
    that a copy of the file gets stored in the Hadoop file system. A property `dfs.replication`
    and its value is set in the `hdfs-site.xml` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Lastly, configure the job tracker; this configuration is done in the `mapred-site.xml`
    file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: To run Hadoop in pseudo distributed mode, we just need the format; in the `bin`
    folder, there are `start` and `stop` Hadoop instance commands.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we shall demonstrate how to integrate Python with Apache Hadoop data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We shall create a simple project using Maven. These are the dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We need a mapper and reducer Python script. A mapper script in Python should
    be implemented to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The script should read from a standard input stream, read the input one line
    at time, and convert it into UTF-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The words in the line have to be split into words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The special characters from the line need to be replaced with blank characters,
    and then get a key value pair as a tab; they are delimited to standard output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is the mapper script in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The Reducer script in Python should be implemented to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The script should read the key-value pair output generated from the `mapper`
    class. Then, count the occurrence of keywords.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the Python script is ready, we need to provide the mapper and reducer
    class names and configurations in the properties files. This is the `.properties`
    file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need to configure `property-placeholder` and Apache Hadoop in the `context.xml`
    file. Here is the configuration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Lastly, we need to configure the Hadoop job and assign the job to job runner,
    which will initialize the job.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Now, we need to invoke the configuration using the application context, so that
    the application context is loaded with all the configurations in the Spring Framework.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the following command in Command Prompt to provide an input file. Let the
    file be placed in a folder named `input`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is made available in the output directory, which can be read using
    the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: You should see an output that shows the occurrence of the word "Amily" in the
    provided text.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have seen how Spring integrates with Apache Hadoop and provides a
    Map and Reduce process to search and count data. We have also discussed the integration
    of Python with Apache Hadoop. We have demonstrated how we can configure Hadoop
    jobs in Spring Framework, and have also seen HDFS configurations.
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop is vast concept. For further information, refer to [http://docs.spring.io/spring-hadoop/docs/current/reference/html/](http://docs.spring.io/spring-hadoop/docs/current/reference/html/)
    and [https://github.com/spring-projects/spring-hadoop-samples](https://github.com/spring-projects/spring-hadoop-samples).
  prefs: []
  type: TYPE_NORMAL
- en: We have demonstrated how we can install a Hadoop instance on Unix machines.
    In the next chapter, we shall see how to use Spring Dynamic Modules with OSGI.
  prefs: []
  type: TYPE_NORMAL
