- en: Chapter 6. Optimizing Divide and Conquer Solutions – The Fork/Join Framework
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 2](part0022_split_000.html#KVCC1-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 2. Managing Lots of Threads – Executors"), *Managing Lots of Threads
    – Executors*, [Chapter 3](part0028_split_000.html#QMFO1-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 3. Getting the Maximum from Executors"), *Getting the Maximum from Executors*,
    and [Chapter 4](part0033_split_000.html#VF2I1-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 4. Getting Data from the Tasks – The Callable and Future Interfaces"),
    *Getting Data from the Tasks – The Callable and Future Interfaces*, you learned
    how to work with executors as a mechanism to improve the performance of concurrent
    applications that executes lots of concurrent tasks. The Java 7 concurrency API
    introduces a special kind of executor through the Fork/Join framework. This framework
    is designed to implement optimal concurrent solutions to those problems that can
    be solved using the divide and conquer design paradigm. In this chapter, we will
    cover the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to the Fork/Join framework
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first example – the k-means clustering algorithm
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second example – a data filtering algorithm
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third example – the merge sort algorithm
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An introduction to the Fork/Join framework
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The executor framework, introduced in Java 5, provides a mechanism to execute
    concurrent tasks without creating, starting, and finishing threads. This framework
    uses a pool of threads that executes the tasks you send to the executor reusing
    them for multiple tasks. This mechanism provides some advantages to programmers
    and these are as follows:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: It's easier to program concurrent applications because you don't have to worry
    to create threads.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It's easier to control the resources used by the executor and your application.
    You can create an executor that only uses a predefined number of threads. If you
    send more tasks, the executor stores them in a queue until a thread is available.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Executors reduce the overhead introduced by thread creation reusing the threads.
    Internally, it manages a pool of threads that reuses threads to execute multiple
    tasks.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The divide and conquer algorithm is a very popular design technique. To solve
    a problem using this technique, you divide it into smaller problems. You repeat
    the process in a recursive way until the problems you have to solve are small
    enough to be solved directly. These kinds of problems can be solved using the
    executor, but to solve them in a more efficient way, the Java 7 concurrency API
    introduced the Fork/Join framework.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 'This framework is based on the `ForkJoinPool` class, which is a special kind
    of executor, two operations, the `fork()` and `join()` methods (and their different
    variants), and an internal algorithm named the **work-stealing algorithm**. In
    this chapter, you will learn the basic characteristics, limitations, and components
    of the Fork/Join framework implementing the following three examples:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: The k-means clustering algorithm applied to the clustering of a set of documents
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A data filter algorithm to get the data that meets certain criteria
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The merge sort algorithm to sort big groups of data in an efficient way
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic characteristics of the Fork/Join framework
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we mentioned before, the Fork/Join framework must be used to implement solutions
    to problems based on the divide and conquer technique. You have to divide the
    original problem into smaller problems until they are small enough to be solved
    directly. With this framework, you will implement tasks whose main method will
    be something like this:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The most important part is the one that allows you to divide and execute the
    child tasks in an efficient way and to get the results of those child tasks to
    calculate the results of the parent tasks. This functionality is supported by
    two methods provided by the `ForkJoinTask` class as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 'The `fork()` method: This method allows you to send a child task to the Fork/Join
    executor'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `join()` method: This method allows you to wait for the finalization of
    a child task and returns its result'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: join()方法：此方法允许您等待子任务的完成并返回其结果
- en: 'These methods have different variants, as you will see in the examples. The
    Fork/Join framework has another critical part: the work-stealing algorithm, which
    determines which tasks to be executed. When a task is waiting for the finalization
    of a child task using the `join()` method, the thread that is executing that task
    takes another task from the pool of tasks that are waiting and starts its execution.
    In this way, the threads of the Fork/Join executor are always executing a task
    by improving the performance of the application.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法有不同的变体，正如您将在示例中看到的那样。Fork/Join框架还有另一个关键部分：工作窃取算法，它确定要执行哪些任务。当一个任务正在等待使用join()方法等待子任务的完成时，执行该任务的线程会从等待的任务池中取出另一个任务并开始执行。这样，Fork/Join执行器的线程总是通过执行任务来提高应用程序的性能。
- en: Java 8 includes a new feature in the Fork/Join framework. Now every Java application
    has a default `ForkJoinPool` named common pool. You can obtain it by calling the
    `ForkJoinPool.commonPool()` static method. You don't need to create one explicitly
    (although you can). This default Fork/Join executor will use by default the number
    of threads determined by the available processors of your computer. You can change
    this default behavior changing the value of the system property `java.util.concurrent.ForkJoinPool.common.parallelism`.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Java 8在Fork/Join框架中包含了一个新特性。现在每个Java应用程序都有一个名为common pool的默认ForkJoinPool。您可以通过调用ForkJoinPool.commonPool()静态方法来获取它。您不需要显式创建一个（尽管您可以）。这个默认的Fork/Join执行器将默认使用计算机可用处理器确定的线程数。您可以通过更改系统属性java.util.concurrent.ForkJoinPool.common.parallelism的值来更改此默认行为。
- en: Some features of the Java API use the Fork/Join framework to implement concurrent
    operations. For example, the `parallelSort()` method of the `Arrays` class to
    sort an array in a parallel way and the parallel streams introduced in Java 8
    (which will be described later in [Chapter 7](part0047_split_000.html#1CQAE2-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 7. Processing Massive Datasets with Parallel Streams – The Map and Reduce
    Model"), *Processing Massive Datasets with Parallel Streams – The Map and Reduce
    Model* and [Chapter 8](part0051_split_000.html#1GKCM1-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 8. Processing Massive Datasets with Parallel Streams – The Map and Collect
    Model"), *Processing Massive Datasets with Parallel Streams – The Map and Collect
    Model*) use this framework.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Java API的一些特性使用Fork/Join框架来实现并发操作。例如，Arrays类的parallelSort()方法以并行方式对数组进行排序，以及Java
    8中引入的并行流（稍后将在第7章和第8章中描述）使用了这个框架。
- en: Limitations of the Fork/Join framework
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Fork/Join框架的限制
- en: 'As the Fork/Join framework has been thought to solve a determined kind of problems,
    it has some limitations you have to take into account when you use it to implement
    your problem, which are as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Fork/Join框架被设计用来解决一种确定类型的问题，因此在使用它来实现您的问题时，您必须考虑一些限制，如下所示：
- en: The basic problems that you're not going to subdivide have to be not very large,
    but not very small. According to the Java API documentation, it should have between
    100 and 10,000 basic computational steps.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您不打算细分的基本问题不应该太大，但也不应该太小。根据Java API文档，它应该在100到10,000个基本计算步骤之间。
- en: You should not use blocking I/O operations like reading user input or data from
    a network socket waiting until the data is available. Such operations will cause
    your CPU cores to become idle, reducing the parallelism level, so you will not
    achieve the full performance.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您不应该使用阻塞I/O操作，比如读取用户输入或等待网络套接字中的数据可用。这样的操作会导致CPU核心空闲，降低并行级别，因此您将无法实现完全的性能。
- en: You can't throw checked exceptions inside a task. You have to include the code
    to handle them (for example, wrapping into unchecked `RuntimeException`). Unchecked
    exceptions have a special treatment, as you will see in the examples.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您不能在任务中抛出已检查的异常。您必须包含处理它们的代码（例如，包装成未检查的RuntimeException）。未检查的异常有特殊处理，正如您将在示例中看到的那样。
- en: Components of the Fork/Join framework
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Fork/Join框架的组件
- en: 'There are five basic classes in the Fork/Join framework:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Fork/Join框架中有五个基本类：
- en: 'The `ForkJoinPool` class: This class implements the `Executor` and `ExecutorService`
    interfaces, and it is the `Executor` interface you''re going to use to execute
    your Fork/Join tasks. Java provides you with a default `ForkJoinPool` object (named
    common pool), but you have some constructors to create one if you want. You can
    specify the parallelism level (the maximum number of running parallel threads).
    By default, it uses the number of available processors as the concurrency level.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ForkJoinPool类：该类实现了Executor和ExecutorService接口，它是您要使用来执行Fork/Join任务的Executor接口。Java为您提供了一个默认的ForkJoinPool对象（名为common
    pool），但如果您愿意，您可以使用一些构造函数来创建一个。您可以指定并行级别（最大运行并行线程数）。默认情况下，它使用可用处理器的数量作为并发级别。
- en: 'The `ForkJoinTask` class: This is the base abstract class of all of the Fork/Join
    tasks. It''s an abstract class, and it provides the `fork()` and `join()` methods
    and some variants of them. It also implements the `Future` interface and provides
    methods to determine if the task finished in a normal way, if it was cancelled
    or if it throws an unchecked exception. The `RecursiveTask`, `RecursiveAction`,
    and `CountedCompleter` classes provide `compute()` abstract methods, which should
    be implemented in subclasses to perform actual computations.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ForkJoinTask`类：这是所有Fork/Join任务的基本抽象类。它是一个抽象类，提供了`fork()`和`join()`方法以及它们的一些变体。它还实现了`Future`接口，并提供了方法来确定任务是否以正常方式完成，是否被取消，或者是否抛出未检查的异常。`RecursiveTask`、`RecursiveAction`和`CountedCompleter`类提供了`compute()`抽象方法，应该在子类中实现以执行实际的计算。'
- en: 'The `RecursiveTask` class: This class extends the `ForkJoinTask` class. It''s
    also an abstract class, and it should be your start point to implement Fork/Join
    tasks that returns a result.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RecursiveTask`类：这个类扩展了`ForkJoinTask`类。它也是一个抽象类，应该是实现返回结果的Fork/Join任务的起点。'
- en: 'The `RecursiveAction` class: This class extends the `ForkJoinTask` class. It''s
    also an abstract class, and it should be your start point to implement Fork/Join
    tasks that don''t return a result.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RecursiveAction`类：这个类扩展了`ForkJoinTask`类。它也是一个抽象类，应该是实现不返回结果的Fork/Join任务的起点。'
- en: 'The `CountedCompleter` class: This class extends the `ForkJoinTask` class.
    It''s a new feature of the Java 8 API, and it should be your start point to implement
    tasks that trigger other tasks when they''re completed.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CountedCompleter`类：这个类扩展了`ForkJoinTask`类。这是Java 8 API的一个新特性，应该是实现任务在完成时触发其他任务的起点。'
- en: The first example – the k-means clustering algorithm
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一个例子 - k均值聚类算法
- en: The **k-means clustering** algorithm is a clustering algorithm to group a set
    of items not previously classified into a predefined number of k clusters. It's
    very popular within the data mining and machine learning world to organize and
    classify data in an unsupervised way.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**k均值聚类**算法是一种聚类算法，用于将一组未经分类的项目分组到预定义数量的k个集群中。在数据挖掘和机器学习领域非常受欢迎，以无监督的方式组织和分类数据。'
- en: Each item is normally defined by a vector of characteristics or attributes.
    All the items have the same number of attributes. Each cluster is also defined
    by a vector with the same number of attributes that represents all the items classified
    into that cluster. This vector is named the centroid. For example, if the items
    are defined by numeric vectors, the clusters are defined by the mean of the items
    classified into that cluster.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 每个项目通常由一组特征或属性的向量来定义。所有项目具有相同数量的属性。每个集群也由具有相同数量属性的向量来定义，表示所有分类到该集群的项目。这个向量被称为质心。例如，如果项目由数值向量定义，那么集群由分类到该集群的项目的平均值来定义。
- en: 'Basically, the algorithm has four steps:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，这个算法有四个步骤：
- en: '**Initialization**: In the first step, you have to create the initial vectors
    that represent the K clusters. Normally, you will initialize those vectors randomly.'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**初始化**：在第一步中，你需要创建代表K个集群的初始向量。通常，你会随机初始化这些向量。'
- en: '**Assignment**: Then, you classify each item into a cluster. To select the
    cluster, you calculate the distance between the item and every cluster. You will
    use a distance measure as the **Euclidean distance** to calculate the distance
    between the vector that represents the item and the vector to represents the cluster.
    You will assign the item to the cluster with the shortest distance.'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**分配**：然后，你将每个项目分类到一个集群中。为了选择集群，你需要计算项目与每个集群之间的距离。你将使用**欧几里得距离**作为距离度量来计算代表项目的向量与代表集群的向量之间的距离。你将把项目分配给距离最短的集群。'
- en: '**Update**: Once all the items have been classified, you have to recalculate
    the vectors that define each cluster. As we mentioned earlier, you normally calculate
    the mean of all the vectors of the items classified into the cluster.'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**更新**：一旦所有项目被分类，你需要重新计算定义每个集群的向量。正如我们之前提到的，通常计算分类到集群的所有向量的平均值。'
- en: '**End**: Finally, you check whether some item has changed its assignment cluster.
    If there has been any change, you go to the assignment step again. Otherwise,
    the algorithm ends, and you have your items classified.'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**结束**：最后，你要检查是否有任何项目改变了分配的集群。如果有任何改变，你需要再次进行分配步骤。否则，算法结束，你的项目被分类了。'
- en: 'This algorithm has the following two main limitations:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法有以下两个主要限制：
- en: If you make a random initialization of the initial vectors of the clusters,
    as we suggested earlier, two executions to classify the same item set may give
    you different results.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你对集群的初始向量进行随机初始化，就像我们之前建议的那样，对同一组项目进行两次执行可能会得到不同的结果。
- en: The numbers of cluster are previously predefined. A bad choice of this attribute
    will give you poor results from a classification point of view.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群的数量是预先定义的。选择这个属性不好会导致分类结果不佳。
- en: Despite of this, this algorithm is very popular to cluster different kinds of
    items. To test our algorithm, you are going to implement an application to cluster
    a set of documents. As a document collection, we have taken a reduced version
    of the Wikipedia pages with information about movies corpus we introduced in [Chapter
    4](part0033_split_000.html#VF2I1-2fff3d3b99304faa8fa9b27f1b5053ba "Chapter 4. Getting
    Data from the Tasks – The Callable and Future Interfaces"), *Getting Data from
    the Tasks – The Callable and Future Interfaces*. We only took 1,000 documents.
    To represent each document, we have to use the vector space model representation.
    With this representation, each document is represented as a numeric vector where
    each dimension of the vector represents a word or a term and its value is a metric
    that defines the importance of that word or term in the document.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: When you represent a document collection using the vector space model, the vectors
    will have as many dimensions as the number of different words of the whole collection,
    so the vectors will have a lot of zero values because each document doesn't have
    all the words. You can use a more optimized representation in memory to avoid
    all those zero values and save memory increasing the performance of your application.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: In our case, we have chosen **term frequency–inverse document frequency** (**tf-idf**)
    as the metric that defines the importance of each word and the 50 words with higher
    tf-idf as the terms that represents each document.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: 'We use two files: the `movies.words` file stores a list of all the words used
    in the vectors, and the `movies.data` stores the representation of each document.
    The `movies.data` file has the following format:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here, `10000202` is the identifier of the document, and the rest of the file
    follows the formant `word:tfxidf`.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: As with other examples, we are going to implement the serial and concurrent
    versions and execute both versions to verify that the Fork/Join framework gives
    us an improvement of the performance of this algorithm.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: The common classes
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are some parts that are shared between the serial and concurrent versions.
    These parts include:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '`VocabularyLoader`: This is a class to load the list of words that forms the
    vocabulary of our corpus.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Word`, `Document`, and `DocumentLoader`: These three classes to load the information
    about the documents. These classes have a little difference between the serial
    and concurrent versions of the algorithm.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DistanceMeasure`: This is a class to calculate the **Euclidean** distance
    between two vectors.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DocumentCluster`: This is a class to store the information about the clusters.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's see these classes in detail.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: The VocabularyLoader class
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we mentioned before, our data is stored in two files. One of those files
    is the `movies.words` file. This file stores a list with all the words used in
    the documents. The `VocabularyLoader` class will transform that file into `HashMap`.
    The key of `HashMap` is the whole word, and the value is an integer value with
    the index of that word in the list. We use that index to determine the position
    of the word in the vector space model that represents each document.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'The class has only one method, named `load()`, that receives the path of the
    file as a parameter and returns the `HashMap`:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The Word, Document, and DocumentLoader classes
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These classes store all the information about the documents we will use in our
    algorithm. First, the `Word` class stores information about a word in a document.
    It includes the index of the word and the tf-idf of that word in the document.
    This class only includes those attributes (`int` and `double`, respectively),
    and implements the `Comparable` interface to sort two words using their tf-idf
    value, so we don't include the source code of this class.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Document` class stores all the relevant information about the document.
    First, an array of `Word` objects with the words in the document. This is our
    representation of the vector space model. We only store the words used in the
    document to save a lot of memory space. Then, a `String` with the name of the
    file that stores the document and finally a `DocumentCluster` object to know the
    cluster associated with the document. It also includes a constructor to initialize
    those attributes and methods to get and set their value. We only include the code
    of the `setCluster()` method. In this case, this method will return a Boolean
    value to indicate if the new value of this attribute is the same as the old value
    or a new one. We will use that value to determine if we stop the algorithm or
    not:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Finally, the `DocumentLoader` class loads the information about the document.
    It includes a static method, `load()` that receives the path of the file, and
    the `HashMap` with the vocabulary and returns an `Array` of `Document` objects.
    It loads the file line by line and converts each line to a `Document` object.
    We have the following code:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To convert a line of the text file to a `Document` object, we use the `processItem()`
    method:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As we mentioned earlier, the first item in the line is the identifier of the
    document. We obtain it from `tokens[0]`, and we pass it to the `Document` class
    constructor. Then, for the rest of the tokens, we split them again to obtain the
    information of every word that includes the whole word and the tf-idf value.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: The DistanceMeasurer class
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This class calculates the Euclidean distance between a document and a cluster
    (represented as a vector). The words in our word arrays after sorting are placed
    in the same order as in centroid array, but some words might be absent. For such
    words, we assume that tf-idf is zero, so the distance is just the square of the
    corresponding value from the centroid array:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The DocumentCluster class
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This class stores the information about each cluster generated by the algorithm.
    This information includes a list of all the documents associated with this cluster
    and the centroid of the vector that is the vector that represents the cluster.
    In this case, this vector has as many dimensions as words are in the vocabulary.
    The class has the two attributes, a constructor to initialize them, and methods
    to get and set their value. It also includes two very important methods. First,
    the `calculateCentroid()` method. It calculates the centroid of the cluster as
    the mean of the vectors that represents the documents associated with this cluster.
    We have the following code:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The second method is the `initialize()` method that receives a `Random` object
    and initializes the centroid vector of the cluster with random numbers as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The serial version
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once we have described the common parts of the application, let''s see how
    to implement the serial version of the k-means clustering algorithm. We are going
    to use two classes: `SerialKMeans`, which implements the algorithm, and `SerialMain`,
    which implements the `main()` method to execute the algorithm.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: The SerialKMeans class
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `SerialKMeans` class implements the serial version of the k-means clustering
    algorithm. The main method of the class is the `calculate()` method. It receives
    the following as parameters:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: The array of `Document` objects with the information about the documents
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of clusters you want to generate
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The size of the vocabulary
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A seed for the random number generator
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The method returns an `Array` of `DocumentCluster` object. Each cluster will
    have the list of documents associated with it. First, the document creates the
    `Array` of clusters determined by the `numberClusters` parameter and initializes
    them using the `initialize()` method and a `Random` object as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then, we repeat the assignment and update phases until all the documents stay
    in the same cluster. Finally, we return the array of clusters with the final organization
    of the documents as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The assignment phase is implemented in the `assignment()` method. This method
    receives the array of `Document` and `DocumentCluster` objects. For each document,
    it calculates the Euclidean distance between the document and all the clusters
    and assigns the document to the cluster with the lowest distance. It returns a
    Boolean value to indicate if one or more of the documents has changed their assigned
    cluster from one step to the next one. We have the following code:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The update step is implemented in the `update()` method. It receives the array
    of `DocumentCluster` with the information of the clusters, and it simply recalculates
    the centroid of each cluster:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The SerialMain class     The `SerialMain` class includes the `main()` method to launch the tests of the
    k-means algorithm. First, it loads the data (words and documents) from the files:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then, it initializes the number of clusters we want to generate and the seed
    for the random number generator. If they don''t come as parameters of the `main()`
    method, we use a default value as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Finally, we launch the algorithm measuring its execution time and write the
    number of documents per cluster.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The concurrent version
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To implement the concurrent version of the algorithm, we have used the Fork/Join
    framework. We have implemented two different tasks based on the `RecursiveAction`
    class. As we mentioned earlier, the `RecursiveAction` task is used when you want
    to use the Fork/Join framework with tasks that do not return a result. We have
    implemented the assignment and the update phases as tasks to be executed in a
    Fork/Join framework.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: To implement the concurrent version of the k-means algorithm, we are going to
    modify some of the common classes to use concurrent data structures. Then, we
    are going to implement the two tasks, and finally, we are going to implement the
    `ConcurrentKMeans` that implements the concurrent version of the algorithm and
    the `ConcurrentMain` class to test it.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Two tasks for the Fork/Join framework – AssignmentTask and UpdateTask
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we mentioned earlier, we have implemented the assignment and update phases
    as tasks to be implemented in the Fork/Join framework.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: The assignment phase assigns a document to the cluster that has the lowest Euclidean
    distance with the document. So, we have to process all the documents and calculate
    the Euclidean distances of all the documents and all the clusters. We are going
    to use the number of documents a task has to process as the measure to control
    whether we have to split the task or not. We start with the tasks that have to
    process all the documents and we are going to split them until we have tasks that
    have to process a number of documents lower than a predefined size.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'The `AssignmentTask` class has the following attributes:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: The array of `ConcurrentDocumentCluster` objects with the data of the clusters
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The array of `ConcurrentDocument` objects with the data of the documents
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two integer attributes, `start` and `end`, that determines the number of documents
    the task has to process
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An `AtomicInteger` attribute, `numChanges`, that stores the number of documents
    that have changed its assigned cluster from the last execution to the current
    one
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An integer attribute, `maxSize`, that stores the maximum number of documents
    a task can process
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have implemented a constructor to initialize all these attributes and methods
    to get and set its values.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 'The main method of these tasks is (as with every task) the `compute()` method.
    First, we check the number of documents the tasks have to process. If it''s less
    or equal than the `maxSize` attribute, we process those documents. We calculate
    the Euclidean distance between each document and all the clusters and select the
    cluster with the lowest distance. If it''s necessary, we increment the `numChanges`
    atomic variable using the `incrementAndGet()` method. The atomic variable can
    be updated by more than one thread at the same time without using synchronization
    mechanisms and without causing any memory inconsistencies. Refer to the following
    code:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'If the number of documents the task has to process is too big, we split that
    set into two parts and create two new tasks to process each of those parts as
    follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: To execute those tasks in the Fork/Join pool, we have used the `invokeAll()`
    method. This method will return when the tasks have finished their execution.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: The update phase recalculates the centroid of each cluster as the mean of all
    the documents. So, we have to process all the clusters. We are going to use the
    number of clusters a task has to process as the measure to control if we have
    to split the task or not. We start with a task that has to process all the clusters,
    and we are going to split it until we have tasks that have to process a number
    of clusters lower than a predefined size.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: 'The `UpdateTask` class has the following attributes:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: The array of `ConcurrentDocumentCluster` objects with the data of the clusters
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two integer attributes, `start` and `end`, that determine the number of clusters
    the task has to process
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An integer attribute, `maxSize`, that stores the maximum number of clusters
    a task can process
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have implemented a constructor to initialize all these attributes and methods
    to get and set its values.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 'The `compute()` method first checks the number of clusters the task has to
    process. If that number is less than or equal to the `maxSize` attribute, it processes
    those clusters and updates their centroid:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'If the number of clusters the task has to process is too big, we are going
    to divide the set of clusters the task has to process in two and create two tasks
    to process each of that part as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The ConcurrentKMeans class
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `ConcurrentKMeans` class implements the concurrent version of the k-means
    clustering algorithm. As the serial version, the main method of the class is the
    `calculate()` method. It receives the following as parameters:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: The array of `ConcurrentDocument` objects with the information about the documents
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of clusters you want to generate
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The size of the vocabulary
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A seed for the random number generator
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The maximum number of items a Fork/Join tasks will process without splitting
    the task into other tasks
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `calculate()` method returns an array of the `ConcurrentDocumentCluster`
    objects with the information of the clusters. Each cluster has the list of documents
    associated with it. First, the document creates the array of clusters determined
    by the `numberClusters` parameter and initializes them using the `initialize()`
    method and a `Random` object:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Then, we repeat the assignment and update phases until all the documents stay
    in the same cluster. Before the loop, we create `ForkJoinPool` that is going to
    execute that task and all of its subtasks. Once the loop has finished, as with
    other `Executor` objects, we have to use the `shutdown()` method with a Fork/Join
    pool to finish its executions. Finally, we return the array of clusters with the
    final organization of the documents:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The assignment phase is implemented in the `assignment()` method. This method
    receives the array of clusters, the array of documents, and the `maxSize` attribute.
    First, we delete the list of associated documents to all the clusters:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Then, we initialize the necessary objects: an `AtomicInteger` to store the
    number of documents whose assigned cluster has changed and the `AssignmentTask`
    that will begin the process.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Then, we execute the tasks in the pool in an asynchronous way using the `execute()`
    method of `ForkJoinPool` and wait for its finalization with the `join()` method
    of the `AssignmentTask` object as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Finally, we check the number of documents that has changed its assigned cluster.
    If there have been changes, we return the `true` value. Otherwise, we return the
    `false` value. We have the following code:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The update phase is implemented in the `update()` method. It receives the array
    of clusters and the `maxSize` parameters. First, we create an `UpdateTask` object
    to update all the clusters. Then, we execute that task in the `ForkJoinPool` object
    the method receives as parameter as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The ConcurrentMain class
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `ConcurrentMain` class includes the `main()` method to launch the tests
    of the k-means algorithm. Its code is equal to the `SerialMain` class, but changing
    the serial classes for the concurrent ones.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Comparing the solutions
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To compare the two solutions, we have executed different experiments changing
    the values of three different parameters:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: The k-parameter will establish the number of clusters we want to generate. We
    have tested the algorithms with the values 5, 10, 15, and 20.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The seed for the `Random` number generator. This seed determines how the initial
    centroid positions. We have tested the algorithms with the values 1 and 13.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the concurrent algorithm, the `maxSize` parameter that determines the maximum
    number of items (documents or clusters), a task can process without being split
    into other tasks. We have tested the algorithms with the values 1, 20, and 400.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We have executed the experiments using the JMH framework ([http://openjdk.java.net/projects/code-tools/jmh/](http://openjdk.java.net/projects/code-tools/jmh/))
    that allows you to implement micro benchmarks in Java. Using a framework for benchmarking
    is a better solution that simply measures time using methods such as `currentTimeMillis()`
    or `nanoTime()`. We have executed them 10 times in a computer with a four-core
    processor and calculated the medium execution time of those 10 times. These are
    the execution times we have obtained in milliseconds:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '|   |   | Serial | Concurrent |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
- en: '| **K** | **Seed** |   | **MaxSize=1** | **MaxSize=20** | **maxSize=400** |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
- en: '| 5 | 1 | 6676.141 | 4696.414 | 3291.397 | 3179.673 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
- en: '| 10 | 1 | 6780.088 | 3365.731 | 2970.056 | 2825.488 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
- en: '| 15 | 1 | 12936.178 | 5308.734 | 4737.329 | 4490.443 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
- en: '| 20 | 1 | 19824.729 | 7937.820 | 7347.445 | 6848.873 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
- en: '| 5 | 13 | 3738.869 | 2714.325 | 1984.152 | 1916.053 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
- en: '| 10 | 13 | 9567.416 | 4693.164 | 3892.526 | 3739.129 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
- en: '| 15 | 13 | 12427.589 | 5598.996 | 4735.518 | 4468.721 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
- en: '| 20 | 13 | 18157.913 | 7285.565 | 6671.283 | 6325.664 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
- en: 'We can draw the following conclusions:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: The seed has an important and unpredictable impact in the execution time. Sometimes,
    the execution times are lower with seed 13, but other times are lower with seed
    1.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you increment the number of clusters, the execution time increments too.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `maxSize` parameter doesn't have much influence in the execution time. The
    parameter K or seed has a higher influence in the execution time. If you increase
    the value of the parameter, you will obtain better performance. The difference
    is bigger between 1 and 20 than between 20 and 400.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In all the cases, the concurrent version of the algorithm has better performance
    than the serial one.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, if we compare the serial algorithm with parameters K=20 and seed=13
    with the concurrent version with parameters K=20, seed=13, and maxSize=400 using
    the speed-up, we obtain the following result:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '![Comparing the solutions](img/00017.jpeg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
- en: The second example – a data filtering algorithm
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Suppose that you have a lot of data that describes a list of items. For example,
    you have a lot of attributes (name, surname, address, phone number, and so on)
    of a lot of people. It's a common need to obtain the data that meets certain criteria,
    for example, you want to obtain people who live in a determined street or with
    a determined name.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you will implement one of those filtering programs. We have
    used the **Census-Income KDD** dataset from the UCI (you can download it from
    [https://archive.ics.uci.edu/ml/datasets/Census-Income+%28KDD%29](https://archive.ics.uci.edu/ml/datasets/Census-Income+%28KDD%29)),
    that contains weighted census data extracted from the 1994 and 1995 current population
    surveys conducted by the U.S. Census Bureau.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: In the concurrent version of this example, you will learn how to cancel tasks
    that are running in the Fork/Join pool and how to manage unchecked exceptions
    that can be thrown in a task.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Common parts
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have implemented some classes to read the data from a file and to filter
    the data. These classes are used by the serial and concurrent versions of the
    algorithm. These are the classes:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 'The `CensusData` class: This class stores 39 attributes that define every person.
    It defines the attributes and methods to get and set their value. We are going
    to identify each attribute by a number. The `evaluateFilter()` method of this
    class contains the association between the number and the name of the attribute.
    You can check the file [https://archive.ics.uci.edu/ml/machine-learning-databases/census-income-mld/census-income.names](https://archive.ics.uci.edu/ml/machine-learning-databases/census-income-mld/census-income.names)
    to get the details of every attribute.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `CensusDataLoader` class: This class loads the census data from a file.
    It has the `load()` method that receives the path to the file as an input parameter
    and returns an array of `CensusData` with the information of all the persons of
    the file.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `FilterData` class: This class defines a filter of data. A filter includes
    the number of an attribute and the value of that attribute.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `Filter` class: This class implements the methods to determine if a `CensusData`
    object meets the conditions of a list of filters.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We don't include the source code of these classes. They are very simple, and
    you can check the source code of the example.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: The serial version
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have implemented the serial version of the filter algorithm in two classes.
    The `SerialSearch` class makes the filtering of data. It provides two methods:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'The `findAny()` method: It receives the array of `CensusData` object as a parameter
    with all the data from the file and a list of filters and returns a `CensusData`
    object with the first person it finds that meet all the criteria from the filters.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `findAll()` method: It receives the array of `CensusData` object as a parameter
    with all the data from the file and a list of filters and returns an array of
    `CensusData` objects with all the persons that meets all the criteria from the
    filter.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `SerialMain` class implements the `main()` method of this version and tests
    it to measure the execution time of this algorithm in some circumstances.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: The SerialSearch class
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we mentioned before, this class implements the filtering of data. It provides
    two methods. The first one, the `findAny()` method, looks for the first data object
    that meets the filters. When it finds the first data object, it finishes its execution.
    Refer to the following code:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The second one, the `findAll()` method, returns an array of `CensusData` objects
    with all the objects that meet the filters as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The SerialMain class
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You''re going to use this class to test the filtering algorithm in different
    circumstances. First, we load the data from the file as follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The first case we are going to test is to use the `findAny()` method to find
    an object that exists in the first places of the array. You construct a list of
    filters and then call the `findAny()` method with the data of the file and the
    list of filters:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Our filters look for the following attributes:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '`32`: This is the country of the birth father attribute'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`31`: This is the country of the birth mother attribute'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`1`: This is the class of the worker attributes; `Not in universe` is one of
    their possible values'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`14`: This is the reason for unemployment attribute; `Not in universe` is one
    of their possible values'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We are going to test other cases as follows:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Use the `findAny()` method to find an object that exists in the last positions
    of the array
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the `findAny()`method to try to find an object that doesn't exist
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the `findAny()` method in an error situation
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the `findAll()` method to obtain all the objects that meet a list of filters
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the `findAll()` method in an error situation
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The concurrent version
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We are going to include more elements in our concurrent version:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'A task manager: When you use the Fork/Join framework, you start with one task
    and you split that task into two (or more) child tasks that you split again and
    again until your problem has the desired size. There can be situations when you
    want to finish the execution of all those tasks. For example, when you implement
    the `findAny()` method and you find an object that meets all the criteria, you
    don''t need to continue with the execution of the rest of the tasks.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A `RecursiveTask` class to implement the `findAny()` method: It''s the `IndividualTask`
    class that extends `RecursiveTask`.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A `RecursiveTask` class to implement the `findAll()` method: It''s the `ListTask`
    class that extends `RecursiveTask`.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's see the details of all those classes.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: The TaskManager class
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We are going to use this class to control the cancellation of tasks. We are
    going to cancel the execution of tasks in the following two situations:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: You're executing the `findAny()` operation and you find an object that meets
    the requirements
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You're executing the `findAny()` or `findAll()` operations and there's an unchecked
    exception in one of the tasks
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The class declares two attributes: `ConcurrentLinkedDeque` to store all the
    tasks we need to cancel and an `AtomicBoolean` variable to guarantee that only
    one task executes the `cancelTasks()` method:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'It defines methods to add a task to `ConcurrentLinkedDeque`, delete a task
    from the `ConcurrentLinkedDeque`, and cancel all the tasks stored in it. To cancel
    the tasks, we use the `cancel()` method defined in the `ForkJoinTask` class. The
    `true` parameter forces the interruption of the task if it is running as follows:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The `cancelTasks()` method receives a `RecursiveTask` object as a parameter.
    We're going to cancel all the tasks except the one that is calling this method.
    We don't want to cancel the tasks that have found the result. The `compareAndSet(false,
    true)` method sets the `AtomicBoolean` variable to `true` and returns `true` only
    if the current value is `false`. If the `AtomicBoolean` variable already has a
    `true` value, then `false` is returned. The whole operation is performed atomically,
    so it's guaranteed that the body of if statement will be executed at most once
    even if the `cancelTasks()` method is concurrently called several times from different
    threads.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: The IndividualTask class
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `IndividualTask` class extends the `RecursiveTask` class parameterized
    with the `CensusData` task and implements the `findAny()` operation. It defines
    the following attributes:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: An array with all the `CensusData` objects
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `start` and `end` attributes that determine the elements it has to process
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `size` attribute that determines the maximum number of elements the task
    will process without splitting the task
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `TaskManager` class to cancel the tasks if necessary
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code gives a list of filters to apply:'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The main method of the class is the `compute()` method. It returns a `CensusData`
    object. If the number of elements the task has to process is less than the size
    attribute, it looks for the object directly. If the method finds the desired object,
    it returns the object and uses the method `cancelTasks()` to cancel the execution
    of the rest of the tasks. If the method doesn''t find the desired object, it returns
    null. We have the following code:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'If the number of items it has to process is more than the size attribute, we
    create two child tasks to process half of the elements:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Then, we add the new created tasks to the task manager and deleted the actual
    tasks. If we want to cancel the tasks, we want to cancel only the tasks that are
    running:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Then, we send the tasks to `ForkJoinPool` with the `fork()` method that sends
    them in an asynchronous way and wait for its finalization with the `quietlyJoin()`
    method. The difference between the `join()` and `quietlyJoin()` methods is that
    the `join()` method launches and exception if the task is canceled or an unchecked
    exception is thrown inside the method while the `quietlyJoin()` method doesn't
    throw any exception.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Then, we delete the child tasks from the `TaskManager` class as follows:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Now, we obtain the results of the tasks using the `join()` method. If a task
    throws an unchecked exception, it will be propagated without special handling
    and cancellation will be just ignored as follows:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The ListTask class
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `ListTask` class extends the `RecursiveTask` class parameterized with a
    `List` of `CensusData`. We are going to use this task to implement the `findAll()`
    operation. It's very similar to the `IndividualTask` task. Both use the same attributes,
    but they have differences in the `compute()` method.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we initialize a `List` object to return the results and check the number
    of elements the task has to process. If the number of elements the task has to
    process is less than the size attribute, add all the objects that meet the criteria
    specified in the filters to the list of results:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'If the number of items it has to process is more than the size attribute, we
    will create two child tasks to process half of the elements:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Then, we will add the new created tasks to the task manager and delete the
    actual tasks. The actual task won''t be canceled; its child tasks will be canceled,
    as follows:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Then, we will send the tasks to `ForkJoinPool` with the `fork()` method that
    sends them in an asynchronous way and wait for its finalization with the `quietlyJoin()`
    method:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Then, we will delete the child tasks from `TaskManager`:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Now, we obtain the results of the tasks using the `join()` method. If a task
    throws an unchecked exception, it will be propagated without special handling
    and cancellation will be just ignored:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The ConcurrentSearch class
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `ConcurrentSearch` class implements the `findAny()` and `findAll()` methods.
    They have the same interface as the ones of the serial version. Internally, they
    initialize the `TaskManager` object and the first task and send to default `ForkJoinPool`
    using the `execute` method; they wait for the finalization of the task and write
    the results. This is the code of the `findAny()` method:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'This is the code of the `findAll()` method:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: The ConcurrentMain class
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `ConcurrentMain` class is used to test the concurrent version of our object
    filter. It is identical to the `SerialMain` class, but uses the concurrent version
    of the operations.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Comparing the two versions
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To compare the serial and concurrent versions of the filtering algorithm, we
    tested them in six different situations:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '**Test 1**: We test the `findAny()` method looking for an object that exists
    in the first positions of the `CensusData` array'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test 2**: We test the `findAny()` method looking for an object that exists
    in the last positions of the `CensusData` array'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test 3**: We test the `findAny()` method looking for an object that doesn''t
    exist'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test 4**: We test the `findAny()` method in an error situation'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test 5**: We test the `findAll()` method in a normal situation'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test 6**: We test the `findAll()` method in an error situation'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the concurrent version of the algorithm, we have tested three different
    values of the size parameter that determines the maximum number of elements a
    task can process without forking in two child tasks. We have tested with 10, 200,
    and 2,000.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: 'We have executed the tests using the JMH framework ([http://openjdk.java.net/projects/code-tools/jmh/](http://openjdk.java.net/projects/code-tools/jmh/))
    that allows you to implement micro benchmarks in Java. Using a framework for benchmarking
    is a better solution that simply measures time using methods as `currentTimeMillis()`
    or `nanoTime()`. We have executed them 10 times in a computer with a four-core
    processor and calculated the medium execution time of those 10 times. As with
    other examples, we have measured the execution time in milliseconds:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '| Test case | Serial | Concurrent size = 10 | Concurrent size = 200 | Concurrent
    size = 2000 | Best |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
- en: '| **Test 1** | 1.177 | 8.124 | 4.547 | 4.073 | Serial |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
- en: '| **Test 2** | 95.237 | 157.412 | 34.581 | 35.691 | Concurrent |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
- en: '| **Test 3** | 66.616 | 41.916 | 74.829 | 37.140 | Concurrent |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
- en: '| **Test 4** | 0.540 | 25869.339 | 643.144 | 9.673 | Serial |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
- en: '| **Test 5** | 61.752 | 37.349 | 40.344 | 22.911 | Concurrent |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
- en: '| **Test 6** | 0.802 | 31663.607 | 231.440 | 7.706 | Serial |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
- en: 'We can draw the following conclusions:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: The serial version of the algorithm has better performance when we have to process
    a smaller number of elements.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The concurrent version of the algorithm has better performance when we have
    to process all the elements or a bit amount of them.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In error situations, the serial version of the algorithm has better performance
    than the concurrent version. The concurrent version has a very poor performance
    in this situation when the value of the `size` parameter is small.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this case, concurrency does not always gives us an improvement of the performance.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: The third example – the merge sort algorithm
  id: totrans-296
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The merge sort algorithm is a very popular sorting algorithm that is always
    implemented using the divide and conquer technique, so it's a very good candidate
    to test with the Fork/Join framework.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: To implement the merge sort algorithm, we divide the unsorted lists into sublists
    of one element. Then, we merge those unsorted sublists to produce ordered sublists
    until we have processed all the sublists, and we have only the original list,
    but with all the elements sorted.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: To make the concurrent version of our algorithm, we have used the new Fork/Join
    tasks, the `CountedCompleter` tasks, introduced in the Java 8 version. The most
    important characteristics of these tasks are that they include a method to be
    executed when all their child tasks have finished their execution.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: To test out implementations, we have used the **Amazon product co-purchasing
    network metadata** (you can download it from [https://snap.stanford.edu/data/amazon-meta.html](https://snap.stanford.edu/data/amazon-meta.html)).
    In particular, we have created a list with the salesrank of 542,184 products.
    We are going to test our versions of the algorithm, sorting this list of products,
    and compare the execution time with the `sort()` and `parallelSort()` methods
    of the `Arrays` class.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: Shared classes
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we mentioned earlier, we have built a list of 542,184 products of Amazon
    with information about each product including an ID, its title, group, salesrank,
    number of reviews, number of similar products, and number of categories the product
    belongs to. We have implemented the `AmazonMetaData` class to store the information
    of a product. This class declares the necessary attributes and the methods to
    get and set their values. This class implements the `Comparable` interface to
    compare two instances of this class. We want to sort the elements by salesrank
    in ascending order. To implement the `compare()` method, we use the `compare()`
    method of the `Long` class to compare the salesrank of both objects as follows:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: We have also implemented `AmazonMetaDataLoader` that provides the `load()` method.
    This method receives a route to the file with the data as a parameter and returns
    an array of `AmazonMetaData` objects with the information of all the products.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We don't include the source code of these classes to focus on the characteristics
    of the Fork/Join framework.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: The serial version
  id: totrans-307
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have implemented the serial version of the merge sort algorithm in the `SerialMergeSort`
    class, which implements the algorithm and the `SerialMetaData` class and provides
    the `main()` method to test the algorithm.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: The SerialMergeSort class
  id: totrans-309
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `SerialMergeSort` class implements the serial version of the merge sort
    algorithm. It provides the `mergeSort()` method that receives the following parameters:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: The array with all the data we want to sort
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first element the method has to process (included)
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last element the method has to process (not included)
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If the method has to process only one element, it returns. Otherwise, it makes
    two recursive calls to the `mergeSort()` method. The first call will process the
    first half of the elements, and the second call will process the second half of
    the elements. Finally, we make a call to the `merge()` method to merge the two
    halves of the elements and get a sorted list of elements:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: We have used the `(end+start)>>>1` operator to obtain the mid-element to split
    the array. If you have, for example, 1.5 billions of elements (not that impossible
    with modern memory chips), it still fits the Java array. However, *(end+start)/2*
    will overflow resulting in a negative number array. You can find a detailed explanation
    of this problem at [http://googleresearch.blogspot.ru/2006/06/extra-extra-read-all-about-it-nearly.html](http://googleresearch.blogspot.ru/2006/06/extra-extra-read-all-about-it-nearly.html).
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: 'The `merge()` method merges two lists of elements to obtain a sorted list.
    It receives the following parameters:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: The array with all the data we want to sort
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The three elements (`start`, `mid`, and `end`) that determine the two parts
    of the array (start-mid, mid-end) we want to merge and sort
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We create a temporary array to sort the elements, sort the elements in the
    array processing both parts of the list, and store the sorted list in the same
    positions of the original array. Check the following code:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: The SerialMetaData class
  id: totrans-322
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `SerialMetaData` class provides the `main()` method to test the algorithm.
    We''re going to execute every sort algorithm 10 times to calculate the average
    execution time. First, we load the data from the file and create a copy of the
    array:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Then, we sort the first array using the `sort()` method of the `Arrays` class:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Then, we sort the second array using our implementation of the merge sort algorithm:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Finally, we check that the sorted arrays are identical:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: The concurrent version
  id: totrans-331
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we mentioned before, we are going to use the new Java 8 `CountedCompleter`
    class as the base class for our Fork/Join tasks. This class provides a mechanism
    to execute a method when all its child tasks have finished their execution. It's
    the `onCompletion()` method. So, we use the `compute()` method to divide the array
    and the `onCompletion()` method to merge the sublists into an ordered list.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: 'The concurrent solution you are going to implement has three classes:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: The `MergeSortTask` class that extends the `CountedCompleter` class and implements
    the task that executes the merge sort algorithm
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `ConcurrentMergeSort` task that launches the first task
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `ConcurrentMetaData` class that provides the `main()` method to test the
    concurrent version of the merge sort algorithm
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The MergeSortTask class
  id: totrans-337
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we mentioned earlier, this class implements the tasks that are going to
    execute the merge sort algorithm. This class uses the following attributes:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: The array of data we want to sort
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The start and end positions of the array the task has to sort
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The class also has a constructor to initialize its parameters:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The `compute()` method, if the difference between the start and end indexes
    are greater or equal that `1024`, we split the task into two child tasks to process
    two subsets of the original set. Both tasks use the `fork()` method to send a
    task to the `ForkJoinPool` in an asynchronous way. Otherwise, we execute `SerialMergeSorg.mergeSort()`
    to sort the part of the array (which have `1024` or less elements) and then we
    call the `tryComplete()` method. This method will internally call the `onCompletion()`
    method when the child task has finished its execution. Take a look at the following
    code:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'In our case, we will use the `onCompletion()` method to make the merge and
    sort operations to obtain the sorted list. Once a task finishes the execution
    of the `onCompletion()` method, it calls `tryComplete()` over its parent to try
    to complete that task. The source code of the `onCompletion()` method is very
    similar to the `merge()` method of the serial version of the algorithm. Refer
    to the following code:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: The ConcurrentMergeSort class
  id: totrans-347
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the concurrent version, this class is very simple. It implements the `mergeSort()`
    method that receives the array of data to sort and the start index (which will
    always be 0) and the end index (which will always be the length of the array)
    to sort the array as parameters. We have chosen to maintain the same interface
    rather than the serial version.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: The method creates a new `MergeSortTask`, sends it to the default `ForkJoinPool`
    using the `invoke()` method that returns when the task has finished its execution
    and the array is sorted.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: The ConcurrentMetaData class
  id: totrans-351
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `ConcurrentMetaData` class provides the `main()` method to test the concurrent
    version of the merge sort algorithm. In our case, the code is equal to the code
    of the `SerialMetaData` class, but using the concurrent versions of the classes
    and the `Arrays.parallelSort()` method instead of the `Arrays.sort()` method,
    so we don't include the source code of the class.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: Comparing the two versions
  id: totrans-353
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have executed our serial and concurrent versions of the merge sort algorithm
    and compared the execution times between them and against the methods `Arrays.sort()`
    and `Arrays.parallelSort()`. We have executed the four versions using the JMH
    framework ([http://openjdk.java.net/projects/code-tools/jmh/](http://openjdk.java.net/projects/code-tools/jmh/))
    that allows you to implement micro benchmarks in Java. Using a framework for benchmarking
    is a better solution that simply measures time using methods as `currentTimeMillis()`
    or `nanoTime()`. We have executed them 10 times in a computer with a four-core
    processor and calculated the medium execution time of those 10 times. These are
    the execution times in millisecond we have obtained when we sort our dataset with
    542,184 objects:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: '|   | Arrays.sort() | Serial merge sort | Arrays.parallelSort() | Concurrent
    merge sort |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
- en: '| **Execution time (ms)** | 561.324 | 711.004 | 261.418 | 353.846 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
- en: 'We can draw the following conclusions:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: The method `Arrays.parallelSort()` obtains the best result. For serial algorithms,
    the `Arrays.sort()` method obtains better execution time than our implementation.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For our implementations, the concurrent version of the algorithm has better
    performance than the serial one.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can compare our serial and concurrent versions of the merge sort algorithm
    using the speed-up:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: '![Comparing the two versions](img/00018.jpeg)'
  id: totrans-362
  prefs: []
  type: TYPE_IMG
- en: Other methods of the Fork/Join framework
  id: totrans-363
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the three examples of this chapter, we have used a lot of methods of the
    class that forms the Fork/Join framework, but there are other interesting methods
    you have to know.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: We have used the methods `execute()` and `invoke()` from the `ForkJoinPool`
    class to send tasks to the pool. We can use another method named `submit()`. The
    main difference between them is that the `execute()` method sends the task to
    `ForkJoinPool` and returns immediately a void value, the `invoke()` method sends
    the task to the `ForkJoinPool` and returns when the task has finished its execution,
    and the `submit()` method sends the task to the `ForkJoinPool` and returns immediately
    a `Future` object to control the status of the task and obtain its result.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: In all the examples of this chapter, we have used classes based on the `ForkJoinTask`
    class, but you can use the `ForkJoinPool` tasks based on the `Runnable` and `Callable`
    interfaces. To do this, you can use the method `submit()` that has versions that
    accept a `Runnable` object, a `Runnable` object with a result, and a `Callable`
    object.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: The `ForkJoinTask` class provides the method `get(long timeout, TimeUnit unit)`
    to obtain the results returned by a task. This method waits for the period of
    time specified in the parameters for the result of the task. If the task finishes
    its execution before that period of time, the method returns the result. Otherwise,
    it throws a `TimeoutException` exception.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: The `ForkJoinTask` provides an alternative to the `invoke()` method. It is the
    `quietlyInvoke()` method. The main difference between the two versions is that
    the `invoke()` method returns the result of the execution of the task or throws
    any exception if necessary. The `quietlyInvoke()` method don't return the result
    of the task and doesn't throw any exception. It's similar to the `quietlyJoin()`
    method used in the examples.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-369
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The divide and conquer design technique is a very popular approach to solve
    different kinds of problems. You divide the original problem into smaller problems
    and those problems into smaller ones until we have enough simple problems to solve
    it directly. In version 7, the Java concurrency API introduced a special kind
    of `Executor` optimized for these kinds of problems. It''s the Fork/Join Framework.
    It''s based on the following two operations:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '**fork**: This allows you to create a new child task'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**join**: This allows you to wait for the finalization of a child task and
    get its results'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using those operations, Fork/Join tasks have the following appearance:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: In this chapter, you have solved three different problems using the Fork/Join
    framework such as the k-means clustering algorithm, a data filtering algorithm,
    and the merge sort algorithm.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: 'You have used default `ForkJoinPool`, provided by the API (this is a new feature
    of the Java 8 version), and created a new `ForkJoinPool` object. You have also
    used the three types of `ForkJoinTask` `s`:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: The `RecursiveAction` class, used as the base class for those `ForkJoinTasks`
    that don't return a result.
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `RecursiveTask` class, used as the base class for those `ForkJoinTasks`
    that return a result.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `CountedCompleter` class, introduced in Java 8 and used as the base class
    for those `ForkJoinTasks` that need to execute a method or launch another task
    when all its child subtasks finish their execution.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next chapter, you will learn how to use the MapReduce programming technique
    using the new Java 8 **parallel streams** to get the best performance processing
    very big datasets.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
