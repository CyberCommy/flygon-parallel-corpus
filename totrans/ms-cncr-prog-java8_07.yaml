- en: Chapter 6. Optimizing Divide and Conquer Solutions – The Fork/Join Framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 2](part0022_split_000.html#KVCC1-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 2. Managing Lots of Threads – Executors"), *Managing Lots of Threads
    – Executors*, [Chapter 3](part0028_split_000.html#QMFO1-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 3. Getting the Maximum from Executors"), *Getting the Maximum from Executors*,
    and [Chapter 4](part0033_split_000.html#VF2I1-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 4. Getting Data from the Tasks – The Callable and Future Interfaces"),
    *Getting Data from the Tasks – The Callable and Future Interfaces*, you learned
    how to work with executors as a mechanism to improve the performance of concurrent
    applications that executes lots of concurrent tasks. The Java 7 concurrency API
    introduces a special kind of executor through the Fork/Join framework. This framework
    is designed to implement optimal concurrent solutions to those problems that can
    be solved using the divide and conquer design paradigm. In this chapter, we will
    cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to the Fork/Join framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first example – the k-means clustering algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second example – a data filtering algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third example – the merge sort algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An introduction to the Fork/Join framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The executor framework, introduced in Java 5, provides a mechanism to execute
    concurrent tasks without creating, starting, and finishing threads. This framework
    uses a pool of threads that executes the tasks you send to the executor reusing
    them for multiple tasks. This mechanism provides some advantages to programmers
    and these are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It's easier to program concurrent applications because you don't have to worry
    to create threads.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It's easier to control the resources used by the executor and your application.
    You can create an executor that only uses a predefined number of threads. If you
    send more tasks, the executor stores them in a queue until a thread is available.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Executors reduce the overhead introduced by thread creation reusing the threads.
    Internally, it manages a pool of threads that reuses threads to execute multiple
    tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The divide and conquer algorithm is a very popular design technique. To solve
    a problem using this technique, you divide it into smaller problems. You repeat
    the process in a recursive way until the problems you have to solve are small
    enough to be solved directly. These kinds of problems can be solved using the
    executor, but to solve them in a more efficient way, the Java 7 concurrency API
    introduced the Fork/Join framework.
  prefs: []
  type: TYPE_NORMAL
- en: 'This framework is based on the `ForkJoinPool` class, which is a special kind
    of executor, two operations, the `fork()` and `join()` methods (and their different
    variants), and an internal algorithm named the **work-stealing algorithm**. In
    this chapter, you will learn the basic characteristics, limitations, and components
    of the Fork/Join framework implementing the following three examples:'
  prefs: []
  type: TYPE_NORMAL
- en: The k-means clustering algorithm applied to the clustering of a set of documents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A data filter algorithm to get the data that meets certain criteria
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The merge sort algorithm to sort big groups of data in an efficient way
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic characteristics of the Fork/Join framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we mentioned before, the Fork/Join framework must be used to implement solutions
    to problems based on the divide and conquer technique. You have to divide the
    original problem into smaller problems until they are small enough to be solved
    directly. With this framework, you will implement tasks whose main method will
    be something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The most important part is the one that allows you to divide and execute the
    child tasks in an efficient way and to get the results of those child tasks to
    calculate the results of the parent tasks. This functionality is supported by
    two methods provided by the `ForkJoinTask` class as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `fork()` method: This method allows you to send a child task to the Fork/Join
    executor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `join()` method: This method allows you to wait for the finalization of
    a child task and returns its result'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These methods have different variants, as you will see in the examples. The
    Fork/Join framework has another critical part: the work-stealing algorithm, which
    determines which tasks to be executed. When a task is waiting for the finalization
    of a child task using the `join()` method, the thread that is executing that task
    takes another task from the pool of tasks that are waiting and starts its execution.
    In this way, the threads of the Fork/Join executor are always executing a task
    by improving the performance of the application.'
  prefs: []
  type: TYPE_NORMAL
- en: Java 8 includes a new feature in the Fork/Join framework. Now every Java application
    has a default `ForkJoinPool` named common pool. You can obtain it by calling the
    `ForkJoinPool.commonPool()` static method. You don't need to create one explicitly
    (although you can). This default Fork/Join executor will use by default the number
    of threads determined by the available processors of your computer. You can change
    this default behavior changing the value of the system property `java.util.concurrent.ForkJoinPool.common.parallelism`.
  prefs: []
  type: TYPE_NORMAL
- en: Some features of the Java API use the Fork/Join framework to implement concurrent
    operations. For example, the `parallelSort()` method of the `Arrays` class to
    sort an array in a parallel way and the parallel streams introduced in Java 8
    (which will be described later in [Chapter 7](part0047_split_000.html#1CQAE2-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 7. Processing Massive Datasets with Parallel Streams – The Map and Reduce
    Model"), *Processing Massive Datasets with Parallel Streams – The Map and Reduce
    Model* and [Chapter 8](part0051_split_000.html#1GKCM1-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 8. Processing Massive Datasets with Parallel Streams – The Map and Collect
    Model"), *Processing Massive Datasets with Parallel Streams – The Map and Collect
    Model*) use this framework.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations of the Fork/Join framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As the Fork/Join framework has been thought to solve a determined kind of problems,
    it has some limitations you have to take into account when you use it to implement
    your problem, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The basic problems that you're not going to subdivide have to be not very large,
    but not very small. According to the Java API documentation, it should have between
    100 and 10,000 basic computational steps.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You should not use blocking I/O operations like reading user input or data from
    a network socket waiting until the data is available. Such operations will cause
    your CPU cores to become idle, reducing the parallelism level, so you will not
    achieve the full performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can't throw checked exceptions inside a task. You have to include the code
    to handle them (for example, wrapping into unchecked `RuntimeException`). Unchecked
    exceptions have a special treatment, as you will see in the examples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Components of the Fork/Join framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are five basic classes in the Fork/Join framework:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `ForkJoinPool` class: This class implements the `Executor` and `ExecutorService`
    interfaces, and it is the `Executor` interface you''re going to use to execute
    your Fork/Join tasks. Java provides you with a default `ForkJoinPool` object (named
    common pool), but you have some constructors to create one if you want. You can
    specify the parallelism level (the maximum number of running parallel threads).
    By default, it uses the number of available processors as the concurrency level.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `ForkJoinTask` class: This is the base abstract class of all of the Fork/Join
    tasks. It''s an abstract class, and it provides the `fork()` and `join()` methods
    and some variants of them. It also implements the `Future` interface and provides
    methods to determine if the task finished in a normal way, if it was cancelled
    or if it throws an unchecked exception. The `RecursiveTask`, `RecursiveAction`,
    and `CountedCompleter` classes provide `compute()` abstract methods, which should
    be implemented in subclasses to perform actual computations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `RecursiveTask` class: This class extends the `ForkJoinTask` class. It''s
    also an abstract class, and it should be your start point to implement Fork/Join
    tasks that returns a result.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `RecursiveAction` class: This class extends the `ForkJoinTask` class. It''s
    also an abstract class, and it should be your start point to implement Fork/Join
    tasks that don''t return a result.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `CountedCompleter` class: This class extends the `ForkJoinTask` class.
    It''s a new feature of the Java 8 API, and it should be your start point to implement
    tasks that trigger other tasks when they''re completed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first example – the k-means clustering algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **k-means clustering** algorithm is a clustering algorithm to group a set
    of items not previously classified into a predefined number of k clusters. It's
    very popular within the data mining and machine learning world to organize and
    classify data in an unsupervised way.
  prefs: []
  type: TYPE_NORMAL
- en: Each item is normally defined by a vector of characteristics or attributes.
    All the items have the same number of attributes. Each cluster is also defined
    by a vector with the same number of attributes that represents all the items classified
    into that cluster. This vector is named the centroid. For example, if the items
    are defined by numeric vectors, the clusters are defined by the mean of the items
    classified into that cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Basically, the algorithm has four steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Initialization**: In the first step, you have to create the initial vectors
    that represent the K clusters. Normally, you will initialize those vectors randomly.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Assignment**: Then, you classify each item into a cluster. To select the
    cluster, you calculate the distance between the item and every cluster. You will
    use a distance measure as the **Euclidean distance** to calculate the distance
    between the vector that represents the item and the vector to represents the cluster.
    You will assign the item to the cluster with the shortest distance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Update**: Once all the items have been classified, you have to recalculate
    the vectors that define each cluster. As we mentioned earlier, you normally calculate
    the mean of all the vectors of the items classified into the cluster.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**End**: Finally, you check whether some item has changed its assignment cluster.
    If there has been any change, you go to the assignment step again. Otherwise,
    the algorithm ends, and you have your items classified.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This algorithm has the following two main limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: If you make a random initialization of the initial vectors of the clusters,
    as we suggested earlier, two executions to classify the same item set may give
    you different results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The numbers of cluster are previously predefined. A bad choice of this attribute
    will give you poor results from a classification point of view.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Despite of this, this algorithm is very popular to cluster different kinds of
    items. To test our algorithm, you are going to implement an application to cluster
    a set of documents. As a document collection, we have taken a reduced version
    of the Wikipedia pages with information about movies corpus we introduced in [Chapter
    4](part0033_split_000.html#VF2I1-2fff3d3b99304faa8fa9b27f1b5053ba "Chapter 4. Getting
    Data from the Tasks – The Callable and Future Interfaces"), *Getting Data from
    the Tasks – The Callable and Future Interfaces*. We only took 1,000 documents.
    To represent each document, we have to use the vector space model representation.
    With this representation, each document is represented as a numeric vector where
    each dimension of the vector represents a word or a term and its value is a metric
    that defines the importance of that word or term in the document.
  prefs: []
  type: TYPE_NORMAL
- en: When you represent a document collection using the vector space model, the vectors
    will have as many dimensions as the number of different words of the whole collection,
    so the vectors will have a lot of zero values because each document doesn't have
    all the words. You can use a more optimized representation in memory to avoid
    all those zero values and save memory increasing the performance of your application.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, we have chosen **term frequency–inverse document frequency** (**tf-idf**)
    as the metric that defines the importance of each word and the 50 words with higher
    tf-idf as the terms that represents each document.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use two files: the `movies.words` file stores a list of all the words used
    in the vectors, and the `movies.data` stores the representation of each document.
    The `movies.data` file has the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here, `10000202` is the identifier of the document, and the rest of the file
    follows the formant `word:tfxidf`.
  prefs: []
  type: TYPE_NORMAL
- en: As with other examples, we are going to implement the serial and concurrent
    versions and execute both versions to verify that the Fork/Join framework gives
    us an improvement of the performance of this algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: The common classes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are some parts that are shared between the serial and concurrent versions.
    These parts include:'
  prefs: []
  type: TYPE_NORMAL
- en: '`VocabularyLoader`: This is a class to load the list of words that forms the
    vocabulary of our corpus.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Word`, `Document`, and `DocumentLoader`: These three classes to load the information
    about the documents. These classes have a little difference between the serial
    and concurrent versions of the algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DistanceMeasure`: This is a class to calculate the **Euclidean** distance
    between two vectors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DocumentCluster`: This is a class to store the information about the clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's see these classes in detail.
  prefs: []
  type: TYPE_NORMAL
- en: The VocabularyLoader class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we mentioned before, our data is stored in two files. One of those files
    is the `movies.words` file. This file stores a list with all the words used in
    the documents. The `VocabularyLoader` class will transform that file into `HashMap`.
    The key of `HashMap` is the whole word, and the value is an integer value with
    the index of that word in the list. We use that index to determine the position
    of the word in the vector space model that represents each document.
  prefs: []
  type: TYPE_NORMAL
- en: 'The class has only one method, named `load()`, that receives the path of the
    file as a parameter and returns the `HashMap`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The Word, Document, and DocumentLoader classes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These classes store all the information about the documents we will use in our
    algorithm. First, the `Word` class stores information about a word in a document.
    It includes the index of the word and the tf-idf of that word in the document.
    This class only includes those attributes (`int` and `double`, respectively),
    and implements the `Comparable` interface to sort two words using their tf-idf
    value, so we don't include the source code of this class.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Document` class stores all the relevant information about the document.
    First, an array of `Word` objects with the words in the document. This is our
    representation of the vector space model. We only store the words used in the
    document to save a lot of memory space. Then, a `String` with the name of the
    file that stores the document and finally a `DocumentCluster` object to know the
    cluster associated with the document. It also includes a constructor to initialize
    those attributes and methods to get and set their value. We only include the code
    of the `setCluster()` method. In this case, this method will return a Boolean
    value to indicate if the new value of this attribute is the same as the old value
    or a new one. We will use that value to determine if we stop the algorithm or
    not:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the `DocumentLoader` class loads the information about the document.
    It includes a static method, `load()` that receives the path of the file, and
    the `HashMap` with the vocabulary and returns an `Array` of `Document` objects.
    It loads the file line by line and converts each line to a `Document` object.
    We have the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'To convert a line of the text file to a `Document` object, we use the `processItem()`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As we mentioned earlier, the first item in the line is the identifier of the
    document. We obtain it from `tokens[0]`, and we pass it to the `Document` class
    constructor. Then, for the rest of the tokens, we split them again to obtain the
    information of every word that includes the whole word and the tf-idf value.
  prefs: []
  type: TYPE_NORMAL
- en: The DistanceMeasurer class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This class calculates the Euclidean distance between a document and a cluster
    (represented as a vector). The words in our word arrays after sorting are placed
    in the same order as in centroid array, but some words might be absent. For such
    words, we assume that tf-idf is zero, so the distance is just the square of the
    corresponding value from the centroid array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The DocumentCluster class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This class stores the information about each cluster generated by the algorithm.
    This information includes a list of all the documents associated with this cluster
    and the centroid of the vector that is the vector that represents the cluster.
    In this case, this vector has as many dimensions as words are in the vocabulary.
    The class has the two attributes, a constructor to initialize them, and methods
    to get and set their value. It also includes two very important methods. First,
    the `calculateCentroid()` method. It calculates the centroid of the cluster as
    the mean of the vectors that represents the documents associated with this cluster.
    We have the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The second method is the `initialize()` method that receives a `Random` object
    and initializes the centroid vector of the cluster with random numbers as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The serial version
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once we have described the common parts of the application, let''s see how
    to implement the serial version of the k-means clustering algorithm. We are going
    to use two classes: `SerialKMeans`, which implements the algorithm, and `SerialMain`,
    which implements the `main()` method to execute the algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: The SerialKMeans class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `SerialKMeans` class implements the serial version of the k-means clustering
    algorithm. The main method of the class is the `calculate()` method. It receives
    the following as parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: The array of `Document` objects with the information about the documents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of clusters you want to generate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The size of the vocabulary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A seed for the random number generator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The method returns an `Array` of `DocumentCluster` object. Each cluster will
    have the list of documents associated with it. First, the document creates the
    `Array` of clusters determined by the `numberClusters` parameter and initializes
    them using the `initialize()` method and a `Random` object as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we repeat the assignment and update phases until all the documents stay
    in the same cluster. Finally, we return the array of clusters with the final organization
    of the documents as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The assignment phase is implemented in the `assignment()` method. This method
    receives the array of `Document` and `DocumentCluster` objects. For each document,
    it calculates the Euclidean distance between the document and all the clusters
    and assigns the document to the cluster with the lowest distance. It returns a
    Boolean value to indicate if one or more of the documents has changed their assigned
    cluster from one step to the next one. We have the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The update step is implemented in the `update()` method. It receives the array
    of `DocumentCluster` with the information of the clusters, and it simply recalculates
    the centroid of each cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The SerialMain class     The `SerialMain` class includes the `main()` method to launch the tests of the
    k-means algorithm. First, it loads the data (words and documents) from the files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, it initializes the number of clusters we want to generate and the seed
    for the random number generator. If they don''t come as parameters of the `main()`
    method, we use a default value as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we launch the algorithm measuring its execution time and write the
    number of documents per cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The concurrent version
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To implement the concurrent version of the algorithm, we have used the Fork/Join
    framework. We have implemented two different tasks based on the `RecursiveAction`
    class. As we mentioned earlier, the `RecursiveAction` task is used when you want
    to use the Fork/Join framework with tasks that do not return a result. We have
    implemented the assignment and the update phases as tasks to be executed in a
    Fork/Join framework.
  prefs: []
  type: TYPE_NORMAL
- en: To implement the concurrent version of the k-means algorithm, we are going to
    modify some of the common classes to use concurrent data structures. Then, we
    are going to implement the two tasks, and finally, we are going to implement the
    `ConcurrentKMeans` that implements the concurrent version of the algorithm and
    the `ConcurrentMain` class to test it.
  prefs: []
  type: TYPE_NORMAL
- en: Two tasks for the Fork/Join framework – AssignmentTask and UpdateTask
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we mentioned earlier, we have implemented the assignment and update phases
    as tasks to be implemented in the Fork/Join framework.
  prefs: []
  type: TYPE_NORMAL
- en: The assignment phase assigns a document to the cluster that has the lowest Euclidean
    distance with the document. So, we have to process all the documents and calculate
    the Euclidean distances of all the documents and all the clusters. We are going
    to use the number of documents a task has to process as the measure to control
    whether we have to split the task or not. We start with the tasks that have to
    process all the documents and we are going to split them until we have tasks that
    have to process a number of documents lower than a predefined size.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `AssignmentTask` class has the following attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: The array of `ConcurrentDocumentCluster` objects with the data of the clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The array of `ConcurrentDocument` objects with the data of the documents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two integer attributes, `start` and `end`, that determines the number of documents
    the task has to process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An `AtomicInteger` attribute, `numChanges`, that stores the number of documents
    that have changed its assigned cluster from the last execution to the current
    one
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An integer attribute, `maxSize`, that stores the maximum number of documents
    a task can process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have implemented a constructor to initialize all these attributes and methods
    to get and set its values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main method of these tasks is (as with every task) the `compute()` method.
    First, we check the number of documents the tasks have to process. If it''s less
    or equal than the `maxSize` attribute, we process those documents. We calculate
    the Euclidean distance between each document and all the clusters and select the
    cluster with the lowest distance. If it''s necessary, we increment the `numChanges`
    atomic variable using the `incrementAndGet()` method. The atomic variable can
    be updated by more than one thread at the same time without using synchronization
    mechanisms and without causing any memory inconsistencies. Refer to the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'If the number of documents the task has to process is too big, we split that
    set into two parts and create two new tasks to process each of those parts as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: To execute those tasks in the Fork/Join pool, we have used the `invokeAll()`
    method. This method will return when the tasks have finished their execution.
  prefs: []
  type: TYPE_NORMAL
- en: The update phase recalculates the centroid of each cluster as the mean of all
    the documents. So, we have to process all the clusters. We are going to use the
    number of clusters a task has to process as the measure to control if we have
    to split the task or not. We start with a task that has to process all the clusters,
    and we are going to split it until we have tasks that have to process a number
    of clusters lower than a predefined size.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `UpdateTask` class has the following attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: The array of `ConcurrentDocumentCluster` objects with the data of the clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two integer attributes, `start` and `end`, that determine the number of clusters
    the task has to process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An integer attribute, `maxSize`, that stores the maximum number of clusters
    a task can process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have implemented a constructor to initialize all these attributes and methods
    to get and set its values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `compute()` method first checks the number of clusters the task has to
    process. If that number is less than or equal to the `maxSize` attribute, it processes
    those clusters and updates their centroid:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'If the number of clusters the task has to process is too big, we are going
    to divide the set of clusters the task has to process in two and create two tasks
    to process each of that part as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The ConcurrentKMeans class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `ConcurrentKMeans` class implements the concurrent version of the k-means
    clustering algorithm. As the serial version, the main method of the class is the
    `calculate()` method. It receives the following as parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: The array of `ConcurrentDocument` objects with the information about the documents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of clusters you want to generate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The size of the vocabulary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A seed for the random number generator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The maximum number of items a Fork/Join tasks will process without splitting
    the task into other tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `calculate()` method returns an array of the `ConcurrentDocumentCluster`
    objects with the information of the clusters. Each cluster has the list of documents
    associated with it. First, the document creates the array of clusters determined
    by the `numberClusters` parameter and initializes them using the `initialize()`
    method and a `Random` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we repeat the assignment and update phases until all the documents stay
    in the same cluster. Before the loop, we create `ForkJoinPool` that is going to
    execute that task and all of its subtasks. Once the loop has finished, as with
    other `Executor` objects, we have to use the `shutdown()` method with a Fork/Join
    pool to finish its executions. Finally, we return the array of clusters with the
    final organization of the documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The assignment phase is implemented in the `assignment()` method. This method
    receives the array of clusters, the array of documents, and the `maxSize` attribute.
    First, we delete the list of associated documents to all the clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we initialize the necessary objects: an `AtomicInteger` to store the
    number of documents whose assigned cluster has changed and the `AssignmentTask`
    that will begin the process.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we execute the tasks in the pool in an asynchronous way using the `execute()`
    method of `ForkJoinPool` and wait for its finalization with the `join()` method
    of the `AssignmentTask` object as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we check the number of documents that has changed its assigned cluster.
    If there have been changes, we return the `true` value. Otherwise, we return the
    `false` value. We have the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The update phase is implemented in the `update()` method. It receives the array
    of clusters and the `maxSize` parameters. First, we create an `UpdateTask` object
    to update all the clusters. Then, we execute that task in the `ForkJoinPool` object
    the method receives as parameter as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The ConcurrentMain class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `ConcurrentMain` class includes the `main()` method to launch the tests
    of the k-means algorithm. Its code is equal to the `SerialMain` class, but changing
    the serial classes for the concurrent ones.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing the solutions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To compare the two solutions, we have executed different experiments changing
    the values of three different parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: The k-parameter will establish the number of clusters we want to generate. We
    have tested the algorithms with the values 5, 10, 15, and 20.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The seed for the `Random` number generator. This seed determines how the initial
    centroid positions. We have tested the algorithms with the values 1 and 13.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the concurrent algorithm, the `maxSize` parameter that determines the maximum
    number of items (documents or clusters), a task can process without being split
    into other tasks. We have tested the algorithms with the values 1, 20, and 400.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We have executed the experiments using the JMH framework ([http://openjdk.java.net/projects/code-tools/jmh/](http://openjdk.java.net/projects/code-tools/jmh/))
    that allows you to implement micro benchmarks in Java. Using a framework for benchmarking
    is a better solution that simply measures time using methods such as `currentTimeMillis()`
    or `nanoTime()`. We have executed them 10 times in a computer with a four-core
    processor and calculated the medium execution time of those 10 times. These are
    the execution times we have obtained in milliseconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   |   | Serial | Concurrent |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **K** | **Seed** |   | **MaxSize=1** | **MaxSize=20** | **maxSize=400** |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 1 | 6676.141 | 4696.414 | 3291.397 | 3179.673 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 1 | 6780.088 | 3365.731 | 2970.056 | 2825.488 |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | 1 | 12936.178 | 5308.734 | 4737.329 | 4490.443 |'
  prefs: []
  type: TYPE_TB
- en: '| 20 | 1 | 19824.729 | 7937.820 | 7347.445 | 6848.873 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 13 | 3738.869 | 2714.325 | 1984.152 | 1916.053 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 13 | 9567.416 | 4693.164 | 3892.526 | 3739.129 |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | 13 | 12427.589 | 5598.996 | 4735.518 | 4468.721 |'
  prefs: []
  type: TYPE_TB
- en: '| 20 | 13 | 18157.913 | 7285.565 | 6671.283 | 6325.664 |'
  prefs: []
  type: TYPE_TB
- en: 'We can draw the following conclusions:'
  prefs: []
  type: TYPE_NORMAL
- en: The seed has an important and unpredictable impact in the execution time. Sometimes,
    the execution times are lower with seed 13, but other times are lower with seed
    1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you increment the number of clusters, the execution time increments too.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `maxSize` parameter doesn't have much influence in the execution time. The
    parameter K or seed has a higher influence in the execution time. If you increase
    the value of the parameter, you will obtain better performance. The difference
    is bigger between 1 and 20 than between 20 and 400.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In all the cases, the concurrent version of the algorithm has better performance
    than the serial one.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, if we compare the serial algorithm with parameters K=20 and seed=13
    with the concurrent version with parameters K=20, seed=13, and maxSize=400 using
    the speed-up, we obtain the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Comparing the solutions](img/00017.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The second example – a data filtering algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Suppose that you have a lot of data that describes a list of items. For example,
    you have a lot of attributes (name, surname, address, phone number, and so on)
    of a lot of people. It's a common need to obtain the data that meets certain criteria,
    for example, you want to obtain people who live in a determined street or with
    a determined name.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you will implement one of those filtering programs. We have
    used the **Census-Income KDD** dataset from the UCI (you can download it from
    [https://archive.ics.uci.edu/ml/datasets/Census-Income+%28KDD%29](https://archive.ics.uci.edu/ml/datasets/Census-Income+%28KDD%29)),
    that contains weighted census data extracted from the 1994 and 1995 current population
    surveys conducted by the U.S. Census Bureau.
  prefs: []
  type: TYPE_NORMAL
- en: In the concurrent version of this example, you will learn how to cancel tasks
    that are running in the Fork/Join pool and how to manage unchecked exceptions
    that can be thrown in a task.
  prefs: []
  type: TYPE_NORMAL
- en: Common parts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have implemented some classes to read the data from a file and to filter
    the data. These classes are used by the serial and concurrent versions of the
    algorithm. These are the classes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `CensusData` class: This class stores 39 attributes that define every person.
    It defines the attributes and methods to get and set their value. We are going
    to identify each attribute by a number. The `evaluateFilter()` method of this
    class contains the association between the number and the name of the attribute.
    You can check the file [https://archive.ics.uci.edu/ml/machine-learning-databases/census-income-mld/census-income.names](https://archive.ics.uci.edu/ml/machine-learning-databases/census-income-mld/census-income.names)
    to get the details of every attribute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `CensusDataLoader` class: This class loads the census data from a file.
    It has the `load()` method that receives the path to the file as an input parameter
    and returns an array of `CensusData` with the information of all the persons of
    the file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `FilterData` class: This class defines a filter of data. A filter includes
    the number of an attribute and the value of that attribute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `Filter` class: This class implements the methods to determine if a `CensusData`
    object meets the conditions of a list of filters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We don't include the source code of these classes. They are very simple, and
    you can check the source code of the example.
  prefs: []
  type: TYPE_NORMAL
- en: The serial version
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have implemented the serial version of the filter algorithm in two classes.
    The `SerialSearch` class makes the filtering of data. It provides two methods:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `findAny()` method: It receives the array of `CensusData` object as a parameter
    with all the data from the file and a list of filters and returns a `CensusData`
    object with the first person it finds that meet all the criteria from the filters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `findAll()` method: It receives the array of `CensusData` object as a parameter
    with all the data from the file and a list of filters and returns an array of
    `CensusData` objects with all the persons that meets all the criteria from the
    filter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `SerialMain` class implements the `main()` method of this version and tests
    it to measure the execution time of this algorithm in some circumstances.
  prefs: []
  type: TYPE_NORMAL
- en: The SerialSearch class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we mentioned before, this class implements the filtering of data. It provides
    two methods. The first one, the `findAny()` method, looks for the first data object
    that meets the filters. When it finds the first data object, it finishes its execution.
    Refer to the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The second one, the `findAll()` method, returns an array of `CensusData` objects
    with all the objects that meet the filters as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The SerialMain class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You''re going to use this class to test the filtering algorithm in different
    circumstances. First, we load the data from the file as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The first case we are going to test is to use the `findAny()` method to find
    an object that exists in the first places of the array. You construct a list of
    filters and then call the `findAny()` method with the data of the file and the
    list of filters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Our filters look for the following attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`32`: This is the country of the birth father attribute'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`31`: This is the country of the birth mother attribute'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`1`: This is the class of the worker attributes; `Not in universe` is one of
    their possible values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`14`: This is the reason for unemployment attribute; `Not in universe` is one
    of their possible values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We are going to test other cases as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Use the `findAny()` method to find an object that exists in the last positions
    of the array
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the `findAny()`method to try to find an object that doesn't exist
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the `findAny()` method in an error situation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the `findAll()` method to obtain all the objects that meet a list of filters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the `findAll()` method in an error situation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The concurrent version
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We are going to include more elements in our concurrent version:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A task manager: When you use the Fork/Join framework, you start with one task
    and you split that task into two (or more) child tasks that you split again and
    again until your problem has the desired size. There can be situations when you
    want to finish the execution of all those tasks. For example, when you implement
    the `findAny()` method and you find an object that meets all the criteria, you
    don''t need to continue with the execution of the rest of the tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A `RecursiveTask` class to implement the `findAny()` method: It''s the `IndividualTask`
    class that extends `RecursiveTask`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A `RecursiveTask` class to implement the `findAll()` method: It''s the `ListTask`
    class that extends `RecursiveTask`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's see the details of all those classes.
  prefs: []
  type: TYPE_NORMAL
- en: The TaskManager class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We are going to use this class to control the cancellation of tasks. We are
    going to cancel the execution of tasks in the following two situations:'
  prefs: []
  type: TYPE_NORMAL
- en: You're executing the `findAny()` operation and you find an object that meets
    the requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You're executing the `findAny()` or `findAll()` operations and there's an unchecked
    exception in one of the tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The class declares two attributes: `ConcurrentLinkedDeque` to store all the
    tasks we need to cancel and an `AtomicBoolean` variable to guarantee that only
    one task executes the `cancelTasks()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'It defines methods to add a task to `ConcurrentLinkedDeque`, delete a task
    from the `ConcurrentLinkedDeque`, and cancel all the tasks stored in it. To cancel
    the tasks, we use the `cancel()` method defined in the `ForkJoinTask` class. The
    `true` parameter forces the interruption of the task if it is running as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The `cancelTasks()` method receives a `RecursiveTask` object as a parameter.
    We're going to cancel all the tasks except the one that is calling this method.
    We don't want to cancel the tasks that have found the result. The `compareAndSet(false,
    true)` method sets the `AtomicBoolean` variable to `true` and returns `true` only
    if the current value is `false`. If the `AtomicBoolean` variable already has a
    `true` value, then `false` is returned. The whole operation is performed atomically,
    so it's guaranteed that the body of if statement will be executed at most once
    even if the `cancelTasks()` method is concurrently called several times from different
    threads.
  prefs: []
  type: TYPE_NORMAL
- en: The IndividualTask class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `IndividualTask` class extends the `RecursiveTask` class parameterized
    with the `CensusData` task and implements the `findAny()` operation. It defines
    the following attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: An array with all the `CensusData` objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `start` and `end` attributes that determine the elements it has to process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `size` attribute that determines the maximum number of elements the task
    will process without splitting the task
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `TaskManager` class to cancel the tasks if necessary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code gives a list of filters to apply:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The main method of the class is the `compute()` method. It returns a `CensusData`
    object. If the number of elements the task has to process is less than the size
    attribute, it looks for the object directly. If the method finds the desired object,
    it returns the object and uses the method `cancelTasks()` to cancel the execution
    of the rest of the tasks. If the method doesn''t find the desired object, it returns
    null. We have the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'If the number of items it has to process is more than the size attribute, we
    create two child tasks to process half of the elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we add the new created tasks to the task manager and deleted the actual
    tasks. If we want to cancel the tasks, we want to cancel only the tasks that are
    running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Then, we send the tasks to `ForkJoinPool` with the `fork()` method that sends
    them in an asynchronous way and wait for its finalization with the `quietlyJoin()`
    method. The difference between the `join()` and `quietlyJoin()` methods is that
    the `join()` method launches and exception if the task is canceled or an unchecked
    exception is thrown inside the method while the `quietlyJoin()` method doesn't
    throw any exception.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we delete the child tasks from the `TaskManager` class as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we obtain the results of the tasks using the `join()` method. If a task
    throws an unchecked exception, it will be propagated without special handling
    and cancellation will be just ignored as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The ListTask class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `ListTask` class extends the `RecursiveTask` class parameterized with a
    `List` of `CensusData`. We are going to use this task to implement the `findAll()`
    operation. It's very similar to the `IndividualTask` task. Both use the same attributes,
    but they have differences in the `compute()` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we initialize a `List` object to return the results and check the number
    of elements the task has to process. If the number of elements the task has to
    process is less than the size attribute, add all the objects that meet the criteria
    specified in the filters to the list of results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'If the number of items it has to process is more than the size attribute, we
    will create two child tasks to process half of the elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will add the new created tasks to the task manager and delete the
    actual tasks. The actual task won''t be canceled; its child tasks will be canceled,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will send the tasks to `ForkJoinPool` with the `fork()` method that
    sends them in an asynchronous way and wait for its finalization with the `quietlyJoin()`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will delete the child tasks from `TaskManager`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we obtain the results of the tasks using the `join()` method. If a task
    throws an unchecked exception, it will be propagated without special handling
    and cancellation will be just ignored:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The ConcurrentSearch class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `ConcurrentSearch` class implements the `findAny()` and `findAll()` methods.
    They have the same interface as the ones of the serial version. Internally, they
    initialize the `TaskManager` object and the first task and send to default `ForkJoinPool`
    using the `execute` method; they wait for the finalization of the task and write
    the results. This is the code of the `findAny()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the code of the `findAll()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: The ConcurrentMain class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `ConcurrentMain` class is used to test the concurrent version of our object
    filter. It is identical to the `SerialMain` class, but uses the concurrent version
    of the operations.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing the two versions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To compare the serial and concurrent versions of the filtering algorithm, we
    tested them in six different situations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Test 1**: We test the `findAny()` method looking for an object that exists
    in the first positions of the `CensusData` array'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test 2**: We test the `findAny()` method looking for an object that exists
    in the last positions of the `CensusData` array'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test 3**: We test the `findAny()` method looking for an object that doesn''t
    exist'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test 4**: We test the `findAny()` method in an error situation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test 5**: We test the `findAll()` method in a normal situation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test 6**: We test the `findAll()` method in an error situation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the concurrent version of the algorithm, we have tested three different
    values of the size parameter that determines the maximum number of elements a
    task can process without forking in two child tasks. We have tested with 10, 200,
    and 2,000.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have executed the tests using the JMH framework ([http://openjdk.java.net/projects/code-tools/jmh/](http://openjdk.java.net/projects/code-tools/jmh/))
    that allows you to implement micro benchmarks in Java. Using a framework for benchmarking
    is a better solution that simply measures time using methods as `currentTimeMillis()`
    or `nanoTime()`. We have executed them 10 times in a computer with a four-core
    processor and calculated the medium execution time of those 10 times. As with
    other examples, we have measured the execution time in milliseconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Test case | Serial | Concurrent size = 10 | Concurrent size = 200 | Concurrent
    size = 2000 | Best |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Test 1** | 1.177 | 8.124 | 4.547 | 4.073 | Serial |'
  prefs: []
  type: TYPE_TB
- en: '| **Test 2** | 95.237 | 157.412 | 34.581 | 35.691 | Concurrent |'
  prefs: []
  type: TYPE_TB
- en: '| **Test 3** | 66.616 | 41.916 | 74.829 | 37.140 | Concurrent |'
  prefs: []
  type: TYPE_TB
- en: '| **Test 4** | 0.540 | 25869.339 | 643.144 | 9.673 | Serial |'
  prefs: []
  type: TYPE_TB
- en: '| **Test 5** | 61.752 | 37.349 | 40.344 | 22.911 | Concurrent |'
  prefs: []
  type: TYPE_TB
- en: '| **Test 6** | 0.802 | 31663.607 | 231.440 | 7.706 | Serial |'
  prefs: []
  type: TYPE_TB
- en: 'We can draw the following conclusions:'
  prefs: []
  type: TYPE_NORMAL
- en: The serial version of the algorithm has better performance when we have to process
    a smaller number of elements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The concurrent version of the algorithm has better performance when we have
    to process all the elements or a bit amount of them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In error situations, the serial version of the algorithm has better performance
    than the concurrent version. The concurrent version has a very poor performance
    in this situation when the value of the `size` parameter is small.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this case, concurrency does not always gives us an improvement of the performance.
  prefs: []
  type: TYPE_NORMAL
- en: The third example – the merge sort algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The merge sort algorithm is a very popular sorting algorithm that is always
    implemented using the divide and conquer technique, so it's a very good candidate
    to test with the Fork/Join framework.
  prefs: []
  type: TYPE_NORMAL
- en: To implement the merge sort algorithm, we divide the unsorted lists into sublists
    of one element. Then, we merge those unsorted sublists to produce ordered sublists
    until we have processed all the sublists, and we have only the original list,
    but with all the elements sorted.
  prefs: []
  type: TYPE_NORMAL
- en: To make the concurrent version of our algorithm, we have used the new Fork/Join
    tasks, the `CountedCompleter` tasks, introduced in the Java 8 version. The most
    important characteristics of these tasks are that they include a method to be
    executed when all their child tasks have finished their execution.
  prefs: []
  type: TYPE_NORMAL
- en: To test out implementations, we have used the **Amazon product co-purchasing
    network metadata** (you can download it from [https://snap.stanford.edu/data/amazon-meta.html](https://snap.stanford.edu/data/amazon-meta.html)).
    In particular, we have created a list with the salesrank of 542,184 products.
    We are going to test our versions of the algorithm, sorting this list of products,
    and compare the execution time with the `sort()` and `parallelSort()` methods
    of the `Arrays` class.
  prefs: []
  type: TYPE_NORMAL
- en: Shared classes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we mentioned earlier, we have built a list of 542,184 products of Amazon
    with information about each product including an ID, its title, group, salesrank,
    number of reviews, number of similar products, and number of categories the product
    belongs to. We have implemented the `AmazonMetaData` class to store the information
    of a product. This class declares the necessary attributes and the methods to
    get and set their values. This class implements the `Comparable` interface to
    compare two instances of this class. We want to sort the elements by salesrank
    in ascending order. To implement the `compare()` method, we use the `compare()`
    method of the `Long` class to compare the salesrank of both objects as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: We have also implemented `AmazonMetaDataLoader` that provides the `load()` method.
    This method receives a route to the file with the data as a parameter and returns
    an array of `AmazonMetaData` objects with the information of all the products.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We don't include the source code of these classes to focus on the characteristics
    of the Fork/Join framework.
  prefs: []
  type: TYPE_NORMAL
- en: The serial version
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have implemented the serial version of the merge sort algorithm in the `SerialMergeSort`
    class, which implements the algorithm and the `SerialMetaData` class and provides
    the `main()` method to test the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: The SerialMergeSort class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `SerialMergeSort` class implements the serial version of the merge sort
    algorithm. It provides the `mergeSort()` method that receives the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: The array with all the data we want to sort
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first element the method has to process (included)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last element the method has to process (not included)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If the method has to process only one element, it returns. Otherwise, it makes
    two recursive calls to the `mergeSort()` method. The first call will process the
    first half of the elements, and the second call will process the second half of
    the elements. Finally, we make a call to the `merge()` method to merge the two
    halves of the elements and get a sorted list of elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: We have used the `(end+start)>>>1` operator to obtain the mid-element to split
    the array. If you have, for example, 1.5 billions of elements (not that impossible
    with modern memory chips), it still fits the Java array. However, *(end+start)/2*
    will overflow resulting in a negative number array. You can find a detailed explanation
    of this problem at [http://googleresearch.blogspot.ru/2006/06/extra-extra-read-all-about-it-nearly.html](http://googleresearch.blogspot.ru/2006/06/extra-extra-read-all-about-it-nearly.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'The `merge()` method merges two lists of elements to obtain a sorted list.
    It receives the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: The array with all the data we want to sort
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The three elements (`start`, `mid`, and `end`) that determine the two parts
    of the array (start-mid, mid-end) we want to merge and sort
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We create a temporary array to sort the elements, sort the elements in the
    array processing both parts of the list, and store the sorted list in the same
    positions of the original array. Check the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: The SerialMetaData class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `SerialMetaData` class provides the `main()` method to test the algorithm.
    We''re going to execute every sort algorithm 10 times to calculate the average
    execution time. First, we load the data from the file and create a copy of the
    array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we sort the first array using the `sort()` method of the `Arrays` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we sort the second array using our implementation of the merge sort algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we check that the sorted arrays are identical:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: The concurrent version
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we mentioned before, we are going to use the new Java 8 `CountedCompleter`
    class as the base class for our Fork/Join tasks. This class provides a mechanism
    to execute a method when all its child tasks have finished their execution. It's
    the `onCompletion()` method. So, we use the `compute()` method to divide the array
    and the `onCompletion()` method to merge the sublists into an ordered list.
  prefs: []
  type: TYPE_NORMAL
- en: 'The concurrent solution you are going to implement has three classes:'
  prefs: []
  type: TYPE_NORMAL
- en: The `MergeSortTask` class that extends the `CountedCompleter` class and implements
    the task that executes the merge sort algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `ConcurrentMergeSort` task that launches the first task
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `ConcurrentMetaData` class that provides the `main()` method to test the
    concurrent version of the merge sort algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The MergeSortTask class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we mentioned earlier, this class implements the tasks that are going to
    execute the merge sort algorithm. This class uses the following attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: The array of data we want to sort
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The start and end positions of the array the task has to sort
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The class also has a constructor to initialize its parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The `compute()` method, if the difference between the start and end indexes
    are greater or equal that `1024`, we split the task into two child tasks to process
    two subsets of the original set. Both tasks use the `fork()` method to send a
    task to the `ForkJoinPool` in an asynchronous way. Otherwise, we execute `SerialMergeSorg.mergeSort()`
    to sort the part of the array (which have `1024` or less elements) and then we
    call the `tryComplete()` method. This method will internally call the `onCompletion()`
    method when the child task has finished its execution. Take a look at the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'In our case, we will use the `onCompletion()` method to make the merge and
    sort operations to obtain the sorted list. Once a task finishes the execution
    of the `onCompletion()` method, it calls `tryComplete()` over its parent to try
    to complete that task. The source code of the `onCompletion()` method is very
    similar to the `merge()` method of the serial version of the algorithm. Refer
    to the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: The ConcurrentMergeSort class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the concurrent version, this class is very simple. It implements the `mergeSort()`
    method that receives the array of data to sort and the start index (which will
    always be 0) and the end index (which will always be the length of the array)
    to sort the array as parameters. We have chosen to maintain the same interface
    rather than the serial version.
  prefs: []
  type: TYPE_NORMAL
- en: The method creates a new `MergeSortTask`, sends it to the default `ForkJoinPool`
    using the `invoke()` method that returns when the task has finished its execution
    and the array is sorted.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: The ConcurrentMetaData class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `ConcurrentMetaData` class provides the `main()` method to test the concurrent
    version of the merge sort algorithm. In our case, the code is equal to the code
    of the `SerialMetaData` class, but using the concurrent versions of the classes
    and the `Arrays.parallelSort()` method instead of the `Arrays.sort()` method,
    so we don't include the source code of the class.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing the two versions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have executed our serial and concurrent versions of the merge sort algorithm
    and compared the execution times between them and against the methods `Arrays.sort()`
    and `Arrays.parallelSort()`. We have executed the four versions using the JMH
    framework ([http://openjdk.java.net/projects/code-tools/jmh/](http://openjdk.java.net/projects/code-tools/jmh/))
    that allows you to implement micro benchmarks in Java. Using a framework for benchmarking
    is a better solution that simply measures time using methods as `currentTimeMillis()`
    or `nanoTime()`. We have executed them 10 times in a computer with a four-core
    processor and calculated the medium execution time of those 10 times. These are
    the execution times in millisecond we have obtained when we sort our dataset with
    542,184 objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | Arrays.sort() | Serial merge sort | Arrays.parallelSort() | Concurrent
    merge sort |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Execution time (ms)** | 561.324 | 711.004 | 261.418 | 353.846 |'
  prefs: []
  type: TYPE_TB
- en: 'We can draw the following conclusions:'
  prefs: []
  type: TYPE_NORMAL
- en: The method `Arrays.parallelSort()` obtains the best result. For serial algorithms,
    the `Arrays.sort()` method obtains better execution time than our implementation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For our implementations, the concurrent version of the algorithm has better
    performance than the serial one.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can compare our serial and concurrent versions of the merge sort algorithm
    using the speed-up:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Comparing the two versions](img/00018.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Other methods of the Fork/Join framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the three examples of this chapter, we have used a lot of methods of the
    class that forms the Fork/Join framework, but there are other interesting methods
    you have to know.
  prefs: []
  type: TYPE_NORMAL
- en: We have used the methods `execute()` and `invoke()` from the `ForkJoinPool`
    class to send tasks to the pool. We can use another method named `submit()`. The
    main difference between them is that the `execute()` method sends the task to
    `ForkJoinPool` and returns immediately a void value, the `invoke()` method sends
    the task to the `ForkJoinPool` and returns when the task has finished its execution,
    and the `submit()` method sends the task to the `ForkJoinPool` and returns immediately
    a `Future` object to control the status of the task and obtain its result.
  prefs: []
  type: TYPE_NORMAL
- en: In all the examples of this chapter, we have used classes based on the `ForkJoinTask`
    class, but you can use the `ForkJoinPool` tasks based on the `Runnable` and `Callable`
    interfaces. To do this, you can use the method `submit()` that has versions that
    accept a `Runnable` object, a `Runnable` object with a result, and a `Callable`
    object.
  prefs: []
  type: TYPE_NORMAL
- en: The `ForkJoinTask` class provides the method `get(long timeout, TimeUnit unit)`
    to obtain the results returned by a task. This method waits for the period of
    time specified in the parameters for the result of the task. If the task finishes
    its execution before that period of time, the method returns the result. Otherwise,
    it throws a `TimeoutException` exception.
  prefs: []
  type: TYPE_NORMAL
- en: The `ForkJoinTask` provides an alternative to the `invoke()` method. It is the
    `quietlyInvoke()` method. The main difference between the two versions is that
    the `invoke()` method returns the result of the execution of the task or throws
    any exception if necessary. The `quietlyInvoke()` method don't return the result
    of the task and doesn't throw any exception. It's similar to the `quietlyJoin()`
    method used in the examples.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The divide and conquer design technique is a very popular approach to solve
    different kinds of problems. You divide the original problem into smaller problems
    and those problems into smaller ones until we have enough simple problems to solve
    it directly. In version 7, the Java concurrency API introduced a special kind
    of `Executor` optimized for these kinds of problems. It''s the Fork/Join Framework.
    It''s based on the following two operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**fork**: This allows you to create a new child task'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**join**: This allows you to wait for the finalization of a child task and
    get its results'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using those operations, Fork/Join tasks have the following appearance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: In this chapter, you have solved three different problems using the Fork/Join
    framework such as the k-means clustering algorithm, a data filtering algorithm,
    and the merge sort algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'You have used default `ForkJoinPool`, provided by the API (this is a new feature
    of the Java 8 version), and created a new `ForkJoinPool` object. You have also
    used the three types of `ForkJoinTask` `s`:'
  prefs: []
  type: TYPE_NORMAL
- en: The `RecursiveAction` class, used as the base class for those `ForkJoinTasks`
    that don't return a result.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `RecursiveTask` class, used as the base class for those `ForkJoinTasks`
    that return a result.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `CountedCompleter` class, introduced in Java 8 and used as the base class
    for those `ForkJoinTasks` that need to execute a method or launch another task
    when all its child subtasks finish their execution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next chapter, you will learn how to use the MapReduce programming technique
    using the new Java 8 **parallel streams** to get the best performance processing
    very big datasets.
  prefs: []
  type: TYPE_NORMAL
