- en: Machine Learning with the ML Module
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will move on to the currently supported machine learning
    module of PySpark—the ML module. The ML module, like MLLib, exposes a vast array
    of machine learning models, almost completely covering the spectrum of the most-used
    (and usable) models. The ML module, however, operates on Spark DataFrames, making
    it much more performant as it can leverage the tungsten execution optimizations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn about the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Transformers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing Estimators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing Pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting the most predictable features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting forest coverage types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimating forest elevation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering forest cover types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tuning hyperparameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting features from text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discretizing continuous variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Standardizing continuous variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Topic mining
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this chapter, we will use data we downloaded from [https://archive.ics.uci.edu/ml/datasets/covertype](https://archive.ics.uci.edu/ml/datasets/covertype).
    The dataset is located in the GitHub repository for this book: `/data/forest_coverage_type.csv`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We load the data in the same manner as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Introducing Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `Transformer` class, introduced in Spark 1.3, transforms one dataset into
    another by normally appending one or more columns to the existing DataFrame. Transformers
    are an abstraction around methods that actually transform features; the abstraction
    also includes trained machine learning models (as we will see in the following
    recipes).
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will introduce two Transformers: `Bucketizer` and `VectorAssembler`.
  prefs: []
  type: TYPE_NORMAL
- en: We will not be introducing all the Transformers; throughout the rest of this
    chapter, the most useful ones will show up. For the rest, the Spark documentation
    is a good place to learn what they do and how to use them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a list of all of the Transformers that convert one feature into another:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Binarizer` is a method that, given a threshold, transforms a continuous numerical
    feature into a binary one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Bucketizer`, similarly to `Binarizer`, uses a list of thresholds to transform
    a continuous numerical variable into a discrete one (with as many levels as the
    length of the list of thresholds plus one).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ChiSqSelector` helps to select a predefined number of features that explain
    the most of the variance of a categorical target (a classification model).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CountVectorizer` converts many lists of strings into a `SparseVector` of counts,
    where each column is a flag for each distinct string found in the lists, and the
    value indicates how many times the string was found in the current list.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DCT` stands for the **Discrete Cosine Transform**. It takes a vector of real
    values and returns a vector of cosine functions oscillating at different frequencies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ElementwiseProduct` can be used to scale your numerical features as it takes
    a vector of values and multiplies it (element by element, as the name suggests)
    by another vector with weights for each value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HashingTF` is a hashing trick transformer that returns a vector (of specified
    length) representation for a tokenized text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IDF` computes an **Inverse Document Frequency** for a list of records, where
    each record is a numerical representation of a body of text (see either `CountVectorizer`
    or `HashingTF`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IndexToString` uses the encoding from the `StringIndexerModel` object to reverse
    the string index to original values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MaxAbsScaler` rescales the data to be within the `-1` to `1` range.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MinMaxScaler` rescales the data to be within the `0` to `1` range.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NGram` returns pairs, triplets, or *n*-mores of subsequent words of a tokenized
    text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Normalizer` scales the data to be of unit norm (by default, `L2`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`OneHotEncoder` encodes a categorical variable into a vector representation
    where only one element is hot, that is, equal to `1` (all others are `0`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PCA` is a dimensionality reduction method to extract principal components
    from data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PolynomialExpansion` returns a polynomial expansion of an input vector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`QuantileDiscretizer` is a similar method to `Bucketizer`, but instead of defining
    the thresholds, only the number of returned bins needs to be specified; the method
    will use quantiles to decide the thresholds.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RegexTokenizer` is a string tokenizer the uses regular expressions to process
    text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RFormula` is a method to pass R-syntax formula to transform data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SQLTransformer` is a method to pass SQL syntax formula to transform data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`StandardScaler` converts a numerical feature to have a 0 mean and a standard
    deviation of 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`StopWordsRemover` is used to remove words such as `a` or `the` from tokenized
    text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`StringIndexer` produces a vector of indices given a list of all words in a
    column.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Tokenizer` is a default tokenizer that takes a sentence (a string), splits
    it on a space, and normalizes the words.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`VectorAssembler` combines the specified (separate) features into a single
    feature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`VectorIndexer` takes a categorical variable (already encoded to be numbers)
    and returns a vector of indices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`VectorSlicer` can be thought of as a converse of `VectorAssembler`, as it
    extracts the data from the vector of features given indices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Word2Vec` converts a sentence (or string) into a map of `{string, vector}`
    representation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To execute this recipe, you will need a working Spark environment and you would
    have already loaded the data into the forest DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: No other prerequisites are required.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Any ideas why we could not use `.QuantileDiscretizer(...)` to achieve this?
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As always, we first load the necessary module we will use throughout, `pyspark.sql.functions`,
    which will allow us to calculate minimum and maximum values of the `Horizontal_Distance_To_Hydrology` feature. `pyspark.ml.feature`
    exposes the `.Bucketizer(...)` transformer for us to use, while NumPy will help
    us to create an equispaced list of thresholds.
  prefs: []
  type: TYPE_NORMAL
- en: We want to bucketize our numerical variable into 10 buckets, hence our `buckets_no`
    is equal to `10`. Next, we calculate the minimum and maximum values for the `Horizontal_Distance_To_Hydrology` feature
    and return these two values to the driver. On the driver, we create the list of
    thresholds (the `splits` list); the first parameter to the `np.arange(...)` method
    is the minimum, the second one is the maximum, and the third one defines the size
    of each step.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the splits list defined, we pass it to the `.Bucketizer(...)`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: Each transformer (Estimators work similarly) has a very similar API, but two
    parameters are always required: `inputCol` and `outputCol`, which define the input
    and output columns to be consumed and their output, respectively. The two classes—`Transformer` and
    `Estimator`—also universally implement the `.getOutputCol()` method, which returns
    the name of the output column.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we use the `bucketizer` object to transform our DataFrame. Here''s
    what we expect to see:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00134.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Almost exclusively, every estimator (or, in other words, an ML model) found
    in the ML module expects to see a *single* column as an input; the column should
    contain all the features a data scientist wants such a model to use. The `.VectorAssembler(...)`
    method, as the name suggests, collates multiple features into a single column.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: First, we use the `.VectorAssembler(...)` method to collate all columns from
    our `forest` DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the  `.VectorAssembler(...)` method, unlike other Transformers, has
    the `inputCols` parameter, not `inputCol`, as it accepts a list of columns, not
    just a single column.
  prefs: []
  type: TYPE_NORMAL
- en: We then use the `feat` column (which is now a `SparseVector` of all the features)
    in the `PCA(...)` method to extract the top five most significant principal components.
  prefs: []
  type: TYPE_NORMAL
- en: Notice how we can now use the `.getOutputCol()` method to get the name of the
    output column? It should become more apparent why we do this when we introduce
    pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the preceding code should look somewhat as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00135.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For an example of a transformer (and more) check this blog post: [https://blog.insightdatascience.com/spark-pipelines-elegant-yet-powerful-7be93afcdd42](https://blog.insightdatascience.com/spark-pipelines-elegant-yet-powerful-7be93afcdd42)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing Estimators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `Estimator` class, just like the `Transformer` class, was introduced in
    Spark 1.3\. The Estimators, as the name suggests, estimate the parameters of a
    model or, in other words, fit the models to data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we will introduce two models: the linear SVM acting as a classification
    model, and a linear regression model predicting the forest elevation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a list of all of the Estimators, or machine learning models, available
    in the ML module:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Classification:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LinearSVC` is an SVM model for linearly separable problems. The SVM''s kernel
    has the ![](img/00136.jpeg) form (a hyperplane), where ![](img/00137.jpeg) is the
    coefficients (or a normal vector to the hyperplane), ![](img/00138.jpeg) is the
    records, and *b* is the offset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LogisticRegression` is a default, *go-to* classification model for linearly
    separable problems. It uses a logit function to calculate the probability of a
    record being a member of a particular class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DecisionTreeClassifier` is a decision tree-based model used for classification
    purposes. It builds a binary tree with the proportions of classes in the terminal
    nodes determining the class membership.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`GBTClassifier` is a member of the group of ensemble models. The **Gradient-Boosted
    Trees** (**GBT**) build several weak models that, when combined, form a strong
    classifier. The model can also be applied to solve regression problems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RandomForestClassifier` is also a member of an ensemble group of models. Unlike
    GBT, however, random forests grows fully-grow decision trees and the total error
    reduction is achieved by reducing variance (while GBTs reduce bias). Just like
    GBT, these models can also be used to solve regression problems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NaiveBayes` uses the Bayes conditional probability theory, ![](img/00139.jpeg),
    to classify observations based on evidence and prior assumptions about the probability
    and likelihood.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MultilayerPerceptronClassifier` is derived from the field of artificial intelligence,
    and, more narrowly, artificial neural networks. The model consists of a directed
    graph of artificial neurons that mimic (to some extent) the fundamental building
    blocks of the brain.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`OneVsRest` is a reduction technique that selects only one class in a multinomial
    scenario.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Regression:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AFTSurvivalRegression` is a parametric model that predicts life expectancy
    and assumes that a marginal effect of one of the features accelerates or decelerates
    a process failure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DecisionTreeRegressor`, a counterpart of `DecisionTreeClassifier`, is applicable
    for regression problems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`GBTRegressor`, a counterpart of `GBTClassifier`, is applicable for regression
    problems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`GeneralizedLinearRegression` is a family of linear models that allow us to
    specify different kernel functions (or link functions). Unlike linear regression,
    which assumes the normality of error terms, the **Generalized Linear Model** (**GLM**)
    allow models to have other distributions of error terms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IsotonicRegression` fits a free-form and non-decreasing line to data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LinearRegression` is the benchmark of regression models. It fits a straight
    line (or a plane defined in linear terms) through the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RandomForestRegressor`, a counterpart of `RandomForestClassifier`, is applicable
    for regression problems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Clustering:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BisectingKMeans` is a model that begins with all observations in a single
    cluster and iteratively splits the data into *k* clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Kmeans` separates data into *k* (defined) clusters by iteratively finding
    centroids of clusters by shifting the cluster boundaries so the sum of all distances
    between data points and cluster centroids is minimized.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`GaussianMixture` uses *k* Gaussian distributions to break the dataset down
    into clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LDA`: The **Latent Dirichlet Allocation** is a model frequently used in topic
    mining. It is a statistical model that makes use of some unobserved (or unnamed)
    groups to cluster observations. For example, a `PLANE_linked` cluster can have
    words included, such as engine, flaps, or wings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To execute this recipe, you will need a working Spark environment and you would
    have already loaded the data into the `forest` DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: No other prerequisites are required.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, let''s learn how to build an SVM model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `.LinearSVC(...)` method is available from `pyspark.ml.classification`,
    so we load it first.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we use `.VectorAssembler(...)` to grab all the columns from the `forest`
    DataFrame, but the last one (the `CoverType`) will be used as a label. We will
    predict the forest cover type equal to `1`, that is, whether the forest is a spruce-fir
    type; we achieve this by checking whether `CoverType` is equal to `1` and casting
    the resulting Boolean to an integer. Finally, we select only `label` and `features`.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we create the `LinearSVC` object. We specify the maximum number of iterations
    to 10 and set the regularization parameter (type L2, or ridge) to 1%.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are not familiar with regularization in terms of machine learning, check
    out this website: [http://enhancedatascience.com/2017/07/04/machine-learning-explained-regularization/](http://enhancedatascience.com/2017/07/04/machine-learning-explained-regularization/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Other parameters include:'
  prefs: []
  type: TYPE_NORMAL
- en: '`featuresCol`: This is set to the name of the features columns, by default
    it is `features` (like in our dataset)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labelCol`: This is set to the name of the label column if something other
    than `label`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`predictionCol`: This is set to the name of the prediction column if you want
    to rename it to something other than `prediction`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tol`: This is a stopping parameter that defines the minimum change between
    iterations in terms of the cost function: if the change (by default) is smaller
    than 10^(-6), the algorithm will assume that it has converged'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rawPredictionCol`: This returns the raw value from the generating function
    (before the threshold is applied); you can specify a different name than `rawPrediction`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fitIntercept`: This instructs the model to fit the intercept (constant) as
    well, not only the model coefficients; this is set to `True` by default'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`standardization`: This is set to `True` by default, and it standardizes the
    features before fitting the model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`threshold`: This is set by default to `0.0`; it is a parameter that decides
    what is classified as `1` or `0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weightCol`: This is a column name if each observation was to be weighted differently'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aggregationDepth`: This is a tree-depth parameter used for aggregation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, we `.fit(...)` the dataset using the object; the object returns a
    `.LinearSVCModel(...)`. Once the model is estimated, we can extract the estimated
    model''s coefficients like so: `svc_model.coefficients`. Here''s what we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00140.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s see whether a linear regression model can be reasonably accurate
    in estimating forest elevation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code is quite similar to the one presented earlier. As a side
    note, this is true for almost all the ML module models, so testing various models
    is extremely simple.
  prefs: []
  type: TYPE_NORMAL
- en: The difference is in the `label` column—right now, we are using `Elevation` and
    casting it as a `float` (since this is a regression problem).
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the linear regression object, `lr_obj`, instantiates the `.LinearRegression(...)`
    object.
  prefs: []
  type: TYPE_NORMAL
- en: For the full list of parameters to `.LinearRegression(...)`, please refer to
    the documentation: [http://bit.ly/2J9OvEJ](http://bit.ly/2J9OvEJ).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the model is estimated, we can check its coefficients by calling `lr_model.coefficients`.
    Here''s what we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00141.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In addition, `.LinearRegressionModel(...)` calculates a summary that returns
    basic performance statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will produce the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00142.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Surprisingly, the linear regression does well in this application: 78% R-squared
    is not a bad result.'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `Pipeline` class helps to sequence, or streamline, the execution of separate
    blocks that lead to an estimated model; it chains multiple Transformers and Estimators
    to form a sequential execution workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Pipelines are useful as they avoid explicitly creating multiple transformed
    datasets as the data gets pushed through different parts of the overall data transformation
    and model estimation process. Instead, Pipelines abstract distinct intermediate
    stages by automating the data flow through the workflow. This makes the code more
    readable and maintainable as it creates a higher abstraction of the system, and
    it helps with code debugging.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will streamline the execution of a generalized linear regression
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To execute this recipe, you will need a working Spark environment and you would
    have already loaded the data into the `forest` DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: No other prerequisites are required.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following code provides a streamlined version of the execution of the linear
    regression model estimation via GLM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The whole code is much shorter than the one we used in the previous example,
    as we do not need to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'However, as before, we specify `vectorAssembler` and `lr_obj` (the `.GeneralizedLinearRegression(...)`
    object). `.GeneralizedLinearRegression(...)` allows us to specify not only the
    model''s family, but also the link function. In order to decide what link function
    and family to choose, we can look at the distribution of our `Elevation` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s the plot that results from running the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00143.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The distribution is a bit skewed, but to a certain degree, we can assume that
    it follows a normal distribution. Thus, we can use `family = 'gaussian'` (default)
    and `link = 'identity'`.
  prefs: []
  type: TYPE_NORMAL
- en: Having created the Transformer (`vectorAssembler`) and the Estimator (`lr_obj`),
    we then put them into a Pipeline. The `stages` parameter is an ordered list of
    the objects to push our data through; in our case, `vectorAssembler` goes first
    as we need to collate all the features, and then we estimate our model using `lr_obj`.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we use the pipeline to estimate the model at the same time. The Pipeline's
    `.fit(...)` method calls either the `.transform(...)` method if the object is
    a Transformer, or the `.fit(...)` method if the object is an Estimator. Consequently,
    calling the `.transform(...)` method on `PipelineModel` calls the `.transform(...)`
    methods of both the Transformer and Estimator objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final result looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00144.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the results are not that much different from the actual ones.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Check out this blog post (even though it's Scala-specific) for an overview of
    Pipelines: [https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html](https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting the most predictable features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A mantra of (almost) every data scientist is: build a simple model while explaining
    as much variance in the target as possible. In other words, you can build a model
    with all your features, but the model may be highly complex and prone to overfitting.
    What''s more, if one of the variables is missing, the whole model might produce
    an erroneous output and some of the variables might simply be unnecessary, as
    other variables would already explain the same portion of the variance (a term
    called *collinearity*).'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to select the best predicting model when building
    either classification or regression models. We will be reusing what we learn in
    this recipe in the recipes that follow.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To execute this recipe, you will need a working Spark environment and you would
    have already loaded the data into the `forest` DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: No other prerequisites are required.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s begin with a code that will help to select the top 10 features with
    the most predictive power to find the best class for an observation in our `forest`
    DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we assemble all the features into a single vector using the `.VectorAssembler(...)`
    method. Note that we do not use the last column as it is the `CoverType` feature
    and this is our target.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we use the `.ChiSqSelector(...)` method to select the best features based
    on the pairwise chi-square test between each variable and the target. Based on
    the values from the test, `numTopFeatures`, the most predictable features, are
    selected. The `selected` vector will contain the top 10 (in this case) most predictable
    features. The `labelCol` specifies the target column.
  prefs: []
  type: TYPE_NORMAL
- en: You can learn more about the chi-square test here: [http://learntech.uwe.ac.uk/da/Default.aspx?pageid=1440](http://learntech.uwe.ac.uk/da/Default.aspx?pageid=1440).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s check it out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s what you should see from running the preceding snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00145.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the resulting `SparseVector` has a length of 10 and includes
    only the most predictable features.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We cannot use the `.ChiSqSelector(...)` method to select features against targets
    that are continuous, that is, the regression problems. One approach to select
    the best features would be to check correlations between each and every feature
    and the target and select those that are the most highly correlated with the target
    but exhibit little to no correlation with other features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: There is no automatic way to do this in Spark, but starting with Spark 2.2,
    we can now calculate correlations between features in DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: The `.Correlation(...)` method is part of the `pyspark.ml.stat` module, so we
    import it first.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we create `.VectorAssembler(...)`, which collates all the columns of the
    `forest` DataFrame. We can now use the Transformer and pass the resulting DataFrame
    to the `Correlation` class. The `.corr(...)` method of the `Correlation` class
    accepts a DataFrame as its first parameter, the name of the column with all the
    features as the second, and the type of correlation to calculate as the third;
    the available values are `pearson` (the default value) and `spearman`.
  prefs: []
  type: TYPE_NORMAL
- en: Check out this website for more information about the two correlation methods: [http://bit.ly/2xm49s7](http://bit.ly/2xm49s7).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s what we would expect to see from running the method:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00146.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that we have the correlation matrix, we can extract the top 10 most correlated
    features with our label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: First, we specify the number of features we want to extract and create a dictionary
    with all the columns from our `forest` DataFrame; note that we ZIP it with the
    index as the correlation matrix does not propagate the feature names, only the
    indices.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we extract the first column from the `corr_matrix` (as this is our target,
    the Elevation feature); the `.toArray()` method converts a DenseMatrix to a NumPy
    array representation. Note that we also append the index to the elements of this
    array so we know which element is most correlated with our target.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we sort the list in descending order by looking at the absolute values
    of the correlation coefficient.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we loop through the top 10 (in this case) elements of the resulting
    list and select the column from the `cols` dictionary that corresponds with the
    selected index.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our problem that aims at estimating the forest elevation, here''s the list
    of features we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00147.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are curious to learn more about feature selection, check out this paper: [http://www.stat.wisc.edu/~loh/treeprogs/guide/lchen.pdf](http://www.stat.wisc.edu/~loh/treeprogs/guide/lchen.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting forest coverage types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we will learn how to process data and build two classification
    models that aim to forecast the forest coverage type: the benchmark logistic regression
    model and the random forest classifier. The problem we have at hand is *multinomial*,
    that is, we have more than two classes that we want to classify our observations
    into.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To execute this recipe, you will need a working Spark environment and you would
    have already loaded the data into the `forest` DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: No other prerequisites are required.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here''s the code that will help us build the logistic regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we split the data we have into two subsets: the first one, `forest_train`,
    we will use for training the model, while `forest_test` will be used for testing
    the performance of the model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we build the usual stages we have already seen earlier in this chapter:
    we collate all the features we want to use to build our model using `.VectorAssembler(...)`
    and then pass them through the `.ChiSqSelector(...)` method to select the top
    10 most predictive features.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As the last step before building the Pipeline, we create `logReg_obj`: the
    `.LogisticRegression(...)` object we will use to fit our data with. We use the
    elastic-net type of regularization in this model: the L2 portion is defined in
    the `regParam` parameter, and the L1 portion in `elasticNetParam`. Note that we
    specify the family of the model to be `multinomial` as we are dealing with a multinomial
    classification problem.'
  prefs: []
  type: TYPE_NORMAL
- en: You can also specify the `family` parameter to be `auto` or `binomial`, if you
    want the model to self-select, or if you have a binary variable.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we build the Pipeline and pass the three objects as the list of stages.
    Next, we push our data through the pipeline using the `.fit(...)` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the model estimated, we can check how well it performs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: First, we load the `pyspark.ml.evaluation` module as it contains all the evaluation
    methods we will use throughout the rest of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we push `forest_test` through our `pModel` so that we can get the predictions
    for the dataset that the model has never seen before.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we create the `MulticlassClassificationEvaluator(...)` object, which
    will calculate the performance metrics of our model. `predictionCol` specifies
    the name of the column that contains the predicted class for an observation, and `labelCol`
    specifies the true label.
  prefs: []
  type: TYPE_NORMAL
- en: The `.evaluate(...)` method of the evaluator, if no other parameters are passed
    but the results of the model, will return the F1-score. If you want to retrieve
    either precision, recall, or accuracy, you need to call either `weightedPrecision`,
    `weightedRecall`, or `accuracy`, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: If you are not familiar with classification metrics, they are nicely explained
    here: [https://turi.com/learn/userguide/evaluation/classification.html](https://turi.com/learn/userguide/evaluation/classification.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s how our logistic regression model performs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00148.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The accuracy of almost 70% indicates it's not a totally terrible model.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see whether the random forest model can do any better:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from the preceding code, we will be reusing most of the objects
    we have already created for the logistic regression model; all we introduced here
    was `.RandomForestClassifier(...)` and we can reuse the `vectorAssembler` and `selector`
    objects. This is one examples of how simple it is to work with Pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `.RandomForestClassifier(...)` object will build the random forest model
    for us. In this example, we specified only four parameters, most of which you
    are most likely familiar with, such as `labelCol` and `featuresCol`.  `minInstancesPerNode`
    specifies the minimum number of records still allowed to split the node into two
    sub-nodes, while `numTrees` specifies how many trees in the forest to estimate.
    Other notable parameters include:'
  prefs: []
  type: TYPE_NORMAL
- en: '`impurity`: This specifies the criterion used for information gain. By default,
    it is set to `gini` but can also be `entropy`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxDepth`: This specifies the maximum depth of any of the trees.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxBins`: This specifies the maximum number of bins in any of the trees.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minInfoGain`: This specifies the minimum level of information gain between
    iterations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a full specification of the class, see [http://bit.ly/2sgQAFa](http://bit.ly/2sgQAFa).
  prefs: []
  type: TYPE_NORMAL
- en: 'Having estimated the model, let''s see how it performs so we can compare it
    to the logistic regression one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code should produce results similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00149.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The results are exactly the same, indicating that the two models perform equally
    well and we might want to increase the number of selected features in the selector
    stage to potentially achieve better results.
  prefs: []
  type: TYPE_NORMAL
- en: Estimating forest elevation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we will build two regression models that will predict forest
    elevation: the random forest regression model and the gradient-boosted trees regressor.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To execute this recipe, you will need a working Spark environment and you would
    have already loaded the data into the `forest` DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: No other prerequisites are required.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will only build a two stage Pipeline with the `.VectorAssembler(...)`
    and the `.RandomForestRegressor(...)` stages. We will skip the feature selection
    stage as it is not currently an automated process.
  prefs: []
  type: TYPE_NORMAL
- en: You can do this manually. Just check the *Selecting the most predictable features*
    recipe earlier from in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the full code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, as always, we collate all the features we want to use in our model using
    the `.VectorAssembler(...)` method. Note that we only use the columns starting
    from the second one as the first one is our target—the elevation feature.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we specify the `.RandomForestRegressor(...)` object. The object uses an
    almost-identical list of parameters as `.RandomForestClassifier(...)`.
  prefs: []
  type: TYPE_NORMAL
- en: See the previous recipe for a list of other notable parameters.
  prefs: []
  type: TYPE_NORMAL
- en: The last step is to build the Pipeline object; `pip` has only two stages: `vectorAssembler`
    and `rf_obj`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s see how our model is performing compared to the linear regression
    model we estimated in the *Introducing Estimators*recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '`.RegressionEvaluator(...)` calculates the performance metrics of regression
    models. By default, it returns `rmse`, the root mean-squared error, but it can
    also return:'
  prefs: []
  type: TYPE_NORMAL
- en: '`mse`: This is the mean-squared error'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`r2`: This is the *R²* metric'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mae`: This is the mean-absolute error'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'From the preceding code, we got:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00150.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This is better than the linear regression model we built earlier, meaning that
    our model might not be as linearly separable as we initially thought.
  prefs: []
  type: TYPE_NORMAL
- en: Check out this website for more information about the different types of regression
    metrics: [http://bit.ly/2sgpONr](http://bit.ly/2sgpONr).
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see whether the gradient-boosted trees model can beat the preceding
    result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The only change compared to the random forest regressor is the fact that we
    now use the `.GBTRegressor(...)` class to fit the gradient-boosted trees model
    to our data. The most notable parameters for this class include:'
  prefs: []
  type: TYPE_NORMAL
- en: '`maxDepth`: This specifies the maximum depth of the built trees, which by default
    is set to `5`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxBins`: This specifies the maximum number of bins'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minInfoGain`: This specifies the minimum level of information gain between
    iterations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minInstancesPerNode`: This specifies the minimum number of instances when
    the tree will still perform a split'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lossType`: This specifies the loss type, and accepts the `squared` or `absolute`
    values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`impurity`: This is, by default, set to `variance`, and for now (in Spark 2.3)
    is the only option allowed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxIter`: This specifies the maximum number of iterations—a stopping criterion
    for the algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s check the performance now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s what we got:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00151.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, we have still (even though ever-so-slightly) improved over the
    random forest regressor.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering forest cover types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering is an unsupervised family of methods that attempts to find patterns
    in data without any indication of what a class might be. In other words, the clustering
    methods find commonalities between records and groups them into clusters, depending
    on how similar they are to each other, and how dissimilar they are from those
    found in other clusters.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will build the most fundamental model of them all—the k-means.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To execute this recipe, you will need a working Spark environment and you would
    have already loaded the data into the `forest` DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: No other prerequisites are required.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The process of building a clustering model in Spark does not deviate significantly
    from what we have already seen in either the classification or regression examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We, as always, start with importing the relevant modules; in this case, it is
    the `pyspark.ml.clustering` module.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we collate all the features together that we will use in building the
    model using the well-known `.VectorAssembler(...)` Transformer.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is followed by instantiating the `.KMeans(...)` object. We only specified
    two parameters, but the list of the most notable ones is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`k`: This specifies the expected number of clusters and is the only required
    parameter to build the k-means model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initMode`: This specifies the initialization type of the cluster centroids; `k-means||`
    to use a parallel variant of k-means, or `random` to choose random centroid points'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initSteps`: This specifies the initialization steps'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxIter`: This specifies the maximum number of iterations after which the
    algorithm stops, even if it had not achieved a convergence'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we build the Pipeline with two stages only.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the results are calculated, we can look at what we got. Our aim was to
    see whether there are any underlying patterns found in the type of forest coverage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s what we got from running the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00152.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, there do not seem to be many patterns that would differentiate
    the forest cover types. However, let''s see whether our segmentation simply performs
    poorly and that this is why we are not finding any patterns, or whether we are
    finding patterns that are simply not really aligning with `CoverType`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '`.ClusteringEvaluator(...)` is a new evaluator available since Spark 2.3 and
    is still experimental. It calculates the Silhouette metrics for the clustering
    results.'
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about the silhouette metrics, check out [http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s what we got for our k-means model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00153.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, we got a decent model, as anything around 0.5 or more indicates
    well-separated clusters.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Check out [http://scikit-learn.org/stable/modules/clustering.html](http://scikit-learn.org/stable/modules/clustering.html) for
    a comprehensive overview of the clustering models. Note that many of them are
    not available in Spark.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tuning hyperparameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many models already mentioned in this chapter have multiple parameters that
    determine how the model will perform. Selecting some is relatively straightforward,
    but there are many that we simply cannot set intuitively. That's where hyperparameters-tuning
    comes to play. The hyperparameters-tuning methods help us select the best (or
    close to) set of parameters that maximizes some metric we defined.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will show you two approaches for hyperparameter-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To execute this recipe, you will need a working Spark environment and you would
    have already loaded the data into the `forest` DataFrame. You would also have
    gone through all the previous recipes as we assume you have a working knowledge
    of Transformers, Estimators, Pipelines, and some of the regression models.
  prefs: []
  type: TYPE_NORMAL
- en: No other prerequisites are required.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We start with grid search. It is a brute-force method that simply loops through
    specific values of parameters, building new models and comparing their performance
    given some objective evaluator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There's a lot happening here, so let's unpack it step-by-step.
  prefs: []
  type: TYPE_NORMAL
- en: We already know the `.VectorAssembler(...)`, `.ChiSqSelector(...)`, and `.LogisticRegression(...)`
    classes, so we will not be repeating ourselves here.
  prefs: []
  type: TYPE_NORMAL
- en: Check out previous recipes if you are not familiar with the preceding concepts.
  prefs: []
  type: TYPE_NORMAL
- en: The core of this recipe starts with the `logReg_grid` object. This is the `.ParamGridBuilder()`
    class, which allows us to add elements to the grid that the algorithm will loop
    through and estimate the models with all the combinations of all the parameters
    and the specified values.
  prefs: []
  type: TYPE_NORMAL
- en: 'A word of caution: the more parameters you include and the more levels you
    specify, the more models you will have to estimate. The number of models grows
    exponentially in both the number of parameters and in the number of levels you
    specify for these parameters. Beware!'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we loop through two parameters: `regParam` and `elasticNetParam`.
    For each of the parameters, we specify two levels, thus we will need to build
    four models.
  prefs: []
  type: TYPE_NORMAL
- en: As an evaluator, we once again use `.MulticlassClassificationEvaluator(...)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we specify the `.CrossValidator(...)` object, which binds all these things
    together: our `estimator` will be `logReg_obj`, `estimatorParamMaps` will be equal
    to the built `logReg_grid`, and `evaluator` is going to be `logReg_ev`.'
  prefs: []
  type: TYPE_NORMAL
- en: The  `.CrossValidator(...)` object splits the training data into a set of folds
    (by default, `3`) and these are used as separate training and test datasets to
    fit the models. Therefore, we not only need to fit four models based on the parameters
    grid we want to traverse, but also for each of those four models we build three
    models with different training and validation datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we first build the Pipeline that is purely data-transformative, that
    is, it only collates the features into the full features vector and then selects
    the top five features with the most predictive power; we do not fit `logReg_obj`
    at this stage.
  prefs: []
  type: TYPE_NORMAL
- en: The model-fitting starts when we use the `cross_v` object to fit the transformed
    data. Only then will Spark estimate four distinct models and select the one that
    performs best.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having now estimated the models and selected the best performing one, let''s
    see whether the selected model performs better than the one we estimated in the *Predicting
    forest coverage types* recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'With the help of the preceding code, we get the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00154.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, we do slightly worse than the previous model, but this is most
    likely due to the fact that we only selected the top 5 (versus 10 before) features
    with our selector.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another approach that aims at finding the best performing model is called **train-validation
    split**. This method performs a split of the training data into two smaller subsets:
    one that is use to train the model, and another one that is used to validate whether
    the model is not overfitting. The split is only performed once, thus in contrast
    to cross-validation, it is less expensive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code is not that dissimilar from what we saw with `.CrossValidator(...)`.
    The only additional parameter we specify for the `.TrainValidationSplit(...)`
    method is the level of parallelism that controls how many threads are spun up
    when you select the best model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `.TrainValidationSplit(...)` method produces the same results as
    the  `.CrossValidator(...)` approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00155.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Extracting features from text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Often, data scientists need to deal with unstructured data such as free-flow
    text: companies receive feedback or recommendations (among other things) from
    customers that can be a gold mine for predicting a customer''s next move or their
    sentiment toward a brand.'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to extract features from text.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To execute this recipe, you will need a working Spark environment.
  prefs: []
  type: TYPE_NORMAL
- en: No other prerequisites are required.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A general process that aims to extract data from text and convert it into something
    a machine learning model can use starts with the free-flow text. The first step
    is to take each sentence of the text and split it on the space character (most
    often). Next, all the stop words are removed. Finally, simply counting distinct
    words in the text or using a hashing trick takes us into the realm of numerical
    representations of free-flow text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s how to achieve this with Spark''s ML module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned earlier, we start with some text. In our example, we use some extracts
    from Spark's documentation.
  prefs: []
  type: TYPE_NORMAL
- en: '`.RegexTokenizer(...)` is the text tokenizer that uses regular expressions
    to split the sentence. In our example, we split the sentences on a minimum of
    one (or more) space—that''s the `\s+` expression. However, our pattern also splits
    on either a comma, period, or the quotation marks—that''s the `[,.\"]` part. The
    pipe, `|`, means split on either the spaces or the punctuation marks. The text,
    after passing through `.RegexTokenizer(...)`, will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00156.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Next, we use the `.StopWordsRemover(...)` method to remove the stop words, as
    the name suggests.
  prefs: []
  type: TYPE_NORMAL
- en: Check out NLTK's list of the most common stop words: [https://gist.github.com/sebleier/554280](https://gist.github.com/sebleier/554280).
  prefs: []
  type: TYPE_NORMAL
- en: '`.StopWordsRemover(...)` simply scans the tokenized text and discards any stop
    word it encounters. After removing the stop words, our text will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00157.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, what is left is an essential meaning of the sentence; a human
    can read these words and somewhat make sense of it.
  prefs: []
  type: TYPE_NORMAL
- en: 'A hashing trick (or feature hashing) is a method that transforms an arbitrary
    list of features into indices in a vector form. It is a space-efficient way of
    tokenizing text and, at the same time, turning text into a numerical representation.
    The hashing trick uses a hashing function to convert from one representation into
    another. A hashing function is essentially any mapping function that transforms
    one representation into another. Normally, it is a lossy and one-way mapping (or
    conversion); different input can be hashed into the same hash (a term called a **collision**)
    and, once hashed, it is almost always prohibitively difficult to reconstruct the
    input. The `.HashingTF(...)` method takes the input column from the `sq_remover`
    object and transforms (or encodes) the tokenized text into a vector of 20 features.
    Here''s what our text will look like after it has been hashed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00158.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Now that we have the features hashed, we could potentially use these features
    to train a machine learning model. However, simply counting the occurrences of
    words might lead to misleading conclusions. A better measure is the **term frequency-inverse
    document frequency** (**TF-IDF**). It is a metric that counts how many times a
    word occurs in the whole corpus and then calculates a proportion of the word's
    count in a sentence to its count in the whole corpus. This measure helps to evaluate
    how important a word is to a document in the whole collection of documents. In
    Spark, we use the `.IDF(...)` method, which does this for us.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s what our text would look like after passing the whole Pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00159.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: So, effectively, we have encoded the passage from Spark's documentation into
    a vector of 20 elements that we could now use to train a machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another way of encoding text into a numerical form is by using the Word2Vec
    algorithm. The algorithm computes a distributed representation of words with the
    advantage that similar words are placed close together in the vector space.
  prefs: []
  type: TYPE_NORMAL
- en: Check out this tutorial to learn more about Word2Vec and the skip-gram model: [http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s how we do it in Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We will get a vector of five elements from the `.Word2Vec(...)` method. Also,
    only words that occur at least twice in the corpus will be used to create the
    word-embedding. Here''s what the resulting vector will look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00160.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about text-feature engineering, check out this position from
    Packt: [http://bit.ly/2IZ7ZZA](http://bit.ly/2IZ7ZZA)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discretizing continuous variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, it is actually useful to have a discrete representation of a continuous
    variable.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to discretize a numerical feature with an
    example drawn from the Fourier series.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To execute this recipe, you will need a working Spark environment.
  prefs: []
  type: TYPE_NORMAL
- en: No other prerequisites are required.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we will use a small dataset that is located in the `data` folder,
    namely, `fourier_signal.csv`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we read the data into `signal_df`. The `fourier_signal.csv` contains
    a single column called `signal`.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we use the `.QuantileDiscretizer(...)` method to discretize the signal
    into 10 buckets. The bin ranges are chosen based on quantiles, that is, each bin
    will have the same number of observations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s what the original signal looks like (the black line), and what its
    discretized representation looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00161.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Standardizing continuous variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building a machine learning model using features that have significantly different
    ranges and resolutions (such as age and salary) might pose not only computational
    problems, but also model-convergence and coefficient-interpretability problems.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to standardize continuous variables so they
    have a mean of 0 and a standard deviation of 1.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To execute this recipe, you will need a working Spark environment. You will
    also have to have executed the previous recipe.
  prefs: []
  type: TYPE_NORMAL
- en: No other prerequisites are required.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To standardize the `signal` column we introduced in the previous recipe, we
    will use the `.StandardScaler(...)` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we need to transform the single feature into a vector representation,
    as the `.StandardScaler(...)` method accepts only vectorized features.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we instantiate the `.StandardScaler(...)` object. The `withMean` parameter
    instructs the method to center the data with the mean, while the `withStd` parameter
    scales to a standard deviation equal to 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s what the standardized representation of our signal look like. Note
    the different scales for the two lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00162.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Topic mining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, it is necessary to cluster text documents into buckets based on their
    content.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will walk through an example of assigning a topic to a set
    of short paragraphs extracted from Wikipedia.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To execute this recipe, you will need a working Spark environment.
  prefs: []
  type: TYPE_NORMAL
- en: No other prerequisites are required.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to cluster the documents, we first need to extract the features from
    our articles. Note that the following text is abbreviated for space considerations—refer
    to the GitHub repository for the full code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we create a DataFrame with our articles.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we go through pretty much the same steps as we went through in the *Extracting
    features from text*recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: We split the sentences using `.RegexTokenizer(...)`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We remove the stop words using `.StopWordsRemover(...)`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We count each word's occurrence using `.CountVectorizer(...)`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To find the clusters in our data, we will use the **Latent Dirichlet Allocation**
    (**LDA**) model. In our case, we know that we expect to have three clusters, but
    if you do not know how many clusters you might have, you can use one of the techniques
    we introduced in the *Tuning hyperparameters* recipe earlier in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we put everything in the Pipeline for our convenience.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the model is estimated, let''s see how it performs. Here''s a piece of
    code that will help us do that; note the NumPy''s `.argmax(...)` method that helps
    us find the index of the highest value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s what we get back:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00163.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, with proper processing, we can properly extract topics from
    the articles; the articles about galaxies are grouped in cluster 2, geographies
    are in cluster 1, and animals are in 0 cluster.
  prefs: []
  type: TYPE_NORMAL
