- en: Deploying Our REST services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are going to see how to deploy our Go applications using
    a few tools such as Nohup and Nginx. To make a website visible to the internet,
    we need to have a **Virtual Private Server** (**VPS**) and deployment tools. We
    will first see how to run a Go executable and make it a background process using
    Nohup. Next, we will install Nginx and configure it to proxy the Go server.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What is an Nginx proxy server?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning Nginx server blocks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load balancing strategies in Nginx
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying our Go service using Nginx
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rate limiting and securing our Nginx proxy server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring our Go service using a tool called Supervisord
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting the code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code for this chapter is available at [https://github.com/narenaryan/gorestful/tree/master/chapter10](https://github.com/narenaryan/gorestful/tree/master/chapter10).
    Copy it to `GOPATH` and run according to the instructions given in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Installing and configuring Nginx
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Nginx is a high performant web server and load balancer, and is well suited
    to deploying high traffic websites. Even though this decision is opinionated,
    Python and Node developers usually use this.
  prefs: []
  type: TYPE_NORMAL
- en: Nginx can also act as an upstream proxy server that allows us to redirect the
    HTTP requests to multiple application servers running on the same server. The
    main contender of Nginx is Apache's httpd.Nginx is an excellent static file server
    that can be used by the web clients. Since we are dealing with APIs, we will look
    into aspects of dealing with HTTP requests.
  prefs: []
  type: TYPE_NORMAL
- en: 'On Ubuntu 16.04, use these commands to install Nginx:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'On macOS X, you can install it with `brew`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[https://brew.sh/](https://brew.sh/) is a very useful software packaging system
    for macOS X users. My recommendation is to use it for installing software. Once
    it is successfully installed, you can check it by opening the machine IP in the
    browser. Open `http://localhost/` on your web browser. You will see this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0b1237f3-8514-42f1-8b8a-deed58642a4e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This means that Nginx is installed successfully. It is serving on port `80`
    and serving the default page. On macOS, the default Nginx listening port will
    be `8000`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'On Ubuntu (Linux), the file will be on this path:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Open the file, and search for a server and modify port `80` to `8000`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now everything is ready. The server runs on the `80` HTTP port, which means
    a client can access it using a URL (`http://localhost/`) and no port (`http://localhost:3000`).
    This basic server serves static files from a directory called `html`. The `root` parameter can
    be modified to any directory where we place our web assets. You can check the
    status of Nginx with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Nginx for the Windows operating system is quite basic and not really intended
    for production-grade deployments. Open-source developers usually prefer Debian
    or Ubuntu servers for deploying the API servers with Nginx.
  prefs: []
  type: TYPE_NORMAL
- en: What is a proxy server?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A proxy server is a server that holds the information of original servers in
    it. It acts as the front block for the client request. Whenever a client makes
    an HTTP request, it can directly go the application server. But, if the application
    server is written in a programming language, you need a translator that can turn
    the application response into a client-understandable response. **Common Gateway
    Interface** (**CGI**) does the same thing. For Go, we can run a simple HTTP server
    and it can work as a normal server (no translation required). So, why are we using
    another server called Nginx? We are using Nginx because it brings a lot of things
    into the picture.
  prefs: []
  type: TYPE_NORMAL
- en: 'The benefits of having a proxy server (Nginx):'
  prefs: []
  type: TYPE_NORMAL
- en: It can act as a load balancer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can sit in front of cluster of applications and redirect HTTP requests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can serve a filesystem with a good performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It streams media very well
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If the same machine is running multiple applications, then we can bring all
    those under one umbrella. Nginx can also act as the API gateway that can be the
    starting point for multiple API endpoints. We will see about a specially dedicated
    API gateway in the next chapter, but Nginx can also work as one. Refer to the
    following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3a9a868e-2d81-4eef-9fc9-35f663a39bb8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If you see, the illustration client is talking directly to Nginx instead of
    the ports where other applications are running. In the diagram, Go is running
    on port `8000` and other applications are running on different ports. It means
    the different servers are providing different API endpoints. If the client wishes
    to call those APIs, it needs to access three ports. Instead, if we have Nginx,
    it can act as a proxy server for all three and simplifies the client request-response
    cycle.
  prefs: []
  type: TYPE_NORMAL
- en: Nginx is also called an upstream server because it serves the requests from
    the other server. From the illustration, a Python app can request an API endpoint
    from a Go app smoothly.
  prefs: []
  type: TYPE_NORMAL
- en: Important Nginx paths
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are a few important Nginx paths we need to know about to work with the
    proxy server. In Nginx, we can host multiple sites (`www.example1.com`, `www.exampl2.com`,
    and so on) at the same time. Take a look at the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Type** | **Path** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| Configuration | `/etc/nginx/nginx.con` | This is the base Nginx configuration
    file. It can be used as the default file. |'
  prefs: []
  type: TYPE_TB
- en: '| Configuration | `/etc/nginx/sites-available/` | If we have multiple sites
    running within Nginx, we can have multiple configuration files. |'
  prefs: []
  type: TYPE_TB
- en: '| Configuration | `/etc/nginx/sites-enabled/` | These are the sites activated
    currently on Nginx. |'
  prefs: []
  type: TYPE_TB
- en: '| Log | `/var/log/nginx/access.log` | This log file records the server activity,
    such as timestamp and API endpoints.  |'
  prefs: []
  type: TYPE_TB
- en: '| Log | `/var/log/nginx/error.log` | This log file logs all proxy server-related
    errors, such as disk space, file system permissions, and so on. |'
  prefs: []
  type: TYPE_TB
- en: These paths are in the Linux operating system. For macOS X, use `/usr/local/nginx`
    as the base path.
  prefs: []
  type: TYPE_NORMAL
- en: Using server blocks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Server blocks are the actual configuration pieces that tell the server what
    to serve and on which port to listen. We can define multiple server blocks in
    the `sites-available` folder. On Ubuntu, the location will be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'On macOS X, the location will be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Until we copy the `sites-available` to the `sites-enabled` directory, the configuration
    has no effect. So, always create a soft link for `sites-available` to `sites-enabled`
    for every new configuration you create.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a sample Go application and proxying it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let us create a bare application server in Go with logging:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This file is a basic Go server to illustrate the proxy server''s functioning.
    Then, we add a configuration to Nginx to proxy port `8000` (Go running port) to
    HTTP port (`80`). Now, let us write the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a simple server that returns book details as an API (dummy data here).
    Run the program and it runs on port `8000`. Now, open a shell and make a CURL
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'It returns the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'But the client needs to request to `8000` port here. How can we proxy this
    server using Nginx? As we previously discussed, we need to edit the default sites-available
    server block, called `default`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Edit this file, find the server block, and add one line to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This section of the `config` file is called the server block. This controls
    the setting up of the proxy server where `listen` says where `nginx` should listen.
    `root`and `index` point to the static files if we need to serve any. `server_name`is
    the domain name of yours. Since we don''t have a domain ready, it is just localhost.
    The `location`is the key section here. In `location`, we can define our `proxy_pass`,
    which can proxy a given `URL:PORT`. Since our Go application is running on `8000`
    port, we mentioned it there. If we are running it on a different machine, such
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We can give the same thing as a parameter to `proxy_pass`. In order to take
    this configuration into effect, we need to restart the Nginx server. Do that using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, make CURL request to `http://localhost` and you will see the Go application''s
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '`location` is a directive that defines a **Unified Resource Identifier** (**URI**)
    that can proxy a given `server:port` combination. It means that by defining various
    URIs, we can proxy multiple applications running on the same server. It looks
    like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, three applications are running on different ports. These, after being
    added to our configuration file, can be accessed by the client as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Load balancing with Nginx
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In practical cases, we use multiple servers instead of one for handling huge
    sets of incoming requests for APIs. But who needs to forward an incoming client request
    to a server instance? Load balancing is a process where the central server distributes
    the load to various servers based on certain criteria. Refer to the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9477923c-dd86-43cd-a6bc-c009d8bbe88d.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Those requesting criteria are called load balancing methods. Let us see what
    each does in a simple table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Load balancing method** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| Round Robin | Requests are distributed evenly across servers and server weights
    are taken into consideration. |'
  prefs: []
  type: TYPE_TB
- en: '| Least Connection | Requests are sent to the server that is currently serving
    the least number of clients. |'
  prefs: []
  type: TYPE_TB
- en: '| IP Hash | This is used to send the requests from a given client''s IP to
    the given server. Only when that server is not available is it given to another
    server.  |'
  prefs: []
  type: TYPE_TB
- en: '| Least Time | A request from the client is sent to the machine with the lowest
    average latency (time to serve client) and least number of active connections.
    |'
  prefs: []
  type: TYPE_TB
- en: 'We now see how load balancing is practically achieved in Nginx for our Go API
    servers. The first step in this process is to create an `upstream` in the `http`
    section of the Nginx configuration file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, servers are the IP addresses or domain names of the servers running the
    same code. We are defining an `upstream` called `backend` here. It is a server
    group that we can refer to in our location directive. Weights should be given
    in proportion to the resources available. In the preceding code, `site1` is given
    a higher weight because it may be a bigger instance (memory and disk). Now, in
    the location directive, we can specify the server group with the `proxy_pass`
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the proxy server that is running will pass requests to the machines in
    the cluster for all API endpoints hitting `/`. The default request routing algorithm
    will be Round Robin, which means all the servers'' turns will be repeated one
    after the other. If we need to change it, we mention that in the upstream definition.
    Take a look at the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The preceding configuration says to *create a cluster of three machines, and
    add load balancing method as least connections*. `least_conn` is the string we
    used to mention the load balancing method. The other values could be `ip_hash`
    or `least_time`. You can try this by having a set of machines in the **Local Area
    Network** (**LAN**). Or else, we can have Docker installed with multiple virtual
    containers as different machines to test out load balancing.
  prefs: []
  type: TYPE_NORMAL
- en: We need to add that `http` block in the `/etc/nginx/nginx.conf` file, whereas
    the server block is in `/etc/nginx/sites-enabled/default`.It is better to separate
    these two settings.
  prefs: []
  type: TYPE_NORMAL
- en: Rate limiting our REST API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can also limit the rate of access to our Nginx proxy server by rate limiting.
    It provides a directive called `limit_conn_zone` ([http://nginx.org/en/docs/http/ngx_http_limit_conn_module.html#limit_conn_zone](http://nginx.org/en/docs/http/ngx_http_limit_conn_module.html#limit_conn_zone)).
    The format of it is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '`client_type` can be of two types:'
  prefs: []
  type: TYPE_NORMAL
- en: IP address (limit requests from a given IP address)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Server name (limit requests from a server)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`zone_type`also changes in correspondence to the `client_type`. It takes values
    as per the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Client type** | **Zone type** |'
  prefs: []
  type: TYPE_TB
- en: '| `$binary_remote_address` | `addr` |'
  prefs: []
  type: TYPE_TB
- en: '| `$server_name` | `servers` |'
  prefs: []
  type: TYPE_TB
- en: 'Nginx needs to save a few things to memory to remember the IP addresses and
    servers for rate limiting. `size` is the storage that we allocate for Nginx to
    perform its memorizing. It takes values such as 8m (8MB) or 16m (16MB). Now, let
    us see where to add these settings. The preceding one should be added as a global
    setting to the `http` directive in the `nginx.conf` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This allocates the shared memory for Nginx to use. Now, in the server directive
    of sites-available/default, add the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The total number of connections for the given server will not exceed 1K in
    the preceding configuration using `limit_conn`. If we try to put the rate limit
    from a given IP address to the client, then use this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This setting stops a client (IP address) from opening more than one connection
    to the server (for example, railway booking online). If we have a file that the
    client downloads and need to set a bandwidth constraint, use `limit_rate`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: In this way, we can control the client's interaction with our services that
    are proxied under Nginx. If we use Go binary directly to run the service, we lose
    all these features.
  prefs: []
  type: TYPE_NORMAL
- en: Securing our Nginx proxy server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is the most important piece in the Nginx set up. In this section, we will
    see how to restrict access to our server using basic authentication. This will
    be very important for our REST API servers because, let us suppose we have servers
    X, Y, and Z that talk to each other. X can serve clients directly, but X talks
    to Y and Z for some information by calling an internal API. Since we know that
    clients should not access Y or Z, we can make it so that only X is allowed to
    access the resources. We can allow or deny the IP addresses using the `nginx`
    access module. It looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This configuration tells Nginx to allow requests from clients ranging `192.168.1.1/24`,
    excluding `192.168.1.2`. The next line says to allow requests from the same host
    and block all other requests from any other client. The complete server block
    looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: For more information regarding this, see the documentation at [nginx_http_access_module](http://nginx.org/en/docs/http/ngx_http_access_module.html?_ga=2.117850185.1364707364.1504109372-1654310658.1503918562).
    We can also add password-secured access to our Nginx served static files. It is
    mostly not applicable to the API because there, the application takes care of
    authenticating the user.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring our Go API server with Supervisord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is fine that Nginx is sitting in front of our Go API server, it just proxies
    a port. However, sometimes that web application may stop due to the operating
    system restarting or crashing. Whenever your web server gets killed, it is someone's
    job to automatically bring it back to life. Supervisord is such a task runner.
    To make our API server run all the time, we need to monitor it. Supervisord is
    a tool that can monitor running processes (system) and can restart them when they
    were terminated.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Supervisord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can easily install Supervisord using Python’s `pip` command. On Ubuntu 16.04,
    just use the `apt-get` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This installs two tools, `supervisor` and `supervisorctl`. `Supervisorctl`
    is intended to control the supervisor and add tasks, restart tasks, and so on.
    Let us use the sample `basicServre.go` program we created for illustrating Nginx
    for this too. Install the binary to the `$GOPATH/bin` directory. Here, suppose
    my `GOPATH` is `/root/workspace`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Always add the `bin` folder of your current `GOPATH` to the system path. Whenever
    you install the project binary, it is available as a normal executable from the
    overall system environment. You can do it adding this line to the `~/.profile`
    file: `export PATH=$PATH:/usr/local/go/bin`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, create a configuration file at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'You can add any number of configuration files and `supervisord` treats them
    as separate processes to run. Add the following content to the preceding file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, we have a file called `supervisord.conf` at `/etc/supervisor/`.
    Look at it for further reference:'
  prefs: []
  type: TYPE_NORMAL
- en: The `[supervisord]` section gives the location of the log file for `supervisord`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`[program:myserver]` is the task block that traverses to a given directory
    and executes the command given.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, we can ask our `supervisorctl` to re-read the configuration and restart
    the task (process). For that, just say:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, launch our `supervisorctl` with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bca1c706-5370-4956-8339-561e3be10032.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, our book service is getting monitored by `Supervisor`. Let us try to kill
    the process and see what `Supervisor` does:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, as soon as possible, `Supervisor` starts a new process (different `pid`)
    by running the binary:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d881926-4c63-499a-9472-245d8cf528db.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is very useful in production scenarios where a service needs to be up
    in case of any crash or OS restart. One question here, how do we start/stop an
    application service? Use the `start` and `stop` commands from `supervisorctl`
    for smooth operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: For more details about the Supervisor, visit [http://supervisord.org/](http://supervisord.org/).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter is dedicated to showing how we can deploy our API services into
    production. One way is to run the Go binary and access it through the `IP: Port`
    combination directly from the client. That IP will be the **Virtual Private Server**
    (**VPS**) IP address. Instead, we can have a domain name registered and pointed
    to the VPS. The second and better way is to hide it behind a proxy server. Nginx
    is such a proxy server, using which we can have multiple application servers under
    one umbrella.'
  prefs: []
  type: TYPE_NORMAL
- en: We saw how to install Nginx and start configuring it. Nginx provides features
    such as load balancing and rate limiting, which could be crucial while giving
    APIs to clients. Load balancing is the process of distributing loads among similar
    servers. We saw what types of loading mechanisms are available. Some of them are
    Round Robin, IP Hash, Least Connection, and so on. Then, we added authentication
    to our servers by allowing and denying a few sets of IP addresses.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we need a process monitor that can bring our crashed application back
    to life. Supervisord is a very good tool for the job. We saw how to install Supervisord
    and also launch supervisorctl, a command-line application to control running servers.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to see how to make our API production-grade
    using an API gateway. We will discuss deeply how we can put our API behind an
    entity that takes care of authentication and rate limiting.
  prefs: []
  type: TYPE_NORMAL
