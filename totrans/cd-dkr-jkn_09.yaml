- en: Advanced Continuous Delivery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last chapter, we covered how server clustering works and how we can use
    it together with Docker and Jenkins. In this chapter, we will see a mixture of
    different aspects that are very important in the Continuous Delivery process but
    have not been described yet.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following points:'
  prefs: []
  type: TYPE_NORMAL
- en: Explaining how to approach database changes in the context of Continuous Delivery
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing the idea of database migration and related tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring different approaches to backwards-compatible and backwards-incompatible
    database updates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using parallel steps in the Jenkins pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a Jenkins shared library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Presenting a way to roll back production changes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing Continuous Delivery for legacy systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring how to prepare zero-downtime deployments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Presenting Continuous Delivery best practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing database changes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have focused on the Continuous Delivery process, which was applied
    to a web service. A simple part of this was that web services are inherently stateless.
    This fact means that they can be easily updated, restarted, cloned in many instances,
    and recreated from the given source code. A web service, however, is usually linked
    to its stateful part, a database that poses new challenges to the delivery process.
    These challenges can be grouped into the following categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Compatibility**: The database schema and the data itself must be compatible
    with the web service all the time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Zero-downtime deployment**: In order to achieve zero-downtime deployment,
    we use rolling updates, which means that a database must be compatible with two
    different web service versions at the same time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rollback**: A rollback of a database can be difficult, limited, or sometimes
    even impossible because not all operations are reversible (for example, removing
    a column that contains data)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test data**: Database-related changes are difficult to test because we need
    test data that is very similar to production'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, I will explain how to address these challenges so that the
    Continuous Delivery process will be as safe as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding schema updates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you think about the delivery process, it's not really the data itself that
    cause difficulties because we don't usually change the data when we deploy an
    application. The data is something that is collected while the system is live
    in the production; whereas, during deployment, we only change the way we store
    and interpret this data. In other words, in the context of the Continuous Delivery
    process, we are interested in the structure of the database, not exactly in its
    content. This is why this section concerns mainly relational databases (and their
    schemas) and focuses less on other types of storage such as NoSQL databases, where
    there is no structure definition.
  prefs: []
  type: TYPE_NORMAL
- en: To understand this better, we think of Redis, which we have already used in
    this book. It stored the cached data, so effectively it was a database. Nevertheless,
    it required zero effort from the Continuous Delivery perspective since it didn't
    have any data structure. All it stored was the key-value entries, which does not
    evolve over time.
  prefs: []
  type: TYPE_NORMAL
- en: NoSQL databases usually don't have any restricting schema and therefore simplify
    the Continuous Delivery process because there is no additional schema update step
    required. This is a huge benefit; however, it doesn't necessarily mean that writing
    applications with NoSQL databases is simpler because we have put more effort into
    data validation in the source code.
  prefs: []
  type: TYPE_NORMAL
- en: Relational databases have static schemas. If we would like to change it, for
    example, to add a new column to the table, we need to write and execute a SQL
    DDL (data definition language) script. Doing this manually for every change requires
    a lot of work and leads to error-prone solutions, in which the operations team
    has to keep in sync the code and the database structure. A much better solution
    is to automatically update the schema in an incremental manner. Such a solution
    is called database migration.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing database migrations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Database schema migration is a process of incremental changes to the relational
    database structure. Let''s have a look at the following diagram to understand
    it better:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/bf27905b-b5aa-4f40-a4b0-37eaf35957ca.png)'
  prefs: []
  type: TYPE_IMG
- en: The database in the version **v1** has the schema defined by the `V1_init.sql` file.
    Additionally, it stores the metadata related to the migration process, for example,
    its current schema version and the migration changelog. When we want to update
    the schema, we provide the changes in the form of a SQL file, such as `V2_add_table.sql`.
    Then, we need to run the migration tool that executes the given SQL file on the
    database (it also updates the metatables). In effect, the database schema is a
    result of all subsequently executed SQL migration scripts. Next, we will see an
    example of a migration.
  prefs: []
  type: TYPE_NORMAL
- en: Migration scripts should be stored in the version control system, usually in
    the same repository as the source code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Migration tools and the strategies they use can be divided into two categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Upgrade and downgrade**: This approach, for example, used by the Ruby on
    Rails framework, means that we can migrate up (from v1 to v2) and down (from v2
    to v1). It allows the database schema to roll back, which may sometimes end up
    in data loss (if the migration is logically irreversible).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Upgrade only**: This approach, for example, used by the Flyway tool, only
    allows us to migrate up (from v1 to v2). In many cases, the database updates are
    not reversible, for example, removing a table from the database. Such a change
    cannot be rolled back because even if we recreate the table, we have already lost
    all the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many database migration tools available on the market, out of which
    the most popular are Flyway, Liquibase, and Rail Migrations (from the Ruby on
    Rails framework). As a next step to understand how such tools work, we will see
    an example based on the Flyway tool.
  prefs: []
  type: TYPE_NORMAL
- en: There are also commercial solutions provided for the particular databases, for
    example, Redgate (for SQL Server) and Optim Database Administrator (for DB2).
  prefs: []
  type: TYPE_NORMAL
- en: Using Flyway
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s use Flyway to create a database schema for the calculator web service.
    The database will store the history of all operations that were executed on the
    service: the first parameter, the second parameter, and the result.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We show how to use the SQL database and Flyway in three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the Flyway tool to work together with Gradle.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Defining the SQL migration script to create the calculation history table.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the SQL database inside the Spring Boot application code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configuring Flyway
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to use Flyway together with Gradle, we need to add the following content
    to the `build.gradle` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s a quick comment on the configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: We used the H2 database, which is an in-memory (and file-based) database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We store the database in the `/tmp/calculator` file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The default database user is called `sa` (system administrator)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the case of other SQL databases (for example, MySQL), the configuration would
    be very similar. The only difference is in the Gradle dependencies and the JDBC
    connection.
  prefs: []
  type: TYPE_NORMAL
- en: 'After this configuration is applied, we should be able to run the Flyway tool
    by executing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The command created the database in the file `/tmp/calculator.mv.db`. Obviously,
    it has no schema since we haven't defined anything yet.
  prefs: []
  type: TYPE_NORMAL
- en: Flyway can be used as a command-line tool, via Java API, or as a plugin for
    the popular building tools Gradle, Maven, and Ant.
  prefs: []
  type: TYPE_NORMAL
- en: Defining  the SQL migration script
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The next step is to define the SQL file that adds the calculation table into
    the database schema. Let''s create the `src/main/resources/db/migration/V1__Create_calculation_table.sql` file
    with the following content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Note the migration file naming convention, `<version>__<change_description>.sql`.
    The SQL file creates a table with four columns, `ID`, `A`, `B`, `RESULT`. The
    `ID` column is an automatically incremented primary key of the table. Now, we
    are ready to run the Flyway command to apply the migration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The command automatically detected the migration file and executed it on the
    database.
  prefs: []
  type: TYPE_NORMAL
- en: The migration files should be always kept in the version control system, usually,
    together with the source code.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing database
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We executed our first migration, so the database is prepared. To see the complete
    example, we should also adapt our project so that it would access the database.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first configure the Gradle dependencies to use the H2 database from
    the Spring Boot project. We can do this by adding the following lines to the `build.gradle` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to set up the database location and the startup behavior in
    the `src/main/resources/application.properties` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The second line means that Spring Boot will not try to automatically generate
    the database schema from the source code model. On the contrary, it will only
    validate if the database schema is consistent with the Java model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s create the Java ORM entity model for the calculation in the new
    `src/main/java/com/leszko/calculator/Calculation.java` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The entity class represents the database mapping in the Java code. A table is
    expressed as a class and each column as a field. The next step is to create the
    repository for loading and storing the `Calculation` entities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create the `src/main/java/com/leszko/calculator/CalculationRepository.java` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can use the `Calculation` and `CalculationRepository` classes to
    store the calculation history. Let''s add the following code to the `src/main/java/com/leszko/calculator/CalculatorController.java `file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Now, when we start the service and execute the `/sum` endpoint, each summing
    operation is logged into the database.
  prefs: []
  type: TYPE_NORMAL
- en: If you would like to browse the database content, then you can add `spring.h2.console.enabled=true` to
    the `application.properties` file, and then browse the database via the `/h2-console` endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: We explained how the database schema migration works and how to use it inside
    a Spring project, built with Gradle. Now, let's have a look at how it integrates
    within the Continuous Delivery process.
  prefs: []
  type: TYPE_NORMAL
- en: Changing database in Continuous Delivery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first approach to use database updates inside the Continuous Delivery pipeline
    could be to add a stage within the migration command execution. This simple solution
    would work correctly for many cases; however, it has two significant drawbacks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Rollback**: As mentioned before, it''s not always possible to roll back the
    database change (Flyway doesn''t support downgrades at all). Therefore, in the
    case of service rollback, the database becomes incompatible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Downtime**: The service update and the database update are not executed exactly
    at the same time, which causes downtime.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This leads us to two constraints that we will need to address:'
  prefs: []
  type: TYPE_NORMAL
- en: The database version needs to be compatible with the service version all the
    time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The database schema migration is not reversible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will address these constraints for two different cases: backwards-compatible
    updates and non-backwards-compatible updates.'
  prefs: []
  type: TYPE_NORMAL
- en: Backwards-compatible changes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Backwards-compatible changes are simpler. Let''s look at the following figure
    to see how they work:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/32844c8c-5496-4c95-b02a-11a8e511ebd5.png)'
  prefs: []
  type: TYPE_IMG
- en: Suppose the schema migration `Database v10` is backwards-compatible. If we need
    to roll back the `Service v1.2.8` release, then we deploy `Service v1.2.7`, and
    there is no need to do anything with the database (database migrations are not
    reversible, so we keep `Database v11`). Since the schema update is backwards-compatible,
    `Service v.1.2.7` works perfectly fine with `Database v11`. The same applies if
    we need to roll back to `Service v1.2.6`, and so on. Now, suppose `Database v10` and
    all other migrations are backwards-compatible, then we could roll back to any
    service version and everything would work correctly.
  prefs: []
  type: TYPE_NORMAL
- en: There is also no problem with the downtime. If the database migration is zero-downtime
    itself, then we can execute it first and then use the rolling updates for the
    service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at an example of a backwards-compatible change. We will create
    a schema update that adds a `created_at` column to the calculation table. The
    migration file `src/main/resources/db/migration/V2__Add_created_at_column.sql` looks
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Besides the migration script, the calculator service requires a new field in
    the `Calculation` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need to adjust its constructor and then its usage in the `CalculatorController`
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: After running the service, the calculation history is stored together with the
    `created_at` column. Note that the change is backwards-compatible because even
    if we revert the Java code and leave the `created_at` column in the database,
    everything would work perfectly fine (the reverted code does not address the new
    column at all).
  prefs: []
  type: TYPE_NORMAL
- en: Non-backwards-compatible changes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Non-backwards-compatible changes are way more difficult. Looking at the previous
    figure, if database change v11 was backwards-incompatible, it would be impossible
    to roll back the service to 1.2.7\. In this case, how can we approach non-backwards-compatible
    database migrations so that rollbacks and zero-downtime deployments would be possible?
  prefs: []
  type: TYPE_NORMAL
- en: 'To make a long story short, we can address this issue by converting a non-backwards-compatible
    change into a change that is backwards-compatible for a certain period of time.
    In other words, we need to put in the extra effort and split the schema migration
    into two parts:'
  prefs: []
  type: TYPE_NORMAL
- en: Backwards-compatible update executed now, which usually means keeping some redundant
    data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non-backwards-compatible update executed after the rollback period time that
    defines how far back we can revert our code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To illustrate this better, let''s look at the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/91d810e9-805d-403b-a399-d45e6b3b2fe3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s think about an example of dropping a column. A proposed method would
    include two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Stop using the column in the source code (v1.2.5, backwards-compatible update,
    executed first).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Drop the column from the database (v11, non-backwards-compatible update, executed
    after the rollback period).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All service versions until `Database v11` can be rolled back to any previous
    version, the services starting from `Service v1.2.8` can be rolled back only within
    the rollback period. Such approach may sound trivial because all we did was delay
    the column removal from the database. However, it addresses both the rollback
    issue and the zero-downtime deployment issue. As a result, it reduces the risk
    associated with the release. If we adjust the rollback period to a reasonable
    amount of time, for example, in the case of multiple releases per day to two weeks,
    then the risk is negligible. We don't usually roll many versions back.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dropping a column was a very simple example. Let''s have a look at a more difficult
    scenario and rename the result column in our calculator service. We present how
    to do this in a few steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Adding a new column to the database.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Changing the code to use both columns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Merging the data in both columns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Removing the old column from the code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dropping the old column from the database.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adding a new column to the database
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s say we need to rename the `result` column to `sum`. The first step is
    to add a new column that will be a duplicate. We must create a `src/main/resources/db/migration/V3__Add_sum_column.sql` migration
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, after executing the migration, we have two columns: `result` and
    `sum`.'
  prefs: []
  type: TYPE_NORMAL
- en: Changing the code to use both columns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The next step is to rename the column in the source code model and to use both
    database columns for the set and get operations. We can change it in the `Calculation` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: To be 100% accurate, in the `getSum()` method, we should compare something like
    the last modification column date (not exactly necessary to always take the new
    column first).
  prefs: []
  type: TYPE_NORMAL
- en: From now on, every time we add a row into the database, the same value is written
    to both the `result` and `sum` columns. While reading `sum`, we first check if
    it exists in the new column, and if not, we read it from the old column.
  prefs: []
  type: TYPE_NORMAL
- en: The same result can be achieved with the use of database triggers that would
    automatically write the same values into both columns.
  prefs: []
  type: TYPE_NORMAL
- en: All the changes we made so far were backwards-compatible, so we can roll back
    the service anytime we want, to any version we want.
  prefs: []
  type: TYPE_NORMAL
- en: Merging the data in both columns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This step is usually done after some time when the release is stable. We need
    to copy the data from the old `result` column into the new `sum` column. Let''s
    create a migration file called `V4__Copy_result_into_sum_column.sql`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We still have no limits for the rollback; however, if we need to deploy the
    version before the change in step 2, then this database migration needs to be
    repeated.
  prefs: []
  type: TYPE_NORMAL
- en: Removing the old column from the code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At this point, we already have all data in the new column, so we can start
    using it without the old column in the data model. In order to do this, we need
    to remove all code related to `result` in the `Calculation` class so that it would
    look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: After this operation, we no longer use the `result` column in the code. Note
    that this operation is only backwards-compatible up to step 2\. If we need to
    roll back to step 1, then we could lose the data stored after this step.
  prefs: []
  type: TYPE_NORMAL
- en: Dropping the old column from the database
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last step is to drop the old column from the database. This migration should
    be performed after the rollback period when we are sure we won't need to roll
    back before step 4.
  prefs: []
  type: TYPE_NORMAL
- en: The rollback period can be very long since we aren't using the column from the
    database anymore. This task can be treated as a cleanup task, so even though it's
    non-backwards-compatible, there is no associated risk.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s add the final migration, `V5__Drop_result_column.sql`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: After this step, we finally completed the column renaming procedure. Note that
    all we did was complicate the operation a little bit, in order to stretch it in
    time. This reduced the risk of backwards-incompatible database changes and allowed
    zero-downtime deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Separating database updates from code changes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, in all figures, we presented that database migrations are run together
    with service releases. In other words, each commit (which implies each release)
    took both database changes and code changes. However, the recommended approach
    is to make a clear separation that a commit to the repository is either a database
    update or a code change. This method is presented in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/af3ebd96-a96f-4565-84b6-f75ecd859d95.png)'
  prefs: []
  type: TYPE_IMG
- en: The benefit of database-service change separation is that we get the backwards-compatibility
    check for free. Imagine that the changes v11 and v1.2.7 concern one logical change,
    for example, adding a new column to the database. Then, we first commit database
    v11, so the tests in the Continuous Delivery pipeline check if database v11 works
    correctly with service v.1.2.6\. In other words, they check if database update
    v11 is backwards-compatible. Then, we commit the v1.2.7 change, so the pipeline
    checks if database v11 works fine with service v1.2.7.
  prefs: []
  type: TYPE_NORMAL
- en: The database-code separation does not mean that we must have two separate Jenkins
    pipelines. The pipeline can always execute both, but we should keep it as a good
    practice that a commit is either a database update or a code change.
  prefs: []
  type: TYPE_NORMAL
- en: To sum up, the database schema changes should be never done manually. Instead,
    we should always automate them using a migration tool, executed as a part of the
    Continuous Delivery pipeline. We should also avoid non-backwards-compatible database
    updates and the best way to assure this is to commit separately the database and
    code changes into the repository.
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding shared database
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In many systems, we can spot that the database becomes the central point that
    is shared between multiple services. In such a case, any update to the database
    becomes much more challenging because we need to coordinate it between all services.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, imagine we develop an online shop and we have a Customers table
    that contains the following columns: first name, last name, username, password,
    email, and discount. There are three services that are interested in the customer''s
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Profile manager**: This enables editing user''s data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Checkout processor**: This processes the checkout (reads username and email)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Discount manager**: This analyzes the customer''s orders and sets the suitable
    discount'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s look at the following image that presents this situation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/794aa472-8c58-4b68-b172-8b85d838d1b8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'They are dependent on the same database schema. There are at least two issues
    with such an approach:'
  prefs: []
  type: TYPE_NORMAL
- en: When we want to update the schema, it must be compatible with all three services.
    While all backwards-compatible changes are fine, any non-backwards-compatible
    update becomes way more difficult or even impossible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each service has a separate delivery cycle and a separate Continuous Delivery
    pipeline. Then, which pipeline should we use for the database schema migrations?
    Unfortunately, there is no good answer to this question.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For the reasons mentioned previously, each service should have its own database
    and the services should communicate via their APIs. Following our example, we
    could apply the following refactoring:'
  prefs: []
  type: TYPE_NORMAL
- en: The checkout processor should communicate with the profile manager's API to
    fetch the customer's data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The discount column should be extracted to a separate database (or schema),
    and the discount manager should take the ownership
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The refactored version is presented in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/58088d1d-5613-45b4-9ef1-978ac0372a8f.png)'
  prefs: []
  type: TYPE_IMG
- en: Such an approach is consistent with the principles of the microservice architecture
    and should always be applied. The communication over APIs is way more flexible
    than the direct database access.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of monolithic systems, a database is usually the integration point.
    Since such an approach causes a lot of issues, it's considered as an anti-pattern.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing test data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already presented database migrations that keep the database schema
    consistent between the environments as a side effect. This is due to the fact
    that if we run the same migration scripts on the development machine, in the staging
    environment, or in the production, then we would always get the result in the
    same schema. However, the data values inside the tables differ. How can we prepare
    the test data so that it would effectively test our system? This is the topic
    of this section.
  prefs: []
  type: TYPE_NORMAL
- en: The answer to this question depends on the type of the test, and it is different
    for unit testing, integration/acceptance testing, and performance testing. Let's
    examine each case.
  prefs: []
  type: TYPE_NORMAL
- en: Unit testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the case of unit testing, we don't use the real database. We either mock
    the test data on the level of the persistence mechanism (repositories, data access
    objects) or we fake the real database with an in-memory database (for example,
    H2 database). Since unit tests are created by developers, the exact data values
    are usually invented by developers and they don't matter much.
  prefs: []
  type: TYPE_NORMAL
- en: Integration/acceptance testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Integration and acceptance tests usually use the test/staging database, which
    should be as similar as possible to the production. One approach, taken by many
    companies, is to snapshot the production data into staging that guarantees that
    it is exactly the same. This approach, however, is treated as an anti-pattern
    for the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Test isolation**: Each test operates on the same database, so the result
    of one test may influence the input of the others'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data security**: Production instances usually store sensitive information
    and are therefore better secured'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reproducibility**: After every snapshot, the test data is different, which
    may result in flaky tests'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the preceding reasons, the preferred approach is to manually prepare the
    test data by selecting a subset of the production data, together with the customer
    or the business analyst. When the production database grows, it's worth revisiting
    its content to see if there are any reasonable cases that should be added.
  prefs: []
  type: TYPE_NORMAL
- en: The best way to add data to the staging database is to use the public API of
    a service. This approach is consistent with acceptance tests, which are usually
    black-box. What's more, using the API guarantees that the data itself is consistent
    and simplifies database refactoring by limiting direct database operations.
  prefs: []
  type: TYPE_NORMAL
- en: Performance testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Test data for the performance testing is usually similar to acceptance testing.
    One significant difference is the amount of data. In order to test the performance
    correctly, we need to provide sufficient volume of input data, at least as large
    as available on the production (during the peak time). For this purpose, we can
    create data generators, which are usually shared between acceptance and performance
    tests.
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We already know everything that is necessary to start a project and set up the
    Continuous Delivery pipeline with Jenkins and Docker. This section is intended
    to extend this knowledge with a few of the recommended general Jenkins pipeline
    practices.
  prefs: []
  type: TYPE_NORMAL
- en: Parallelizing pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this book, we have always executed the pipeline sequentially, stage
    by stage, step by step. This approach makes it easy to reason the state and the
    result of the build. If there is first the acceptance test stage and then the
    release stage, it means that the release won't ever happen until the acceptance
    tests are successful. Sequential pipelines are simple to understand and usually
    do not cause any surprises. This is why the first method to solve any problem
    is to do it sequentially.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, in some cases, the stages are time-consuming and it''s worth running
    them in parallel. A very good example is performance tests. They usually take
    a lot of time, so assuming they are independent and isolated, it makes sense to
    run them in parallel. In Jenkins, we can parallelize the pipeline on two different
    levels:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Parallel steps**: Within one stage, parallel processes run on the same agent.
    This method is simple because all Jenkins workspace-related files are located
    on one physical machine, however, as always with the vertical scaling, the resources
    are limited to that single machine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parallel stages**: Each stage can be run in parallel on a separate agent
    machine that provides horizontal scaling of resources. We need to take care of
    the file transfer between the environments (using the `stash` Jenkinsfile keyword)
    if a file created in the previous stage is needed on the other physical machine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the time of writing this book, parallel stages are not available in the declarative
    pipeline. The feature is supposed to be added in Jenkins Blue Ocean v1.3\. In
    the meantime, the only possibility is to use the deprecated feature in the Groovy-based
    scripting pipeline, as described here at [https://jenkins.io/doc/book/pipeline/jenkinsfile/#executing-in-parallel](https://jenkins.io/doc/book/pipeline/jenkinsfile/#executing-in-parallel).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at how it looks in practice. If we would like to run two steps
    in parallel, the Jenkinsfile script should look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In `Stage 1`, with the use of the `parallel` keyword, we execute two parallel
    steps, `one` and `two`. Note that `Stage 2` is executed only after both parallel
    steps are completed. This is why such solutions are perfectly safe to run tests
    in parallel; we can always be sure that the deployment stage is run only after
    all parallelized tests have already passed.
  prefs: []
  type: TYPE_NORMAL
- en: There is a very useful plugin called `Parallel Test Executor` that helps to
    automatically split tests and run them in parallel. Read more at [https://jenkins.io/doc/pipeline/steps/parallel-test-executor/](https://jenkins.io/doc/pipeline/steps/parallel-test-executor/).
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding description concerned the parallel steps level. The other solution
    would be to use parallel stages and therefore run each stage on a separate agent
    machine. The decision on which type of parallelism to use usually depends on two
    factors:'
  prefs: []
  type: TYPE_NORMAL
- en: How powerful the agent machines are
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How much time the given stage takes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a general recommendation, unit tests are fine to run in parallel steps, but
    performance tests are usually better off on separate machines.
  prefs: []
  type: TYPE_NORMAL
- en: Reusing pipeline components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When the Jenkinsfile script grows in size and becomes more complex, we may want
    to reuse its parts between similar pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we may want to have separate, but similar, pipelines for different
    environments (dev, QA, prod). Another common example in the microservice world
    is that each service has a very similar Jenkinsfile. Then, how do we write Jenkinsfile
    scripts so that we don't repeat the same code all over again? There are two good
    patterns for this purpose, parameterized build and shared libraries. Let's describe
    them one by one.
  prefs: []
  type: TYPE_NORMAL
- en: Build parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We already mentioned in [Chapter 4](a07d252d-3812-45ad-a567-1c70dae74d9d.xhtml),
    *Continuous Integration Pipeline*, that a pipeline can have input parameters.
    We can use them to provide different use cases with the same pipeline code. As
    an example, let''s create a pipeline parametrized with the environment type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The build takes one input parameter, `Environment`. Then, all we do in this
    step is print the parameter. We can also add a condition to execute different
    code for different environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this configuration, when we start the build, we will see a prompt for
    the input parameter, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/d84f40b8-8d26-4581-b262-a063d3a1d1f5.png)'
  prefs: []
  type: TYPE_IMG
- en: Parametrized build can help reuse the pipeline code for scenarios when it differs
    just a little bit. This feature, however, should not be overused because too many
    conditions can make the Jenkinsfile difficult to understand.
  prefs: []
  type: TYPE_NORMAL
- en: Shared libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The other solution to reuse the pipeline is to extract its parts into a shared
    library.
  prefs: []
  type: TYPE_NORMAL
- en: 'A shared library is a Groovy code that is stored as a separate source-controlled
    project. This code can be later used in many Jenkinsfile scripts as pipeline steps.
    To make it clear, let''s have a look at an example. A shared library technique
    always requires three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a shared library project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configure the shared library in Jenkins.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the shared library in Jenkins file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Creating a shared library project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We start by creating a new Git project, in which we put the shared library code.
    Each Jenkins step is expressed as a Groovy file located in the `vars` directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a `sayHello` step that takes the `name` parameter and echoes
    a simple message. This should be stored in the `vars/sayHello.groovy` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Human-readable descriptions for shared library steps can be stored in the `*.txt`
    files. In our example, we could add the `vars/sayHello.txt` file with the step
    documentation.
  prefs: []
  type: TYPE_NORMAL
- en: When the library code is done, we need to push it to the repository, for example,
    as a new GitHub project.
  prefs: []
  type: TYPE_NORMAL
- en: Configure the shared library in Jenkins
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The next step is to register the shared library in Jenkins. We open Manage
    Jenkins | Configure System, and find the Global Pipeline Libraries section. There,
    we can add the library giving it a name chosen, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/d0fac181-7915-4db7-b4c3-5776e527e325.png)'
  prefs: []
  type: TYPE_IMG
- en: We specified the name under which the library is registered and the library
    repository address. Note that the latest version of the library will be automatically
    downloaded during the pipeline build.
  prefs: []
  type: TYPE_NORMAL
- en: We presented importing the Groovy code as *Global Shared Library*, but there
    are also other alternative solutions. Read more at [https://jenkins.io/doc/book/pipeline/shared-libraries/](https://jenkins.io/doc/book/pipeline/shared-libraries/).
  prefs: []
  type: TYPE_NORMAL
- en: Use shared library in Jenkinsfile
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, we can use the shared library in the Jenkinsfile script.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s have a look at the example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: If "Load implicitly" hadn't been checked in the Jenkins configuration, then
    we would need to add "`@Library('example') _`" at the beginning of the Jenkinsfile
    script.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we can use the Groovy code as a pipeline step `sayHello`. Obviously,
    after the pipeline build completes, in the console output, we should see `Hello
    Rafal!`.
  prefs: []
  type: TYPE_NORMAL
- en: Shared libraries are not limited to one step. Actually, with the power of the
    Groovy language, they can even act as templates for entire Jenkins pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Rolling back deployments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I remember the words of my colleague, a senior architect, <q>You don't need
    more QAs, you need a faster rollback</q>. While this statement is oversimplified
    and the QA team is often of great value, there is a lot of truth in this sentence.
    Think about it; if you introduce a bug in the production but roll it back soon
    after the first user reports an error, then usually nothing bad happens. On the
    other hand, if production errors are rare but no rollback is applied, then the
    process to debug the production usually ends up in long sleepless nights and a
    number of dissatisfied users. This is why we need to think about the rollback
    strategy up front while creating the Jenkins pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the context of Continuous Delivery, there are two moments when the failure
    can happen:'
  prefs: []
  type: TYPE_NORMAL
- en: During the release process, in the pipeline execution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After the pipeline build is completed, in production
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first scenario is pretty simple and harmless. It concerns a case when the
    application is already deployed to production but the next stage fails, for example,
    the smoke test. Then, all we need to do is execute a script in the `post` pipeline
    section for the `failure` case, which downgrades the production service to the
    older Docker image version. If we use blue-green deployment (as described later
    in this chapter), the risk of any downtime is minimal since we usually execute
    the load-balancer switch as the last pipeline stage, after the smoke test.
  prefs: []
  type: TYPE_NORMAL
- en: The second scenario, when we notice a production bug after the pipeline is successfully
    completed, is more difficult and requires a few comments. Here, the rule is that
    we should always release the rolled back service using exactly the same process
    as the standard release. Otherwise, if we try to do something manually, in a faster
    way, we are asking for trouble. Any nonrepetitive task is risky, especially under
    stress, when the production is out of order.
  prefs: []
  type: TYPE_NORMAL
- en: As a side note, if the pipeline completes successfully but there is a production
    bug, then it means that our tests are not good enough. So, the first thing after
    rollback is to extend the unit/acceptance test suites with the corresponding scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: The most common Continuous Delivery process is one fully automated pipeline
    that starts by checking out the code and ends up with release to the production.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure presents how this works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/338a8478-71c8-40cb-9b94-ec25d9d5e0b8.png)'
  prefs: []
  type: TYPE_IMG
- en: We already presented the classic Continuous Delivery pipeline throughout this
    book. If the rollback should use exactly the same process, then all we need to
    do is revert the latest code change from the repository. As a result, the pipeline
    automatically builds, tests, and finally, releases the right version.
  prefs: []
  type: TYPE_NORMAL
- en: Repository reverts and emergency fixes should never skip the testing stages
    in the pipeline. Otherwise, we may end up with a release that is still not working
    correctly because of another issue that makes debugging even harder.
  prefs: []
  type: TYPE_NORMAL
- en: The solution is very simple and elegant. The only drawback is the downtime that
    we need to spend on the complete pipeline build. This downtime can be avoided
    if we use blue-green deployment or canary releases, in which cases, we only change
    the load balancer setting to address the healthy environment.
  prefs: []
  type: TYPE_NORMAL
- en: The rollback operation becomes way more complex in the case of orchestrated
    releases, during which many services are deployed at the same time. This is one
    of the reasons why orchestrated releases are treated as an anti-pattern, especially
    in the microservice world. The correct approach is to always maintain backwards
    compatibility, at least for some time (like we presented for the database at the
    beginning of this chapter). Then, it's possible to release each service independently.
  prefs: []
  type: TYPE_NORMAL
- en: Adding manual steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In general, the Continuous Delivery pipelines should be fully automated, triggered
    by a commit to the repository, and end up after the release. Sometimes, however,
    we can't avoid having manual steps. The most common example is the release approval,
    which means that the process is fully automated, but there is a manual step to
    approve the new release. Another common example is manual tests. Some of them
    may exist because we operate on the legacy system; some others may occur when
    a test simply cannot be automated. No matter what the reason is, sometimes there
    is no choice but to add a manual step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Jenkins syntax offers a keyword `input` for manual steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The pipeline will stop execution on the `input` step and wait until it's manually
    approved.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that manual steps quickly become a bottleneck in the delivery process,
    and this is why they should always be treated as a solution that's inferior to
    complete automation.
  prefs: []
  type: TYPE_NORMAL
- en: It is sometimes useful to set a timeout for the input in order to avoid waiting
    forever for the manual interaction. After the configured time is elapsed, the
    whole pipeline is aborted.
  prefs: []
  type: TYPE_NORMAL
- en: Release patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last section, we discussed the Jenkins pipeline patterns used to speed
    up the build execution (parallel steps), help with the code reuse (shared libraries),
    limit the risk of production bugs (rollback), and deal with manual approvals (manual
    steps). This section presents the next group of patterns, this time related to
    the release process. They are designed to reduce the risk of updating the production
    to a new software version.
  prefs: []
  type: TYPE_NORMAL
- en: 'We already described one of the release patterns, rolling updates, in [Chapter
    8](05fbbfd9-ff58-4ee8-be5d-90cb291f6320.xhtml), *Clustering with Docker Swarm*.
    Here, we present two more: blue-green deployment and canary releases.'
  prefs: []
  type: TYPE_NORMAL
- en: Blue-green deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Blue-green deployment is a technique to reduce the downtime associated with
    the release. It concerns having two identical production environments, one called
    green, the other called blue, as presented in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/4d853c46-e422-49e2-8c86-ab0a6bb780fa.png)'
  prefs: []
  type: TYPE_IMG
- en: In the figure, the currently accessible environment is blue. If we want to make
    a new release, then we deploy everything to the green environment and, at the
    end of the release process, change the load balancer to the green environment.
    As a result, a user, all of a sudden, starts using the new version. The next time
    we want to make a release, we make changes to the blue environment and, at the
    end, we change the load balancer to blue. We proceed the same every time, switching
    from one environment to another.
  prefs: []
  type: TYPE_NORMAL
- en: 'The blue-green deployment technique works correctly with two assumptions: environment
    isolation and no orchestrated releases.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This solution gives two significant benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Zero downtime**: All the downtime from the user perspective is a moment of
    changing the load balance switch, which is negligible'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rollback**: In order to roll back one version, it''s enough to change back
    the load balance switch blue-green deployment include:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Database**: Schema migrations can be tricky in case of a rollback, so it''s
    worth using the patterns presented at the beginning of this chapter'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transactions**: Running database transactions must be handed over to the
    new database'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Redundant infrastructure/resources**: We need to have double the resources'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are techniques and tools to overcome these challenges, so the blue-green
    deployment pattern is highly recommended and widely used in the IT industry.
  prefs: []
  type: TYPE_NORMAL
- en: You can read more about the blue-green deployment technique on the excellent
    Martin Fowler's blog [https://martinfowler.com/bliki/BlueGreenDeployment.html](https://martinfowler.com/bliki/BlueGreenDeployment.html).
  prefs: []
  type: TYPE_NORMAL
- en: Canary release
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Canary releasing is a technique to reduce the risk associated with introducing
    a new version of the software. Similar to blue-green deployment, it uses two identical
    environments, as presented in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/14406026-be79-4f9a-bd10-c3c7ede552fc.png)'
  prefs: []
  type: TYPE_IMG
- en: Also, similar to the blue-green deployment technique, the release process starts
    by deploying a new version in the environment that is currently unused. Here,
    however, the similarities end. The load balancer, instead of switching to the
    new environment, is set to link only a selected group of users to the new environment.
    All the rest still use the old version. This way, a new version can be tested
    by some users and in case of a bug, only a small group is affected. After the
    testing period, all users are switched to the new version.
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach has some great benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Acceptance and performance testing**: If the acceptance and performance testing
    is difficult to run in the staging environment, then it''s possible to test it
    in production, minimizing the impact on a small group of users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Simple rollback**: If a new change causes a failure, then rolling back is
    done by switching back all users to the old version.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A/B testing**: If we are not sure whether the new version is better than
    the UX or the performance perspective, then it''s possible to compare it with
    the old version.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Canary releasing shares the same drawbacks as blue-green deployment. The additional
    challenge is that we have two production systems running at the same time. Nevertheless,
    canary releasing is an excellent technique used in most companies to help with
    the release and testing.
  prefs: []
  type: TYPE_NORMAL
- en: You can read more about the canary releasing technique on the excellent Martin
    Fowler's blog [https://martinfowler.com/bliki/CanaryRelease.html](https://martinfowler.com/bliki/CanaryRelease.html).
  prefs: []
  type: TYPE_NORMAL
- en: Working with legacy systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All we have described so far applies smoothly to greenfield projects, for which
    setting up a Continuous Delivery pipeline is relatively simple.
  prefs: []
  type: TYPE_NORMAL
- en: Legacy systems are, however, way more challenging because they usually depend
    on manual tests and manual deployment steps. In this section, we will walk through
    the recommended scenario to incrementally apply Continuous Delivery to a legacy
    system.
  prefs: []
  type: TYPE_NORMAL
- en: As a step zero, I recommend reading an excellent book by Michael Feathers, *Working
    Effectively with Legacy Code*. His ideas on how to deal with testing, refactoring,
    and adding new features clear most of the concerns about how to automate the delivery
    process for legacy systems.
  prefs: []
  type: TYPE_NORMAL
- en: For many developers, it may be tempting to completely rewrite a legacy system,
    rather than refactor it. While the idea is interesting from a developer's perspective,
    it is usually a bad business decision that results in product failure. You can
    read more about the history of rewriting the Netscape browser in an excellent
    blog post by Joel Spolsky, *Things You Should Never Do,* at [https://www.joelonsoftware.com/2000/04/06/things-you-should-never-do-part-i](https://www.joelonsoftware.com/2000/04/06/things-you-should-never-do-part-i).
  prefs: []
  type: TYPE_NORMAL
- en: 'The way to apply the Continuous Delivery process depends a lot on the current
    project''s automation, the technologies used, the hardware infrastructure, and
    the current release process. Usually, it can be split into three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Automating build and deployment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Automating tests.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Refactoring and introducing new features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's look at them in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Automating build and deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step includes automating the deployment process. The good news is
    that in most legacy systems I have worked with, there was already some automation
    in place, for example, in the form of shell scripts.
  prefs: []
  type: TYPE_NORMAL
- en: 'In any case, the activities for automated deployment includes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Build and package**: Some automation usually already exists in the form of
    Makefile, Ant, Maven, any other build tool configuration, or a custom script.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Database migration**: We need to start managing the database schema in an
    incremental manner. It requires putting the current schema as an initial migration
    and making all the further changes with tools such as Flyway or Liquibase, as
    already described in this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deployment**: Even if the deployment process is fully manual, then there
    is usually a text/wiki page description that needs to be converted into an automated
    script.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Repeatable configuration**: In legacy systems, configuration files are usually
    changed manually. We need to extract the configuration and use a configuration
    management tool, as described in [Chapter 6](75a4971b-4eb7-45cb-a3ac-81a7d5ca79b2.xhtml),
    *Configuration Management with Ansible*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After the preceding steps, we can put everything into a deployment pipeline
    and use it as an automated phase after a manual UAT (user acceptance testing)
    cycle.
  prefs: []
  type: TYPE_NORMAL
- en: From the process perspective, it's worth already starting releasing more often.
    For example, if the release is yearly, try to do it quarterly, then monthly. The
    push for that factor will later result in faster-automated delivery adoption.
  prefs: []
  type: TYPE_NORMAL
- en: Automating tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The next step, usually much more difficult, is to prepare the automated tests
    for the system. It requires communicating with the QA team in order to understand
    how they currently test the software so that we can move everything into an automated
    acceptance test suite. This phase requires two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Acceptance/sanity test suite**: We need to add automated tests that replace
    some of the regression activities of the QA team. Depending on the system, they
    can be provided as a black-box Selenium test or Cucumber test.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**(Virtual) test environments**: At this point, we should be already thinking
    of the environments in which our tests would be run. Usually, the best solution
    to save resources and limit the number of machines required is to virtualize the
    testing environment using Vagrant or Docker.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ultimate goal is to have an automated acceptance test suite that will replace
    the whole UAT phase from the development cycle. Nevertheless, we can start with
    a sanity test that will shortly check if the system is correct from the regression
    perspective.
  prefs: []
  type: TYPE_NORMAL
- en: While adding test scenarios, remember that the test suite should execute in
    reasonable time. For sanity tests, it is usually less than 10 minutes.
  prefs: []
  type: TYPE_NORMAL
- en: Refactoring and introducing new features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we have at least the fundamental regression testing suite, we are ready
    to add new features and refactor the old code. It's always better to do it in
    small pieces, step by step because refactoring everything at once usually ends
    up in a chaos that leads to production failures (not clearly related to any particular
    change).
  prefs: []
  type: TYPE_NORMAL
- en: 'This phase usually includes the following activities:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Refactoring**: The best place to start refactoring the old code is where
    the new features are expected. Starting this way, we are prepared for the new
    feature requests to come.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rewrite**: If we plan to rewrite parts of the old code, we should start from
    the code that is the most difficult to test. This way, we constantly increase
    the code coverage in our project.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Introducing new features**: During the new feature implementation, it''s
    worth using the **feature toggle** pattern. Then, in case anything bad happens,
    we can quickly turn off the new feature. Actually, the same pattern should be
    used during refactoring.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For this phase, it''s worth reading an excellent book by *Martin Fowler*, *Refactoring:
    Improving the Design of Existing Code*.'
  prefs: []
  type: TYPE_NORMAL
- en: While touching the old code, it's good to follow the rule to always add a passing
    unit test first, and only then, change the code. With this approach, we can depend
    on automation to check that we don't change the business logic by accident.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the human element
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While introducing the automated delivery process to a legacy system, it's possible
    you will feel, more than anywhere else, the human factor. In order to automate
    the build process, we need to communicate well with the operations team, and they
    must be willing to share their knowledge. The same story applies to the manual
    QA team; they need to be involved in writing automated tests because only they
    know how to test the software. If you think about it, both the operations and
    QA teams need to contribute to the project that will later automate their work.
    At some point, they may realize that their future in the company is not stable
    and become less helpful. Many companies struggle with introducing the Continuous
    Delivery process because teams do not want to get involved enough.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we discussed how to approach legacy systems and the challenges
    they pose. If you are in progress of converting your project and organization
    into the Continuous Delivery approach, then you may want to have a look at the
    Continuous Delivery Maturity Model, which aims to give structure to the process
    of adopting the automated delivery.
  prefs: []
  type: TYPE_NORMAL
- en: A good description of the Continuous Delivery Maturity Model can be found at [https://developer.ibm.com/urbancode/docs/continuous-delivery-maturity-model/](https://developer.ibm.com/urbancode/docs/continuous-delivery-maturity-model/).
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we have covered various aspects of the Continuous Delivery
    process. Since practice makes perfect, we recommend the following exercises:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use Flyway to create a non-backwards-compatible change in the MySQL database:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the official Docker image, `mysql`, to start the database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configure Flyway with proper database address, username, and password
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Create an initial migration that creates a `users` table with three columns:
    `id`, `email`, and `password`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add a sample data to the table
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Change the `password` column to `hashed_password`, which will store the hashed
    passwords
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Split the non-backwards-compatible change into three migrations as described
    in this chapter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use MD5 or SHA for hashing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check that, as a result, the database stores no passwords in plain text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Create a Jenkins shared library with steps to build and unit test Gradle projects:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a separate repository for the library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Create two files in the library: `gradleBuild.groovy` and `gradleTest.groovy`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write the appropriate `call` methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add the library to Jenkins
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the steps from the library in a pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter was a mixture of various Continuous Delivery aspects that were
    not covered before. The key takeaways from the chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Databases are an essential part of most applications and should, therefore,
    be included in the Continuous Delivery process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Database schema changes are stored in the version control system and managed
    by database migration tools.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are two types of database schema change: backwards-compatible and backwards-incompatible.
    While the first type is simple, the second requires a bit of overhead (split to
    multiple migrations spread over time).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A database should not be the central point of the whole system. The preferred
    solution is to provide each service with its own database.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The delivery process should always be prepared for the rollback scenario.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Three release patterns should always be considered: rolling updates, blue-green
    deployment, and canary releasing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Legacy systems can be converted into the Continuous Delivery process in small
    steps rather than all at once.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thank you for reading this book. I hope you are ready to introduce the Continuous
    Delivery approach to your IT projects. As the last section of this book, I propose
    a list of the top 10 Continuous Delivery practices. Enjoy!
  prefs: []
  type: TYPE_NORMAL
- en: Practice 1 – own process within the team!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Own the entire process within the team, from receiving requirements to monitoring
    the production. As once said: <q>A program running on the developer''s machine
    makes no money.</q> This is why it''s important to have a small DevOps team that
    takes complete ownership of a product. Actually, that is the true meaning of DevOps
    - Development and Operations from the beginning to the end:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Own every stage of the Continuous Delivery pipeline: how to build the software,
    what the requirements are in acceptance tests, and how to release the product.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoid having a pipeline expert! Every member of the team should be involved
    in creating the pipeline.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find a good way to share the current pipeline state (and the production monitoring)
    among team members. The most effective solution is big screens in the team space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a developer, QA, and IT Operations engineer are separate experts, then make
    sure they work together in one agile team. Separate teams based on expertise result
    in taking no responsibility for the product.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remember that autonomy given to the team results in high job satisfaction and
    exceptional engagement. This leads to great products!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Practice 2 – automate everything!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Automate everything from business requirements (in the form of acceptance tests)
    to the deployment process. Manual descriptions, wiki pages with instruction steps,
    they all quickly become out of date and lead to tribal knowledge that makes the
    process slow, tedious, and unreliable. This, in turn, leads to a need for release
    rehearsals and makes every deployment unique. Don''t go down this path! As a rule,
    if you do anything for the second time, automate it:'
  prefs: []
  type: TYPE_NORMAL
- en: Eliminate all manual steps; they are a source of errors! The whole process must
    be repeatable and reliable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Don't ever make any changes directly in production! Use configuration management
    tools instead.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use precisely the same mechanism to deploy to every environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Always include an automated smoke test to check if the release completed successfully.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use database schema migrations to automate database changes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use automatic maintenance scripts for backup and cleanup. Don't forget to remove
    unused Docker images!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Practice 3 – version everything!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Version everything: software source code, build scripts, automated tests, configuration
    management files, Continuous Delivery pipelines, monitoring scripts, binaries,
    and documentation. Simply everything. Make your work task-based, where each task
    results in a commit to the repository, no matter whether it''s related to requirement
    gathering, architecture design, configuration, or the software development. A
    task starts on the agile board and ends up in the repository. This way, you maintain
    a single point of truth with the history and reasons for the changes:'
  prefs: []
  type: TYPE_NORMAL
- en: Be strict about the version control. Everything means everything!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep source code and configuration in the code repository, binaries in the artifact
    repository, and tasks in the agile issue tracking tool.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Develop the Continuous Delivery pipeline as a code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use database migrations and store them in a repository.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Store documentation in the form of markdown files that can be version-controlled.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Practice 4 – use business language for acceptance tests!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Use business-facing language for acceptance tests to improve the mutual communication
    and the common understanding of the requirements. Work closely with the product
    owner to create what Eric Evan called the *ubiquitous language*, a common dialect
    between the business and technology. Misunderstandings are the root cause of most
    project failures:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a common language and use it inside the project.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use an acceptance testing framework such as Cucumber or FitNesse to help the
    business team understand and get them involved.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Express business values inside acceptance tests and don't forget about them
    during development. It's easy to spend too much time on unrelated topics!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improve and maintain acceptance tests so that they always act as regression
    tests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make sure everyone is aware that a passing acceptance test suite means a green
    light from the business to release the software.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Practice 5 – be ready to roll back!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Be ready to roll back; sooner or later you will need to do it. Remember, You
    don''t need more QAs, you need a faster rollback. If anything goes wrong in production,
    the first thing you want to do is to play safe and come back to the last working
    version:'
  prefs: []
  type: TYPE_NORMAL
- en: Develop a rollback strategy and the process of what to do when the system is
    down
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Split non-backwards-compatible database changes into compatible ones
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Always use the same process of delivery for rollbacks and for standard releases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider introducing blue-green deployments or canary releases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Don't be afraid of bugs, the user won't leave you if you react quickly!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Practice 6 – don't underestimate the impact of people
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Don''t underestimate the impact of people. They are usually way more important
    than tools. You won''t automate the delivery if the IT Operations team won''t
    help you. After all, they have the knowledge about the current process. The same
    applies to QAs, business, and everyone involved. Make them important and involved:'
  prefs: []
  type: TYPE_NORMAL
- en: Let QAs and IT operations be a part of the DevOps team. You need their knowledge
    and skills!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide training to members that are currently doing manual activities so that
    they can move to automation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Favor informal communication and a flat structure of organization over hierarchy
    and orders. You won't do anything without goodwill!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Practice 7 – build in traceability!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Build in traceability for the delivery process and working system. There is
    nothing worse than a failure without any log messages. Monitor the number of requests,
    the latency, the load of production servers, the state of the Continuous Delivery
    pipeline, and everything you can think of that could help you to analyze your
    current software. Be proactive! At some point, you will need to check the stats
    and logs:'
  prefs: []
  type: TYPE_NORMAL
- en: Log pipeline activities! In the case of failure, notify the team with an informative
    message.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement proper logging and monitoring of the running system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use specialized tools for system monitoring such as Kibana, Grafana, or Logmatic.io.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrate production monitoring into your development ecosystem. Consider having
    big screens with the current production stats in the common team space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Practice 8 – integrate often!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Integrate often, actually, all the time! As someone said: *Continuous is more
    often than you think*. There is nothing more frustrating than resolving merge
    conflicts. Continuous Integration is less about the tool and more about the team
    practice. Integrate the code into one codebase at least a few times a day. Forget
    about long-lasting feature branches and a huge number of local changes. Trunk-base
    development and feature toggles for the win!'
  prefs: []
  type: TYPE_NORMAL
- en: Use trunk-based development and feature toggles instead of feature branches.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you need a branch or local changes, make sure that you integrate with the
    rest of the team at least once a day.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Always keep the trunk healthy; make sure you run tests before you merge into
    the baseline.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run the pipeline after every commit to the repository for a fast feedback cycle.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Practice 9 – build binaries only once!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Build binaries only once and run the same one on each of the environments.
    No matter if they are in a form of Docker images or JAR packages, building only
    once eliminates the risk of differences introduced by various environments. It
    also saves time and resources:'
  prefs: []
  type: TYPE_NORMAL
- en: Build once and pass the same binary between environments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use artifact repository to store and version binaries. Don't ever use the source
    code repository for that purpose.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Externalize configurations and use a configuration management tool to introduce
    differences between environments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Practice 10 – release often!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Release often, preferably after each commit to the repository. As the saying
    goes, *If it hurts, do it more often.* Releasing as a daily routine makes the
    process predictable and calm. Stay away from being trapped in the rare release
    habit. That will only get worse and you will end up with releasing once a year
    having a three months' preparation period!
  prefs: []
  type: TYPE_NORMAL
- en: Rephrase your definition of done to *Done means released*. Take ownership of
    the whole process!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use feature toggles to hide (from users) features that are still in progress.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use canary releases and quick rollback to reduce the risk of bugs in the production.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adopt a zero-downtime deployment strategy to enable frequent releases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
