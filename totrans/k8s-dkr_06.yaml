- en: '*Chapter 4*: Deploying Kubernetes Using KinD'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the largest obstacles to learning Kubernetes is having enough resources
    to create a cluster for testing or development. Like most IT professionals, we
    like to have a Kubernetes cluster on our laptops for demonstrations and for testing
    products in general.
  prefs: []
  type: TYPE_NORMAL
- en: Often, you may have a need to run multiple clusters for a complex demonstration,
    such as a multi-cluster service mesh or testing **kubefed2**. These scenarios
    would require multiple servers to create the necessary clusters, which, in turn,
    would require a lot of RAM and a hypervisor.
  prefs: []
  type: TYPE_NORMAL
- en: To do full testing on a multiple cluster scenario, you would need to create
    six nodes for each cluster. If you created the clusters using virtual machines,
    you would need to have enough resources to run 6 virtual machines. Each of the
    machines would have an overhead including disk space, memory, and CPU utilization.
  prefs: []
  type: TYPE_NORMAL
- en: But what if you could create a cluster using just containers? Using containers,
    rather than full virtual machines, will give you the ability to run additional
    nodes due to the reduced system requirements, create and delete clusters in minutes
    with a single command, script cluster creation, and allow you to run multiple
    clusters on a single host.
  prefs: []
  type: TYPE_NORMAL
- en: Using containers to run a Kubernetes cluster provides you with an environment
    that would be difficult for most people to deploy using virtual machines, or physical
    hardware due to resource constraints. To explain how to run a cluster using only
    containers locally, we will use KinD to create a Kubernetes cluster on your Docker
    host. We will deploy a multi-node cluster that you will use in future chapters
    to test and deploy components such as Ingress controllers, authentication, RBAC,
    security policies, and more.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Kubernetes components and objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using development clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing KinD
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a KinD cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reviewing your KinD cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding a custom load balancer for Ingress
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter has the following technical requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: A Docker host installed using the steps from [*Chapter 1*](B15514_01_Final_ASB_ePub.xhtml#_idTextAnchor018),
    *Docker and Container Essentials*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installation scripts from this book's GitHub repository
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can access the code for this chapter by going to this book''s GitHub repository:
    [https://github.com/PacktPublishing/Kubernetes-and-Docker-The-Complete-Guide](https://github.com/PacktPublishing/Kubernetes-and-Docker-The-Complete-Guide).'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We thought it was important to point out that this chapter will reference multiple
    Kubernetes objects, some without a lot of context. [*Chapter 5*](B15514_05_Final_ASB_ePub.xhtml#_idTextAnchor150)*,
    Kubernetes Bootcamp*, goes over Kubernetes objects in detail, many with commands
    you can use to understand them, so we thought having a cluster to use while reading
    about this would be useful.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the base Kubernetes topics covered in this chapter will be discussed
    in future chapters, so if some topics are a bit foggy after you've read this chapter,
    don't fear! They will be discussed in detail in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Kubernetes components and objects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since this chapter will refer to common Kubernetes objects and components, we
    wanted to provide a short table of terms that you will see and a brief definition
    of each to provide context.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [*Chapter 5*](B15514_05_Final_ASB_ePub.xhtml#_idTextAnchor150), *Kubernetes
    Bootcamp*, we will go over the components of Kubernetes and the base set of objects
    that are included in a cluster. We will also discuss how to interact with a cluster
    using the kubectl executable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 4.1 – Kubernetes components and objects'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Table_4.1.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Table 4.1 – Kubernetes components and objects
  prefs: []
  type: TYPE_NORMAL
- en: While these are only a few of the objects that are available in a Kubernetes
    cluster, they are the main objects we will mention in this chapter. Knowing what
    each object is and having basic knowledge of their functionality will help you
    understand this chapter and deploy a KinD cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Interacting with a cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To test our KinD installation, we will interact with the cluster using the
    kubectl executable. We will go over kubectl in [*Chapter 5*](B15514_05_Final_ASB_ePub.xhtml#_idTextAnchor150)*,
    Kubernetes Bootcamp*, but since we will be using a few commands in this chapter,
    we wanted to provide the commands we will use in a table with an explanation of
    what the options provide:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 4.2 – Basic kubectl commands'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Table_4.2.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Table 4.2 – Basic kubectl commands
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will use these basic commands to deploy parts of the cluster
    that we will use throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will introduce the concept of development clusters and then focus
    on one of the most popular tools used to create development clusters: KinD.'
  prefs: []
  type: TYPE_NORMAL
- en: Using development clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Over the years, various tools have been created to install development Kubernetes
    clusters, allowing admins and developers to perform testing on a local system.
    Many of these tools worked for basic Kubernetes tests, but they often had limitations
    that made them less than ideal for quick, advanced scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the most common solutions available are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Docker Desktop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: minikube
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: kubeadm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each solution has benefits, limitations, and use cases. Some solutions limit
    you to a single node that runs both the control plane and worker nodes. Others
    offer multi-node support but require additional resources to create multiple virtual
    machines. Depending on your development or testing requirements, these solutions
    may not fill your needs completely.
  prefs: []
  type: TYPE_NORMAL
- en: It seems that a new solution is coming out every few weeks, and one of the newest
    options for creating development clusters is a project from a **Kubernetes in
    Docker** (**KinD**) Kubernetes SIG.
  prefs: []
  type: TYPE_NORMAL
- en: Using a single host, KinD allows you to create multiple clusters, and each cluster
    can have multiple control plane and worker nodes. The ability to run multiple
    nodes allows advanced testing that would have required more resources using another
    solution. KinD has been very well received by the community and has an active
    Git community at [https://github.com/kubernetes-sigs/kind](https://github.com/kubernetes-sigs/kind),
    as well as a Slack channel (*#kind*).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Do not use KinD as a production cluster or expose a KinD cluster to the internet.
    While KinD clusters offer most of the same features you would want in a production
    cluster, it has **not** been designed for production environments.
  prefs: []
  type: TYPE_NORMAL
- en: Why did we select KinD for this book?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we started this book, we wanted to include theory, as well as hands-on
    experience. KinD allows us to provide scripts to spin up and spin down clusters,
    and while other solutions can do something similar, KinD can create a new multi-node
    cluster in minutes. We wanted to separate the control plane and worker nodes to
    provide a more "realistic" cluster. In order to limit the hardware requirements
    and to make Ingress easier to configure, we will only create a two-node cluster
    for the exercises in this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'A multi-node cluster can be created in a few minutes and once testing has been
    completed, clusters can be torn down in a few seconds. The ability to spin up
    and spin down clusters makes KinD the perfect platform for our exercises. KinD''s
    requirements are simple: you only need a running Docker daemon to create a cluster.
    This means that it is compatible with most operating systems, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Linux
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: macOS running Docker Desktop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Windows running Docker Desktop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Windows running WSL2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, KinD does not offer support for Chrome OS.
  prefs: []
  type: TYPE_NORMAL
- en: While KinD supports most operating systems, we have selected Ubuntu 18.04 as
    our host system. Some of the exercises in this book require files to be in specific
    directories and selecting a single Linux version helps us make sure the exercises
    work as designed. If you do not have access to an Ubuntu server at home, you can
    create a virtual machine in a cloud provider such as GCP. Google offers $300 in
    credit, which is more than enough to run a single Ubuntu server for a few weeks.
    You can view GCP's free options at [https://cloud.google.com/free/](https://cloud.google.com/free/).
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's explain how KinD works and what a base KinD Kubernetes cluster looks
    like.
  prefs: []
  type: TYPE_NORMAL
- en: Working with a base KinD Kubernetes cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At a high level, you can think of a KinD cluster as consisting of a **single**
    Docker container that runs a control plane node and a worker node to create a
    Kubernetes cluster. To make the deployment easy and robust, KinD bundles every
    Kubernetes object into a single image, known as a node image. This node image
    contains all the required Kubernetes components to create a single-node cluster,
    or a multi-node cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once a cluster is running, you can use Docker to exec into a control plane
    node container and look at the process list. In the process list, you will see
    the standard Kubernetes components for the control plane nodes running:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – Host process list showing control plane components'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_4.1_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.1 – Host process list showing control plane components
  prefs: []
  type: TYPE_NORMAL
- en: 'If you were to exec into a worker node to check the components, you would see
    all the standard worker node components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – Host process list showing worker components'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_4.2_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.2 – Host process list showing worker components
  prefs: []
  type: TYPE_NORMAL
- en: We will cover the standard Kubernetes components in [*Chapter 5*](B15514_05_Final_ASB_ePub.xhtml#_idTextAnchor150)*,
    Kubernetes Bootcamp*, including **kube-apiserver**, **kubelets**, **kube-proxy**,
    **kube-scheduler**, and **kube-controller-manager**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to standard Kubernetes components, both KinD nodes have an additional
    component that is not part of most standard installations: Kindnet. Kindnet is
    the included, default CNI when you install a base KinD cluster. While Kindnet
    is the default CNI, you have the option to disable it and use an alternative,
    such as Calico.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you have seen each node and the Kubernetes components, let''s take
    a look at what''s included with a base KinD cluster. To show the complete cluster
    and all the components that are running, we can run the **kubectl get pods --all-namespaces**
    command. This will list all the running components for the cluster, including
    the base components we will discuss in [*Chapter 5*](B15514_05_Final_ASB_ePub.xhtml#_idTextAnchor150),
    *Kubernetes Bootcamp*. In addition to the base cluster components, you may notice
    a running pod in a namespace called **local-path-storage**, along with a pod named
    **local-path-provisioner**. This pod is running one of the add-ons that KinD includes,
    providing the cluster with the ability to auto-provision **PersistentVolumeClaims**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – kubectl get pods showing local-path-provisioner'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_4.3_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.3 – kubectl get pods showing local-path-provisioner
  prefs: []
  type: TYPE_NORMAL
- en: Most development cluster offerings provide similar, common functions that people
    need to test deployments on Kubernetes. They all provide a Kubernetes control
    plane and worker nodes, and most include a default CNI for networking. Few offerings
    go beyond this base functionality, and as Kubernetes workloads mature, you may
    find the need for additional plugins such as **local-path-provisioner**. We will
    leverage this component heavily in some of the exercises in this book because
    without it, we will have a tougher time creating some of the procedures.
  prefs: []
  type: TYPE_NORMAL
- en: Why should you care about persistent volumes in your development cluster? Most
    production clusters running Kubernetes will provide persistent storage to developers.
    Usually, the storage will be backed by storage systems based on block storage,
    S3, or NFS. Aside from NFS, most home labs rarely have the resources to run a
    full-featured storage system. **local-path-provisioner** removes this limitation
    from users by providing all the functions to your KinD cluster that an expensive
    storage solution would provide.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 5*](B15514_05_Final_ASB_ePub.xhtml#_idTextAnchor150)*, Kubernetes
    Bootcamp*, we will discuss a few API objects that are part of Kubernetes storage.
    We will discuss the **CSIdrivers**, **CSInodes**, and **StorageClass** objects.
    These objects are used by the cluster to provide access to the backend storage
    system. Once installed and configured, pods consume the storage using the **PersistentVolumes**
    and **PersistentVolumeClaims** objects. Storage objects are important to understand,
    but when they were first released, they were difficult for most people to test
    since they aren't included in most Kubernetes development offerings.
  prefs: []
  type: TYPE_NORMAL
- en: KinD recognized this limitation and chose to bundle a project from Rancher called
    **local-path-provisioner**, which is based on the Kubernetes local persistent
    volumes that were introduced in Kubernetes 1.10.
  prefs: []
  type: TYPE_NORMAL
- en: You may be wondering why anyone would need an add-on since Kubernetes has native
    support for local host persistent volumes. While support may have been added for
    local persistent storage, Kubernetes has not added auto-provisioning capabilities.
    CNCF does offer an auto-provisioner, but it must be installed and configured as
    a separate Kubernetes component. KinD makes it easy to auto-provision since the
    provisioner is included in all base installations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rancher''s project provides the following to KinD:'
  prefs: []
  type: TYPE_NORMAL
- en: Auto-creation of **PersistentVolumes** when a PVC request is created
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A default **StorageClass** named standard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the auto-provisioner sees a **PersistentVolumeClaim** request hit the API
    server, a **PersistentVolume** will be created and the pod's PVC will be bound
    to the newly created PVC.
  prefs: []
  type: TYPE_NORMAL
- en: '**local-path-provisioner** adds a feature to KinD clusters that greatly expands
    the potential test scenarios that you can run. Without the ability to auto-provision
    persistent disks, it would be a challenge to test many pre-built deployments that
    require persistent disks.'
  prefs: []
  type: TYPE_NORMAL
- en: With the help of Rancher, KinD provides you with a solution so that you can
    experiment with dynamic volumes, storage classes, and other storage tests that
    would otherwise be impossible to run outside a data center. We will use the provisioner
    in multiple chapters to provide volumes to different deployments. We will point
    these out to reinforce the advantages of using auto-provisioning.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the node image
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The node image is what provides KinD the magic to run Kubernetes inside a Docker
    container. This is an impressive accomplishment since Docker relies on a **systemd**
    running system and other components that are not included in most container images.
  prefs: []
  type: TYPE_NORMAL
- en: KinD starts off with a base image, which is an image the team has developed
    that contains everything required for Docker, Kubernetes, and **systemd**. Since
    the base image is based on an Ubuntu image, the team removes services that are
    not required and configures **systemd** for Docker. Finally, the node image is
    created using the base image.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: If you want to know the details of how the base image is created, you can look
    at the Dockerfile in the KinD team's GitHub repository at [https://github.com/kubernetes-sigs/kind/blob/controlplane/images/base/Dockerfile](https://github.com/kubernetes-sigs/kind/blob/controlplane/images/base/Dockerfile).
  prefs: []
  type: TYPE_NORMAL
- en: KinD and Docker networking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since KinD uses Docker as the container engine to run the cluster nodes, all
    clusters are limited to the same network constraints that a standard Docker container
    is. In [*Chapter 3*](B15514_03_Final_ASB_ePub.xhtml#_idTextAnchor062)*, Understanding
    Docker Networking*, we had a refresher on Docker networking and the potential
    limitations of Docker's default networking stack. These limitations do not limit
    testing your KinD Kubernetes cluster from the local host, but they can lead to
    issues when you want to test containers from other machines on your network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Along with the Docker networking considerations, we must consider the Kubernetes
    **Container Network Interface** (**CNI**) as well. Officially, the KinD team has
    limited the networking options to only two CNIs: Kindnet and Calico. Kindnet is
    the only CNI they will support but you do have the option to disable the default
    Kindnet installation, which will create a cluster without a CNI installed. After
    the cluster has been deployed, you can deploy a CNI manifest such as Calico.'
  prefs: []
  type: TYPE_NORMAL
- en: Many Kubernetes installations for both small development clusters and enterprise
    clusters use Tigera's Calico for the CNI and as such, we have elected to use Calico
    as our CNI for the exercises in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Keeping track of the nesting dolls
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Running a solution such as KinD can get confusing due to the container-in-a-container
    deployment. We compare this to Russian nesting dolls, where one doll fits into
    another, then another, and so on. As you start to play with KinD for your own
    cluster, you may lose track of the communication paths between your host, Docker,
    and the Kubernetes nodes. To keep your sanity, you should have a solid understanding
    of where each component is running and how you can interact with each one.
  prefs: []
  type: TYPE_NORMAL
- en: The following diagram shows the three layers that must be running to form a
    KinD cluster. It's important to note that each layer can only interact with the
    layer directly above it. This means that the KinD container in layer 3 can only
    see the Docker image running in layer 2, and the Docker image can see the Linux
    host running in layer 1\. If you wanted to communicate directly from the host
    to a container running in your KinD cluster, you would need to go through the
    Docker layer, and then to the Kubernetes container in layer 3\.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is important to understand so that you can use KinD effectively as a testing
    environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – Host cannot communicate with KinD directly'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_4.4_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.4 – Host cannot communicate with KinD directly
  prefs: []
  type: TYPE_NORMAL
- en: As an example, consider that you want to deploy a web server to your Kubernetes
    cluster. You deploy an Ingress controller in the KinD cluster and you want to
    test the site using Chrome on your Docker host or a different workstation on the
    network. You attempt to target the host on port 80 and receive a failure in the
    browser. Why would this fail?
  prefs: []
  type: TYPE_NORMAL
- en: 'The pod running the web server is in layer 3 and cannot receive direct traffic
    from the host or machines on the network. In order to access the web server from
    your host, you will need to forward the traffic from the Docker layer to the KinD
    layer. Remember that in [*Chapter 3*](B15514_03_Final_ASB_ePub.xhtml#_idTextAnchor062)*,
    Understanding Docker Networking*, we explained how to expose a container to the
    network by adding a listening port to the container. In our case, we need port
    80 and port 443\. When a container is started with a port, the Docker daemon will
    forward the incoming traffic from the host to the running Docker container:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 – Host communicates with KinD via an Ingress controller'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_4.5_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.5 – Host communicates with KinD via an Ingress controller
  prefs: []
  type: TYPE_NORMAL
- en: With ports 80 and 443 exposed on the Docker container, the Docker daemon will
    now accept incoming requests for 80 and 443 and the NGINX Ingress controller will
    receive the traffic. This works because we have exposed ports 80 and 443 in two
    places on the Docker layer. We have exposed it in the Kubernetes layer by running
    our NGINX container using host ports 80 and 443\. This installation process will
    be explained later in this chapter, but for now, you just need to understand the
    basic flow.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the host, you make a request for a web server that has an Ingress rule in
    your Kubernetes cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: The request looks at the IP address that was requested (in this case, the local
    IP address).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Docker container running our Kubernetes node is listening on the IP address
    for ports 80 and 443, so the request is accepted and sent to the running container.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The NGINX pod in your Kubernetes cluster has been configured to use the host
    ports 80 and 443, so the traffic is forwarded to the pod.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The user receives the requested web page from the web server via the NGINX Ingress
    controller.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is a little confusing, but the more you use KinD and interact with it,
    the easier this becomes.
  prefs: []
  type: TYPE_NORMAL
- en: To use a KinD cluster for your development requirements, you need to understand
    how KinD works. So far, you have learned about the node image and how the image
    is used to create a cluster. You've also learned how KinD network traffic flows
    between the Docker host and the containers running the cluster. With this base
    knowledge, we will move on to creating a Kubernetes cluster using KinD.
  prefs: []
  type: TYPE_NORMAL
- en: Installing KinD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The files for this chapter are located in the KinD directory. You can use the
    provided files, or you can create your own files from this chapter's content.
    We will explain each step of the installation process in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, the current version of KinD is .0.8.1\. Version .0.8.0
    introduced a new feature; that is, maintaining cluster state between reboot and
    Docker restarts.
  prefs: []
  type: TYPE_NORMAL
- en: Installing KinD – prerequisites
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: KinD requires a few prerequisites before you can create a cluster. In this section,
    we will detail each requirement and what how to install each component.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Kubectl
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since KinD is a single executable, it does not install **kubectl**. If you
    do not have **kubectl** installed and you are using an Ubuntu 18.04 system, you
    can install it by running a snap install:'
  prefs: []
  type: TYPE_NORMAL
- en: sudo snap install kubectl --classic
  prefs: []
  type: TYPE_NORMAL
- en: Installing Go
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we can create a KinD cluster, you will need to install Go on your host.
    If you already have Go installed and working, you can skip this step. Installing
    Go requires you to download the Go archive, extract the executable, and set a
    project path. The following commands can be used to install Go on your machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'The script to install Go can be executed from this book''s repository by running
    **/chapter4/install-go.sh**:'
  prefs: []
  type: TYPE_NORMAL
- en: wget https://dl.google.com/go/go1.13.3.linux-amd64.tar.gz
  prefs: []
  type: TYPE_NORMAL
- en: tar -xzf go1.13.3.linux-amd64.tar.gz
  prefs: []
  type: TYPE_NORMAL
- en: sudo mv go /usr/local
  prefs: []
  type: TYPE_NORMAL
- en: mkdir -p $HOME/Projects/Project1
  prefs: []
  type: TYPE_NORMAL
- en: cat << 'EOF' >> ~/.bash_profile
  prefs: []
  type: TYPE_NORMAL
- en: export -p GOROOT=/usr/local/go
  prefs: []
  type: TYPE_NORMAL
- en: export -p GOPATH=$HOME/Projects/Project1
  prefs: []
  type: TYPE_NORMAL
- en: export -p PATH=$GOPATH/bin:$GOROOT/bin:$PATH
  prefs: []
  type: TYPE_NORMAL
- en: EOF
  prefs: []
  type: TYPE_NORMAL
- en: source ~/.bash_profile
  prefs: []
  type: TYPE_NORMAL
- en: 'The commands in the preceding list will do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Download Go to your host, uncompress the archive, and move the files to **/usr/local**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a Go project folder in your home directory called **Projects/Project1**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add Go environment variables to**.bash_profile**, which are required to execute
    Go applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that you have the prerequisites in place, we can move on to installing KinD.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the KinD binary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Installing KinD is an easy process; it can be done with a single command. You
    can install KinD by running the included script in this book''s repository, located
    at **/chapter4/install-kind.sh**. Alternatively, you can execute the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: GO111MODULE="on" go get sigs.k8s.io/kind@v0.7.0
  prefs: []
  type: TYPE_NORMAL
- en: 'Once installed, you can verify that KinD has been installed correctly by typing
    **kind version** into the prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: kind version
  prefs: []
  type: TYPE_NORMAL
- en: 'This will return the installed version:'
  prefs: []
  type: TYPE_NORMAL
- en: kind v0.7.0 go1.13.3 linux/amd64
  prefs: []
  type: TYPE_NORMAL
- en: 'The KinD executable provides every option you will need to maintain a cluster''s
    life cycle. Of course, the KinD executable can create and delete clusters, but
    it also provides the following capabilites:'
  prefs: []
  type: TYPE_NORMAL
- en: The ability to create custom build base and node images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can export **kubeconfig** or log files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can retrieve clusters, nodes, or **kubeconfig** files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can load images into nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that you have installed the KinD utility, you are almost ready to create
    your KinD cluster. Before we execute a few **create cluster** commands, we will
    explain some of the creation options that KinD provides.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a KinD cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you have met all the requirements, you can create your first cluster
    using the KinD executable. The KinD utility can create a single-node cluster,
    as well as a complex cluster that's running multiple nodes for the control plane
    with multiple worker nodes. In this section, we will discuss the KinD executable
    options. By the end of the chapter, you will have a two-node cluster running –
    a single control plane node and a single worker node.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: For the exercises in this book, we will install a multi-node cluster. The simple
    cluster configuration is an example and should not be used for our exercises.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a simple cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To create a simple cluster that runs the control plane and a worker node in
    a single container, you only need to execute the KinD executable with the **create
    cluster** option.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a quick single-node cluster to see how quickly KinD creates a
    fast development cluster. On your host, create a cluster using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: kind create cluster
  prefs: []
  type: TYPE_NORMAL
- en: 'This will quickly create a cluster with all the Kubernetes components in a
    single Docker container by using a cluster name of **kind**. It will also assign
    the Docker container a name of **kind-control-plane**. If you want to assign a
    cluster name, rather than the default name, you need to add the **--name <cluster
    name>** option to the **create cluster** command:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Creating cluster "kind" ...**'
  prefs: []
  type: TYPE_NORMAL
- en: '**  Ensuring node image (kindest/node:v1.18.2)**'
  prefs: []
  type: TYPE_NORMAL
- en: '**  Preparing nodes**'
  prefs: []
  type: TYPE_NORMAL
- en: '**  Writing configuration**'
  prefs: []
  type: TYPE_NORMAL
- en: '**  Starting control-plane**'
  prefs: []
  type: TYPE_NORMAL
- en: '**  Installing CNI**'
  prefs: []
  type: TYPE_NORMAL
- en: '**  Installing StorageClass**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Set kubectl context to "kind-kind"**'
  prefs: []
  type: TYPE_NORMAL
- en: '**You can now use your cluster with:**'
  prefs: []
  type: TYPE_NORMAL
- en: '**kubectl cluster-info --context kind-kind**'
  prefs: []
  type: TYPE_NORMAL
- en: The **create** command will create the cluster and modify the kubectl **config**
    file. KinD will add the new cluster to your current kubectl **config** file, and
    it will set the new cluster as the default context.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can verify that the cluster was created successfully by listing the nodes
    using the kubectl utility:'
  prefs: []
  type: TYPE_NORMAL
- en: kubectl get nodes
  prefs: []
  type: TYPE_NORMAL
- en: 'This will return the running nodes, which, for a basic cluster, are single
    nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: NAME               STATUS   ROLES    AGE   VERSION
  prefs: []
  type: TYPE_NORMAL
- en: kind-control-plane Ready    master   130m  v1.18.2
  prefs: []
  type: TYPE_NORMAL
- en: The main point of deploying this single-node cluster was to show you how quickly
    KinD can create a cluster that you can use for testing. For our exercises, we
    want to split up the control plane and worker node so that we can delete this
    cluster using the steps in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Deleting a cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When you have finished testing, you can delete the cluster using the **delete**
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: kind delete cluster –name <cluster name>
  prefs: []
  type: TYPE_NORMAL
- en: The **delete** command will quickly delete the cluster, including any entries
    in your **kubeconfig** file.
  prefs: []
  type: TYPE_NORMAL
- en: A quick single-node cluster is useful for many use cases, but you may want to
    create a multi-node cluster for various testing scenarios. Creating a more complex
    cluster requires that you create a config file.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a cluster config file
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When creating a multi-node cluster, such as a two-node cluster with custom
    options, we need to create a cluster config file. The config file is a YAML file
    and the format should look familiar. Setting values in this file allows you to
    customize the KinD cluster, including the number of nodes, API options, and more.
    The config file we''ll use to create the cluster for the book is shown here –
    it is included in this book''s repository at **/chapter4/cluster01-kind.yaml**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'kind: Cluster'
  prefs: []
  type: TYPE_NORMAL
- en: 'apiVersion: kind.x-k8s.io/v1alpha4'
  prefs: []
  type: TYPE_NORMAL
- en: 'networking:'
  prefs: []
  type: TYPE_NORMAL
- en: 'apiServerAddress: "0.0.0.0"'
  prefs: []
  type: TYPE_NORMAL
- en: 'disableDefaultCNI: true'
  prefs: []
  type: TYPE_NORMAL
- en: 'kubeadmConfigPatches:'
  prefs: []
  type: TYPE_NORMAL
- en: '- |'
  prefs: []
  type: TYPE_NORMAL
- en: 'apiVersion: kubeadm.k8s.io/v1beta2'
  prefs: []
  type: TYPE_NORMAL
- en: 'kind: ClusterConfiguration'
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: config'
  prefs: []
  type: TYPE_NORMAL
- en: 'networking:'
  prefs: []
  type: TYPE_NORMAL
- en: 'serviceSubnet: "10.96.0.1/12"'
  prefs: []
  type: TYPE_NORMAL
- en: 'podSubnet: "192.168.0.0/16"'
  prefs: []
  type: TYPE_NORMAL
- en: 'nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '- role: control-plane'
  prefs: []
  type: TYPE_NORMAL
- en: '- role: worker'
  prefs: []
  type: TYPE_NORMAL
- en: 'extraPortMappings:'
  prefs: []
  type: TYPE_NORMAL
- en: '- containerPort: 80'
  prefs: []
  type: TYPE_NORMAL
- en: 'hostPort: 80'
  prefs: []
  type: TYPE_NORMAL
- en: '- containerPort: 443'
  prefs: []
  type: TYPE_NORMAL
- en: 'hostPort: 443'
  prefs: []
  type: TYPE_NORMAL
- en: 'extraMounts:'
  prefs: []
  type: TYPE_NORMAL
- en: '- hostPath: /usr/src'
  prefs: []
  type: TYPE_NORMAL
- en: 'containerPath: /usr/src'
  prefs: []
  type: TYPE_NORMAL
- en: 'Details about each of the custom options in the file are provided in the following
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 4.3 – KinD configuration options'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Table_4.3.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Table 4.3 – KinD configuration options
  prefs: []
  type: TYPE_NORMAL
- en: If you plan to create a cluster that goes beyond a single-node cluster without
    using advanced options, you will need to create a configuration file. Understanding
    the options available to you will allow you to create a Kubernetes cluster that
    has advanced components such as Ingress controllers or multiple nodes to test
    failure and recovery procedures for deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know how to create a simple all-in-one container for running a
    cluster and how to create a multi-node cluster using a config file, let's discuss
    a more complex cluster example.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-node cluster configuration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you only wanted a multi-node cluster without any extra options, you could
    create a simple configuration file that lists the number and node types you want
    in the cluster. The following **config** file will create a cluster with three
    control plane nodes and three worker nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'kind: Cluster'
  prefs: []
  type: TYPE_NORMAL
- en: 'apiVersion: kind.x-k8s.io/v1alpha4'
  prefs: []
  type: TYPE_NORMAL
- en: 'nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '- role: control-plane'
  prefs: []
  type: TYPE_NORMAL
- en: '- role: control-plane'
  prefs: []
  type: TYPE_NORMAL
- en: '- role: control-plane'
  prefs: []
  type: TYPE_NORMAL
- en: '- role: worker'
  prefs: []
  type: TYPE_NORMAL
- en: '- role: worker'
  prefs: []
  type: TYPE_NORMAL
- en: '- role: worker'
  prefs: []
  type: TYPE_NORMAL
- en: Using multiple control plane servers introduces additional complexity since
    we can only target a single host or IP in our configuration files. To make this
    configuration usable, we need to deploy a load balancer in front of our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'KinD has considered this, and if you do deploy multiple control plane nodes,
    the installation will create an additional container running a HAProxy load balancer.
    If we look at the running containers from a multi-node config, we will see six
    node containers running and a HAProxy container:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 4.4 – KinD configuration options'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Table_4.4.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Table 4.4 – KinD configuration options
  prefs: []
  type: TYPE_NORMAL
- en: Remember that, in [*Chapter 3*](B15514_03_Final_ASB_ePub.xhtml#_idTextAnchor062)*,
    Understanding Docker Networking*, we explained ports and sockets. Since we have
    a single host, each control plane node and the HAProxy container are running on
    unique ports. Each container needs to be exposed to the host so that they can
    receive incoming requests. In this example, the important one to note is the port
    assigned to HAProxy, since that's the target port for the cluster. If you were
    to look at the Kubernetes config file, you would see that it is targeting [https://127.0.0.1:32791](https://127.0.0.1:32791),
    which is the port that's been assigned to the HAProxy container.
  prefs: []
  type: TYPE_NORMAL
- en: 'When a command is executed using **kubectl**, it is sent to directly to the
    HAProxy server. Using a configuration file that was created by KinD during the
    cluster''s creation, the HAProxy container knows how to route traffic between
    the three control plane nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: generated by kind
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: global
  prefs: []
  type: TYPE_NORMAL
- en: log /dev/log local0
  prefs: []
  type: TYPE_NORMAL
- en: log /dev/log local1 notice
  prefs: []
  type: TYPE_NORMAL
- en: daemon
  prefs: []
  type: TYPE_NORMAL
- en: defaults
  prefs: []
  type: TYPE_NORMAL
- en: log global
  prefs: []
  type: TYPE_NORMAL
- en: mode tcp
  prefs: []
  type: TYPE_NORMAL
- en: option dontlognull
  prefs: []
  type: TYPE_NORMAL
- en: 'TODO: tune these'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: timeout connect 5000
  prefs: []
  type: TYPE_NORMAL
- en: timeout client 50000
  prefs: []
  type: TYPE_NORMAL
- en: timeout server 50000
  prefs: []
  type: TYPE_NORMAL
- en: frontend control-plane
  prefs: []
  type: TYPE_NORMAL
- en: bind *:6443
  prefs: []
  type: TYPE_NORMAL
- en: default_backend kube-apiservers
  prefs: []
  type: TYPE_NORMAL
- en: backend kube-apiservers
  prefs: []
  type: TYPE_NORMAL
- en: option httpchk GET /healthz
  prefs: []
  type: TYPE_NORMAL
- en: 'TODO: we should be verifying (!)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: server config2-control-plane 172.17.0.8:6443 check check-ssl verify none
  prefs: []
  type: TYPE_NORMAL
- en: server config2-control-plane2 172.17.0.6:6443 check check-ssl verify none
  prefs: []
  type: TYPE_NORMAL
- en: server config2-control-plane3 172.17.0.5:6443 check check-ssl verify none
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding configuration file, there is a backend section called
    **kube-apiservers** that contains the three control plane containers. Each entry
    contains the Docker IP address of a control plane node with a port assignment
    of 6443, targeting the API server running in the container. When you request [https://127.0.0.1:32791](https://127.0.0.1:32791),
    that request will hit the HAProxy container. Using the rules in the HAProxy configuration
    file, the request will be routed to one of the three nodes in the list.
  prefs: []
  type: TYPE_NORMAL
- en: Since our cluster is now fronted by a load balancer, we have a highly available
    control plane for testing.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The included HAProxy image is not configurable. It is only provided to handle
    the control plane and to load balance the API servers. Due to this limitation,
    if you needed to use a load balancer for the worker nodes, you will need to provide
    your own.
  prefs: []
  type: TYPE_NORMAL
- en: An example use case for this would be if you wanted to use an Ingress controller
    on multiple worker nodes. You would need a load balancer in front of the worker
    nodes to accept incoming 80 and 443 requests that would forward the traffic to
    each node running NGINX. At the end of this chapter, we have provided an example
    configuration that includes a custom HAProxy configuration for load balancing
    traffic to the worker nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Customizing the control plane and Kubelet options
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You may want to go further than this to test features such as OIDC integration
    or Kubernetes feature gates. KinD uses the same configuration that you would use
    for a kubeadm installation. As an example, if you wanted to integrate a cluster
    with an OIDC provider, you could add the required options to the configuration
    patch section:'
  prefs: []
  type: TYPE_NORMAL
- en: 'kind: Cluster'
  prefs: []
  type: TYPE_NORMAL
- en: 'apiVersion: kind.x-k8s.io/v1alpha4'
  prefs: []
  type: TYPE_NORMAL
- en: 'kubeadmConfigPatches:'
  prefs: []
  type: TYPE_NORMAL
- en: '- |'
  prefs: []
  type: TYPE_NORMAL
- en: 'kind: ClusterConfiguration'
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: config'
  prefs: []
  type: TYPE_NORMAL
- en: 'apiServer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'extraArgs:'
  prefs: []
  type: TYPE_NORMAL
- en: 'oidc-issuer-url: "https://oidc.testdomain.com/auth/idp/k8sIdp"'
  prefs: []
  type: TYPE_NORMAL
- en: 'oidc-client-id: "kubernetes"'
  prefs: []
  type: TYPE_NORMAL
- en: 'oidc-username-claim: sub'
  prefs: []
  type: TYPE_NORMAL
- en: 'oidc-client-id: kubernetes'
  prefs: []
  type: TYPE_NORMAL
- en: 'oidc-ca-file: /etc/oidc/ca.crt'
  prefs: []
  type: TYPE_NORMAL
- en: 'nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '- role: control-plane'
  prefs: []
  type: TYPE_NORMAL
- en: '- role: control-plane'
  prefs: []
  type: TYPE_NORMAL
- en: '- role: control-plane'
  prefs: []
  type: TYPE_NORMAL
- en: '- role: worker'
  prefs: []
  type: TYPE_NORMAL
- en: '- role: worker'
  prefs: []
  type: TYPE_NORMAL
- en: '- rol: worker'
  prefs: []
  type: TYPE_NORMAL
- en: For a list of available configuration options, take a look at *Customizing control
    plane configuration with kubeadm* on the Kubernetes site at [https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/control-plane-flags/](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/control-plane-flags/).
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have created the cluster file, you can create your KinD cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a custom KinD cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally! Now that you are familiar with KinD, we can move forward and create
    our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: We need to create a controlled, known environment, so we will give the cluster
    a name and provide the config file that we discussed in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: Make sure that you are in your cloned repository under the **chapter4** directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a KinD cluster with our required options, we need to run the KinD
    installer with the following options:'
  prefs: []
  type: TYPE_NORMAL
- en: kind create cluster --name cluster01 --config c
  prefs: []
  type: TYPE_NORMAL
- en: luster01-kind.yamlThe option **--name** will set the name of the cluster to
    cluster01 and the **--config** tells the installer to use the config file **cluster01-kind.yaml**.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you execute the installer on your host, KinD will start the installation
    and tell you each step that is being performed. The entire cluster creation process
    should take less than 2 minutes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – KinD cluster creation output'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_4.6_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.6 – KinD cluster creation output
  prefs: []
  type: TYPE_NORMAL
- en: The final step in the deployment creates or edits an existing Kubernetes config
    file. In either case, the installer creates a new context with the name **kind-<cluster
    name>** and sets it as the default context.
  prefs: []
  type: TYPE_NORMAL
- en: While it may appear that the cluster installation procedure has completed its
    tasks, the cluster **is not** ready yet. Some of the tasks take a few minutes
    to fully initialize and since we disabled the default CNI to use Calico, we still
    need to deploy Calico to provide cluster networking.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Calico
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To provide networking to the pods in the cluster, we need to install a Container
    Network Interface, or CNI. We have elected to install Calico as our CNI and since
    KinD only includes the Kindnet CNI, we need to install Calico manually.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you were to pause after the creation step and look at the cluster, you would
    notice that some pods are in a pending state:'
  prefs: []
  type: TYPE_NORMAL
- en: coredns-6955765f44-86l77  0/1  Pending  0  10m
  prefs: []
  type: TYPE_NORMAL
- en: coredns-6955765f44-bznjl  0/1  Pending  0  10m
  prefs: []
  type: TYPE_NORMAL
- en: local-path-provisioner-7  0/1  Pending  0  11m 745554f7f-jgmxv
  prefs: []
  type: TYPE_NORMAL
- en: The pods listed here require a working CNI to start. This puts the pods into
    a pending state, where they are waiting for a network. Since we did not deploy
    the default CNI, our cluster does not have networking support. To get these pods
    from pending to running, we need to install a CNI – and for our cluster, that
    will be Calico.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install Calico, we will use the standard Calico deployment, which only requires
    a single manifest. To start deploying Calico, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: kubectl apply -f https://docs.projectcalico.org/v3.11/manifests/calico.yaml
  prefs: []
  type: TYPE_NORMAL
- en: 'This will pull the manifests from the internet and apply them to the cluster.
    As it deploys, you will see that that a number of Kubernetes objects are created:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – Calico installation output'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_4.7_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.7 – Calico installation output
  prefs: []
  type: TYPE_NORMAL
- en: 'The installation process will take about a minute and you can check on its
    status using **kubectl get pods -n kube-system**. You will see that three Calico
    pods were created. Two are **calico-node** pods, while the other is the **calico-kube-controller**
    pod:'
  prefs: []
  type: TYPE_NORMAL
- en: NAME                    READY STATUS RESTARTS AGE
  prefs: []
  type: TYPE_NORMAL
- en: calico-kube-controllers  1/1  Running    0    64s -5b644bc49c-nm5wn
  prefs: []
  type: TYPE_NORMAL
- en: calico-node-4dqnv        1/1  Running    0    64s
  prefs: []
  type: TYPE_NORMAL
- en: calico-node-vwbpf        1/1  Running    0    64s
  prefs: []
  type: TYPE_NORMAL
- en: 'If you check the two CoreDNS pods in the **kube-system** namespace again, you
    will notice that they have changed from the pending state, from before we installed
    Calico, to being in a running state:'
  prefs: []
  type: TYPE_NORMAL
- en: coredns-6955765f44-86l77   1/1  Running   0  18m
  prefs: []
  type: TYPE_NORMAL
- en: coredns-6955765f44-bznjl   1/1  Running   0  18m
  prefs: []
  type: TYPE_NORMAL
- en: Now that the cluster has a working CNI installed, any pods that were dependent
    on networking will be in a running state.
  prefs: []
  type: TYPE_NORMAL
- en: Installing an Ingress controller
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have a chapter dedicated to Ingress to explain all the technical details.
    Since we are deploying a cluster and we require Ingress for future chapters, we
    need to deploy an Ingress controller to show a complete cluster build. All these
    details will be explained in more detail in [*Chapter 6*](B15514_06_Final_ASB_ePub.xhtml#_idTextAnchor174),
    *Services, Load Balancing, and External DNS*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Installing the NGINX Ingress controller requires only two manifests, which
    we will pull from the internet to make the installation easy. To install the controller,
    execute the following two lines:'
  prefs: []
  type: TYPE_NORMAL
- en: kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.28.0/deploy/static/mandatory.yaml
  prefs: []
  type: TYPE_NORMAL
- en: kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.27.0/deploy/static/provider/baremetal/service-nodeport.yaml
  prefs: []
  type: TYPE_NORMAL
- en: 'The deployment will create a few Kubernetes objects that are required for Ingress
    in a namespace called **ingress-nginx**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – NGINX installation output'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_4.8_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.8 – NGINX installation output
  prefs: []
  type: TYPE_NORMAL
- en: 'We have one more step so that we have a fully functioning Ingress controller:
    we need to expose ports 80 and 443 to the running pod. This can be done by patching
    the deployment. Here, we have included the patch to patch the deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: kubectl patch deployments -n ingress-nginx nginx-ingress-controller -p '{"spec":{"template":{"spec":{"containers":[{"name":"nginx-ingress-controller","ports":[{"containerPort":80,"hostPort":80},{"containerPort":443,"hostPort":443}]}]}}}}'
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You now have a fully functioning, two-node Kubernetes cluster
    running Calico with an Ingress controller.
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing your KinD cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With a Kubernetes cluster now available, we have the ability to look at Kubernetes
    objects first-hand. This will help you understand the previous chapter, where
    we covered many of the base objects included in a Kubernetes cluster. In particular,
    we will discuss the storage objects that are included with your KinD cluster.
  prefs: []
  type: TYPE_NORMAL
- en: KinD storage objects
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Remember that KinD includes Rancher's auto-provisioner to provide automated
    persistent disk management for the cluster. In [*Chapter 5*](B15514_05_Final_ASB_ePub.xhtml#_idTextAnchor150),
    *Kubernetes Bootcamp*, we went over the storage-related objects, and now that
    we have a cluster with a storage system configured, we can explain them in greater
    detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is one object that the auto-provisioner does not require since it uses
    a base Kubernetes feature: it does not require a **CSIdriver**. Since the ability
    to use local host paths as PVCs is part of Kubernetes, we will not see any **CSIdriver**
    objects in our KinD cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first objects in our KinD cluster we will discuss are our **CSInodes**.
    In the bootcamp, we mentioned that this object was created to decouple any CSI
    objects from the base node object. Any node that can run a workload will have
    a **CSInode** object. On our KinD clusters, both nodes have a **CSInode** object.
    You can verify this by executing **kubectl get csinodes**:'
  prefs: []
  type: TYPE_NORMAL
- en: NAME                      CREATED AT
  prefs: []
  type: TYPE_NORMAL
- en: cluster01-control-plane   2020-03-27T15:18:19Z
  prefs: []
  type: TYPE_NORMAL
- en: cluster01-worker          2020-03-27T15:19:01Z
  prefs: []
  type: TYPE_NORMAL
- en: 'If we were to describe one of the nodes using **kubectl describe csinodes <node
    name>**, you would see the details of the object:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9 – CSInode describe'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_4.9_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.9 – CSInode describe
  prefs: []
  type: TYPE_NORMAL
- en: The main thing to point out is the **Spec** section of the output. This lists
    the details of any drivers that may be installed to support backend storage systems.
    Since we do not have a backend storage system, we do not require an additional
    driver on our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'To show an example of what a node would list, here is the output from a cluster
    that has two drivers installed, supporting two different vendor storage solutions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.10 – Multiple driver example'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_4.10_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.10 – Multiple driver example
  prefs: []
  type: TYPE_NORMAL
- en: If you look at the **spec.drivers** section of this node, you will see two different
    name sections. The first shows that we have a driver installed to support NetApp
    SolidFire, while the second is a driver that supports Reduxio's storage solution.
  prefs: []
  type: TYPE_NORMAL
- en: Storage drivers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we already mentioned, your KinD cluster does not have any additional storage
    drivers installed. If you execute **kubectl get csidrivers**, the API will not
    list any resources.
  prefs: []
  type: TYPE_NORMAL
- en: KinD storage classes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To attach to any cluster-provided storage, the cluster requires a **StorageClass**
    object. Rancher's provider creates a default storage class called standard. It
    also sets the class as the default **StorageClass**, so you do not need to provide
    a **StorageClass** name in your PVC requests. If a default **StorageClass** is
    not set, every PVC request will require a **StorageClass** name in the request.
    If a default class is not enabled and a PVC request fails to set a **StorageClass**
    name, the PVC allocation will fail since the API server won't be able to link
    the request to a **StorageClass**.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: On a production cluster, it is considered a good practice to omit assigning
    a default **StorageClass**. Depending on your users, you may have deployments
    that forget to set a class, and the default storage system may not fit the deployment
    needs. This issue may not occur until it becomes a production issue, and that
    may impact business revenue or the company's reputation. If you don't assign a
    default class, the developer will have a failed PVC request, and the issue will
    be discovered before any harm comes to the business.
  prefs: []
  type: TYPE_NORMAL
- en: 'To list the storage classes on the cluster, execute **kubectl get storageclasses**,
    or use the shortened version by using **sc** instead of **storageclasses**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11 – Default storage class'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_4.11_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.11 – Default storage class
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's learn how to use the provisioner.
  prefs: []
  type: TYPE_NORMAL
- en: Using KinD's storage provisioner
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using the included provisioner is very simple. Since it can auto-provision the
    storage and is set as the default class, any PVC requests that are coming in are
    seen by the provisioning pod, which then creates **PersistentVolume** and **PersistentVolumeClaim**.
  prefs: []
  type: TYPE_NORMAL
- en: 'To show this process, let''s go through the necessary steps. The following
    is the output of running **get pv** and **get pvc** on a base KinD cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12 – PV and PVC example'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_4.12_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.12 – PV and PVC example
  prefs: []
  type: TYPE_NORMAL
- en: Remember that **PersistentVolume** is not a namespaced object, so we don't need
    to add a namespace option to the command. PVCs are namespaced objects, so I told
    Kubernetes to show me the PVCs that are available in all the namespaces. Since
    this is a new cluster and none of the default workloads require persistent disk,
    there are no PV or PVC objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Without an auto-provisioner, we would need to create a PV before a PVC could
    claim the volume. Since we have the Rancher provisioner running in our cluster,
    we can test the creation process by deploying a pod with a PVC request like the
    one listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'kind: PersistentVolumeClaim'
  prefs: []
  type: TYPE_NORMAL
- en: 'apiVersion: v1'
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: test-claim'
  prefs: []
  type: TYPE_NORMAL
- en: 'spec:'
  prefs: []
  type: TYPE_NORMAL
- en: 'accessModes:'
  prefs: []
  type: TYPE_NORMAL
- en: '- ReadWriteOnce'
  prefs: []
  type: TYPE_NORMAL
- en: 'resources:'
  prefs: []
  type: TYPE_NORMAL
- en: 'requests:'
  prefs: []
  type: TYPE_NORMAL
- en: 'storage: 1Mi'
  prefs: []
  type: TYPE_NORMAL
- en: '---'
  prefs: []
  type: TYPE_NORMAL
- en: 'kind: Pod'
  prefs: []
  type: TYPE_NORMAL
- en: 'apiVersion: v1'
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: test-pvc-claim'
  prefs: []
  type: TYPE_NORMAL
- en: 'spec:'
  prefs: []
  type: TYPE_NORMAL
- en: 'containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '- name: test-pod'
  prefs: []
  type: TYPE_NORMAL
- en: 'image: busybox'
  prefs: []
  type: TYPE_NORMAL
- en: 'command:'
  prefs: []
  type: TYPE_NORMAL
- en: '- "/bin/sh"'
  prefs: []
  type: TYPE_NORMAL
- en: 'args:'
  prefs: []
  type: TYPE_NORMAL
- en: '- "-c"'
  prefs: []
  type: TYPE_NORMAL
- en: '- "touch /mnt/test && exit 0 || exit 1"'
  prefs: []
  type: TYPE_NORMAL
- en: 'volumeMounts:'
  prefs: []
  type: TYPE_NORMAL
- en: '- name: test-pvc'
  prefs: []
  type: TYPE_NORMAL
- en: 'mountPath: "/mnt"'
  prefs: []
  type: TYPE_NORMAL
- en: 'restartPolicy: "Never"'
  prefs: []
  type: TYPE_NORMAL
- en: 'volumes:'
  prefs: []
  type: TYPE_NORMAL
- en: '- name: test-pvc'
  prefs: []
  type: TYPE_NORMAL
- en: 'persistentVolumeClaim:'
  prefs: []
  type: TYPE_NORMAL
- en: 'claimName: test-claim'
  prefs: []
  type: TYPE_NORMAL
- en: This PVC request will be named **test-claim** in the default namespace and it
    is requesting a 1 MB volume. We do need to include the **StorageClass** option
    since KinD has set a default **StorageClass** for the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: To create the PVC, we can execute a **create** command using kubectl, such as
    **kubectl create -f pvctest.yaml** – Kubernetes will return, stating that the
    PVC has been created, but it's important to note that this does not mean that
    the PVC is fully working. The PVC object has been created, but if any dependencies
    are missing in the PVC request, it will still create the object, though it will
    fail to fully create the PVC request.
  prefs: []
  type: TYPE_NORMAL
- en: 'After creating a PVC, you can check the real status using one of two options.
    The first is a simple **get** command; that is, **kubectl get pvc**. Since my
    request is in the default namespace, I don''t need to include a namespace value
    in the **get** command (note that we had to shorten the volume''s name so that
    it fits the page):'
  prefs: []
  type: TYPE_NORMAL
- en: NAME         STATUS          VOLUME                                     CAPACITY   ACCESS
    MODES   STORAGECLASS   AGE
  prefs: []
  type: TYPE_NORMAL
- en: test-claim   Bound    pvc-9c56cf65-d661-49e3-         1Mi            RWO          standard     2s
  prefs: []
  type: TYPE_NORMAL
- en: 'We know that we created a PVC request in the manifest, but we did not create
    a PV request. If we look at the PVs now, we will see that a single PV was created
    from our PVC request. Again, we shortened the PV name in order to fit the output
    on a single line:'
  prefs: []
  type: TYPE_NORMAL
- en: NAME                   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM
  prefs: []
  type: TYPE_NORMAL
- en: pvc-9c56cf65-d661-49e3-   1Mi          RWO           Delete       Bound    default/test-claim
  prefs: []
  type: TYPE_NORMAL
- en: This completes the KinD storage section.
  prefs: []
  type: TYPE_NORMAL
- en: With so many workloads requiring persistent disks, it is very important to understand
    how Kubernetes workloads integrate with storage systems. In this section, you
    learned how KinD adds the auto-provisioner to the cluster. We will reinforce our
    knowledge of these Kubernetes storage objects in the next chapter, [*Chapter 5*](B15514_05_Final_ASB_ePub.xhtml#_idTextAnchor150)*,
    Kubernetes Bootcamp*.
  prefs: []
  type: TYPE_NORMAL
- en: Adding a custom load balancer for Ingress
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This section is a complex topic that covers adding a custom HAProxy container
    that you can use to load balance worker nodes in a KinD cluster. *You should not
    deploy these steps on the KinD cluster that we will use for the remaining chapters.*
  prefs: []
  type: TYPE_NORMAL
- en: We added this section for anybody that may want to know more about how to load
    balance between multiple worker nodes.
  prefs: []
  type: TYPE_NORMAL
- en: KinD does not include a load balancer for worker nodes. The included HAProxy
    container only creates a configuration file for the API server; the team does
    not officially support any modifications to the default image or configuration.
    Since you will interact with load balancers in your everyday work, we wanted to
    add a section on how to configure your own HAProxy container in order to load
    balance between three KinD nodes.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will not use this configuration for any of chapters in this book.
    We want to make the exercises available to everyone, so to limit the required
    resources, we will always use the two-node cluster that we created earlier in
    this chapter. If you want to test KinD nodes with a load balancer, we suggest
    using a different Docker host or waiting until you have finished this book and
    deleting your KinD cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Installation prerequisites
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We assume that you have a KinD cluster based on the following configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: Any number of control plane nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Three worker nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cluster name is **cluster01**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A working version of **Kindnet or Calico** (**CNI**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NGINX Ingress controller installed – patched to listen on ports 80 and 443 on
    the host
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating the KinD cluster configuration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since you will use an HAProxy container exposed on ports 80 and 443 on your
    Docker host, you do not need to expose any ports in your cluster **config** file.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make a test deployment easier, you can use the example cluster config shown
    here, which will create a simple six-node cluster with Kindnet disabled:'
  prefs: []
  type: TYPE_NORMAL
- en: 'kind: Cluster'
  prefs: []
  type: TYPE_NORMAL
- en: 'apiVersion: kind.x-k8s.io/v1alpha4'
  prefs: []
  type: TYPE_NORMAL
- en: 'networking:'
  prefs: []
  type: TYPE_NORMAL
- en: 'apiServerAddress: "0.0.0.0"'
  prefs: []
  type: TYPE_NORMAL
- en: 'disableDefaultCNI: true'
  prefs: []
  type: TYPE_NORMAL
- en: 'kubeadmConfigPatches:'
  prefs: []
  type: TYPE_NORMAL
- en: '- |'
  prefs: []
  type: TYPE_NORMAL
- en: 'apiVersion: kubeadm.k8s.io/v1beta2'
  prefs: []
  type: TYPE_NORMAL
- en: 'kind: ClusterConfiguration'
  prefs: []
  type: TYPE_NORMAL
- en: 'metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: 'name: config'
  prefs: []
  type: TYPE_NORMAL
- en: 'networking:'
  prefs: []
  type: TYPE_NORMAL
- en: 'serviceSubnet: "10.96.0.1/12"'
  prefs: []
  type: TYPE_NORMAL
- en: 'podSubnet: "192.168.0.0/16"'
  prefs: []
  type: TYPE_NORMAL
- en: 'nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '- role: control-plane'
  prefs: []
  type: TYPE_NORMAL
- en: '- role: control-plane'
  prefs: []
  type: TYPE_NORMAL
- en: '- role: control-plane'
  prefs: []
  type: TYPE_NORMAL
- en: '- role: worker'
  prefs: []
  type: TYPE_NORMAL
- en: '- role: worker'
  prefs: []
  type: TYPE_NORMAL
- en: '- role: worker'
  prefs: []
  type: TYPE_NORMAL
- en: You need to install Calico using the same manifest that we used earlier in this
    chapter. After installing Calico, you need to install the NGINX Ingress controller
    using the steps provided earlier in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Once you've deployed Calico and NGINX, you should have a working base cluster.
    Now, you can move on to deploying a custom HAProxy container.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a custom HAProxy container
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: HAProxy offers a container on Docker Hub that is easy to deploy, requiring only
    a config file to start the container.
  prefs: []
  type: TYPE_NORMAL
- en: To create the configuration file, you will need you to know the IP addresses
    of each worker node in the cluster. In this book's GitHub repository, we have
    included a script file that will find this information for you, create the config
    file, and start the HAProxy container. It is located under the **HAProxy** directory
    and it's called **HAProxy-ingress.sh**.
  prefs: []
  type: TYPE_NORMAL
- en: 'To help you better understand this script, we will break out sections of the
    script and detail what each section is executing. Firstly, the following code
    block is getting the IP addresses of each worker node in our cluster and saving
    the results in a variable. We will need this information for the backend server
    list:'
  prefs: []
  type: TYPE_NORMAL
- en: '#!/bin/bash'
  prefs: []
  type: TYPE_NORMAL
- en: worker1=$(docker inspect --format '{{ .NetworkSettings.IPAddress }}' cluster01-worker)
  prefs: []
  type: TYPE_NORMAL
- en: worker2=$(docker inspect --format '{{ .NetworkSettings.IPAddress }}' cluster01-worker2)
  prefs: []
  type: TYPE_NORMAL
- en: worker3=$(docker inspect --format '{{ .NetworkSettings.IPAddress }}' cluster01-worker3)
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, since we will use a bind mount when we start the container, we need to
    have the configuration file in a known location. We elected to store it in the
    current user''s home folder, under a directory called **HAProxy**:'
  prefs: []
  type: TYPE_NORMAL
- en: Create an HAProxy directory in the current users home folder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: mkdir ~/HAProxy
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, the following part of the script will create the **HAProxy** directory:'
  prefs: []
  type: TYPE_NORMAL
- en: Create the HAProxy.cfg file for the worker nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: tee ~/HAProxy/HAProxy.cfg <<EOF
  prefs: []
  type: TYPE_NORMAL
- en: 'The **global** section of the configuration sets process-wide security and
    performance settings:'
  prefs: []
  type: TYPE_NORMAL
- en: global
  prefs: []
  type: TYPE_NORMAL
- en: log /dev/log local0
  prefs: []
  type: TYPE_NORMAL
- en: log /dev/log local1 notice
  prefs: []
  type: TYPE_NORMAL
- en: daemon
  prefs: []
  type: TYPE_NORMAL
- en: 'The **defaults** section is used to configure values that will apply to all
    frontend and backend sections in the configuration value:'
  prefs: []
  type: TYPE_NORMAL
- en: defaults
  prefs: []
  type: TYPE_NORMAL
- en: log global
  prefs: []
  type: TYPE_NORMAL
- en: mode tcp
  prefs: []
  type: TYPE_NORMAL
- en: timeout connect 5000
  prefs: []
  type: TYPE_NORMAL
- en: timeout client 50000
  prefs: []
  type: TYPE_NORMAL
- en: timeout server 50000
  prefs: []
  type: TYPE_NORMAL
- en: frontend workers_https
  prefs: []
  type: TYPE_NORMAL
- en: bind *:443
  prefs: []
  type: TYPE_NORMAL
- en: mode tcp
  prefs: []
  type: TYPE_NORMAL
- en: use_backend ingress_https
  prefs: []
  type: TYPE_NORMAL
- en: backend ingress_https
  prefs: []
  type: TYPE_NORMAL
- en: option httpchk GET /healthz
  prefs: []
  type: TYPE_NORMAL
- en: mode tcp
  prefs: []
  type: TYPE_NORMAL
- en: server worker $worker1:443 check port 80
  prefs: []
  type: TYPE_NORMAL
- en: server worker2 $worker2:443 check port 80
  prefs: []
  type: TYPE_NORMAL
- en: server worker3 $worker3:443 check port 80
  prefs: []
  type: TYPE_NORMAL
- en: This tells HAProxy to create a frontend called **workers_https** and the IP
    addresses and ports to bind for incoming requests, to use TCP mode, and to use
    a backend named **ingress_https**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **ingress_https** backend includes the three worker nodes that are using
    port 443 as a destination. The check port is a health check that will test port
    80\. If a server replies on port 80, it will be added as a target for requests.
    While this is an HTTPS port 443 rule, we are only using port 80 to check for a
    network reply from the NGINX pod:'
  prefs: []
  type: TYPE_NORMAL
- en: frontend workers_http
  prefs: []
  type: TYPE_NORMAL
- en: bind *:80
  prefs: []
  type: TYPE_NORMAL
- en: use_backend ingress_http
  prefs: []
  type: TYPE_NORMAL
- en: backend ingress_http
  prefs: []
  type: TYPE_NORMAL
- en: mode http
  prefs: []
  type: TYPE_NORMAL
- en: option httpchk GET /healthz
  prefs: []
  type: TYPE_NORMAL
- en: server worker $worker1:80 check port 80
  prefs: []
  type: TYPE_NORMAL
- en: server worker2 $worker2:80 check port 80
  prefs: []
  type: TYPE_NORMAL
- en: server worker3 $worker3:80 check port 80
  prefs: []
  type: TYPE_NORMAL
- en: 'This **frontend** section creates a frontend that accepts incoming HTTP traffic
    on port 80\. It then uses the list of servers in the backend, named **ingress_http**,
    for endpoints. Just like in the HTTPS section, we are using port 80 to check for
    any nodes that are running a service on port 80\. Any endpoint that replies to
    the check will be added as a destination for HTTP traffic, and any nodes that
    do not have NGINX running on them will not reply, which means they won''t be added
    as destinations:'
  prefs: []
  type: TYPE_NORMAL
- en: EOF
  prefs: []
  type: TYPE_NORMAL
- en: 'This ends the creation of our file. The final file will be created in the **HAProxy**
    directory:'
  prefs: []
  type: TYPE_NORMAL
- en: Start the HAProxy Container for the Worker Nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: docker run --name HAProxy-workers-lb -d -p 80:80 -p 443:443 -v ~/HAProxy:/usr/local/etc/HAProxy:ro
    HAProxy -f /usr/local/etc/HAProxy/HAProxy.cfg
  prefs: []
  type: TYPE_NORMAL
- en: The final step is to start a Docker container running HAProxy with our created
    configuration file containing the three worker nodes, exposed on the Docker host
    on ports 80 and 443.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have learned how to install a custom HAProxy load balancer for
    your worker nodes, let's look at how the configuration works.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding HAProxy traffic flow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The cluster will have a total of eight containers running. Six of these containers
    will be the standard Kubernetes components; that is, three control plane servers
    and three worker nodes. The other two containers are KinD''s HAProxy server, and
    your own custom HAProxy container:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.13 – Custom HAProxy container running'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_4.13_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.13 – Custom HAProxy container running
  prefs: []
  type: TYPE_NORMAL
- en: There are a few differences between this cluster output versus our two-node
    cluster for the exercises. Notice that the worker nodes are not exposed on any
    host ports. The worker nodes do not need any mappings since we have our new HAProxy
    server running. If you look at the HAProxy container we created, it is exposed
    on host ports 80 and 443\. This means that any incoming requests to the host on
    port 80 or 443 will be directed to the custom HAProxy container.
  prefs: []
  type: TYPE_NORMAL
- en: 'The default NGINX deployment only has a single replica, which means that the
    Ingress controller is running on a single node. If we look at the logs for the
    HAProxy container, we will see something interesting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[NOTICE] 093/191701 (1) : New worker #1 (6) forked'
  prefs: []
  type: TYPE_NORMAL
- en: '[WARNING] 093/191701 (6) : Server ingress_https/worker is DOWN, reason: Layer4
    connection problem, info: "SSL handshake failure (Connection refused)", check
    duration: 0ms. 2 active and 0 backup servers left. 0 sessions active, 0 requeued,
    0 remaining in queue.'
  prefs: []
  type: TYPE_NORMAL
- en: '[WARNING] 093/191702 (6) : Server ingress_https/worker3 is DOWN, reason: Layer4
    connection problem, info: "SSL handshake failure (Connection refused)", check
    duration: 0ms. 1 active and 0 backup servers left. 0 sessions active, 0 requeued,
    0 remaining in queue.'
  prefs: []
  type: TYPE_NORMAL
- en: '[WARNING] 093/191702 (6) : Server ingress_http/worker is DOWN, reason: Layer4
    connection problem, info: "Connection refused", check duration: 0ms. 2 active
    and 0 backup servers left. 0 sessions active, 0 requeued, 0 remaining in queue.'
  prefs: []
  type: TYPE_NORMAL
- en: '[WARNING] 093/191703 (6) : Server ingress_http/worker3 is DOWN, reason: Layer4
    connection problem, info: "Connection refused", check duration: 0ms. 1 active
    and 0 backup servers left. 0 sessions active, 0 requeued, 0 remaining in queue.'
  prefs: []
  type: TYPE_NORMAL
- en: You may have noticed a few errors in the log, such as SSL handshake failure
    and **Connection refused**. While these do look like errors, they are actually
    failed checked events on the worker nodes. Remember that NGINX is only running
    in a single pod, and since we have all three nodes in our HAProxy backend configuration,
    it will check for the ports on each node. Any nodes that fail to reply will not
    be used to load balance traffic. In our current config, this does load balance,
    since we only have NGINX on one node. It does, however, provide high availability
    to the Ingress controller.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you look carefully at the log output, you will see how many servers are
    active on a defined backend; for example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'check duration: 0ms. 1 active and 0 backup servers left.'
  prefs: []
  type: TYPE_NORMAL
- en: Each server pool in the log output shows 1 active endpoint, so we know that
    the HAProxy has successfully found a NGINX controller on both port 80 and 443.
  prefs: []
  type: TYPE_NORMAL
- en: 'To find out what worker the HAProxy server has connected to, we can use the
    failed connections in the log. Each backend will list the failed connections.
    For example, we know that the node that is working is **cluster01-worker2** based
    on the logs that the other two worker nodes show as **DOWN**:'
  prefs: []
  type: TYPE_NORMAL
- en: Server ingress_https/worker is DOWN Server ingress_https/worker3 is DOWN
  prefs: []
  type: TYPE_NORMAL
- en: Let's simulate a node failure to prove that HAProxy is providing high availability
    to NGINX.
  prefs: []
  type: TYPE_NORMAL
- en: Simulating a Kubelet failure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Remember that KinD nodes are ephemeral and that stopping any container may cause
    it to fail on restart. So, how can we simulate a worker node failure since we
    can't simply stop the container?
  prefs: []
  type: TYPE_NORMAL
- en: To simulate a failure, we can stop the kubelet service on a node, which will
    alert **kube-apisever** so that it doesn't schedule any additional pods on the
    node. In our example, we want to prove that HAProxy is providing HA support for
    NGINX. We know that the running container is on **worker2**, so that's the node
    we want to "take down."
  prefs: []
  type: TYPE_NORMAL
- en: 'The easiest way to stop **kubelet** is to send a **docker exec** command to
    the container:'
  prefs: []
  type: TYPE_NORMAL
- en: docker exec cluster01-worker2 systemctl stop kubelet
  prefs: []
  type: TYPE_NORMAL
- en: 'You will not see any output from this command, but if you wait a few minutes
    for the cluster to receive the updated node status, you can verify the node is
    down by looking at a list of nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: kubectl get nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will receive the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.14 – worker2 is in a NotReady state'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Fig_4.14_B15514.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.14 – worker2 is in a NotReady state
  prefs: []
  type: TYPE_NORMAL
- en: This verifies that we just simulated a kubelet failure and that **worker2**
    is in a **NotReady** status.
  prefs: []
  type: TYPE_NORMAL
- en: Any pods that were running before the kubelet "failure" will continue to run,
    but **kube-scheduler** will not schedule any workloads on the node until the kubelet
    issue is resolved. Since we know the pod will not restart on the node, we can
    delete the pod so that it can be rescheduled on a different node.
  prefs: []
  type: TYPE_NORMAL
- en: 'You need to get the pod name and then delete it to force a restart:'
  prefs: []
  type: TYPE_NORMAL
- en: kubectl get pods -n ingress-nginx
  prefs: []
  type: TYPE_NORMAL
- en: nginx-ingress-controller-7d6bf88c86-r7ztq
  prefs: []
  type: TYPE_NORMAL
- en: kubectl delete pod nginx-ingress-controller-7d6bf88c86-r7ztq -n ingress-nginx
  prefs: []
  type: TYPE_NORMAL
- en: This will force the scheduler to start the container on another worker node.
    It will also cause the HAProxy container to update the backend list, since the
    NGINX controller has moved to another worker node.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you look at the HAProxy logs again, you will see that HAProxy has updated
    the backends to include **cluster01-worker3** and that it removed **cluster01-worker2**
    from the active servers list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[WARNING] 093/194006 (6) : Server ingress_https/worker3 is UP, reason: Layer7
    check passed, code: 200, info: "OK", check duration: 4ms. 2 active and 0 backup
    servers online. 0 sessions requeued, 0 total in queue.'
  prefs: []
  type: TYPE_NORMAL
- en: '[WARNING] 093/194008 (6) : Server ingress_http/worker3 is UP, reason: Layer7
    check passed, code: 200, info: "OK", check duration: 0ms. 2 active and 0 backup
    servers online. 0 sessions requeued, 0 total in queue.'
  prefs: []
  type: TYPE_NORMAL
- en: '[WARNING] 093/195130 (6) : Server ingress_http/worker2 is DOWN, reason: Layer4
    timeout, check duration: 2000ms. 1 active and 0 backup servers left. 0 sessions
    active, 0 requeued, 0 remaining in queue.'
  prefs: []
  type: TYPE_NORMAL
- en: '[WARNING] 093/195131 (6) : Server ingress_https/worker2 is DOWN, reason: Layer4
    timeout, check duration: 2001ms. 1 active and 0 backup servers left. 0 sessions
    active, 0 requeued, 0 remaining in queue.'
  prefs: []
  type: TYPE_NORMAL
- en: If you plan to use this HA cluster for additional tests, you will want to restart
    the kubelet on **cluster01-worker2**. If you plan to delete the HA cluster, you
    can just run a KinD cluster delete and all the nodes will be deleted.
  prefs: []
  type: TYPE_NORMAL
- en: Deleting the HAProxy container
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once you have deleted your KinD cluster, you will need to manually remove the
    HAProxy container we added. Since KinD didn't create our custom load balancer,
    deleting the cluster will not remove the container.
  prefs: []
  type: TYPE_NORMAL
- en: 'To delete the custom HAProxy container, run the **docker rm** command to force
    remove the image:'
  prefs: []
  type: TYPE_NORMAL
- en: docker rm HAProxy-workers-lb –force
  prefs: []
  type: TYPE_NORMAL
- en: This will stop the container and remove it from Docker's list, allowing you
    to run it again using the same name with a future KinD cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about the Kubernetes SIG project called KinD.We
    went into details on how to install optional components in a KinD cluster, including
    Calico as the CNI and NGINX as the Ingress controller. Finally, we covered the
    details of the Kubernetes storage objects that are included with a KinD cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully, with the help of this chapter, you now understand the power that
    using KinD can bring to you and your organization. It offers an easy to deploy,
    fully configurable Kubernetes cluster. The number of running clusters on a single
    host is theoretically limited only by the host resources.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will dive into Kubernetes objects. We've called the
    next chapter *Kubernetes Bootcamp* since it will cover the majority of the base
    Kubernetes objects and what each one is used for. The next chapter can be considered
    a "Kubernetes pocket guide." It contains a quick reference to Kubernetes objects
    and what they do, as well as when to use them.
  prefs: []
  type: TYPE_NORMAL
- en: It's a packed chapter and is designed to be a refresher for those of you who
    have experience with Kubernetes, or as a crash course for those of you who are
    new to Kubernetes. Our intention for this book is to go beyond the base Kubernetes
    objects since there are many books on the market today that cover the basics of
    Kubernetes very well.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What object must be created before you can create a **PersistentVolumeClaim**?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. PVC
  prefs: []
  type: TYPE_NORMAL
- en: B. Disk
  prefs: []
  type: TYPE_NORMAL
- en: C. **PersistentVolume**
  prefs: []
  type: TYPE_NORMAL
- en: D. **VirtualDisk**
  prefs: []
  type: TYPE_NORMAL
- en: KinD includes a dynamic disk provisioner. What company created the provisioner?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. Microsoft
  prefs: []
  type: TYPE_NORMAL
- en: B. CNCF
  prefs: []
  type: TYPE_NORMAL
- en: C. VMware
  prefs: []
  type: TYPE_NORMAL
- en: D. Rancher
  prefs: []
  type: TYPE_NORMAL
- en: If you create a KinD cluster with multiple worker nodes, what would you install
    to direct traffic to each node?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. Load balancer
  prefs: []
  type: TYPE_NORMAL
- en: B. Proxy server
  prefs: []
  type: TYPE_NORMAL
- en: C. Nothing
  prefs: []
  type: TYPE_NORMAL
- en: D. Network load balancer
  prefs: []
  type: TYPE_NORMAL
- en: 'True or false: A Kubernetes cluster can only have one CSIdriver installed.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. True
  prefs: []
  type: TYPE_NORMAL
- en: B. False
  prefs: []
  type: TYPE_NORMAL
