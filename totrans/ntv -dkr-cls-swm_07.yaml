- en: Chapter 7. Scaling Up Your Platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we're going to extend what we saw in [Chapter 6](ch06.html
    "Chapter 6. Deploy Real Applications on Swarm"), *Deploy Real Applications on
    Swarm*. Our goal is to deploy a realistic production-grade Spark cluster on top
    of Swarm, add storage capacity, launch some Spark jobs and setup monitoring for
    the underlying infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: In order to do that, this chapter is mostly infrastructure-oriented. In fact,
    we'll see how to coalesce **Libnetwork**, **Flocker**, and **Prometheus** with
    Swarm.
  prefs: []
  type: TYPE_NORMAL
- en: For network, we'll use the basic Docker Network overlay system, based on Libnetwork.
    There are a few great networking plugins out there, such as Weave and others,
    but either they are not compatible with the new Docker Swarm Mode yet, or they
    are made obsolete by Swarm-integrated routing mesh mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: 'For storage, the situation is more prosperous, because there is much more choice
    (refer to [https://docs.docker.com/engine/extend/plugins/](https://docs.docker.com/engine/extend/plugins/)).
    We''ll go with Flocker. Flocker is the *grandfather* of Docker storage, and can
    be configured with a vast plethora of storage backends, making it one of the best
    choices for production loads. Scared by Flocker complexity? Unjustified: We''ll
    see how to set up a multiple nodes Flocker cluster for any usage, in minutes.'
  prefs: []
  type: TYPE_NORMAL
- en: For monitoring, finally, we'll introduce Prometheus. It's the most promising
    among the monitoring systems available for Docker nowadays, and its APIs may be
    integrated into the Docker engine very soon.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, what we''ll cover here:'
  prefs: []
  type: TYPE_NORMAL
- en: A Spark example over Swarm, ready for running any Spark job
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automate the installation of Flocker for infrastructures at a scale
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Demonstrate how to use Flocker locally
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use Flocker with Swarm Mode
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scale our Spark app
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitor the health of this infrastructure with Prometheus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Spark example, again
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We're going to rearchitect the example of [Chapter 6](ch06.html "Chapter 6. Deploy
    Real Applications on Swarm"), *Deploy Real Applications on Swarm*, so we'll deploy
    Spark on Swarm, but this time with a realistic networking and storage setup.
  prefs: []
  type: TYPE_NORMAL
- en: Spark storage backend usually runs on Hadoop, or on NFS when on filesystem.
    For jobs not requiring storage, Spark will create local data on workers, but for
    storage computations, you will need a shared filesystem on each node, which cannot
    be guaranteed automatically by Docker volume plugins (at least, so far).
  prefs: []
  type: TYPE_NORMAL
- en: A possibility to achieve that goal on Swarm is to create NFS shares on each
    Docker host, and then mount them transparently inside service containers.
  prefs: []
  type: TYPE_NORMAL
- en: Our focus here is not to illustrate Spark job details and their storage organization,
    but to introduce an opinionated storage option for Docker and give an idea of
    how to organize and scale a fairly-complex service on Docker Swarm.
  prefs: []
  type: TYPE_NORMAL
- en: Docker plugins
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For a detailed introduction on Docker plugins, we can suggest to read the official
    documentation pages. Here is a starting point [https://docs.docker.com/engine/extend/](https://docs.docker.com/engine/extend/)and,
    also, Docker will probably release a tool to get plugins with a single command,
    refer to [https://docs.docker.com/engine/reference/commandline/plugin_install/](https://docs.docker.com/engine/reference/commandline/plugin_install/).
  prefs: []
  type: TYPE_NORMAL
- en: We recommend you to refer to *Extending Docker* book, Packt, if you want to
    explore how to integrate new features into Docker. The book emphasis is on Docker
    plugins, volume plugins, network plugins, and how to create your own plugins.
  prefs: []
  type: TYPE_NORMAL
- en: For Flocker, **ClusterHQ** made available an automated deployment mechanism
    to deploy a Flocker cluster on AWS with **CloudForm** templates, which you can
    install using the **Volume Hub**. For registering and starting such a cluster,
    go to [https://flocker-docs.clusterhq.com/en/latest/docker-integration/cloudformation.html](https://flocker-docs.clusterhq.com/en/latest/docker-integration/cloudformation.html).
    For a step-by-step explanation of the detailed procedure, refer to Chapter 3 of *Extending
    Docker*, Packt.
  prefs: []
  type: TYPE_NORMAL
- en: Here we'll go manually, because we must integrate Flocker and Swarm.
  prefs: []
  type: TYPE_NORMAL
- en: The lab
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this tutorial, we'll create the infrastructure on AWS. Ideally, for a production
    environment, you would setup three or five Swarm managers and some workers, and
    eventually add new worker nodes later depending on the load.
  prefs: []
  type: TYPE_NORMAL
- en: Here we'll setup a Swarm cluster with three Swarm managers, six Swarm workers
    and one Flocker control node with Machine, and won't add new workers.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Flocker requires several manual steps, which can be automated (as
    we'll see). So, to make the example as less complex as possible, we'll run all
    these commands initially, in linear order, without repeating procedures to increase
    the system capacity.
  prefs: []
  type: TYPE_NORMAL
- en: If you don't like Ansible, you can easily adapt the flow to your favorite tool,
    be it **Puppet**, **Salt**, **Chef** or others.
  prefs: []
  type: TYPE_NORMAL
- en: A unique key
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For simplicity, we will install our lab using an SSH key generated ad hoc, and
    we'll install Docker Machines with this key copied to the host in `authorized_keys`.
    The goal is to have a unique key to authenticate Ansible later, that we'll use
    to automate the many steps that we should otherwise perform manually.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we start by generating a `flocker` key and we''ll put it into the `keys/`
    directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Docker Machine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To provision our Docker hosts, we''ll go with Docker Machine. These are the
    system details for this tutorial:'
  prefs: []
  type: TYPE_NORMAL
- en: 'AWS instances will be called from aws-101 to aws-110\. This standardized naming
    will be important later when we''ll need to generate and create node certificates
    for Flocker:'
  prefs: []
  type: TYPE_NORMAL
- en: Nodes aws-101, 102, 103 will our Swarm managers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Node aws-104 will be the Flocker control node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nodes from aws-105 to aws-110 will be our Swarm workers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The instance type will be `t2.medium` (2 vCPUs, 4G memory, EBS storage)
  prefs: []
  type: TYPE_NORMAL
- en: The flavor will be Ubuntu 14.04 Trusty (specified with the `--amazonec2-ami`
    parameter)
  prefs: []
  type: TYPE_NORMAL
- en: The security group will be the standard `docker-machine` (we'll summarize the
    requirements again in a few seconds)
  prefs: []
  type: TYPE_NORMAL
- en: The Flocker version will be 1.15.
  prefs: []
  type: TYPE_NORMAL
- en: The exact AMI ID to use can be searched on [https://cloud-images.ubuntu.com/locator/ec2/](https://cloud-images.ubuntu.com/locator/ec2/).
  prefs: []
  type: TYPE_NORMAL
- en: The AWS calculator computes this setup's cost to roughly 380$ monthly, storage
    usage excluded.
  prefs: []
  type: TYPE_NORMAL
- en: '![Docker Machine](images/image_07_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'So, we create the infrastructure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: and running.
  prefs: []
  type: TYPE_NORMAL
- en: After some time, we'll have it up and running.
  prefs: []
  type: TYPE_NORMAL
- en: Security groups
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Additionally, we''ll need open three additional new ports in the security Group
    used for this project (`docker-machine`) in the EC2 console. There are ports used
    by Flocker services:'
  prefs: []
  type: TYPE_NORMAL
- en: Port `4523/tcp`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Port `4524/tcp`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Also, the following is a port used by Swarm:'
  prefs: []
  type: TYPE_NORMAL
- en: Port `2377/tcp`![Security groups](images/image_07_003.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Networking configuration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We use a standard configuration with an additional overlay network, called **Spark**.
    Traffic data will pass through the spark network, making it possible to extend
    the lab configuration with new hosts and workers running even on other providers,
    such as **DigitalOcean** or **OpenStack**. When new Swarm workers join this cluster,
    this network is propagated to them and made available for Swarm services.
  prefs: []
  type: TYPE_NORMAL
- en: Storage configuration and architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As mentioned, we chose Flocker ([https://clusterhq.com/flocker/introduction/](https://clusterhq.com/flocker/introduction/)),
    which is among the top Docker storage projects. ClusterHQ describes it as:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Flocker is an open-source container data volume manager for your Dockerized
    applications. By providing tools for data migrations, Flocker gives the ops teams
    the tools they need to run containerized stateful services such as databases in
    production. Unlike a Docker data volume that is tied to a single server, a Flocker
    data volume, called a dataset, is portable and can be used with any container
    in your cluster.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Flocker supports a very wide set of storage options, from AWS EBS to EMC, NetApp,
    Dell, Huawei solutions, to OpenStack Cinder and Ceph, just to mention some.
  prefs: []
  type: TYPE_NORMAL
- en: 'Its design is straightforward: Flocker has a **control node**, which exposes
    its service APIs to manage the Flocker cluster and Flocker volumes, and a **Flocker
    Agent** alongside with the Docker plugin runs on each **node** of the cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Storage configuration and architecture](images/image_07_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'To use Flocker, at the command line, you would need to run something like this
    with Docker to read or write stateful data on a Flocker `myvolume` volume mounted
    as `/data` inside the container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, you can manage volume with the `docker volume` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In this tutorial architecture, we'll install the Flocker control node on aws-104,
    that will be hence dedicated, and flocker agents on all nodes (node-104 included).
  prefs: []
  type: TYPE_NORMAL
- en: Also, we'll install the Flocker client that used to interact with the Flocker
    control node APIs in order to manage the cluster status and volumes. For our convenience,
    we'll also use it from aws-104.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Flocker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A series of operations are necessary to get a running Flocker cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: Install the `flocker-ca` utility to generate certificates.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate the authority certificate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate the control node certificate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate the node certificates, one per node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate the flocker plugin certificate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate the client certificate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install some software from packages.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Distribute certificates to the Flocker cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configure the installation, adding the main configuration file, `agent.yml`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configure the packet filter on hosts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start and restart system services.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can execute them manually on a small cluster, but they are repetitive and
    tedious, so we'll illustrate the procedure using some self-explanatory Ansible
    playbooks published to [https://github.com/fsoppelsa/ansible-flocker](https://github.com/fsoppelsa/ansible-flocker).
  prefs: []
  type: TYPE_NORMAL
- en: 'These plays are trivial and probably not production ready. There are also the
    official ClusterHQ playbooks for Flocker roles (refer to [https://github.com/ClusterHQ/ansible-role-flocker](https://github.com/ClusterHQ/ansible-role-flocker)),
    but for the linearity of the explanation, we''ll use the first repository, so
    let''s clone it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Generating Flocker certificates
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For certificate generation, the `flocker-ca` utility is required. Instructions
    on how to install it are available at [https://docs.clusterhq.com/en/latest/flocker-standalone/install-client.html](https://docs.clusterhq.com/en/latest/flocker-standalone/install-client.html).
    For Linux distributions, it's a matter of installing a package. On Mac OS X, instead,
    the tool can be pulled using Python's `pip` utility.
  prefs: []
  type: TYPE_NORMAL
- en: '**On Ubuntu**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '**On Mac OS X**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Once in possess of this tool, we generate the required certificates. To make
    the things simple, we''ll create the following certificate structure:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A directory `certs/` including all certificates and keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cluster.crt` and `.key` are the authority certificate and key'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`control-service.crt` and `.key` are the control node certificate and key'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`plugin.crt` and `.key` are the Docker Flocker plugin certificate and key'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`client.crt` and `.key` are the Flocker client certificate and key'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From `node-aws-101.crt` and `.key` to `node-aws-110.crt` and `.key` are the
    node certificates and keys, one per node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate the authority certificate: `flocker-ca initialize cluster`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once in possess of the authority certificate and key, generate the control
    node certificate in the same directory: `flocker-ca create-control-certificate
    aws-101`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then generate the plugin certificate: `flocker-ca create-api-certificate plugin`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then generate the client certificate: `flocker-ca create-api-certificate client`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, generate each nodes'' certificate: `flocker-ca create-node-certificate
    node-aws-X`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Of course, we must cheat and use the `utility/generate_certs.sh` script available
    in the `ansible-flocker` repository, which will do the work for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'After this script execution, we now have all our certificates available in
    `certs/`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Generating Flocker certificates](images/image_07_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Installing software
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'On each Flocker node, we must perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Add the ClusterHQ Ubuntu repository to the APT source list.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the packages cache.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Install these packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`clusterhq-python-flocker`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clusterhq-flocker-node`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clusterhq-flocker-docker-plugin`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a directory `/etc/flocker`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy the Flocker configuration file `agent.yml` to `/etc/flocker`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy the certificates appropriate for that node to `/etc/flocker`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configure security by enabling **ufw**, and opening TCP ports `2376`, `2377`,
    `4523`, `4524`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start the system services.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Restart the docker daemon.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once again, we love the machines to work for us, so let's setup this with Ansible
    while we have a coffee.
  prefs: []
  type: TYPE_NORMAL
- en: 'But, before, we must specify who will be the Flocker control node and who the
    bare nodes, so we fill in the `inventory` file with the host IPs of nodes. The
    file is in `.ini` format, and what''s required is just to specify the list of
    nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Installing software](images/image_07_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, we create the directory from where Ansible will take files, certificates,
    and configurations to copy to the nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We now copy all our certificates we created previously, from the `certs/` directory
    to `files/`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we define the Flocker configuration file in `files/agent.yml` with
    the following content, adapting the AWS region and modifying `hostname`, `access_key_id`,and
    `secret_access_key`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This is the core Flocker configuration file, which will be in `/etc/flocker`
    on every node. Here, you specify and configure the credentials of the backend
    of choice. In our case, we go with the basic AWS option, EBS, so we include our
    AWS credentials.
  prefs: []
  type: TYPE_NORMAL
- en: With inventory, `agent.yml` and all certificates ready in `files/`, we can proceed.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the control node
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The playbook to install the control node is `flocker_control_install.yml`.
    This play executes a software installation script, copies the cluster certificate,
    the control node certificate and key, the node certificate and key, the client
    certificate and key, the plugin certificate and key, configures the firewall opening
    ports for SSH, Docker and Flocker, and starts these system services:'
  prefs: []
  type: TYPE_NORMAL
- en: '`flocker-control`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`flocker-dataset-agent`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`flocker-container-agent`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`flocker-docker-plugin`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, it refreshes the `docker` service, restarting it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s run it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Installing the cluster nodes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Similarly, we install the other nodes with another playbook, `flocker_nodes_install.yml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The steps are more or less the same as before, except that this playbook doesn't
    copy some certificates and doesn't start the `flocker-control` service. Only the
    Flocker agent and Flocker Docker plugin services run there. We wait for some time
    until Ansible exits.
  prefs: []
  type: TYPE_NORMAL
- en: '![Installing the cluster nodes](images/image_07_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Testing whether everything is up and running
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To check that Flocker is installed correctly, we now log in to the control
    node, check that the Flocker plugin is running (alas, it has the `.sock` file),
    and then we install the `flockerctl` utility (refer to [https://docs.clusterhq.com/en/latest/flocker-features/flockerctl.html](https://docs.clusterhq.com/en/latest/flocker-features/flockerctl.html))
    with the `curl` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We now set some environment variables used by `flockerctl:`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now list the nodes and volumes (we still have no volumes yet, of course):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![Testing whether everything is up and running](images/image_07_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we can go to another node of the cluster to check the connectivity of
    the Flocker cluster (especially if the plugin and the agent can reach and authenticate
    to the control node), say `aws-108`, create a volume and write some data into
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![Testing whether everything is up and running](images/image_07_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If we go back to the control node, `aws-104`, we can verify that volumes with
    persistent data got created by listing them with the docker and `flockerctl` commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![Testing whether everything is up and running](images/image_07_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Excellent! So now we can remove the exited containers, delete the test volume
    dataset from Flocker, and then we are ready to install a Swarm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Installing and configuring Swarm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can now install a Swarm with our favorite method, as shown in the previous
    chapters. We'll have **aws-101** to **aws-103** as managers, and the rest of nodes
    except **aws-104**, workers. This cluster can be expanded even further. For practical
    things, we'll keep it at 10-nodes size.
  prefs: []
  type: TYPE_NORMAL
- en: '![Installing and configuring Swarm](images/image_07_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We now add a dedicated `spark` overlay VxLAN network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: A volume for Spark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We now connect to any Docker host and create a `75G` sized volume to be used
    to save some persistent Spark data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The option to discuss here is `profile`. This is a sort of flavor of storage
    (speed, mostly). As explained in the link [https://docs.clusterhq.com/en/latest/flocker-features/aws-configuration.html#aws-dataset-backend](https://docs.clusterhq.com/en/latest/flocker-features/aws-configuration.html#aws-dataset-backend),
    ClusterHQ maintains three available profiles for AWS EBS:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Gold**: EBS Provisioned IOPS / API named io1\. Configured for maximum IOPS
    for its size - 30 IOPS/GB, with a maximum of 20,000 IOPS'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Silver**: EBS General Purpose SSD / API named gp2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bronze**: EBS Magnetic / API named standard'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can check on the Flocker control node whether this volume was generated,
    with `flockerctl list`.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Spark, again
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We choose a host where we want to run the Spark standalone manager, be `aws-105`,
    and tag it as such:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Other nodes will host our Spark workers.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start the Spark master on `aws-105`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: First, the image. I discovered that there are some annoying things included
    into the Google images (such as unsetting some environment variables, so making
    a configuration from external, with `--env` switches, impossible). Thus, I created
    myself a pair of Spark 1.6.2 master and worker images.
  prefs: []
  type: TYPE_NORMAL
- en: Then,  `--network`. Here we say to this container to attach to the user-defined
    overlay network called spark.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, storage: `--mount`, which works with Docker volumes. We specify it
    to:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Work with a volume: `type=volume`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mount the volume inside the container on `/data`: `target=/data`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Use the `spark` volume that we created previously: `source=spark`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use Flocker as a `volume-driver`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you create a service and mount a certain volume, if volume does not exist,
    it will get created.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The current releases of Flocker only support replicas of 1\. The reason being
    that iSCSI/block level mounts cannot be attached across multiple nodes. So only
    one service can use a volume at a given point of time with replica factor of 1\.
    This makes Flocker more useful for storing and moving database data (which is
    what it's used for, especially). But here we'll use it to show a tiny example
    with persistent data in `/data` in the Spark master container.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, with this configuration, let''s add the workhorses, three Spark workers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Here, we pass some environment variables into the container, to limit resources
    usage to 1 core and 1G of memory per container.
  prefs: []
  type: TYPE_NORMAL
- en: 'After some minutes, this system is up, we connect to `aws-105`, port `8080`
    and see this page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deploying Spark, again](images/image_07_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Testing Spark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So, we access the Spark shell and run a Spark task to check if things are up
    and running.
  prefs: []
  type: TYPE_NORMAL
- en: 'We prepare a container with some Spark utilities, for example, `fsoppelsa/spark-worker`,
    and run it to compute the value of Pi using the Spark binary `run-example`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'After a ton of output messages, Spark finishes the computation giving us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: If we go back to the Spark UI, we can see that our amazing Pi application was
    successfully completed.
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing Spark](images/image_07_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'More interesting is running an interactive Scala shell connecting to the master
    to execute Spark jobs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '![Testing Spark](images/image_07_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Using Flocker storage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Only for the purpose of this tutorial, we now run an example using the spark
    volume we created previously to read and write some persistent data from Spark.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to do that and because of Flocker limitation of the replica factor,
    we kill the current set of three workers and create a set of only one, mounting
    spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We now gain the Docker credentials of host `aws-105` with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: We can try to write some data in `/data` by connecting to the Spark master container.
    In this example, we just save some text data (The content of lorem ipsum, available
    for example at [http://www.loremipsum.net](http://www.loremipsum.net/) ) to `/data/file.txt`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '![Using Flocker storage](images/image_07_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, we connect to the Spark shell to execute a simple Spark job:'
  prefs: []
  type: TYPE_NORMAL
- en: Load `file.txt`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Map the words it contains to the number of their occurrences.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Save the result in `/data/output`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '![Using Flocker storage](images/image_07_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s start a `busybox` container on any Spark node and check the content
    of the `spark` volume, verifying that the output was written. We run the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '![Using Flocker storage](images/image_07_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The preceding screenshot shows the output, as expected. The interesting thing
    about Flocker volume is that they can be even moved from one host to another.
    A number of operations can be done in a reliable way. Flocker is a good idea if
    one is looking for a good storage solution for Docker. For example, it's used
    in production by the Swisscom Developer cloud ([http://developer.swisscom.com/](http://developer.swisscom.com/)),
    which lets you provision databases such as **MongoDB** backed by Flocker technology.
    Upcoming releases of Flocker will aim at slimming down the Flocker codebase and
    making it more lean and durable. Items such as built in HA, snapshotting, certificate
    distribution, and easily deployable agents in containers are some of things that
    are up next. So, a bright future!
  prefs: []
  type: TYPE_NORMAL
- en: Scaling Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we illustrate the most amazing feature of Swarm Mode--the `scale` command.
    We restore the configuration we had before trying Flocker, so we destroy the `spark-worker`
    service and re-create it with a replica factor of `3`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we scale up the service with `30` Spark workers using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'After some minutes, necessary to eventually pull the image, we check once again:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Scaling Spark](images/image_07_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'From the Spark web UI:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Scaling Spark](images/image_07_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Scale can be used to scale up or down the size of the replicas. So far, still
    there are no automated mechanisms for auto-scaling or for distributing the load
    to newly added nodes. But they can be implemented with custom utilities, or we
    may even expect them to be integrated into Swarm soon day.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring Swarm hosting apps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I (Fabrizio) was following a thread on Reddit ([https://www.reddit.com/r/docker/comments/4zous1/monitoring_containers_under_112_swarm/](https://www.reddit.com/r/docker/comments/4zous1/monitoring_containers_under_112_swarm/))
    in August 2016, where users complained that the new Swarm Mode is harder to monitor.
  prefs: []
  type: TYPE_NORMAL
- en: 'If, for now, there are no official Swarm monitoring solutions, one of the most
    popular combinations of emerging technologies is: Google''s **cAdvisor** to collect
    data, **Grafana** to show graphs, and **Prometheus** as the data model.'
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The team at Prometheus describes the product as:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Prometheus is an open-source systems monitoring and alerting toolkit originally
    built at SoundCloud.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Prometheus''s main features are:'
  prefs: []
  type: TYPE_NORMAL
- en: Multi-dimensional data model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A flexible query language
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No reliance on distributed storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time series collection happens via a pull model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pushing time series is supported via a gateway
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiple modes of graphing and dashboarding support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a great presentation on [https://prometheus.io/docs/introduction/overview/](https://prometheus.io/docs/introduction/overview/) that
    we will not repeat here. The top feature of Prometheus is, in our opinion, the
    ease of installation and usage. Prometheus itself consists of just a single binary
    built from Go code, plus a configuration file.
  prefs: []
  type: TYPE_NORMAL
- en: Installing a monitoring system
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Things are probably going to change very soon, so we just sketch a way to set
    up a monitoring system for Swarm, tried on Docker version 1.12.3.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create a new overlay network to not interfere with the `ingress`
    or `spark` networks, called `monitoring`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we start a cAdvisor service in mode `global`, meaning that a cAdvisor
    container will run on each Swarm node. We mount some system paths inside the container
    so that they can be accessed by cAdvisor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we use `basi/prometheus-swarm` to set up Prometheus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'And we add the `node-exporter` service (again `global`, must run on each node):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we start **Grafana** with one replica:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Importing Prometheus in Grafana
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When Grafana is available, to get impressive graphs of the Swarm health, we
    login with these credentials on the node where Grafana runs, port `3000`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'As admins, we click on the Grafana logo, go to **Data Sources**, and add `Prometheus`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Importing Prometheus in Grafana](images/image_07_020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Some options will appear, but the mapping is already present, so it''s sufficient
    to **Save & Test**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Importing Prometheus in Grafana](images/image_07_021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we can go back to the Dashboard and click on **Prometheus**, so we will
    be presented the Grafana main panel:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Importing Prometheus in Grafana](images/image_07_022.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Once again, we took advantage of what the open source community released, and
    glued different opinionated technologies with just some simple commands, to get
    the desired result. Monitoring Docker Swarm and its applications is a field of
    research that is completely open now, so we can expect an amazing evolution there
    too.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we added storage capacity to a Swarm infrastructure using Flocker,
    and set a dedicated overlay network to make our example app, a Spark cluster,
    to work on it and be easily extendible by adding new nodes (also on new providers,
    such as DigitalOcean). After using our Spark installation and Flocker, we finally
    introduced Prometheus and Grafana to monitor the Swarm health and status. We will
    see new additional features that can be plugged into Swarm and how to secure a
    Swarm infrastructure in the next two chapters.
  prefs: []
  type: TYPE_NORMAL
