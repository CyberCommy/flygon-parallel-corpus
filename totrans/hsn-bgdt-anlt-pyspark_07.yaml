- en: Transformations and Actions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transformations and actions are the main building blocks of an Apache Spark
    program. In this chapter, we will look at Spark transformations to defer computations
    and then look at which transformations should be avoided. We will then use the `reduce`
    and `reduceByKey` methods to carry out calculations from a dataset. We will then
    perform actions that trigger actual computations on graphs. By the end of this
    chapter, we will also have learned how to reuse the same `rdd` for different actions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Using Spark transformations to defer computations to a later time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoiding transformations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the `reduce` and `reduceByKey` methods to calculate the result
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing actions that trigger actual computations of our **Directed Acyclic
    Graph** (**DAG**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reusing the same `rdd` for different actions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Spark transformations to defer computations to a later time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's first understand Spark DAG creation. We will be executing DAG by issuing
    the action and also deferring the decision about starting the job until the last
    possible moment to check what this possibility gives us.
  prefs: []
  type: TYPE_NORMAL
- en: Let's have a look at the code we will be using in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to initialize Spark. Every test we carry out will be the same.
    We need to initialize it before we start using it, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will have the actual test. Here, `test` is called `should defer computation`. It
    is simple but shows a very powerful abstraction of Spark. We start by creating
    an `rdd` of `InputRecord`, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '`InputRecord` is a case class that has a unique identifier, which is an optional
    argument.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It can be a random `uuid` if we are not supplying it and the required argument,
    that is, `userId`. The `InputRecord` will be used throughout this book for testing
    purposes. We have created two records of the `InputRecord` that we will apply
    a transformation on, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We will only filter records that have `A` in the `userId` field. We will then
    transform it to the `keyBy(_.userId)` and then extract `userId` from value and
    map it `toLowerCase`. This is our `rdd`. So, here, we have only created DAG, which
    we have not executed yet. Let's assume that we have a complex program and we are
    creating a lot of those acyclic graphs before the actual logic.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pros of Spark are that this is not executed until action is issued, but
    we can have some conditional logic. For example, we can get a fast path execution.
    Let''s assume that we have `shouldExecutePartOfCode()`, which can check a configuration
    switch, or go to the rest service to calculate if the `rdd` calculation is still
    relevant, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We have used simple methods for testing purposes that we are just returning
    `true` for, but, in real life, this could be complex logic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: After it returns `true`, we can decide if we want to execute the DAG or not.
    If we want to, we can call `rdd.collect().toList` or `saveAsTextFile` to execute
    the `rdd`. Otherwise, we can have a fast path and decide that we are no longer
    interested in the input `rdd`. By doing this, only the graph will be created.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we start the test, it will take some time to complete and return the following
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We can see that our test passed and we can conclude that it worked as expected.
    Now, let's look at some transformations that should be avoided.
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will look at the transformations that should be avoided.
    Here, we will focus on one particular transformation.
  prefs: []
  type: TYPE_NORMAL
- en: We will start by understanding the `groupBy` API. Then, we will investigate
    data partitioning when using `groupBy`, and then we will look at what a skew partition
    is and why should we avoid skew partitions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we are creating a list of transactions. `UserTransaction` is another
    model class that includes `userId` and `amount`. The following code block shows
    a typical transaction where we are creating a list of five transactions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We have created four transactions for `userId = "A"`, and one for `userId =
    "B"`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s consider that we want to coalesce transactions for a specific `userId`
    to have the list of transactions. We have an `input` that we are grouping by `userId`,
    as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'For every `x` element, we will create a tuple. The first element of a tuple
    is an ID, while the second element is an iterator of every transaction for that
    specific ID. We will transform it into a list using `toList`. Then, we will collect
    everything and assign it to `toList` to have our result. Let''s assert the result. `rdd`
    should contain the same element as `B`, that is, the key and one transaction,
    and `A`, which has four transactions, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s start this test and check if this behaves as expected. We get the following
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: At first glance, it has passed and it works as expected. But the question arises
    as to why we want to group it. We want to group it to save it to the filesystem
    or do some further operations, such as concatenating all the amounts.
  prefs: []
  type: TYPE_NORMAL
- en: We can see that our input is not a normal distribution, since almost all the
    transactions are for the `userId = "A"`. Because of that, we have a key that is
    skewed. This means that one key has the majority of the data in it and that the other
    keys have a lower amount of data. When we use `groupBy` in Spark, it takes all
    the elements that have the same grouping, which in this example is `userId`, and
    sends those values to exactly the same executors.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if our executors have 5 GB of memory and we have a really big dataset
    that has hundreds of gigabytes and one key has 90 percent of data, it means that everything
    will go to one executor and the rest of the executors will take a minority of
    the data. So, the data will not be normally distributed and, because of the non-uniform
    distribution, processing will not be as efficient as possible.
  prefs: []
  type: TYPE_NORMAL
- en: So, when we use the `groupBy` key, we must first answer the question of why
    we want to group it. Maybe we can filter it or aggregate it at the lower level
    before `groupBy` and then we will only group the results, or maybe we don't group
    at all. We will be investigating how to solve that problem with Spark API in the
    following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Using the reduce and reduceByKey methods to calculate the results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will use the `reduce` and `reduceBykey` functions to calculate
    our results and understand the behavior of `reduce`. We will then compare the `reduce`
    and `reduceBykey` functions to check which of the functions should be used in
    a particular use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will first focus on the `reduce` API. First, we need to create an input
    of `UserTransaction`. We have the user transaction `A` with amount `10`, `B` with
    amount `1`, and `A` with amount `101`. Let''s say that we want to find out the
    global maximum. We are not interested in the data for the specific key, but in
    the global data. We want to scan it, take the maximum, and return it, as shown
    in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'So, this is the reduced use case. Now, let''s see how we can implement it,
    as per the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: For the `input`, we need to first map the field that we're interested in. In
    this case, we are interested in `amount`. We will take `amount` and then take
    the maximum value.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding code example, `reduce` has two parameters, `a` and `b`. One
    parameter will be the current maximum in the specific Lambda that we are passing,
    and the second one will be our actual value, which we are investigating now. If
    the value was higher than the maximum state until now, we will return `a`; if
    not, it will return `b`. We will go through all the elements and, at the end,
    the result will be just one long number.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s test this and check whether the result is indeed `101`, as shown
    in the following code output. This means our test passed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s consider a different situation. We want to find the maximum transaction
    amount, but this time we want to do it according to users. We not only want to
    find out the maximum transaction for user `A` but also for user `B`, but we want
    those things to be independent. So, for every key that is the same, we want to
    take only the maximum value from our data, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'To achieve this, `reduce` is not a good choice because it will go through all
    of the values and give us the global maximum. We have key operations in Spark
    but, first, we want to do it for a specific group of elements. We need to use `keyBy`
    to tell Spark which ID should be taken as the unique one and it will execute the `reduce`
    function only within the specific key. So, we use `keyBy(_.userId)` and then we
    get the `reducedByKey` function. The `reduceByKey` function is similar to `reduce`
    but it works key-wise so, inside the Lambda, we''ll only get values for a specific
    key, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: By doing this, we get the first transaction and then the second one. The first
    one will be a current maximum and the second one will be the transaction that
    we are investigating right now. We will create a helper function that is taking
    those transactions and call it `higherTransactionAmount`.
  prefs: []
  type: TYPE_NORMAL
- en: The `higherTransactionAmount` function is used in taking the `firstTransaction`
    and `secondTransaction`. Please note that for the `UserTransaction` type, we need
    to pass that type. It also needs to return `UserTransaction` that we cannot return
    a different type.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are using the `reduceByKey` method from Spark, we need to return the
    same type as that of the `input` arguments. If `firstTransaction.amount` is higher
    than `secondTransaction.amount`, we will just return the `firstTransaction` since
    we are returning the `secondTransaction`, so transaction objects not the total
    amount. This is shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will collect, add, and test the transaction. After our test, we have
    the output where, for the key `B`, we should get transaction `("B", 1)` and, for
    the key `A`, transaction `("A", 101)`. There will be no transaction `("A", 10)` because
    we filtered it out, but we can see that for every key, we are able to find out
    the maximums. This is shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the test passed and everything is as expected, as shown in
    the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will perform actions that trigger the computations of
    our data.
  prefs: []
  type: TYPE_NORMAL
- en: Performing actions that trigger computations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark has a lot more actions that issue DAG, and we should be aware of all of
    them because they are very important. In this section, we'll understand what can
    be an action in Spark, do a walk-through of actions, and test those actions if
    they behave as expected.
  prefs: []
  type: TYPE_NORMAL
- en: The first action we covered is `collect`. We also covered two actions besides
    that—we covered both `reduce` and `reduceByKey` in the previous section. Both
    methods are actions because they return a single result.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will create the `input` of our transactions and then apply some transformations
    just for testing purposes. We will take only the user that contains `A`, using
    `keyBy_.userId`, and then take only the amount of the required transaction, as
    shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The first action that we are already aware of is `rdd.collect().toList`. The
    next one is `count()`, which needs to take all of the values and calculate how
    many values are inside the `rdd`. There is no way to execute `count()` without
    triggering the transformation. Also, there are different methods in Spark, such
    as `countApprox`, `countApproxDistinct`, `countByValue`, and `countByValueApprox`.
    The following example shows us the code for `rdd.collect().toList`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'If we have a huge dataset and the approximate counter is enough, you can use `countApprox`
    as it will be a lot faster. We then use `rdd.first()`, but this option is a bit
    different because it only needs to take the first element. Sometimes, if you want
    to take the first element and execute everything inside our DAG, we need to be
    focus on that and check it in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Also, on the `rdd`, we have `foreach()`, which is a for loop to which we can
    pass any function. A Scala function or a Java function is assumed to be Lambda,
    but to execute elements of our result `rdd`, DAG needs to be calculated because from
    here onwards, it is an action. Another variant of the `foreach()` method is `foreachPartition()`,
    which takes every partition and returns an iterator for the partition. Inside
    that, we have an iterator to carry our iterations again and then print our elements.
    We also have our `max()` and `min()` methods and, as expected, `max()` is taking
    the maximum value and `min()` is taking the minimum value. But these methods are
    taking the implicit ordering.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we have an `rdd` of a simple primitive type, like `Long`, we don''t need
    to pass it here. But if we do not use `map()`, we need to define the ordering
    for the `UserTransaction` for Spark to find out which element is `max` and which
    element is `min`. These two things need to execute the DAG and so they are classed
    as actions, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We then have `takeOrdered()`, which is a more time-consuming operation than `first()`
    because `first()` takes a random element. `takeOrdered()` needs to execute DAG
    and sort everything. When everything is sorted, only then does it take the top
    element.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, we are taking `num = 1`. But sometimes, for testing or monitoring
    purposes, we need to take only the sample of the data. To take a sample, we use
    the `takeSample()` method and pass a number of elements, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s start the test and see the output of implementing the previous
    actions, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The first action returns all values. The second actions return `4` as a count.
    We will consider the first element, `1001`, but this is a random value and it
    is not ordered. We will then print all the elements in the loop, as shown in the following
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: We then get `max` and `min` values like `1001` and `1`, which are similar to
    `first()`. After that, we get an ordered list, `List(1)`, and sample `List(100,
    1``)`, which is random. So, in the sample, we get random values from input data
    and applied transformations.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn how to reuse the `rdd` for different actions.
  prefs: []
  type: TYPE_NORMAL
- en: Reusing the same rdd for different actions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will reuse the same `rdd` for different actions. First,
    we will minimize the execution time by reusing the `rdd`. We will then look at
    caching and a performance test for our code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example is the test from the preceding section but a bit modified,
    as here we take `start` by `currentTimeMillis()` and the `result`. So, we are
    just measuring the `result` of all actions that are executed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'If someone doesn''t know Spark very well, they will assume that all actions
    are cleverly executed. We know that every action count means that we are going
    up to the `rdd` in the chain, which means we are going to all transformations
    to load data. In the production system, loading data will be from an external
    PI system such as HDFS. This means that every action causes the call to the filesystem,
    which retrieves all data and then applies transformations, as shown in the following
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a very expensive operation as every action is very costly. When we
    start this test, we can see that the time taken without caching will take `632`
    milliseconds, as shown in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s compare this with the caching use. Our test, at first glance, looks
    very similar, but this is not the same because you are issuing `cache()` and we
    are returning `rdd`. So, `rdd` will be already cached and every subsequent call
    to the `rdd` will go through the `cache`, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The first action will execute DAG, save the data into our cache, and then the
    subsequent actions will just retrieve the specific things according to the method
    that was called from memory. There will be no HDFS lookup, so let''s start this
    test, as per the following example, and see how long it takes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The first output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The second output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Without cache, the value is `585` milliseconds and with cache, the value is
    `336`. The difference is not much as we are just creating data in tests. However,
    in real production systems, this will be a big difference because we need to look
    up data from external filesystems.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, let's sum up this chapter. Firstly, we used Spark transformations to defer
    computation to a later time, and then we learned which transformations should
    be avoided. Next, we looked at how to use `reduceByKey` and `reduce` to calculate
    our result globally and per specific key. After that, we performed actions that
    triggered computations then learned that every action means a call to the loading
    data. To alleviate that problem, we learned how to reduce the same `rdd` for different
    actions.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll be looking at the immutable design of the Spark engine.
  prefs: []
  type: TYPE_NORMAL
