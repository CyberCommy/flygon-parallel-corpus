- en: Exploring Windows Forensic Artifacts Recipes - Part I
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following recipes will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: One man's trash is a forensic examiner's treasure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A sticky situation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading the registry
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gathering user activity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The missing link
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Searching high and low
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Windows has long reigned supreme as the operating system of choice in the PC
    market. In fact, Windows makes up approximately 47 percent of the users visiting
    government websites, with the second most popular PC operating system, macOS,
    making up only 8.5 percentage. There is no reason to suspect that this will be
    changing anytime soon, especially with the warm reception that Windows 10 has
    received. Therefore, it is exceedingly likely that future investigations will
    continue to require the analysis of Windows artifacts.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers many types of artifacts and how to interpret them with
    Python, using various first and third-party libraries, directly from forensic
    evidence containers. We will leverage the framework we developed in [Chapter 8](part0241.html#75QNI0-260f9401d2714cb9ab693c4692308abe),
    *Working with Forensic Evidence Container Recipes* to process these artifacts
    directly from forensic acquisitions. In this manner, we can provide captured raw
    or EWF images to our code and not worry about the process of extracting the required
    files or mounting the image prior to processing the data. Specifically, we will
    cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting `$I` files to learn more about files sent to the Recycle Bin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading content and metadata from Sticky Notes on Window 7 systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting values from the registry to learn about the operating system version
    and other configuration details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Revealing user activity related to searches, typed paths, and run commands
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parsing LNK files to learn about historical and recent file access
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examining `Windows.edb` for information about indexed files, folders, and messages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To view more interesting metrics, visit [https://analytics.usa.gov/](https://analytics.usa.gov/).
  prefs: []
  type: TYPE_NORMAL
- en: Visit [www.packtpub.com/books/content/support](http://www.packtpub.com/books/content/support)
    to download the code bundle for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: One man's trash is a forensic examiner's treasure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recipe difficulty: Medium'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python version: 2.7'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operating system: Linux'
  prefs: []
  type: TYPE_NORMAL
- en: While that may not be the exact saying, forensic examination of deleted files
    residing in the Recycle Bin is an important step in most investigations. The non-technical
    custodian likely does not understand that these files sent to the Recycle Bin
    are still present and that we can learn a good deal about the original file, such
    as its original file path and the time that it was sent to the Recycle Bin. While
    the specific artifacts vary between versions of Windows, this recipe focuses on
    the Windows 7 version of the Recycle Bin's `$I` and `$R` files.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This recipe requires the installation of three third-party modules to function:
    `pytsk3`, `pyewf`, and `unicodecsv`. *Refer to [Chapter 8](part0241.html#75QNI0-260f9401d2714cb9ab693c4692308abe),
    Working with Forensic Evidence Container Recipes* for a detailed explanation of
    installing the `pytsk3` and `pyewf` modules*.* All other libraries used in this
    script are present in Python''s standard library'
  prefs: []
  type: TYPE_NORMAL
- en: 'Because we are developing these recipes in Python 2.x, we are likely to encounter
    Unicode encode and decode errors. To account for that, we use the `unicodecsv`
    library to write all CSV output in this chapter. This third-party module takes
    care of Unicode support, unlike Python 2.x''s standard `csv` module, and will
    be put to great use here. As usual, we can use `pip` to install `unicodecsv`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: To learn more about the `unicodecsv` library, visit [https://github.com/jdunck/python-unicodecsv](https://github.com/jdunck/python-unicodecsv).
  prefs: []
  type: TYPE_NORMAL
- en: In addition to these, we'll continue to use the `pytskutil` module developed
    from [Chapter 8](https://cdp.packtpub.com/python_digital_forensics_cookbook/wp-admin/post.php?post=260&action=edit#post_218),
    *Working with Forensic Evidence Container recipes**,* to allow interaction with
    forensic acquisitions. This module is largely similar to what we previously wrote,
    with some minor changes to better suit our purposes. You can review the code by
    navigating to the utility directory within the code package.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To parse the `$I` and `$R` files from a Windows 7 machine, we will need to:'
  prefs: []
  type: TYPE_NORMAL
- en: Recurse through the `$Recycle.bin` folder in the evidence file, selecting all
    files starting with `$I`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read the contents of the files and parse the available metadata structures.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Search for the associated `$R` file and check if it is a file or folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write the results into a CSV file for review.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We import the `argparse`, `datetime`, `os`, and `struct` built-in libraries
    to assist with running the script and interpreting the binary data within these
    files. We also bring in our Sleuth Kit utilities for handling the evidence files,
    reading the content, and iterating through folders and files. Lastly, we import
    the `unicodecsv` library to assist with writing the CSV report:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This recipe's command-line handler takes three positional arguments, `EVIDENCE_FILE`,
    `IMAGE_TYPE`, and `CSV_REPORT`, which represent the path to the evidence file,
    the type of evidence file, and the desired output path to the CSV report, respectively.
    These three arguments are passed to the `main()` function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The `main()` function handles the necessary interactions with the evidence
    file to identify and provide any `$I` files for processing. To access the evidence
    file, one must provide the path to the container and the image type. This initiates
    the `TSKUtil` instance, which we use to search for files and folders within the
    image. To find the `$I` files, we call the `recurse_files()` method on the `tsk_util`
    instance, specifying the file name pattern to look for, the `path` to start the
    search in, and the string `logic` used to find the filename. The `logic` keyword
    argument accepts the following values which correspond to string operations: `startswith`,
    `endswith`, `contains`, and `equals`. These dictate the string operation used
    to search for our `$I` pattern within the scanned file and folder names.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If any `$I` files are found, we pass this list to the `process_dollar_i()`
    function along with the `tsk_util` object. After they are all processed, we write
    the extracted metadata to a CSV report with the `write_csv()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The `process_dollar_i()` function accepts as its input, the `tsk_util` object
    and the list of discovered `$I` files. We iterate through this list and inspect
    each of these files. Each element within the `dollar_i_files` list is itself a
    list of tuples, where each tuple element contains (in order) the file''s name,
    relative path, handle to access the file''s content, and filesystem identifier.
    With these available attributes, we will call our `read_dollar_i()` function and
    provide it the third tuple, the file object handle. If this is a valid `$I` file,
    this method returns a dictionary of extracted metadata from the raw file, otherwise,
    it returns `None`. If the file is valid, we continue processing it by adding the
    file path to the `$I` file to the `file_attribs` dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we search for the associated `$R` file within the image. In preparation
    of this, we join base path to the `$I` file (including the `$Recycle.bin` and
    the `SID` folders) to reduce the amount of time required to search for the corresponding
    `$R` file. On Windows 7, the `$I` and `$R` files have a similar file name, where
    the first two letters are `$I` and `$R`, respectively, followed by a shared identifier.
    By using that identifier in our search and specifying the specific folder we expect
    to find the `$R` file, we have reduced the likelihood of false positives. Using
    these patterns, we query our evidence file again with the `startswith` logic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'If the search for the `$R` files is unsuccessful, we try to query for a directory
    with the same information. If this query is also unsuccessful, we append dictionary
    values that the `$R` file was not found and that we are unsure if it was a file
    or directory. If, however, we do find a matching directory, we log the path of
    the directory and set the `is_directory` attribute to `True`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: If the search for the `$R` file returned one or more hits, we create a list
    of the matched files, using list comprehension, to store in the CSV, delimited
    by semicolons, and mark the `is_directory` attribute as `False`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Prior to exiting the loop, we append the `file_attribs` dictionary to the `processed_files`
    list which stores all `$I` processed dictionaries. This list of dictionaries is
    returned to the `main()` function where it is used in the reporting process.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Let's briefly look at the `read_dollar_i()` method, used to parse metadata from
    the binary file with `struct`. We start by checking the file header, using the
    Sleuth Kit's `read_random()` method to read the signature's first eight bytes.
    If the signature does not match, we return `None` to alert that the `$I` failed
    validation and is an invalid file format.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'If we detect a valid file, we continue to read and unpack values from the `$I`
    file. The first is the file size attribute, which is located at byte offset `8`
    and is `8` bytes long. We unpack this with `struct` and store the integer in a
    temporary variable. The next attribute, deletion time, is stored at byte offset
    `16` and `8` bytes long. This is a Windows `FILETIME` object and we will borrow
    some old code to later process it into a human-readable timestamp. The last attribute
    is the former file path, which we read from byte `24` to the end of the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'With these values extracted, we interpret the integers into human-readable
    values. We use the `sizeof_fmt()` function to convert the file size integer into
    a human-readable size, containing size prefixes such as MB or GB. Next, we interpret
    the timestamp using the logic from our date parsing recipe from [Chapter 7](part0212.html#6A5N80-260f9401d2714cb9ab693c4692308abe),
    *Log-Based Artifact Recipes* (after adapting the function to work only with integers).
    Lastly, we decode the path as UTF-16 and remove null-byte values. These refined
    details are then returned as a dictionary to the calling function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Our `sizeof_fmt()` function is borrowed from [StackOverflow.com](https://stackoverflow.com/),
    a site filled with many solutions to programming problems. While we could have
    drafted our own, this code is well formed for our purpose. It takes the integer
    `num` and iterates through the listed unit suffixes. If the number is less than
    `1024`, the number, unit, and suffix are joined into a string and returned; otherwise,
    the number is divided by `1024` and run through the next iteration. If the number
    is larger than a zettabyte, it returns the information in terms of yottabytes.
    For your sake, we hope the number is never that large.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Our next support function is `parse_windows_filetime()`, adapted from the previous
    date-parsing recipe in [Chapter 7](part0212.html#6A5N80-260f9401d2714cb9ab693c4692308abe),
    *Log-Based Artifact Recipes*. We borrow the logic and condense the code to only
    interpret integers into a formatted date that is then returned to the calling
    function. Generic functions, like the two we just discussed, are handy to keep
    in your arsenal as you never know when you may need this logic.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we are ready to write the processed results to a CSV file. As you
    have no doubt come to expect, this function is similar to all of our other CSV
    functions. The only difference is that it is using the `unicodecsv` library under
    the hood, though the method and function names used here are the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following two screenshots, we can see examples of the type of data this
    recipe extracts from `$I` and `$R` files:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00096.jpeg)![](../images/00097.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: A sticky situation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recipe difficulty: Medium'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python version: 2.7'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operating system: Linux'
  prefs: []
  type: TYPE_NORMAL
- en: Computers have replaced pen and paper. We have transferred many processes and
    habits, one relegated solely to the confines of paper, to these machines, including
    taking notes and making lists. One feature that replicates a real-world habit
    is Windows Sticky Notes. These sticky notes allow persistent notes to float on
    the desktop, with options for color, fonts, and more. This recipe will allow us
    to explore these sticky notes and add them to our investigative workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This recipe requires the installation of four third-party modules to function:
    `olefile`, `pytsk3`, `pyewf`, and `unicodecsv`. Refer to [Chapter 8](part0241.html#75QNI0-260f9401d2714cb9ab693c4692308abe),
    *Working with Forensic Evidence Container* *Recipes* for a detailed explanation
    of installing the `pytsk3` and `pyewf` modules. Likewise, refer to the *Getting
    started* section in the *One man''s trash is a forensic examiner''s treasure*
    recipe for details on installing `unicodecsv`. All other libraries used in this
    script are present in Python''s standard library.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Windows Sticky Note file is stored as an `OLE` file. Therefore, we will
    leverage the `olefile` library to interact with and extract data from Windows
    Sticky Notes. The `olefile` library can be installed with `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: To learn more about the `olefile` library, visit [https://olefile.readthedocs.io/en/latest/index.html](https://olefile.readthedocs.io/en/latest/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To properly form this recipe, we need to take the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the evidence file and find all `StickyNote.snt` files across the user profiles.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Parse metadata and content from the OLE streams.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write the RTF content to files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a CSV report of the metadata.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This script, like the others, begins with import statements of the libraries
    required for its execution. The two new libraries here are `olefile` which, as
    we discussed, parses the Windows Sticky Note OLE streams and `StringIO`, a built-in
    library used to interpret a string of data as a file-like object. This library
    will be used to convert the `pytsk` file object into a stream the `olefile` library
    can interpret:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We specify a global variable, `REPORT_COLS`, which represent the report columns.
    These static columns will be used across several functions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This recipe''s command-line handler takes three positional arguments, `EVIDENCE_FILE`,
    `IMAGE_TYPE`, and `REPORT_FOLDER`, which represent the path to the evidence file,
    the type of evidence file, and the desired output directory path, respectively.
    This is similar to the previous recipe, with the exception of the `REPORT_FOLDER`,
    which is a directory that we will write the Sticky Note RTF files to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Our main function starts similarly to the last, by handling the evidence file
    and searching for the files we seek to parse. In this case, we are looking for
    the `StickyNotes.snt` file, which is found within each user''s `AppData` directory.
    For this reason, we limit the search to the `/Users` folder and look for a file
    matching the exact name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We then iterate through the resulting files, splitting out the user's home directory
    name and setting up the file-like object required for processing by the `olefile`
    library. Next, we call the `parse_snt_file()` function to process the file and
    return a list of results to iterate through. At this point, if the `note_data`
    is not `None`, we write the RTF file with the `write_note_rtf()` method. Additionally,
    we append the processed the processed data from the `prep_note_report()` to the
    `report_details` list. Once the `for` loop completes, we write the CSV report
    with the `write_csv()` method by providing the report name, report columns, and
    the list we have built of the sticky note information.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The `create_file_like_obj()` function takes our `pytsk` file object and reads
    the size of the file. This size is used in the `read_random()` function to read
    the entire sticky note content into memory. We feed the `file_content` into the
    `StringIO()` class to convert it into a file-like object the `olefile` library
    can read before returning it to the parent function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The `parse_snt_file()` function accepts the file-like object as its input and
    is used to read and interpret the sticky note file. We begin by validating that
    the file-like object is an OLE file, returning `None` if it is not. If it is,
    we open the file-like object using the `OleFileIO()` method. This provides a list
    of streams, allowing us to iterate through each element of each sticky note. As
    we iterate over the list, we check if the stream contains three dashes, as this
    suggests that the stream contains a unique identifier for a sticky note. This
    file can contain one or more sticky notes, each identified by the unique IDs.
    The sticky note data is either read directly as RTF data or UTF-16 encoded data
    based on the value of the element in the first index of the stream.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also read the created and modified information from the stream using the
    `getctime()` and `getmtime()` functions, respectively. Next, we extract the sticky
    note RTF or UTF-16 encoded data to the `content` variable. Note, we must decode
    the UTF-16 encoded data prior to storing it. If there is content to save, we add
    it to the `note` dictionary and continue processing all remaining streams. Once
    all streams are processed, the `note` dictionary is returned to the parent function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: To create the RTF files, we pass the dictionary of note data to the `write_note_rtf()`
    function. If the report folder does not exist, we use the `os` library to create
    it. At this point, we iterate through the `note_data` dictionary, splitting the
    `note_id` keys from `stream_data` values. The `note_id` is used to create the
    output RTF filename prior to opening it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data stored in stream zero is then written to the ouput RTF file before
    it is closed and the next sticky note is handled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: With the content of the sticky notes written, we now move onto the CSV report
    itself which is handled a little differently by the `prep_note_report()` function.
    This translates the nested dictionary into a flat list of dictionaries that are
    more conducive and appropriate for a CSV spreadsheet. We flatten it by including
    the `note_id` key and naming the fields using the keys specified in the global
    `REPORT_COLS` list.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Lastly, in the `write_csv()` method, we create a `csv.Dictwriter` object to
    create an overview report of the sticky note data. This CSV writer also uses the
    `unicodecsv` library and writes the list of dictionaries to the file, using the
    `REPORT_COLS` list of columns as the `fieldnames`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then view the output as we have a new directory containing the exported
    sticky notes and report:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00098.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Opening our report, we can view the note metadata and gather some of the internal
    content, though most spreadsheet viewers have difficulty with non-ASCII character
    interpretations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00099.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Lastly, we can open the output RTF files and view the raw content:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00100.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Reading the registry
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recipe Difficulty: Medium'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python Version: 2.7'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operating System: Linux'
  prefs: []
  type: TYPE_NORMAL
- en: The Windows registry contains many important details related to the operating
    system configuration, user activity, software installation and usage, and so much
    more. These files are often heavily scrutinized and researched due to the number
    of artifacts they contain and their relevance to Windows systems. Parsing registry
    files gives us access to the keys and values that can reveal basic operating system
    information, access to folders and files, application usage, USB devices, and
    more. In this recipe, we focus on accessing common baseline information from the
    `SYSTEM` and `SOFTWARE` hives.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This recipe requires the installation of three third-party modules to function:
    `pytsk3`, `pyewf`, and `Registry`. Refer to [Chapter 8](part0241.html#75QNI0-260f9401d2714cb9ab693c4692308abe),
    *Working with Forensic Evidence Container* *Recipes,* for a detailed explanation
    of installing the `pytsk3` and `pyewf` modules. All other libraries used in this
    script are present in Python''s standard library.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we use the `Registry` module to interact with registry hives
    in an object-oriented manner. Critically, this module can be used to interact
    with external and standalone registry files. The `Registry` module can be installed
    with `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: To learn more about the `Registry` library, visit [https://github.com/williballenthin/python-registry](https://github.com/williballenthin/python-registry).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To build our registry system overview script, we will need to:'
  prefs: []
  type: TYPE_NORMAL
- en: Find the registry hives to process by name and path.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open these files using the `StringIO` and `Registry` modules.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Process each hive, printing the parsed values to the console for interpretation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The imports overlap with the other recipes in this chapter. These modules allow
    us to handle argument parsing, date manipulation, read our files into memory for
    the `Registry` library, and unpack and interpret binary data we extract from registry
    values. We also import the `TSKUtil()` class and the `Registry` module to process
    registry files.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This recipe''s command-line handler takes two positional arguments, `EVIDENCE_FILE`
    and `IMAGE_TYPE`, which represent the path to the evidence file and the type of
    evidence file, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The `main()` function starts by creating a `TSKUtil` object from the evidence
    and searches for the `SYSTEM` and `SOFTWARE` hives within the `/Windows/System32/config`
    folder. We create `Registry()` class instances of these hives with the `open_file_as_reg()`
    function before they are passed to their respective processing functions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'To open the registry files, we need to gather the size of the file from the
    `pytsk` metadata and read the entire file, from byte zero to the end of the file
    into a variable. We then provide this variable to a `StringIO()` instance which
    allows us to open the file-like object with the `Registry()` class. We return
    the `Registry` class instance to the calling function for further processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s start with the `SYSTEM` hive processing. This hive holds the majority
    of its information within control sets. The `SYSTEM` hive generally has two or
    more control sets that act as a backup system for the configurations they store.
    For simplicity, we will only read the current control set. To identify the current
    control set, we get our foothold within the hive with the `root` key and use the
    `find_key()` method to get the `Select` key. Within this key, we read the `Current`
    value, using the `value()` method to select it and the `value()` method on the
    `value` object to present the content of the value. While the method naming is
    a little ambiguous, the values within a key are named, so we first need to select
    them by name before then calling out the content that they hold. Using this information,
    we select the current control set key, passing an appropriately padded integer
    for the current control set (such as `ControlSet0001`). This object will be used
    through the remainder of the function to navigate to specific `subkeys` and `values`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The first piece of information we will extract from the `SYSTEM` hive is the
    shutdown time. We read the `Control\Windows\ShutdownTime` value from the current
    control set and pass the hexadecimal value into `struct` to convert it to a `64-bit`
    integer. We then provide this integer to the Windows `FILETIME` parser to obtain
    a human-readable date string which we print to the console.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will ascertain the time zone information for the machine. This is
    found within the `Control\TimeZoneInformation\TimeZoneKeyName` value. This returns
    a string value that we can print directly to the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Following that, we gather the machine''s hostname. This is found under the
    `Control\ComputerName\ComputerName` key in the `ComputerName` value. The extracted
    value is a string that we can print to the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Pretty easy so far, right? Lastly, for the `System` hive, we parse information
    about the last access timestamp configuration. This `registry` key determines
    if the NTFS volume''s last access timestamp is maintained, and is generally disabled
    by default on systems. To confirm this, we look for the `NtfsDisableLastAccessUpdate`
    value in the `Control\FileSystem` key and see if it is equal to `1`. If it is,
    the last access timestamp is not maintained and marked as disabled before printing
    to the console. Notice the one-liner `if-else` statement, while perhaps a little
    more difficult to read it does have its uses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Our Windows `FILETIME` parser borrows logic from our former date-parsing recipe,
    accepting an integer that we convert into a human-readable date string. We also
    borrowed the logic for the `Unix` epoch date parser from the same date-parsing
    recipe and will use it to interpret dates from the `Software` hive.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Our last function processes the `SOFTWARE` hive, presenting information to users
    in the console window. This function also begins by gathering the root of the
    hive and then selecting the `Microsoft\Windows NT\CurrentVersion` key. This key
    contains values about OS installation metadata and other useful subkeys. In this
    function, we will extract the `ProductName`, `CSDVersion`, `CurrentBuild number`,
    `RegisteredOwner`, `RegisteredOrganization`, and `InstallDate` values. While most
    of these values are strings we can print directly to the console, we need to use
    the `Unix` epoch converter to interpret the installation date value prior to printing
    it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run this script, we can learn about the information stored in the keys
    we interpreted:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00101.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This script can be further improved. We have provided one or more recommendations
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Add logic to handle the situation where the `SYSTEM` or `SOFTWARE` hives are
    not found in the initial search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider adding support for `NTUSER.DAT` files, pulling basic information about
    mount points and shell bags queries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: List basic USB device information from the `System` hive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parse the `SAM` hive to show user and group information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gathering user activity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recipe Difficulty: Medium'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python Version: 2.7'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operating System: Linux'
  prefs: []
  type: TYPE_NORMAL
- en: Windows stores a plethora of information about user activity, and like other
    registry hives, the `NTUSER.DAT` file is a great resource to be relied upon during
    an investigation. This hive lives within each user's profile and stores information
    and configurations as they relate to the specific user's on the system.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we cover multiple keys within `NTUSER.DAT` that throw light
    on the actions of a user on a system. This includes the prior searches run in
    Windows Explorer, paths typed into Explorer's navigation bar, and the recently
    used statements in the Windows `run` command. These artifacts better illustrate
    how the user interacted with the system and may give insight into what normal,
    or abnormal, usage of the system looked like for the user.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This recipe requires the installation of four third-party modules to function:
    `jinja2`, `pytsk3`, `pyewf`, and `Registry`. Refer to [Chapter 8](part0241.html#75QNI0-260f9401d2714cb9ab693c4692308abe),
    *Working with Forensic Evidence Container* *Recipes,* for a detailed explanation
    of installing the `pytsk3` and `pyewf` modules. Likewise, refer to the *Getting
    started* section in the *Reading the registry* recipe for details on installing
    `Registry`. All other libraries used in this script are present in Python''s standard
    library.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will reintroduce `jinja2`, first introduced in [Chapter 2](part0071.html#23MNU0-260f9401d2714cb9ab693c4692308abe),
    *Creating Artifact Report* R*ecipes*, to build an HTML report. This library is
    a template language that allows us to build text files programmatically using
    a Pythonic syntax. As a reminder, we can use `pip` to install this library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To extract these values from `NTUSER.DAT` files within the image, we must:'
  prefs: []
  type: TYPE_NORMAL
- en: Search for all `NTUSER.DAT` files across the system.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Parse the `WordWheelQuery` key for each `NTUSER.DAT` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read the `TypedPath` key for each `NTUSER.DAT` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract the `RunMRU` key for each `NTUSER.DAT` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write each of the processed artifacts to an HTML report.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our imports start in the same manner as our prior recipe, adding in the `jinja2`
    module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: This recipe's command-line handler takes three positional arguments, `EVIDENCE_FILE`,
    `IMAGE_TYPE`, and `REPORT`, which represent the path to the evidence file, the
    type of evidence file, and the desired output path to the HTML report, respectively.
    These three arguments are passed to the `main()` function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The `main()` function begins by reading the evidence file and searching for
    all `NTUSER.DAT` files. Following this, we set up a dictionary object, `nt_rec`,
    which, while complex, is designed in a manner that eases the HTML report generation
    process. We then begin iterating through the discovered hives and parse out the
    username from the path for reference in the processing functions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Next, we pass the `pytsk` file handle to be opened as a `Registry` object. This
    resulting object is used to gather the `root` key in common with all of the desired
    values (`Software\Microsoft\Windows\CurrentVersion\Explorer`). If this key path
    is not found, we continue to the next `NTUSER.DAT` file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'If they key is found, we call the three processing functions responsible for
    each artifact and provide the shared key object and username. The returned data
    is stored in the respective data key within the dictionary. We can easily extend
    the number of artifacts parsed by the code by expanding the storage object definition
    and adding a new function with the same profile as the others shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: After iterating through the `NTUSER.DAT` files, we set up the headers for each
    of the record types by extracting the key list of the first item on our data list.
    Since all of the dictionary objects in our data list have uniform keys, we can
    use this method to reduce the number of arguments or variables passed around.
    These statements are also easily extensible.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, we take our completed dictionary object and pass it, along with the
    path to the report file, to our `write_html()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: We've seen the `open_file_as_reg()` method before in the previous recipe. As
    a reminder, it takes the `pytsk` file handle and reads it into the `Registry`
    class via the `StringIO` class. The returned `Registry` object allows us to interact
    and read the registry in an object-oriented manner.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: The first processing function handles the `WordWheelQuery` key, which stores
    information about searches run by a user within Windows Explorer. We can parse
    this artifact by accessing the key by name from our `explorer_key` object. If
    the key does not exist, we will return an empty list as we do not have any values
    to extract.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'On the other hand, should the key exist, we iterate through the `MRUListEx`
    value, which holds a list of integers that contain the order of the searches.
    Each number in the list matches a value of the same number in the key. For this
    reason, we read the order of the list and interpret the remaining values in the
    order they appear. Each value name is stored as a two-byte integer, and so we
    split this list into two-byte chunks and read the integers with `struct`. We then
    append this value to the list after checking that it does not exist. If it does
    exist in the list, and is `\x00` or `\xFF`, we have reached the end of the `MRUListEx`
    data and break out of the loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Using our ordered value list, we iterate through it to extract the search terms
    in the order they were run. Since we know the order of use, we can associate the
    last write time of the `WordWheelQuery` key as the timestamp for the search term.
    This timestamp is only associated with the most recently run search. All other
    searches are given the value of `N/A`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Afterwards, we build out the dictionary within the `append` statement, adding
    the time value, username, order (as the count integer), the value's name, and
    the search content. To properly display the search content, we will need to provide
    the key name as a string and decode the text as UTF-16\. This text, once stripped
    of null termination, is ready for the report. The list is built out until all
    values are processed and then ultimately returned.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: The next processing function handles the typed paths key, taking the same arguments
    as the prior processing function. We access the key in the same manner and return
    the empty list in case the `TypedPaths` subkey is not found.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: This key does not have an MRU value ordering the typed paths, so we read all
    of its values and add them directly to the list. We can gather the value's name
    and path from this key, adding the username value for additional context. We finish
    this function by returning the list of dictionary values to the `main()` function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Our last processing function handles the `RunMRU` key. If it does not exist
    in the `explorer_key`, we return an empty list as seen before.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Since this key can be empty, we first check if there are values for us to parse
    and, if there are not, return an empty list to prevent any unnecessary processing.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Similar to the `WordWheelQuery`, this key also has an MRU value, which we process
    to learn the correct order of the other values. This list stores items differently,
    as its values are letters as opposed to integers. This makes our job quite simple
    as we directly query for the necessary values using these characters without additional
    processing. We append the order of values to a list and move on.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: As we iterate through the order of values, we begin to build out our dictionary
    of results. First, we handle the timestamps in the same manner as our `WordWheelQuery`
    processor, by assigning a default `N/A` value and updating it with the key's last
    written time if it is the first entry in our ordered list. Following this, we
    append a dictionary containing the relevant entries, such as the username, the
    value order, value name, and value content. This list of dictionaries is returned
    once we have processed all remaining values in the `Run` key.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: The last function handles the creation of the HTML report. This function starts
    by preparing the path of the code and the `jinja2` environment class. This class
    is used to store shared resources within the library, and we use it to point the
    library to the directory it should search for template files. In our case, we
    want it to look for template HTML files in the current directory, so we use the
    `os` library to get the current working directory and provide it to the `FileSystemLoader()`
    class.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: With the environment configured, we call the template we would like to use and
    then the `render()` method to create an HTML file with our passed dictionary.
    The `render` function returns a string representing the rendered HTML output with
    the results of the processed data inserted which we write to the output file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Let's look at the template file, it starts as any HTML document with the `html`,
    `head`, and `body` tags. While we've included scripts and style sheets in our
    `head` tag, we have omitted the unrelated material here. This information is available
    for review in full in the code bundle.
  prefs: []
  type: TYPE_NORMAL
- en: We start the HTML document with a `div` that holds the processed data tables
    and section headers. To simplify the amount of HTML we need to write, we use a
    `for` loop to gather each of the nested dictionaries from the `nt_data` values.
    The `jinja2` template language allows us to still use Python loops as long as
    they are wrapped in curly brackets, a percentage symbol, and a space character.
    We can also reference properties and methods of objects, allowing us to iterate
    through the values of the `nt_data` dictionary without extra code.
  prefs: []
  type: TYPE_NORMAL
- en: The other commonly used template syntax is shown within the `h2` tag, where
    we access the title attribute we set in the `main()` function. Variables we would
    like the `jinja2` engine to interpret (versus show as literal strings) need to
    be enclosed in double curly brackets and a space character. This will now print
    the section header for each section in our `nt_data` dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Within this loop, we set up our data table using the `data` tag and create
    a new row to hold the table headers. To generate the headers, we step through
    each of the headers we gathered and assign the value in a nested `for` loop. Notice
    how we need to specify the end of the loop with the `endfor` statement; this is
    required by the templating engine, as (unlike Python) it is not sensitive to indents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Following the table headers, we enter a separate loop to iterate through each
    dictionary in our data list. Inside each table row, we use similar logic as the
    table headers to create another `for` loop to write each value into a cell in
    the row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the HTML data table is populated, we close the `for` loop for the
    current data point: we draw a horizontal line and start writing the next artifact''s
    data table. Once we completely iterate through those, we close the outer `for`
    loop and the tags we opened at the start of the HTML report.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Our generated report is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00102.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This script can be further improved. We have provided one or more recommendations
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Add additional `NTUser` or other easy to review artifacts to the dashboard to
    provide more useful information at a glance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add charts, a timeline, or other interactive elements to this dashboard using
    various JavaScript and CSS elements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide export options from the dashboard into CSV or Excel spreadsheets with
    additional JavaScript
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The missing link
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recipe Difficulty: Medium'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python Version: 2.7'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operating System: Linux'
  prefs: []
  type: TYPE_NORMAL
- en: Shortcut files, also known as link files, are common across operating system
    platforms. They enable the user to use one file to reference another, located
    elsewhere on the system. On the Windows platform, these link files also record
    historical access to the files they reference. Generally, the creation time of
    a link file represents the first access time of a file with that name, and the
    modification time represents the most recent access time of the file with that
    name. Using this, we can extrapolate a window of activity and learn about how,
    and where, these files were accessed.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This recipe requires the installation of three third-party modules to function:
    `pytsk3`, `pyewf`, and `pylnk`. Refer to [Chapter 8](part0241.html#75QNI0-260f9401d2714cb9ab693c4692308abe),
    *Working with Forensic Evidence Container* *Recipes* for a detailed explanation
    of installing the `pytsk3` and `pyewf` modules. All other libraries used in this
    script are present in Python''s standard library.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to the GitHub repository and download the desired release of the `pylnk`
    library. This recipe was developed using the `pylnk-alpha-20170111` release. Next,
    once the contents of the release are extracted, open a terminal and navigate to
    the extracted directory and execute the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: To learn more about the `pylnk` library, visit [https://github.com/libyal/liblnk](https://github.com/libyal/liblnk).
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we can check our library's installation by opening a Python interpreter,
    importing `pylnk`, and running the `gpylnk.get_version()` method to ensure we
    have the correct release version.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This script will leverage the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Search for all `lnk` files within the system.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Iterate through discovered `lnk` files and extract relevant attributes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write all artifacts to a CSV report.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Starting with the imports, we bring in the Sleuth Kit utilities and `pylnk`
    library. We also bring in libraries for argument parsing, writing the CSV reports,
    and `StringIO` to read the Sleuth Kit objects as files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: This recipe's command-line handler takes three positional arguments, `EVIDENCE_FILE`,
    `IMAGE_TYPE`, and `CSV_REPORT`, which represent the path to the evidence file,
    the type of evidence file, and the desired output path to the CSV report, respectively.
    These three arguments are passed to the `main()` function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'The `main()` function begins with creating the `TSKUtil` object used to interpret
    the evidence file and iterate through the filesystem to find files ending in `lnk`.
    If there are not any `lnk` files found on the system, the script alerts the user
    and exits. Otherwise, we specify columns representing the data attributes we want
    to store for each of the `lnk` files. While there are other attributes available,
    these are some of the more relevant ones we extract in this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Next, we iterate through the discovered `lnk` files, opening each as a file
    using the `open_file_as_lnk()` function. The returned object is an instance of
    the `pylnk` library, ready for us to read the attributes from. We initialize the
    attribute dictionary with the file's name and path and then iterate through the
    columns we specified in the `main()` function. For each of the columns, we try
    to read the specified attribute value, and, if we are unable to, store an "`N/A`"
    value otherwise. These attributes are stored in the `lnk_data` dictionary which
    is appended to the `parsed_lnks` list once all attributes are extracted. After
    this process completes for each `lnk` file, we pass this list, along with the
    output path, and column names, to the `write_csv()` method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'To open our `pytsk` file object as a `pylink` object, we use the `open_file_as_lnk()`
    function which operates like other similarly named functions throughout this chapter.
    This function reads the entire file, using the `read_random()` method and file
    size property, into a `StringIO` buffer that is then passed into a `pylnk` file
    object. Reading in this manner allows us to read the data as a file without needing
    to cache it to the disk. Once we have loaded the file into our `lnk` object, we
    return it to the `main()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: The last function is the common CSV writer, which uses the `csv.DictWriter`
    class to iterate through the data structure and write the relevant fields to a
    spreadsheet. The order of the columns list defined in the `main()` function determines
    their order here as the `fieldnames` argument. One could change that order, if
    desired, to modify the order in which they are displayed in the resulting spreadsheet.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the script, we can view the results in a single CSV report as
    seen in the following two screenshots. Since there are many visible columns, we
    have elected to display only a few for the readability sake:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00103.jpeg)![](../images/00104.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This script can be further improved. We have provided one or more recommendations
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Add checks to see if the target file still exists
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify target locations on remote or removable volumes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add support for parsing jumplists
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Searching high and low
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recipe difficulty: Hard'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python version: 2.7'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operating system: Linux'
  prefs: []
  type: TYPE_NORMAL
- en: Most modern operating systems maintain an index of files and other data content
    stored on the system. These indexes allow for more efficient searches across file
    formats, emails, and other content found on the system's volumes. On Windows,
    such an index is found in the `Windows.edb` file. This database is stored in the
    **Extensible Storage Engine** (**ESE**) file format and found within the `ProgramData`
    directory. We will leverage another library from the `libyal` project to parse
    this file to extract information about the indexed content on the system.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This recipe requires the installation of four third-party modules to function:
    `pytsk3`, `pyewf`, `pyesedb`, and `unicodecsv`. Refer to [Chapter 8](part0241.html#75QNI0-260f9401d2714cb9ab693c4692308abe),
    *Working with Forensic Evidence Container* *Recipes* for a detailed explanation
    on installing the `pytsk3` and `pyewf` modules. Likewise, refer to the *Getting
    started* section in the *One man''s trash is a forensic examiner''s treasure*
    recipe for details on installing `unicodecsv`. All other libraries used in this
    script are present in Python''s standard library.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to the GitHub repository and download the desired release for each
    library. This recipe was developed using the `libesedb-experimental-20170121`
    release. Once the contents of the release are extracted, open a terminal, navigate
    to the extracted directory, and execute the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: To learn more about the `pyesedb` library, visit [**https://github.com/libyal/libesedb**](https://github.com/libyal/libesedb)**.**
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we can check our library's installation by opening a Python interpreter,
    importing `pyesedb`, and running the `epyesedb.get_version()` method to ensure
    we have the correct release version.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To draft this script we will need to:'
  prefs: []
  type: TYPE_NORMAL
- en: Recurse the `ProgramData` directory to search for the `Windows.edb` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Iterate through discovered `Windows.edb` files (though there should really only
    be one) and open the files using the `pyesedb` library.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Process each of the files to extract key columns and attributes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write these key columns and attributes to the report.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The imports here include those libraries we''ve used for most recipes in the
    chapter for argument parsing, string buffer file-like objects, and the `TSK` utilities.
    We also import the `unicodecsv` library to handle any Unicode objects in the CSV
    report, the `datetime` library to assist with timestamp parsing, and the `struct`
    module to help make sense of the binary data we read. Additionally, we define
    a global variable, `COL_TYPES`, that aliases the column types from the `pyesedb`
    library, used to help identify the types of data that we will extract later in
    the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: This recipe's command-line handler takes three positional arguments, `EVIDENCE_FILE`,
    `IMAGE_TYPE`, and `CSV_REPORT`, which represent the path to the evidence file,
    the type of evidence file, and the desired output path to the CSV report, respectively.
    These three arguments are passed to the `main()` function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'The `main()` function opens the evidence and searches for the `Windows.edb`
    file within the `ProgramData` directory. If one or more files are found, we iterate
    through the list and open each ESE database for further processing with the `process_windows_search()`
    function. This function returns the spreadsheet column headers to use and a list
    of dictionaries containing the data to include in the report. This information
    is then written to the output CSV for review by the `write_csv()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: Reading the responsive ESE database requires the `open_file_as_esedb()` function.
    This code block uses similar logic to the previous recipes to read the file into
    a `StringIO` object and open the file-like object with the library. Note, this
    could cause errors on your system if the file is rather large or your machine
    has lower amounts of memory. You can use the built-in `tempfile` library to cache
    the file to a temporary location on disk, reading from there if you would prefer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Our `process_windows_search()` function starts with column definitions. While
    our previous recipe used a simple list of columns, the `pyesedb` library takes
    a column index as an input to retrieve a value from a row within a table. For
    this reason, our column list must consist of tuples, where the first element is
    a number (the index) and the second is the string description. Since the description
    isn''t used in the function to select columns, we name these in the manner we
    would like them displayed in the report. For this recipe, we have defined the
    following column indexes and names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: After we define the columns of interest, we access the `SystemIndex_0A` table,
    which contains the indexed file, mail, and other entries. We iterate through the
    records within the table, building a `record_info` dictionary of the column values
    for each record that will eventually be appended to the `table_data` list. A second
    loop iterates through the columns we defined earlier and attempts to extract the
    value and value type for each column in the record.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: Using the `COL_TYPES` global variable we defined earlier, we can reference the
    various data types and ensure we are interpreting the values correctly. The logic
    in the following code block focuses on interpreting the values correctly based
    on their data type. First, we handle dates, which may be stored as Windows `FILETIME`
    values. We attempt to convert the `FILETIME` value, if possible, or present the
    date value in hexadecimal if not. The next statement checks for text values, interpreting
    the value with the `pyesedb` `get_value_data_as_string()` function or as a UTF-16
    big-endian and replacing any unrecognized character for completeness.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then individually handle integer and Boolean data type interpretation using
    the `pyesedb` `get_value_data_as_integer()` function and a simple comparison statement,
    respectively. Specifically, we check if the `rec_val` is equal to `"\x01"` and
    allow `rec_val` to be set `True` or `False` based on that comparison. If none
    of these data types are valid, we interpret the value as hex and store it with
    the associated column name before appending the value to the table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: We then return a tuple to our calling function, where the first element is the
    list of names of the columns in the `report_cols` dictionary and the second is
    a list of data dictionaries.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Borrowing our logic from our date-parsing recipe in [Chapter 7](part0212.html#6A5N80-260f9401d2714cb9ab693c4692308abe),
    *Log-Based Artifact Recipes*, we implement a function to parse the Windows `FILETIME`
    value into a human-readable state. This accepts an integer value as input and
    returns a human-readable string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: The last function is the CSV report writer, which writes the columns and the
    rows of collected information to the open CSV spreadsheet using the `DictWriter`
    class. While we selected a subset of the available columns at the outset, there
    are many more to choose from that may be useful in varying case types. Therefore,
    we recommend taking a look at all available columns to better understand this
    recipe and what columns may or may not be useful for you.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the recipe, we can review the output CSV shown here. As there
    are many columns to this report, we have highlighted a few interesting ones in
    the following two screenshots:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00105.jpeg)![](../images/00106.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This script can be further improved. We have provided one or more recommendations
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Add support to check for the existence of referenced files and folders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write our `Windows.edb` file to a temporary location to relieve memory pressure
    when parsing large databases with the Python `tempfile` library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add more columns or create separate (targeted) reports using more of the over
    300 available columns in the table
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
