- en: Monitoring and Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we investigated the declarative structures used in
    Kubernetes objects and resources. With the end goal of having Kubernetes help
    run software for us, in this chapter we will look at how we can get more information,
    when we're running our applications at a greater scale, and some open source tools
    that we can use for that purpose. Kubernetes is already gathering and using some
    information about how utilized the nodes of the cluster are, and there is a growing
    capability within Kubernetes to start to collect application-specific metrics,
    and even use those metrics as a control point for managing the software.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, will we dig into these aspects of basic observability and
    walk through how you can set them up for your local development use, and how to
    leverage them to gather, aggregate, and expose details of how your software is
    running, when you scale it up. Topics within this chapter will include:'
  prefs: []
  type: TYPE_NORMAL
- en: Built-in metrics with Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Kubernetes concept—Quality of Service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Capturing metrics with Prometheus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing and using Grafana
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Prometheus to view application metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Built-in metrics with Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes comes built in with some basic instrumentation to know how much CPU
    and memory are consumed on each node in the cluster. Exactly what is captured
    and how it is captured has been evolving rapidly in recent Kubernetes releases
    (1.5 through 1.9). Many Kubernetes installations will be capturing information
    about what resources the underlying containers are using with a program called
    cAdvisor. This code was created by Google to collect, aggregate, and expose the
    metrics of how containers are operating, as a critical step of being able to know
    where best to place new containers, based on what resources a node has and where
    resources are available.
  prefs: []
  type: TYPE_NORMAL
- en: Every node within a Kubernetes cluster will have cAdvisor running and collecting
    information, and this, in turn, is captured and used by *kubelet*, which is the
    local agent on every node that is responsible for starting, stopping, and managing
    the various resources needed to run containers.
  prefs: []
  type: TYPE_NORMAL
- en: cAdvisor exposes a simple web-based UI that you can use to look at the details
    for any node, manually. If you can access port `4194` of the node, that is the
    default location that exposes the cAdvisor details. Depending on your cluster
    setup, this may not be easy to get access to. In the case of using Minikube, it
    is easily and directly available.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have Minikube installed and running, you can use this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To get the IP address of the virtual machine local to your development machine
    that is running your single-node Kubernetes cluster, you can access cAdvisor running,
    thereby opening a browser and navigating to that IP address at port `4194`. For
    example, on macOS running Minikube, you could use this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'And you''ll see the simple UI, showing a page that looks something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/07b5bff8-6ca7-4575-95f9-453e7e300d7e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Scroll down a bit, and you will see a number of gauges and a table of information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/1385db01-d5e4-46dd-b882-9904088ced18.png)'
  prefs: []
  type: TYPE_IMG
- en: Beneath that is a set of simple graphs showing CPU, memory, network, and filesystem
    usage. These graphs and tables will update and automatically refresh as you watch
    them, and represent the basic information that Kubernetes is capturing about your
    cluster, as it operates.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes also makes metrics about itself—its API server and relevant components,
    available through its own API. You can see these metrics directly using the `curl`
    command after making the API available through `kubectl` proxy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'And in a separate Terminal window:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Many installations of Kubernetes have used a program called Heapster to collect
    metrics from Kubernetes and from each node's instance of cAdvisor, and store them
    in a time-series database such as InfluxDB. As of Kubernetes 1.9, the open source
    project is shifting a bit further away from Heapster towards a pluggable solution,
    with a common alternative solution being Prometheus, which is frequently used
    for short-term metrics capture.
  prefs: []
  type: TYPE_NORMAL
- en: If you are using Minikube, you can easily add Heapster to your local environment
    with a `minikube` add-on. Like the dashboard, this will run software for Kubernetes
    on its own infrastructure, in this case, Heapster, InfluxDB, and Grafana.
  prefs: []
  type: TYPE_NORMAL
- en: 'This will enable the add-on within Minikube, you can use this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In the background, Minikube will start up and configure Heapster, InfluxDB,
    and Grafana, creating it as a service. You can use this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This will open a browser window to Grafana. The command will wait while the
    containers are being set up, but when the service endpoint is available, it will
    open a browser window:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/c588d346-eee6-4972-84bb-a6d7559cb24d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Grafana is a single-page application used to display graphs and build dashboards
    from common data sources. In the version that is created by the Minikube Heapster
    add-on, Grafana is configured with two dashboards: Cluster and Pods. If you select
    the pull-down menu labeled Home in the default view, you can choose the other
    dashboards to view.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It will take a minute or two before Heapster, InfluxDB, and Grafana have all
    coordinated to collect and capture some of the basic metrics of the environment,
    but fairly shortly, you can go to these other dashboards to see information about
    what''s running. For example, I deployed all the sample applications from the
    prior chapters in this book and went to the Cluster dashboard, and after 10 minutes
    or so, the view looked like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/08832507-6a28-453a-b701-7623903350b0.png)'
  prefs: []
  type: TYPE_IMG
- en: Scroll down through this dashboard, and you will see CPU, memory, filesystem,
    and network usage by node, as well as overall cluster views for this. You may
    notice the CPU graphs have three lines being tracked—usage, limit, and request—which
    match the resources actually in use, the amount requested, and any limits set
    on the pods and containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you switch to the Pods dashboard, you will see that the dashboard has selections
    for all of the pods currently running in your cluster, and provides a detailed
    view of each one. In the example shown here, I selected the pod from our `flask`
    example application that I deployed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ee592f34-3aa0-4a4f-95e9-6573b8a6c755.png)'
  prefs: []
  type: TYPE_IMG
- en: Scrolling down, you can see graphs that include memory, CPU, network, and disk
    utilization. The collection of Heapster, Grafana, and InfluxDB will automatically
    record new pods as you create them, and you can select between namespaces and
    pod names in the Pods dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes concept – Quality of Service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When a pod is created in Kubernetes, it is also assigned a Quality of Service
    class, based on the data provided about the pod when it was requested. This is
    used by the scheduler to provide some upfront assurances during the scheduling
    process, and later in the management of the pods themselves. The three classes
    supported are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Guaranteed`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Burstable`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BestEffort`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which class is assigned to your pod is based on what resource limits and requests
    you report with the containers within your pod for CPU and memory utilization.
    In the previous examples, none of the containers were assigned a requests or limit,
    so all of those pods were classified as `BestEffort` when they were run.
  prefs: []
  type: TYPE_NORMAL
- en: Resource requests and limits are defined on each container within a pod. If
    we add a request to a container, we are asking for Kubernetes to make sure that
    the cluster has sufficient resources to run our pod (memory, CPU, or both) and
    it will validate that availability as a part of the scheduling. If we add a limit,
    we are requesting Kubernetes to watch the pod and react if the container exceeds
    the limits we set. For limits, if the container tries to exceed a CPU limit, the
    container will simply be throttled to the defined CPU limit. If a memory limit
    is exceeded, the container is frequently terminated and you will likely see the
    error message `OOM killed` in the `reason` description for those terminated containers.
  prefs: []
  type: TYPE_NORMAL
- en: If a request is set, the pods are generally set to the Quality of Service class
    of `Burstable`, with the exception of when a limit is also set, and that limit
    has the same value as the request, in which case the service class of `Guaranteed`
    is assigned. As a part of scheduling, if a pod is deemed to be in the `Guaranteed`
    service class, Kubernetes will reserve resources within the cluster and if overloaded,
    it will bias towards expiring and evicting `BestEffort` containers first, and
    then `Burstable`. A cluster will generally need to expect to lose resource capacity
    (for example, one or more nodes fail). In these cases, a `Guaranteed` class pod
    will have the most longevity in the face of such failures once it has been scheduled
    into a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can update our `flask` example pod so that it will operate with a `Guaranteed` Quality
    of Service, by adding a request and limit for both CPU and memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This places a request and limit of the same value for both CPU and memory—in
    this case, 100 MB of memory and roughly half a core of CPU utilization.
  prefs: []
  type: TYPE_NORMAL
- en: It is generally considered a best practice to at least define requests, and
    ideally limits as well, for all containers and pods that you want to run in a
    production mode.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing requests and limits for your containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are uncertain of what values to use to set a request and/or limit for
    a container, the best means of determining those values is to watch them. With
    Heapster, or Prometheus, and Grafana, you can see how many resources are being
    consumed by each pod.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a three-step process that you can use with your code to see what it''s
    taking:'
  prefs: []
  type: TYPE_NORMAL
- en: Run your code and review how many resources are consumed while idle
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add load to your code and verify the resource consumption under load
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Having set constraints, run another load test for an sustained period of time
    to see that your code fits within the defined boundaries
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first step (reviewing while idle) will give you good numbers to start with.
    Leverage Grafana, or utilize cAdvisor available at your cluster node, and simply
    deploy the pod in question. In the preceding examples, where we did that with
    the `flask` example from earlier in this book, you can see an that idle flask
    application was consuming roughly 3 millicores (.003% of a core) and roughly 35
    MB of RAM. This makes a base of what to expect for a request for both CPU and
    memory.
  prefs: []
  type: TYPE_NORMAL
- en: The second step is often best done by running an **increasing load test** (also
    known as a **ramp load test**) to review how your pod reacts under load. Generally,
    you will see your load ramp up linearly with the requests, and then make a bend,
    or knee, where it starts to become bottlenecked. You can review the same Grafana
    or cAdvisor panels to show utilization during that load.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you wanted to generate a simple bit of load, you could generate some specific
    load points with tools such as Apache benchmark ( [https://httpd.apache.org/docs/2.4/programs/ab.html](https://httpd.apache.org/docs/2.4/programs/ab.html)).
    For example, to run an interactive container with this tool that could work against
    the Flask application, you could use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This image has both `curl` and `ab` installed, so you can verify that you can
    talk to the Flask-service that we created in our earlier example with this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This should return some verbose output, showing the connection and basic request
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you have verified that everything is operating as you expect, you can
    run some load with `ab`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: You will see a corresponding jump in resource usage in cAdvisor, or after a
    minute or so, Grafana with Heapster. To get useful values in Heapster and Grafana,
    you will want to run extended load tests since that data is being aggregated—you
    will want to run your load test ideally for several minutes, as one minute of
    granularity is the basic level that Grafana aggregates to with Heapster.
  prefs: []
  type: TYPE_NORMAL
- en: 'cAdvisor will update more quickly, and if you''re viewing the interactive graphs,
    you will see them update as the load test progresses:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/5e3a3c74-1d06-41a8-8589-b8b18fefcdc1.png)'
  prefs: []
  type: TYPE_IMG
- en: In this case, you see our memory usage stayed fairly consistent at around 36
    MB, and our CPU peaked (as you might expect for this application) during the load
    test.
  prefs: []
  type: TYPE_NORMAL
- en: If we then applied the preceding request and limit examples, and updated the
    flask deployment, then you would see the load flatten off when the CPU hit the
    roughly 1/2 core CPU limit.
  prefs: []
  type: TYPE_NORMAL
- en: The third step in this process is primarily to validate your assessments for
    CPU and memory needs over a longer-running load test. Typically, you would run
    an extended load (a minimum of several minutes long) with your requests and limits
    set to validate that the container could serve the traffic expected. The most
    common flaw in this assessment is seeing memory slowly climb while an extended
    load test is being performed, resulting in the container being OOM killed (terminated
    for exceeding its memory constraints).
  prefs: []
  type: TYPE_NORMAL
- en: The 100 MiB of RAM that we had in the example is reserving significantly more
    memory than this container needs, so we could easily reduce it down to 40 MiB
    and do the final validation step.
  prefs: []
  type: TYPE_NORMAL
- en: 'When setting requests and limits, you want to choose values that most efficiently
    characterize your needs, but don''t waste reserved resources. To run a more extended
    load test, type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting Grafana output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/965cf39b-d505-4897-abea-4bfbac400e5d.png)'
  prefs: []
  type: TYPE_IMG
- en: Capturing metrics with Prometheus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Prometheus is a prominent open source tool used for monitoring, and quite a
    bit of symbiotic work is happening between it and the Kubernetes community. Kubernetes
    application metrics are exposed in the Prometheus format. This format includes
    the data types of *counter*, *gauge*, *histogram*, and *summary*, as well as a
    means of specifying labels to be associated with specific metrics. As Prometheus
    and Kubernetes have both evolved, the metrics format from Prometheus appears to
    be emerging as a de facto standard within the project and across its various components.
  prefs: []
  type: TYPE_NORMAL
- en: 'More information about this format is available online at the Prometheus project''s
    documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://prometheus.io/docs/concepts/data_model/](https://prometheus.io/docs/concepts/data_model/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://prometheus.io/docs/concepts/metric_types/](https://prometheus.io/docs/concepts/metric_types/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://prometheus.io/docs/instrumenting/exposition_formats/](https://prometheus.io/docs/instrumenting/exposition_formats/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Beyond the metrics format, Prometheus offers quite a variety of capabilities
    as its own open source project, and is used outside Kubernetes. The architecture
    of this project gives a reasonable sense of its primary components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/2531e4a8-92de-4873-9164-9c5c23ec7806.png)'
  prefs: []
  type: TYPE_IMG
- en: The Prometheus server itself is what we will examine in this chapter. At its
    core, it periodically sweeps through a number of remote locations, collecting
    data from those locations, storing it in a short-term time-series database, and
    providing a means to query that database. Extensions to Prometheus allow the system
    to export these time-series metrics to other systems for longer-term storage.
    In addition, Prometheus includes an alert manager that can be configured to send
    alerts, or more generally invoke actions, based on the information captured and
    derived from the time-series metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus is not intended to be a long-term storage for metrics, and can work
    with a variety of other systems to capture and manage the data on a longer term.
    Common Prometheus installations maintain data for 6 to 24 hours, configurable
    per installation.
  prefs: []
  type: TYPE_NORMAL
- en: The most minimal installation of Prometheus would include the Prometheus server
    itself and configuration for the service. But to fully leverage Prometheus, the
    installation is frequently more expansive and complex, with a separate deployment
    each for the Alertmanager and Prometheus server, optionally a deployment for the
    push gateway (to allow other systems to actively send metrics to Prometheus),
    and a DaemonSet to capture data from every node within the cluster to expose and
    export that information into Prometheus, leveraging cAdvisor.
  prefs: []
  type: TYPE_NORMAL
- en: More complex installations of software can be done by managing a set of YAML
    files, as we have been exploring earlier in this book. There are other options
    for how to manage and install sets of deployments, services, configurations, and
    so forth. Rather than documenting all the pieces, we will leverage one of the
    more common tools for this sort of work, Helm, which is closely tied to the Kubernetes
    project and is commonly referred to as *the package manager for Kubernetes*.
  prefs: []
  type: TYPE_NORMAL
- en: You can find significantly more information about Helm from the project's documentation
    site at [https://helm.sh](https://helm.sh).
  prefs: []
  type: TYPE_NORMAL
- en: Installing Helm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Helm is a two-part system: a command-line tool and software that runs within
    your Kubernetes cluster that the command-line tool interacts with. Typically,
    what you need locally is the command-line tool, and that in turn will be used
    to install the components it needs into your cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: The documentation for installing the Helm command-line tool is available at
    the project's website: [https://github.com/kubernetes/helm/blob/master/docs/install.md.](https://github.com/kubernetes/helm/blob/master/docs/install.md)
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are using macOS locally, it is available via Homebrew and can be installed
    with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Or if you''re working from a Linux host, the Helm project offers a script you
    can use to install Helm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Once Helm is installed, you use it to install the component (called Tiller)
    that runs within your cluster, using the command `helm init`. You should see output
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition to setting up some local configuration files for its use, this
    made a deployment on your cluster within the `kube-system` namespace for its cluster-side
    component, **Tiller**. You can view that deployment, if you want to see it in
    more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, you have Helm installed, and you can verify the version of your
    installation (both command line and what''s on the cluster) with the command Helm
    version. This operates very much like `kubectl` version, reporting both its version
    and the version of the system it''s communicating with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can move on to the reason we set up Helm: to install Prometheus.'
  prefs: []
  type: TYPE_NORMAL
- en: Installing Prometheus using Helm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Helm uses a set of configuration files to describe what it needs to install,
    in what order, and with what parameters. These configurations are called charts,
    and are maintained in GitHub, where the default Helm repository is maintained.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can view the repository that Helm is using with the command `helm repo
    list`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This default is a wrapper around a GitHub repository, and you can view the contents
    of the repository at [https://github.com/kubernetes/charts](https://github.com/kubernetes/charts).
    Another way to see all the charts that are available for use is the command `helm
    search`.
  prefs: []
  type: TYPE_NORMAL
- en: It is a good idea to make sure you have the latest cache of the repository available.
    You can update your cache to the latest, mirroring the charts in GitHub, with
    the command `helm repo update`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting update should report success with output similar to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We are going to use the stable/Prometheus chart (hosted at [https://github.com/kubernetes/charts/tree/master/stable/prometheus](https://github.com/kubernetes/charts/tree/master/stable/prometheus)).
    We can use Helm to pull that chart locally, to look at it in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This command downloads the chart from the default repository and unpacks it
    locally in a directory called Prometheus. Take a look in the directory, and you
    should see several files and a directory called `templates`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This is the common pattern for charts, where `Chart.yaml` describes the software
    that will be installed by the chart. `values.yaml` is a collection of default
    configuration values that are used throughout all the various Kubernetes resources
    that will be created, and the templates directory contains the collection of templated
    files that will get rendered out to install all the Kubernetes resources needed
    for this software in your cluster. Typically, the `README.md` will include a description
    of all the values within the `values.yaml`, what they're used for, and suggestions
    for installation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now install `prometheus`, and we will do so by taking advantage of a
    couple of Helm''s options, setting a release name and using a namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This installs the chart included in the `prometheus` directory, installs all
    the components included into the namespace, `monitoring`, and prefixes all the
    objects with a release name of `monitor`. If we had not specified either of those
    values, Helm would have used the default namespace, and generated a random release
    name to uniquely identify the installation.
  prefs: []
  type: TYPE_NORMAL
- en: 'When this is invoked, you will see quite a bit of output describing what was
    created and its state at the start of the process, followed by a section of notes
    that provides information about how to access the software you just installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '`helm list` will show you the current releases that you have installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'And you can use the `helm status` command, along with the name of the release,
    to get the current state of all the Kubernetes resources created by the chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The notes section is included in the templates and rendered again on every status
    call, and is generally written to include notes on how to access the software.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can install a chart without having explicitly retrieved it first. Helm
    will use any local charts first, but fall back to searching through its available
    repositories, so we could have installed this same chart with just this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '`**helm install stable/prometheus -n monitor --namespace monitoring**`'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also have Helm mix together `values.yaml` and its templates, to render
    out all the objects it will create and simply display them, which can be useful
    to see how all the pieces will come together. The command to do this is `helm
    template`, and to render the YAML that was used to create the Kubernetes resources,
    the command would be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The `helm template` command does require the chart to be available on the local
    filesystem, so while `helm install` can work from a remote repository, you will
    need to use `helm fetch` to have the chart locally, in order to take advantage
    of the `helm template` command.
  prefs: []
  type: TYPE_NORMAL
- en: Viewing metrics with Prometheus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using the details available in the notes, you can set up a port-forward, as
    we have done earlier in this book and access Prometheus directly. The information
    that was displayed from the notes is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: This will allow you to directly access the Prometheus server with a browser.
    Run these commands in a terminal, and then open a browser and navigate to `http://localhost:9090/`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can view the current state of what Prometheus is monitoring by looking
    at the list of targets at `http://localhost:9090/targets`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/207a582a-322b-4fbf-9b78-88134b726460.png)'
  prefs: []
  type: TYPE_IMG
- en: Switch to the Prometheus query/browser at `http://localhost:9090/graph` to be
    able to view the metrics that have been collected by Prometheus. There are a large
    number of metrics that are collected, and we are specifically interested in the
    ones that match the information we saw earlier with cAdvisor and Heapster. In
    Kubernetes clusters of version 1.7 and later, these metrics moved and are specifically
    collected by the `kubernetes-nodes-cadvisor` job that we see in the targets in
    the screenshot.
  prefs: []
  type: TYPE_NORMAL
- en: In the query browser, you can start typing metric names and it will attempt
    to autocomplete, or you can use the pull-down menu to see the list of all possible
    metrics. Type in the metric name, `container_memory_usage_bytes`, and hit *Enter*
    to see a list of those metrics in table form.
  prefs: []
  type: TYPE_NORMAL
- en: The general form of good metrics will have some identifiers of what the metric
    is and usually end with a unit identifier, in this case, bytes. Looking at the
    table, you can see the metrics that are collected along with a fairly dense set
    of key-value pairs for each metric.
  prefs: []
  type: TYPE_NORMAL
- en: These key-value pairs are labels on the metrics, and function similarly to the
    way labels and selectors do, within Kubernetes in general.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example metric, reformatted to make it easier to read, is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'In the query, we can filter just the metrics we are interested in by including
    matches in the query to those labels. For example, all of the metrics that are
    associated with specific containers will have an `image` tag, so we can filter
    to just those metrics with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'You may have noticed that the namespaces and pod names are also included, and
    we can match on those as well. For example, to just view the metrics related to
    the default namespace where we have been deploying our sample applications, we
    could add `namespace="default"`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This is starting to get down to a more reasonable number of metrics. While
    the table will show you the most recent value, the history of these values is
    what we''re interested in seeing. If you select the Graph button on the current
    query, it will attempt to render out a single graph of the metrics that you have
    selected, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/fbccdafc-027d-4fae-ac33-27b56dde19fa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since the metrics also include `container_name` to match the deployment, you
    can tune this down to a single container. For example, to see the memory usage
    associated with our `flask` deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: If we scale up the number of replicas in our `flask` deployment, it will create
    new metrics for each container, so in order to view not just single containers
    but also sets at a time, we can take advantage of aggregating operators in the
    Prometheus query language. Some of the most useful operators include `sum`, `count`,
    `count_values`, and `topk`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also use these same aggregation operators to group metrics together
    where the aggregated set has different tag values within it. For example, after
    increasing the replicas on the `flask` deployment to three, we can view the total
    memory usage of the deployment with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'And then break it out into each container again by the pod name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The graph function can give you a nice visual overview, including stacking
    the values, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/2b9ea59d-0862-40c8-a304-1fd6932379de.png)'
  prefs: []
  type: TYPE_IMG
- en: With the graphs getting more complex, you may want to start to collect the queries
    you find most interesting, as well as put together dashboards of these graphs
    to be able to use them. This leads us to another open source project, Grafana,
    which can be easily hosted on Kubernetes, providing the dashboards and graphs.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Grafana
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Grafana isn't by itself a complex installation, but configuring it can be. Grafana
    can plug into a number of different backend systems and provide dashboarding and
    graphing for them. In our example, we would like to have it provide dashboards
    from Prometheus. We will set up an installation and then configure it through
    its user interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use Helm again to install Grafana, and since we have put Prometheus
    in the namespace monitoring, we will do the same with Grafana. We could do `helm
    fetch` and install to look at the charts. In this case, we will just install them
    directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'In the resulting output, you will see a secret, ConfigMap, and deployment among
    the resources created, and in the notes, something like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The notes first include information about retrieving the secret. This highlights
    a feature that you will see used in several charts: where it requires a confidential
    password, it will generate a unique one and save it as a secret. This secret is
    directly available to people with access to the namespace and `kubectl`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the provided command to retrieve the password for your Grafana interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'And then open a Terminal and run these commands to get access to the dashboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, open a browser window and navigate to `https://localhost:3000/`, which
    should show you the Grafana login window:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/56c2c32b-f460-4561-b192-3e2a24c4a6b8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, log in with the username `admin`; the password is the secret you retrieved
    earlier. This will bring you to a Home dashboard in Grafana, where you can configure
    data sources and assemble graphs into dashboards:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/229ace04-bb99-4ac2-b282-cdc03ef23b21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Click on Add data source and you will see a window with two tabs: Config allows
    you to set the location to a data source, and Dashboards allows you to import
    dashboard configurations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Under Config, set the type of the data source to Prometheus, and where it says
    Name, you can enter `prometheus`. Naming the data source after the type is a bit
    redundant, and if you had multiple Prometheus instances on your cluster, you would
    want to name them separately and specifically for their purpose. Add in the DNS
    name of our Prometheus instance to the URL so that Grafana can access it: `http://monitor-prometheus-server.monitoring.svc.cluster.local`.
    This same name was listed in the notes when we installed Prometheus using Helm.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the Dashboards tab and import Prometheus stats and Grafana metrics,
    which will provide built-in dashboards for Prometheus and Grafana itself. Click
    back to the Config tab, scroll down, and click on the Add button to set up the
    Prometheus data source. You should see Data source is working when you add it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/9c2bdd4a-b03e-4fdb-a19f-4cd7946cef4b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, you can navigate to the built-in dashboards and see some of the information.
    The top of the web page user interface consists of pull-downs, the top left leading
    to overall Grafana configuration and the next listing the dashboards you have
    set up, which generally starts with a Home dashboard. Select the Prometheus Stats
    dashboard that we just imported, and you should see some initial information about
    Prometheus:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/8fc15a8e-3e4b-4a7b-ac73-78c26e24e863.png)'
  prefs: []
  type: TYPE_IMG
- en: The Grafana project maintains a collection of dashboards that you can search
    through and either use directly, or use for inspiration to modify and create your
    own. You can search through the dashboards that have been shared—for example,
    limit it to dashboards sourced from Prometheus and related to Kubernetes. You'll
    see a variety of dashboards to browse through, some including screenshots, at [https://grafana.com/dashboards?dataSource=prometheus&amp;search=kubernetes](https://grafana.com/dashboards?dataSource=prometheus&search=kubernetes).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can import these into your instance of Grafana using the dashboard number.
    For example, dashboards 1621 and 162 are common dashboards for monitoring the
    health of the overall cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ee7702a3-ee8c-4556-9a53-292187d189f7.png)'
  prefs: []
  type: TYPE_IMG
- en: The best value of these dashboards is to show you how to configure your own
    graphs and make your own dashboards. Within every dashboard, you may select the
    graphs and choose Edit to see the queries used and display choices made, and tweak
    them to your values. Every dashboard can also be shared back to the Grafana hosting
    site, or you can view the JSON that is the configuration and save it locally.
  prefs: []
  type: TYPE_NORMAL
- en: The work going on with the Prometheus operator is working towards making it
    easier to bring up Prometheus and Grafana together, pre-configured and running
    to monitor your cluster and the applications within your cluster. If you are interested
    in trying it out, see the project README that is hosted by CoreOS at [https://github.com/coreos/prometheus-operator/tree/master/helm](https://github.com/coreos/prometheus-operator/tree/master/helm),
    which can also be installed with Helm.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have Grafana and Prometheus installed, you may use them to follow
    a similar process to determine the CPU and memory utilization of your own software,
    while running load tests. One of the benefits of running Prometheus locally is
    the ability it provides to collect metrics about your application.
  prefs: []
  type: TYPE_NORMAL
- en: Using Prometheus to view application metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While you could add a job within Prometheus to include the configuration to
    scrape Prometheus metrics from a specific endpoint, the installation we did earlier
    includes a configuration that will update what it is looking at dynamically, based
    on annotations on pods. One of the benefits of Prometheus is that it has support
    for automatically detecting changes in your cluster, based on annotations, and
    it can look up the endpoints for the pods that back a service.
  prefs: []
  type: TYPE_NORMAL
- en: Since we deployed Prometheus using Helm, you can find the relevant configuration
    embedded within the `values.yaml` file. Look for the Prometheus job `kubernetes-service-endpoints`,
    and you will find both the configuration and some documentation of how it can
    be used. If you don't have the files locally, you can view this configuration
    at [https://github.com/kubernetes/charts/blob/master/stable/prometheus/values.yaml#L747-L776.](https://github.com/kubernetes/charts/blob/master/stable/prometheus/values.yaml#L747-L776)
  prefs: []
  type: TYPE_NORMAL
- en: This configuration looks for services within the cluster that have an annotation
     `prometheus.io/scrape` . If this is set to `true`, then Prometheus will automatically
    attempt to add that endpoint to the list of targets it is watching. By default,
    it will attempt to access the metrics at the URI `/metrics` and use the same port
    as the service. You can use additional annotations to change those defaults, for
    example, `prometheus.io/path = "/alternatemetrics"` will attempt to read the metrics
    from the path `/alternatemetrics`.
  prefs: []
  type: TYPE_NORMAL
- en: By using the service as the means of organizing metric gathering, we have a
    mechanism that will automatically scale with the number of pods. Whereas in other
    environments you might have to reconfigure the monitoring every time you add or
    remove instances, Prometheus and Kubernetes working together seamlessly capture
    this data.
  prefs: []
  type: TYPE_NORMAL
- en: This capability allows us to easily expose custom metrics from our application
    and have them picked up by Prometheus. There are several ways this can be used,
    but the most obvious is simply getting better visibility on how your application
    is doing. With Prometheus collecting the metrics and Grafana installed as a dashboard
    tool, you could also use the combination to create your own application dashboards.
  prefs: []
  type: TYPE_NORMAL
- en: The Prometheus project supports client libraries in a wide variety of languages
    that make it easier to collect and expose metrics in its format. We will use some
    of these libraries to show you how to instrument our Python and Node.js examples.
    Before you dive in to directly using these libraries yourself, it is very worthwhile
    reading the documentation that the Prometheus project provides on how to write
    metric exporters, and their expected conventions for metric names. You can find
    this documentation at the project website: [https://prometheus.io/docs/instrumenting/writing_exporters/](https://prometheus.io/docs/instrumenting/writing_exporters/).
  prefs: []
  type: TYPE_NORMAL
- en: Flask metrics with Prometheus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can find the library to expose metrics from Python at [https://github.com/prometheus/client_python](https://github.com/prometheus/client_python) and
    can install it using `pip` with this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Depending on your setup, you may need to use `**sudo pip install prometheus_client**` to
    install the client with `pip`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our `flask` example, you can download the full sample code from [https://github.com/kubernetes-for-developers/kfd-flask](https://github.com/kubernetes-for-developers/kfd-flask)
    from branch 0.5.0\. The commands to get this updated sample are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'If you look within `exampleapp.py`, you can see the code where we use two metrics,
    a histogram and a counter, and use the flask framework to add in callbacks at
    the beginning of a request, and the end of a request, and capture that time difference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The library also includes a helper application to make it very easy to generate
    the metrics to be scraped by Prometheus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'This code has been made into a container image, `quay.io/kubernetes-for-developers/flask:0.5.0`.
    With those additions in place, we only need to add the annotation to `flask-service`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Once deployed with `kubectl apply -f deploy/` from the example's directory,
    the service will be backed by a single pod, and Prometheus will begin to pick
    it up as a target. If you use the `kubectl proxy` command, you can see the specific
    metric response that this generates. In our case, the pods name is `flask-6596b895b-nqqqz`,
    so the metrics can be easily queried at `http://localhost:8001/api/v1/proxy/namespaces/default/pods/flask-6596b895b-nqqqz/metrics`.
  prefs: []
  type: TYPE_NORMAL
- en: 'A sample of those metrics is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: You can see the metrics named `flask_request_latency_seconds` and `flask_request_count`
    in this sample, and you can query for those same metrics in the Prometheus browser
    interface.
  prefs: []
  type: TYPE_NORMAL
- en: Node.js metrics with Prometheus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'JavaScript has similar client libraries to Python. In fact, it is even easier
    to instrument Node.js express applications with the use of `express-prom-bundle`,
    which in turn uses `prom-client`. You can install this library for your use with
    this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'You can then use it in your code. The following will set up a middleware for
    express:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'And then, you simply include the middleware, as you''re setting up this application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The example code at [https://github.com/kubernetes-for-developers/kfd-nodejs](https://github.com/kubernetes-for-developers/kfd-nodejs)
    has these updates, and you can check out this code from branch 0.5.0 with the
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Like the Python code, the Node.js example includes updating the service with
    the annotation `prometheus.io/scrape: "true"`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Service signals in Prometheus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can tell the health and status of your service with three key metrics. It
    has become relatively common for service dashboards to instrument and build on
    these metrics as a baseline for understanding how your service is running.
  prefs: []
  type: TYPE_NORMAL
- en: 'These key metrics for a web-based service are:'
  prefs: []
  type: TYPE_NORMAL
- en: Error rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Response time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Throughput
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Error rate can be gathered by using the labels within the `http_request_duration_seconds_count`
    metric, which is included from `express-prom-bundle`. The query we can use in
    Prometheus. We can match on the format of the response code and count the increase
    in the number of 500 responses versus all responses.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Prometheus query could be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: With little load on our own example services and probably no errors, this query
    isn't likely to return any data points, but you can use it as an example to explore
    building your own error response queries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Response time is complex to measure and understand, especially with busy services.
    The reason that we typically include a histogram metric for the time it takes
    to process the request is to be able to view the distribution of those requests
    over time. Using a histogram, we can aggregate the requests across a window, and
    then look at the rate of those requests. In our preceding Python example, the
    `flask_request_latency_seconds` is a histogram, and each request gets label with
    where it was in the histogram bucket, the HTTP method used, and the URI endpoint.
    We can aggregate the rates of those requests using those labels, and look at the
    median, 95^(th), and 99^(th) percentile with the following Prometheus queries:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Median:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '95^(th) percentile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '99^(th) percentile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Throughput is about measuring the total number of requests over a given timeframe,
    and can be derived directly from `flask_request_latency_seconds_count` and viewed
    against endpoint and method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced Prometheus and showed how to install it, used
    it to capture metrics from your Kubernetes cluster, and showed how to install
    and use Grafana to provide dashboards, using the metrics captured and temporarily
    stored in Prometheus. We then looked at how you can expose custom metrics from
    your own code and leverage Prometheus to capture them, along with a few examples
    of metrics you might be interested in tracking, such as error rate, response time,
    and throughput.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we continue to look into observability for our applications
    with tools to help us capture logs and traces.
  prefs: []
  type: TYPE_NORMAL
