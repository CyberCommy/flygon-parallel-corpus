- en: Chapter 12. Cloud and DevOps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We did not want to end the book without talking about Cloud and DevOps functions.
    Having a server in house is not a good solution when Cloud platforms exist; so,
    in this chapter, you will understand why you should use Cloud for your application
    and which provider is the best for your requirements. Also, you will learn how
    to deploy your application into these Cloud platforms using automated tools.
  prefs: []
  type: TYPE_NORMAL
- en: The DevOps' role is closely related to the Cloud, so we will go through this
    subject and what the DevOps tasks are.
  prefs: []
  type: TYPE_NORMAL
- en: What is Cloud?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The fastest way to explain what we know as Cloud is by saying that the Cloud
    is the delivery of online services hosted on the Internet, but we can also say
    that Cloud allows us to consume digital resources in a very easy way. Some common
    Cloud services used these days are disk storage, virtual machines or TV services
    among others. As you can imagine, the main benefit of the Cloud is that we do
    not need to build and maintain these infrastructures at home.
  prefs: []
  type: TYPE_NORMAL
- en: As developers, you will know that Cloud is a good approach for our applications.
    Let's take a look at some advantages.
  prefs: []
  type: TYPE_NORMAL
- en: Autoscalable and elastic
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When your application is online, it is impossible to predict whether the traffic
    will be very high in a few months or even a few days. Cloud allows us to have
    an autoscalable infrastructure that matches the traffic or consumption resources
    of our application. It can grow if your traffic is higher or decrease if your
    application does not have the traffic you hoped for.
  prefs: []
  type: TYPE_NORMAL
- en: 'Usually, there are three options when you want to resize the servers. In the
    next picture we will be showing you graphically the different options you have
    to resize your servers. The yellow line is the maximum load your application can
    manage and the blue line the current load of your site:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Picture 1**: Use more servers than you need in order to avoid traffic problems
    when peak traffic occurs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Picture 2**: Use enough servers for normal traffic. You should know that
    it is possible to have problems on specific days. For example, if your application
    is an online shop, problems may arise on days like Black Friday.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Picture 3**: Use an elastic Cloud; it increases and decreases by adding or
    removing servers automatically depending on the peak traffic so that you always
    have the infrastructure you need.![Autoscalable and elastic](graphics/B06142_12_01.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resizing ways
  prefs: []
  type: TYPE_NORMAL
- en: Lower management efforts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you lose time setting up your server and performing maintaining tasks, you
    are losing time that you could use on improving your application. Cloud allows
    us to just focus on our application because it provides us with a new way to develop
    applications, providing preconfigured resources that allow us to develop applications
    without worrying about the infrastructure we are working on.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, Cloud usually provides us with a complete and useful dashboard
    to manage the machines, so we do not need to use an SSH console anymore, making
    our tasks easier. It even provides us with better ways to manage our databases
    and load balancers or certificates.
  prefs: []
  type: TYPE_NORMAL
- en: Cheaper
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Using Cloud is cheaper than having the servers at home. These savings are because
    of the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: You are only paying for the infrastructure you need all the time, so you do
    not need to change your machines when the traffic on your application grows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The IT guys (in case you need them) will be more productive because they will
    only focus on problems that Cloud cannot help with
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be aware when you pay for a Cloud server that you are not only paying for the
    servers; this server also includes the storage, an operating system, virtualization,
    the physical space, updates, a cooling system, and many other things, such as
    energy or data center operations.
  prefs: []
  type: TYPE_NORMAL
- en: Grow faster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This point is closely linked to the last one. If your application is new and
    you do not know if it will be successful, it is not a good idea to buy physical
    servers and all the related things in order to put your application online.
  prefs: []
  type: TYPE_NORMAL
- en: Using Cloud, you can pay monthly for the server you need and if the application
    does not go as expected, you can reduce the plan or even close it and you will
    spend less money.
  prefs: []
  type: TYPE_NORMAL
- en: Also, saving money at the beginning will allow you to grow faster, paying attention
    and putting money into the application instead of spending money on hardware.
  prefs: []
  type: TYPE_NORMAL
- en: Time to market
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you want to test new ideas and put them online, it will be faster. This is
    the main Cloud advantage and it is very precious on the internet. For big companies,
    it is very difficult to go as fast as small companies and Cloud allows them to
    include changes online in an easy and fast way, making this a very competitive
    advantage.
  prefs: []
  type: TYPE_NORMAL
- en: Select your Cloud provider
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Choosing the best Cloud provider is not an easy thing, but you can check your
    application needs in order to select the provider that suits it best.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following things:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ensure that your provider knows your needs**: The communication between your
    team and your Cloud provider is essential. It is very important that your provider
    knows things such as read/write number per second, where the users are from if
    there are concurrent users, how your deploy scripts work, or what your development,
    staging, and production environments are like.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Where is my data**: The Cloud servers are located somewhere, so it is important
    to know where they are because if you store customers'' data, the law may not
    allow you to store data in some countries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security**: If your application is not secure, you are at risk all the time,
    so it is good to know what protection systems your Cloud provider has, such as
    firewalls or how they isolate the hardware, in order to avoid intrusions and whether they
    offer 24-hour support.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test it before moving your application**: Your Cloud provider may allow you
    to test the service before moving the entire application, so use this option in
    order to check whether the servers will be enough for your traffic and resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Web Services (AWS)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The internet giant Amazon.com has its own Cloud. It provides web hosting and
    also, many other services to help companies. The most important feature on **Amazon
    Web Services** (**AWS**) is the load balancer and the possibility of sending some
    of your application's tasks (such as processing data or web hosting) to Amazon.
  prefs: []
  type: TYPE_NORMAL
- en: To sum up, AWS is not just a simple Cloud, it includes many services (more than
    50) for web experts and other people that require specific features that Amazon
    offers.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon gives us some different price plan options depending on the time we use
    it for--price per hour, per year, or even 3 years are the possibilities of AWS.
  prefs: []
  type: TYPE_NORMAL
- en: An important thing on the Cloud is the **Service Level Agreements** (**SLA**).
    For Amazon, these include a 99.95% of uptime guarantee with monthly cycles.
  prefs: []
  type: TYPE_NORMAL
- en: The customization on AWS works like templates; in other words, at the moment
    a full configuration of CPU, RAM, and space for your application is not possible;
    you should choose between some template options, so if you only need more RAM,
    you can not just upgrade the RAM, you should choose a different template that
    can upgrade the CPU and hard disk too.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, the Cloud servers are in many countries around the world. Amazon EC2
    (the servers of AWS) are currently located in North America (16), South America
    (3), Europe (7), Asia (14).
  prefs: []
  type: TYPE_NORMAL
- en: It may be worth noting that, at the moment and according to general consensus,
    AWS gives us the worst cost-benefit ratio in bandwidth and processing power among
    others. It's still the most feature complete solution out in the market, but it
    might not be the best fit for you.
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft Azure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Azure is the Microsoft operating system that provides us with an environment
    to execute and deploy applications and services on the Cloud. It provides us with
    a custom environment and servers located on the Microsoft data centers.
  prefs: []
  type: TYPE_NORMAL
- en: The applications we store on Azure should work on Windows Server 2008 R2, and
    they can be developed on .NET, PHP, C++, Ruby, or Java. Also, Azure provides us
    with some database mechanisms, such as NoSQL, blobs, message queues, and NTFS
    drives to read/write disk operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main advantages of Windows Azure are as listed:'
  prefs: []
  type: TYPE_NORMAL
- en: It reduces operation costs and provisioning on the applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fast response to customer need changes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scalability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure gives us different ways of paying for the services, you can pay for hour
    fractions or you can make yearly payments. The SLA of Azure is the same as Amazon
    and includes a 99.95% of up time guarantee with monthly cycles; the customization
    works with templates too.
  prefs: []
  type: TYPE_NORMAL
- en: Azure Cloud servers are currently located in North America (9), South America
    (1), Europe (6), Asia (9), and Australia (2).
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, Amazon and Azure are very similar; the main difference between
    them is the operating system used. If your application is developed on .NET or
    requires Windows servers, Azure is the best option.
  prefs: []
  type: TYPE_NORMAL
- en: Rackspace
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Rackspace is not one of the biggest ones (such as Amazon or Microsoft), but
    it is considered as one that we should mention when we talk about Cloud services.
  prefs: []
  type: TYPE_NORMAL
- en: When we hire Rackspace, we are paying to use the service, for example, when
    we need to increment the capacity of our application at a specific moment. The
    Rackspace servers are administered by them and it is even possible to hire only
    the support system, having our servers outside Rackspace.
  prefs: []
  type: TYPE_NORMAL
- en: Rackspace gives us the option of paying for less than an hour of time, yearly,
    or even for 3 years. The SLA is 99.90% uptime guarantee with monthly cycles, and
    the customization works using templates, like Amazon and Azure do.
  prefs: []
  type: TYPE_NORMAL
- en: The servers are currently located in North America (3), Europe (1), Asia (1),
    and Australia (1).
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, Rackspace is cheaper than Amazon or Azure, and it is a very good
    solution to start working with Cloud. Also, it has a very good distributed DNS
    and they are the creators of **OpenStack**, an open source stack of different
    software components used to implement the Cloud servers through virtualization.
    This offers a new dashboard, add-on services with databases, server monitoring,
    block storages, and creation of virtual networks.
  prefs: []
  type: TYPE_NORMAL
- en: DigitalOcean
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**DigitalOcean** is the second biggest hosting company in the world. Their
    plans are the cheapest ones and the DigitalOcean community is really good--they
    have forums for developers and a lot of tutorials about administration servers.'
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to choose the option of paying hourly or monthly. Also, the SLA
    is 99.99% uptime guarantee, even better than Amazon or Azure, and the customization
    works using templates, just like Amazon, Azure, and Rackspace do.
  prefs: []
  type: TYPE_NORMAL
- en: They currently have five servers in North America, five in Europe, and one in
    Asia.
  prefs: []
  type: TYPE_NORMAL
- en: DigitalOcean is a good solution for experts because they do not administer the
    servers. The servers are always Linux, so this is not a solution for projects
    that require Windows. Also, an advantage of using DigitalOcean is that if your
    project grows, it is possible to easily and quickly scale your server.
  prefs: []
  type: TYPE_NORMAL
- en: Joyent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Samsung bought **Joyent**. This Cloud has great potential. It was created to
    compete with Amazon EC2, and it had some important customers, such as Twitter
    and LinkedIn.
  prefs: []
  type: TYPE_NORMAL
- en: Joyent created Node.js and they have the best technology for containers; it
    was inherited from Solaris and implemented on their own operating system, **SmartOS**,
    (an operating system designed for the Cloud). If you are looking for the best
    performance and you do not care about the price, Joyent is your best friend.
  prefs: []
  type: TYPE_NORMAL
- en: You can choose the option of paying hourly, yearly, or even every 3 years. Also,
    the SLA is 100% uptime guarantee with 30 minutes cycles, the best one. The customization
    works using templates, just like Amazon, Azure, Rackspace, or DigitalOcean do.
  prefs: []
  type: TYPE_NORMAL
- en: They have three servers in North America and one in Europe at the moment.
  prefs: []
  type: TYPE_NORMAL
- en: Rackspace and Joyent have an open source infrastructure, so it is possible to
    download and use it on your own machine.
  prefs: []
  type: TYPE_NORMAL
- en: Google Compute Engine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Google Compute Engine** is a complete product that includes infrastructure
    and service, allowing us to execute virtual machines with Linux on the same infrastructure
    as Google works.'
  prefs: []
  type: TYPE_NORMAL
- en: The Google Compute Engine dashboard could not be better. It is clean and easy
    to navigate through. Also, it is very fast during the deployment and the scalability
    process, and the tools included on this Cloud make Google Compute Engine a good
    solution for analytics and big data.
  prefs: []
  type: TYPE_NORMAL
- en: For Google Compute Engine, the SLA is 99.95% uptime guarantee with monthly cycles.
    It allows us to store up to seven snapshots for free.
  prefs: []
  type: TYPE_NORMAL
- en: They have nine servers in North America, three in Europe, and six in Asia at
    the moment.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying your application to the Cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout the book, we were working with containers; we already told you how
    beneficial they are for your projects. Now, it's time to deploy your application
    to the Cloud. There are different providers out there, and we gave you some hints
    on how to choose the best provider for your project. In this section, we will show
    you some interesting options you have to orchestrate and manage your containers
    in production.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We were playing with Docker and their **Docker Engine** throughout the book.
    With the Docker engine, we are able to spin up and down the containers we use
    in our application. As you imagine, you can install the Docker Engine in your
    production server and use it like our development environment, but do you think
    that this approach is fault tolerant? Obviously the response is no. You can try
    to do some magic having multiple Docker Engines in different servers, but it will
    be hard to set up and maintain. Fortunately, Docker created Docker Swarm; this
    software provides you with native clustering capabilities and turns your group
    of Docker Engines into a single virtual Docker Engine. It will be like working
    in your development machine; the Swarm will be dealing with all the hard stuff
    for you, you only need to take care of your application.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we want you to have a global overview of Docker Swarm, listed are the main
    features of this tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Compatible with Docker tools**: Docker Swarm uses and provides the standard
    Docker API, so any tool that already uses the Docker API can use Docker Swarm
    to transparently scale to multiple hosts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High scalability and performance**: Like all the Docker software, Swarm is
    production ready and it was tested to scale up to one thousand (1,000) nodes and
    fifty thousand (50,000) containers. The results of those tests showed that you
    can achieve these high deploy numbers without performance degradation in the node
    cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Failover and high availability**: Docker Swarm is ready to manage failover
    and give you high availability. You can create multiple Swarm masters and specify
    your policy for leader election on master failures. At the time of writing this
    book, there is an experimental support for container rescheduling from failed
    nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexible container scheduling**: Swarm comes with a built-in scheduler that will
    be responsible for maximizing the performance and resource utilization of your
    infrastructure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pluggable schedulers and node discovery**: If the built-in scheduler that comes
    with Swarm does not fit well with your requirements, you can plug in external
    schedulers, such as **Apache Mesos** or **Kubernetes**. To fulfill all the autodiscovery
    requirements of your application, you can choose between the different available
    methods in Swarm: a hosted discovery service, a static file, Consul, or Zookeeper.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing Docker Swarm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: From Docker 1.12 and higher, the Swarm mode comes out of the box, so installing
    it is very easy. You only need to follow the steps we showed you in the [Chapter
    2](ch02.html "Chapter 2. Development Environment"), *Development Environment*,
    but instead of performing them on your development machine, you need to perform them
    on your production servers. We recommend using Linux/Unix in your production nodes,
    so all the steps we are describing here are for a Linux/Unix system.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will build a Swarm cluster so, before you continue reading, ensure that
    you have the following requirements ready:'
  prefs: []
  type: TYPE_NORMAL
- en: A minimum of three host machines with Docker Engine 1.12 (or higher) installed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the machines will be the manager machine, so ensure that you have all
    the IP addresses of your hosts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open ports between your hosts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TCP port 2377**: This port is used for our cluster management messages'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TCP and UDP port 7946**: These ports are used for the communication between
    our nodes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TCP and UDP port 4789**: These ports are used for overlaying network traffic'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In summary, our production environment will be composed of the following hosts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Manager node**: This is responsible for all the heavy work of orchestration
    and scheduling. We will call this machine `manager_01`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Two workers**: These are dummy nodes we will use to host our containers.
    We will call these hosts `worker_01` and `worker_02`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we mentioned before, you need to know the IP addresses of your different
    Docker hosts and the most important one is the IP address of the manager machine.
    The workers will be connecting to this IP address to know what they need to do.
    For example, imagine that our manager host has the `192.168.99.100` IP.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, you are ready to set up your Swarm cluster. First of all, ensure
    that the Docker Engine is running in all your nodes. Once you have checked that
    the engine is running in your hosts, you need to enter your `manager_01` node
    through SSH or console. In your `manager_01` node, run the following command to
    start the Swarm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command will initialize Swarm and establish the current node
    as the manager. It also gives you the commands you need to run to add more managers
    or to add workers to the cluster. The output of your `init` command should be
    similar to the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you have your Swarm initialized, you need to get a token to be used to
    join other machines to the cluster. To get this token, you only need to run the
    following command at any moment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Imagine that you need to add a new `worker` to the swarm; in this case, you
    only need to get the token from the preceding command and run the following one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command will add a `worker` to the Swarm, and if you need to add
    another manager to this cluster, you can run `docker swarm join-token manager` and
    follow the instructions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In theory, the init command started the Swarm; if you want to check the correct
    cluster initialization, you only need to execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding `info` command will give you some output similar to the following
    one; note that we removed some information to fit it in the book:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, we have our Swarm active and ready; if you want to get more
    information about the nodes, you only need to execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command will give you an output similar to the following one.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: At this point, you have one host, which is your manager node, but you do not
    have any workers where you can spin up your containers. Let's add some worker
    nodes to our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Adding a worker node to our cluster is very easy with Swarm--you only need
    to access the host through SSH or console and run the command output by your swarm
    init on the manager node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'If everything went fine, the preceding command will give you the following
    output that indicates that the current node was added to the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Imagine that you didn''t save the join token; you can obtain it again from
    your manager node. You only need to login in the manager node and execute the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command will give you the command you need to run in your workers
    nodes again.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, it is very easy to add nodes to your cluster; add all the remaining
    workers. To check the status of your cluster nodes, you can execute the following
    command in your manager node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Adding services to our Swarm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At this point, you will have your production environment ready for deployments;
    let's deploy something to test the new environment. We will deploy a very simple
    image that does a ping to `google.com`. As soon as you feel comfortable deploying
    services to the Swarm, you can give it a try and deploy our example application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a connection or login in your manager node and run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command creates a new service in the cluster, with the `--name`
    flag we are assigning a pretty name for our service, in this case `pingtest`.
    The `--replicas` parameter indicates the number of instances we want for our service;
    in our example we specified only one instance. With `alpine ping google.com`,
    we are telling our Swarm which image we want to use (`alpine`) and the command
    we want to execute in this image (`ping google.com`).
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, it was very easy to deploy the new testing service. If you
    want to see which services are running in your cluster, login in your manager
    node and execute `docker service ls`, the output will be similar to the following
    one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you have your service running in production, at some point you will need
    to have more information about the service. It is very easy with Docker, you only
    need to execute the following command in your manager node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Your output can be similar to the following one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want the output to be returned as a JSON, you only need to remove the
    `--pretty` parameter from the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command will be similar to the next one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the JSON format has more information; feel free to use the option
    that is more suitable for you. In case you need to know where your service is
    running, you can do a `docker service ps pingtest`, as always, in your manager
    node.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling your services in Swarm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We showed you how easy it is to create new services in your Swarm cluster,
    now it''s time to let you know how you can scale your services up/down. Go to
    your manager node and run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command will create (or destroy) the required amount of containers
    to adjust it to your desire; in our case, we want 10 containers for our `pingtest`
    service. You can check the correct execution of the command with `docker service
    ps pingtest`, giving you the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Scaling your services in Swarm](graphics/12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding output, you can check that the service is running in 10 containers
    and also, you can check in which node it is running. In our case, they are all
    running in the same host as we only added one node to our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: You now know how to create your Swarm cluster and how you can easily start new
    services, now it's time to show you how you can stop any running service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Connect to your manager node and execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command will remove the `pingtest` service from your Swarm cluster.
    As always, you can check if the service was stopped with the `docker service inspect
    pingtest` command or by checking the running containers with the usual `docker
    ps`.
  prefs: []
  type: TYPE_NORMAL
- en: At this point in the chapter, you will be able to create a Swarm cluster and
    spin up any service; give it a try and move our example application to use Swarm.
  prefs: []
  type: TYPE_NORMAL
- en: As you can imagine, we like how easy Docker makes your development cycle and
    how simple the deployment can be, but there are other projects out there that
    you can use in your production environment. Let's look at the most commonly used
    ones these days so that you can choose which option is better for your project.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Mesos and DC/OS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Apache Mesos abstracts all the compute resources from machines, enabling fault-tolerant
    and elastic distributed systems. Creating an Apache Mesos distributed system can
    be complicated, so Mesosphere created DC/OS, an OS built on top of Apache Mesos.
    Thanks to DC/OS, you can have all the power of Mesos but it's easier to install
    or manage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the features available in Apache Mesos and, of course, in DC/OS are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear scalability**: You can scale up to 10,000 hosts without any problems'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High availability**: Mesos uses Zookeeper to provide a fault-tolerant replicated
    master and agents'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Containers support**: Native support for launching containers with Docker
    and AppC images'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pluggable isolation**: Isolation support for CPU, memory, disk, ports, GPU,
    and modules for custom isolation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**APIs and Web UI**: The built-in APIs and Web UI allow you to easily manage
    any aspect of Mesos'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cross Platform**: You can run Mesos in Linux, OSX, and even Windows'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, Apache Mesos and DC/OS are an interesting alternative to Docker
    or Kubernetes. Those projects unify all your resources segregated between all
    your nodes and transform them into one distributed system. It gives you the impression
    that you are only managing a single machine.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubernetes is one of the mainstream open source systems used for automating
    deployment, scaling, and management of your containers. It was created by Google
    and it has a vibrant community.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is a full orchestration service and among all the features it has, we can
    highlight the following ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Self-healing**: This is an interesting feature that restarts failed containers
    and replaces and reschedules containers when any of your nodes die. It is responsible
    for killing unhealthy containers and also avoids advertising containers that are
    not ready.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Service discovery and load balancing**: This feature allows you to forget
    about creating and managing your own discovery service; you can use what comes
    out of the box. Kubernetes also gives each container its own IP address and a
    DSN name for a set of containers. Thanks to all of this, you can do load-balancing
    easily.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automated rollouts and rollbacks**: There is nothing more critical than rollouts
    and rollbacks. Kubernetes can manage these actions for you; it will monitor your
    application to ensure that it continues running smoothly while you are doing a
    rollout/rollback.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Horizontal scaling**: You can scale your application up or down with a single
    command, using a user interface or even define some rules to do it automatically.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automatic binpacking**: Kubernetes takes care of how to place your containers
    based on their resource requirements and other constraints. The decision is taken
    by trying to maximize the availability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, Kubernetes comes with most of the features needed in big projects
    out of the box. For this reason, it is one of the most used systems for containers
    orchestration. We recommend that you investigate more about this project. You
    can find all the information you need on the official page. If you have a specific
    question that you can't find the response in the official documentation for, the
    big community behind this project can help you.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying to Joyent Triton
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Earlier, we showed you how you can build your Swarm cluster. It is an interesting
    way of managing your infrastructure, but what happens if you need all the power
    of the Cloud but without the inconvenience of dealing with the orchestration?
    In the following example, we assume that you don't have the budget or time to
    set up your Cloud servers with the orchestration software of your choice.
  prefs: []
  type: TYPE_NORMAL
- en: At the beginning of the chapter, we talked about the major Cloud providers and,
    among all of them, we talked about Joyent. This company has a hosting solution
    called **Triton**; you can use this solution to create VMs or containers with
    a single click or an API call.
  prefs: []
  type: TYPE_NORMAL
- en: The first thing you need to do if you want to use their hosting services is
    to create an account on their [https://www.joyent.com](https://www.joyent.com) page.
    Once you have your account ready, you will have full access to their environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once your account is ready, add an SSH key to your account. This key will be
    used to authenticate you against your containers and the Joyent''s API. If you
    do not have an SSH key to use, you can create one manually. It is very easy to
    create a SSH key, for example, in Mac OS you only need to execute the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This command will ask you some questions about where the key will be stored
    or the passphrase you want to use to secure your key. Once you answer all the
    questions, usually the key will be stored in the `~/.ssh/id_rsa.pub` file. You
    only need to copy the content of this file to your Joyent's account. If you are
    using Linux, the process of creating a SSH key is pretty similar.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have your account ready, you can start creating containers; you can
    do it using their web UI, but in our case we will show you how to do it from your
    terminal.
  prefs: []
  type: TYPE_NORMAL
- en: 'We were using Docker and Joyent with the Docker API implemented in Triton,
    so you will see how easy it is to make your deployments. The first thing you need
    to do is install the Triton CLI tool; this application was built on Node.js, so
    you need Node.js ([https://nodejs.org](https://nodejs.org)) installed on your
    development machine. Once you have node, you only need to execute the following
    command to install the Triton CLI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command will install Triton as a global application on your machine.
    As soon as Triton is available on your computer, you need to configure the tool;
    enter the following command and answer all the questions asked:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, your Triton CLI tool will be ready to be used. Now, it is time
    to configure Docker to use Triton. Open your terminal and execute the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command will configure your local Docker to use Triton. From
    now on, all your Docker commands will be sent to Triton. Let''s try to deploy
    our example application--go to the location of your `docker-compose.yml` file
    and execute the next command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command will work like always but, instead of using our development
    machine engine, it will spin up our containers in the Cloud. One of the advantages
    of Triton is that they assign at least one IP address to each container, so if
    you need to get the IP address of a specific container you only need to execute `docker
    ps` to get all the containers running (in Triton) and their names. Once you have
    the name of the container, you only need to execute the following command to obtain
    the IP address from the Triton CLI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command will give you the IP address of the container you have
    chosen. Another way of obtaining the IP is from the web UI.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A docker-compose stop will kill all the containers deployed to the Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Now you can use everything you have learned in the book and deploy to your Joyent
    Triton Cloud without too many problems. This is probably the easiest way of deploying
    your Docker containers in the market right now. Working with Joyent Triton is
    like working in local.
  prefs: []
  type: TYPE_NORMAL
- en: Be aware that this is not the only option you have to deploy Docker in production;
    we only showed a simple and easy way. You can try other options, such as CoreOS
    and Mesosphere (DC/OS), among others.
  prefs: []
  type: TYPE_NORMAL
- en: What is DevOps?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DevOps is a set of practices that emphasizes the collaboration and communication
    between development and operations (IT). The main goal is to establish a culture
    of a rapid, frequent, and more reliable way of releasing software. To achieve
    this goal, the DevOps usually try to automate as much as they can. If your project
    (or company) grows, at some point, you will put some DevOps principles in place
    to secure the future of your application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the technical benefits of adopting this culture in your organization
    can be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Aims to maximize the use of **Continuous Integration** and **Continuous Delivery**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduces the complexity of the issues to fix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduces the number of failures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provides a faster resolution of problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main pillar of DevOps is the culture of communication between all parts
    involved in the development of your application, especially between development
    and operations guys. It doesn't matter how nice or amazing your application is,
    a lack of communication inside your organization can shut down your entire project.
  prefs: []
  type: TYPE_NORMAL
- en: Once your organization has a really good communication channel between all the
    parts involved in the development of your software, you can analyze and put the
    next pillar of the DevOps culture--the automation--in place. To create a reliable
    system, you need to invest in the automation of repetitive manual tasks and processes;
    this is where continuous integration and continuous delivery come in. Creating
    your CI/CD pipeline will help you automate repetitive tasks like unit tests or
    the deploy, improving the overall quality of your application. It will even help
    you save some time in your day-to-day tasks. Imagine that deploying your application
    manually to your production environment takes an average of 8 minutes each time;
    if you deploy at least once a day, you will be wasting more than 30 hours in a
    whole year only making deploys.
  prefs: []
  type: TYPE_NORMAL
- en: 'DevOps is all about your application and the process around the development
    and deployment, and the core principles are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Agile software development
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous integration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous delivery pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automated and continuous testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Proactive monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improved communication and collaboration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, the DevOps culture is not something you can implement in a few
    hours in your organization, it is a long process in which you need to analyze
    how the development process in your organization works and the required changes
    you need to make to find a more flexible and agile way. We covered most of the
    core principles in this book; it is now up to you to fill the gaps and implement
    a DevOps culture in your organization.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we talked about what a Cloud is and what you need to know to
    choose your hosting provider. We also told you about the different options you
    have to orchestrate your application into the Cloud. Now, it is your turn to analyze
    and choose the best option for your project.
  prefs: []
  type: TYPE_NORMAL
