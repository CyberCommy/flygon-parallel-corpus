- en: Advanced Python Modules
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter will allow us to become familiar with certain advanced Python
    modules that are very handy when it comes to parameters such as response time,
    processing speed, interoperability, and sending data over the network. We will
    be looking at parallel processing in Python with the help of threads and processes.
    We will also read about establishing communication between processes with the
    help of IPC and subprocesses. After that, we will explore socket programming in
    Python and end by entering the domain of cybersecurity by implementing a reverse
    TCP shell. The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Multitasking with threads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multitasking with processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Subprocesses
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The basics of socket programming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a reverse TCP shell with Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multitasking with threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **thread** is a lightweight process that shares the same address and memory
    space as its parent process. It runs in parallel on the processor cores, thereby
    giving us parallelism and multitasking capabilities. The fact that it shares the
    same address and memory space as that of the parent process makes the whole operation
    of multitasking very lightweight, because there is no context switching overhead
    involved. In context switching, when a new process is scheduled to be executed,
    the operating system needs to save the state of the previous process, including
    the process ID, the instruction pointer, the return address, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a time-consuming activity. Since multitasking with threads doesn''t
    involve the creation of new processes to achieve parallelism, threads provide
    a very good performance in multitasking activities. Just as in Java we have the
    `Thread` class or the runnable interface to implement threads, in Python we can
    do this using the `Thread` module to implement threads. There are typically two
    ways to implement threads in Python: one in Java style and one that is more Pythonic.
    Let''s take a look at both.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows the Java-like implementation, where we subclass the
    threading class and override the `run()` method. We place the logic or task that
    we wish to run in parallel with the threads inside the `run()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have got a method (`run()`), which, in this case, is made to execute
    in parallel. This is what Python explores with its other method of threading,
    in which we can make any method execute in parallel with the help of threads.
    We can use any method of our choice and that method can take any arguments.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows the other way of using threading. Here, we
    can see that we defined an `add(num1,num2)` method normally and then used it with
    threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `for` loop creates a thread object `t`. On calling the `start()` method,
    the method specified in the target parameter while creating the thread object
    is invoked. In the preceding case, we have passed the `add()` method to the thread
    instance. The arguments that are to be passed to the method to be invoked with
    threads are passed under the `args` parameter as a tuple. The `add()` method is
    called five times via threads and the output is printed on the screen, as shown
    in the preceding example.
  prefs: []
  type: TYPE_NORMAL
- en: Demonic and non-demonic threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It must be noted that the thread is invoked from the main program and the main
    program will not exit (by default) until the thread is executed completely. The
    reason is that the main program invokes the thread by default in non-demonic mode,
    which makes the thread run in the foreground rather than wait for it to run in
    the background. Thus, a non-demonic thread is one that runs in the foreground,
    causing the main program to wait for the running threads to finish their execution.
    A demonic thread, on the other hand, is one that runs in the background, therefore
    not causing the main program to wait for it to finish its execution. Take a look
    at the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6e397ca8-b18b-4f3c-8ba6-924d43fa7c65.png)'
  prefs: []
  type: TYPE_IMG
- en: As can be seen from the preceding code snippet, when we create and execute a
    non-demonic thread (default), after printing `Main Ended`, the Terminal window
    halts for 4 seconds, waiting for the `ND` thread to finish its execution. When
    it finishes, we get an  `Exit Non Demonic` message, which is when the main program
    exits. Up until this point, the main program would not exit.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how this changes with demonic threads, which run in the background:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1282d560-dbce-4a16-afca-418123f2620f.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding code snippet, we saw how to make use of a demonic thread. Interestingly
    enough, the main program did not wait for the demonic thread to finish execution.
    The demonic thread ran in the background and by the time it finished, the main
    thread had already exited from the memory, and thus we did not see the `Exit :Daemonic` message printed
    on the screen. In this case, we are making use of the logging module. By default,
    the logging module will log to the `stdout`, which, in our case, happens to be
    the Terminal.
  prefs: []
  type: TYPE_NORMAL
- en: Thread joins and enumeration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we saw in the previous section, the main thread will, by default, wait until
    the thread is executed. Despite this, the code of the main method will still be
    executed, as the main thread will run on a different processor core to the child
    thread. There may be occasions in which we want to control the execution of the
    main thread, in line with the execution cycle of the child threads. Let's say
    that we want a portion of the code of the main thread to be executed only after
    the child threads are executed. This can be achieved with the help of the `join()`
    method. If we invoke this on a thread T from a main thread M at line X, then the
    line X+1 of the main thread will not be executed until the T thread has finished
    its execution. In other words, we joined the tail of the main thread with the
    thread T, and therefore the execution of the main thread will be halted until
    T is complete. Take a look at the following example, in which we use thread enumeration
    and `join()` to execute threads in batches of three.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main program must validate that all the threads have executed before exiting:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d55f6fa9-2ca8-4942-a604-9df8396064a2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot depicts the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bcdb3300-4256-4c30-8704-458809e8c604.png)'
  prefs: []
  type: TYPE_IMG
- en: Intercommunication between threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although threads are meant to be executed independently of each other, there
    are many occasions in which threads need to communicate with each other, such
    as if a thread needs to start a task only when another thread has reached a certain
    point. Let's say we are dealing with a producer and consumer problem, where one
    thread (the producer) is responsible for putting items in the queue. The producer
    thread needs to send a message to the consumer thread, so that it knows that it
    can consume data from the queue. This can be achieved with the help of thread
    events in Python. Invoking `threading.event()` returns an event instance, which
    can be set using the `set()` method and reset using the `clear()` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code block, we will see an example in which one thread will
    be incrementing a counter. The other thread is required to perform an action when
    the counter value reaches 5\. It must be noted that the event also has a `wait()`
    method, which waits until the event is blocked or set. The event can wait for
    a timeout interval, or it can wait indefinitely, but once the set flag is `true`,
    the `wait()` method will not actually block the execution of the thread. This
    is depicted in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1032f145-f810-4804-85e9-895ad8059ce6.png)'
  prefs: []
  type: TYPE_IMG
- en: Thread concurrency control
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are many occasions in which multiple threads need to share a resource.
    We want to ensure that if one thread is changing the state of an object, the other
    must wait. In order to avoid inconsistent results, a shared resource must be locked
    before changing its state. Once the state is changed, the lock should be released.
    Python provides thread locks to do this. Take a look at the following code snippet, `Thread_locking.py`
    , which demonstrates thread locking and concurrency control:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f5d301dc-49fd-4ef6-a601-f7f981c008b7.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding code snippet shows thread locking. Here, `count` is a shared variable
    that multiple threads try to update. The first output did not have the locking
    mechanism (lines 16 and 22 were commented out). When there is no locking in place,
    it can be seen that `thread_3` read the value as 1 when it acquired the lock and
    the same is the case with `thread_4`. Each thread increments the value of the
    count by 1, but by the end of `thread_4`, the value of the count is 3. It can
    be seen from the second output obtained when we make use of locking that while
    the shared resource `counter` is being updated, no other thread can actually read
    it, so the results obtained are consistent.
  prefs: []
  type: TYPE_NORMAL
- en: Multitasking with processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Like the threading module, the multiprocessing module is also used to provide
    multitasking capabilities. The threading module is actually a bit deceptive: its
    implementation in Python is not actually for parallel processing, but instead
    for processing on a single core with time-sharing. The default Python implementation
    **CPython**, at interpreter level, is not thread safe. Whenever threads are used,
    there is a **global interpreter lock** (**GIL**) that is placed over the objects
    that are accessed within Python threads. This lock executes the threads in time-sharing
    manner, giving a small quantity of time to every thread, and thus there is no
    performance gain in our program. The multiprocessing module was developed, therefore,
    to provide parallel processing to the Python ecosystem. This decreases the execution
    time by spawning the load across multiple processor cores. Take a look at the
    following code, which uses multiprocessing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code snippet represents two implementations of multiprocessing:
    a simple approach and a class-based approach.'
  prefs: []
  type: TYPE_NORMAL
- en: Demonic and non-demonic processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have already studied what demonic and non-demonic threads are. The same
    principle applies to processes as well. A demonic process runs in the background
    without blocking the main process, while a non-demonic process runs in the foreground.
    This is shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/75c7a70a-38f0-4f46-b35e-17593e3f939a.png)'
  prefs: []
  type: TYPE_IMG
- en: It can be seen from the preceding code snippet that when we create and execute
    a non-demonic process (the default option) as shown in output 1 and in line 20,
    after printing `Main Ended`, the Terminal window halts for 4 seconds while waiting
    for the non-demonic process to finish its execution. When it finishes, we get
    the `Exit Non Daemonic` message, which is when the main program exits. In the
    second case (shown in output 2), the main program does not wait for the demonic
    process to finish its execution. The daemonic process runs in the background and
    by the time it is finished, the main thread has already exited from the memory.
    For this reason, we did not see the `Exit :Daemonic` message printed on the screen.
  prefs: []
  type: TYPE_NORMAL
- en: Process joins, enumeration, and termination
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The same theory we saw relating to thread joins and enumeration can be applied
    to processes. The process can be joined to the main thread or to another process
    in such a way that another thread will not exit until the joined process finishes.
    On top of joins and enumeration, we can also explicitly terminate processes in
    Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at following code snippet, which demonstrates the preceding concepts.
    The objective of the following code is to spawn a few processes and make the main
    process wait for 10 seconds for the spawned processes to finish execution. If
    they do not finish, those that are still running will be terminated before exiting:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b26127f3-7bd5-4c9c-a991-e4171692dab8.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding code `Join_enumerate_terminate.py` is fairly simple; what we are
    doing is identical to what we did with threads previously. The only difference
    here is that we apply the join operation for only 3 seconds, so that we deliberately
    get some processes that are alive. We then kill those processes by applying `terminate()`
    on them.
  prefs: []
  type: TYPE_NORMAL
- en: Multiprocess pooling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the coolest features of multiprocessing libraries is **pooling**. This
    lets us distribute the tasks evenly across all the processor cores, without having
    to worry about the number of processes that are run actively at one time. This
    implies that this module has the ability to spawn a group of processes in a batch.
    Let's say that we define the batch size as 4, which is the number of processor
    cores we may have. This means that, at any time, the maximum number of processes
    that can be executed is four and if one of the processes completes its execution,
    meaning we now have three running processes, the module automatically picks the
    next set of processes to make the batch size equal to four again. The process
    will continue until we either finish our distributed task or we explicitly define
    a condition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the following example, where we are required to write 8 million
    records in eight different files (1 million records in each file). We have a four-core
    processor to carry out this task. Ideally, we need to spawn a batch of four processes
    twice, so that each process writes 1 million records in the file. Since we have
    four cores, we want each core to carry out a different part of our task. If we
    choose to spawn eight processes together, we would waste some time in context
    switching, so we need to use our processor and processing capabilities wisely
    to get the maximum throughput:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7fc29816-ce6c-46a5-ab31-5a75a7b60de3.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding code `Multiprocess_pool.py`, we are creating a multiprocessing
    pool at line 30\. We define the size of the pool as `size=mp.cpu_count()`, which
    in our case is `4`, so we are defining a pool of size four. We need to create
    eight files, each holding 1 million records. We use a `for` loop to define eight
    processes that would be sent to the pool object by invoking `apply_async()` on
    the created pool object. The `apply_async()` method expects the name of the method
    that we wish to execute as a process with multiprocessing module as an argument. The
    second argument is the parameters that are passed to the method that we wish to
    execute. Note that the process, when it gets executed with the pool module, also
    has the capability to return data from the method.
  prefs: []
  type: TYPE_NORMAL
- en: As can be seen from the output, at no time are there more than four processes
    being executed simultaneously. It can also be verified that the first process
    to finish is `Forkpoolworker4`. When the batch size is 3, another process is immediately
    spawned by the module. This can be verified by the output, which states `Started
    process Poolworker4` on the sixth line of section (1) .
  prefs: []
  type: TYPE_NORMAL
- en: Note that two batches are executed in parallel. Each process took 13 to 14 seconds,
    but since they executed in parallel, one on each core, the overall batch execution
    time for each batch was 14 seconds. For two batches, therefore, the total time
    was 28 seconds. It can be clearly seen that by using parallelism, we solved our
    problem in a mere 28 seconds. If we had gone for a sequential or thread approach,
    the total time would have been close to *(13*8) = 104* seconds. Try it yourself
    as an exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s take another example, to show another dimension of the power of
    the pool module. Let''s say that as a part of our requirements, we need to parse
    four of the 8 million files that are created, those whose ID `%1700` yields a
    zero. We must then combine the results across all the four files in a different
    file. This is a very good example of distributed processing and aggregation of
    results: the processes should not only read the files in parallel, they must also
    aggregate the results as well. It is somewhat similar to Hadoop''s map-reduce
    problem. In a typical map-reduce problem, there are two sets of operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Map**: This involves splitting a huge dataset across various nodes in a distributed
    system. Each node processes the chunk of data it receives.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reduce**: This is the aggregation operation, where the output of the map
    phase from each node is returned, and, depending on the logic, the results are
    finally aggregated and given back.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We are doing the same thing here, the only difference being that we are using
    processor cores in place of the nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ba23f995-6aaf-4f2f-ae31-75806d6206f2.png)'
  prefs: []
  type: TYPE_IMG
- en: As can be seen in the preceding code snippet, with the help of the `map()` method
    of the `Pool` module, we can make multiple processes work on different files in
    parallel and then combine all the results and send them as a single structure.
    The processes are executed in parallel and the records for which the `record_id
    %1700` returned us a zero are returned to us. Finally, we save the aggregated
    result in the `Modulo_1700_agg` file. This is a very powerful feature of the multiprocessing
    module, and can reduce the processing time and aggregation time by huge margin,
    if used properly.
  prefs: []
  type: TYPE_NORMAL
- en: Subprocesses
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Invoking an external process from another process is called **subprocessing**.
    In this case, the communication between the processes happens with the help of
    OS pipes. In other words, if a process A is invoked as a subprocess by a process
    B, then the process B can pass an input to it and also read the output from it
    via OS pipes. This module is crucial when it comes to automating penetration testing
    and invoking other tools and utilities with Python. Python provides a very powerful
    module called `subprocess` to handle subprocessing. Take a look at the following
    code snippet `Subprocessing.py`, which shows how to invoke a system command called `ls` using
    subprocessing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/efe81042-63ce-4ee0-a4ea-6fe512989192.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding code snippet, we used the `subprocess.Popen()` method to call
    the `subprocess`. There are few other ways to call or invoke the `subprocess`,
    such as `call()`, but the one we are discussing here is `Popen`. This is because
    the `Popen` method returns the process ID of the process that would be spawned,
    which, in turn, gives us good control over that process. The `Popen` method takes
    many arguments, the first of which is actually the command that is to be executed
    at OS level. The named arguments include `stderr=subprocess.PIPE`, which means
    that if the external program or script produces an error, that error must be redirected
    to the OS pipe, from which the parent process must read the error. The `stdout=subprocess.PIPE`
    suggests that the output that the subprocess would produce must also be sent over
    the pipe to the parent process. `shell=True` suggests that whatever command is
    given, the first argument must be treated as the `shell` command, and if it has
    some arguments, they must be passed as arguments of the process to be invoked.
    Finally, if we want our parent process to read the output and error produced by
    the child process, we must call the `communicate()` method on the invoked `subprocess`.
    The `communicate()` method opens the `subprocess` pipe and the communication starts
    with the subprocess writing to one end of the pipe and the parent process reading
    from the other. It must be noted that the `communicate()` method will make the
    parent process wait until the child process is finished. The method returns a
    tuple with the output at the 0th index and the std error at the 1st index.
  prefs: []
  type: TYPE_NORMAL
- en: 'It should be noted that we should never use `shell=True` in real-world examples,
    as this makes an application vulnerable to shell injection. Avoid using the following
    line:'
  prefs: []
  type: TYPE_NORMAL
- en: '`>>> subprocess.Popen(command, shell=True) #This would remove everything !!`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the following example, in which we will use `shell=False`. With
    `shell=False`, the command and  arguments to the process/command that we invoke
    must be passed separately as a list. Let''s try to execute `ls -l`  with `shell=False`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2b25936f-6483-4e7e-b78d-28918afffe3b.png)'
  prefs: []
  type: TYPE_IMG
- en: So this is how we execute external processes with Python, with the help of the
    subprocess module.
  prefs: []
  type: TYPE_NORMAL
- en: Socket programming basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we talk of sockets, we are referring to both the TCP and the UDP socket.
    A **socket** connection is nothing but a combination of the IP address and the
    port number. Every service that we can think of that runs on a port implements
    and uses sockets internally.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, our web server, which always listens on port `80` (by default),
    opens a socket connection to the outside world and binds to the socket with the
    IP address and the port `80`. The socket connection can be used in the following
    two modes:'
  prefs: []
  type: TYPE_NORMAL
- en: Server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Client
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When the socket is used as a server, the sequence of steps that the server
    performs is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a socket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bind to the socket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Listen at the socket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Accept connections.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Receive and send data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the other hand, when the socket connection is used as a client to connect
    to a server socket, the sequence of steps is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a socket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Connect to the socket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Receive and send data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Take a look at the following code snippet `server_socket.py`, which implements
    a TCP server socket at port `80`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c22814d3-51d0-4002-90b1-323f1a762121.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding case, we created a socket with the `socket.socket` statement.
    Here, `socket.AF_INET` represents the IPv4 protocol and `socket.SOCK_STREAM` suggests
    the use of stream-based socket packets, which are nothing but TCP streams. The
    `bind()` method takes a tuple as an argument, with the first argument being the
    local IP address. You should replace this with your personal IP, or `127.0.0.1`.
    The second parameter that is given to tuple is the port, which in turn calls the
    `bind()` method. We then start listening on the socket and finally start a loop
    where we accept client connections. Note that the method creates a single-threaded
    server, which means that if any other client connects, it has to wait until the
    active client is disconnected. The `send ()` and `recv()` methods are self-explanatory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now create a basic client socket code ,`client_socket.py`, that connects
    to the previously created servers and passes messages to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8a85859c-469a-48c9-a9ea-bfa613bc246f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The output produced by both the client and server sockets is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4f357662-4615-410d-8204-25603f3bd866.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is how we use a socket connection with UDP:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Reverse TCP shells with Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have understood the basics of subprocessing, multiprocessing, and
    so on, implementing a basic TCP reverse shell with Python is pretty straightforward.
    For this example, `rev_tcp.py`, we will be using the bash-based reverse TCP shell.
    In the later chapters of the book, we will see how to pass a reverse shell entirely
    with Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/91cc6614-9d64-417f-b396-ff81b57f0271.png)'
  prefs: []
  type: TYPE_IMG
- en: It should be noted that `OS.dup2` is used to create a duplicate of a file descriptor
    in Python. The `stdin` is defined to be file descriptor `0`, `stdout` is defined
    to be file descriptor `1`, and `stderr` is defined to be file descriptor `2`.
    The code line `OS.dup2(s.fileno(),0)` indicates that we should create a duplicate
    of `stdin` and redirect the traffic to the socket file, which happens to be on
    the localhost and port `1234` (where Netcat is listening). Finally, we invoke
    the shell in interactive mode and since we are not specifying the `stderr`, `stdin`
    and `stdout` parameters, by default, the parameters will be sent to `stdin` and
    `stdout` at system level, which is again mapped to the socket for the scope of
    the program. For this reason, the preceding code snippet will open the shell in
    interactive mode and pass it on to the socket. All input is taken from the socket
    as `stdin`, and all output is passed to the socket via `stdout`. This can be validated
    by looking at the output produced.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed some more advanced concepts of Python, which allow
    us to increase the throughput. We discussed multiprocessing Python modules and
    how they can be used to reduce the time taken and increase our processing capabilities.
    With this chapter, we have essentially covered everything that we would need from
    Python for us to step into the world of penetration testing, automation, and various
    cybersecurity use cases. It should be noted that, from here on, our emphasis will
    be on applying the concepts we have studied so far, with less explanation as to
    how they work. For this reason, if you have any doubts, I would strongly recommend
    that you clarify these before moving ahead. In the next chapter, we will talk
    about how can we use Python to parse PCAP files, automate Nmap scanning, and much
    more. For all security enthusiasts, let's get to business.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are other multiprocessing libraries that we can use with Python?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Where would threads become useful in Python, given that they actually execute
    on the same core?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Multiprocessing: [https://docs.python.org/2/library/multiprocessing.html](https://docs.python.org/2/library/multiprocessing.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Subprocesses: [https://docs.python.org/2/library/subprocess.html](https://docs.python.org/2/library/subprocess.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
