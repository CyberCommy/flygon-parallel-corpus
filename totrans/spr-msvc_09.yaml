- en: Chapter 9. Managing Dockerized Microservices with Mesos and Marathon
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In an Internet-scale microservices deployment, it is not easy to manage thousands
    of dockerized microservices. It is essential to have an infrastructure abstraction
    layer and a strong cluster control platform to successfully manage Internet-scale
    microservice deployments.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will explain the need and use of Mesos and Marathon as an infrastructure
    abstraction layer and a cluster control system, respectively, to achieve optimized
    resource usage in a cloud-like environment when deploying microservices at scale.
    This chapter will also provide a step-by-step approach to setting up Mesos and
    Marathon in a cloud environment. Finally, this chapter will demonstrate how to
    manage dockerized microservices in the Mesos and Marathon environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will have learned about:'
  prefs: []
  type: TYPE_NORMAL
- en: The need to have an abstraction layer and cluster control software
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mesos and Marathon from the context of microservices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing dockerized BrownField Airline's PSS microservices with Mesos and Marathon
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reviewing the microservice capability model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will explore the **Cluster Control & Provisioning** microservices
    capability from the microservices capability model discussed in [Chapter 3](ch03.html
    "Chapter 3. Applying Microservices Concepts"), *Applying Microservices Concepts*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Reviewing the microservice capability model](img/B05447_09_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The missing pieces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 8](ch08.html "Chapter 8. Containerizing Microservices with Docker"),
    *Containerizing Microservices with Docker*, we discussed how to dockerize BrownField
    Airline's PSS microservices. Docker helped package the JVM runtime and OS parameters
    along with the application so that there is no special consideration required
    when moving dockerized microservices from one environment to another. The REST
    APIs provided by Docker have simplified the life cycle manager's interaction with
    the target machine in starting and stopping artifacts.
  prefs: []
  type: TYPE_NORMAL
- en: In a large-scale deployment, with hundreds and thousands of Docker containers,
    we need to ensure that Docker containers run with their own resource constraints,
    such as memory, CPU, and so on. In addition to this, there may be rules set for
    Docker deployments, such as replicated copies of the container should not be run
    on the same machine. Also, a mechanism needs to be in place to optimally use the
    server infrastructure to avoid incurring extra cost.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are organizations that deal with billions of containers. Managing them
    manually is next to impossible. In the context of large-scale Docker deployments,
    some of the key questions to be answered are:'
  prefs: []
  type: TYPE_NORMAL
- en: How do we manage thousands of containers?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we monitor them?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we apply rules and constraints when deploying artifacts?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we ensure that we utilize containers properly to gain resource efficiency?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we ensure that at least a certain number of minimal instances are running
    at any point in time?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we ensure dependent services are up and running?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we do rolling upgrades and graceful migrations?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we roll back faulty deployments?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All these questions point to the need to have a solution to address two key
    capabilities, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A cluster abstraction layer that provides a uniform abstraction over many physical
    or virtual machines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A cluster control and init system to manage deployments intelligently on top
    of the cluster abstraction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The life cycle manager is ideally placed to deal with these situations. One
    can add enough intelligence to the life cycle manager to solve these issues. However,
    before attempting to modify the life cycle manager, it is important to understand
    the role of cluster management solutions a bit more.
  prefs: []
  type: TYPE_NORMAL
- en: Why cluster management is important
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As microservices break applications into different micro-applications, many
    developers request more server nodes for deployment. In order to manage microservices
    properly, developers tend to deploy one microservice per VM, which further drives
    down the resource utilization. In many cases, this results in an overallocation
    of CPUs and memory.
  prefs: []
  type: TYPE_NORMAL
- en: In many deployments, the high-availability requirements of microservices force
    engineers to add more and more service instances for redundancy. In reality, though
    it provides the required high availability, this will result in underutilized
    server instances.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, microservice deployment requires more infrastructure compared to
    monolithic application deployments. Due to the increase in cost of the infrastructure,
    many organizations fail to see the value of microservices:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Why cluster management is important](img/B05447_09_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In order to address the issue stated before, we need a tool that is capable
    of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Automating a number of activities, such as the allocation of containers to the
    infrastructure efficiently and keeping it transparent to developers and administrators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Providing a layer of abstraction for the developers so that they can deploy
    their application against a data center without knowing which machine is to be
    used to host their applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting rules or constraints against deployment artifacts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Offering higher levels of agility with minimal management overheads for developers
    and administrators, perhaps with minimal human interaction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building, deploying, and managing the application's cost effectively by driving
    a maximum utilization of the available resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Containers solve an important issue in this context. Any tool that we select
    with these capabilities can handle containers in a uniform way, irrespective of
    the underlying microservice technologies.
  prefs: []
  type: TYPE_NORMAL
- en: What does cluster management do?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Typical cluster management tools help virtualize a set of machines and manage
    them as a single cluster. Cluster management tools also help move the workload
    or containers across machines while being transparent to the consumer. Technology
    evangelists and practitioners use different terminologies, such as cluster orchestration,
    cluster management, data center virtualization, container schedulers, or container
    life cycle management, container orchestration, data center operating system,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Many of these tools currently support both Docker-based containers as well as
    noncontainerized binary artifact deployments, such as a standalone Spring Boot
    application. The fundamental function of these cluster management tools is to
    abstract the actual server instance from the application developers and administrators.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster management tools help the self-service and provisioning of infrastructure
    rather than requesting the infrastructure teams to allocate the required machines
    with a predefined specification. In this automated cluster management approach,
    machines are no longer provisioned upfront and preallocated to the applications.
    Some of the cluster management tools also help virtualize data centers across
    many heterogeneous machines or even across data centers, and create an elastic,
    private cloud-like infrastructure. There is no standard reference model for cluster
    management tools. Therefore, the capabilities vary between vendors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the key capabilities of cluster management software are summarized
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cluster management**: It manages a cluster of VMs and physical machines as
    a single large machine. These machines could be heterogeneous in terms of resource
    capabilities, but they are, by and large, machines with Linux as the operating
    system. These virtual clusters can be formed on the cloud, on-premises, or a combination
    of both.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deployments**: It handles the automatic deployment of applications and containers
    with a large set of machines. It supports multiple versions of the application
    containers and also rolling upgrades across a large number of cluster machines.
    These tools are also capable of handling the rollback of faulty promotes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: It handles the automatic and manual scalability of application
    instances as and when required, with optimized utilization as the primary goal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Health**: It manages the health of the cluster, nodes, and applications.
    It removes faulty machines and application instances from the cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Infrastructure abstraction**: It abstracts the developers from the actual
    machine on which the applications are deployed. The developers need not worry
    about the machine, its capacity, and so on. It is entirely the cluster management
    software''s decision to decide how to schedule and run the applications. These
    tools also abstract machine details, their capacity, utilization, and location
    from the developers. For application owners, these are equivalent to a single
    large machine with almost unlimited capacity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resource optimization**: The inherent behavior of these tools is to allocate
    container workloads across a set of available machines in an efficient way, thereby
    reducing the cost of ownership. Simple to extremely complicated algorithms can
    be used effectively to improve utilization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resource allocation**: It allocates servers based on resource availability
    and the constraints set by application developers. Resource allocation is based
    on these constraints, affinity rules, port requirements, application dependencies,
    health, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Service availability**: It ensures that the services are up and running somewhere
    in the cluster. In case of a machine failure, cluster control tools automatically
    handle failures by restarting these services on some other machine in the cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Agility**: These tools are capable of quickly allocating workloads to the
    available resources or moving the workload across machines if there is change
    in resource requirements. Also, constraints can be set to realign the resources
    based on business criticality, business priority, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Isolation**: Some of these tools provide resource isolation out of the box.
    Hence, even if the application is not containerized, resource isolation can be
    still achieved.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A variety of algorithms are used for resource allocation, ranging from simple
    algorithms to complex algorithms, with machine learning and artificial intelligence.
    The common algorithms used are random, bin packing, and spread. Constraints set
    against applications will override the default algorithms based on resource availability:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What does cluster management do?](img/B05447_09_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding diagram shows how these algorithms fill the available machines
    with deployments. In this case, it is demonstrated with two machines:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Spread**: This algorithm performs the allocation of workload equally across
    the available machines. This is showed in diagram **A**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bin packing**: This algorithm tries to fill in data machine by machine and
    ensures the maximum utilization of machines. Bin packing is especially good when
    using cloud services in a pay-as-you-use style. This is shown in diagram **B**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Random**: This algorithm randomly chooses machines and deploys containers
    on randomly selected machines. This is showed in diagram **C**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a possibility of using cognitive computing algorithms such as machine
    learning and collaborative filtering to improve efficiency. Techniques such as
    **oversubscription** allow a better utilization of resources by allocating underutilized
    resources for high-priority tasks—for example, revenue-generating services for
    best-effort tasks such as analytics, video, image processing, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Relationship with microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The infrastructure of microservices, if not properly provisioned, can easily
    result in oversized infrastructures and, essentially, a higher cost of ownership.
    As discussed in the previous sections, a cloud-like environment with a cluster
    management tool is essential to realize cost benefits when dealing with large-scale
    microservices.
  prefs: []
  type: TYPE_NORMAL
- en: The Spring Boot microservices turbocharged with the Spring Cloud project is
    the ideal candidate workload to leverage cluster management tools. As Spring Cloud-based
    microservices are location unaware, these services can be deployed anywhere in
    the cluster. Whenever services come up, they automatically register to the service
    registry and advertise their availability. On the other hand, consumers always
    look for the registry to discover the available service instances. This way, the
    application supports a full fluid structure without preassuming a deployment topology.
    With Docker, we were able to abstract the runtime so that the services could run
    on any Linux-based environments.
  prefs: []
  type: TYPE_NORMAL
- en: Relationship with virtualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cluster management solutions are different from server virtualization solutions
    in many aspects. Cluster management solutions run on top of VMs or physical machines
    as an application component.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster management solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many cluster management software tools available. It is unfair to
    do an apple-to-apple comparison between them. Even though there are no one-to-one
    components, there are many areas of overlap in capabilities between them. In many
    situations, organizations use a combination of one or more of these tools to fulfill
    their requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the position of cluster management tools from the
    microservices context:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cluster management solutions](img/B05447_09_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this section, we will explore some of the popular cluster management solutions
    available on the market.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Docker Swarm is Docker's native cluster management solution. Swarm provides
    a native and deeper integration with Docker and exposes APIs that are compatible
    with Docker's remote APIs. Docker Swarm logically groups a pool of Docker hosts
    and manages them as a single large Docker virtual host. Instead of application
    administrators and developers deciding on which host the container is to be deployed
    in, this decision making will be delegated to Docker Swarm. Docker Swarm will
    decide which host to be used based on the bin packing and spread algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: As Docker Swarm is based on Docker's remote APIs, its learning curve for those
    already using Docker is narrower compared to any other container orchestration
    tools. However, Docker Swarm is a relatively new product on the market, and it
    only supports Docker containers.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm works with the concepts of **manager** and **nodes**. A manager
    is the single point for administrations to interact and schedule the Docker containers
    for execution. Nodes are where Docker containers are deployed and run.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubernetes (k8s) comes from Google's engineering, is written in the Go language,
    and is battle-tested for large-scale deployments at Google. Similar to Swarm,
    Kubernetes helps manage containerized applications across a cluster of nodes.
    Kubernetes helps automate container deployments, scheduling, and the scalability
    of containers. Kubernetes supports a number of useful features out of the box,
    such as automatic progressive rollouts, versioned deployments, and container resiliency
    if containers fail due to some reason.
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes architecture has the concepts of **master**, **nodes**, and **pods**.
    The master and nodes together form a Kubernetes cluster. The master node is responsible
    for allocating and managing workload across a number of nodes. Nodes are nothing
    but a VM or a physical machine. Nodes are further subsegmented as pods. A node
    can host multiple pods. One or more containers are grouped and executed inside
    a pod. Pods are also helpful in managing and deploying co-located services for
    efficiency. Kubernetes also supports the concept of labels as key-value pairs
    to query and find containers. Labels are user-defined parameters to tag certain
    types of nodes that execute a common type of workloads, such as frontend web servers.
    The services deployed on a cluster get a single IP/DNS to access the service.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes has out-of-the-box support for Docker; however, the Kubernetes learning
    curve is steeper compared to Docker Swarm. RedHat offers commercial support for
    Kubernetes as part of its OpenShift platform.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Mesos
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Mesos is an open source framework originally developed by the University of
    California at Berkeley and is used by Twitter at scale. Twitter uses Mesos primarily
    to manage the large Hadoop ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: Mesos is slightly different from the previous solutions. Mesos is more of a
    resource manager that relays on other frameworks to manage workload execution.
    Mesos sits between the operating system and the application, providing a logical
    cluster of machines.
  prefs: []
  type: TYPE_NORMAL
- en: Mesos is a distributed system kernel that logically groups and virtualizes many
    computers to a single large machine. Mesos is capable of grouping a number of
    heterogeneous resources to a uniform resource cluster on which applications can
    be deployed. For these reasons, Mesos is also known as a tool to build a private
    cloud in a data center.
  prefs: []
  type: TYPE_NORMAL
- en: Mesos has the concepts of the **master** and **slave** nodes. Similar to the
    earlier solutions, master nodes are responsible for managing the cluster, whereas
    slaves run the workload. Mesos internally uses ZooKeeper for cluster coordination
    and storage. Mesos supports the concept of frameworks. These frameworks are responsible
    for scheduling and running noncontainerized applications and containers. Marathon,
    Chronos, and Aurora are popular frameworks for the scheduling and execution of
    applications. Netflix Fenzo is another open source Mesos framework. Interestingly,
    Kubernetes also can be used as a Mesos framework.
  prefs: []
  type: TYPE_NORMAL
- en: Marathon supports the Docker container as well as noncontainerized applications.
    Spring Boot can be directly configured in Marathon. Marathon provides a number
    of capabilities out of the box, such as supporting application dependencies, grouping
    applications to scale and upgrade services, starting and shutting down healthy
    and unhealthy instances, rolling out promotes, rolling back failed promotes, and
    so on.
  prefs: []
  type: TYPE_NORMAL
- en: Mesosphere offers commercial support for Mesos and Marathon as part of its DCOS
    platform.
  prefs: []
  type: TYPE_NORMAL
- en: Nomad
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Nomad from HashiCorp is another cluster management software. Nomad is a cluster
    management system that abstracts lower-level machine details and their locations.
    Nomad has a simpler architecture compared to the other solutions explored earlier.
    Nomad is also lightweight. Similar to other cluster management solutions, Nomad
    takes care of resource allocation and the execution of applications. Nomad also
    accepts user-specific constraints and allocates resources based on this.
  prefs: []
  type: TYPE_NORMAL
- en: Nomad has the concept of **servers**, in which all jobs are managed. One server
    acts as the **leader**, and others act as **followers**. Nomad has the concept
    of **tasks**, which is the smallest unit of work. Tasks are grouped into **task
    groups**. A task group has tasks that are to be executed in the same location.
    One or more task groups or tasks are managed as **jobs**.
  prefs: []
  type: TYPE_NORMAL
- en: Nomad supports many workloads, including Docker, out of the box. Nomad also
    supports deployments across data centers and is region and data center aware.
  prefs: []
  type: TYPE_NORMAL
- en: Fleet
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fleet is a cluster management system from CoreOS. It runs on a lower level and
    works on top of systemd. Fleet can manage application dependencies and make sure
    that all the required services are running somewhere in the cluster. If a service
    fails, it restarts the service on another host. Affinity and constraint rules
    are possible to supply when allocating resources.
  prefs: []
  type: TYPE_NORMAL
- en: Fleet has the concepts of **engine** and **agents**. There is only one engine
    at any point in the cluster with multiple agents. Tasks are submitted to the engine
    and agent run these tasks on a cluster machine.
  prefs: []
  type: TYPE_NORMAL
- en: Fleet also supports Docker out of the box.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster management with Mesos and Marathon
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we discussed in the previous section, there are many cluster management solutions
    or container orchestration tools available. Different organizations choose different
    solutions to address problems based on their environment. Many organizations choose
    Kubernetes or Mesos with a framework such as Marathon. In most cases, Docker is
    used as a default containerization method to package and deploy workloads.
  prefs: []
  type: TYPE_NORMAL
- en: For the rest of this chapter, we will show how Mesos works with Marathon to
    provide the required cluster management capability. Mesos is used by many organizations,
    including Twitter, Airbnb, Apple, eBay, Netflix, PayPal, Uber, Yelp, and many
    others.
  prefs: []
  type: TYPE_NORMAL
- en: Diving deep into Mesos
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Mesos can be treated as a data center kernel. DCOS is the commercial version
    of Mesos supported by Mesosphere. In order to run multiple tasks on one node,
    Mesos uses resource isolation concepts. Mesos relies on the Linux kernel''s **cgroups**
    to achieve resource isolation similar to the container approach. It also supports
    containerized isolation using Docker. Mesos supports both batch workload as well
    as the OLTP kind of workloads:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diving deep into Mesos](img/B05447_09_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Mesos is an open source top-level Apache project under the Apache license. Mesos
    abstracts lower-level computing resources such as CPU, memory, and storage from
    lower-level physical or virtual machines.
  prefs: []
  type: TYPE_NORMAL
- en: Before we examine why we need both Mesos and Marathon, let's understand the
    Mesos architecture.
  prefs: []
  type: TYPE_NORMAL
- en: The Mesos architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following diagram shows the simplest architectural representation of Mesos.
    The key components of Mesos includes a Mesos master node, a set of slave nodes,
    a ZooKeeper service, and a Mesos framework. The Mesos framework is further subdivided
    into two components: a scheduler and an executor:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Mesos architecture](img/B05447_09_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The boxes in the preceding diagram are explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Master**: The Mesos master is responsible for managing all the Mesos slaves.
    The Mesos master gets information on the resource availability from all slave
    nodes and take the responsibility of filling the resources appropriately based
    on certain resource policies and constraints. The Mesos master preempts available
    resources from all slave machines and pools them as a single large machine. The
    master offers resources to frameworks running on slave machines based on this
    resource pool.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For high availability, the Mesos master is supported by the Mesos master's standby
    components. Even if the master is not available, the existing tasks can still
    be executed. However, new tasks cannot be scheduled in the absence of a master
    node. The master standby nodes are nodes that wait for the failure of the active
    master and take over the master's role in the case of a failure. It uses ZooKeeper
    for the master leader election. A minimum quorum requirement must be met for leader
    election.
  prefs: []
  type: TYPE_NORMAL
- en: '**Slave**: Mesos slaves are responsible for hosting task execution frameworks.
    Tasks are executed on the slave nodes. Mesos slaves can be started with attributes
    as key-value pairs, such as *data center = X*. This is used for constraint evaluations
    when deploying workloads. Slave machines share resource availability with the
    Mesos master.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ZooKeeper**: ZooKeeper is a centralized coordination server used in Mesos
    to coordinate activities across the Mesos cluster. Mesos uses ZooKeeper for leader
    election in case of a Mesos master failure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Framework**: The Mesos framework is responsible for understanding the application''s
    constraints, accepting resource offers from the master, and finally running tasks
    on the slave resources offered by the master. The Mesos framework consists of
    two components: the framework scheduler and the framework executor:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The scheduler is responsible for registering to Mesos and handling resource
    offers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The executor runs the actual program on Mesos slave nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The framework is also responsible for enforcing certain policies and constraints.
    For example, a constraint can be, let's say, that a minimum of 500 MB of RAM is
    available for execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Frameworks are pluggable components and are replaceable with another framework.
    The framework workflow is depicted in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Mesos architecture](img/B05447_09_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The steps denoted in the preceding workflow diagram are elaborated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The framework registers with the Mesos master and waits for resource offers.
    The scheduler may have many tasks in its queue to be executed with different resource
    constraints (tasks **A** to **D**, in this example). A task, in this case, is
    a unit of work that is scheduled—for example, a Spring Boot microservice.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Mesos slave offers the available resources to the Mesos master. For example,
    the slave advertises the CPU and memory available with the slave machine.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Mesos master then creates a resource offer based on the allocation policies
    set and offers it to the scheduler component of the framework. Allocation policies
    determine which framework the resources are to be offered to and how many resources
    are to be offered. The default policies can be customized by plugging additional
    allocation policies.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The scheduler framework component, based on the constraints, capabilities, and
    policies, may accept or reject the resource offering. For example, a framework
    rejects the resource offer if the resources are insufficient as per the constraints
    and policies set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the scheduler component accepts the resource offer, it submits the details
    of one more task to the Mesos master with resource constraints per task. Let's
    say, in this example, that it is ready to submit tasks **A** to **D**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Mesos master sends this list of tasks to the slave where the resources are
    available. The framework executor component installed on the slave machines picks
    up and runs these tasks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Mesos supports a number of frameworks, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Marathon and Aurora for **long-running** processes, such as web applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hadoop, Spark, and Storm for **big data** processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chronos and Jenkins for **batch scheduling**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cassandra and Elasticsearch for **data management**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will use Marathon to run dockerized microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Marathon
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Marathon is one of the Mesos framework implementations that can run both container
    as well as noncontainer execution. Marathon is particularly designed for long-running
    applications, such as a web server. Marathon ensures that the service started
    with Marathon continues to be available even if the Mesos slave it is hosted on
    fails. This will be done by starting another instance.
  prefs: []
  type: TYPE_NORMAL
- en: Marathon is written in Scala and is highly scalable. Marathon offers a UI as
    well as REST APIs to interact with Marathon, such as the start, stop, scale, and
    monitoring applications.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to Mesos, Marathon's high availability is achieved by running multiple
    Marathon instances pointing to a ZooKeeper instance. One of the Marathon instances
    acts as a leader, and others are in standby mode. In case the leading master fails,
    a leader election will take place, and the next active master will be determined.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the basic features of Marathon include:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting resource constraints
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling up, scaling down, and the instance management of applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application version management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Starting and killing applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some of the advanced features of Marathon include:'
  prefs: []
  type: TYPE_NORMAL
- en: Rolling upgrades, rolling restarts, and rollbacks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blue-green deployments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Mesos and Marathon for BrownField microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, the dockerized Brownfield microservice developed in [Chapter
    8](ch08.html "Chapter 8. Containerizing Microservices with Docker"), *Containerizing
    Microservices with Docker*, will be deployed into the AWS cloud and managed with
    Mesos and Marathon.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the purposes of demonstration, only three of the services (**Search**,
    **Search API Gateway**, and **Website**) are covered in the explanations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing Mesos and Marathon for BrownField microservices](img/B05447_09_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The logical architecture of the target state implementation is shown in the
    preceding diagram. The implementation uses multiple Mesos slaves to execute dockerized
    microservices with a single Mesos master. The Marathon scheduler component is
    used to schedule dockerized microservices. Dockerized microservices are hosted
    on the Docker Hub registry. Dockerized microservices are implemented using Spring
    Boot and Spring Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the physical deployment architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing Mesos and Marathon for BrownField microservices](img/B05447_09_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As shown in the preceding diagram, in this example, we will use four EC2 instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '**EC2-M1**: This hosts the Mesos master, ZooKeeper, the Marathon scheduler,
    and one Mesos slave instance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**EC2-M2**: This hosts one Mesos slave instance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**EC2-M3**: This hosts another Mesos slave instance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**EC2-M4**: This hosts Eureka, Config server, and RabbitMQ'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a real production setup, multiple Mesos masters as well as multiple instances
    of Marathon are required for fault tolerance.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up AWS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Launch the four **t2.micro** EC2 instances that will be used for this deployment.
    All four instances have to be on the same security group so that the instances
    can see each other using their local IP addresses.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following tables show the machine details and IP addresses for indicative
    purposes and to link subsequent instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Setting up AWS](img/B05447_09_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '| Instance ID | Private DNS/IP | Public DNS/IP |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `i-06100786` | `ip-172-31-54-69.ec2.internal``172.31.54.69` | `ec2-54-85-107-37.compute-1.amazonaws.com``54.85.107.37`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `i-2404e5a7` | `ip-172-31-62-44.ec2.internal``172.31.62.44` | `ec2-52-205-251-150.compute-1.amazonaws.com``52.205.251.150`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `i-a7df2b3a` | `ip-172-31-49-55.ec2.internal``172.31.49.55` | `ec2-54-172-213-51.compute-1.amazonaws.com``54.172.213.51`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `i-b0eb1f2d` | `ip-172-31-53-109.ec2.internal``172.31.53.109` | `ec2-54-86-31-240.compute-1.amazonaws.com``54.86.31.240`
    |'
  prefs: []
  type: TYPE_TB
- en: Replace the IP and DNS addresses based on your AWS EC2 configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Installing ZooKeeper, Mesos, and Marathon
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following software versions will be used for the deployment. The deployment
    in this section follows the physical deployment architecture explained in the
    earlier section:'
  prefs: []
  type: TYPE_NORMAL
- en: Mesos version 0.27.1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker version 1.6.2, build 7c8fca2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Marathon version 0.15.3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The detailed instructions to set up ZooKeeper, Mesos, and Marathon are available
    at [https://open.mesosphere.com/getting-started/install/](https://open.mesosphere.com/getting-started/install/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps for a minimal installation of ZooKeeper, Mesos,
    and Marathon to deploy the BrownField microservice:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a prerequisite, JRE 8 must be installed on all the machines. Execute the
    following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Install Docker on all machines earmarked for the Mesos slave via the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Open a terminal window and execute the following commands. These commands set
    up the repository for installation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following command to install Mesos and Marathon. This will also
    install Zookeeper as a dependency:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Repeat the preceding steps on all the three EC2 instances reserved for the Mesos
    slave execution. As the next step, ZooKeeper and Mesos have to be configured on
    the machine identified for the Mesos master.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring ZooKeeper
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Connect to the machine reserved for the Mesos master and Marathon scheduler.
    In this case, `172.31.54.69` will be used to set up ZooKeeper, the Mesos master,
    and Marathon.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two configuration changes required in ZooKeeper, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to set `/etc/zookeeper/conf/myid` to a unique integer between
    `1` and `255`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to edit `/etc/zookeeper/conf/zoo.cfg`. Update the file to
    reflect the following changes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Replace the IP addresses with the relevant private IP address. In this case,
    we will use only one ZooKeeper server, but in a production scenario, multiple
    servers are required for high availability.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Mesos
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Make changes to the Mesos configuration to point to ZooKeeper, set up a quorum,
    and enable Docker support via the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Edit `/etc/mesos/zk` to set the following value. This is to point Mesos to
    a ZooKeeper instance for quorum and leader election:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Edit the `/etc/mesos-master/quorum` file and set the value as `1`. In a production
    scenario, we may need a minimum quorum of three:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The default Mesos installation does not support Docker on Mesos slaves. In
    order to enable Docker, update the following `mesos-slave` configuration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Running Mesos, Marathon, and ZooKeeper as services
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'All the required configuration changes are implemented. The easiest way to
    start Mesos, Marathon, and Zookeeper is to run them as services, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following commands start services. The services need to be started in the
    following order:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'At any point, the following commands can be used to stop these services:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Once the services are up and running, use a terminal window to verify whether
    the services are running:![Running Mesos, Marathon, and ZooKeeper as services](img/B05447_09_11.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running the Mesos slave in the command line
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this example, instead of using the Mesos slave service, we will use a command-line
    version to invoke the Mesos slave to showcase additional input parameters. Stop
    the Mesos slave and use the command line as mentioned here to start the slave
    again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The command-line parameters used are explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--master=172.31.54.69:5050`: This parameter is to tell the Mesos slave to
    connect to the correct Mesos master. In this case, there is only one master running
    at `172.31.54.69:5050`. All the slaves connect to the same Mesos master.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--containerizers=mesos,docker`: This parameter is to enable support for Docker
    container execution as well as noncontainerized executions on the Mesos slave
    instances.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--resources="ports(*):[8000-9000, 31000-32000]`: This parameter indicates
    that the slave can offer both ranges of ports when binding resources. `31000`
    to `32000` is the default range. As we are using port numbers starting with `8000`,
    it is important to tell the Mesos slave to allow exposing ports starting from
    `8000` as well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Perform the following steps to verify the installation of Mesos and Marathon:'
  prefs: []
  type: TYPE_NORMAL
- en: Execute the command mentioned in the previous step to start the Mesos slave
    on all the three instances designated for the slave. The same command can be used
    across all three instances as all of them connect to the same master.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If the Mesos slave is successfully started, a message similar to the following
    will appear in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The preceding message indicates that the Mesos slave started sending the current
    state of resource availability periodically to the Mesos master.
  prefs: []
  type: TYPE_NORMAL
- en: Open `http://54.85.107.37:8080` to inspect the Marathon UI. Replace the IP address
    with the public IP address of the EC2 instance:![Running the Mesos slave in the
    command line](img/B05447_09_12.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As there are no applications deployed so far, the **Applications** section of
    the UI is empty.
  prefs: []
  type: TYPE_NORMAL
- en: Open the Mesos UI, which runs on port `5050`, by going to `http://54.85.107.37:5050`:![Running
    the Mesos slave in the command line](img/B05447_09_13.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **Slaves** section of the console shows that there are three activated Mesos
    slaves available for execution. It also indicates that there is no active task.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing BrownField PSS services
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous section, we successfully set up Mesos and Marathon. In this
    section, we will take a look at how to deploy the BrownField PSS application previously
    developed using Mesos and Marathon.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The full source code of this chapter is available under the `Chapter 9` project
    in the code files. Copy `chapter8.configserver`, `chapter8.eurekaserver`, `chapter8.search`,
    `chapter8.search-apigateway`, and `chapter8.website` into a new STS workspace
    and rename them `chapter9.*`.
  prefs: []
  type: TYPE_NORMAL
- en: Before we deploy any application, we have to set up the Config server, Eureka
    server, and RabbitMQ in one of the servers. Follow the steps described in the
    *Running BrownField services on EC2* section in [Chapter 8](ch08.html "Chapter 8. Containerizing
    Microservices with Docker"), *Containerizing Microservices with Docker*. Alternately,
    we can use the same instance as used in the previous chapter for this purpose.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Change all `bootstrap.properties` files to reflect the Config server IP address.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Before we deploy our services, there are a few specific changes required on
    the microservices. When running dockerized microservices with the BRIDGE mode
    on, we need to tell the Eureka client the hostname to be used to bind. By default,
    Eureka uses the **instance ID** to register. However, this is not helpful as Eureka
    clients won't be able to look up these services using the instance ID. In the
    previous chapter, the HOST mode was used instead of the BRIDGE mode.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The hostname setup can be done using the `eureka.instance.hostname` property.
    However, when running on AWS specifically, an alternate approach is to define
    a bean in the microservices to pick up AWS-specific information, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code provides a custom Eureka server configuration using the Amazon
    host information using Netflix APIs. The code overrides the hostname and instance
    ID with the private DNS. The port is read from the Config server. This code also
    assumes one host per service so that the port number stays constant across multiple
    deployments. This can also be overridden by dynamically reading the port binding
    information at runtime.
  prefs: []
  type: TYPE_NORMAL
- en: The previous code has to be applied in all microservices.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rebuild all the microservices using Maven. Build and push the Docker images
    to the Docker Hub. The steps for the three services are shown as follows. Repeat
    the same steps for all the other services. The working directory needs to be switched
    to the respective directories before executing these commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Deploying BrownField PSS services
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Docker images are now published to the Docker Hub registry. Perform the
    following steps to deploy and run BrownField PSS services:'
  prefs: []
  type: TYPE_NORMAL
- en: Start the Config server, Eureka server, and RabbitMQ on its dedicated instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make sure that the Mesos server and Marathon are running on the machine where
    the Mesos master is configured.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the Mesos slave on all the machines as described earlier using the command
    line.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'At this point, the Mesos Marathon cluster is up and running and is ready to
    accept deployments. The deployment can be done by creating one JSON file per service,
    as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The preceding JSON code will be stored in the `search.json` file. Similarly,
    create a JSON file for other services as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'The JSON structure is explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`id`: This is the unique ID of the application. This can be a logical name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cpus` and `mem`: This sets the resource constraints for this application.
    If the resource offer does not satisfy this resource constraint, Marathon will
    reject this resource offer from the Mesos master.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`instances`: This decides how many instances of this application to start with.
    In the preceding configuration, by default, it starts one instance as soon as
    it gets deployed. Marathon maintains the number of instances mentioned at any
    point.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`container`: This parameter tells the Marathon executor to use a Docker container
    for execution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image`: This tells the Marathon scheduler which Docker image has to be used
    for deployment. In this case, this will download the `search-service:1.0` image
    from the Docker Hub repository `rajeshrv`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`network`: This value is used for Docker runtime to advise on the network mode
    to be used when starting the new docker container. This can be BRIDGE or HOST.
    In this case, the BRIDGE mode will be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`portMappings`: The port mapping provides information on how to map the internal
    and external ports. In the preceding configuration, the host port is set as `8090`,
    which tells the Marathon executor to use `8090` when starting the service. As
    the container port is set as `0`, the same host port will be assigned to the container.
    Marathon picks up random ports if the host port value is `0`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Additional health checks are also possible with the JSON descriptor, as shown
    here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Once this JSON code is created and saved, deploy it to Marathon using the Marathon
    REST APIs as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Repeat this step for all the other services as well.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding step will automatically deploy the Docker container to the Mesos
    cluster and start one instance of the service.
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing the deployment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The steps for this are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the Marathon UI. As shown in the following screenshot, the UI shows that
    all the three applications are deployed and are in the **Running** state. It also
    indicates that **1 of 1** instance is in the **Running** state:![Reviewing the
    deployment](img/B05447_09_14.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visit the Mesos UI. As shown in the following screenshot, there are three **Active
    Tasks**, all of them in the **Running** state. It also shows the host in which
    these services run:![Reviewing the deployment](img/B05447_09_15.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the Marathon UI, click on a running application. The following screenshot
    shows the **search-apigateway-1.0** application. In the **Instances** tab, the
    IP address and port in which the service is bound is indicated:![Reviewing the
    deployment](img/B05447_09_16.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **Scale Application** button allows administrators to specify how many instances
    of the service are required. This can be used to scale up as well as scale down
    instances.
  prefs: []
  type: TYPE_NORMAL
- en: Open the Eureka server console to take a look at how the services are bound.
    As shown in the screenshot, **AMIs** and **Availability Zones** are reflected
    when services are registered. Follow `http://52.205.251.150:8761`:![Reviewing
    the deployment](img/B05447_09_17.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open `http://54.172.213.51:8001` in a browser to verify the **Website** application.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A place for the life cycle manager
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The life cycle manager introduced in [Chapter 6](ch06.html "Chapter 6. Autoscaling
    Microservices"), *Autoscaling Microservices*, has the capability of autoscaling
    up or down instances based on demand. It also has the ability to take decisions
    on where to deploy and how to deploy applications on a cluster of machines based
    on polices and constraints. The life cycle manager''s capabilities are shown in
    the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A place for the life cycle manager](img/B05447_09_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Marathon has the capability to manage clusters and deployments to clusters based
    on policies and constraints. The number of instances can be altered using the
    Marathon UI.
  prefs: []
  type: TYPE_NORMAL
- en: There are redundant capabilities between our life cycle manager and Marathon.
    With Marathon in place, SSH work or machine-level scripting is no longer required.
    Moreover, deployment policies and constraints can be delegated to Marathon. The
    REST APIs exposed by Marathon can be used to initiate scaling functions.
  prefs: []
  type: TYPE_NORMAL
- en: '**Marathon autoscale** is a proof-of-concept project from Mesosphere for autoscaling.
    The Marathon autoscale provides basic autoscale features such as the CPU, memory,
    and rate of request.'
  prefs: []
  type: TYPE_NORMAL
- en: Rewriting the life cycle manager with Mesos and Marathon
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We still need a custom life cycle manager to collect metrics from the Spring
    Boot actuator endpoints. A custom life cycle manager is also handy if the scaling
    rules are beyond the CPU, memory, and rate of scaling.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the updated life cycle manager using the Marathon
    framework:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Rewriting the life cycle manager with Mesos and Marathon](img/B05447_09_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The life cycle manager, in this case, collects actuator metrics from different
    Spring Boot applications, combines them with other metrics, and checks for certain
    thresholds. Based on the scaling policies, the decision engine informs the scaling
    engine to either scale down or scale up. In this case, the scaling engine is nothing
    but a Marathon REST client. This approach is cleaner and neater than our earlier
    primitive life cycle manager implementation using SSH and Unix scripts.
  prefs: []
  type: TYPE_NORMAL
- en: The technology metamodel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have covered a lot of ground on microservices with the BrownField PSS microservices.
    The following diagram sums it up by bringing together all the technologies used
    into a technology metamodel:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The technology metamodel](img/B05447_09_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned the importance of a cluster management and init
    system to efficiently manage dockerized microservices at scale.
  prefs: []
  type: TYPE_NORMAL
- en: We explored the different cluster control or cluster orchestration tools before
    diving deep into Mesos and Marathon. We also implemented Mesos and Marathon in
    the AWS cloud environment to demonstrate how to manage dockerized microservices
    developed for BrownField PSS.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of this chapter, we also explored the position of the life cycle
    manager in conjunction with Mesos and Marathon. Finally, we concluded this chapter
    with a technology metamodel based on the BrownField PSS microservices implementation.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have discussed all the core and supporting technology capabilities
    required for a successful microservices implementation. A successful microservice
    implementation also requires processes and practices beyond technology. The next
    chapter, the last in the book, will cover the process and practice perspectives
    of microservices.
  prefs: []
  type: TYPE_NORMAL
