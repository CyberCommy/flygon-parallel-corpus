- en: Asynchronous Microservice Architectures Using Message Queues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the past two chapters, you learned how to build REST-based microservices
    with the Go programming language. The REST architectural style is both simple
    and flexible at the same time, which makes it an excellent choice for many use
    cases. However, being built on top of HTTP, all communication in a REST architecture
    will follow the client/server model with request/reply transactions. In some use
    cases, this might be restrictive and other communication models might be better
    suited.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will introduce the publish/subscribe communication model,
    along with the technologies that you need to implement it. Typically, publish/subscribe
    architectures require a central infrastructure component—the message broker. In
    the open source world, there are many different implementations of message brokers;
    so, in this chapter, we will introduce two different message brokers that we feel
    to be among the most important ones—**RabbitMQ** and **Apache Kafka**. Both are
    suited for specific use cases; you will learn how to set up each of these two
    message brokers, how to connect your Go application, and when you should use one
    or the other.
  prefs: []
  type: TYPE_NORMAL
- en: We will then show you how to use this knowledge in order to extend the event
    management microservice that you have worked in the previous chapters to publish
    an event whenever something important happens. This allows us to implement a second
    microservice that listens on those events. You will also learn about advanced
    architectural patterns that usually work well alongside asynchronous communication,
    such as *event collaboration* and *event sourcing*, and how (and when) to use
    them in your application.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The publish/subscribe architectural pattern
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Event collaboration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Event sourcing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AMQP with RabbitMQ
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Kafka
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The publish/subscribe pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The publish/subscribe pattern is a communication pattern alternative to the
    well-known request/reply pattern. Instead of a client (issuing a request) and
    a server (replying with a response to that request), a publish/subscribe architecture
    consists of publishers and subscribers.
  prefs: []
  type: TYPE_NORMAL
- en: Each publisher can emit messages. It is of no concern to the publisher who actually
    gets these messages. This is the concern of the subscribers; each subscriber can
    subscribe to a certain type of message and be notified whenever a publisher publishes
    a given type of message. In reverse, each subscriber does not concern itself with
    where a message actually came from.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e967a901-db08-4ba1-a019-ab96d69ff7bc.png)'
  prefs: []
  type: TYPE_IMG
- en: The request/reply and the publish/subscribe communication patterns
  prefs: []
  type: TYPE_NORMAL
- en: In practice, many publish/subscribe architectures require a central infrastructure
    component—the message broker. Publishers publish messages at the message broker,
    and subscribers subscribe to messages at the message broker. One of the broker's
    main tasks then is to route published messages to the subscribers that have expressed
    interest in them.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, messages will be routed **topic-based**. This means that each publisher
    specified a topic for a published message (a topic usually just being a string
    identifier, for example, `user.created`). Each subscriber will also subscribe
    to a certain topic. Often, a broker will also allow a subscriber to subscribe
    to an entire set of topic using wildcard expressions such as `user.*`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast to request/reply, the publish/subscribe pattern brings some clear
    advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Publishers and subscribers are very loosely coupled. This goes to the extent
    that they do not even know about one another.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A pub/sub architecture is very flexible. It is possible to add new subscribers
    (and, therefore, extend existing processes) without having to modify the publisher.
    The inverse also applies; you can add new publishers without having to modify
    the subscribers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In case the messages are routed by a message broker, you also gain resiliency.
    Usually, the message broker stores all messages in a queue, in which they are
    kept until they have been processed by a subscriber. If a subscriber becomes unavailable
    (for example, due to a failure or an intentional shutdown), the messages that
    should have been routed to that subscriber will become queued until the subscriber
    is available again.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Often, you will also get some kind of reliability guaranteed by the message
    broker on a protocol level. For example, RabbitMQ guarantees *reliable delivery*
    by requiring each subscriber to acknowledge a received message. Only when the
    message has been acknowledged, the broker will remove the message from the queue.
    If the subscriber should fail (for example, by disconnection) when a message had
    already been delivered, but not yet acknowledged, the message will be put back
    into the message queue. If another subscriber listens on the same message queue,
    the message might be routed to that subscriber; otherwise, it will remain in the
    queue until the subscriber is available again.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can easily scale out. In case that too many messages are published for a
    single subscriber to efficiently handle them, you can add more subscribers and
    have the message broker load-balance the messages sent to these subscribers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, introducing a central infrastructure component such as a message
    broker brings its own risk. When not done right, your message broker might become
    a single point of failure, taking your entire application down with it in case
    it fails. When introducing a message broker in a production environment, you should
    take appropriate measures to ensure high-availability (usually by clustering and
    automatic failover).
  prefs: []
  type: TYPE_NORMAL
- en: In case your application is run in a cloud environment, you may also take advantage
    of one of the managed message queuing and delivery services that are offered by
    the cloud providers, for example, AWS **Simple Queue Service** (**SQS**) or the
    Azure Service Bus.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn how to use two of the most popular open source
    message brokers—RabbitMQ and Apache Kafka. In [Chapter 8](25f18fd2-4d08-41fb-a8b2-acc927bd0876.xhtml),
    *AWS Part II - S3, SQS, API Gateway, and DynamoDB*, you will learn about AWS SQS.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the booking service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will start by implementing a publish/subscribe architecture
    using RabbitMQ. For this, we will need new microservices to our architecture—the
    booking service will handle bookings for events. Its responsibilities will include
    making sure that events are not overbooked. For this, it will need to know about
    existing events and locations. In order to achieve this, we will modify the **EventService**
    to emit events whenever a location or an event was created (yes, the terminology
    is confusing—make sure not to mistake the *notification that something has happened*
    kind-of-event with the *Metallica is playing here* kind-of-event). The **BookingService**
    can then listen to these events and emit events itself whenever someone books
    a ticket for one of these events.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5e8ba22f-e4f5-4afe-a62e-a02471ec46b7.png)'
  prefs: []
  type: TYPE_IMG
- en: An overview of our microservices and the events that they will be publishing
    and subscribing to
  prefs: []
  type: TYPE_NORMAL
- en: Event collaboration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Event collaboration describes an architectural principle that works well together
    with an event-driven publish/subscribe architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following example that uses the regular request/reply communication
    pattern—a user requests the booking service to book a ticket for a certain event.
    Since the events are managed by another microservice (the **EventService**), the
    **BookingService** will need to request information on both the event and its
    location from the **EventService.** Only then can the **BookingService** check
    whether there are still seats available and save the user''s booking in its own
    database. The requests and responses required for this transaction are illustrated
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/33a76190-6cbc-46dc-a92c-74f6f5cf3070.png)'
  prefs: []
  type: TYPE_IMG
- en: requests and responses
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, consider the same scenario in a publish/subscribe architecture, in which
    the **BookingService** and **EventService** are integrated using events: every
    time data changes in the **EventService**, it emits an event (for example, *a
    new location was created*, *a new event was created*, *an event was updated*,
    and so on).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the **BookingService** can listen to these events. It can build its own
    database of all currently existing locations and events. Now, if a user requests
    a new booking for a given event, the **BookingService** can simply use the data
    from its own local database, without having to request this data from another
    service. Refer to the following diagram for another illustration of this principle:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/42e36988-e61f-42cd-a54b-9e1133e416b3.png)'
  prefs: []
  type: TYPE_IMG
- en: BookingService using the data from its own local database
  prefs: []
  type: TYPE_NORMAL
- en: This is the key point of an event collaboration architecture. In the preceding
    diagram, a service almost never needs to query another service for data, because
    it already knows everything it needs to know by listening to the events emitted
    by other services.
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, this architectural pattern works extremely well together with publish/subscribe.
    In the preceding example, the **EventService** would be the publisher and the
    **BookingService** (potentially, among others) the subscriber. Of course, one
    might flinch at the fact that this principle will inevitably lead to redundant
    data being stored by the two services. However, this is not necessarily a bad
    thing—since every service constantly listens to events emitted by the other services,
    the entire dataset can be kept (eventually) consistent. Also, this increases the
    system's overall resiliency; for example, if the event service suffers a sudden
    failure, the **BookingService** would stay operational since it does not rely
    on the event service to be working anymore.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing publish/subscribe with RabbitMQ
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the following section, you will learn how to implement a basic publish/subscribe
    architecture. For this, we will take a look at the **Advanced Message Queueing
    Protocol** (**AMQP**) and one of its most popular implementations, RabbitMQ.
  prefs: []
  type: TYPE_NORMAL
- en: The Advanced Message Queueing Protocol
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: On a protocol level, RabbitMQ implements the AMQP. Before getting started with
    RabbitMQ, let's get started by taking a look at the basic protocol semantics of
    AMQP.
  prefs: []
  type: TYPE_NORMAL
- en: 'An AMQP message broker manages two basic kinds of resources—**Exchanges** and
    **Queues**. Each publisher publishes its messages into an exchange. Each subscriber
    consumes a queue. The AMQP broker is responsible for putting the messages that
    are published in an exchange into the respective queue. Where messages go after
    they have been published to an exchange depends on the **exchange type** and the
    routing rules called **bindings**. AMQP knows three different types of exchanges:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Direct exchanges**: Messages are published with a given topic (called **routing
    key** in AMQP) that is a simple string value. Bindings between a direct exchange
    and queue can be defined to match exactly that topic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fanout exchanges**: Messages are routed to all queues that are connected
    to a fanout exchange via a binding. Messages can have a routing key, but it will
    be ignored. Every bound queue will receive all messages that are published in
    the fanout exchange.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Topic exchanges**: This works similar to direct exchanges. However, queues
    are now bound to the exchange using patterns that the message''s routing key must
    match. Topic exchanges usually assume routing keys to be segmented with the dot
    character `''.''`. As an example, your routing keys could follow the `"<entityname>.<state-change>.<location>"`
    pattern (for example, `"event.created.europe"`). You can now create queue bindings
    that may contain wildcards using the `''*''` or `''#''` characters. `*` will match
    any single routing key segment, whereas `#` will match any number of segments.
    So, for the preceding example, valid bindings might be as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`event.created.europe` (obviously)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`event.created.*` (listen to whenever an event is created anywhere in the world)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`event.#` (listen to whenever any change is made to an event anywhere in the
    world)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`event.*.europe` (listen to whenever any change is made to an event in Europe)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One possible example exchange and queue topology are shown in the next diagram.
    In this case, we have one service that publishes messages, the **EventService**.
    We have two queues in which messages will be routed. The first queue, **evts_booking**,
    will receive any and all messages that are related to any kind of change made
    to an event. The second queue, **evts_search**, will receive messages only regarding
    the creation of new events. Note that the **evts_booking** queue has two subscribers.
    When two or more subscribers subscribe to the same queue, the message broker will
    dispatch messages to one of the subscribers on a rotating basis.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cc668950-bec2-43b7-beb1-5ca489f7e80d.png)'
  prefs: []
  type: TYPE_IMG
- en: Message broker displaying messages to one of the subscribers on a rotating basis
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that the entire AMQP topology (meaning all the exchanges
    and queues and how they are bound to one another) is not defined by the broker,
    but by the publishers and consumers themselves. AMQP specifies several methods
    that clients can use to declare the exchanges and queues they need. For example,
    a publisher would typically use the `exchange.declare` method to assert that the
    exchange it wants to publish actually exists (the broker will then create it if
    it did not exist before). On the other hand, a subscriber might use the `queue.declare`
    and `queue.bind` methods to declare a queue that it wants to subscribe and bind
    it to an exchange.
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple open source message brokers that implement AMQP. One of the
    most popular ones (and also the one that we will be working within this chapter)
    is the RabbitMQ broker, an open source AMQP broker developed by **Pivotal** and
    made available under the **Mozilla Public License**. Other message brokers that
    implement AMQP are **Apache QPID** ([https://qpid.apache.org](https://qpid.apache.org))
    and **Apache ActiveMQ** ([http://activemq.apache.org](http://activemq.apache.org)).
  prefs: []
  type: TYPE_NORMAL
- en: Although we will use RabbitMQ in this example, the code written in this chapter
    should work will all kinds of AMQP implementations.
  prefs: []
  type: TYPE_NORMAL
- en: RabbitMQ quickstart with Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before building our publish/subscribe architecture, you will need to set up
    a running RabbitMQ message broker in your development environment. The easiest
    way to get started with RabbitMQ is by using the official Docker images.
  prefs: []
  type: TYPE_NORMAL
- en: For this example, we will assume that you have a working Docker installation
    on your local machine. Take a look at the official installation instructions to
    learn how you can install Docker on your operating system at: [https://docs.docker.com/engine/installation](https://docs.docker.com/engine/installation).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can start a new RabbitMQ broker using the following command on your command
    line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command will create a new container named `rabbitmq` on your machine.
    For this, Docker will use the `rabbitmq:3-management` image. This image contains
    the latest release of RabbitMQ 3 (at the time of writing, 3.6.6) and the management
    UI. The `-p 5672:5672` flag will instruct Docker to map the TCP port `5672` (which
    is the IANA-assigned port number for AMQP) to your `localhost` address. The `-p
    15672:15672` flag will do the same for the management user interface.
  prefs: []
  type: TYPE_NORMAL
- en: After starting the container, you will be able to open an AMQP connection to
    `amqp://localhost:5672` and open the management UI in your browser at `http://localhost:15672`.
  prefs: []
  type: TYPE_NORMAL
- en: When you are using Docker on Windows, you will need to substitute localhost
    with the IP address of your local Docker virtual machine. You can determine this
    IP address using the following command on the command line: `$ docker-machine
    ip default`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Regardless whether you are using docker-machine or a local Docker installation,
    the RabbitMQ user interface should look very much like it does in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6075ceff-4af4-4e70-bfa2-de38dd0501c0.png)'
  prefs: []
  type: TYPE_IMG
- en: RabbitMQ's management user interface
  prefs: []
  type: TYPE_NORMAL
- en: Open the management interface in your browser (`http://localhost:15672` or your
    docker-machine IP address). The RabbitMQ image ships a default guest user whose
    password is also `guest`. When running RabbitMQ in production, this is, of course,
    the first thing that you should change. For development purposes, it will do fine.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced RabbitMQ setups
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Docker-based setup described in the preceding section allows you to get
    started quickly and are also (with a few adjustments) suitable for production
    setups. If you do not want to use Docker for your message broker, you can also
    install RabbitMQ on most common Linux distribution from package repositories.
    For example, on Ubuntu and Debian, you can install RabbitMQ using the following
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Similar commands also work on **CentOS** and **RHEL**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: For a production setup, you might want to consider setting up RabbitMQ as a
    cluster to ensure high availability. Take a look at the official documentation
    at [http://www.rabbitmq.com/clustering.html](http://www.rabbitmq.com/clustering.html)
    for more information on how to set up a RabbitMQ cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Connecting RabbitMQ with Go
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For connecting to a RabbitMQ broker (or any AMQP broker, for that matter),
    we recommend that you use the `github.com/streadway/amqp` library (which is the
    de facto standard Go library for AMQP). Let''s start by installing the library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'You can then start by importing the library into your code. Open a new connection
    using the `amqp.Dial` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In this case, `"amqp://guest:guest@localhost:5672"` is the URL of your AMQP
    broker. Note that the user credentials are embedded into the URL. The `amqp.Dial`
    method returns a connection object on success, or `nil` and an error, otherwise
    (as usual in Go, make sure that you actually check for this error). Also, do not
    forget to close the connection using the `Close()` method when you do not need
    it anymore.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, it is usually not a good practice to hardcode connection details
    such as these (much fewer credentials) into your application. Remember what you
    learned about twelve-factor applications, and let''s introduce an environment
    variable `AMQP_URL` that we can use to dynamically configure the AMQP broker:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In AMQP, most operations are done not directly on the connection, but on channels.
    Channels are used to *multiplex* several virtual connections over one actual TCP
    connection.
  prefs: []
  type: TYPE_NORMAL
- en: Channels themselves are not thread-safe. In Go, we will need to keep this in
    mind and pay attention to not access the same channel from multiple goroutines.
    However, using multiple channels, with each channel being accessed by only one
    thread, is completely safe. So, when in doubt, it is best to create a new channel.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continue by creating a new channel on the existing connection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We can now use this channel object for some actual AMQP operations, for example,
    publishing messages and subscribing to messages.
  prefs: []
  type: TYPE_NORMAL
- en: Publishing and subscribing to AMQP messages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before diving back into the MyEvents microservice architecture, let's take a
    look at the basic AMQP methods that we can use. For this, we will start by building
    a small example program that is capable of publishing messages to an exchange.
  prefs: []
  type: TYPE_NORMAL
- en: 'After opening a channel, a message publisher should declare the exchange into
    which it intends to publish messages. For this, you can use the `ExchangeDeclare()`
    method on the channel object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, `ExchangeDeclare` takes quite a number of parameters. These
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The exchange name
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The exchange type (remember that AMQP knows `direct`, `fanout`, and `topic` exchanges)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `durable` flag will cause the exchange to remain declared when the broker
    restarts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `autoDelete` flag will cause the exchange to be deleted as soon as the channel
    that declared it is closed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `internal` flag will prevent publishers from publishing messages into this
    queue
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `noWait` flag will instruct the `ExchangeDeclare` method not to wait for
    a successful response from the broker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `args` argument may contain a map with additional configuration parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After having declared an exchange, you can now publish a message. For this,
    you can use the channel''s `Publish()` method. The emitted message will be an
    instance of the `amqp.Publishing` struct that you need to instantiate at first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, use the `Publish()` method to publish your message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Publish()` method takes the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: The name of the exchange to publish to
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The message's routing key
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `mandatory` flag will instruct the broker to make sure that the message
    is actually routed into at least one queue
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `immediate` flag will instruct the broker to make sure that the message
    is actually delivered to at least one subscriber
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `msg` argument contains the actual message that is to be published
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a publish/subscribe architecture, in which a publisher does not need to
    know about who is subscribing its published messages, the `mandatory` and `immediate`
    flags are obviously unsuited, so we simply set them to false in this example (and
    all following ones).
  prefs: []
  type: TYPE_NORMAL
- en: You can now run this program, and it will connect to your local AMQP broker,
    declare an exchange, and publish a message. Of course, this message will not be
    routed anywhere and vanish. In order to actually process it, you will need a subscriber.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continue by creating a second Go program in which you connect to the AMQP broker
    and create a new channel just like in the previous section. However, now, instead
    of declaring an exchange and publishing a message, let''s declare a queue and
    bind it to that exchange:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'After having declared and bound a queue, you can now start consuming this queue.
    For this, use the channel''s `Consume()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Consume()` method takes the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: The name of the queue to be consumed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A string that uniquely identifies this consumer. When left empty (like in this
    case), a unique identifier will be automatically generated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the `autoAck` flag is set, received messages will be acknowledged automatically.
    When it is not set, you will need to explicitly acknowledge messages after processing
    them using the received message's `Ack()` method (see the following code example).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the `exclusive` flag is set, this consumer will be the only one allowed
    to consume this queue. When not set, other consumers might listen on the same
    queue.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `noLocal` flag indicated to the broker that this consumer should not be
    delivered messages that were published on the same channel.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `noWait` flag instructs the library not to wait for confirmation from the
    broker.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `args` argument may contain a map with additional configuration parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this example, `msgs` will be a channel (this time, meaning an actual Go
    channel, not an AMQP channel) of `amqp.Delivery` structs. In order to receive
    messages from the queue, we can simply read values from that channel. If you want
    to read messages continuously, the easiest way to do this is using a `range` loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Note that we explicitly acknowledge the message using the `msg.Ack` function
    in the preceding code. This is necessary because we have set the `Consume()` function's
    `autoAck` parameter to false, earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Explicitly acknowledging a message serves an important purpose—if your consumer
    fails for whatever reason between receiving and acknowledging the message, the
    message will be put back into the queue, and then redelivered to another consumer
    (or stay in the queue, if there are no other consumers). For this reason, a consumer
    should only acknowledge a message when it has finished processing it. If a message
    is acknowledged before it was actually processed by the consumer (which is what
    the `autoAck` parameter would cause), and the consumer then unexpectedly dies,
    the message will be lost forever. For this reason, explicitly acknowledging messages
    is an important step in making your system resilient and failure-tolerant.
  prefs: []
  type: TYPE_NORMAL
- en: Building an event emitter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the preceding example, we used AMQP channels to send simple string messages
    from publisher to subscriber. In order to use AMQP to build an actual publish/subscribe
    architecture, we will need to transmit more complex messages with structured data.
    In general, each AMQP message is simply a string of bytes. To submit structured
    data, we can use serialization formats, such as JSON or XML. Also, since AMQP
    is not limited to ASCII messages, we could also use binary serialization protocols
    such as `MessagePack` or `ProtocolBuffers`.
  prefs: []
  type: TYPE_NORMAL
- en: For whichever serialization format you decide, you need to make sure that both
    publisher and subscriber understand both the serialization format and the actual
    internal structure of the messages.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding the serialization format, we will take the easy choice in this chapter
    and use the JSON serialization format. It is widely adopted; serializing and unserializing
    messages are easily done using Go standard libraries and also in other programming
    languages (which is important—although in this book we have committed ourselves
    exclusively to Go, it is common in microservice architectures to have lots of
    different application runtimes).
  prefs: []
  type: TYPE_NORMAL
- en: We also need to make sure that both publisher and subscribers know how the messages
    will be structured. For example, a `LocationCreated` event might have a `name`
    property and an `address` property. To solve this issue, we will introduce a shared
    library that will contain struct definitions for all possible events, together
    with instructions for the JSON (un)serialization. This library can then be shared
    between the publisher and all subscribers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by creating the `todo.com/myevents/contracts` directory in your GOPATH.
    The first event type that we will describe is the `EventCreatedEvent` event. This
    message will later be published by the event service when a new event is created.
    Let''s define this event as a struct in the `event_created.go` file in the newly
    created package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, we need a possibility to generate a topic name for each event (in RabbitMQ,
    the topic name will also be used as a routing key for the messages). For this,
    add a new method—`EventName()`—to your newly defined struct:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now use a Go interface to define a generic event type. This type can
    be used to enforce that each event type actually implements an `EventName()` method.
    Since both event publisher and event subscriber will also later be used across
    multiple services, we will put the event interface code into the `todo.com/myevents/lib/msgqueue` package.
    Start by creating the package directory and a new file, `event.go`, within that
    package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Of course, our example application uses more events than just the `EventCreatedEvent`.
    For example, we also have a `LocationCreatedEvent` and an `EventBookedEvent`.
    Since showing all their implementations in print would be fairly repetitive, we
    would like to refer to the example files for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now continue by building an event emitter that can actually publish
    these messages to an AMQP broker. Since we will also explore other message brokers
    in later sections of this chapter, we will start by defining the interface that
    any event emitter should fulfil. For this, create a `emitter.go` file in the `msgqueue`
    package that was created before with the following contents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This interface describes the methods (actually, just one method) that all event
    emitter implementations need to fulfil. Let's now continue by creating a `todo.com/myevents/lib/msgqueue/amqp`
    subpackage with a `emitter.go` file. This file will contain a struct definition
    for the `AMQPEventEmitter`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following code example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Note how the `amqpEventEmitter` type is declared package-private, as it is declared
    with a lowercase name. This will prevent users from instantiating the `amqpEventEmitter`
    type directly. For a proper instantiation, we will provide a constructor method,
    instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s add a `setup` method that we can use to declare the exchange that
    this publisher is going to publish into:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: You might be wondering why we created a new AMQP channel in this method and
    closed it immediately after declaring the exchange. After all, we could reuse
    this same channel for publishing messages later. We will get to that in a moment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continue by adding a constructor function—`NewAMQPEventEmitter`—for building
    new instances of this struct:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, to the actual heart of the `amqpEventEmitter` event—the `Emit` method.
    First, we will need to transform the event that has been passed into the method
    as a parameter into a JSON document:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can create a new AMQP channel and publish our message to the events
    exchange:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Note that we used the `Headers` field of `amqp.Publishing` to add the event
    name in a special message header. This will make it easier for us to implement
    the event listener later.
  prefs: []
  type: TYPE_NORMAL
- en: Also, note that we are creating a new channel for each published message within
    this code. While it is, in theory, possible to reuse the same channel for publishing
    multiple messages, we need to keep in mind that a single AMQP channel is not thread-safe.
    This means that calling the event emitter's `Emit()` method from multiple go-routines
    might lead to strange and unpredictable results. This is exactly the problem that
    AMQP channels are there to solve; using multiple channels, multiple threads can
    use the same AMQP connection.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can integrate our new event emitter into the existing event service
    that you have already built in [Chapter 2](def2621c-74c4-4f60-a37a-b0b2f86c6339.xhtml), *Building
    Microservices Using Rest APIs*, and [Chapter 3](9c1db13f-619b-43a7-96a1-c6fc65e13b67.xhtml),* Securing
    Microservices*. Start by adding a configuration option for the AMQP broker in
    the `ServiceConfig` struct:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This allows you to specify the AMQP broker via the JSON configuration file.
    In the `ExtractConfiguration()` function, we can also add a fallback that optionally
    extracts this value from an environment variable, if set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now use this configuration option to construct a new event emitter in
    the event service''s `main` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now pass this event emitter into the `rest.ServeAPI` function, which
    can, in turn, pass it into the `newEventHandler` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The event emitter can then be stored as a field in the `eventServiceHandler`
    struct:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the `eventServiceHandler` holds a reference to the event emitter that
    you can use in the actual REST handlers. This allows you, for example, to emit
    an `EventCreatedEvent` whenever a new event is created via the API. For this,
    modify the `newEventHandler` method of `eventServiceHandler`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Building an event subscriber
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we can publish events on a `RabbitMQ` broker using the `EventEmitter`,
    we also need a possibility to listen to these events. This will be the purpose
    of the `EventListener`, which we will build in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Like before, let''s start by defining the interface that all event listeners
    (the AMQP event listener being one of them) should fulfil. For this, create the `listener.go` file
    in the `todo.com/myevents/lib/msgqueue` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This interface looks quite different than the event emitter''s interface. This
    is because each call to the event emitter''s `Emit()` method simply publishes
    one message immediately. However, an event listener is typically active for a
    long time and needs to react to incoming messages whenever they may be received.
    This reflects in the design of our `Listen()` method: first of all, it will accept
    a list of event names for which the event listener should listen. It will then
    return two Go channels: the first will be used to stream any events that were
    received by the event listener. The second will contain any errors that occurred
    while receiving those events.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start with building the AMQP implementation by creating a new `listener.go` file
    in the `todo.com/myevents/lib/msgqueue/amqp` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Similar to the event emitter, continue by adding a `setup` method. In this
    method, we will need to declare the AMQP queue that the listener will be consuming:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Note that the name of the queue that the listener will consume is configurable
    using the `amqpEventListener` struct's `queue` field. This is because later, multiple
    services will use the event listener to listen to their events, and each service
    will require its own AMQP queue for this.
  prefs: []
  type: TYPE_NORMAL
- en: You may have noticed that we did not yet actually bind the newly declared queue
    to the events exchange. This is because we do not know yet which events we actually
    have to listen for (remember the `Listen` method's `events` parameter?).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s add a constructor function to create new AMQP event listeners:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'With the possibility to construct new AMQP event listeners, let''s implement
    the actual `Listen()` method. The first thing to do is use the `eventNames` parameter
    and bind the event queue accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can use the channel''s `Consume()` method to receive messages from
    the queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The `msgs` variable now holds a channel of `amqp.Delivery` structs. However,
    our event listener is supposed to return a channel of `msgqueue.Event`. This can
    be solved by consuming the `msgs` channel in our own goroutine, build the respective
    event structs, and then publish these in another channel that we return from this
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The tricky part is now within the inner goroutine. Here, we will need to map
    the raw AMQP message to one of the actual event structs (as the `EventCreatedEvent`
    defined before).
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember how the EventEmitter added an additional `x-event-name` header to
    the AMQP message when publishing events? This is something that we can use now
    to map these messages back to their respective struct types. Let''s start by extracting
    the event name from the AMQP message headers:'
  prefs: []
  type: TYPE_NORMAL
- en: All of the following code goes into the inner `range` loop of the `Listen` method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code tries to read the `x-event-name` header from the AMQP message.
    Since the `msg.Headers` attribute is basically a `map[string]interface{}`, we
    will need a few map index and type assertions until we can actually use the event
    name. In case a message is received that does not contain the required header,
    an error will be written into the errors channel. Also, the message will be nack'ed
    (short for negative acknowledgment), indicating to the broker that it could not
    be successfully processed.
  prefs: []
  type: TYPE_NORMAL
- en: 'After knowing the event name, we can use a simple switch/case construct to
    create a new event struct from this name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Building the booking service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have an event listener, we can use it to implement the booking service.
    Its general architecture will follow that of the event service, so we will not
    go too much into detail on that matter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by creating a new package `todo.com/myevents/bookingservice` and create
    a new `main.go` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'This will set up the booking service with both a database connection and working
    event listener. We can now use this event listener to listen to the events emitted
    by the event service. For this, add a new subpackage `todo.com/myevents/bookingservice/listener`
    and create a new `event_listener.go\` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: In the `ProcessEvents()` function, we are calling the event listener's `Listen`
    function to listen for newly created events. The `Listen` function returns two
    channels, one for received messages and one for errors that occur during listening.
    We will then use an infinitely running for loop and a select statement to read
    from both of these channels at once. Received events will be passed to the `handleEvent`
    function (which we still need to write), and received errors will be simply printed
    to the standard output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s continue with the `handleEvent` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: This function uses a type switch to determine the actual type of the incoming
    event. Currently, our event listener processes the two events, `EventCreated`
    and `LocationCreated`, by storing them in their local database.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we are using a shared library `todo.com/myevents/lib/persistence` for
    managing database access. This is for convenience only. In real microservice architectures,
    individual microservices typically use completely independent persistence layers
    that might be built on completely different technology stacks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our `main.go` file, we can now instantiate the `EventProcessor` and call
    the `ProcessEvents()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Apart from listening to events, the booking service also needs to implement
    its own REST API that can be used by users to book tickets for a specified event.
    This will follow the same principles that you have already learned about in [Chapter
    2](def2621c-74c4-4f60-a37a-b0b2f86c6339.xhtml), *Building Microservices Using
    Rest APIs*, and [Chapter 3](9c1db13f-619b-43a7-96a1-c6fc65e13b67.xhtml),* Securing
    Microservices*. For this reason, we will refrain from explaining the Booking Service's
    REST API in detail and just describe the highlights. You can find a full implementation
    of the REST service in the code examples for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `main.go` file, we will need to move the `processor.ProcessEvents()`
    call into its own go-routine. Otherwise, it would block and the program would
    never reach the `ServeAPI` method call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we will move on to the actual request handler. It is registered for
    POST requests at `/events/{eventID}/bookings`; it checks how many bookings are
    currently placed for this event and whether the event's location still has the
    capacity for one more booking. In this case, it will create and persist a new
    booking and emit an `EventBooked` event. Take a look at the example files to view
    the full implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Event sourcing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building your applications using asynchronous messaging opens the door for applying
    some advanced architectural patterns, one of which you will learn about in this
    section.
  prefs: []
  type: TYPE_NORMAL
- en: When using messaging, publish/subscribe, and event collaboration, every change
    in the entire system's state is reflected in the form of an event that is emitted
    by one of the participating services. Often, each of these services has its own
    database, keeping its own view on the system's state (at least, as far as required)
    and staying up to date by continually listening to the events that are published
    by the other services.
  prefs: []
  type: TYPE_NORMAL
- en: However, the fact that each change in the system state is also represented by
    a published event presents an interesting opportunity. Imagine that someone recorded
    and saved each and every event that was published by anyone into an event log.
    In theory (and also in practice), you can use this event log to reconstruct the
    entire system state, without having to rely on any other kind of database.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, consider the following (small) event log:'
  prefs: []
  type: TYPE_NORMAL
- en: '8:00 am—User #1 with name Alice was created'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '9:00 am—User #2 with name Bob was created'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '1:00 pm—User #1 was deleted'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '3:00 pm—User #2 changes name to Cedric'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By replaying these events, it is easy to reconstruct the state of your system
    at the end of the day—there is one user named Cedric. However, there is more.
    Since each event is timestamped, you can reconstruct the state that your application
    had at any given point in time (for example, at 10:00 am, your application had
    two users, Alice and Bob).
  prefs: []
  type: TYPE_NORMAL
- en: Besides point-in-time recovery, event sourcing offers you a complete audit log
    over everything that happened in your system. Audit logging often is an actual
    requirement on its own in many cases, but also makes it easier to debug the system
    in case of errors. Having a complete event log allows you to replicate the system's
    state at the exact point in time and then replay events step by step to actually
    reproduce a specific error.
  prefs: []
  type: TYPE_NORMAL
- en: Also, having an event log makes the individual services less dependent on their
    local databases. In the extreme, you can abandon databases entirely and have each
    service reconstruct its entire query model from the event log in-memory each time
    it starts up.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing publish/subscribe and event sourcing with Apache Kafka
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the remainder of this chapter, we will not build our own event sourcing system.
    Previously, we used RabbitMQ to accomplish messaging between our services. However,
    RabbitMQ only handles message dispatching, so if you need an event log containing
    all events, you will need to implement it yourself by listening to all events
    and persisting them. You will also need to take care of event replaying yourself.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Kafka is a distributed message broker that also ships with an integrated
    transaction log. It was originally built by LinkedIn and is available as an open
    source product licensed under the Apache License.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding section, we already built implementations of the `EventEmitter`
    and `EventListener` interfaces using an AMQP connection. In this section, we will
    implement the same interfaces using Kafka.
  prefs: []
  type: TYPE_NORMAL
- en: Kafka quickstart with Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Contrary to RabbitMQ, Apache Kafka is a bit more complex to set up. Kafka itself
    requires a working Zookeeper setup in order to perform leader election, managing
    cluster state, and persisting cluster-wide configuration data. However, for development
    purposes, we can use the `spotify/kafka` image. This image comes with a built-in
    Zookeeper installation, allowing quick and easy setup.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just as with the RabbitMQ image before, use the `docker run` command to get
    started quickly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: This will start a single-node Kafka instance and bind it to the localhost TCP
    port `9092`.
  prefs: []
  type: TYPE_NORMAL
- en: Basic principles of Apache Kafka
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kafka offers a publish/subscribe message broker, but is not based on AMQP and
    therefore uses a different terminology.
  prefs: []
  type: TYPE_NORMAL
- en: The first basic concept in Kafka is the topic. A topic is something like a category
    or event name that subscribers can write to. It contains a complete log of all
    messages that were ever published into this topic. Each topic is divided into
    a configurable number of partitions. When a new message is published, it needs
    to contain a partition key. The partition key is used by the broker to decide
    into which partition of the topic the message should be written.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/228af662-564a-48ba-90ca-d1c11e87a40f.png)'
  prefs: []
  type: TYPE_IMG
- en: Each Kafka topic consists of a configurable number of partitions; each published
    message has a partition key, which is used to decide into which partition a message
    should be saved
  prefs: []
  type: TYPE_NORMAL
- en: The Kafka broker guarantees that within each partition, the order of messages
    will be the same as in which they were published. For each topic, messages will
    be kept for a configurable retention period. However, the broker's performance
    does not degrade significantly when the transaction logs get larger. For this
    reason, it is entirely possible to operate Kafka with an infinite retention period,
    and by this way use it as an event log. Of course, you do need to consider that
    the required disk storage will grow proportionally. Luckily, Kafka supports horizontal
    scale-out quite well.
  prefs: []
  type: TYPE_NORMAL
- en: From each topic, any number of subscribers (called *consumers* in Kafka jargon)
    can read messages and any number of publishers (*producers*) can write them. Each
    consumer defines for itself at which offset in the event log it would like to
    start consuming. For example, a freshly initialized consumer that only operates
    in-memory could read the entire event log from the start (offset = `0`) to rebuild
    its entire query model. Another consumer that has a local database and only needs
    new events that occurred after a certain point in time can start reading the event
    log at a later point.
  prefs: []
  type: TYPE_NORMAL
- en: Each consumer is a member of a consumer group. A message published in a given
    topic is published to one consumer of each group. This can be used to implement
    a publish/subscribe communication, similar to what we have already built with
    AMQP. The following figure illustrates the different terms and actors in a publish/subscribe
    architecture using AMQP and Kafka. In both cases, every message that is published
    in the exchange/topic will be routed to every consumer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/27552bd3-67e3-4bf0-9a6b-7a99d8bdef10.png)'
  prefs: []
  type: TYPE_IMG
- en: Publish/Subscribe with both AMQP (1) and Apache Kafka (2); each message that
    is published in the exchange/topic is routed to every subscriber
  prefs: []
  type: TYPE_NORMAL
- en: In AMQP, you can also have multiple subscribers listen on the same queue. In
    this case, incoming messages will be routed not to all, but to one of the connected
    subscribers. This can be used to build some kind of load-balancing between different
    subscriber instances.
  prefs: []
  type: TYPE_NORMAL
- en: 'The same can be built in Kafka by putting multiple subscriber instances into
    the same consumer group. In Kafka, however, each subscriber is assigned to a fixed
    set of (possibly multiple) partitions. For this reason, the number of consumers
    that can consume a topic in parallel is limited by the number of topic partitions.
    The following diagram illustrates this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00d574d2-a4b6-4166-a6c2-9b88b89e846a.png)'
  prefs: []
  type: TYPE_IMG
- en: Load-balancing with both AMQP (1) and Apache Kafka (2); each message that is
    published in the exchange/topic routed to one of the connected subscribers
  prefs: []
  type: TYPE_NORMAL
- en: If you should decide to have multiple consumers within the same consumer group
    subscribe the same partition of a topic, the broker will simply dispatch all messages
    in that partition to the consumer that connected last.
  prefs: []
  type: TYPE_NORMAL
- en: Connecting to Kafka with Go
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we connected to an AMQP broker in the previous sections of this chapter,
    we used the de facto standard library `github.com/streadway/amqp`. For connecting
    to a Kafka broker, there is a little more diversity among the available Go libraries.
    At the time of writing this book, the most popular Kafka client libraries for
    Go are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`github.com/Shopify/sarama` offers full protocol support and is implemented
    in pure Go. It is licensed under the MIT license. It is actively maintained.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`github.com/elodina/go_kafka_client` is also implemented in pure Go. It offers
    more features than the `Shopify` library, but appears to be less actively maintained.
    It is licensed under the Apache license.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`github.com/confluentinc/confluent-kafka-go` provides a Go wrapper for the
    `librdkafka` C library (meaning that you will need to have `librdkafka` installed
    on your system for this library to work). It is reported to be faster than the
    `Shopify` library since it relies on a highly optimized C library. For the same
    reason though, it might prove difficult to build. It is actively maintained, although
    its community seems smaller than the `Shopify` library.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For this chapter, we will use the `github.com/Shopify/sarama` library. Start
    by installing it via `go get`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: In the previous sections, we have already defined the `EventEmitter` and `EventListener`
    interfaces in the `todo.com/myevents/lib/msgqueue` package. In this section, we
    will now add alternative implementations for these two interfaces. Before diving
    in, let's take a quick look at how to use the `sarama` library to connect to a
    Kafka broker, in general.
  prefs: []
  type: TYPE_NORMAL
- en: 'Regardless of whether you intend to publish or consume messages, you will need
    to start by instantiating a `sarama.Client` struct. For this, you can use the
    `sarama.NewClient` function. For instantiating a new client, you will need a list
    of Kafka broker addresses (remember, Kafka is designed for being operated in a
    cluster, so you can actually connect to many clustered brokers at the same time)
    and a configuration object. The easiest way to create a configuration object is
    the `sarama.NewConfig` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Of course, using `localhost` as a single broker works fine in a development
    setup. For a production setup, the broker list should be read from the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: You can use the `config` object to fine-tune various parameters of your Kafka
    connection. For most purposes, the default settings will do just fine, though.
  prefs: []
  type: TYPE_NORMAL
- en: Publishing messages with Kafka
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Sarama library offers two implementations for publishing messages—the `sarama.SyncProducer`
    and the `sarama.AsyncProducer`.
  prefs: []
  type: TYPE_NORMAL
- en: The `AsyncProducer` offers an asynchronous interface that uses Go channels both
    for publishing messages and for checking the success of these operations. It allows
    high-throughput of messages, but is a bit bulky to use if all you want to do is
    to emit a single message. For this reason, the `SyncProducer` offers a simpler
    interface that takes a message for producing and blocks until it receives confirmation
    from the broker that the message has been successfully published to the event
    log.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can instantiate a new Producer using the `sarama.NewSyncProducerFromClient`
    and `sarama.NewAsyncProducerFromClient` functions. In our example, we will use
    the `SyncProducer` that you can create as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s continue by using the `SyncProducer` to create a Kafka implementation
    of our `EventEmitter` interface. Start by creating the `todo.com/myevents/lib/msgqueue/kafka` package
    and the `emitter.go`  file within that package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Continue by adding a constructor function for instantiating this struct:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to emit messages, you will need to construct an instance of the `sarama.ProducerMessage`
    struct. For this, you will need the topic (which, in our case, is supplied by
    the `msgqueue.Event`''s `EventName()` method) and the actual message body. The
    body needs to be supplied as an implementation of the `sarama.Encoder` interface.
    You can use the `sarama.ByteEncoder` and `sarama.StringEncoder` types to simply
    typecast a byte array or a string to an `Encoder` implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: The key in this code sample is the producer's `SendMessage()` method. Note that
    we are actually ignoring a few of this method's return values. The first two return
    values return the number of the partition that the messages were written in and
    the offset number that the message has in the event log.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding code works, but has one fatal flaw: it creates a new Kafka topic
    for each event type. While it is entirely possible for a subscriber to consume
    multiple topics at once, you will have no guaranteed order of processing. This
    may result in a producer emitting a `location #1 created` and `location #1 updated` sequentially
    in short order and a subscriber receiving them in the other order.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to solve this problem, we will need to do two things:'
  prefs: []
  type: TYPE_NORMAL
- en: All messages must be published on the same topic. This implies that we will
    need another way to store the actual event name within the message.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each message must expose a partition key. We can use the message's partition
    key to ensure that messages concerning the same entity (that is, the same event,
    the same user) are stored in a single partition of the event log and are routed
    to the same consumer in-order.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s start with the partitioning key. Remember the `Event` interface in the
    `todo.com/myevents/lib/msgqueue` package? It looked like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Continue by adding a new method `PartitionKey()` to this interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can modify the existing event structs that we have defined before
    (for example, `EventCreatedEvent`) to implement this `PartitionKey()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s return to the `kafkaEventEmitter`. We can now use each event''s
    `PartitionKey()` method when publishing a message to Kafka. Now, we just need
    to send the event name alongside the event. To solve this issue, we will use an
    envelope for the message body: this means that the message body will not just
    contain the JSON-serialized event object, but rather another object that can contain
    metadata (like the event name) and the actual event body as payload. Let''s define
    this event in a new file `payload.go` in the package `todo.com/myevents/lib/msgqueue/kafka`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now adjust the `kafkaEventEmitter` to first construct an instance of
    the `messageEnvelope` struct and then JSON-serialize that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Consuming messages from Kafka
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Consuming Messages from a Kafka broker is a little bit more complex than in
    AMQP. You have already learned that a Kafka topic may consist of many partitions
    that each consumer can consume one or more (up to all) of these partitions. Kafka
    architectures allow horizontal scaling by dividing a topic into more partitions
    and having one consumer subscribe to each partition.
  prefs: []
  type: TYPE_NORMAL
- en: This means that each subscriber needs to know which partitions of a topic exist
    and which of those it should consume. Some of the libraries that we introduced
    earlier in this section (especially the Confluent library) actually support automatic
    subscriber partitioning and automatic group balancing. The `sarama` library does
    not offer this feature, so our `EventListener` will need to select the partitions
    it wants to consume manually.
  prefs: []
  type: TYPE_NORMAL
- en: For our example, we will implement the `EventListener` so that it, by default,
    listens on all available partitions of a topic. We'll add a special property that
    can be used to explicitly specify the partitions to listen on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new file `listener.go` in the `todo.com/myevents/lib/msgqueue/kafka`
    package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Continue by adding a constructor function for this struct:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Listen()` method of `kafkaEventListener` follows the same interface as
    the `amqpEventListener` that we implemented in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'The first thing to do is to determine which topic partitions should be consumed.
    We will assume that the listener should listen on all partitions when an empty
    slice was passed to the `NewKafkaEventListener` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'A Sarama consumer can only consume one partition. If we want to consume multiple
    partitions, we will need to start multiple consumers. In order to preserve the
    `EventListener`''s interface, we will start multiple consumers, each in its own
    goroutine in the `Listen()` method and then have them all write to the same results
    channel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Note the goroutines that are started within the first for loop. Each of these
    contains an inner for loop that iterates over all messages received in a given
    partition. We can now JSON-decode the incoming messages and reconstruct the appropriate
    event types.
  prefs: []
  type: TYPE_NORMAL
- en: All of the following code examples are placed within the inner for loop of the`Listen()`
    method of `kafkaEventListener`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have a new problem. We have unmarshalled the event body into a `messageEnvelope`
    struct. This contains the event name and the actual event body. However, the event
    body is just typed as `interface{}`. Ideally, we would need to convert this `interface{}`
    type back to the correct event type (for example, `contracts.EventCreatedEvent`)
    dependent on the event name. For this, we can use the `github.com/mitchellh/mapstructure`
    package that you can install via go get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'The `mapstructure` library works similar to the `encoding/json` library, only
    that it does not take `[]byte` input variables, but generic `interface{}` input
    values. This allows you to take JSON input of unknown structure (by calling `json.Unmarshal`
    on an `interface{}` value) and then later map the already-decoded type of unknown
    structure to a well-known struct type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: The `TagName` property in the `mapstructure.DecoderConfig` struct that is created
    before doing the actual decoding instructs the `mapstructure` library to respect
    the existing ``json:"..."`` annotations that are already present in the event
    contracts.
  prefs: []
  type: TYPE_NORMAL
- en: 'After successfully decoding a message, it can be published into the results
    channel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Our Kafka event listener is now fully functional. Since it implements the `msgqueue.EventListener`
    interface, you can use it as a drop-in replacement for the existing AMQP event
    listener.
  prefs: []
  type: TYPE_NORMAL
- en: There is one caveat, though. When started, our current Kafka event listener
    always starts consuming from the very start of the event log. Have a closer look
    at the `ConsumePartition` call in the preceding code example—its third parameter
    (in our case, `0`) describes the offset in the event log at which the consumer
    should start consuming.
  prefs: []
  type: TYPE_NORMAL
- en: Using `0` as an offset will instruct the event listener to read the entire event
    log right from the start. This is the ideal solution if you want to implement
    event sourcing with Kafka. If you just want to use Kafka as a message broker,
    your service will need to remember the offset of the last message read from the
    event log. When your service is restarted, you can then resume consuming from
    that last known position.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to integrate multiple services with asynchronous
    communication using message queues such as RabbitMQ and Apache Kafka. You also
    learned about architectural patterns such as event collaboration and event sourcing
    that help you to build scalable and resilient applications that are well-suited
    for cloud deployments.
  prefs: []
  type: TYPE_NORMAL
- en: The technologies that we have worked with in this chapter are not tied to any
    specific cloud provider. You can easily roll your own RabbitMQ or Kafka infrastructure
    on any cloud infrastructure or your own servers. In [Chapter 8](25f18fd2-4d08-41fb-a8b2-acc927bd0876.xhtml),
    *AWS Part II - S3, SQS, API Gateway, and DynamoDB*, we will take another look
    at message queues—this time with a special focus on the managed messaging solutions
    that are offered to you by AWS.
  prefs: []
  type: TYPE_NORMAL
