- en: Asynchronous Microservice Architectures Using Message Queues
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the past two chapters, you learned how to build REST-based microservices
    with the Go programming language. The REST architectural style is both simple
    and flexible at the same time, which makes it an excellent choice for many use
    cases. However, being built on top of HTTP, all communication in a REST architecture
    will follow the client/server model with request/reply transactions. In some use
    cases, this might be restrictive and other communication models might be better
    suited.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will introduce the publish/subscribe communication model,
    along with the technologies that you need to implement it. Typically, publish/subscribe
    architectures require a central infrastructure component—the message broker. In
    the open source world, there are many different implementations of message brokers;
    so, in this chapter, we will introduce two different message brokers that we feel
    to be among the most important ones—**RabbitMQ** and **Apache Kafka**. Both are
    suited for specific use cases; you will learn how to set up each of these two
    message brokers, how to connect your Go application, and when you should use one
    or the other.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: We will then show you how to use this knowledge in order to extend the event
    management microservice that you have worked in the previous chapters to publish
    an event whenever something important happens. This allows us to implement a second
    microservice that listens on those events. You will also learn about advanced
    architectural patterns that usually work well alongside asynchronous communication,
    such as *event collaboration* and *event sourcing*, and how (and when) to use
    them in your application.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: The publish/subscribe architectural pattern
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Event collaboration
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Event sourcing
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AMQP with RabbitMQ
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Kafka
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The publish/subscribe pattern
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The publish/subscribe pattern is a communication pattern alternative to the
    well-known request/reply pattern. Instead of a client (issuing a request) and
    a server (replying with a response to that request), a publish/subscribe architecture
    consists of publishers and subscribers.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Each publisher can emit messages. It is of no concern to the publisher who actually
    gets these messages. This is the concern of the subscribers; each subscriber can
    subscribe to a certain type of message and be notified whenever a publisher publishes
    a given type of message. In reverse, each subscriber does not concern itself with
    where a message actually came from.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e967a901-db08-4ba1-a019-ab96d69ff7bc.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
- en: The request/reply and the publish/subscribe communication patterns
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: In practice, many publish/subscribe architectures require a central infrastructure
    component—the message broker. Publishers publish messages at the message broker,
    and subscribers subscribe to messages at the message broker. One of the broker's
    main tasks then is to route published messages to the subscribers that have expressed
    interest in them.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Typically, messages will be routed **topic-based**. This means that each publisher
    specified a topic for a published message (a topic usually just being a string
    identifier, for example, `user.created`). Each subscriber will also subscribe
    to a certain topic. Often, a broker will also allow a subscriber to subscribe
    to an entire set of topic using wildcard expressions such as `user.*`.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast to request/reply, the publish/subscribe pattern brings some clear
    advantages:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Publishers and subscribers are very loosely coupled. This goes to the extent
    that they do not even know about one another.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A pub/sub architecture is very flexible. It is possible to add new subscribers
    (and, therefore, extend existing processes) without having to modify the publisher.
    The inverse also applies; you can add new publishers without having to modify
    the subscribers.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In case the messages are routed by a message broker, you also gain resiliency.
    Usually, the message broker stores all messages in a queue, in which they are
    kept until they have been processed by a subscriber. If a subscriber becomes unavailable
    (for example, due to a failure or an intentional shutdown), the messages that
    should have been routed to that subscriber will become queued until the subscriber
    is available again.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Often, you will also get some kind of reliability guaranteed by the message
    broker on a protocol level. For example, RabbitMQ guarantees *reliable delivery*
    by requiring each subscriber to acknowledge a received message. Only when the
    message has been acknowledged, the broker will remove the message from the queue.
    If the subscriber should fail (for example, by disconnection) when a message had
    already been delivered, but not yet acknowledged, the message will be put back
    into the message queue. If another subscriber listens on the same message queue,
    the message might be routed to that subscriber; otherwise, it will remain in the
    queue until the subscriber is available again.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can easily scale out. In case that too many messages are published for a
    single subscriber to efficiently handle them, you can add more subscribers and
    have the message broker load-balance the messages sent to these subscribers.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, introducing a central infrastructure component such as a message
    broker brings its own risk. When not done right, your message broker might become
    a single point of failure, taking your entire application down with it in case
    it fails. When introducing a message broker in a production environment, you should
    take appropriate measures to ensure high-availability (usually by clustering and
    automatic failover).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: In case your application is run in a cloud environment, you may also take advantage
    of one of the managed message queuing and delivery services that are offered by
    the cloud providers, for example, AWS **Simple Queue Service** (**SQS**) or the
    Azure Service Bus.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn how to use two of the most popular open source
    message brokers—RabbitMQ and Apache Kafka. In [Chapter 8](25f18fd2-4d08-41fb-a8b2-acc927bd0876.xhtml),
    *AWS Part II - S3, SQS, API Gateway, and DynamoDB*, you will learn about AWS SQS.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the booking service
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will start by implementing a publish/subscribe architecture
    using RabbitMQ. For this, we will need new microservices to our architecture—the
    booking service will handle bookings for events. Its responsibilities will include
    making sure that events are not overbooked. For this, it will need to know about
    existing events and locations. In order to achieve this, we will modify the **EventService**
    to emit events whenever a location or an event was created (yes, the terminology
    is confusing—make sure not to mistake the *notification that something has happened*
    kind-of-event with the *Metallica is playing here* kind-of-event). The **BookingService**
    can then listen to these events and emit events itself whenever someone books
    a ticket for one of these events.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5e8ba22f-e4f5-4afe-a62e-a02471ec46b7.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
- en: An overview of our microservices and the events that they will be publishing
    and subscribing to
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Event collaboration
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Event collaboration describes an architectural principle that works well together
    with an event-driven publish/subscribe architecture.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following example that uses the regular request/reply communication
    pattern—a user requests the booking service to book a ticket for a certain event.
    Since the events are managed by another microservice (the **EventService**), the
    **BookingService** will need to request information on both the event and its
    location from the **EventService.** Only then can the **BookingService** check
    whether there are still seats available and save the user''s booking in its own
    database. The requests and responses required for this transaction are illustrated
    in the following diagram:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/33a76190-6cbc-46dc-a92c-74f6f5cf3070.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
- en: requests and responses
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, consider the same scenario in a publish/subscribe architecture, in which
    the **BookingService** and **EventService** are integrated using events: every
    time data changes in the **EventService**, it emits an event (for example, *a
    new location was created*, *a new event was created*, *an event was updated*,
    and so on).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the **BookingService** can listen to these events. It can build its own
    database of all currently existing locations and events. Now, if a user requests
    a new booking for a given event, the **BookingService** can simply use the data
    from its own local database, without having to request this data from another
    service. Refer to the following diagram for another illustration of this principle:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/42e36988-e61f-42cd-a54b-9e1133e416b3.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
- en: BookingService using the data from its own local database
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: This is the key point of an event collaboration architecture. In the preceding
    diagram, a service almost never needs to query another service for data, because
    it already knows everything it needs to know by listening to the events emitted
    by other services.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, this architectural pattern works extremely well together with publish/subscribe.
    In the preceding example, the **EventService** would be the publisher and the
    **BookingService** (potentially, among others) the subscriber. Of course, one
    might flinch at the fact that this principle will inevitably lead to redundant
    data being stored by the two services. However, this is not necessarily a bad
    thing—since every service constantly listens to events emitted by the other services,
    the entire dataset can be kept (eventually) consistent. Also, this increases the
    system's overall resiliency; for example, if the event service suffers a sudden
    failure, the **BookingService** would stay operational since it does not rely
    on the event service to be working anymore.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Implementing publish/subscribe with RabbitMQ
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the following section, you will learn how to implement a basic publish/subscribe
    architecture. For this, we will take a look at the **Advanced Message Queueing
    Protocol** (**AMQP**) and one of its most popular implementations, RabbitMQ.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: The Advanced Message Queueing Protocol
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: On a protocol level, RabbitMQ implements the AMQP. Before getting started with
    RabbitMQ, let's get started by taking a look at the basic protocol semantics of
    AMQP.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: 'An AMQP message broker manages two basic kinds of resources—**Exchanges** and
    **Queues**. Each publisher publishes its messages into an exchange. Each subscriber
    consumes a queue. The AMQP broker is responsible for putting the messages that
    are published in an exchange into the respective queue. Where messages go after
    they have been published to an exchange depends on the **exchange type** and the
    routing rules called **bindings**. AMQP knows three different types of exchanges:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '**Direct exchanges**: Messages are published with a given topic (called **routing
    key** in AMQP) that is a simple string value. Bindings between a direct exchange
    and queue can be defined to match exactly that topic.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fanout exchanges**: Messages are routed to all queues that are connected
    to a fanout exchange via a binding. Messages can have a routing key, but it will
    be ignored. Every bound queue will receive all messages that are published in
    the fanout exchange.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Topic exchanges**: This works similar to direct exchanges. However, queues
    are now bound to the exchange using patterns that the message''s routing key must
    match. Topic exchanges usually assume routing keys to be segmented with the dot
    character `''.''`. As an example, your routing keys could follow the `"<entityname>.<state-change>.<location>"`
    pattern (for example, `"event.created.europe"`). You can now create queue bindings
    that may contain wildcards using the `''*''` or `''#''` characters. `*` will match
    any single routing key segment, whereas `#` will match any number of segments.
    So, for the preceding example, valid bindings might be as follows:'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主题交换**：这与直接交换类似。但是，现在队列是使用消息的路由键必须匹配的模式绑定到交换。主题交换通常假定路由键使用句点字符`''.''`进行分段。例如，您的路由键可以遵循`"<entityname>.<state-change>.<location>"`模式（例如，`"event.created.europe"`）。现在可以创建包含通配符的队列绑定，使用`''*''`或`''#''`字符。`*`将匹配任何单个路由键段，而`#`将匹配任意数量的段。因此，对于前面的示例，有效的绑定可能如下：'
- en: '`event.created.europe` (obviously)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`event.created.europe`（显然）'
- en: '`event.created.*` (listen to whenever an event is created anywhere in the world)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`event.created.*`（每当在世界的任何地方创建事件时都会收到通知）'
- en: '`event.#` (listen to whenever any change is made to an event anywhere in the
    world)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`event.#`（每当在世界的任何地方对事件进行任何更改时都会收到通知）'
- en: '`event.*.europe` (listen to whenever any change is made to an event in Europe)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`event.*.europe`（每当在欧洲对事件进行任何更改时都会收到通知）'
- en: One possible example exchange and queue topology are shown in the next diagram.
    In this case, we have one service that publishes messages, the **EventService**.
    We have two queues in which messages will be routed. The first queue, **evts_booking**,
    will receive any and all messages that are related to any kind of change made
    to an event. The second queue, **evts_search**, will receive messages only regarding
    the creation of new events. Note that the **evts_booking** queue has two subscribers.
    When two or more subscribers subscribe to the same queue, the message broker will
    dispatch messages to one of the subscribers on a rotating basis.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个图表显示了一个可能的示例交换和队列拓扑结构。在这种情况下，我们有一个发布消息的服务**EventService**。我们有两个队列，消息将被路由到这两个队列中。第一个队列**evts_booking**将接收与事件的任何更改相关的所有消息。第二个队列**evts_search**将只接收关于新事件创建的消息。请注意，**evts_booking**队列有两个订阅者。当两个或更多订阅者订阅同一个队列时，消息代理将轮流将消息分发给其中一个订阅者。
- en: '![](img/cc668950-bec2-43b7-beb1-5ca489f7e80d.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cc668950-bec2-43b7-beb1-5ca489f7e80d.png)'
- en: Message broker displaying messages to one of the subscribers on a rotating basis
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 消息代理将消息轮流显示给其中一个订阅者
- en: It is important to note that the entire AMQP topology (meaning all the exchanges
    and queues and how they are bound to one another) is not defined by the broker,
    but by the publishers and consumers themselves. AMQP specifies several methods
    that clients can use to declare the exchanges and queues they need. For example,
    a publisher would typically use the `exchange.declare` method to assert that the
    exchange it wants to publish actually exists (the broker will then create it if
    it did not exist before). On the other hand, a subscriber might use the `queue.declare`
    and `queue.bind` methods to declare a queue that it wants to subscribe and bind
    it to an exchange.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，整个AMQP拓扑（即所有交换和队列以及它们如何相互绑定）不是由代理定义的，而是由发布者和消费者自己定义的。AMQP指定了客户端可以使用的几种方法来声明它们需要的交换和队列。例如，发布者通常会使用`exchange.declare`方法来断言它想要发布的交换实际上存在（如果之前不存在，代理将创建它）。另一方面，订阅者可能会使用`queue.declare`和`queue.bind`方法来声明它想要订阅的队列，并将其绑定到一个交换。
- en: There are multiple open source message brokers that implement AMQP. One of the
    most popular ones (and also the one that we will be working within this chapter)
    is the RabbitMQ broker, an open source AMQP broker developed by **Pivotal** and
    made available under the **Mozilla Public License**. Other message brokers that
    implement AMQP are **Apache QPID** ([https://qpid.apache.org](https://qpid.apache.org))
    and **Apache ActiveMQ** ([http://activemq.apache.org](http://activemq.apache.org)).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 有多个实现AMQP的开源消息代理。其中最流行的之一（也是我们在本章中将要使用的）是RabbitMQ代理，这是一个由**Pivotal**开发并在**Mozilla
    Public License**下提供的开源AMQP代理。其他实现AMQP的消息代理包括**Apache QPID**（[https://qpid.apache.org](https://qpid.apache.org)）和**Apache
    ActiveMQ**（[http://activemq.apache.org](http://activemq.apache.org)）。
- en: Although we will use RabbitMQ in this example, the code written in this chapter
    should work will all kinds of AMQP implementations.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在这个例子中我们将使用RabbitMQ，但本章中编写的代码应该适用于所有类型的AMQP实现。
- en: RabbitMQ quickstart with Docker
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Docker快速启动RabbitMQ
- en: Before building our publish/subscribe architecture, you will need to set up
    a running RabbitMQ message broker in your development environment. The easiest
    way to get started with RabbitMQ is by using the official Docker images.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建我们的发布/订阅架构之前，您需要在开发环境中设置一个正在运行的RabbitMQ消息代理。使用官方的Docker镜像是开始使用RabbitMQ的最简单方法。
- en: For this example, we will assume that you have a working Docker installation
    on your local machine. Take a look at the official installation instructions to
    learn how you can install Docker on your operating system at: [https://docs.docker.com/engine/installation](https://docs.docker.com/engine/installation).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本例，我们将假设您的本地机器上已经安装了Docker。请查看官方安装说明，了解如何在您的操作系统上安装Docker：[https://docs.docker.com/engine/installation](https://docs.docker.com/engine/installation)。
- en: 'You can start a new RabbitMQ broker using the following command on your command
    line:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下命令在命令行上启动一个新的RabbitMQ代理：
- en: '[PRE0]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The preceding command will create a new container named `rabbitmq` on your machine.
    For this, Docker will use the `rabbitmq:3-management` image. This image contains
    the latest release of RabbitMQ 3 (at the time of writing, 3.6.6) and the management
    UI. The `-p 5672:5672` flag will instruct Docker to map the TCP port `5672` (which
    is the IANA-assigned port number for AMQP) to your `localhost` address. The `-p
    15672:15672` flag will do the same for the management user interface.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: After starting the container, you will be able to open an AMQP connection to
    `amqp://localhost:5672` and open the management UI in your browser at `http://localhost:15672`.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: When you are using Docker on Windows, you will need to substitute localhost
    with the IP address of your local Docker virtual machine. You can determine this
    IP address using the following command on the command line: `$ docker-machine
    ip default`.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: 'Regardless whether you are using docker-machine or a local Docker installation,
    the RabbitMQ user interface should look very much like it does in the following
    screenshot:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6075ceff-4af4-4e70-bfa2-de38dd0501c0.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
- en: RabbitMQ's management user interface
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Open the management interface in your browser (`http://localhost:15672` or your
    docker-machine IP address). The RabbitMQ image ships a default guest user whose
    password is also `guest`. When running RabbitMQ in production, this is, of course,
    the first thing that you should change. For development purposes, it will do fine.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Advanced RabbitMQ setups
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Docker-based setup described in the preceding section allows you to get
    started quickly and are also (with a few adjustments) suitable for production
    setups. If you do not want to use Docker for your message broker, you can also
    install RabbitMQ on most common Linux distribution from package repositories.
    For example, on Ubuntu and Debian, you can install RabbitMQ using the following
    commands:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Similar commands also work on **CentOS** and **RHEL**:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: For a production setup, you might want to consider setting up RabbitMQ as a
    cluster to ensure high availability. Take a look at the official documentation
    at [http://www.rabbitmq.com/clustering.html](http://www.rabbitmq.com/clustering.html)
    for more information on how to set up a RabbitMQ cluster.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Connecting RabbitMQ with Go
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For connecting to a RabbitMQ broker (or any AMQP broker, for that matter),
    we recommend that you use the `github.com/streadway/amqp` library (which is the
    de facto standard Go library for AMQP). Let''s start by installing the library:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You can then start by importing the library into your code. Open a new connection
    using the `amqp.Dial` method:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In this case, `"amqp://guest:guest@localhost:5672"` is the URL of your AMQP
    broker. Note that the user credentials are embedded into the URL. The `amqp.Dial`
    method returns a connection object on success, or `nil` and an error, otherwise
    (as usual in Go, make sure that you actually check for this error). Also, do not
    forget to close the connection using the `Close()` method when you do not need
    it anymore.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, it is usually not a good practice to hardcode connection details
    such as these (much fewer credentials) into your application. Remember what you
    learned about twelve-factor applications, and let''s introduce an environment
    variable `AMQP_URL` that we can use to dynamically configure the AMQP broker:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In AMQP, most operations are done not directly on the connection, but on channels.
    Channels are used to *multiplex* several virtual connections over one actual TCP
    connection.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Channels themselves are not thread-safe. In Go, we will need to keep this in
    mind and pay attention to not access the same channel from multiple goroutines.
    However, using multiple channels, with each channel being accessed by only one
    thread, is completely safe. So, when in doubt, it is best to create a new channel.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 'Continue by creating a new channel on the existing connection:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We can now use this channel object for some actual AMQP operations, for example,
    publishing messages and subscribing to messages.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Publishing and subscribing to AMQP messages
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before diving back into the MyEvents microservice architecture, let's take a
    look at the basic AMQP methods that we can use. For this, we will start by building
    a small example program that is capable of publishing messages to an exchange.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 'After opening a channel, a message publisher should declare the exchange into
    which it intends to publish messages. For this, you can use the `ExchangeDeclare()`
    method on the channel object:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As you can see, `ExchangeDeclare` takes quite a number of parameters. These
    are as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: The exchange name
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The exchange type (remember that AMQP knows `direct`, `fanout`, and `topic` exchanges)
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `durable` flag will cause the exchange to remain declared when the broker
    restarts
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `autoDelete` flag will cause the exchange to be deleted as soon as the channel
    that declared it is closed
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `internal` flag will prevent publishers from publishing messages into this
    queue
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `noWait` flag will instruct the `ExchangeDeclare` method not to wait for
    a successful response from the broker
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `args` argument may contain a map with additional configuration parameters
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After having declared an exchange, you can now publish a message. For this,
    you can use the channel''s `Publish()` method. The emitted message will be an
    instance of the `amqp.Publishing` struct that you need to instantiate at first:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then, use the `Publish()` method to publish your message:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The `Publish()` method takes the following parameters:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: The name of the exchange to publish to
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The message's routing key
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `mandatory` flag will instruct the broker to make sure that the message
    is actually routed into at least one queue
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `immediate` flag will instruct the broker to make sure that the message
    is actually delivered to at least one subscriber
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `msg` argument contains the actual message that is to be published
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a publish/subscribe architecture, in which a publisher does not need to
    know about who is subscribing its published messages, the `mandatory` and `immediate`
    flags are obviously unsuited, so we simply set them to false in this example (and
    all following ones).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: You can now run this program, and it will connect to your local AMQP broker,
    declare an exchange, and publish a message. Of course, this message will not be
    routed anywhere and vanish. In order to actually process it, you will need a subscriber.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: 'Continue by creating a second Go program in which you connect to the AMQP broker
    and create a new channel just like in the previous section. However, now, instead
    of declaring an exchange and publishing a message, let''s declare a queue and
    bind it to that exchange:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'After having declared and bound a queue, you can now start consuming this queue.
    For this, use the channel''s `Consume()` function:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The `Consume()` method takes the following parameters:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: The name of the queue to be consumed.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A string that uniquely identifies this consumer. When left empty (like in this
    case), a unique identifier will be automatically generated.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the `autoAck` flag is set, received messages will be acknowledged automatically.
    When it is not set, you will need to explicitly acknowledge messages after processing
    them using the received message's `Ack()` method (see the following code example).
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the `exclusive` flag is set, this consumer will be the only one allowed
    to consume this queue. When not set, other consumers might listen on the same
    queue.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `noLocal` flag indicated to the broker that this consumer should not be
    delivered messages that were published on the same channel.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `noWait` flag instructs the library not to wait for confirmation from the
    broker.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `args` argument may contain a map with additional configuration parameters.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this example, `msgs` will be a channel (this time, meaning an actual Go
    channel, not an AMQP channel) of `amqp.Delivery` structs. In order to receive
    messages from the queue, we can simply read values from that channel. If you want
    to read messages continuously, the easiest way to do this is using a `range` loop:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，`msgs`将是一个通道（这次是一个实际的Go通道，而不是一个AMQP通道）的`amqp.Delivery`结构。为了从队列中接收消息，我们可以简单地从该通道中读取值。如果要连续读取消息，最简单的方法是使用`range`循环：
- en: '[PRE12]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note that we explicitly acknowledge the message using the `msg.Ack` function
    in the preceding code. This is necessary because we have set the `Consume()` function's
    `autoAck` parameter to false, earlier.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在前面的代码中，我们使用`msg.Ack`函数显式确认消息。这是必要的，因为我们之前将`Consume()`函数的`autoAck`参数设置为false。
- en: Explicitly acknowledging a message serves an important purpose—if your consumer
    fails for whatever reason between receiving and acknowledging the message, the
    message will be put back into the queue, and then redelivered to another consumer
    (or stay in the queue, if there are no other consumers). For this reason, a consumer
    should only acknowledge a message when it has finished processing it. If a message
    is acknowledged before it was actually processed by the consumer (which is what
    the `autoAck` parameter would cause), and the consumer then unexpectedly dies,
    the message will be lost forever. For this reason, explicitly acknowledging messages
    is an important step in making your system resilient and failure-tolerant.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 显式确认消息具有重要目的——如果您的消费者在接收和确认消息之间由于任何原因失败，消息将被放回队列，然后重新传递给另一个消费者（或者如果没有其他消费者，则留在队列中）。因此，消费者应该只在完成处理消息时确认消息。如果消息在消费者实际处理之前就被确认（这就是`autoAck`参数会导致的情况），然后消费者意外死机，消息将永远丢失。因此，显式确认消息是使系统具有弹性和容错性的重要步骤。
- en: Building an event emitter
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建事件发射器
- en: In the preceding example, we used AMQP channels to send simple string messages
    from publisher to subscriber. In order to use AMQP to build an actual publish/subscribe
    architecture, we will need to transmit more complex messages with structured data.
    In general, each AMQP message is simply a string of bytes. To submit structured
    data, we can use serialization formats, such as JSON or XML. Also, since AMQP
    is not limited to ASCII messages, we could also use binary serialization protocols
    such as `MessagePack` or `ProtocolBuffers`.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们使用AMQP通道从发布者向订阅者发送简单的字符串消息。为了使用AMQP构建实际的发布/订阅架构，我们需要传输更复杂的带有结构化数据的消息。一般来说，每个AMQP消息只是一串字节。为了提交结构化数据，我们可以使用序列化格式，比如JSON或XML。此外，由于AMQP不限于ASCII消息，我们还可以使用二进制序列化协议，比如`MessagePack`或`ProtocolBuffers`。
- en: For whichever serialization format you decide, you need to make sure that both
    publisher and subscriber understand both the serialization format and the actual
    internal structure of the messages.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您决定使用哪种序列化格式，您都需要确保发布者和订阅者都了解序列化格式和消息的实际内部结构。
- en: Regarding the serialization format, we will take the easy choice in this chapter
    and use the JSON serialization format. It is widely adopted; serializing and unserializing
    messages are easily done using Go standard libraries and also in other programming
    languages (which is important—although in this book we have committed ourselves
    exclusively to Go, it is common in microservice architectures to have lots of
    different application runtimes).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 关于序列化格式，我们将在本章中选择简单的JSON序列化格式。它被广泛采用；使用Go标准库和其他编程语言（这一点很重要——尽管在本书中我们专门致力于Go，但在微服务架构中，有许多不同的应用运行时是很常见的）轻松地进行序列化和反序列化消息。
- en: We also need to make sure that both publisher and subscribers know how the messages
    will be structured. For example, a `LocationCreated` event might have a `name`
    property and an `address` property. To solve this issue, we will introduce a shared
    library that will contain struct definitions for all possible events, together
    with instructions for the JSON (un)serialization. This library can then be shared
    between the publisher and all subscribers.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要确保发布者和订阅者都知道消息的结构。例如，一个`LocationCreated`事件可能有一个`name`属性和一个`address`属性。为了解决这个问题，我们将引入一个共享库，其中包含所有可能事件的结构定义，以及JSON（反）序列化的说明。然后，这个库可以在发布者和所有订阅者之间共享。
- en: 'Start by creating the `todo.com/myevents/contracts` directory in your GOPATH.
    The first event type that we will describe is the `EventCreatedEvent` event. This
    message will later be published by the event service when a new event is created.
    Let''s define this event as a struct in the `event_created.go` file in the newly
    created package:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 首先在您的GOPATH中创建`todo.com/myevents/contracts`目录。我们将描述的第一种事件类型是`EventCreatedEvent`事件。当创建新事件时，此消息将由事件服务发布。让我们在新创建的包的`event_created.go`文件中将此事件定义为一个结构：
- en: '[PRE13]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Also, we need a possibility to generate a topic name for each event (in RabbitMQ,
    the topic name will also be used as a routing key for the messages). For this,
    add a new method—`EventName()`—to your newly defined struct:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们需要为每个事件生成一个主题名称（在RabbitMQ中，主题名称也将用作消息的路由键）。为此，请向您新定义的结构添加一个新方法——`EventName()`：
- en: '[PRE14]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can now use a Go interface to define a generic event type. This type can
    be used to enforce that each event type actually implements an `EventName()` method.
    Since both event publisher and event subscriber will also later be used across
    multiple services, we will put the event interface code into the `todo.com/myevents/lib/msgqueue` package.
    Start by creating the package directory and a new file, `event.go`, within that
    package:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用Go接口来定义一个通用的事件类型。这种类型可以用来强制每种事件类型实际上都实现了一个`EventName()`方法。由于事件发布者和事件订阅者以后也将被用于多个服务，我们将事件接口代码放入`todo.com/myevents/lib/msgqueue`包中。首先创建包目录和一个新文件`event.go`：
- en: '[PRE15]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Of course, our example application uses more events than just the `EventCreatedEvent`.
    For example, we also have a `LocationCreatedEvent` and an `EventBookedEvent`.
    Since showing all their implementations in print would be fairly repetitive, we
    would like to refer to the example files for this chapter.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们的示例应用程序使用的事件不仅仅是`EventCreatedEvent`。例如，我们还有一个`LocationCreatedEvent`和一个`EventBookedEvent`。由于在打印中显示它们的所有实现会相当重复，我们希望在本章的示例文件中查看它们。
- en: 'Let''s now continue by building an event emitter that can actually publish
    these messages to an AMQP broker. Since we will also explore other message brokers
    in later sections of this chapter, we will start by defining the interface that
    any event emitter should fulfil. For this, create a `emitter.go` file in the `msgqueue`
    package that was created before with the following contents:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在继续构建一个事件发射器，它可以实际将这些消息发布到AMQP代理。由于我们将在本章的后面部分探索其他消息代理，因此我们将首先定义任何事件发射器应该满足的接口。为此，在之前创建的`msgqueue`包中创建一个`emitter.go`文件，内容如下：
- en: '[PRE16]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This interface describes the methods (actually, just one method) that all event
    emitter implementations need to fulfil. Let's now continue by creating a `todo.com/myevents/lib/msgqueue/amqp`
    subpackage with a `emitter.go` file. This file will contain a struct definition
    for the `AMQPEventEmitter`.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 此接口描述了所有事件发射器实现需要满足的方法（实际上只有一个方法）。让我们继续创建一个`todo.com/myevents/lib/msgqueue/amqp`子包，其中包含一个`emitter.go`文件。该文件将包含`AMQPEventEmitter`的结构定义。
- en: 'Consider the following code example:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下代码示例：
- en: '[PRE17]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Note how the `amqpEventEmitter` type is declared package-private, as it is declared
    with a lowercase name. This will prevent users from instantiating the `amqpEventEmitter`
    type directly. For a proper instantiation, we will provide a constructor method,
    instead.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意`amqpEventEmitter`类型声明为包私有，因为它使用小写名称声明。这将阻止用户直接实例化`amqpEventEmitter`类型。为了正确实例化，我们将提供一个构造方法。
- en: 'Next, let''s add a `setup` method that we can use to declare the exchange that
    this publisher is going to publish into:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们添加一个`setup`方法，我们可以用来声明此发布者将要发布到的交换机：
- en: '[PRE18]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: You might be wondering why we created a new AMQP channel in this method and
    closed it immediately after declaring the exchange. After all, we could reuse
    this same channel for publishing messages later. We will get to that in a moment.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能想知道为什么我们在此方法中创建了一个新的AMQP通道，并在声明交换机后立即关闭它。毕竟，我们可以在以后重用相同的通道来发布消息。我们稍后会解决这个问题。
- en: 'Continue by adding a constructor function—`NewAMQPEventEmitter`—for building
    new instances of this struct:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 继续添加一个构造函数`NewAMQPEventEmitter`，用于构建此结构的新实例：
- en: '[PRE19]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now, to the actual heart of the `amqpEventEmitter` event—the `Emit` method.
    First, we will need to transform the event that has been passed into the method
    as a parameter into a JSON document:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，到`amqpEventEmitter`事件的实际核心——`Emit`方法。首先，我们需要将作为参数传递给方法的事件转换为JSON文档：
- en: '[PRE20]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, we can create a new AMQP channel and publish our message to the events
    exchange:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以创建一个新的AMQP通道，并将我们的消息发布到事件交换机中：
- en: '[PRE21]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Note that we used the `Headers` field of `amqp.Publishing` to add the event
    name in a special message header. This will make it easier for us to implement
    the event listener later.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用`amqp.Publishing`的`Headers`字段来将事件名称添加到特殊的消息头中。这将使我们更容易实现事件监听器。
- en: Also, note that we are creating a new channel for each published message within
    this code. While it is, in theory, possible to reuse the same channel for publishing
    multiple messages, we need to keep in mind that a single AMQP channel is not thread-safe.
    This means that calling the event emitter's `Emit()` method from multiple go-routines
    might lead to strange and unpredictable results. This is exactly the problem that
    AMQP channels are there to solve; using multiple channels, multiple threads can
    use the same AMQP connection.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，在此代码中，我们为每个发布的消息创建了一个新通道。虽然理论上可以重用相同的通道来发布多个消息，但我们需要记住，单个AMQP通道不是线程安全的。这意味着从多个go协程调用事件发射器的`Emit()`方法可能会导致奇怪和不可预测的结果。这正是AMQP通道的问题所在；使用多个通道，多个线程可以使用相同的AMQP连接。
- en: 'Next, we can integrate our new event emitter into the existing event service
    that you have already built in [Chapter 2](def2621c-74c4-4f60-a37a-b0b2f86c6339.xhtml), *Building
    Microservices Using Rest APIs*, and [Chapter 3](9c1db13f-619b-43a7-96a1-c6fc65e13b67.xhtml),* Securing
    Microservices*. Start by adding a configuration option for the AMQP broker in
    the `ServiceConfig` struct:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以将新的事件发射器集成到您已经在[第2章](def2621c-74c4-4f60-a37a-b0b2f86c6339.xhtml)和[第3章](9c1db13f-619b-43a7-96a1-c6fc65e13b67.xhtml)中构建的现有事件服务中。首先，在`ServiceConfig`结构中添加一个AMQP代理的配置选项：
- en: '[PRE22]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This allows you to specify the AMQP broker via the JSON configuration file.
    In the `ExtractConfiguration()` function, we can also add a fallback that optionally
    extracts this value from an environment variable, if set:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这使您可以通过JSON配置文件指定AMQP代理。在`ExtractConfiguration()`函数中，我们还可以添加一个备用选项，如果设置了环境变量，则可以从中提取此值：
- en: '[PRE23]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We can now use this configuration option to construct a new event emitter in
    the event service''s `main` function:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以在事件服务的`main`函数中使用此配置选项来构造一个新的事件发射器：
- en: '[PRE24]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We can now pass this event emitter into the `rest.ServeAPI` function, which
    can, in turn, pass it into the `newEventHandler` function:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将此事件发射器传递给`rest.ServeAPI`函数，然后再传递给`newEventHandler`函数：
- en: '[PRE25]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The event emitter can then be stored as a field in the `eventServiceHandler`
    struct:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，事件发射器可以作为`eventServiceHandler`结构的字段存储：
- en: '[PRE26]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now, the `eventServiceHandler` holds a reference to the event emitter that
    you can use in the actual REST handlers. This allows you, for example, to emit
    an `EventCreatedEvent` whenever a new event is created via the API. For this,
    modify the `newEventHandler` method of `eventServiceHandler`, as follows:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`eventServiceHandler`持有对事件发射器的引用，您可以在实际的REST处理程序中使用它。例如，通过API创建新事件时，您可以发出`EventCreatedEvent`。为此，请修改`eventServiceHandler`的`newEventHandler`方法如下：
- en: '[PRE27]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Building an event subscriber
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we can publish events on a `RabbitMQ` broker using the `EventEmitter`,
    we also need a possibility to listen to these events. This will be the purpose
    of the `EventListener`, which we will build in this section.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'Like before, let''s start by defining the interface that all event listeners
    (the AMQP event listener being one of them) should fulfil. For this, create the `listener.go` file
    in the `todo.com/myevents/lib/msgqueue` package:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This interface looks quite different than the event emitter''s interface. This
    is because each call to the event emitter''s `Emit()` method simply publishes
    one message immediately. However, an event listener is typically active for a
    long time and needs to react to incoming messages whenever they may be received.
    This reflects in the design of our `Listen()` method: first of all, it will accept
    a list of event names for which the event listener should listen. It will then
    return two Go channels: the first will be used to stream any events that were
    received by the event listener. The second will contain any errors that occurred
    while receiving those events.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'Start with building the AMQP implementation by creating a new `listener.go` file
    in the `todo.com/myevents/lib/msgqueue/amqp` package:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Similar to the event emitter, continue by adding a `setup` method. In this
    method, we will need to declare the AMQP queue that the listener will be consuming:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Note that the name of the queue that the listener will consume is configurable
    using the `amqpEventListener` struct's `queue` field. This is because later, multiple
    services will use the event listener to listen to their events, and each service
    will require its own AMQP queue for this.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: You may have noticed that we did not yet actually bind the newly declared queue
    to the events exchange. This is because we do not know yet which events we actually
    have to listen for (remember the `Listen` method's `events` parameter?).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s add a constructor function to create new AMQP event listeners:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'With the possibility to construct new AMQP event listeners, let''s implement
    the actual `Listen()` method. The first thing to do is use the `eventNames` parameter
    and bind the event queue accordingly:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Next, we can use the channel''s `Consume()` method to receive messages from
    the queue:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The `msgs` variable now holds a channel of `amqp.Delivery` structs. However,
    our event listener is supposed to return a channel of `msgqueue.Event`. This can
    be solved by consuming the `msgs` channel in our own goroutine, build the respective
    event structs, and then publish these in another channel that we return from this
    function:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The tricky part is now within the inner goroutine. Here, we will need to map
    the raw AMQP message to one of the actual event structs (as the `EventCreatedEvent`
    defined before).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember how the EventEmitter added an additional `x-event-name` header to
    the AMQP message when publishing events? This is something that we can use now
    to map these messages back to their respective struct types. Let''s start by extracting
    the event name from the AMQP message headers:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: All of the following code goes into the inner `range` loop of the `Listen` method.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The preceding code tries to read the `x-event-name` header from the AMQP message.
    Since the `msg.Headers` attribute is basically a `map[string]interface{}`, we
    will need a few map index and type assertions until we can actually use the event
    name. In case a message is received that does not contain the required header,
    an error will be written into the errors channel. Also, the message will be nack'ed
    (short for negative acknowledgment), indicating to the broker that it could not
    be successfully processed.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'After knowing the event name, we can use a simple switch/case construct to
    create a new event struct from this name:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Building the booking service
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have an event listener, we can use it to implement the booking service.
    Its general architecture will follow that of the event service, so we will not
    go too much into detail on that matter.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by creating a new package `todo.com/myevents/bookingservice` and create
    a new `main.go` file:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'This will set up the booking service with both a database connection and working
    event listener. We can now use this event listener to listen to the events emitted
    by the event service. For this, add a new subpackage `todo.com/myevents/bookingservice/listener`
    and create a new `event_listener.go\` file:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: In the `ProcessEvents()` function, we are calling the event listener's `Listen`
    function to listen for newly created events. The `Listen` function returns two
    channels, one for received messages and one for errors that occur during listening.
    We will then use an infinitely running for loop and a select statement to read
    from both of these channels at once. Received events will be passed to the `handleEvent`
    function (which we still need to write), and received errors will be simply printed
    to the standard output.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s continue with the `handleEvent` function:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: This function uses a type switch to determine the actual type of the incoming
    event. Currently, our event listener processes the two events, `EventCreated`
    and `LocationCreated`, by storing them in their local database.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we are using a shared library `todo.com/myevents/lib/persistence` for
    managing database access. This is for convenience only. In real microservice architectures,
    individual microservices typically use completely independent persistence layers
    that might be built on completely different technology stacks.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'In our `main.go` file, we can now instantiate the `EventProcessor` and call
    the `ProcessEvents()` function:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Apart from listening to events, the booking service also needs to implement
    its own REST API that can be used by users to book tickets for a specified event.
    This will follow the same principles that you have already learned about in [Chapter
    2](def2621c-74c4-4f60-a37a-b0b2f86c6339.xhtml), *Building Microservices Using
    Rest APIs*, and [Chapter 3](9c1db13f-619b-43a7-96a1-c6fc65e13b67.xhtml),* Securing
    Microservices*. For this reason, we will refrain from explaining the Booking Service's
    REST API in detail and just describe the highlights. You can find a full implementation
    of the REST service in the code examples for this chapter.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `main.go` file, we will need to move the `processor.ProcessEvents()`
    call into its own go-routine. Otherwise, it would block and the program would
    never reach the `ServeAPI` method call:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Finally, we will move on to the actual request handler. It is registered for
    POST requests at `/events/{eventID}/bookings`; it checks how many bookings are
    currently placed for this event and whether the event's location still has the
    capacity for one more booking. In this case, it will create and persist a new
    booking and emit an `EventBooked` event. Take a look at the example files to view
    the full implementation.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Event sourcing
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building your applications using asynchronous messaging opens the door for applying
    some advanced architectural patterns, one of which you will learn about in this
    section.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: When using messaging, publish/subscribe, and event collaboration, every change
    in the entire system's state is reflected in the form of an event that is emitted
    by one of the participating services. Often, each of these services has its own
    database, keeping its own view on the system's state (at least, as far as required)
    and staying up to date by continually listening to the events that are published
    by the other services.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: However, the fact that each change in the system state is also represented by
    a published event presents an interesting opportunity. Imagine that someone recorded
    and saved each and every event that was published by anyone into an event log.
    In theory (and also in practice), you can use this event log to reconstruct the
    entire system state, without having to rely on any other kind of database.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, consider the following (small) event log:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '8:00 am—User #1 with name Alice was created'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '9:00 am—User #2 with name Bob was created'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '1:00 pm—User #1 was deleted'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '3:00 pm—User #2 changes name to Cedric'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By replaying these events, it is easy to reconstruct the state of your system
    at the end of the day—there is one user named Cedric. However, there is more.
    Since each event is timestamped, you can reconstruct the state that your application
    had at any given point in time (for example, at 10:00 am, your application had
    two users, Alice and Bob).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Besides point-in-time recovery, event sourcing offers you a complete audit log
    over everything that happened in your system. Audit logging often is an actual
    requirement on its own in many cases, but also makes it easier to debug the system
    in case of errors. Having a complete event log allows you to replicate the system's
    state at the exact point in time and then replay events step by step to actually
    reproduce a specific error.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Also, having an event log makes the individual services less dependent on their
    local databases. In the extreme, you can abandon databases entirely and have each
    service reconstruct its entire query model from the event log in-memory each time
    it starts up.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Implementing publish/subscribe and event sourcing with Apache Kafka
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the remainder of this chapter, we will not build our own event sourcing system.
    Previously, we used RabbitMQ to accomplish messaging between our services. However,
    RabbitMQ only handles message dispatching, so if you need an event log containing
    all events, you will need to implement it yourself by listening to all events
    and persisting them. You will also need to take care of event replaying yourself.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Apache Kafka is a distributed message broker that also ships with an integrated
    transaction log. It was originally built by LinkedIn and is available as an open
    source product licensed under the Apache License.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding section, we already built implementations of the `EventEmitter`
    and `EventListener` interfaces using an AMQP connection. In this section, we will
    implement the same interfaces using Kafka.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Kafka quickstart with Docker
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Contrary to RabbitMQ, Apache Kafka is a bit more complex to set up. Kafka itself
    requires a working Zookeeper setup in order to perform leader election, managing
    cluster state, and persisting cluster-wide configuration data. However, for development
    purposes, we can use the `spotify/kafka` image. This image comes with a built-in
    Zookeeper installation, allowing quick and easy setup.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 'Just as with the RabbitMQ image before, use the `docker run` command to get
    started quickly:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: This will start a single-node Kafka instance and bind it to the localhost TCP
    port `9092`.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Basic principles of Apache Kafka
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kafka offers a publish/subscribe message broker, but is not based on AMQP and
    therefore uses a different terminology.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: The first basic concept in Kafka is the topic. A topic is something like a category
    or event name that subscribers can write to. It contains a complete log of all
    messages that were ever published into this topic. Each topic is divided into
    a configurable number of partitions. When a new message is published, it needs
    to contain a partition key. The partition key is used by the broker to decide
    into which partition of the topic the message should be written.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/228af662-564a-48ba-90ca-d1c11e87a40f.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
- en: Each Kafka topic consists of a configurable number of partitions; each published
    message has a partition key, which is used to decide into which partition a message
    should be saved
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: The Kafka broker guarantees that within each partition, the order of messages
    will be the same as in which they were published. For each topic, messages will
    be kept for a configurable retention period. However, the broker's performance
    does not degrade significantly when the transaction logs get larger. For this
    reason, it is entirely possible to operate Kafka with an infinite retention period,
    and by this way use it as an event log. Of course, you do need to consider that
    the required disk storage will grow proportionally. Luckily, Kafka supports horizontal
    scale-out quite well.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: From each topic, any number of subscribers (called *consumers* in Kafka jargon)
    can read messages and any number of publishers (*producers*) can write them. Each
    consumer defines for itself at which offset in the event log it would like to
    start consuming. For example, a freshly initialized consumer that only operates
    in-memory could read the entire event log from the start (offset = `0`) to rebuild
    its entire query model. Another consumer that has a local database and only needs
    new events that occurred after a certain point in time can start reading the event
    log at a later point.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Each consumer is a member of a consumer group. A message published in a given
    topic is published to one consumer of each group. This can be used to implement
    a publish/subscribe communication, similar to what we have already built with
    AMQP. The following figure illustrates the different terms and actors in a publish/subscribe
    architecture using AMQP and Kafka. In both cases, every message that is published
    in the exchange/topic will be routed to every consumer.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/27552bd3-67e3-4bf0-9a6b-7a99d8bdef10.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
- en: Publish/Subscribe with both AMQP (1) and Apache Kafka (2); each message that
    is published in the exchange/topic is routed to every subscriber
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: In AMQP, you can also have multiple subscribers listen on the same queue. In
    this case, incoming messages will be routed not to all, but to one of the connected
    subscribers. This can be used to build some kind of load-balancing between different
    subscriber instances.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: 'The same can be built in Kafka by putting multiple subscriber instances into
    the same consumer group. In Kafka, however, each subscriber is assigned to a fixed
    set of (possibly multiple) partitions. For this reason, the number of consumers
    that can consume a topic in parallel is limited by the number of topic partitions.
    The following diagram illustrates this example:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00d574d2-a4b6-4166-a6c2-9b88b89e846a.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
- en: Load-balancing with both AMQP (1) and Apache Kafka (2); each message that is
    published in the exchange/topic routed to one of the connected subscribers
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: If you should decide to have multiple consumers within the same consumer group
    subscribe the same partition of a topic, the broker will simply dispatch all messages
    in that partition to the consumer that connected last.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Connecting to Kafka with Go
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we connected to an AMQP broker in the previous sections of this chapter,
    we used the de facto standard library `github.com/streadway/amqp`. For connecting
    to a Kafka broker, there is a little more diversity among the available Go libraries.
    At the time of writing this book, the most popular Kafka client libraries for
    Go are as follows:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '`github.com/Shopify/sarama` offers full protocol support and is implemented
    in pure Go. It is licensed under the MIT license. It is actively maintained.'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`github.com/elodina/go_kafka_client` is also implemented in pure Go. It offers
    more features than the `Shopify` library, but appears to be less actively maintained.
    It is licensed under the Apache license.'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`github.com/confluentinc/confluent-kafka-go` provides a Go wrapper for the
    `librdkafka` C library (meaning that you will need to have `librdkafka` installed
    on your system for this library to work). It is reported to be faster than the
    `Shopify` library since it relies on a highly optimized C library. For the same
    reason though, it might prove difficult to build. It is actively maintained, although
    its community seems smaller than the `Shopify` library.'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For this chapter, we will use the `github.com/Shopify/sarama` library. Start
    by installing it via `go get`:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: In the previous sections, we have already defined the `EventEmitter` and `EventListener`
    interfaces in the `todo.com/myevents/lib/msgqueue` package. In this section, we
    will now add alternative implementations for these two interfaces. Before diving
    in, let's take a quick look at how to use the `sarama` library to connect to a
    Kafka broker, in general.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: 'Regardless of whether you intend to publish or consume messages, you will need
    to start by instantiating a `sarama.Client` struct. For this, you can use the
    `sarama.NewClient` function. For instantiating a new client, you will need a list
    of Kafka broker addresses (remember, Kafka is designed for being operated in a
    cluster, so you can actually connect to many clustered brokers at the same time)
    and a configuration object. The easiest way to create a configuration object is
    the `sarama.NewConfig` function:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Of course, using `localhost` as a single broker works fine in a development
    setup. For a production setup, the broker list should be read from the environment:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: You can use the `config` object to fine-tune various parameters of your Kafka
    connection. For most purposes, the default settings will do just fine, though.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Publishing messages with Kafka
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Sarama library offers two implementations for publishing messages—the `sarama.SyncProducer`
    and the `sarama.AsyncProducer`.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: The `AsyncProducer` offers an asynchronous interface that uses Go channels both
    for publishing messages and for checking the success of these operations. It allows
    high-throughput of messages, but is a bit bulky to use if all you want to do is
    to emit a single message. For this reason, the `SyncProducer` offers a simpler
    interface that takes a message for producing and blocks until it receives confirmation
    from the broker that the message has been successfully published to the event
    log.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: 'You can instantiate a new Producer using the `sarama.NewSyncProducerFromClient`
    and `sarama.NewAsyncProducerFromClient` functions. In our example, we will use
    the `SyncProducer` that you can create as follows:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Let''s continue by using the `SyncProducer` to create a Kafka implementation
    of our `EventEmitter` interface. Start by creating the `todo.com/myevents/lib/msgqueue/kafka` package
    and the `emitter.go`  file within that package:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Continue by adding a constructor function for instantiating this struct:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'In order to emit messages, you will need to construct an instance of the `sarama.ProducerMessage`
    struct. For this, you will need the topic (which, in our case, is supplied by
    the `msgqueue.Event`''s `EventName()` method) and the actual message body. The
    body needs to be supplied as an implementation of the `sarama.Encoder` interface.
    You can use the `sarama.ByteEncoder` and `sarama.StringEncoder` types to simply
    typecast a byte array or a string to an `Encoder` implementation:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The key in this code sample is the producer's `SendMessage()` method. Note that
    we are actually ignoring a few of this method's return values. The first two return
    values return the number of the partition that the messages were written in and
    the offset number that the message has in the event log.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding code works, but has one fatal flaw: it creates a new Kafka topic
    for each event type. While it is entirely possible for a subscriber to consume
    multiple topics at once, you will have no guaranteed order of processing. This
    may result in a producer emitting a `location #1 created` and `location #1 updated` sequentially
    in short order and a subscriber receiving them in the other order.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to solve this problem, we will need to do two things:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: All messages must be published on the same topic. This implies that we will
    need another way to store the actual event name within the message.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each message must expose a partition key. We can use the message's partition
    key to ensure that messages concerning the same entity (that is, the same event,
    the same user) are stored in a single partition of the event log and are routed
    to the same consumer in-order.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s start with the partitioning key. Remember the `Event` interface in the
    `todo.com/myevents/lib/msgqueue` package? It looked like this:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Continue by adding a new method `PartitionKey()` to this interface:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Next, we can modify the existing event structs that we have defined before
    (for example, `EventCreatedEvent`) to implement this `PartitionKey()` method:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Now, let''s return to the `kafkaEventEmitter`. We can now use each event''s
    `PartitionKey()` method when publishing a message to Kafka. Now, we just need
    to send the event name alongside the event. To solve this issue, we will use an
    envelope for the message body: this means that the message body will not just
    contain the JSON-serialized event object, but rather another object that can contain
    metadata (like the event name) and the actual event body as payload. Let''s define
    this event in a new file `payload.go` in the package `todo.com/myevents/lib/msgqueue/kafka`:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We can now adjust the `kafkaEventEmitter` to first construct an instance of
    the `messageEnvelope` struct and then JSON-serialize that:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Consuming messages from Kafka
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Consuming Messages from a Kafka broker is a little bit more complex than in
    AMQP. You have already learned that a Kafka topic may consist of many partitions
    that each consumer can consume one or more (up to all) of these partitions. Kafka
    architectures allow horizontal scaling by dividing a topic into more partitions
    and having one consumer subscribe to each partition.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: This means that each subscriber needs to know which partitions of a topic exist
    and which of those it should consume. Some of the libraries that we introduced
    earlier in this section (especially the Confluent library) actually support automatic
    subscriber partitioning and automatic group balancing. The `sarama` library does
    not offer this feature, so our `EventListener` will need to select the partitions
    it wants to consume manually.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: For our example, we will implement the `EventListener` so that it, by default,
    listens on all available partitions of a topic. We'll add a special property that
    can be used to explicitly specify the partitions to listen on.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new file `listener.go` in the `todo.com/myevents/lib/msgqueue/kafka`
    package:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Continue by adding a constructor function for this struct:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The `Listen()` method of `kafkaEventListener` follows the same interface as
    the `amqpEventListener` that we implemented in the previous section:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The first thing to do is to determine which topic partitions should be consumed.
    We will assume that the listener should listen on all partitions when an empty
    slice was passed to the `NewKafkaEventListener` method:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'A Sarama consumer can only consume one partition. If we want to consume multiple
    partitions, we will need to start multiple consumers. In order to preserve the
    `EventListener`''s interface, we will start multiple consumers, each in its own
    goroutine in the `Listen()` method and then have them all write to the same results
    channel:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Note the goroutines that are started within the first for loop. Each of these
    contains an inner for loop that iterates over all messages received in a given
    partition. We can now JSON-decode the incoming messages and reconstruct the appropriate
    event types.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: All of the following code examples are placed within the inner for loop of the`Listen()`
    method of `kafkaEventListener`.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'We now have a new problem. We have unmarshalled the event body into a `messageEnvelope`
    struct. This contains the event name and the actual event body. However, the event
    body is just typed as `interface{}`. Ideally, we would need to convert this `interface{}`
    type back to the correct event type (for example, `contracts.EventCreatedEvent`)
    dependent on the event name. For this, we can use the `github.com/mitchellh/mapstructure`
    package that you can install via go get:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'The `mapstructure` library works similar to the `encoding/json` library, only
    that it does not take `[]byte` input variables, but generic `interface{}` input
    values. This allows you to take JSON input of unknown structure (by calling `json.Unmarshal`
    on an `interface{}` value) and then later map the already-decoded type of unknown
    structure to a well-known struct type:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: The `TagName` property in the `mapstructure.DecoderConfig` struct that is created
    before doing the actual decoding instructs the `mapstructure` library to respect
    the existing ``json:"..."`` annotations that are already present in the event
    contracts.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: 'After successfully decoding a message, it can be published into the results
    channel:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Our Kafka event listener is now fully functional. Since it implements the `msgqueue.EventListener`
    interface, you can use it as a drop-in replacement for the existing AMQP event
    listener.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: There is one caveat, though. When started, our current Kafka event listener
    always starts consuming from the very start of the event log. Have a closer look
    at the `ConsumePartition` call in the preceding code example—its third parameter
    (in our case, `0`) describes the offset in the event log at which the consumer
    should start consuming.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: Using `0` as an offset will instruct the event listener to read the entire event
    log right from the start. This is the ideal solution if you want to implement
    event sourcing with Kafka. If you just want to use Kafka as a message broker,
    your service will need to remember the offset of the last message read from the
    event log. When your service is restarted, you can then resume consuming from
    that last known position.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-316
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to integrate multiple services with asynchronous
    communication using message queues such as RabbitMQ and Apache Kafka. You also
    learned about architectural patterns such as event collaboration and event sourcing
    that help you to build scalable and resilient applications that are well-suited
    for cloud deployments.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: The technologies that we have worked with in this chapter are not tied to any
    specific cloud provider. You can easily roll your own RabbitMQ or Kafka infrastructure
    on any cloud infrastructure or your own servers. In [Chapter 8](25f18fd2-4d08-41fb-a8b2-acc927bd0876.xhtml),
    *AWS Part II - S3, SQS, API Gateway, and DynamoDB*, we will take another look
    at message queues—this time with a special focus on the managed messaging solutions
    that are offered to you by AWS.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
