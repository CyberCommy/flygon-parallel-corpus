- en: Chapter 13. Concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Concurrency and one of its manifestations—parallel processing—is one of the
    broadest topics in the area of software engineering. Most of the chapters in this
    book also cover vast areas, and almost all of them could be big enough topics
    for a separate book. But the topic of concurrency by itself is so huge that it
    could take dozens of positions and we would still not be able to discuss all of
    its important aspects and models.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is why I won''t try to fool you, and from the very beginning state that
    we will barely touch the surface of this topic. The purpose of this chapter is
    to show why concurrency may be required in your application, when to use it, and
    what are the most important concurrency models that you may use in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: Multithreading
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiprocessing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Asynchronous programming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will also discuss some of the language features, built-in modules, and third-party
    packages that allow you to implement these models in your code. But we won't cover
    them in much detail. Treat the content of this chapter as an entry point for your
    further research and reading. It is here to guide you through the basic ideas
    and help in deciding if you really need concurrency, and if so, which approach
    will best suit your needs.
  prefs: []
  type: TYPE_NORMAL
- en: Why concurrency?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we answer the question *why concurrency*, we need to ask *what is concurrency
    at all?*
  prefs: []
  type: TYPE_NORMAL
- en: And the answer to the second question may be surprising for some who used to
    think that this is a synonym for **parallel processing**. But concurrency is not
    the same as parallelism. Concurrency is not a matter of application implementation
    but only a property of a program, algorithm, or problem. And parallelism is only
    one of the possible approaches to problems that are concurrent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Leslie Lamport in his *Time, Clocks, and the Ordering of Events in Distributed
    Systems* paper from 1976, says:'
  prefs: []
  type: TYPE_NORMAL
- en: '*"Two events are concurrent if neither can causally affect the other."*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: By extrapolating events to programs, algorithms, or problems, we can say that
    something is concurrent if it can be fully or partially decomposed into components
    (units) that are order-independent. Such units may be processed independently
    from each other, and the order of processing does not affect the final result.
    This means that they can also be processed simultaneously or in parallel. If we
    process information this way, then we are indeed dealing with parallel processing.
    But this is still not obligatory.
  prefs: []
  type: TYPE_NORMAL
- en: Doing work in a distributed manner, preferably using capabilities of multicore
    processors or computing clusters, is a natural consequence of concurrent problems.
    Anyway, it does not mean that this is the only way of efficiently dealing with
    concurrency. There are a lot of use cases where concurrent problems can be approached
    in other than synchronous ways, but without the need for parallel execution.
  prefs: []
  type: TYPE_NORMAL
- en: So, once we know what concurrency really is, it is time to explain what the
    fuss is about. When the problem is concurrent, it gives you the opportunity to
    deal with it in a special, preferably more efficient, way.
  prefs: []
  type: TYPE_NORMAL
- en: 'We often get used to deal with problems in a classical way by performing a
    sequence of steps. This is how most of us think and process information—using
    synchronous algorithms that do one thing at a time, step by step. But this way
    of processing information is not well suited for solving large-scale problems
    or when you need to satisfy the demands of multiple users or software agents simultaneously:'
  prefs: []
  type: TYPE_NORMAL
- en: The time to process the job is limited by the performance of the single processing
    unit (single machine, CPU core, and so on)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You are not able to accept and process new inputs until your program has finished
    processing the previous one
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So generally, approaching concurrent problems concurrently is the best approach
    when:'
  prefs: []
  type: TYPE_NORMAL
- en: The scale of problems is so big that the only way to process them in an acceptable
    time or within the range of available resources is to distribute execution to
    multiple processing units that can handle the work in parallel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your application needs to maintain responsiveness (accept new inputs) even if
    it has not finished processing the old ones
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This covers most of the situations where concurrent processing is a reasonable
    option. The first group of problems definitely needs the parallel processing solution
    so it is usually solved with multithreading and multiprocessing models. The second
    group does not necessarily need to be processed in parallel, so the actual solution
    really depends on the problem details. Note that this group also covers cases
    where the application needs to serve multiple clients (users or software agents)
    independently, without the need to wait for others to be successfully served.
  prefs: []
  type: TYPE_NORMAL
- en: The other thing worth mentioning is that the preceding two groups are not exclusive.
    Very often you need to maintain application responsiveness and at the same time
    you are not able to handle the input on a single processing unit. This is the
    reason why different and seemingly alternative or conflicting approaches to concurrency
    may often be used at the same time. This is especially common in the development
    of web servers where it may be necessary to use asynchronous event loops, or threads
    with a conjunction of multiple processes, in order to utilize all the available
    resources and still maintain low latencies under high load.
  prefs: []
  type: TYPE_NORMAL
- en: Multithreading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Threading is often considered to be a complex topic by developers. While this
    statement is totally true, Python provides high-level classes and functions that
    ease the usage of threading. CPython's implementation of threads comes with some
    inconvenient details that make them less useful than in other languages. They
    are still completely fine for some set problems that you may want to solve, but
    not for as many as in C or Java. In this section, we will discuss the limitations
    of multithreading in CPython, as well as the common concurrent problems where
    Python threads are a viable solution.
  prefs: []
  type: TYPE_NORMAL
- en: What is multithreading?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Thread is short for a thread of execution. A programmer can split his or her
    work into threads that run simultaneously and share the same memory context. Unless
    your code depends on third-party resources, multithreading will not speed it up
    on a single-core processor, and will even add some overhead for thread management.
    Multi-threading will benefit from a multiprocessor or multi-core machine and will
    parallelize each thread execution on each CPU core, thus making the program faster.
    Note that this is a general rule that should hold true for most programming languages.
    In Python, the performance benefit from multithreading on multicore CPUs has some
    limits, but we will discuss that later. For simplicity, let's assume for now that
    this statement is true.
  prefs: []
  type: TYPE_NORMAL
- en: The fact that the same context is shared among threads means you must protect
    data from concurrent access. If two threads update the same data without any protection,
    a race condition occurs. This is called a **race hazard**, where unexpected results
    may happen because of the code run by each thread making false assumptions about
    the state of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Lock mechanisms help in protecting data, and thread programming has always been
    a matter of making sure that the resources are accessed by threads in a safe way.
    This can be quite hard and thread programming often leads to bugs that are hard
    to debug, since they are hard to reproduce. The worst problem occurs when, due
    to poor code design, two threads lock a resource and try to get the resource that
    the other thread has locked. They will wait for each other forever. This is called
    a **deadlock** and is quite hard to debug. **Reentrant locks** help a bit in this
    by making sure a thread doesn't get locked by attempting to lock a resource twice.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, when threads are used for isolated needs with tools that were
    built for them, they might increase the speed of the program.
  prefs: []
  type: TYPE_NORMAL
- en: Multithreading is usually supported at the system kernel level. When the machine
    has one single processor with a single core, the system uses a **timeslicing**
    mechanism. Here, the CPU switches from one thread to another so fast that there
    is an illusion of threads running simultaneously. This is done at the processing
    level as well. Parallelism without multiple processing units is obviously virtual
    and there is no performance gain from running multiple threads on such hardware.
    Anyway, sometimes it is still useful to implement code with threads even if it
    has to execute on a single core, and we will see a possible use case later.
  prefs: []
  type: TYPE_NORMAL
- en: Everything changes when your execution environment has multiple processors or
    multiple CPU cores for its disposition. Even if timeslicing is used, processes
    and threads are distributed among CPUs, providing the ability to run your program
    faster.
  prefs: []
  type: TYPE_NORMAL
- en: How Python deals with threads
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unlike some other languages, Python uses multiple kernel-level threads that
    can each run any of the interpreter-level threads. But the standard implementation
    of the language—CPython—comes with major limitation that renders threads less
    usable in many contexts. All threads accessing Python objects are serialized by
    one global lock. This is done because much of the interpreter internal structures,
    as well as third-party C code, are not thread-safe and need to be protected.
  prefs: []
  type: TYPE_NORMAL
- en: This mechanism is called the **Global Interpreter Lock** (**GIL**) and its implementation
    details on the Python/C API level were already discussed in the *Releasing GIL*
    section of [Chapter 7](ch07.html "Chapter 7. Python Extensions in Other Languages"),
    *Python Extensions in Other Languages*. The removal of GIL is a topic that occasionally
    appears on the python-dev e-mail list and was postulated by developers multiple
    times. Sadly, until this time, no one ever managed to provide a reasonable and
    simple solution that would allow us to get rid of this limitation. It is highly
    improbable that we will see any progress in this area soon. It is safer to assume
    that GIL will stay in CPython forever. So we need to learn how to live with it.
  prefs: []
  type: TYPE_NORMAL
- en: So what is the point of multithreading in Python?
  prefs: []
  type: TYPE_NORMAL
- en: When threads contain only pure Python code, there is little point in using threads
    to speed up the program since the GIL will serialize it. But remember that GIL
    just enforces that only one thread can execute the Python code at any time. In
    practice, the global interpreter lock is released on a number of blocking system
    calls and can be released in sections of C extensions that do not use any Python/C
    API functions. This means, multiple threads can do I/O operations or execute C
    code in certain third-party extensions in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: For nonpure code blocks where external resources are used or C code is involved,
    multithreading is useful for waiting for a third-party resource to return results.
    This is because a sleeping thread that has explicitly released the GIL can stand
    by and wake up when the results are back. Last, whenever a program needs to provide
    a responsive interface, multithreading is the answer even if it uses timeslicing.
    The program can interact with the user while doing some heavy computing in the
    so-called background.
  prefs: []
  type: TYPE_NORMAL
- en: Note that GIL does not exist in every implementation of the Python language.
    It is a limitation of CPython, Stackless Python, and PyPy, but does not exist
    in Jython and IronPython (see [Chapter 1](ch01.html "Chapter 1. Current Status
    of Python"), *Current Status of Python*). There is although some development of
    the GIL-free version of PyPy, but at the time of writing this book, it is still
    at an experimental stage and the documentation is lacking. It is based on Software
    Transactional Memory and is called PyPy-STM. It is really hard to say when (or
    if) it will be officially released as a production-ready interpreter. Everything
    seems to indicate that it won't happen soon.
  prefs: []
  type: TYPE_NORMAL
- en: When should threading be used?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Despite the GIL limitation, threads can be really useful in some cases. They
    can help in:'
  prefs: []
  type: TYPE_NORMAL
- en: Building responsive interfaces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Delegating work
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building multiuser applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building responsive interfaces
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's say you ask your system to copy files from a folder to another through
    a graphical user interface. The task will possibly be pushed into the background
    and the interface window will be constantly refreshed by the main thread. This
    way you get live feedback on the progress of the whole process. You will also
    be able to cancel the operation. This is less irritating than a raw `cp` or `copy`
    shell command that does not provide any feedback until all work is finished.
  prefs: []
  type: TYPE_NORMAL
- en: A responsive interface also allows a user to work on several tasks at the same
    time. For instance, Gimp will let you play around with a picture while another
    one is being filtered, since the two tasks are independent.
  prefs: []
  type: TYPE_NORMAL
- en: When trying to achieve such responsive interfaces, a good approach is to try
    to push long running tasks into the background, or at least try to provide constant
    feedback to the user. The easiest way to achieve that is to use threads. In such
    a scenario, they are not intended to increase performance, but only to make sure
    that the user can still operate the interface even if it needs to process some
    data for a longer period of time.
  prefs: []
  type: TYPE_NORMAL
- en: In case such background tasks perform a lot of I/O operations, you are able
    to still get some benefit from multicore CPUs. Then it's a *win-win* situation.
  prefs: []
  type: TYPE_NORMAL
- en: Delegating work
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If your process depends on third-party resources, threads might really speed
    up everything.
  prefs: []
  type: TYPE_NORMAL
- en: Let's consider the case of a function that indexes files in a folder and pushes
    the built indexes into a database. Depending on the type of file, the function
    calls a different external program. For example, one is specialized in PDFs and
    another one in OpenOffice files.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of treating each file in a sequence, by executing the right program
    and then storing the result into the database, your function can set up a thread
    for each converter and push jobs to be done to each one of them through a queue.
    The overall time taken by the function will be closer to the processing time of
    the slowest converter than to the sum of all the work.
  prefs: []
  type: TYPE_NORMAL
- en: Converter threads can be initialized from the start and the code in charge of
    pushing the result into the database can also be a thread that consumes available
    results in the queue.
  prefs: []
  type: TYPE_NORMAL
- en: Note that such an approach is somewhat a hybrid between multithreading and multiprocessing.
    If you delegate the work to external processes (for example, using the `run()`
    function from the `subprocess` module), you are in fact doing work in multiple
    processes, so this has symptoms of multiprocessing. But in our scenario, we are
    waiting for the processing results in separate threads, so it is still mostly
    multithreading from the view of the Python code.
  prefs: []
  type: TYPE_NORMAL
- en: The other common use case for threads is performing multiple HTTP requests to
    external services. For instance, if you want to fetch multiple results from a
    distant web API, it could take a lot of time to do that synchronously. If you
    wait for every previous response before making new requests, you will spend a
    lot of time just waiting for the external service to respond and additional roundtrip
    time delays will be added to every such request. If you are communicating with
    an efficient service (Google Maps API, for instance), it is highly probable that
    it can serve most of your requests concurrently without affecting response times
    of separate requests. It is then reasonable to perform multiple queries in separate
    threads. Remember that when doing an HTTP request, most of time is spent on reading
    from the TCP socket. This is a blocking I/O operation, so CPython will release
    the GIL when performing the `recv()` C function. This allows for great improvements
    in your application's performance.
  prefs: []
  type: TYPE_NORMAL
- en: Multiuser applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Threading is also used as a concurrency base for multiuser applications. For
    instance, a web server will push a user request into a new thread and then will
    become idle, waiting for new requests. Having a thread dedicated to each request
    simplifies a lot of work, but requires the developer to take care of locking the
    resources. But this is not a problem when all the shared data is pushed into a
    relational database that takes care of concurrency matters. So threads in a multi-user
    application act almost like separate independent processes. They are under the
    same process only to simplify their management at the application level.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, a web server will be able to put all requests in a queue and
    wait for a thread to be available to send the work to it. Furthermore, it allows
    memory sharing that can boost some work and reduce the memory load. The two very
    popular Python WSGI-compliant webservers: **Gunicorn** (refer to [http://gunicorn.org/](http://gunicorn.org/))
    and **uWSGI** (refer to [https://uwsgi-docs.readthedocs.org](https://uwsgi-docs.readthedocs.org)),
    allow you to serve HTTP requests with threaded workers in a way that generally
    follows this principle.'
  prefs: []
  type: TYPE_NORMAL
- en: Using multithreading to enable concurrency in multiuser applications is less
    expensive than using multiprocessing. Separate processes cost more resources since
    a new interpreter needs to be loaded for each one of them. On the other hand,
    having too many threads is expensive too. We know that the GIL isn't such a problem
    for I/O extensive applications, but there is always a time where you will need
    to execute Python code. Since you cannot parallelize all of the application parts
    with bare threads, you will never be able to utilize all resources on machines
    with multicore CPUs and a single Python process. This is why often the optimal
    solution is a hybrid of multiprocessing and multithreading—multiple workers (processes)
    running with multiple threads. Fortunately, many of the WSGI-compliant web servers
    allow for such a setup.
  prefs: []
  type: TYPE_NORMAL
- en: But before you marry multithreading with multiprocessing, consider if such an
    approach is really worth all the cost. Such an approach uses multiprocessing for
    better resource utilization and additionally multithreading for more concurrency,
    which should be lighter than running multiple processes. But it does not need
    to be true. Maybe getting rid of threads and increasing the number of processes
    is not as expensive as you think? When choosing the best setup, you always need
    to do load testing of your application (see the *Load and performance testing*
    section in [Chapter 10](ch10.html "Chapter 10. Test-Driven Development"), *Test-Driven
    Development*). Also, as a side effect of using multiple threads, you get a less
    safe environment where shared memory creates a risk of data corruption or dreadful
    deadlock. Maybe a better alternative would be using some asynchronous approach
    with event loops, green threads, or coroutines. We will cover such solutions later
    in the *Asynchronous programming* section. Again, without sensible load testing
    and experimentation, you cannot really tell what approach will work best in your
    context.
  prefs: []
  type: TYPE_NORMAL
- en: An example of a threaded application
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To see how Python threading works in practice, let's construct an example application
    that can take some benefit from implementing multithreading. We will discuss a
    simple problem that you may encounter from time to time in your professional practice—making
    multiple parallel HTTP queries. This problem was already mentioned as a common
    use case for multithreading.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say we need to fetch data from some web service using multiple queries
    that cannot be batched into a single big HTTP request. As a realistic example,
    we will use geocoding endpoints from Google Maps API. The reasons for that choice
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It is very popular and a well-documented service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a free tier of this API that does not require any authentication keys
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a `python-gmaps` package available on PyPI that allows you to interact
    with various Google Maps API endpoints and is extremely easy to use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Geocoding means simply the transformation of address or place into coordinates.
    We will try to geocode a predefined list of various cities into latitude/longitude
    tuples and display results on the standard output with `python-gmaps`. It is as
    simple as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Since our goal is to show how a multithreaded solution to concurrent problems
    compares to standard synchronous solution, we will start with an implementation
    that does not use threads at all. Here is the code of a program that loops over
    the list of cities, queries the Google Maps API, and displays information about
    their addresses and coordinates in a text-formatted table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Around the execution of the `main()` function, we added a few statements that
    are intended to measure how much time it took to finish the job. On my computer,
    this program usually takes around 2 to 3 seconds to complete its task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Every run of our script will always take a different amount of time because
    it mostly depends on a remote service accessible through a network connection.
    So there is a lot of nondeterministic factors affecting the final result. The
    best approach would be to make longer tests, repeat them multiple times, and also
    calculate some average from the measurements. But for the sake of simplicity,
    we won't do that. You will see later that this simplified approach is just enough
    for illustrational purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Using one thread per item
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now it is time for improvement. We don't do a lot of processing in Python and
    the long execution time is caused by communication with the external service.
    We send an HTTP request to the server, it calculates the answer, and then we wait
    until the response is transferred back. There is a lot of I/O involved, so multithreading
    seems like a viable option. We can start all the requests at once in separate
    threads and then just wait until they receive data. If the service that we are
    communicating with is able to process our request concurrently, we should definitely
    see a performance improvement.
  prefs: []
  type: TYPE_NORMAL
- en: 'So let''s start with the easiest approach. Python provides clean and easy to
    use abstraction over system threads with the `threading` module. The core of this
    standard library is the `Thread` class that represents a single thread instance.
    Here is a modified version of the `main()` function, which creates and starts
    a new thread for every place to geocode and then waits until all the threads finish:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'It is quick-and-dirty change that has some serious issues that we will try
    to address later. It approaches the problem in a bit of a frivolous way, and it
    is not a way to write reliable software that will serve thousands or millions
    of users. But hey, it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'So when we know that threads have a beneficial effect on our application, it
    is time to use them in a slightly saner way. First we need to identify the issues
    in the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: We start a new thread for every parameter. Thread initialization also takes
    some time but this minor overhead is not the only problem. Threads also consume
    other resources such as memory and file descriptors. Our example input has a strictly
    defined number of items, what if it did not have? You definitely don't want to
    run an unbound number of threads that depend on the arbitrary size of data input.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `fetch_place()` function executed in threads calls the built-in `print()`
    function and in practice it is very unlikely that you would want to do that outside
    of the main application thread. At first, it is due to the fact how the standard
    output is buffered in Python. You can experience malformed output when multiple
    calls to this function interleave between threads. Also, the `print()` function
    is considered slow. If used recklessly in multiple threads, it can lead to serialization,
    which will undo all the benefits of multithreading.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Last but not least, by delegating every function call to a separate thread,
    we make it extremely hard to control the rate at which our input is processed.
    Yes, we want to do the job as fast as possible, but very often external services
    enforce hard limits on the rate of requests from a single client that they can
    process. Sometimes it is reasonable to design the program in a way that enables
    you to throttle the rate of processing, so your application won't be blacklisted
    by external APIs for abusing their usage limits.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a thread pool
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The first issue we will try to solve is the unbound limit of threads that are
    run by our program. A good solution would be to build a pool of threaded workers
    with strictly defined sizes that will handle all the parallel work and communicate
    with workers through some thread-safe data structure. By using this thread pool
    approach, we will also make it easier to solve the two other problems that we
    just mentioned.
  prefs: []
  type: TYPE_NORMAL
- en: 'So the general idea is to start some predefined number of threads that will
    consume the work items from a queue until it is done. When there is no other work
    to do, the threads will return and we will be able to exit from the program. A
    good candidate for our structure to be used to communicate with the workers is
    the `Queue` class from the built-in `queue` module. It is a FIFO (First In First
    Out) queue implementation that is very similar to the `deque` collection from
    the `collections` module and was specifically designed to handle interthread communication.
    Here is a modified version of the `main()` function that starts only a limited
    number of worker threads with a new `worker()` function as a target, and communicates
    with them using a thread-safe queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of running a modified version of our program is similar to the previous
    one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The run time will be slower than in a situation with one thread per argument,
    but at least now it is not possible to exhaust all the computing resources with
    an arbitrary long input. Also, we can tweak the `THREAD_POOL_SIZE` parameter a
    for better resource/time balance.
  prefs: []
  type: TYPE_NORMAL
- en: Using two-way queues
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The other issue that we are now able to solve is the potentially problematic
    printing of the output in threads. It would be much better to leave such a responsibility
    to the main thread that started the other threads. We can handle that by providing
    another queue that will be responsible for collecting results from our workers.
    Here is the complete code that puts everything together with the main changes
    highlighted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This eliminates the risk of malformed output, which we could experience if
    the `present_result()` function does more `print()` statements or performs some
    additional computation. We don''t expect any performance improvement from this
    approach with small inputs, but in fact we also reduce the risk of thread serialization
    due to slow `print()` execution. Here is our final output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Dealing with errors and rate limiting
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The last of the issues mentioned earlier that you may experience when dealing
    with such problems are rate limits imposed by external service providers. In the
    case of the Google Maps API, at the time of writing this book, the official rate
    limit for free and non-authenticated requests is 10 requests per second and 2,500
    requests per day. When using multiple threads, it is very easy to exhaust such
    a limit. The problem is even more serious due to the fact that we did not cover
    any failure scenarios yet, and dealing with exceptions in multithreaded Python
    code is a bit more complicated than usual.
  prefs: []
  type: TYPE_NORMAL
- en: The `api.geocode()` function will raise an exception when the client exceeds
    Google's rate and this is good news. But this exception is raised separately and
    will not crash the entire program. The worker thread will of course exit immediately,
    but the main thread will wait for all tasks stored on `work_queue` to be finished
    (with the `work_queue.join()` call). This means that our worker threads should
    gracefully handle possible exceptions and make sure that all items from the queue
    are processed. Without further improvement, we may end up in a situation where
    some of the worker threads crashed and the program will never exit.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s make some minor changes to our code in order to be prepared for any
    issues that may occur. In the case of exceptions in the worker thread, we may
    put an error instance in the `results_queue` queue and mark the current task as
    done, the same as we would do if there was no error. That way we make sure that
    the main thread won''t lock indefinitely while waiting in `work_queue.join()`.
    The main thread might then inspect the results and re-raise any of the exceptions
    found on the results queue. Here are the improved versions of the `worker()` and
    `main()` functions that can deal with exceptions in a safer way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'When we are ready to handle exceptions, it is time to break our code and exceed
    the rate limit. We can do that easily by modifying some initial conditions. Let''s
    increase the number of places to geocode and the size of our thread pool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'If your execution environment is fast enough, you should get a similar error
    soon:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The preceding exception is of course not the result of faulty code. This program
    simply is a bit too fast for this free service. It makes too many concurrent requests,
    and in order to work correctly, we need to have a way to limit their rate.
  prefs: []
  type: TYPE_NORMAL
- en: Limiting the pace of work is often called throttling. There are a few packages
    on PyPI that allow you to limit the rate of any kind of work and are really easy
    to use. But we won't use any external code here. Throttling is a good opportunity
    to introduce some locking primitives for threading, so we will try to build a
    solution from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm we will use is sometimes called token bucket and is very simple:'
  prefs: []
  type: TYPE_NORMAL
- en: There is a bucket with a predefined amount of tokens.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each token responds to a single permission to process one item of work.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Each time the worker asks for a single or multiple tokens (permission):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We measure how much time was spent from the last time we refilled the bucket
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the time difference allows for it, we refill the bucket with the amount of
    tokens that respond to this time difference
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the amount of stored tokens is bigger or equal to the amount requested, we
    decrease the number of stored tokens and return that value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the amount of stored tokens is less than requested, we return zero
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The two important things are to always initialize the token bucket with zero
    tokens and never allow it to fill with more tokens that is available by its rate,
    expressed in tokens, as per our standard quant of time. If we don''t follow these
    precautions, we can release the tokens in bursts that exceed the rate limit. Because
    in our situation the rate limit is expressed in requests per second, we don''t
    need to deal with arbitrary quants of time. We assume that the base for our measurement
    is one second, so we will never store more tokens than the number of requests
    allowed for that quant of time. Here is an example implementation of the class
    that allows for throttling with a token bucket algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The usage of this class is very simple. Assume that we created only one instance
    of `Throttle` (with `Throttle(10)` for instance) in the main thread and passed
    it to every worker thread as a positional argument. Using the same data structure
    in different threads is safe because we guarded manipulation of its internal state
    with the instance of `Lock` class from the `threading` module. We can now update
    the `worker()` function implementation to wait with every item until throttle
    releases a new token:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Multiprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's be honest, multithreading is challenging—we have already seen that in
    the previous section. It's a fact that the simplest approach to the problem required
    only minimal effort. But dealing with threads in a sane and safe manner required
    a tremendous amount of code.
  prefs: []
  type: TYPE_NORMAL
- en: We had to set up thread pool and communication queues, gracefully handle exceptions
    from threads, and also care about thread safety when trying to provide rate limiting
    capability. Tens lines of code only to execute one function from an external library
    in parallel! And we only assume that this is production-ready because there is
    a promise from the external package creator that his library is thread-safe. Sounds
    like a high price for a solution that is practically applicable only for doing
    I/O bound tasks.
  prefs: []
  type: TYPE_NORMAL
- en: An alternative approach that allows you to achieve parallelism is multiprocessing.
    Separate Python processes that do not constrain each other with GIL allow for
    better resource utilization. This is especially important for applications running
    on multicore processors that are performing really CPU-extensive tasks. Right
    now this is the only built-in concurrent solution available for Python developers
    (using the CPython interpreter) that allows you to take benefit from multiple
    processor cores.
  prefs: []
  type: TYPE_NORMAL
- en: The other advantage of using multiple processes is the fact that they do not
    share memory context. So it is harder to corrupt data and introduce deadlocks
    into your application. Not sharing the memory context means that you need some
    additional effort to pass the data between separate processes, but fortunately
    there are many good ways to implement reliable interprocess communication. In
    fact, Python provides some primitives that make communication between processes
    as easy as possible between threads.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most basic way to start new processes in any programming language is usually
    by **forking** the program at some point. On POSIX systems (Unix, Mac OS, and
    Linux) a fork is a system call exposed in Python through the `os.fork()` function,
    which will create a new child process. The two processes then continue the program
    on their own right after forking. Here is an example script that forks itself
    exactly once:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'And here is an example of running it in a terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Notice how both processes have exactly the same initial state of their data
    before the `os.fork()` call. They both have the same PID number (process identifier)
    as a first value of the `pid_list` collection. Later, both states diverge and
    we can see that the child process added the `21916` value while the parent duplicated
    its `21915` PID. This is because the memory contexts of these two processes are
    not shared. They have the same initial conditions but cannot affect each other
    after the `os.fork()` call.
  prefs: []
  type: TYPE_NORMAL
- en: After the fork memory context is copied to the child, each process deals with
    its own address space. To communicate, processes need to work with system-wide
    resources or use low-level tools such as **signals**.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, `os.fork` is not available under Windows, where a new interpreter
    needs to be spawned in order to mimic the fork feature. So it needs to be different
    depending on the platform. The `os` module also exposes functions that allow you
    to spawn new processes under Windows, but eventually you will use them rarely.
    This is also true for `os.fork()`. Python provides great a `multiprocessing` module
    that creates a high-level interface for multiprocessing. The great advantage of
    this module is that it provides some of the abstractions that we had to code from
    scratch in *An example of a threaded application* section. It allows you to limit
    the amount of boilerplate code, so it improves application maintainability and
    reduces its complexity. Surprisingly, despite its name, the `multiprocessing`
    module also exposes a similar interface for threads, so you will probably want
    to use the same interface for both approaches.
  prefs: []
  type: TYPE_NORMAL
- en: The built-in multiprocessing module
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`multiprocessing` provides a portable way to work with processes as if they
    were threads.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This module contains a `Process` class that is very similar to the `Thread`
    class, and can be used on any platform:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding script, when executed, gives the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: When the processes are created, the memory is forked (on POSIX systems). The
    most efficient usage of processes is to let them work on their own after they
    have been created to avoid overhead, and check on their states from the main thread.
    Besides the memory state that is copied, the `Process` class also provides an
    extra `args` argument in its constructor so that data can be passed along.
  prefs: []
  type: TYPE_NORMAL
- en: 'The communication between process modules requires some additional work because
    their local memory is not shared by default. To simplify this, the multiprocessing
    module provides a few ways of communication between processes:'
  prefs: []
  type: TYPE_NORMAL
- en: Using the `multiprocessing.Queue` class, which is a near clone of `queue.Queue`,
    which was used earlier for communication between threads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using `multiprocessing.Pipe`, which is a socket-like two-way communication channel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the `multiprocessing.sharedctypes` module, which allows you to create
    arbitrary C types (from the `ctypes` module) in a dedicated pool of memory that
    is shared between processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `multiprocessing.Queue` and `queue.Queue` classes have the same interface.
    The only difference is that the first is designed for use in multiple process
    environments, rather than with multiple threads, so it uses different internal
    transports and locking primitives. We already saw how to use Queue with multithreading
    in the *An example of a threaded application* section, so we won't do the same
    for multiprocessing. The usage stays exactly the same, so such an example would
    not bring anything new.
  prefs: []
  type: TYPE_NORMAL
- en: 'A more interesting pattern right now is provided by the `Pipe` class. It is
    a duplex (two-way) communication channel that is very similar in concept to Unix
    pipes. The interface of Pipe is also very similar to a simple socket from the
    built-in `socket` module. The difference from raw system pipes and sockets is
    that it allows you to send any pickable object (using the `pickle` module) instead
    of just raw bytes. This allows for a lot easier communication between processes
    because you can send almost any basic Python type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'When looking at an example output of the preceding script, you will see that
    you can easily pass custom class instances and that they have different addresses
    depending on the process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The other way to share a state between processes is to use raw types in a shared
    memory pool with the classes provided in `multiprocessing.sharedctypes`. The most
    basic ones are `Value` and `Array`. Here is an example code from the official
    documentation of the `multiprocessing` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'And this example will print the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: When working with `multiprocessing.sharedctypes`, you need to remember that
    you are dealing with shared memory, so to avoid the risk of data corruption you
    need to use locking primitives. Multiprocessing provides some of the classes available
    in threading, such as `Lock`, `RLock`, and `Semaphore`, to do that. The downside
    of classes from `sharedctypes` is that they allow you only to share the basic
    C types from the `ctypes` module. If you need to pass more complex structures
    or class instances, you need to use Queue, Pipe, or other interprocess communication
    channels instead. In most cases, it is reasonable to avoid types from `sharedctypes`
    because they increase code complexity and bring all the dangers known from multithreading.
  prefs: []
  type: TYPE_NORMAL
- en: Using process pools
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using multiple processes instead of threads adds some substantial overhead.
    Mostly, it increases the memory footprint because each process has its own independent
    memory context. This means allowing for an unbound number of child processes is
    even more of a problematic issue than in multithreaded applications.
  prefs: []
  type: TYPE_NORMAL
- en: The best pattern to control resource usage in applications that rely on multiprocessing
    for better resource utilization is to build a process pool in a similar way as
    described for threads in the *Using a thread pool* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'And the best thing about the `multiprocessing` module is that it provides a
    ready-to-use `Pool` class that handles all the complexity of managing multiple
    process workers for you. This pool implementation greatly reduces the amount of
    boilerplate required and the number of issues related to two-way communication.
    You also are not required to use the `join()` method manually, as Pool can be
    used as the context manager (using the `with` statement). Here is one of our previous
    threading examples rewritten to use the `Pool` class from the `multiprocessing`
    module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the code is now a lot shorter. It means that it is now easier
    to maintain and debug in case of issues. Actually, there are now only two lines
    of code that explicitly deal with multiprocessing. This is a great improvement
    over the situation where we had to build the processing pool from scratch. Now
    we don't even need to care about communication channels because they are created
    implicitly inside of the `Pool` implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Using multiprocessing.dummy as a multithreading interface
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The high-level abstractions from the `multiprocessing` module, such as the `Pool`
    class, are great advantages over the simple tools provided in the `threading`
    module. But no, it does not mean that multiprocessing is always a better approach
    than multithreading. There are a lot of use cases where threads may be a better
    solution than processes. This is especially true for situations where low latency
    and/or high resource efficiency is required.
  prefs: []
  type: TYPE_NORMAL
- en: But it does not mean that you need to sacrifice all the useful abstractions
    from the `multiprocessing` module whenever you want to use threads instead of
    processes. There is the `multiprocessing.dummy` module, which replicates the `multiprocessing`
    API but uses multiple threads instead of forking/spawning new processes.
  prefs: []
  type: TYPE_NORMAL
- en: 'This allows you to reduce the amount of boilerplate in your code and also make
    a more pluggable interface. For instance, let''s take yet another look at our
    `main()` function from the previous examples. If we wanted to give the user control
    over which processing backend he wants to use (processes or threads), we could
    do that simply by replacing the `Pool` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Asynchronous programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Asynchronous programming has gained a lot of traction in recent years. In Python
    3.5, it finally got some syntax features that solidify concepts of asynchronous
    execution. But it does not mean that asynchronous programming is only possible
    starting from Python 3.5\. A lot of libraries and frameworks were provided a lot
    earlier, and most of them have origins in the old versions of Python 2\. There
    is even a whole alternate implementation of Python called Stackless (see [Chapter
    1](ch01.html "Chapter 1. Current Status of Python"), *Current Status of Python*),
    which concentrated on this single programming approach. Some of these solutions,
    such as Twisted, Tornado, or Eventlet, still have huge and active communities
    and are really worth knowing. Anyway, starting from Python 3.5, asynchronous programming
    is easier than ever before. So it is expected that its built-in asynchronous features
    will replace the bigger parts of older tools, or external projects will gradually
    transform into a kind of high-level frameworks based on Python built-ins.
  prefs: []
  type: TYPE_NORMAL
- en: When trying to explain what asynchronous programming is, the easiest way is
    to think about this approach as something similar to threads but without system
    scheduling involved. This means that an asynchronous program can concurrently
    process problems but its context is switched internally and not by a system scheduler.
  prefs: []
  type: TYPE_NORMAL
- en: 'But, of course, we don''t use threads to concurrently handle the work in an
    asynchronous program. Most of the solutions use a different kind of concept and,
    depending on the implementation, it is named differently. Some example names used
    to describe such concurrent program entities are:'
  prefs: []
  type: TYPE_NORMAL
- en: Green threads or greenlets (greenlet, gevent, or eventlet projects)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coroutines (Python 3.5 native asynchronous programming)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tasklets (Stackless Python)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are mainly the same concepts, but often implemented in a bit different
    way. For obvious reasons, in this section, we will concentrate only on coroutines
    that are natively supported by Python, starting from version 3.5.
  prefs: []
  type: TYPE_NORMAL
- en: Cooperative multitasking and asynchronous I/O
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Cooperative multitasking** is the core of asynchronous programming. In this
    style of computer multitasking, it''s not a responsibility of the operating system
    to initiate a context switch (to another process or thread), but instead every
    process voluntarily releases control when it is idle to enable simultaneous execution
    of multiple programs. This is why it is called *cooperative*. All processes need
    to cooperate in order to multitask smoothly.'
  prefs: []
  type: TYPE_NORMAL
- en: This model of multitasking was sometimes employed in operating systems, but
    now it is hardly ever found as a system-level solution. This is because there
    is a risk that one poorly designed service can easily break the whole system's
    stability. Thread and process scheduling with context switches managed directly
    by the operating system is now the dominant approach for concurrency on the system
    level. But cooperative multitasking is still a great concurrency tool on the application
    level.
  prefs: []
  type: TYPE_NORMAL
- en: When speaking about cooperative multitasking on the application level, we do
    not deal with threads or processes that need to release control because all the
    execution is contained within a single process and thread. Instead, we have multiple
    tasks (coroutines, tasklets, and green threads) that release control to the single
    function that handles the coordination of tasks. This function is usually some
    kind of event loop.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid confusion later (due to Python terminology), from now on we will refer
    to such concurrent tasks as *coroutines*. The most important problem in cooperative
    multitasking is when to release control. In most of asynchronous applications,
    control is released to the scheduler or event loop on I/O operations. No matter
    whether a program reads data from a filesystem or communicates through a socket,
    such I/O operation is always related to some waiting time when the process becomes
    idle. The waiting time depends on the external resource, so it is a good opportunity
    to release control so that other coroutines can do their work until they too would
    need to wait.
  prefs: []
  type: TYPE_NORMAL
- en: This makes such an approach somewhat similar in behavior to how multithreading
    is implemented in Python. We know that GIL serializes Python threads but it is
    also released on every I/O operation. The main difference is that threads in Python
    are implemented as system-level threads, so the operating system can preempt the
    currently running thread and give control to another one at any point in time.
    In asynchronous programming, tasks are never preempted by the main event loop.
    This is why this style of multitasking is also called **non-preemptive multitasking**.
  prefs: []
  type: TYPE_NORMAL
- en: Of course every Python application runs on an operating system where there are
    other processes competing for resources. This means that the operating system
    always has the right to preempt the whole process and give control to another
    one. But when our asynchronous application is running back, it continues from
    the same place where it was paused when the system scheduler stepped in. This
    is why coroutines are still considered nonpreemptive.
  prefs: []
  type: TYPE_NORMAL
- en: Python async and await keywords
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `async` and `await` keywords are the main building blocks in Python asynchronous
    programming.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `async` keyword used before the `def` statement defines a new coroutine.
    The execution of the coroutine function may be suspended and resumed in strictly
    defined circumstances. Its syntax and behavior is very similar to generators (refer
    to [Chapter 2](ch02.html "Chapter 2. Syntax Best Practices – below the Class Level"),
    *Syntax Best Practices – below the Class Level*) In fact, generators need to be
    used in older versions of Python in order to implement coroutines. Here is an
    example of a function declaration that uses the `async` keyword:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Functions defined with the `async` keyword are special. When called, they do
    not execute the code inside but instead return a coroutine object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The coroutine object does not do anything until its execution is scheduled
    in the event loop. The `asyncio` module is available in order to provide the basic
    event loop implementation, as well as lot of other asynchronous utilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Obviously, since we have created only one simple coroutine, there is no concurrency
    involved in our program. In order to see something really concurrent, we need
    to create more tasks that will be executed by the event loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'New tasks can be added to the loop by calling the `loop.create_task()` method
    or by providing another object to wait for using the `asyncio.wait()` function.
    We will use the latter approach and try to asynchronously print a sequence of
    numbers generated with the `range()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The `asyncio.wait()` function accepts a list of coroutine objects and returns
    immediately. The result is a generator that yields objects representing future
    results (futures). As the name suggests, it is used to wait until all of the provided
    coroutines complete. The reason why it returns a generator instead of a coroutine
    object is backwards compatibility with previous versions of Python, which will
    be explained later. The result of running this script may be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the numbers are not printed in the same order as we created our
    coroutines. But this is exactly what we wanted to achieve.
  prefs: []
  type: TYPE_NORMAL
- en: The second important keyword added in Python 3.5 is `await`. It is used to wait
    for the results of coroutine or a future (explained later) and release the control
    over execution to the event loop. To better understand how it works, we need to
    review a more complex example of code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say we want to create two coroutines that will perform some simple task
    in a loop:'
  prefs: []
  type: TYPE_NORMAL
- en: Wait a random number of seconds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Print some text provided as an argument and the amount of time spent in sleep
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s start with a simple implementation that has some concurrency issues
    which we will later try to improve with the additional `await` usage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'When executed in the terminal (with the `time` command to measure time), it
    might give the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, both the coroutines completed their execution but not in an asynchronous
    manner. The reason is that they both use the `time.sleep()` function that is blocking
    but not releasing the control to the event loop. This would work better in a multithreaded
    setup, but we don't want to use threads now. So how do we fix this?
  prefs: []
  type: TYPE_NORMAL
- en: 'The answer is to use `asyncio.sleep()`, which is the asynchronous version of
    `time.sleep()` and await its result using the `await` keyword. We already used
    this statement in the first version of the `main()` function, but it was only
    to improve clarity of code. It clearly did not make our implementation more concurrent.
    Let''s see an improved version of the `waiter()` coroutine that uses `await asyncio.sleep()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'If we run the updated script, we can see how the output of two functions interleave
    with each other:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The additional advantage of this simple improvement is that the code ran faster.
    The overall execution time was less than the sum of all sleeping times because
    coroutines were cooperatively releasing control.
  prefs: []
  type: TYPE_NORMAL
- en: asyncio in older versions of Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `asyncio` module appeared in Python 3.4\. So it is the only version of Python
    that has serious support for asynchronous programming before Python 3.5\. Unfortunately,
    it looks like these two subsequent versions are just enough to introduce compatibility
    concerns.
  prefs: []
  type: TYPE_NORMAL
- en: Like it or not, the core of asynchronous programming in Python was introduced
    earlier than the syntax elements supporting this pattern. Better late than never,
    but this created a situation where there are two syntaxes available for working
    with coroutines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Starting from Python 3.5, you can use `async` and `await`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'But for Python 3.4, you need to use the `asyncio.coroutine` decorator and the
    `yield from` statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The other useful fact is that the `yield from` statement was introduced in Python
    3.3 and there is an `asyncio` backport available on PyPI. This means that you
    can use this implementation of cooperative multitasking with Python 3.3 too.
  prefs: []
  type: TYPE_NORMAL
- en: A practical example of asynchronous programming
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As has already been mentioned multiple times in this chapter, asynchronous programming
    is a great tool for handling I/O bound operations. So it's time to build something
    more practical than the simple printing of sequences or asynchronous waiting.
  prefs: []
  type: TYPE_NORMAL
- en: For the sake of consistency, we will try to handle the same problem we solved
    with the help of multithreading and multiprocessing. So we will try to asynchronously
    fetch some data from external resources through the network connection. It would
    be great if we could use the same `python-gmaps` package as in the previous sections.
    Unfortunately, we can't.
  prefs: []
  type: TYPE_NORMAL
- en: The creator of `python-gmaps` was a bit lazy and took a shortcut. In order to
    simplify development, he chose a `requests` package as his HTTP client library
    of choice. Unfortunately, `requests` do not support asynchronous I/O with `async`
    and `await`. There are some other projects that aim to provide some concurrency
    to the `requests` project, but they either rely on Gevent (`grequests`, refer
    to [https://github.com/kennethreitz/grequests](https://github.com/kennethreitz/grequests))
    or thread/process pool execution (`requests-futures`, refer to [https://github.com/ross/requests-futures](https://github.com/ross/requests-futures)).
    Neither of these solves our problem.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before you get upset that I'm scolding an innocent open source developer, calm
    down. The person behind the `python-gmaps` package is me. Poor selection of dependencies
    is one of the issues of this project. I just like to publicly criticize myself
    from time to time. This should be a bitter lesson for me as `python-gmaps` in
    its most recent version (0.3.1 at the time of writing this book) cannot be easily
    integrated with Python's asynchronous I/O. Anyway, this may change in the future,
    so nothing is lost.
  prefs: []
  type: TYPE_NORMAL
- en: 'Knowing the limitations of the library that was so easy to use in the previous
    examples, we need to build something that will fill in the gap. The Google Maps
    API is really simple to use, so we will build a quick-and-dirty asynchronous utility
    only for illustration purposes. The standard library for Python in version 3.5
    still lacks a library that would make asynchronous HTTP requests as simple as
    calling `urllib.urlopen()`. We definitely don''t want to build the whole protocol
    support from scratch, so we will use a little help from the `aiohttp` package
    available on PyPI. It''s a really promising library that adds both client and
    server implementations for asynchronous HTTP. Here is a small module built on
    top of `aiohttp` that creates a single `geocode()` helper function which makes
    geocoding requests to the Google Maps API service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s assume that this code is stored in a module named `asyncgmaps`, which
    we are going to use later. Now we are ready to rewrite the example used when discussing
    multithreading and multiprocessing. Previously, we used to split the whole operation
    into two separate steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Perform all request to the external service in parallel using the `fetch_place()`
    function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Display all the results in a loop using the `present_result()` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'But because cooperative multitasking is something completely different from
    using multiple processes or threads, we can slightly modify our approach. Most
    of the issues raised in the *Using one thread per item* section are no longer
    our concern. Coroutines are nonpreemptive, so we can easily display results immediately
    after HTTP responses are awaited. This will simplify our code and make it clearer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Integrating nonasynchronous code with async using futures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Asynchronous programming is great, especially for backend developers interested
    in building scalable applications. In practice, it is one of the most important
    tools for building highly concurrent servers.
  prefs: []
  type: TYPE_NORMAL
- en: 'But the reality is painful. A lot of popular packages that deal with I/O bound
    problems are not meant to be used with asynchronous code. The main reasons for
    that are:'
  prefs: []
  type: TYPE_NORMAL
- en: Still low adoption of Python 3 and some of its advanced features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Low understanding of various concurrency concepts among Python beginners
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This means that very often migration of the existing synchronous multithreaded
    applications and packages is either impossible (due to architectural constraints)
    or too expensive. A lot of projects could benefit greatly from incorporating the
    asynchronous style of multitasking, but only a few of them will eventually do
    that.
  prefs: []
  type: TYPE_NORMAL
- en: This means that right now, you will experience a lot of difficulties when trying
    to build asynchronous applications from the start. In most cases, this will be
    something similar to the problem mentioned in the *A practical example of asynchronous
    programming* section—incompatible interfaces and nonasynchronous blocking of I/O
    operations.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, you can sometimes resign from `await` when you experience such incompatibility
    and just fetch the required resources synchronously. But this will block every
    other coroutine from executing its code while you wait for the results. It technically
    works but also ruins all the gains of asynchronous programming. So in the end,
    joining asynchronous I/O with synchronous I/O is not an option. It is a kind of
    *all or nothing* game.
  prefs: []
  type: TYPE_NORMAL
- en: The other problem is long running CPU-bound operations. When you are performing
    an I/O operation, it is not a problem to release control from a coroutine. When
    writing/reading from a filesystem or socket, you will eventually wait, so calling
    using `await` is the best you can do. But what to do when you need to actually
    compute something and you know it will take a while? You can of course slice the
    problem into parts and release control every time you move the work forward a
    bit. But you will shortly find that this is not a good pattern. Such a thing may
    make the code a mess, and also does not guarantee good results. Timeslicing should
    be the responsibility of the interpreter or operating system.
  prefs: []
  type: TYPE_NORMAL
- en: So what to do if you have some code that makes long synchronous I/O operations
    that you can't or are unwilling to rewrite. Or what to do when you have to make
    some heavy CPU-bound operations in an application designed mostly with asynchronous
    I/O in mind? Well... you need to use a workaround. And by workaround I mean multithreading
    or multiprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: This may not sound nice, but sometimes the best solution may be the one that
    we tried to escape from. Parallel processing of CPU-extensive tasks in Python
    is always done better with multiprocessing. And multithreading may deal with I/O
    operations equally good (fast and without lot of resource overhead) as `async`
    and `await`, if set-up properly and handled with care.
  prefs: []
  type: TYPE_NORMAL
- en: So sometimes when you don't know what to do, when something simply does not
    fit your asynchronous application, use a piece of code that will defer it to separate
    thread or process. You can pretend that this was a coroutine, release control
    to the event loop and eventually process the results when they are ready. Fortunately
    for us, the Python standard library provides the `concurrent.futures` module,
    which is also integrated with the `asyncio` module. These two modules together
    allow you to schedule blocking functions executed in threads or additional processes
    as it were asynchronous nonblocking coroutines.
  prefs: []
  type: TYPE_NORMAL
- en: Executors and futures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we see how to inject threads or processes into an asynchronous event
    loop, we will take a closer look at the `concurrent.futures` module, which will
    later be the main ingredient of our so-called workaround.
  prefs: []
  type: TYPE_NORMAL
- en: The most important classes in the `concurrent.futures` module are `Executor`
    and `Future`.
  prefs: []
  type: TYPE_NORMAL
- en: '`Executor` represents a pool of resources that may process work items in parallel.
    This may seem very similar in purpose to classes from the `multiprocessing` module—`Pool`
    and `dummy.Pool`—but has a completely different interface and semantics. It is
    a base class not intended for instantiation and has two concrete implementations:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ThreadPoolExecutor`: This is the one that represents a pool of threads'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ProcessPoolExecutor`: This is the one that represents a pool of processes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Every executor provides three methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`submit(fn, *args, **kwargs)`: This schedules the `fn` function for execution
    on a pool of resources and returns the `Future` object representing the execution
    of a callable'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`map(func, *iterables, timeout=None, chunksize=1)`: This executes the func
    function over an iterable in a similar way to the `multiprocessing.Pool.map()`
    method'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`shutdown(wait=True)`: This shuts down the executer and frees all of its resources'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The most interesting method is `submit()` because of the `Future` object it
    returns. It represents the asynchronous execution of a callable and only indirectly
    represents its result. In order to obtain the actual return value of the submitted
    callable, you need to call the `Future.result()` method. And if the callable is
    already finished, the `result()` method will not block it and will just return
    the function output. If it is not true, it will block it until the result is ready.
    Treat it like a promise of a result (actually it is the same concept as a promise
    in JavaScript). You don''t need to unpack it immediately after receiving it (with
    the `result()` method), but if you try to do that it is guaranteed to eventually
    return something:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to use the `Executor.map()` method, it does not differ in usage
    from the `Pool.map()` method of the Pool class from `multiprocessing` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Using executors in an event loop
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `Future` class instances returned by the `Executor.submit()` method is conceptually
    very close to the coroutines used in asynchronous programming. This is why we
    can use executors to make hybrid between cooperative multitasking and multiprocessing
    or multithreading.
  prefs: []
  type: TYPE_NORMAL
- en: The core of this workaround is the `BaseEventLoop.run_in_executor(executor,
    func, *args)` method of the event loop class. It allows you to schedule the execution
    of the `func` function in a process or thread pool represented by the `executor`
    argument. The most important thing about that method is that it returns a new
    *awaitable* (an object that can be *awaited* with the `await` statement). So thanks
    to this, you can execute a blocking function that is not a coroutine exactly as
    it were a coroutine, and it will not block no matter how long it takes to finish.
    It will stop only the function that is awaiting results from such a call, but
    the whole event loop will still keep spinning.
  prefs: []
  type: TYPE_NORMAL
- en: And a useful fact is that you don't need to even create your executor instance.
    If you pass `None` as an executor argument, the `ThreadPoolExecutor` class will
    be used with the default number of threads (for Python 3.5 it is the number of
    processors multiplied by 5).
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s assume that we did not want to rewrite the problematic part of the
    `python-gmaps` package that was the cause of our headache. We can easily defer
    the blocking call to a separate thread with the `loop.run_in_executor()` invocation
    while still leaving the `fetch_place()` function as an awaitable coroutine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Such a solution is not as good as having a fully asynchronous library to do
    the job, but you know *half a loaf is better than no bread*.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It was a long journey, but we successfully struggled through the most basic
    approaches to concurrent programming available for Python programmers.
  prefs: []
  type: TYPE_NORMAL
- en: After explaining what concurrency really is, we jumped into action and dissected
    one of the typical concurrent problems with the help of multithreading. After
    identifying the basic deficiencies of our code and fixing them, we turned to multiprocessing
    to see how it would work in our case.
  prefs: []
  type: TYPE_NORMAL
- en: We found that multiple processes are much easier to use with the `multiprocessing`
    module than base threads with `threading`. But just after that, we have realized
    that we can use the same API with threads too, thanks to `multiprocessing.dummy`.
    So the choice between multiprocessing and multithreading is now only a matter
    of which solution better suits the problem and not which solution has a better
    interface.
  prefs: []
  type: TYPE_NORMAL
- en: And speaking about problem fit, we finally tried asynchronous programming, which
    should be the best solution for I/O bound applications, only to realize that we
    cannot completely forget about threads and processes. So we made a circle, back
    to the place where we started!
  prefs: []
  type: TYPE_NORMAL
- en: And this leads us to the final conclusion of this chapter. There is no silver
    bullet. There are some approaches that you may prefer or like more. There are
    some approaches that may fit better for a given set of problems, but you need
    to know them all in order to be successful. In realistic scenarios, you may find
    yourself using the whole arsenal of concurrency tools and styles in a single application
    and this is not uncommon.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding conclusion is a great introduction to the topic of the next chapter,
    [Chapter 14](ch14.html "Chapter 14. Useful Design Patterns"), *Useful Design Patterns*.
    This is because there is no single pattern that will solve all of your problems.
    You should know as many as possible because eventually you will end up using all
    of them on a daily basis.
  prefs: []
  type: TYPE_NORMAL
