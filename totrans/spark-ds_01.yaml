- en: Chapter 1.  Big Data and Data Science – An Introduction
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章。大数据和数据科学-介绍
- en: '*Big data is definitely a big deal!* It promises a wealth of opportunities
    by deriving hidden insights in huge data silos and by opening new avenues to excel
    in business. Leveraging **big data** through advanced analytics techniques has
    become a no-brainer for organizations to create and maintain their competitive
    advantage.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*大数据绝对是一件大事！*它承诺通过从庞大的数据堆中获取隐藏的见解，并开辟新的业务发展途径，为组织创造和保持竞争优势提供了丰富的机会。通过先进的分析技术利用大数据已经成为组织的必然选择。'
- en: This chapter explains what big data is all about, the various challenges with
    big data analysis and how **Apache Spark** pitches in as the de facto standard
    to address computational challenges and also serves as a data science platform.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章解释了大数据的全部内容，大数据分析的各种挑战，以及Apache Spark如何成为解决计算挑战的事实标准，并且还作为数据科学平台。
- en: 'The topics covered in this chapter are as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖的主题如下：
- en: Big data overview - what is all the fuss about?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大数据概述-为什么如此重要？
- en: Challenges with big data analytics - why was it so difficult?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大数据分析的挑战-为什么如此困难？
- en: Evolution of big data analytics - the data analytics trend
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大数据分析的演变-数据分析趋势
- en: Spark for data analytics - the solution to big data challenges
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据分析的Spark-解决大数据挑战的解决方案
- en: The Spark stack - all that makes it up for a complete big data solution
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark堆栈-构成完整大数据解决方案的一切
- en: Big data overview
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大数据概述
- en: Much has already been spoken and written about what big data is, but there is
    no specific standard as such to clearly define it. It is actually a relative term
    to some extent. Whether small or big, your data can be leveraged only if you can
    analyze it properly. To make some sense out of your data, the right set of analysis
    techniques is needed and selecting the right tools and techniques is of utmost
    importance in data analytics. However, when the data itself becomes a part of
    the problem and the computational challenges need to be addressed prior to performing
    data analysis, it becomes a big data problem.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 关于大数据已经有很多言论和文章，但没有明确的标准来清晰地定义它。在某种程度上，这实际上是一个相对的术语。无论数据是小还是大，只有当你能够正确地分析它时，你才能利用它。为了从数据中得出一些意义，需要正确的分析技术，并且在数据分析中选择正确的工具和技术至关重要。然而，当数据本身成为问题的一部分，并且在执行数据分析之前需要解决计算挑战时，它就成为了一个大数据问题。
- en: A revolution took place in the World Wide Web, also referred to as Web 2.0,
    which changed the way people used the Internet. Static web pages became interactive
    websites and started collecting more and more data. Technological advancements
    in cloud computing, social media, and mobile computing created an explosion of
    data. Every digital device started emitting data and many other sources started
    driving the data deluge. The dataflow from every nook and corner generated varieties
    of voluminous data, at speed! The formation of big data in this fashion was a
    natural phenomenon, because this is how the World Wide Web had evolved and no
    explicit efforts were involved in specifics. This is about the past! If you consider
    the change that is happening now, and is going to happen in future, the volume
    and speed of data generation is beyond what one can anticipate. I am propelled
    to make such a statement because every device is getting smarter these days, thanks
    to the **Internet of Things** (**IoT**).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在万维网上发生了一场革命，也被称为Web 2.0，改变了人们使用互联网的方式。静态网页变成了互动网站，并开始收集越来越多的数据。云计算、社交媒体和移动计算的技术进步创造了数据爆炸。每个数字设备开始发出数据，许多其他来源开始驱动数据洪流。来自各个角落的数据流产生了各种大量的数据，速度之快！以这种方式形成大数据是一种自然现象，因为这就是万维网的演变方式，没有明确的特定努力。这是关于过去的事情！如果考虑到正在发生的变化，以及未来将会发生的变化，数据生成的数量和速度超出了人们的预期。我之所以要做出这样的表态，是因为如今每个设备都变得更加智能，这要感谢物联网（IoT）。
- en: The IT trend was such that the technological advancements also facilitated the
    data explosion. Data storage had experienced a paradigm shift with the advent
    of cheaper clusters of online storage pools and the availability of commodity
    hardware with bare minimal price. Storing data from disparate sources in its native
    form in a single data lake was rapidly gaining over carefully designed data marts
    and data warehouses. Usage patterns also shifted from rigid schema-driven, RDBMS-based
    approaches to schema-less, continuously available **NoSQL** data-store-driven
    solutions. As a result, the rate of data creation, whether structured, semi-structured,
    or unstructured, started accelerating like never before.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: IT趋势是技术进步也促进了数据爆炸。随着更便宜的在线存储集群和廉价的通用硬件的出现，数据存储经历了一次范式转变。将来自不同来源的数据以其原生形式存储在单一数据湖中，迅速取代了精心设计的数据集市和数据仓库。使用模式也从严格的基于模式的RDBMS方法转变为无模式、持续可用的NoSQL数据存储驱动的解决方案。因此，无论是结构化、半结构化还是非结构化的数据创建速度都加速了前所未有的速度。
- en: Organizations are very much convinced that not only can specific business questions
    be answered by leveraging big data; it also brings in opportunities to cover the
    uncovered possibilities in businesses and address the uncertainties associated
    with this. So, apart from the natural data influx, organizations started devising
    strategies to generate more and more data to maintain their competitive advantages
    and to be future ready. Here, an example would help to understand this better.
    Imagine sensors are installed on the machines of a manufacturing plant which are
    constantly emitting data, and hence the status of the machine parts, and a company
    is able to predict when the machine is going to fail. It lets the company prevent
    a failure or damage and avoid unplanned downtime, saving a lot of money.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 组织非常确信，利用大数据不仅可以回答特定的业务问题，还可以带来机会，以覆盖业务中未发现的可能性，并解决与此相关的不确定性。因此，除了自然的数据涌入外，组织开始制定策略，产生越来越多的数据，以保持其竞争优势并做好未来准备。举个例子来更好地理解这一点。想象一下，在制造工厂的机器上安装了传感器，这些传感器不断地发出数据，因此可以得知机器零部件的状态，公司能够预测机器何时会发生故障。这让公司能够预防故障或损坏，避免计划外停机，从而节省大量资金。
- en: Challenges with big data analytics
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大数据分析的挑战
- en: There are broadly two types of formidable challenges in the analysis of big
    data. The first challenge is the requirement for a massive computation platform,
    and once it is in place, the second challenge is to analyze and make sense out
    of huge data at scale.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在大数据分析中，主要存在两种类型的严峻挑战。第一个挑战是需要一个庞大的计算平台，一旦建立起来，第二个挑战就是在规模上分析和理解大量数据。
- en: Computational challenges
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算挑战
- en: With the increase in data, the storage requirement for big data also grew more
    and more. Data management became a cumbersome task. The latency involved in accessing
    the disk storage due to the seek time became the major bottleneck even though
    the processing speed of the processor and the frequency of RAM were up to the
    mark.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据量的增加，大数据的存储需求也越来越大。数据管理变成了一项繁琐的任务。尽管处理器的处理速度和RAM的频率达到了标准，但由于寻道时间导致的访问磁盘存储的延迟成为了主要瓶颈。
- en: Fetching structured and unstructured data from across the gamut of business
    applications and data silos, consolidating them, and processing them to find useful
    business insights was challenging. There were only a few applications that could
    address any one area, or just a few areas of diversified business requirement.
    However, integrating those applications to address most of the business requirements
    in a unified way only increased the complexity.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 从各种业务应用程序和数据孤岛中提取结构化和非结构化数据，对其进行整合并加工以找到有用的业务见解是具有挑战性的。只有少数应用程序能够解决任何一个领域，或者只能解决少数多样化的业务需求。然而，将这些应用程序集成在一起以统一方式解决大部分业务需求只会增加复杂性。
- en: To address these challenges, people turned to the distributed computing framework
    with distributed file system, for example, Hadoop and **Hadoop Distributed File
    System** (**HDFS**). This could eliminate the latency due to disk I/O, as the
    data could be read in parallel across the cluster of machines.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些挑战，人们转向了具有分布式文件系统的分布式计算框架，例如Hadoop和**Hadoop分布式文件系统**（**HDFS**）。这可以消除由于磁盘I/O而产生的延迟，因为数据可以在机器集群上并行读取。
- en: Distributed computing technologies had existed for decades before, but gained
    more prominence only after the importance of big data was realized in the industry.
    So, technology platforms such as Hadoop and HDFS or Amazon S3 became the industry
    standard. On top of Hadoop, many other solutions such as Pig, Hive, Sqoop, and
    others were developed to address different kinds of industry requirements such
    as storage, **Extract, Transform, and Load** (**ETL**), and data integration to
    make Hadoop a unified platform.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式计算技术在之前已存在几十年，但直到行业意识到大数据的重要性后才变得更加突出。因此，诸如Hadoop和HDFS或Amazon S3之类的技术平台成为了行业标准。除了Hadoop之外，还开发了许多其他解决方案，如Pig、Hive、Sqoop等，以满足不同类型的行业需求，如存储、**提取、转换和加载**（**ETL**）以及数据集成，从而使Hadoop成为一个统一的平台。
- en: Analytical challenges
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析挑战
- en: Analyzing data to find some hidden insights has always been challenging because
    of the additional intricacies involved in dealing with huge datasets. The traditional
    BI and OLAP solutions could not address most of the challenges that arose due
    to big data. As an example, if there were multiple dimensions to a dataset, say
    100, it got really difficult to compare these variables with one another to draw
    a conclusion because there would be around 100C2 combinations for it. Such cases
    required statistical techniques such as *correlation* and the like to find the
    hidden patterns.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 分析数据以发现一些隐藏的见解一直是具有挑战性的，因为处理大型数据集涉及到额外的复杂性。传统的BI和OLAP解决方案无法解决由大数据带来的大部分挑战。举个例子，如果数据集有多个维度，比如100个，那么很难将这些变量相互比较以得出结论，因为会有大约100C2种组合。这种情况需要使用统计技术，如*相关性*等，来发现隐藏的模式。
- en: Though there were statistical solutions to many problems, it got really difficult
    for data scientists or analytics professionals to slice and dice the data to find
    intelligent insights unless they loaded the entire dataset into a **DataFrame**
    in memory. The major roadblock was that most of the general-purpose algorithms
    for statistical analysis and machine learning were single-threaded and written
    at a time when datasets were usually not so huge and could fit in the RAM on a
    single computer. Those algorithms written in R or Python were no longer very useful
    in their native form to be deployed on a distributed computing environment because
    of the limitation of in-memory computation.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: To address this challenge, statisticians and computer scientists had to work
    together to rewrite most of the algorithms that would work well in a distributed
    computing environment. Consequently, a library called **Mahout** for machine learning
    algorithms was developed on Hadoop for parallel processing. It had most of the
    common algorithms that were being used most often in the industry. Similar initiatives
    were taken for other distributed computing frameworks.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Evolution of big data analytics
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous section outlined how the computational and data analytics challenges
    were addressed for big data requirements. It was possible because of the convergence
    of several related trends such as low-cost commodity hardware, accessibility to
    big data, and improved data analytics techniques. Hadoop became a cornerstone
    in many large, distributed data processing infrastructures.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: However, people soon started realizing the limitations of Hadoop. Hadoop solutions
    were best suited for only specific types of big data requirements such as ETL;
    it gained popularity for such requirements only.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: There were scenarios when data engineers or analysts had to perform ad hoc queries
    on the data sets for interactive data analysis. Every time they ran a query on
    Hadoop, the data was read from the disk (HDFS-read) and loaded into the memory
    - which was a costly affair. Effectively, jobs were running at the speed of I/O
    transfers over the network and cluster of disks, instead of the speed of CPU and
    RAM.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a pictorial representation of the scenario:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '![Evolution of big data analytics](img/image_01_001.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
- en: One more case where Hadoop's MapReduce model could not fit in well was with
    machine learning algorithms that were iterative in nature. Hadoop MapReduce was
    underperforming, with huge latency in iterative computation. Since MapReduce had
    a restricted programming model with forbidden communication between Map and Reduce
    workers, the intermediate results needed to be stored in a stable storage. So,
    those were pushed on to the HDFS, which in turn writes into the instead of saving
    in RAM and then loading back in the memory for the subsequent iteration, similarly
    for the rest of the iterations. The number of disk I/O was dependent on the number
    of iterations involved in an algorithm and this was topped with the serialization
    and deserialization overhead while saving and loading the data. Overall, it was
    computationally expensive and could not get the level of popularity compared to
    what was expected of it.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a pictorial representation of this scenario:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '![Evolution of big data analytics](img/image_01_002.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
- en: To address this, tailor-made solutions were developed, for example, Google's
    Pregel, which was an iterative graph processing algorithm and was optimized for
    inter-process communication and in-memory storage for the intermediate results
    to make it run faster. Similarly, many other solutions were developed or redesigned
    that would best suit some of the specific needs that the algorithms used were
    designed for.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Instead of redesigning all the algorithms, a general-purpose engine was needed
    that could be leveraged by most of the algorithms for in-memory computation on
    a distributed computing platform. It was also expected that such a design would
    result in faster execution of iterative computation and ad hoc data analysis.
    This is how the Spark project paved its way out at the AMPLab at UC Berkeley.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Spark for data analytics
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Soon after the Spark project was successful in the AMP labs, it was made open
    source in 2010 and transferred to the Apache Software Foundation in 2013\. It
    is currently being led by Databricks.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark offers many distinct advantages over other distributed computing platforms,
    such as:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: A faster execution platform for both iterative machine learning and interactive
    data analysis
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Single stack for batch processing, SQL queries, real-time stream processing,
    graph processing, and complex data analytics
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provides high-level API to develop a diverse range of distributed applications
    by hiding the complexities of distributed programming
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seamless support for various data sources such as RDBMS, HBase, Cassandra, Parquet,
    MongoDB, HDFS, Amazon S3, and so on
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Spark for data analytics](img/image_01_003.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
- en: 'The following is a pictorial representation of in-memory data sharing for iterative
    algorithms:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '![Spark for data analytics](img/image_01_004.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
- en: Spark hides the complexities in writing the core MapReduce jobs and provides
    most of the functionalities through simple function calls. Because of its simplicity,
    it is able to cater to wider and bigger audience groups such as data scientists,
    data engineers, statisticians, and R/Python/Scala/Java developers.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: The Spark architecture broadly consists of a data storage layer, management
    framework, and API. It is designed to work on top of an HDFS filesystem, and thereby
    leverages the existing ecosystem. Deployment could be as a standalone server or
    on distributed computing frameworks such as Apache Mesos or YARN. An API is provided
    for Scala, the language in which Spark is written, along with Java, R and Python.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: The Spark stack
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark is a general-purpose cluster computing system that empowers other higher-level
    components to leverage its core engine. It is interoperable with Apache Hadoop,
    in the sense that it can read and write data from/to HDFS and can also integrate
    with other storage systems that are supported by the Hadoop API.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: While it allows building other higher-level applications on top of it, it already
    has a few components built on top that are tightly integrated with its core engine
    to take advantage of the future enhancements at the core. These applications come
    bundled with Spark to cover the broader sets of requirements in the industry.
    Most of the real-world applications need to be integrated across projects to solve
    specific business problems that usually have a set of requirements. This is eased
    out with Apache Spark as it allows its higher level components to be seamlessly
    integrated, such as libraries in a development project.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, with Spark''s built-in support for Scala, Java, R and Python, a broader
    range of developers and data engineers are able to leverage the entire Spark stack:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '![The Spark stack](img/image_01_005.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
- en: Spark core
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Spark core, in a way, is similar to the kernel of an operating system. It
    is the general execution engine, which is fast as well as fault tolerant. The
    entire Spark ecosystem is built on top of this core engine. It is mainly designed
    to do job scheduling, task distribution, and monitoring of jobs across worker
    nodes. It is also responsible for memory management, interacting with various
    heterogeneous storage systems, and various other operations.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: The primary building block of Spark core is the **Resilient Distributed Dataset**
    (**RDD**), which is an immutable, fault-tolerant collection of elements. Spark
    can create RDDs from a variety of data sources such as HDFS, local filesystems,
    Amazon S3, other RDDs, NoSQL data stores such as Cassandra, and so on. They are
    resilient in the sense that they automatically rebuild on failure. RDDs are built
    through lazy parallel transformations. They may be cached and partitioned, and
    may or may not be materialized.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Spark核心的主要构建模块是**弹性分布式数据集**（**RDD**），它是一个不可变的、容错的元素集合。Spark可以从各种数据源（如HDFS、本地文件系统、Amazon
    S3、其他RDD、Cassandra等NoSQL数据存储）创建RDD。它们在失败时会自动重建，因此具有容错性。RDD是通过惰性并行转换构建的。它们可以被缓存和分区，也可以或者不可以被实现。
- en: The entire Spark core engine may be viewed as a set of simple operations on
    distributed datasets. All the scheduling and execution of jobs in Spark is done
    based on the methods associated with each RDD. Also, the methods associated with
    each RDD define their own ways of distributed in-memory computation.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 整个Spark核心引擎可以被视为对分布式数据集进行简单操作的集合。Spark中所有作业的调度和执行都是基于与每个RDD相关联的方法完成的。此外，与每个RDD相关联的方法定义了它们自己的分布式内存计算方式。
- en: Spark SQL
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark SQL
- en: This module of Spark is designed to query, analyze, and perform operations on
    structured data. This is a very important component in the entire Spark stack
    because of the fact that most of the organizational data is structured, though
    unstructured data is growing rapidly. Acting as a distributed query engine, it
    enables Hadoop Hive queries to run up to 100 times faster on it without any modification.
    Apart from Hive, it also supports Apache Parquet, an efficient columnar storage,
    JSON, and other structured data formats. Spark SQL enables running SQL queries
    along with complex programs written in Python, Scala, and Java.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这个Spark模块旨在查询、分析和对结构化数据执行操作。这是整个Spark堆栈中非常重要的一个组件，因为大多数组织数据都是结构化的，尽管非结构化数据正在迅速增长。作为一个分布式查询引擎，它使Hadoop
    Hive查询在不进行任何修改的情况下可以运行得更快，最多可以提高100倍。除了Hive，它还支持Apache Parquet（一种高效的列存储）、JSON和其他结构化数据格式。Spark
    SQL使得可以在Python、Scala和Java中运行SQL查询以及复杂程序。
- en: Spark SQL provides a distributed programming abstraction called **DataFrames**,
    referred to as SchemaRDD before, which had fewer functions associated with it.
    DataFrames are distributed collections of named columns, analogous to SQL tables
    or Python's Pandas DataFrames. They can be constructed with a variety of data
    sources that have schemas with them such as Hive, Parquet, JSON, other RDBMS sources,
    and also from Spark RDDs.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL提供了一个名为**数据框**的分布式编程抽象，之前称为SchemaRDD，它的相关函数较少。数据框是命名列的分布式集合，类似于SQL表或Python的Pandas数据框。它们可以使用具有模式的各种数据源构建，例如Hive、Parquet、JSON、其他RDBMS源，以及Spark
    RDD。
- en: Spark SQL can be used for ETL processing across different formats and then running
    ad hoc analysis. Spark SQL comes with an optimizer framework called Catalyst that
    can transform SQL queries for better efficiency.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL可用于跨不同格式的ETL处理，然后进行临时分析。Spark SQL配备了一个名为Catalyst的优化器框架，可以将SQL查询转换为更高效的形式。
- en: Spark streaming
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark流处理
- en: The processing window for the enterprise data is becoming shorter than ever.
    To address the real-time processing requirement of the industry, this component
    of Spark was designed, which is fault tolerant as well as scalable. Spark enables
    real-time data analytics on live streams of data by supporting data analysis,
    machine learning, and graph processing on them.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 企业数据的处理窗口正在变得比以往任何时候都要短。为了满足行业的实时处理需求，设计了Spark的这个组件，它既具有容错性又可扩展。Spark通过支持对实时数据流进行数据分析、机器学习和图处理，实现了对实时数据流的实时数据分析。
- en: It provides an API called **Discretised Stream** (**DStream**) to manipulate
    the live streams of data. The live streams of data are sliced up into small batches
    of, say, *x* seconds. Spark treats each batch as an RDD and processes them as
    basic RDD operations. DStreams can be created out of live streams of data from
    HDFS, Kafka, Flume, or any other source which can stream data on the TCP socket.
    By applying some higher-level operations on DStreams, other DStreams can be produced.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 它提供了一个名为**离散流**（**DStream**）的API，用于操作实时数据流。实时数据流被切分成小批次，比如说，*x*秒。Spark将每个批次视为RDD并对它们进行基本的RDD操作。DStreams可以从HDFS、Kafka、Flume或任何其他能够通过TCP套接字流式传输数据的源创建出来。通过在DStreams上应用一些高级操作，可以产生其他DStreams。
- en: The final result of Spark streaming can either be written back to the various
    data stores supported by Spark or can be pushed to any dashboard for visualization.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Spark流处理的最终结果可以被写回到Spark支持的各种数据存储中，也可以被推送到任何仪表板进行可视化。
- en: MLlib
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MLlib
- en: MLlib is the built-in machine learning library in the Spark stack. This was
    introduced in Spark 0.8\. Its goal is to make machine learning scalable and easy.
    Developers can seamlessly use Spark SQL, Spark Streaming, and GraphX in their
    programming language of choice, be it Java, Python, or Scala. MLlib provides the
    necessary functions to perform various statistical analyses such as correlations,
    sampling, hypothesis testing, and so on. This component also has a broad coverage
    of applications and algorithms in classification, regression, collaborative filtering,
    clustering, and decomposition.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib是Spark堆栈中内置的机器学习库。它是在Spark 0.8中引入的。其目标是使机器学习变得可扩展和简单。开发人员可以无缝地在他们选择的编程语言（Java、Python或Scala）中使用Spark
    SQL、Spark流处理和GraphX。MLlib提供了执行各种统计分析（如相关性、抽样、假设检验等）所需的函数。此组件还涵盖了分类、回归、协同过滤、聚类和分解等领域的广泛应用和算法。
- en: The machine learning workflow involves collecting and preprocessing data, building
    and deploying the model, evaluating the results, and refining the model. In the
    real world, the preprocessing steps take up significant effort. These are typically
    multi-stage workflows involving expensive intermediate read/write operations.
    Often, these processing steps may be performed multiple times over a period of
    time. A new concept called **ML Pipelines** was introduced to streamline these
    preprocessing steps. A Pipeline is a sequence of transformations where the output
    of one stage is the input of another, forming a chain. The ML Pipeline leverages
    Spark and MLlib and enables developers to define reusable sequences of transformations.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: GraphX
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GraphX is a thin-layered unified graph analytics framework on Spark. It was
    designed to be a general-purpose distributed dataflow framework in place of specialized
    graph processing frameworks. It is fault tolerant and also exploits in-memory
    computation.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '**GraphX** is an embedded graph processing API for manipulating graphs (for
    example, social networks) and to do graph parallel computation (for example, Google''s
    Pregel). It combines the advantages of both graph-parallel and data-parallel systems
    on the Spark stack to unify exploratory data analysis, iterative graph computation,
    and ETL processing. It extends the RDD abstraction to introduce the **Resilient
    Distributed Graph** (**RDG**), which is a directed graph with properties associated
    to each of its vertices and edges.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: GraphX includes a decently large collection of graph algorithms, such as PageRank,
    K-Core, Triangle Count, LDA, and so on.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: SparkR
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The SparkR project was started to integrate the statistical analysis and machine
    learning capability of R with the scalability of Spark. It addressed the limitation
    of R, which was its ability to process as much data as fitted in the memory of
    a single machine. R programs can now scale in a distributed setting through SparkR.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: SparkR is actually an R Package that provides an R shell to leverage Spark's
    distributed computing engine. With R's rich set of built-in packages for data
    analytics, data scientists can analyze large datasets interactively at scale.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we briefly covered what big data is all about. We then discussed
    the computational and analytical challenges involved in big data analytics. Later,
    we looked at how the analytics space in the context of big data has evolved over
    a period of time and what the trend has been. We also covered how Spark addressed
    most of the big data analytics challenges and became a general-purpose unified
    analytics platform for data science as well as parallel computation. At the end
    of this chapter, we just gave you a heads-up on the Spark stack and its components.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about the Spark programming model. We will
    take a deep dive into the basic building block of Spark, which is the RDD. Also,
    we will learn how to program with the RDD API on Scala and Python.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Apache Spark overview:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/](http://spark.apache.org/docs/latest/)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://databricks.com/spark/about](https://databricks.com/spark/about)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Apache Spark architecture:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[http://lintool.github.io/SparkTutorial/slides/day1_context.pdf](http://lintool.github.io/SparkTutorial/slides/day1_context.pdf)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
