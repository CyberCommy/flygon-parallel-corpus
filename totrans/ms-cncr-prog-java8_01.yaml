- en: Chapter 1. The First Step – Concurrency Design Principles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Users of computer systems are always looking for better performance for their
    systems. They want to get higher quality videos, better video games, and faster
    network speed. Some years ago, processors gave better performance to users by
    increasing their speed. But now, processors don''t increase their speed. Instead
    of this, they add more cores so that the operating system can execute more than
    one task at a time. This is named **concurrency**. Concurrent programming includes
    all the tools and techniques to have multiple tasks or processes running at the
    same time in a computer, communicating and synchronizing between them without
    data loss or inconsistency. In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Basic concurrency concepts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Possible problems in concurrent applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A methodology to design concurrent algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Java concurrency API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Java memory model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concurrency design patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tips and tricks to design concurrency algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic concurrency concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First of all, let's present the basic concepts of concurrency. You must understand
    these concepts to follow the rest of the book.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency versus parallelism
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Concurrency and parallelism are very similar concepts. Different authors give
    different definitions to these concepts. The most accepted definition talks about
    concurrency when you have more than one task in a single processor with a single
    core and the operating system's task scheduler quickly switches from one task
    to another, so it seems that all the tasks run simultaneously. The same definition
    talks about parallelism when you have more than one task that run simultaneously
    at the same time, in a different computer, processor, or core inside a processor.
  prefs: []
  type: TYPE_NORMAL
- en: Another definition talks about concurrency when you have more than one task
    (different tasks) running simultaneously on your system. One more definition discusses
    parallelism when you have different instances of the same task running simultaneously
    over different parts of a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The last definition that we include talks about parallelism when you have more
    than one task that runs simultaneously in your system and talks about concurrency
    to explain the different techniques and mechanisms programmers have to synchronize
    with the tasks and their access to shared resources.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, both concepts are very similar and this similarity has increased
    with the development of multicore processors.
  prefs: []
  type: TYPE_NORMAL
- en: Synchronization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In concurrency, we can define **synchronization** as the coordination of two
    or more tasks to get the desired results. We have two kinds of synchronization:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Control synchronization**: When, for example, one task depends on the end
    of another task, the second task can''t start before the first has finished'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data access synchronization**: When two or more tasks have access to a shared
    variable and only one of the tasks can access the variable at any given time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A concept closely related to synchronization is **critical section**. A critical
    section is a piece of code that can be only executed by a task at any given time
    because of its access to a shared resource. **Mutual exclusion** is the mechanism
    used to guarantee this requirement and can be implemented by different ways.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that synchronization helps you avoid some errors you can have with
    concurrent tasks (they will be described later in this chapter), but it introduces
    some overhead to your algorithm. You have to calculate very carefully the number
    of tasks, which can be performed independently without intercommunication in your
    parallel algorithm. It's the **granularity** of your concurrent algorithm. If
    you have a **coarse-grained granularity** (big tasks with low intercommunication),
    the overhead due to synchronization will be low. However, maybe you won't benefit
    all the cores of your system. If you have a **fine-grained granularity** (small
    tasks with high intercommunication), the overhead due to synchronization will
    be high and maybe the throughput of your algorithm won't be good.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are different mechanisms to get synchronization in a concurrent system.
    The most popular mechanisms from a theoretical point of view are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Semaphore**: A semaphore is a mechanism that can be used to control the access
    to one or more units of a resource. It has a variable that stores the number of
    resources that can be used and two atomic operations to manage the value of the
    variable. A **mutex** (short for **mutual exclusion**) is a special kind of semaphore
    that can take only two values (*resource is free* and *resource is busy*), and
    only the process that sets the mutex to *busy* can release it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitor**: A monitor is a mechanism to get mutual exclusion over a shared
    resource. It has a mutex, a condition variable, and two operations to wait for
    the condition and to signal the condition. Once you signal the condition, only
    one of the tasks that are waiting for it continues with its execution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last concept related to synchronization you're going to learn in this chapter
    is **thread safety**. A piece of code (or a method or an object) is **thread-safe**
    if all the users of shared data are protected by synchronization mechanisms, a
    nonblocking **compare-and-swap** (**CAS**) primitive or data is immutable, so
    you can use that code in a concurrent application without any problem.
  prefs: []
  type: TYPE_NORMAL
- en: Immutable object
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An **immutable object** is an object with a very special characteristic. You
    can't modify its visible state (the value of its attributes) after its initialization.
    If you want to modify an immutable object, you have to create a new one.
  prefs: []
  type: TYPE_NORMAL
- en: Its main advantage is that it is thread-safe. You can use it in concurrent applications
    without any problem.
  prefs: []
  type: TYPE_NORMAL
- en: An example of an immutable object is the `String` class in Java. When you assign
    a new value to a `String` object, you are creating a new string.
  prefs: []
  type: TYPE_NORMAL
- en: Atomic operations and variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An **atomic operation** is a kind of operation that appears to occur instantaneously
    to the rest of the tasks of the program. In a concurrent application, you can
    implement an atomic operation with a critical section to the whole operation using
    a synchronization mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: An **atomic variable** is a kind of variable with atomic operations to set and
    get its value. You can implement an atomic variable using a synchronization mechanism
    or in a lock-free manner using CAS, which doesn't need any synchronization.
  prefs: []
  type: TYPE_NORMAL
- en: Shared memory versus message passing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tasks can use two different methods to communicate with each other. The first
    one is **shared memory,** and normally it is used when the tasks are running in
    the same computer. The tasks use the same memory area where they write and read
    values. To avoid problems, the access to this shared memory has to be in a critical
    section protected by a synchronization mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: The other synchronization mechanism is **message passing** and normally is used
    when the tasks are running in different computers. When a task needs to communicate
    with another, it sends a message that follows a predefined protocol. This communication
    can be synchronous if the sender is blocked waiting for a response or asynchronous
    if the sender continues with their execution after sending the message.
  prefs: []
  type: TYPE_NORMAL
- en: Possible problems in concurrent applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Programming a concurrent application is not an easy job. If you incorrectly
    use the synchronization mechanisms, you can have different problems with the tasks
    in your application. In this section, we describe some of these problems.
  prefs: []
  type: TYPE_NORMAL
- en: Data race
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can have a **data race** (also named **race condition**) in your application
    when you have two or more tasks writing a shared variable outside a critical section—that's
    to say, without using any synchronization mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Under these circumstances, the final result of your application may depend
    on the order of execution of the tasks. Look at the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Imagine that two different tasks execute the `modify()` method in the same `Account`
    object. Depending on the order of execution of the sentences in the tasks, the
    final result can vary. Suppose that the initial balance is 1000 and the two tasks
    call the `modify()` method with 1000 as a parameter. The final result should be
    3000, but if both tasks execute the first sentence at the same time and then the
    second sentence at the same time, the final result will be 2000\. As you can see,
    the `modify()` method is not atomic and the `Account` class is not thread-safe.
  prefs: []
  type: TYPE_NORMAL
- en: Deadlock
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There is a **deadlock** in your concurrent application when there are two or
    more tasks waiting for a shared resource that must be free from the other, so
    none of them will get the resources they need and will be blocked indefinitely.
    It happens when four conditions happen simultaneously in the system. They are
    **Coffman''s conditions**, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mutual exclusion**: The resources involved in the deadlock must be nonshareable.
    Only one task can use the resource at a time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hold and wait condition**: A task has the mutual exclusion for a resource
    and it''s requesting the mutual exclusion for another resource. While it''s waiting,
    it doesn''t release any resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**No pre-emption**: The resources can only be released by the tasks that hold
    them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Circular wait**: There is a circular waiting where Task 1 is waiting for
    a resource that is being held by Task 2, which is waiting for a resource being
    held by Task 3, and so on until we have Task *n* that is waiting for a resource
    being held by Task 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There exist some mechanisms that you can use to avoid deadlocks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ignore them**: This is the most commonly used mechanism. You suppose that
    a deadlock will never occur on your system, and if it occurs, you can see the
    consequences of stopping your application and having to re-execute it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Detection**: The system has a special task that analyzes the state of the
    system to detect if a deadlock has occurred. If it detects a deadlock, it can
    take action to remedy the problem. For example, finishing one task or forcing
    the liberation of a resource.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prevention**: If you want to prevent deadlocks in your system, you have to
    prevent one or more of Coffman''s conditions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Avoidance**: Deadlocks can be avoided if you have information about the resources
    that are used by a task before it begins its execution. When a task wants to start
    its execution, you can analyze the resources that are free in the system and the
    resources that the task needs to decide that it can start its execution or not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Livelock
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A **livelock** occurs when you have two tasks in your systems that are always
    changing their states due to the actions of the other. Consequently, they are
    in a loop of state changes and unable to continue.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, you have two tasks—Task 1 and Task 2—and both need two resources:
    Resource 1 and Resource 2\. Suppose that Task 1 has a lock on Resource 1, and
    Task 2 has a lock on Resource 2\. As they are unable to gain access to the resource
    they need, they free their resources and begin the cycle again. This situation
    can continue indefinitely, so the tasks will never end their execution.'
  prefs: []
  type: TYPE_NORMAL
- en: Resource starvation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Resource starvation** occurs when you have a task in your system that never
    gets a resource that it needs to continue with its execution. When there is more
    than one task waiting for a resource and the resource is released, the system
    has to choose the next task that can use it. If your system has not got a good
    algorithm, it can have threads that are waiting for a long time for the resource.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fairness** is the solution to this problem. All the tasks that are waiting
    for a resource must have the resource in a given period of time. An option is
    to implement an algorithm that takes into account the time that a task has been
    waiting for a resource when it chooses the next task that will hold a resource.
    However, fair implementation of locks requires additional overhead, which may
    lower your program throughput.'
  prefs: []
  type: TYPE_NORMAL
- en: Priority inversion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Priority inversion** occurs when a low-priority task holds a resource that
    is needed by a high-priority task, so the low-priority task finishes its execution
    before the high-priority task.'
  prefs: []
  type: TYPE_NORMAL
- en: A methodology to design concurrent algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we''re going to propose a five-step methodology to get a concurrent
    version of a sequential algorithm. It''s based on the one presented by Intel in
    their *Threading Methodology: Principles and Practices* document.'
  prefs: []
  type: TYPE_NORMAL
- en: The starting point – a sequential version of the algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our starting point to implement a concurrent algorithm will be a sequential
    version of it. Of course, we can design a concurrent algorithm from scratch, but
    I think that a sequential version of the algorithm will give us two advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: We can use the sequential algorithm to test if our concurrent algorithm generates
    correct results. Both algorithms must generate the same output when they receive
    the same input, so we can detect some problems in the concurrent version, such
    as data races or similar conditions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can measure the throughput of both algorithms to see if the use of concurrency
    gives us a real improvement in the response time or in the amount of data the
    algorithm can process in a time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Step 1 – analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this step, we are going to analyze the sequential version of the algorithm
    to look for the parts of its code that can be executed in a parallel way. We should
    pay special attention to those parts that are executed most of the time or that
    execute more code because, by implementing a concurrent version of those parts,
    we're going to get a greater performance improvement.
  prefs: []
  type: TYPE_NORMAL
- en: Good candidates for this process are loops where one step is independent of
    the other steps or portions of code that are independent of other parts of the
    code (for example, an algorithm to initialize an application that opens the connections
    with the database, loads the configuration files, initialize some objects. All
    the previous tasks are independent of each other).
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 – design
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once you know what parts of the code you are going to parallelize, you have
    to decide how to do that parallelization.
  prefs: []
  type: TYPE_NORMAL
- en: 'The changes in the code will affect two main parts of the application:'
  prefs: []
  type: TYPE_NORMAL
- en: The structure of the code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The organization of the data structures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can take two different approaches to accomplish this task:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Task decomposition**: You do task decomposition when you split the code in
    two or more independent tasks that can be executed at once. Maybe some of these
    tasks have to be executed in a given order or have to wait at the same point.
    You must use synchronization mechanisms to get this behavior.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data decomposition**: You do data decomposition when you have multiple instances
    of the same task that work with a subset of the dataset. This dataset will be
    a shared resource, so if the tasks need to modify the data you have to protect
    access to it by implementing a critical section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another important point to keep in mind is the granularity of your solution.
    The objective of implementing a parallel version of an algorithm is to achieve
    improved performance, so you should use all the available processors or cores.
    On the other hand, when you use a synchronization mechanism, you introduce some
    extra instructions that must be executed. If you split the algorithm into a lot
    of small tasks (fine-grained granularity), the extra code introduced by the synchronization
    can provoke performance degradation. If you split the algorithm into fewer tasks
    than cores (coarse-grained granularity), you are not taking advantage of all the
    resources. Also, you must take into account the work every thread must do, especially
    if you implement a fine-grained granularity. If you have a task longer than the
    rest, that task will determine the execution time of the application. You have
    to find the equilibrium between these two points.
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 – implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next step is to implement the parallel algorithm using a programming language
    and, if it's necessary, a thread library. In the examples of this book, you are
    going to use Java to implement all the algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Step 4 – testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After finishing the implementation, you have to test the parallel algorithm.
    If you have a sequential version of the algorithm, you can compare the results
    of both algorithms to verify that your parallel implementation is correct.
  prefs: []
  type: TYPE_NORMAL
- en: Testing and debugging a parallel implementation are difficult tasks because
    the order of execution of the different tasks of the application is not guaranteed.
    In [Chapter 11](part0063_split_000.html#1S2JE2-2fff3d3b99304faa8fa9b27f1b5053ba
    "Chapter 11. Testing and Monitoring Concurrent Applications"), *Testing and Monitoring
    Concurrent Applications*, you will learn tips, tricks, and tools to do these tasks
    efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Step 5 – tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The last step is to compare the throughput of the parallel and the sequential
    algorithms. If the results are not as expected, you must review the algorithm,
    looking for the cause of the bad performance of the parallel algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: You can also test different parameters of the algorithm (for example, granularity
    or number of tasks) to find the best configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are different metrics to measure the possible performance improvement
    you can obtain parallelizing an algorithm. The three most popular metrics are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Speedup**: This is a metric for relative performance improvement between
    the parallel and the sequential versions of the algorithm:![Step 5 – tuning](img/00002.jpeg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, *T* *[sequential]* is the execution time of the sequential version of
    the algorithm and *T* *[concurrent]* is the execution time of the parallel version.
  prefs: []
  type: TYPE_NORMAL
- en: '**Amdahl''s law**: This is used to calculate the maximum expected improvement
    obtained with the parallelization of an algorithm:![Step 5 – tuning](img/00003.jpeg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, *P* is the percentage of code that can be parallelized and *N* is the
    number of cores of the computer where you're going to execute the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if you can parallelize 75% of the code and you have four cores,
    the maximum speedup will be given by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Step 5 – tuning](img/00004.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '**Gustafson-Barsis'' law**: Amdahl''s law has a limitation. It supposes that
    you have the same input dataset when you increase the number of cores, but normally,
    when you have more cores, you want to process more data. Gustafson law proposes
    that when you have more cores available, bigger problems can be solved in the
    same time using the following formula:![Step 5 – tuning](img/00005.jpeg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, *N* is the number of cores and *P* is the percentage of parallelizable
    code.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we use the same example as before, the scaled speedup calculated by the
    Gustafson law is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Step 5 – tuning](img/00006.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, you learned some important issues you have to take into account
    when you want to parallelize a sequential algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: First of all, not every algorithm can be parallelized. For example, if you have
    to execute a loop where the result of an iteration depends on the result of the
    previous iteration, you can't parallelize that loop. Recurrent algorithms are
    another example of algorithms that can be parallelized for a similar reason.
  prefs: []
  type: TYPE_NORMAL
- en: Another important thing you have to keep in mind is that the sequential version
    of an algorithm with better performance can be a bad starting point to parallelize
    it. If you start parallelizing an algorithm and you find yourself in trouble because
    you don't easily find independent portions of the code, you have to look for other
    versions of the algorithm and verify that the version can be parallelized in an
    easier way.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, when you implement a concurrent application (from scratch or based
    on a sequential algorithm), you must take into account the following points:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Efficiency**: The parallel algorithm must end in less time than the sequential
    algorithm. The first goal of parallelizing an algorithm is that its running time
    is less than the sequential one, or it can process more data in the same time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Simplicity**: When you implement an algorithm (parallel or not), you must
    keep it as simple as possible. It will be easier to implement, test, debug, and
    maintain, and it will have fewer errors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Portability**: Your parallel algorithm should be executed on different platforms
    with minimal changes. As in this book you will use Java, this point will be very
    easy. With Java, you can execute your programs in every operating system without
    any change (if you implement the program as you must).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: What happens to your algorithm if you increase the number
    of cores? As mentioned before, you should use all the available cores, so your
    algorithm should be ready to take advantage of all available resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Java concurrency API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Java programming language has a very rich concurrency API. It contains classes
    to manage the basic elements of concurrency, such as `Thread`, `Lock`, and `Semaphore`,
    and classes that implement very high-level synchronization mechanisms, such as
    the **executor framework** or the new parallel `Stream` API.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will cover the basic classes that form the concurrency API.
  prefs: []
  type: TYPE_NORMAL
- en: Basic concurrency classes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The basic classes of the Java concurrency API are:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Thread` class: This class represents all the threads that execute a concurrent
    Java application'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `Runnable` interface: This is another way to create concurrent applications
    in Java'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `ThreadLocal` class: This is a class to store variables locally to a thread'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `ThreadFactory` interface: This is the base of the Factory design pattern
    that you can use to create customized threads'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synchronization mechanisms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Java concurrency API includes different synchronization mechanisms that
    allow you to:'
  prefs: []
  type: TYPE_NORMAL
- en: Define a critical section to access a shared resource
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synchronize different tasks in a common point
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following mechanisms are considered to be the most important synchronization
    mechanisms:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `synchronized` keyword: The `synchronized` keyword allows you to define
    a critical section in a block of code or in an entire method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `Lock` interface: `Lock` provides a more flexible synchronization operation
    than the `synchronized` keyword. There are different kinds of Locks: `ReentrantLock`,
    to implement a Lock that can be associated with a condition; `ReentrantReadWriteLock`,
    which separates read and write operations; and `StampedLock`, a new feature of
    Java 8 that includes three modes for controlling read/write access.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `Semaphore` class: The class that implements the classical semaphore to
    implement synchronization. Java supports binary and general semaphores.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `CountDownLatch` class: A class that allows a task to wait for the finalization
    of multiple operations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `CyclicBarrier` class: A class that allows the synchronization of multiple
    threads in a common point.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `Phaser` class: A class that allows you to control the execution of tasks
    divided into phases. None of the tasks advance to the next phase until all of
    the tasks have finished the current phase.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Executors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The executor framework is a mechanism that allows you to separate thread creation
    and management for the implementation of concurrent tasks. You don''t have to
    worry about the creation and management of threads, only about creating tasks
    and sending them to the executor. The main classes involved in this framework
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Executor` and `ExecutorService` interface: They include methods common
    to all executors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ThreadPoolExecutor`: This is a class that allows you to get an executor with
    a pool of threads and optionally define a maximum number of parallel tasks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ScheduledThreadPoolExecutor`: This is a special kind of executor to allow
    you to execute tasks after a delay or periodically'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Executors`: This is a class that facilitates the creation of executors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `Callable` interface: This is an alternative to the *Runnable* interface—a
    separate task that can return a value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `Future` interface: This is an interface that includes the methods to obtain
    the value returned by a `Callable` interface and to control its status'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Fork/Join framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **Fork/Join framework** defines a special kind of executor specialized
    in the resolution of problems with the divide and conquer technique. It includes
    a mechanism to optimize the execution of the concurrent tasks that solve these
    kinds of problems. Fork/Join is specially tailored for fine-grained parallelism
    as it has a very low overhead in order to place the new tasks into the queue and
    take queued tasks for execution. The main classes and interfaces involved in this
    framework are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ForkJoinPool`: This is a class that implements the executor that is going
    to run the tasks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ForkJoinTask`: This is a task that can be executed in the `ForkJoinPool` class'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ForkJoinWorkerThread`: This is a thread that is going to execute tasks in
    the `ForkJoinPool` class'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallel streams
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Streams** and **Lambda expressions** are maybe the two most important new
    features of the Java 8 version. Streams have been added as a method in the `Collection`
    interface and other data sources and allow processing all elements of a data structure,
    generating new structures, filtering data and implementing algorithms using the
    map and reduce technique.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A special kind of stream is a parallel stream which realizes its operations
    in a parallel way. The most important elements involved in the use of parallel
    streams are:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Stream` interface: This is an interface that defines all the operations
    that you can perform on a stream.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Optional`: This is a container object that may or may not contain a non-null
    value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Collectors`: This is a class that implements reduction operations that can
    be used as part of a stream sequence of operations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lambda expressions: Streams has been thought to work with Lambda expressions.
    Most stream methods accept a lambda expression as a parameter. This allows you
    to implement a more compact version of the operations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concurrent data structures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Normal data structures of the Java API (`ArrayList`, `Hashtable`, and so on)
    are not ready to work in a concurrent application unless you use an external synchronization
    mechanism. If you use it, you will be adding a lot of extra computing time to
    your application. If you don't use it, it's probable that you will have race conditions
    in your application. If you modify them from several threads and a race condition
    occurs, you may experience various exceptions thrown (such as, `ConcurrentModificationException`
    and `ArrayIndexOutOfBoundsException`), there may be silent data loss or your program
    may even stuck in an endless loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Java concurrency API includes a lot of data structures that can be used
    in concurrent applications without risk. We can classify them in two groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Blocking data structures**: These include methods that block the calling
    task when, for example, the data structure is empty and you want to get a value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Non-blocking data structures:** If the operation can be made immediately,
    it won''t block the calling tasks. Otherwise, it returns the `null` value or throws
    an exception.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These are some of the data structures:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ConcurrentLinkedDeque`: This is a non-blocking list'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ConcurrentLinkedQueue`: This is a non-blocking queue'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LinkedBlockingDeque`: This is a blocking list'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LinkedBlockingQueue`: This is a blocking queue'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PriorityBlockingQueue`: This is a blocking queue that orders its elements
    based on its priority'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ConcurrentSkipListMap`: This is a non-blocking navigable map'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ConcurrentHashMap`: This is a non-blocking hash map'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AtomicBoolean`, `AtomicInteger`, `AtomicLong`, and `AtomicReference`: These
    are atomic implementations of the basic Java data types'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concurrency design patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In software engineering, a **design pattern** is a solution to a common problem.
    This solution has been used many times, and it has proved to be an optimal solution
    to the problem. You can use them to avoid 'reinventing the wheel' every time you
    have to solve one of these problems. **Singleton** or **Factory** are the examples
    of common design patterns used in almost every application.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency also has its own design patterns. In this section, we describe some
    of the most useful concurrency design patterns and their implementation in the
    Java language.
  prefs: []
  type: TYPE_NORMAL
- en: Signaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This design pattern explains how to implement the situation where a task has
    to notify an event to another task. The easiest way to implement this pattern
    is with a semaphore or a mutex, using the `ReentrantLock` or `Semaphore` classes
    of the Java language or even the `wait()` and `notify()` methods included in the
    `Object` class.
  prefs: []
  type: TYPE_NORMAL
- en: 'See the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Under these circumstances, the `section2()` method will always be executed after
    the `section1()` method.
  prefs: []
  type: TYPE_NORMAL
- en: Rendezvous
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This design pattern is a generalization of the **Signaling** pattern. In this
    case, the first task waits for an event of the second task and the second task
    waits for an event of the first task. The solution is similar to that of Signaling,
    but in this case you must use two objects instead of one.
  prefs: []
  type: TYPE_NORMAL
- en: 'See the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Under these circumstances, `section2_2()` always will be executed after `section1_1()`
    and `section1_2()` after `section2_1()`, take into account that, if you put the
    call to the `wait()` method before the call to the `notify()` method, you will
    have a deadlock.
  prefs: []
  type: TYPE_NORMAL
- en: Mutex
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A mutex is a mechanism that you can use to implement a critical section ensuring
    mutual exclusion. That is to say, only one task can execute the portion of code
    protected by the mutex at one time. In Java, you can implement a critical section
    using the `synchronized` keyword (that allows you to protect a portion of code
    or a full method), the `ReentrantLock` class, or the `Semaphore` class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Look at the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Multiplex
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Multiplex design pattern** is a generalization of the mutex. In this case,
    a determined number of tasks can execute the critical section at once. It is useful,
    for example, when you have multiple copies of a resource. The easiest way to implement
    this design pattern in Java is using the `Semaphore` class initialized to the
    number of tasks that can execute the critical section at once.
  prefs: []
  type: TYPE_NORMAL
- en: 'Look at the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Barrier
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This design pattern explains how to implement the situation where you need to
    synchronize some tasks at a common point. None of the tasks can continue with
    their execution until all the tasks have arrived at the synchronization point.
    The Java concurrency API provides the `CyclicBarrier` class, which is an implementation
    of this design pattern.
  prefs: []
  type: TYPE_NORMAL
- en: 'Look at the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Double-checked locking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This design pattern provides a solution to the problem that occurs when you
    acquire a lock and then check for a condition. If the condition is false, you
    have had the overhead of acquiring the lock ideally. An example of this situation
    is the lazy initialization of objects. If you have a class implementing the `Singleton`
    design pattern, you may have some code like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'A possible solution can be to include the lock inside the conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This solution still has problems. If two tasks check the condition at once,
    you will create two objects. The best solution to this problem doesn''t use any
    explicit synchronization mechanism:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Read-write lock
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When you protect access to a shared variable with a lock, only one task can
    access that variable, independently of the operation you are going to perform
    on it. Sometimes, you will have variables that you modify a few times but read
    many times. In this circumstance, a lock provides poor performance because all
    the read operations can be made concurrently without any problem. To solve this
    problem, there exists the read-write lock design pattern. This pattern defines
    a special kind of lock with two internal locks: one for read operations and the
    other for write operations. The behavior of this lock is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: If one task is doing a read operation and another task wants to do another read
    operation, it can do it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If one task is doing a read operation and another task wants to do a write operation,
    it's blocked until all the readers finish
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If one task is doing a write operation and another task wants to do an operation
    (read or write), it's blocked until the writer finishes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Java concurrency API includes the class `ReentrantReadWriteLock` that implements
    this design pattern. If you want to implement this pattern from scratch, you have
    to be very careful with the priority between read-tasks and write-tasks. If too
    many read-tasks exist, write-tasks can be waiting too long.
  prefs: []
  type: TYPE_NORMAL
- en: Thread pool
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This design pattern tries to remove the overhead introduced by creating a thread
    for the task you want to execute. It's formed by a set of threads and a queue
    of tasks you want to execute. The set of threads usually has a fixed size. When
    a thread approaches the execution of a task, it doesn't finish its execution;
    it looks for another task in the queue. If there is another task, it executes
    it. If not, the thread waits until a task is inserted in the queue, but it's not
    destroyed.
  prefs: []
  type: TYPE_NORMAL
- en: The Java concurrency API includes some classes that implement the `ExecutorService`
    interface, which internally uses a pool of threads.
  prefs: []
  type: TYPE_NORMAL
- en: Thread local storage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This design pattern defines how to use global or static variables locally to
    tasks. When you have a static attribute in a class, all the objects of a class
    access the same occurrences of the attribute. If you use thread local storage,
    each thread accesses a different instance of the variable.
  prefs: []
  type: TYPE_NORMAL
- en: The Java concurrency API includes the `ThreadLocal` class to implement this
    design pattern.
  prefs: []
  type: TYPE_NORMAL
- en: The Java memory model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you execute a concurrent application in a computer with several cores or
    processors, you can have a problem with memory caches. They are very useful to
    increment the performance of the application, but they can cause data inconsistency.
    When a task modifies the value of a variable, it's modified in the cache, but
    it's not modified in the main memory immediately. If another task reads the value
    of that variable before it's updated in the main memory, it will read the old
    value of the variable.
  prefs: []
  type: TYPE_NORMAL
- en: Other problems that may exist with concurrent applications are the optimizations
    introduced by the compilers and code optimizer. Sometimes, they reorder the instructions
    to get a better performance. In sequential applications, this doesn't cause any
    problems, but in concurrent applications it can provoke unexpected results.
  prefs: []
  type: TYPE_NORMAL
- en: To solve problems such as this, programming languages introduced memory models.
    A memory model describes how individual tasks interact with each other through
    memory and when changes made by one task will be visible to another. It also defines
    what optimizations of code are allowed and under what circumstances.
  prefs: []
  type: TYPE_NORMAL
- en: There are different memory models. Some of them are very strict (all of the
    tasks always have access to the same values) and others are less stringent (only
    some instructions update the values in the main memory). The memory model must
    be known by the compiler and optimizer developers, and it's transparent to the
    rest of the programmers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Java was the first programming language that defined its memory model. The
    original memory model defined in the JVM had some issues, and it was redefined
    in Java 5\. That memory model is the same in Java 8\. It''s defined in JSR 133\.
    Basically, the Java Memory Model defines the following:'
  prefs: []
  type: TYPE_NORMAL
- en: It defines the behavior of the volatile, synchronized, and final keywords.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It ensures that a properly synchronized concurrent program runs correctly on
    all architectures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It creates a partial ordering of the **volatile read**, **volatile write**,
    **lock**, and **unlock** instructions denominated as **happens-before**. Task
    synchronization helps us establish the happens-before relations too. If one action
    happens-before another, then the first is visible to and ordered before the second.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When a task acquires a monitor, the memory cache is invalidated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When a task releases a monitor, the cache data is flushed into the main memory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It's transparent for Java programmers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main objective of the Java memory model is that the properly written concurrent
    application will behave correctly on every **Java Virtual Machine** (**JVM**)
    regardless of operating system, CPU architecture, and the number of CPUs and cores.
  prefs: []
  type: TYPE_NORMAL
- en: Tips and tricks to design concurrent algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we have compiled some tips and tricks you have to keep in mind
    to design good concurrent applications.
  prefs: []
  type: TYPE_NORMAL
- en: Identify the correct independent tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can only execute concurrent tasks that are independent of each other. If
    you have two or more tasks with an order dependency between them, maybe you have
    no interest in trying to execute them concurrently and including a synchronization
    mechanism to guarantee the execution order. The tasks will execute in a sequential
    way, and you will have to overcome the synchronization mechanism. A different
    situation is when you have a task with some prerequisites, but these prerequisites
    are independent of each other. In this case, you can execute the prerequisites
    concurrently and then use a synchronization class to control the execution of
    the task after completion of all the prerequisites.
  prefs: []
  type: TYPE_NORMAL
- en: Another situation where you can't use concurrency is when you have a loop, and
    all the steps use data generated in the step before or there is some status information
    that goes from one step to the next step.
  prefs: []
  type: TYPE_NORMAL
- en: Implement concurrency at the highest possible level
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Rich threading APIs, as the Java concurrency API, offer you different classes
    to implement concurrency in your applications. In the case of Java, you can control
    the creation and synchronization of threads using the `Thread` or `Lock` classes,
    but it also offers you high-level concurrency objects, such as executors or the
    Fork/Join framework that allows you to execute concurrent tasks. This high-level
    mechanism offers you the following benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: You don't have to worry about the creation and management of threads. You only
    create tasks and send them for execution. The Java concurrency API controls the
    creation and management of threads for you.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are optimized to give better performance than using threads directly. For
    example, they use a pool of threads to reuse them and avoid thread creation for
    every task. You can implement these mechanisms from scratch, but it will take
    you a lot of time, and it will be a complex task.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They include advanced features that make the API more powerful. For example,
    with executors in Java, you can execute tasks that return a result in the form
    of a `Future` object. Again, you can implement these mechanisms from scratch,
    but it's not advisable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your application will be migrated easier from one operating system to another,
    and it will be more scalable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your application might become faster in the future Java versions. Java developers
    constantly improve the internals, and JVM optimizations will be likely more tailored
    for JDK APIs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, for performance and development time reasons, analyze the high-level
    mechanisms your thread API offers you before implementing your concurrent algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Take scalability into account
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the main objectives when you implement a concurrent algorithm is to take
    advantage of all the resources of your computer, especially the number of processors
    or cores. But this number may change over time. Hardware is constantly evolving
    and its cost becomes lower each year.
  prefs: []
  type: TYPE_NORMAL
- en: When you design a concurrent algorithm using data decomposition, don't presuppose
    the number of cores or processors that your application will execute on. Get the
    information about the system dynamically (for example, in Java you can get it
    with the method `Runtime.getRuntime().availableProcessors()`) and make your algorithm
    use that information to calculate the number of tasks it's going to execute. This
    process will have an overhead over the execution time of your algorithm, but your
    algorithm will be more scalable.
  prefs: []
  type: TYPE_NORMAL
- en: If you design a concurrent algorithm using task decomposition, the situation
    can be more difficult. You depend on the number of independent tasks you have
    in the algorithm and forcing a bigger number of tasks will increment the overhead
    introduced by synchronization mechanisms, and the global performance of the application
    can be even worse. Analyze in detail the algorithm to determine whether you can
    have a dynamic number of tasks or not.
  prefs: []
  type: TYPE_NORMAL
- en: Use thread-safe APIs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you need to use a Java library in a concurrent application, read its documentation
    first to know if it''s thread-safe or not. If it''s thread-safe, you can use it
    in your application without any problem. If it''s not, you have the following
    two options:'
  prefs: []
  type: TYPE_NORMAL
- en: If a thread-safe alternative exists, you should use it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a thread-safe alternative doesn't exist, you should add the necessary synchronization
    to avoid all possible problematic situations, especially data race conditions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, if you need a List in a concurrent application, you should not
    use the `ArrayList` class if you are going to update it from several threads because
    it's not thread-safe. In this case, you can use a thread-safe class such as `ConcurrentLinkedDeque,CopyOnWriteArrayList`,
    or `LinkedBlockingDeque`. If the class you want to use is not thread-safe, first
    you must look for a thread-safe alternative. Probably, it will be more optimized
    to work with concurrency that any alternative that you can implement.
  prefs: []
  type: TYPE_NORMAL
- en: Never assume an execution order
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The execution of tasks in a concurrent application when you don't use any synchronization
    mechanism is nondeterministic. The order in which the tasks are executed and the
    time each task is in execution before the processor moves on to another task is
    determined by the scheduler of the operating system. It doesn't care if you observe
    that the execution order is the same in a number of executions. The next one could
    be different.
  prefs: []
  type: TYPE_NORMAL
- en: The result of this assumption used to be a data race problem. The final result
    of your algorithm depends on the execution order of the tasks. Sometimes, the
    result can be right, but at other times it can be wrong. It can be very difficult
    to detect the cause of data race conditions, so you must be careful not to forget
    all the necessary synchronization elements.
  prefs: []
  type: TYPE_NORMAL
