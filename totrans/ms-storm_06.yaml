- en: Storm Scheduler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we covered the basics of Storm, the installation of
    Storm, the development and deployment of Storm, and the Trident topology in Storm
    clusters. In this chapter, we are focusing on Storm schedulers.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to cover the following points:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Storm schedulers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Default scheduler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Isolation scheduler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resource-aware scheduler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customer-aware scheduler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to Storm scheduler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned in the first two chapters, the Nimbus is responsible for deploying
    the topology and the supervisor is responsible for performing the computation
    tasks as defined in a Storm topology's spouts and bolts components. As we have
    shown, we can configure the number of worker slots for each supervisor node that
    are assigned to a topology as per the scheduler policy, as well as the number
    of workers allocated to a topology. In short, Storm schedulers help the Nimbus
    to decide the worker distribution of any given topology.
  prefs: []
  type: TYPE_NORMAL
- en: Default scheduler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Storm default scheduler assigns component executors as evenly as possible
    between all the workers (supervisor slots) assigned to a given topology.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider a sample topology with one spout and one bolt, with both components
    having two executors. The following diagram shows the assignment of executors
    if we have submitted the topology by allocating two workers (supervisor slots):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00041.gif)'
  prefs: []
  type: TYPE_IMG
- en: As shown in the preceding diagram, each worker node contains one executor for
    a spout and one executor for a bolt. The even distribution of executors between
    workers is only possible if the number of executors in each component is divisible
    by the number of workers assigned to a topology.
  prefs: []
  type: TYPE_NORMAL
- en: Isolation scheduler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The isolation scheduler provides a mechanism for the easy and safe sharing of
    Storm cluster resources among many topologies. The isolation scheduler helps to
    allocate/reserve the dedicated sets of Storm nodes for topologies within the Storm
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to define the following property in the Nimbus configuration file to
    switch to the isolation scheduler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We can allocate/reserve the resources for any topology by specifying the topology
    name and the number of nodes inside the `isolation.scheduler.machines` property,
    as mentioned in the following section. We need to define the `isolation.scheduler.machines`
    property in the Nimbus configuration, as Nimbus is responsible for the distribution
    of topology workers between Storm nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding configuration, two nodes are assigned to `Topology-Test1`,
    one node is assigned to `Topology-Test2`, and four nodes are assigned to `Topology-Test3`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the key points of the isolation scheduler:'
  prefs: []
  type: TYPE_NORMAL
- en: The topologies mentioned in the isolation list are given priority over non-isolation
    topologies, which means that resources will be allocated to isolation topologies
    first if there's competition with non-isolation topologies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no way to change the isolation setting of topologies during runtime
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The isolation topology solves the multitenancy problem by assigning dedicated
    machines to topologies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resource-aware scheduler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A resource-aware scheduler helps users specify the amount of resources required
    for a single instance of any component (spout or bolt). We can enable the resource-aware
    scheduler by specifying the following property in the `storm.yaml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Component-level configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can allocate the memory requirement to any component. Here are the methods
    available to allocate the memory to a single instance of any component:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, you can use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the definition of each argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '`onHeap`: The amount of on heap space an instance of this component will consume
    in megabytes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`offHeap`: The amount of off heap space an instance of this component will
    consume in megabytes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data type of both `onHeap` and `offHeap` is `Number`, and the default value
    is `0.0`.
  prefs: []
  type: TYPE_NORMAL
- en: Memory usage example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s consider a topology that has two components--one spout and one bolt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The memory request for a single instance of the `spout1` component is 1.5 GB
    (1 GB on heap and 0.5 GB off heap), which means that the total memory request
    for the `spout1` component is 4 x 1.5 GB = 6 GB.
  prefs: []
  type: TYPE_NORMAL
- en: 'The memory request for a single instance of the `bolt1` component is 0.5 GB
    (0.5 GB on heap and 0.0 GB off heap), which means that the total memory request
    for the `bolt1` component is 5 x 0.5 GB = 2.5 GB. The method of calculating the
    total memory required for both components can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Total memory allocated to topology = spout1 + bolt1 = 6 + 2.5 = 8.5 GB*'
  prefs: []
  type: TYPE_NORMAL
- en: You can also allocate the CPU requirement to any component.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the method required to allocate the amount of CPU resources to a single
    instance of any component:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `amount` is the amount of CPU resources an instance of any given component
    will consume. CPU usage is a difficult concept to define. Different CPU architectures
    perform differently depending on the task at hand. By convention, a CPU core will
    typically have 100 points. If you feel that your processors are more or less powerful,
    you can adjust this accordingly. Heavy tasks that are CPU-bound will get 100 points,
    as they can consume an entire core. Medium tasks should get 50, light tasks 25,
    and tiny tasks 10.
  prefs: []
  type: TYPE_NORMAL
- en: CPU  usage example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s consider a topology that has two components--one spout and one bolt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Worker-level configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can allocate the heap size per worker/slot. Here is the method required
    to define the heap size of each worker node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Here, `size` is the amount of heap space available to a single worker in megabytes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Node-level configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can configure the amount of memory and CPU a Storm node can use by setting
    the following properties in the `storm.yaml` file. We need to set the following
    properties on each Storm node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Here, `100` means an entire core, as discussed previously.
  prefs: []
  type: TYPE_NORMAL
- en: Global component configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As mentioned in the previous section, we can define the memory and CPU requirements
    for each component by defining the topology. The user can also set the default
    resource usage of components in the `storm.yaml` file. If we are defining the
    component configuration in the code, then the code value will overwrite the default
    one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Custom scheduler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Storm, Nimbus uses a scheduler to assign tasks to the supervisors. The default
    scheduler aims to allocate computing resources evenly to topologies. It works
    well in terms of fairness among topologies, but it is impossible for users to
    predict the placement of topology components in the Storm cluster, regarding which
    component of a topology needs to be assigned to which supervisor node.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider an example. Say that we have a topology that has one spout
    and two bolts, and each of the components has one executor and one task. The following
    diagram shows the distribution of the topology if we submit the topology to a
    Storm cluster. Assume that the number of workers assigned to the topology is three
    and the number of supervisors in the Storm cluster is three:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00042.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s assume that the last bolt in our topology, **Bolt2**, needs to process
    some data using a GPU rather than the CPU, and there''s only one of the supervisors
    with a GPU. We need to write our own custom scheduler to achieve the assignment
    of any component to a specific supervisor node. Here are the steps we need to
    perform to achieve this:'
  prefs: []
  type: TYPE_NORMAL
- en: Configure changes in the supervisor node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configure settings at the component level.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a custom scheduler class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Register the customer scheduler class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configuration changes in the supervisor node
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Storm offers a field in the supervisor''s configuration for users to specify
    custom scheduling metadata. In this case, we type `/tag` in the supervisors, along
    with the type they are running on, which we do with a single line of config in
    their `$STORM_HOME/conf/storm.yaml` file. For example, each supervisor node should
    have the following in its config:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We need to restart the supervisor node after adding the configuration changes
    to each supervisor node. You need to use the CPU type for all non-GPU machines.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration setting at component level
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This step is done when building the topology with `TopologyBuilder` in the
    main method of a topology. `ComponentConfigurationDeclarer` has a method called
    `addConfiguration(String config, String value)` that allows adding of custom configuration--that
    is, metadata. In our case, we add the type information using this method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code shows that we have typed our `bolt2` component with `type`
    as `GPU`.
  prefs: []
  type: TYPE_NORMAL
- en: Writing a custom supervisor class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can write our `CustomScheduler` class by implementing the `org.apache.storm.scheduler.IScheduler`
    interface. The interface contains two important methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`prepare(Map conf)`: This method only initializes the scheduler'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`schedule(Topologies topologies, Cluster cluster)`: This method contains logic
    that is responsible for topology workers in the cluster supervisor slots'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CustomScheduler` contains the following private method, which is responsible
    for assigning workers to the cluster supervisor slots.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `getSupervisorsByType()` method returns the map. The key of the map represents
    the node type (for example, CPU or GPU) and the value contains the list of supervisor
    nodes of that type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The `populateComponentsByType()` method also returns the map. The key of the
    map represents the type (CPU or GPU) and the value contains the list of components
    of the topology that needs to be assigned to that type of supervisor node. We
    use an untyped type here to group components with no types. The purpose of this
    is to effectively handle these untyped components in the same way that the default
    scheduler performs its assigning. That means that a topology with no typed components
    will be successfully scheduled in the same way, with no issues, across untyped
    supervisors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The `populateComponentsByTypeWithStormInternals()` method returns the details
    of the internal components that Storm launches to a topology''s data flow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The first three methods manage the maps of the supervisors and components.
    Now, we will write the `typeAwareScheduler()` method, which will use both the
    maps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Apart from the preceding four methods, we are also using more methods that do
    the following things.
  prefs: []
  type: TYPE_NORMAL
- en: Converting component IDs to executors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now let's make the jump from component IDs to actual executors, as that's the
    level at which the Storm cluster deals with assignments.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process is quite straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: Get a map of executors by component from the cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check which components' executors need scheduling, according to the cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Create a map of the types to the executors, populating only those executors
    that are awaiting scheduling:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Converting supervisors to slots
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'And now for the final conversion we have to perform: jumping from supervisors
    down to slots. As before with components and their executors, we need this because
    the cluster assigns executors at the slot level, not the supervisor level.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few things to do at this point; we have broken the process down
    into smaller methods to preserve readability. The main steps we need to perform
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Find out which slots we can assign to, given a list of supervisors for a type.
    This is simply the case of using a for loop that collects all supervisors' slots,
    and then returning as many of the slots as are requested by the topology.
  prefs: []
  type: TYPE_NORMAL
- en: Divide the executors awaiting scheduling for the type into even groups across
    the slots.
  prefs: []
  type: TYPE_NORMAL
- en: Populate a map with entries in the slot to the executors.
  prefs: []
  type: TYPE_NORMAL
- en: The idea here is to call the `populateComponentExecutorsToSlotsMap` method once
    per type, which results in a single map holding all the assignments we need to
    perform.
  prefs: []
  type: TYPE_NORMAL
- en: 'As explained in the code''s comments, we previously found that sometimes we
    would eagerly assign a type''s executors to a slot, only to have a successive
    type fail to assign its executors, leading to partial scheduling. We have since
    made sure that the flow of scheduling ensures that no partial scheduling is ever
    performed (either all are scheduled or none are), at the cost of an extra for
    loop, as we believe that''s a cleaner state for a topology to be in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Registering a CustomScheduler class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We need to create a JAR for the `CustomScheduler` class, and place it in `$STORM_HOME/lib/`,
    and tell Nimbus to use the new scheduler by appending the following lines to the
    configuration file at `$STORM_HOME/conf/storm.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Restart the Nimbus daemon to reflect the changes to the configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, if we deploy the same topology shown in the previous diagram, then the
    distribution of executors will look like this (**Bolt2** is assigned to a GPU-typed
    supervisor):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00043.gif)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about the built-in Storm scheduler and also covered
    how we can write and configure a custom scheduler.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will be covering the monitoring of a Storm cluster using
    Graphite and Ganglia.
  prefs: []
  type: TYPE_NORMAL
