- en: Curse of High-Dimensionality in Big Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Two methods of ingesting and preparing a CSV file for processing in Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Singular Value Decomposition** (**SVD**) to reduce high-dimensionality in
    Spark'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Principal Component Analysis** (**PCA**) to pick the most effective latent
    factor for machine learning in Spark'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The curse of dimensionality is not a new term or concept. The term was originally
    coined by R. Bellman when tackling problems in dynamic programming (the Bellman
    equation). The core concepts in machine learning refer to the problem that as
    we increase the number of dimensions (axes or features), the number of training
    data (samples) remains the same (or relatively low), which causes less accuracy
    in our predictions. This phenomenon is also referred to as the *Hughes Effect*,
    named after G. Hughes, which talks about the problem caused by rapid (exponential)
    increase of search space as we introduce more and more dimensions to the problem
    space. It is a bit counterintuitive, but if the number of samples does not expand
    at the same rate as you add more dimensions, you actually end up with a less accurate
    model!
  prefs: []
  type: TYPE_NORMAL
- en: In a nutshell, most machine learning algorithms are statistical by nature and
    they attempt to learn the properties of the target space by cutting up the space
    during training and by doing some sort of counting for the number of each classes
    in each subspace. The curse of dimensionality is caused by having fewer and fewer
    data samples, which can help the algorithm to discriminate and learn as we add
    more dimensions. Generally speaking, if we have *N* samples in a dense *D* dimension,
    then we need *(N)^D* samples to keep the sample density constant.
  prefs: []
  type: TYPE_NORMAL
- en: For example, let us say that you have 10 patient datasets that are measured
    along two dimensions (height, weight). This results in 10 data points in a two-dimensional
    plane. What happens if we start introducing other dimensions such as region, calorie
    intake, ethnicity, income, and so on? In this case, we still have 10 observation
    points (10 patients) but in a much larger space of six dimensions. This inability
    for the sample data (needed for training) to expand exponentially as new dimensions
    are introduced is called the **Curse of Dimensionality**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at a graphical example to show the growth of the search space versus
    data samples. The following figure depicts a set of five data points that are
    being measured in 5 x 5 (25 cells). What happens to the prediction accuracy as
    we add another dimension? We still have five data points in 125 3D-cells, which
    results in a lot of sparse subspace that cannot help the ML algorithm to learn
    better (discriminate) so it results in less accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00254.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Our goal should be to strive toward a near-optimal number of features or dimensions
    rather than adding more and more features (maximum features or dimensions). After
    all, shouldn't we have a better classification error if we just add more and more
    features or dimensions? It seems like a good idea at first, but the answer in
    most cases is "no" unless you can increase the samples exponentially, which is
    neither practical nor possible in almost all cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us take a look at the following figure, which depicts learning error versus
    total number of features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00255.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the previous section, we examined the core concept beyond the curse of dimensionality,
    but we have not talked about its other side effects or how to deal with the curse
    itself. As we have seen previously, contrary to popular belief, it is not the
    dimensions themselves, but the reduction of the ratio of samples to search space
    which subsequently results in a less accurate forecast.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine a simple ML system, as shown in the following figure. The ML system
    shown here takes the MNIST ([http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/))
    type handwriting dataset and wants to train itself so it can predict what six-digit
    zip code is used on a parcel:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00256.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: MNIST'
  prefs: []
  type: TYPE_NORMAL
- en: 'Even though the MNIST data is 20 x 20, to make the problem more visible let''s
    assume we have a 40 x 40 pixel patch for each digit that has to be stored, analyzed,
    and then used for future prediction. If we assume black/white, then the *apparent*
    dimensionality is two (40 x 40) or 21,600, which is large. The next question that
    should be asked is: given the 21,600 apparent dimensions for the data, what is
    the actual dimension that we need to do our work? If we look at all the possible
    samples drawn from a 40 x 40 patch, how many of them actually look for digits?
    Once we look at the problem a bit more carefully, we will see that the "actual"
    dimensions (that is, limited to a smaller manifold subspace which is the space
    used by a pen stroke to make the digits. In practice, the actual subspace is much
    smaller and not randomly distributed across the 40 x 40 patch) are actually a
    lot smaller! What is happening here is that the actual data (the digits drawn
    by humans) exists in much smaller dimensions and most likely is confined to a
    small set of manifolds in the subspace (that is, the data lives around a certain
    subspace). To understand this better, draw 1,000 random samples from a 40 x 40
    patch and visually inspect the samples. How many of them actually look alike a
    3, 6, or a 5?'
  prefs: []
  type: TYPE_NORMAL
- en: 'When we add dimensions, we can unintentionally increase the error rate by introducing
    noise to the system due to the fact that there would not be enough samples to
    predict accurately or simply act if the measurement introduces noise by itself.
    Common problems with adding more dimensions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Longer compute time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increased noise
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More samples needed to keep the same learning/prediction rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overfitting of the data due to lack of actionable samples in sparse space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A pictorial presentation can help us understand the difference between *apparent
    dimensions* versus *actual dimensions* and why *less is more* in this case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00257.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The reasons we want to reduce dimensions can be expressed as the ability to:'
  prefs: []
  type: TYPE_NORMAL
- en: Visualize data better
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compress the data and reduce storage requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increase signal to noise ratio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Achieve faster running time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature selection versus feature extraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have two options, feature selection and feature extraction, at our disposal
    for reducing the dimensions to a more manageable space. Each of these techniques
    is a distinct discipline and has its own methods and complexity. Even though they
    sound the same, they are very different and require a separate treatment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure provides a mind map, which compares the feature selection
    versus feature extraction for reference. While the feature selection, also referred
    to as feature engineering, is beyond the scope of this book, we cover the two
    most common feature extraction techniques (PCA and SVD) via detailed recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00258.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'The two techniques available for picking a set of features or inputs to a ML
    algorithm are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature selection**: In this technique, we use our domain knowledge to select
    a subset of features that best describes the variance in the data. What we are
    trying to do is to select the best dependent variables (features) that can help
    us predict the outcome. This method is often referred to as "feature engineering"
    and requires a data engineer or domain expertise to be effective.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, we might look at 200 independent variables (dimensions, features)
    that are proposed for a logistics classifier to predict whether a house would
    sell or not in the city of Chicago. After talking to real-estate experts with
    20+ years of experience in buying/selling houses in the Chicago market, we found
    out that only 4 of the 200 initially proposed dimensions, such as number of bedrooms,
    price, total square foot area, and quality of schools, are adequate for predictions.
    While this is great, it is usually very expensive, time-consuming, and requires
    a domain expert to analyze and provide direction.
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature extraction**: This refers to a more algorithmic approach that uses
    a mapping function to map high-dimensional data to a lower-dimensional space.
    For example, mapping a three-dimensional space (for example, height, weight, eye
    color) to a one-dimensional space (for example, latent factors) that can capture
    almost all variances in the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What we are trying to do here is to come up with a set of latent factors that
    are a combination (usually linear) of the original factor but can capture and
    explain the data in an accurate way. For example, we use words to describe documents
    which usually end in 10⁶ to 10⁹ space, but wouldn't it be nice to describe the
    documents by topics (for example, romance, war, peace, science, art, and so on)
    that are more abstract and high level? Do we really need to look at or include
    every word to do a better job with text analytics? At what cost?
  prefs: []
  type: TYPE_NORMAL
- en: Feature extraction is about an algorithmic approach to dimensionality reduction
    which itself is a proxy for mapping from "apparent dimensionality" to "actual
    dimensionality".
  prefs: []
  type: TYPE_NORMAL
- en: Two methods of ingesting and preparing a CSV file for processing in Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore reading, parsing, and preparing a CSV file for a
    typical ML program. A **comma-separated values** (**CSV**) file normally stores
    tabular data (numbers and text) in a plain text file. In a typical CSV file, each
    row is a data record, and most of the time, the first row is also called the header
    row, which stores the field's identifier (more commonly referred to as a column
    name for the field). Each record consists of one or more fields, separated by
    commas.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The sample CSV data file is from movie ratings. The file can be retrieved at [http://files.grouplens.org/datasets/movielens/ml-latest-small.zip](http://files.grouplens.org/datasets/movielens/ml-latest-small.zip).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once the file is extracted, we will use the `ratings.csv` file for our CSV
    program to load the data into Spark. The CSV files will look like the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '| **userId** | **movieId** | **rating** | **timestamp** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 16 | 4 | 1217897793 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 24 | 1.5 | 1217895807 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 32 | 4 | 1217896246 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 47 | 4 | 1217896556 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 50 | 4 | 1217896523 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 110 | 4 | 1217896150 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 150 | 3 | 1217895940 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 161 | 4 | 1217897864 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 165 | 3 | 1217897135 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 204 | 0.5 | 1217895786 |'
  prefs: []
  type: TYPE_TB
- en: '| ... | ... | ... | ... |'
  prefs: []
  type: TYPE_TB
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`package spark.ml.cookbook.chapter11`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary packages for Spark to get access to the cluster and `Log4j.Logger`
    to reduce the amount of output produced by Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Spark''s configuration and Spark session so we can have access to the
    cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We read in the CSV files as a text file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We process the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: It should be mentioned that the `split` function here is for demonstration purposes
    only and a more robust tokenizer technique should be used in production.
  prefs: []
  type: TYPE_NORMAL
- en: First, we trim the line, remove any empty spaces, and load the CSV file into
    the `headerAndData` RDD since `ratings.csv` does have a header row.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then read the first row as the header, and read the rest of the data into
    the data RDD. Any further computing could use the data RDD to perform the machine
    learning algorithm. For demo purposes, we mapped the header row to the data RDD
    and printed out the first 10 rows.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the application console, you will see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: There is also another option to load the CSV file into Spark with the help of
    the Spark-CSV package.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To utilize this feature, you will need to download the following JAR file and
    place them on the classpath: [http://repo1.maven.org/maven2/com/databricks/spark-csv_2.10/1.4.0/spark-csv_2.10-1.4.0.jar](http://repo1.maven.org/maven2/com/databricks/spark-csv_2.10/1.4.0/spark-csv_2.10-1.4.0.jar)'
  prefs: []
  type: TYPE_NORMAL
- en: Since the Spark-CSV package is also dependent on `common-csv`, you will need
    to get the `common-csv` JAR file from the following location: [https://commons.apache.org/proper/commons-csv/download_csv.cgi](https://commons.apache.org/proper/commons-csv/download_csv.cgi)
  prefs: []
  type: TYPE_NORMAL
- en: We get the `common-csv-1.4-bin.zip` and extract the `commons-csv-1.4.jar` out,
    and put the preceding two jars on the classpath.
  prefs: []
  type: TYPE_NORMAL
- en: 'We load the CSV file using the Databricks `spark-csv` package with the following
    code. It will create a DataFrame object after successfully loading the CSV file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We register a temp in-memory view named `ratings` from the DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We then use a SQL query against the table and display 10 rows. In the console,
    you will see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00259.gif)'
  prefs: []
  type: TYPE_IMG
- en: Further machine learning algorithms could be performed on the DataFrame that
    was created previously.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We then close the program by stopping the Spark session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the older version of Spark, we needed to use a special package to read in
    CSV, but we now can take advantage of `spark.sparkContext.textFile(dataFile)`
    to ingest the file. The `Spark` which starts the statement is the Spark session
    (handle to cluster) and can be named anything you like via the creation phase,
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Spark 2.0+ uses `spark.sql.warehouse.dir` to set the warehouse location to store
    tables rather than `hive.metastore.warehouse.dir`. The default value for `spark.sql.warehouse.dir`
    is `System.getProperty("user.dir")`.
  prefs: []
  type: TYPE_NORMAL
- en: Also see `spark-defaults.conf` for more details.
  prefs: []
  type: TYPE_NORMAL
- en: 'Going forward, we prefer this method as opposed to obtaining the special package
    and the dependent JAR, as explained in step 9 of this recipe, followed by step
    10:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This demonstrates how to consume the file.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The CSV file format has a lot of variations. The basic idea of separating fields
    with a comma is clear, but it could also be a tab, or other special character.
    Sometimes even the header row is optional.
  prefs: []
  type: TYPE_NORMAL
- en: A CSV file is widely used to store raw data due to its portability and simplicity.
    It's portable across different applications. We will introduce two simple and
    typical ways to load a sample CSV file into Spark, and it can be easily modified
    to fit your use case.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For more information regarding the Spark-CSV package, visit [https://github.com/databricks/spark-csv](https://github.com/databricks/spark-csv)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singular Value Decomposition (SVD) to reduce high-dimensionality in Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will explore a dimensionality reduction method straight out
    of the linear algebra, which is called **SVD** (**Singular Value Decomposition**).
    The key focus here is to come up with a set of low-rank matrices (typically three)
    that approximates the original matrix but with much less data, rather than choosing
    to work with a large *M* by *N* matrix.
  prefs: []
  type: TYPE_NORMAL
- en: SVD is a simple linear algebra technique that transforms the original data to
    eigenvector/eigenvalue low rank matrices that can capture most of the attributes
    (the original dimensions) in a much more efficient low rank matrix system.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure depicts how SVD can be used to reduce dimensions and then
    use the S matrix to keep or eliminate higher-level concepts derived from the original
    data (that is, a low rank matrix with fewer columns/features than the original):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00260.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use the movie rating data for the SVD analysis. The movieLens 1M dataset
    contains around 1 million records which consist of anonymous ratings of around
    3,900 movies made by 6,000 movieLens users.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The dataset can be retrieved at: [http://files.grouplens.org/datasets/movielens/ml-1m.zip](http://files.grouplens.org/datasets/movielens/ml-1m.zip)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset contains the following files:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ratings.dat`: Contains the user ID, movie ID, ratings, and timestamp'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`movies.dat`: Contains the movie ID, titles, and genres'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`users.dat`: Contains the user ID, genders, ages, occupations, and zip code'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will use the `ratings.dat` for our SVD analysis. Sample data for the `ratings.dat`
    looks like the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use the following program to convert the data into a ratings matrix
    and fit it into the SVD algorithm model (in this case, we have 3,953 columns in
    total):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Movie 1** | **Movie 2** | **Movie ...** | **Movie 3953** |'
  prefs: []
  type: TYPE_TB
- en: '| user 1 | 1 | 4 | - | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| user 2 | 5 | - | 2 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| user ... | - | 3 | - | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| user N | 2 | 4 | - | 5 |'
  prefs: []
  type: TYPE_TB
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`package spark.ml.cookbook.chapter11`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary packages for the Spark session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Spark''s configuration and Spark session so we can have access to the
    cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We read in the original raw data file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We preprocess the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Since we are more interested in the ratings, we extract the `userId`, `movieId`,
    and rating values from the data file, `fields(0)`, `fields(1)`, and `fields(2)`,
    and create a ratings RDD based on the records.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then find out how many movies are available in the ratings data and calculate
    the max movie index:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In total, we get 3,953 movies based on the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'We put all the user movie item ratings together, using RDD''s `groupByKey`
    function, so a single user''s movie ratings are grouped together:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We then print out the top two records to see the collection. Since we might
    have a large dataset, we cache RDD to improve performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the console, you will see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding records, the user ID is `4904`. For the movie ID `2054`, the
    rating is `4.0`, movie ID is `588`, rating is `4`, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then create a sparse vector to host the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We then convert the data into a more useful format. We use the `userID` as the
    key (sorted), and create a sparse vector to host the movie rating data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the console, you will see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding printout, for user `1`, in total, there are `3,953` movies.
    For movie ID `1`, the rating is `5.0`. The sparse vector contains a `movieID`
    array together with a rating value array.
  prefs: []
  type: TYPE_NORMAL
- en: 'We just need the rating matrix for our SVD analysis:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code will get the sparse vector part out and create a row RDD.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then create a RowMatrix based on the RDD. Once the RowMatrix object is created,
    we can call Spark''s `computeSVD` function to compute the SVD out of the matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The preceding parameters could also be adjusted to fit our needs. Once we have
    the SVD computed, we can get the model data out.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We print the singular values out:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see the following output on the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00261.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'From the Spark Master (`http://localhost:4040/jobs/`), you should see the tracking
    as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00262.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We then close the program by stopping the Spark session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The core of the work gets done by declaring a `RowMatrix()` and then invoking
    the `computeSVD()` method to decompose the matrix into subcomponents that are
    much smaller, but approximate the original with uncanny accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'SVD is a factorization technique for a real or complex matrix. At its core,
    it is a straight linear algebra which was actually derived from PCA itself. The
    concept is used extensively in recommender systems (ALS, SVD), topic modeling
    (LDA), and text analytics, to derive concepts from primitive high-dimensional
    matrices. Let''s try to outline this without getting into the mathematical details
    of what goes in and what comes out in an SVD decomposition. The following figure
    depicts how this dimensionality reduction recipe and its dataset (`MovieLens`)
    relate to an SVD decomposition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00263.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will end up with much more efficient (low-ranked) matrices for computation
    based on the original dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The following equation depicts the decomposition of an array of *m x n*, which
    is large and hard to work with. The right-hand side of the equation helps to solve
    the decomposition problem which is the basis of the SVD technique.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00264.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following steps provides a concrete example of the SVD decomposition step
    by step:'
  prefs: []
  type: TYPE_NORMAL
- en: Consider a matrix of 1,000 x 1,000 which provides 1,000,000 data points (M=
    users, N = Movies).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assume there are 1,000 rows (number of observations) and 1,000 columns (number
    of movies).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's assume we use Spark's SVD method to decompose A into three new matrices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matrix `U [m x r]` has 1,000 rows, but only 5 columns now (`r=5`; `r` can be
    thought of as concepts)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matrix `S [ r x r ]` holds the singular values, which are the strength of each
    concept (only interested in diagonals)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matrix `V [n x r ]` has the right singular value vectors (`n= Movies`, `r =
    concepts`, such as romance, sci-fi, and so on)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's assume that after decomposition, we end up with five concepts (romantic,
    sci-fi-drama, foreign, documentary, and adventure)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How did the low rank help?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Originally we had 1,000,000 points of interest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After SVD and even before we started selecting what we want to keep using singular
    values (diagonals of matrix S), we ended up with total points of interest = membership
    in U (1,000 x 5) + S (5 x 5) + V(1,000 x 5)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rather than working with 1 million data points (matrix A, which is 1,000 x 1,000),
    we now have 5,000+25+5,000, which is about 10,000 data points, which is considerably
    less
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The act of selecting singular values allows us to decide how much we want to
    keep and how much do we want to throw away (do you really want to show the user
    the lowest 900 movie recommendations--does it have any value?)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for RowMatrix can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.distributed.RowMatrix](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.distributed.RowMatrix)
    and [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.SingularValueDecomposition](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.SingularValueDecomposition)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Principal Component Analysis (PCA) to pick the most effective latent factor
    for machine learning in Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we use **PCA** (**Principal Component Analysis**) to map the
    higher-dimension data (the apparent dimensions) to a lower-dimensional space (actual
    dimensions). It is hard to believe, but PCA has its root as early as 1901(see
    K. Pearson's writings) and again independently in the 1930s by H. Hotelling.
  prefs: []
  type: TYPE_NORMAL
- en: PCA attempts to pick new components in a manner that maximizes the variance
    along perpendicular axes and effectively transforms high-dimensional original
    features to a lower-dimensional space with derived components that can explain
    the variation (discriminate classes) in a more concise form.
  prefs: []
  type: TYPE_NORMAL
- en: 'The intuition beyond PCA is depicted in the following figure. Let''s assume
    for now that our data has two dimensions (x, y) and the question we are going
    to ask the data is whether most of the variation (and discrimination) can be explained
    by only one dimension or more precisely with a linear combination of original
    features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00265.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Cleveland Heart Disease database is a published dataset used by ML researchers.
    The dataset contains more than a dozen fields, and experiments with the Cleveland
    database have concentrated on simply attempting to distinguish presence (value
    1,2,3) and absence (value 0) of the disease (in the goal column, 14th column).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Cleveland Heart Disease dataset is available at [http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data](http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The dataset contains the following attributes (age, sex, cp, trestbps, chol,
    fbs, restecg, thalach, exang, oldpeak, slope, ca, thal, num) that are depicted
    as the header  of the table below:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For a detailed explanation on the individual attributes, refer to: [http://archive.ics.uci.edu/ml/datasets/Heart+Disease](http://archive.ics.uci.edu/ml/datasets/Heart+Disease)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset will look like the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '| **age** | **sex** | **cp** | **trestbps** | **chol** | **fbs** | **restecg**
    | **thalach** | **exang** | **oldpeak** | **slope** | **ca** | **thal** | **num**
    |'
  prefs: []
  type: TYPE_TB
- en: '| 63 | 1 | 1 | 145 | 233 | 1 | 2 | 150 | 0 | 2.3 | 3 | 0 | 6 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 67 | 1 | 4 | 160 | 286 | 0 | 2 | 108 | 1 | 1.5 | 2 | 3 | 3 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 67 | 1 | 4 | 120 | 229 | 0 | 2 | 129 | 1 | 2.6 | 2 | 2 | 7 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 37 | 1 | 3 | 130 | 250 | 0 | 0 | 187 | 0 | 3.5 | 3 | 0 | 3 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 41 | 0 | 2 | 130 | 204 | 0 | 2 | 172 | 0 | 1.4 | 1 | 0 | 3 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 56 | 1 | 2 | 120 | 236 | 0 | 0 | 178 | 0 | 0.8 | 1 | 0 | 3 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 62 | 0 | 4 | 140 | 268 | 0 | 2 | 160 | 0 | 3.6 | 3 | 2 | 3 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| 57 | 0 | 4 | 120 | 354 | 0 | 0 | 163 | 1 | 0.6 | 1 | 0 | 3 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 63 | 1 | 4 | 130 | 254 | 0 | 2 | 147 | 0 | 1.4 | 2 | 1 | 7 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 53 | 1 | 4 | 140 | 203 | 1 | 2 | 155 | 1 | 3.1 | 3 | 0 | 7 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 57 | 1 | 4 | 140 | 192 | 0 | 0 | 148 | 0 | 0.4 | 2 | 0 | 6 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 56 | 0 | 2 | 140 | 294 | 0 | 2 | 153 | 0 | 1.3 | 2 | 0 | 3 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 56 | 1 | 3 | 130 | 256 | 1 | 2 | 142 | 1 | 0.6 | 2 | 1 | 6 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 44 | 1 | 2 | 120 | 263 | 0 | 0 | 173 | 0 | 0 | 1 | 0 | 7 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 52 | 1 | 3 | 172 | 199 | 1 | 0 | 162 | 0 | 0.5 | 1 | 0 | 7 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 57 | 1 | 3 | 150 | 168 | 0 | 0 | 174 | 0 | 1.6 | 1 | 0 | 3 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ...
    | ... |'
  prefs: []
  type: TYPE_TB
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`package spark.ml.cookbook.chapter11`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary packages for the Spark session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Spark''s configuration and Spark session so we can have access to the
    cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We read in the original raw data file and count the raw data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'In the console, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We pre-process the dataset (see the preceding code for details):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we filter the missing data record, and use Spark DenseVector
    to host the data. After filtering the missing data, we get the following count
    of data in the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The record print, `2`, will look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We create a DataFrame from the data RDD, and create a PCA object for computing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The parameters for the PCA model are shown in the preceding code. We set the
    `K` value to `4`. `K` represents the number of top K principal components that
    we are interested in after completing the dimensionality reduction algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'An alternative is also available via the Matrix API: `mat.computePrincipalComponents(4)`.
    In this case, the `4` represents the top K principal components after the dimensionality
    reduction is completed.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We use the transform function to do computing and show the result in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The following will be displayed on the console.
  prefs: []
  type: TYPE_NORMAL
- en: 'What you are seeing are the four new PCA components (PC1, PC2, PC3, and PC4),
    which can be substituted for the original 14 features. We have successfully mapped
    the high-dimensional space (14 dimensions) to a lower-dimensional space (four
    dimensions):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00266.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'From the Spark Master (`http://localhost:4040/jobs`), you can also track the
    job, as shown in the following figure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00267.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We then close the program by stopping the Spark session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After loading and processing the data, the core of the work for PCA is done
    via the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The `PCA()` call allows us to select how many components we need (`setK(4)`).
    In the case of this recipe, we selected the first four components.
  prefs: []
  type: TYPE_NORMAL
- en: The goal is to find a lower-dimension space (a reduced PCA space) from the original
    higher-dimension data while preserving the structural properties (variance of
    data along principal component axis) in such a way that allows for maximum discrimination
    of labeled data without the original high-dimensional space requirement.
  prefs: []
  type: TYPE_NORMAL
- en: 'A sample PCA chart is shown in the following figure. After dimension reduction,
    it will look like the following--in this case, we can easily see that most of
    the variance is explained by the first four principal components. If you quickly
    examine the graph (red line), you see how fast the variance disappears after the
    fourth component. This type of knee chart (variance versus number of components)
    helps us to quickly pick the number of components that are needed (in this case,
    four components) to explain most of the variance. To recap, almost all the variance
    (green line) can be cumulatively attributed to the first four components, since
    it reaches almost 1.0 while the amount of contribution from each individual component
    can be traced via the red line at the same time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00268.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The chart above is a depiction of ‘Kaiser Rule’ which is the most commonly used
    approach to selecting the number of components. To produce the chart, one can
    use R to plot eigenvalues against principal components or write your own using
    Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'See the following link from university of Missouri for plotting a chart in
    R:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://web.missouri.edu/~huangf/data/mvnotes/Documents/pca_in_r_2.html](http://web.missouri.edu/~huangf/data/mvnotes/Documents/pca_in_r_2.html).'
  prefs: []
  type: TYPE_NORMAL
- en: As stated, the chart relates to Kaiser rule which states the more correlation
    variables loaded in a particular principal component, the more important that
    factor is in summarizing the data. The eigenvalue in this case can be thought
    of as a sort of index that measures how good a component is summarizing the data
    (in direction of maximum variance).
  prefs: []
  type: TYPE_NORMAL
- en: Using PCA is similar to other methods in which we try to learn the distribution
    for the data. We still need the average of each attribute and K (the number of
    components to keep), which is simply an estimated covariance. In short, dimension
    reduction occurs because we are ignoring the directions (the PCA components) that
    have the least variance. Keep in mind that PCA can be difficult, but you are in
    control of what happens and how much you keep (use knee charts to select K or
    the number of components to keep).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two methods for calculating PCA:'
  prefs: []
  type: TYPE_NORMAL
- en: Covariance method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Singular Value Decomposition** (**SVD**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will outline the covariance matrix method (the straight eigenvector and eigenvalue
    plus centering) here, but feel free to refer to the SVD recipe (*Singular Value
    Decomposition (SVD) to reduce high-dimensionality in Spark*) for the inner workings
    of SVD as it relates to PCA.
  prefs: []
  type: TYPE_NORMAL
- en: 'The PCA algorithm using the covariance matrix method, in a nutshell, involves
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a matrix of N by M:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: N = total number of training data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: M is a particular dimension (or feature)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Intersection of M x N is a call with the sample value
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compute the mean:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00269.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Center (normalize) the data by subtracting the average from each observation:![](img/00270.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Construct the covariance matrix:![](img/00271.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute eigenvectors and eigenvalues of the covariance matrix (it's straightforward,
    but bear in mind that not all matrices can be decomposed).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pick the eigenvectors that have the largest eigenvalues.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The larger the eigenvalue, the more contribution to the variance of a component.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The net result of using PCA in this recipe is that the original search space
    of 14 dimensions (the same as saying14 features) is reduced to 4 dimensions that
    explain almost all the variations in the original dataset.
  prefs: []
  type: TYPE_NORMAL
- en: PCA is not purely a ML concept and has been in use in finance for many years
    prior to the ML movement. At its core, PCA uses an orthogonal transformation (each
    component is perpendicular to the other component) to map the original features
    (apparent dimensions) to a set of newly derived dimensions so that most of the
    redundant and co-linear attributes are removed. The derived (actual latent dimension)
    components are linear combinations of the original attributes.
  prefs: []
  type: TYPE_NORMAL
- en: While it is easy to program PCA from scratch using RDD, the best way to learn
    it is to try to do PCA with a neuron network implementation and look at the intermediate
    result. You can do this in Café (on Spark), or just Torch, to see that it is a
    straight linear transformation despite the mystery surrounding it. At its core,
    PCA is a straight exercise in linear algebra regardless of whether you use the
    covariance matrix or SVD for decomposition.
  prefs: []
  type: TYPE_NORMAL
- en: Spark provides examples for PCA via source code on GitHub under both the dimensionality
    reduction and feature extraction sections.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for PCA can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.PCA](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.PCA) [and ](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.PCA)[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.PCAModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.PCAModel)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some words of caution regarding PCA usage and shortcomings:'
  prefs: []
  type: TYPE_NORMAL
- en: Some datasets are mutually exclusive so that eigenvalues do not dropoff, (every
    single value is needed for the matrix). For example, the following vectors (.5,0,0),
    (0,.5,0,0), (0,0,.5,0), and (0,0,0,.5) ...... will not allow any eigenvalue to
    drop.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PCA is linear in nature and attempts to learn a Gaussian distribution by using
    mean and the covariance matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sometimes two Gaussian distributions parallel to each other will not allow PCA
    to find the right direction. In this case, PCA will eventually terminate and find
    some directions and output them, but are they the best?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
