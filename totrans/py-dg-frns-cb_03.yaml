- en: A Deep Dive into Mobile Forensic Recipes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following recipes are covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Parsing PLIST files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling SQLite databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying gaps in SQLite databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing iTunes backups
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Putting Wi-Fi on the map
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Digging deep to recover messages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Perhaps it is becoming a bit of a cliché, but it remains true that as technology
    evolves it continues to become more integrated with our lives. Never has this
    been so apparent as with the development of the first smartphone. These precious
    devices seemingly never leave the possession of their owners and often receive
    more interaction than human companions. It should be no surprise then that a smartphone
    can supply investigators with lots of insight into their owner. For example, messages
    may provide insight into the state of mind of the owner or knowledge of particular
    facts. They may even shed light on previously unknown information. Location history
    is another useful artifact we can extract from these devices and can be helpful
    to validate an individual's alibi. We will learn to extract this information and
    more.
  prefs: []
  type: TYPE_NORMAL
- en: 'A common source of evidentiary value on smartphones are SQLite databases. These
    databases serve as the de facto storage for applications in most smartphone operating
    systems. For this reason, many scripts in this chapter will focus on teasing out
    data and drawing inferences from these databases. In addition to that, we will
    also learn how to process PLIST files, commonly used with Apple operating systems,
    including iOS, and extract relevant data. The scripts in this chapter focus on
    solving specific problems and are ordered by complexity:'
  prefs: []
  type: TYPE_NORMAL
- en: Learning to process XML and binary PLIST files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Python to interact with SQLite databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying missing gaps in SQLite databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting an iOS backup into a human-readable format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing output from Cellebrite and performing Wi-Fi MAC address geolocation
    lookups with WiGLE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying potentially intact deleted content from SQLite databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visit [www.packtpub.com/books/content/support](http://www.packtpub.com/books/content/support)
    to download the code bundle for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Parsing PLIST files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recipe Difficulty: Easy'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python Version: 2.7 or 3.5'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operating System: Any'
  prefs: []
  type: TYPE_NORMAL
- en: 'This recipe will process the `Info.plist` file present in every iOS backup
    and extract device-specific information such as the device name, IMEI, serial
    number, product make, model, and iOS version, and the last backup date. Property
    lists, or PLISTs, come in two different formats: XML or binary. Typically, when
    dealing with binary PLISTs, one will need to use the plutil utility on a macOS
    platform to convert it to a readable XML format. However, we will introduce a
    Python library that handles both types readily and easily. Once we extract the
    relevant data elements from the `Info.plist` file, we will print this data to
    the console.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe requires the installation of the third-party library `biplist`.
    All other libraries used in this script are present in Python's standard library.
    The `biplist` module provides a means of processing both XML and binary PLIST
    files.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about the `biplist` library, visit [https://github.com/wooster/biplist](https://github.com/wooster/biplist).
  prefs: []
  type: TYPE_NORMAL
- en: Python has a built-in PLIST library, `plistlib`; however, this library was found
    to not support binary PLIST files as extensively as `biplist` does.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about the `plistlib` library, visit [https://docs.python.org/3/library/plistlib.html](https://docs.python.org/3/library/plistlib.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Installing `biplist` can be accomplished using `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Be sure to grab your own `Info.plist` file to process with this script. If you
    cannot find an `Info.plist` file, any PLIST file should be suitable. Our script
    is not so specific and should technically work with any PLIST file.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will employ the following steps to process the PLIST file:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the input PLIST file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read PLIST data into a variable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print formatted PLIST data to the console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we import the required libraries to handle argument parsing and processing
    PLISTs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This recipe''s command-line handler accepts one positional argument, `PLIST_FILE`,
    which represents the path to the PLIST file we will process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the `os.exists()` and `os.path.isfile()` functions to validate that
    the input file exists and is a file, as opposed to a directory. We do not perform
    any further validation on this file, such as confirming whether it is a PLIST
    file rather than a text file and instead rely on the `biplist` library (and common
    sense) to catch such errors. If the input file passes our tests, we call the `main()`
    function and pass it the PLIST file path:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The `main()` function is relatively straightforward and accomplishes the goal
    of reading the PLIST file and then printing the data to the console. First, we
    print an update to the console that we are attempting to open the file. Then,
    we use the `biplist.readPlist()` method to open and read the PLIST into our `plist_data`
    variable. If the PLIST is corrupt or otherwise inaccessible, `biplist` will raise
    an `InvalidPlistException` or `NotBinaryPlistException` error. We catch both of
    these in a `try` and `except` block and `exit` the script accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have successfully read in the PLIST data, we iterate through the keys
    in the resulting `plist_data` dictionary and print them to the console. Notice
    that we print all keys in the `Info.plist` file with the exception of the `Applications`
    and `iTunes Files` keys. Both of these keys contain a great deal of data that
    floods the console and therefore are not desirable for this type of output. We
    use the format method to help create legible console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice the additional formatting characters in the first curly brackets. We
    are specifying here to left-align the input string with a static width of 25 characters.
    As you can see in the following screenshot, this ensures the data is presented
    in an orderly and structured format:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00022.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This script can be further improved. We have provided a couple of recommendations
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: Rather than printing data to the console, add a CSV function to write the data
    to a CSV file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add support for processing a directory full of PLIST files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling SQLite databases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recipe Difficulty: Easy'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python Version: 3.5'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operating System: Any'
  prefs: []
  type: TYPE_NORMAL
- en: As discussed, SQLite databases serve as the primary data repository on mobile
    devices. Python has a built-in library, `sqlite3`, which can be used to interface
    with these databases. In this script, we will interact with the iPhone `sms.db`
    file and extract data from the `message` table. We will also use this script as
    an opportunity to introduce the `csv` library and write the message data to a
    spreadsheet.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about the `sqlite3` library, visit [https://docs.python.org/3/library/sqlite3.html](https://docs.python.org/3/library/sqlite3.html).
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All libraries used in this script are present in Python's standard library.
    For this script, make sure to have an `sms.db` file from which to query. With
    some minor modification, you can use this script with any database; however, we
    will specifically be talking about it with respect to the iPhone SMS database
    from an iOS 10.0.1 device.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The recipe follows these basic principles:'
  prefs: []
  type: TYPE_NORMAL
- en: Connect to the input database.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Query table PRAGMA to extract column names.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fetch all table content.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write all table content to CSV.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we import the required libraries to handle argument parsing, writing
    spreadsheets, and interacting with SQLite databases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This recipe''s command-line handler accepts two positional arguments, `SQLITE_DATABASE`
    and `OUTPUT_CSV`, which represent the file paths for the input database and the
    desired CSV output, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we use the `os.dirname()` method to extract just the directory path of
    the output file. We do this to check if the output directory already exists. If
    it does not, we use the `os.makedirs()` method to create each directory in the
    output path that does not already exist. This avoids issues later on if we were
    to try to write the output CSV to a directory that does not exist:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have verified that the output directory exists, we pass the supplied
    arguments to the `main()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The `main()` function prints a status update for the user to the console and
    then checks if the input file exists and is a file. If it does not exist, we use
    the `sys.exit()` method to exit the script using a value greater than 0 to indicate
    the script exited due to an error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we use the `sqlite3.conn()` method to connect to the input database.
    It is important to note that the `sqlite3.conn()` method opens a database of the
    supplied name regardless of whether it exists or not. Therefore, it is vital to
    check that the file exists before trying to open a connection to it. Otherwise,
    we could create an empty database, which would likely cause issues in the script
    when we interact with it. Once we have a connection, we need to create a `Cursor`
    object to interact with the database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now perform queries against the database using the `Cursor` object''s
    `execute()` command. At this point, the strings we pass into the execute function
    are just standard SQLlite queries. For the most part, you can run any query that
    you normally would when interacting with an SQLite database. The results returned
    from a given command are stored in the `Cursor` object. We need to use the `fetchall()`
    method to dump the results into a variable we can manipulate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The `fetchall()` method returns a tuple of results. Each column''s name is
    stored in the first index of each tuple. By using list comprehension, we store
    the column names for the `message` table into a list. This comes into play later
    when we write the results of the data to a CSV file. After we obtain the column
    names for the `message` table, we directly query that table for all of its data
    and store it in the `message_data` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'With the data extracted, we print a status message to the console and pass
    the output CSV and the message table columns and data to the `write_csv()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: You'll find that most of the scripts end with writing data to a CSV file. There
    are a few reasons for that. Writing CSVs in Python is very straightforward and
    can be accomplished in a few lines of code for most datasets. Additionally, having
    data in a spreadsheet allows one to sort and filter on columns to help summarize
    and understand large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Before we begin to write to the CSV file, we use the `open()` method to create
    a file object and its alias, `csvfile`. The way in which you open this file changes
    depending on if you are using Python 2.x or Python 3.x. For Python 2.x, you open
    the file in `wb` mode and without the newline keyword argument. With Python 3.x,
    you instead open the file in `w` mode and with the newline keyword set to an empty
    string. Where possible the code is written for Python 3.x, so we use the latter.
    Failing to open the file object in this manner results in the output CSV file
    containing an empty row between each row that is written.
  prefs: []
  type: TYPE_NORMAL
- en: 'After opening the file object, we pass it to the `csv.writer()` method. We
    can use the `writerow()` and `writerows()` methods from this object to write the
    column header list and the list of tuples, respectively. As an aside, we could
    iterate through each tuple in the `msgs` list and call `writerow()` for each tuple.
    The `writerows()` method eliminates the need for the unnecessary loop and is used
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run this script, we see the following console message. Within the CSV
    we can gather in-depth details about the messages sent and received, along with
    interesting metadata including dates, errors, the source, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00023.jpeg)![](../images/00024.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Identifying gaps in SQLite databases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recipe Difficulty: Easy'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python Version: 2.7 or 3.5'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operating System: Any'
  prefs: []
  type: TYPE_NORMAL
- en: This recipe will demonstrate how to programmatically identify missing entries
    for a given table by using its primary key. This technique allows us to identify
    records that are no longer active in the database. We will use this to identify
    which and how many messages have been deleted from an iPhone SMS database. This,
    however, will work with any table that uses an auto-incrementing primary key.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about SQLite tables and primary keys, visit [https://www.sqlite.org/lang_createtable.html](https://www.sqlite.org/lang_createtable.html).
  prefs: []
  type: TYPE_NORMAL
- en: One fundamental idea governing SQLite databases and their tables are primary
    keys. A primary key is typically a column that serves as a unique integer for
    a particular row in the table. A common implementation is the auto-incrementing
    primary key, starting typically at `1` for the first row, and incrementing by
    `1` for each successive row. When rows are removed from the table, the primary
    key does not change to account for that or reorder the table.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we had a database with 10 messages and deleted messages `4`
    through `6`, we would have a gap in the primary key column from `3` to `7`. With
    our understanding of auto-incrementing primary keys, we can make the inference
    that messages `4` through `6`, at one point present, are no longer active entries
    in the database. In this manner, we can quantify the number of messages no longer
    active in the database and the primary key value associated with them. We will
    use this in a later recipe, *Digging deep to recover messages*, to then go hunt
    for those entries in an effort to determine if they are intact and recoverable.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All libraries used in this script are present in Python's standard library.
    This recipe does require a database to run against. For this example, we will
    use the iPhone `sms.db` database.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will perform the following steps in this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: Connect to the input database.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Query table PRAGMA to identify a table's primary key(s).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fetch all primary key values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate and display gaps in the table to the console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we import the required libraries to handle argument parsing and interacting
    with SQLite databases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This recipe''s command-line handler accepts two positional arguments, `SQLITE_DATABASE`
    and `TABLE`, which represents the path of the input database and the name of the
    table to review, respectively. An optional argument, `column`, indicated by the
    dash, can be used to manually supply the primary key column if it is known:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'If the optional column argument is supplied, we pass it to the `main()` function
    as a keyword argument along with the database and table name. Otherwise, we just
    pass the database and table name to the `main()` function without the `col` keyword
    argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The `main()` function, like the previous recipe, first performs some validation
    that the input database exists and is a file. Because we are using keyword arguments
    with this function, we must indicate this with the `**kwargs` argument in the
    function definition. This argument serves as a dictionary that stores all provided
    keyword arguments. In this case, if the optional column argument were supplied,
    this dictionary would contain a `col` key/value pair:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'After validating the input file, we use `sqlite3` to connect to this database
    and create the `Cursor` object we use to interact with it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to identify the primary key for the desired table, we run the `pragma
    table_info` command with the table name inserted in parentheses. We use the `format()`
    method to dynamically insert the name of the table into the otherwise static string.
    After we store the command''s results in the `table_data` variable, we perform
    validation on the table name input. If the user supplied a table name that does
    not exist, we will have an empty list as the result. We check for this and exit
    the script if the table does not exist:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: At this point, we create an `if-else` statement for the remainder of the script,
    depending on whether the optional column argument was supplied by the user. If
    `col` is a key in the `kwargs` dictionary, we immediately call the `find_gaps()`
    function and pass it the `Cursor` object, `c`, the table name, and the user-specified
    primary key column name. Otherwise, we try to identify the primary key(s) in the
    `table_data` variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'The command previously executed and stored in the `table_data` variable returns
    a tuple for each column in the given table. The last element of each tuple is
    a binary option between `1` or `0`, where `1` indicates that the column is a primary
    key. We iterate through each of the last elements in the returned tuples and,
    if they are equal to `1`, the column name, stored in index one of the tuple, is
    appended to the `potential_pks` list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have identified all primary keys, we check the list to determine if
    there are zero or more than one keys present. If either of these cases exists,
    we alert the user and exit the script. In these scenarios, the user would need
    to specify which column should be treated as the primary key column. If the list
    contains a single primary key, we pass the name of that column along with the
    database cursor and table name to the `find_gaps()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The `find_gaps()` method starts by displaying a message to the console, alerting
    the user of the current execution status of the script. We attempt the database
    query in a `try` and `except` block. If the user-specified column does not exist
    or was misspelled, we will receive an `OperationalError` from the `sqlite3` library.
    This is the last validation step of user-supplied arguments and will exit the
    script if the except block is triggered. If the query executes successfully, we
    fetch all of the data and store it in the `results` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: We use list comprehension and the built-in `sorted()` function to create a list
    of sorted primary keys. The `results` list contains tuples with one element at
    index `0`, the primary key, which for the `sms.db` `message` table is the column
    named ROWID. With the sorted list of ROWIDs, we can quickly calculate the number
    of entries missing from the table. This would be the most recent ROWID minus the
    number of ROWIDs present in the list. If all entries were active in the database,
    this value would be zero.
  prefs: []
  type: TYPE_NORMAL
- en: We are working under the assumption that the most recent ROWID is the actual
    most recent ROWID. It is possible that one could delete the last few entries and
    the recipe would only detect the most recent active entry as the highest ROWID.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'If we are not missing any values from the list, we print this fortuitous message
    to the console and exit with `0`, indicating a successful termination. On the
    other hand, if we are missing entries, we print that to the console along with
    the count of the missing entries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'To calculate the missing gaps, we generate a set of all ROWIDs between the
    first ROWID and the last using the `range()` method and then compare that against
    the sorted list that we have. The `difference()` function can be used with a set
    to return a new set with elements in the first set that are not present in the
    object in parentheses. We then print the identified gaps to the console, which
    completes the execution of the script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'An example of the output of this script may look like the following screenshot.
    Note how quickly the console can become cluttered based on the number of deleted
    messages. This, however, is not the intended end of this recipe. We will use the
    logic from this script in a more advanced recipe, *Digging deep to recover messages*,
    later in the chapter to identify and then attempt to locate potentially recoverable
    messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00025.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For more on SQLite database structure and primary keys, refer to their extensive
    documentation at [https://www.sqlite.org/](https://www.sqlite.org/).
  prefs: []
  type: TYPE_NORMAL
- en: Processing iTunes backups
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recipe Difficulty: Easy'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python Version: 2.7 or 3.5'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operating System: Any'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will convert unencrypted iTunes backups into a human-readable
    format, allowing us to easily explore its contents without any third-party tools.
    Backups can be found in the `MobileSync\Backup` folder on the host computer.
  prefs: []
  type: TYPE_NORMAL
- en: For details on default iTunes backup locations for Windows and OS X, visit [https://support.apple.com/en-us/HT204215](https://support.apple.com/en-us/HT204215).
  prefs: []
  type: TYPE_NORMAL
- en: If an Apple product has been backed up to the machine, there will be a number
    of folders whose name is a GUID representing a specific device within the backup
    folder. These folders contain differential backups for each device over a period
    of time.
  prefs: []
  type: TYPE_NORMAL
- en: With the new backup format introduced in iOS 10, files are stored in subfolders
    containing the first two hexadecimal characters of the file name. Each file's
    name is a `SHA-1` hash of its path on the device. In the root of the device's
    backup folder, there are a few files of interest, such as the `Info.plist` file
    we discussed earlier and the `Manifest.db` database. This database stores details
    on each backed up file, including its `SHA-1` hash, file path, and name. We will
    use this information to recreate the native backup folder structure with human-friendly
    names.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All libraries used in this script are present in Python's standard library.
    To follow along, you will need to procure an unencrypted iTunes backup to work
    with. Make sure the backup is of the newer iTunes backup format (iOS 10+) matching
    what was described previously.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use these steps to process the iTunes backup in this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: Identify all backups in the `MobileSync\Backup folder`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Iterate through each backup.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read the Manifest.db file and associate `SHA-1` hash names with filenames.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy and rename backed-up files to the output folder with the appropriate file
    path and name.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we import the required libraries to handle argument parsing, logging,
    copying files, and interacting with SQLite databases. We also set up a variable
    used to later construct the recipe''s logging component:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This recipe''s command-line handler accepts two positional arguments, `INPUT_DIR`
    and `OUTPUT_DIR`, which represent the iTunes backup folder and the desired output
    folder, respectively. An optional argument can be supplied to specify the location
    of the log file and the verbosity for the log messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we begin to set up the log for this recipe. We check if the optional
    verbosity argument was supplied by the user, and if it has been, we increase the
    level from `INFO` to `DEBUG`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'For this log, we set up the message format and configure handlers for the console
    and file output, attaching them to our defined `logger`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'With the log file set up, we log a few debug details to the log, including
    the arguments supplied to this script and details about the host and Python version.
    We exclude the first element of the `sys.argv` list, which is the name of the
    script and not one of the supplied arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `os.makedirs()` function, we create any necessary folders for the
    desired output directory if they do not already exist:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, if the input directory exists and is actually a directory, we pass
    the supplied input and output directories to the `main()` function. If the input
    directory fails validation, we print an error to the console and log before exiting
    the script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The `main()` function starts by calling the `backup_summary()` function to
    identify all backups present in the input folder. Let''s first look at the `backup_summary()`
    function and understand what it does before continuing on with the `main()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The `backup_summary()` function uses the `os.listdir()` method to list the
    contents of the input directory. We also instantiate the `backups` dictionary,
    which stores details for each discovered backup:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'For each item in the input directory, we use the `os.path.join()` method with
    the input directory and item. We then check if this is a directory, rather than
    a file and if the name of the directory is 40 characters long. If the directory
    passes these checks, this is likely a backup directory and so we instantiate two
    variables to keep track of the number of files within the backup and the total
    size of those files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the `os.walk()` method discussed in [Chapter 1](part0029.html#RL0A0-260f9401d2714cb9ab693c4692308abe),
    *Essential Scripting and File Information Recipes,* and create lists for the root,
    subdirectories, and files under the backup folder. We can, therefore, use the
    length of the files list and continue to add it to the `num_files` variable as
    we iterate through the backup folder. In a similar manner, we use a nifty one-liner
    to add each file''s size to the `size` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'After we finish iterating through the backup, we add the backup to the `backups`
    dictionary using its name as the key and store the backup folder path, file count,
    and size as values. Once we complete iteration of all backups, we return this
    dictionary to the `main()` function. Let''s pick it back up there:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Back in the `main()` function, we print a summary of each backup to the console
    if any were found. For each backup, we print an arbitrary number identifying the
    backup, the name of the backup, the number of files, and the size. We use the
    `format()` method and manually specify newlines (`\n`) to ensure the console remains
    legible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we use a `try-except` block to dump the contents of the `Manifest.db`
    file to the `db_items` variable. If the `Manifest.db` file is not found, the identified
    backup folder is either of an older format or invalid and so we skip it with the
    `continue` command. Let''s briefly discuss the `process_manifest()` function,
    which uses `sqlite3` to connect to and extract all data in the `Manifest.db` files
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The `process_manifest()` method takes the directory path of the backup as its
    only input. To this input, we join the `Manifest.db` string, to represent the
    location where this database should exist in a valid backup. If it is found that
    this file does not exist, we log that error and raise an `IOError` to the `main()`
    function, which as we just discussed will cause a message to be printed to the
    console and continue on to the next backup:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'If the file does exist, we connect to it and create the `Cursor` object using
    `sqlite3`. The `items` dictionary stores each entry in the `Files` table using
    the item''s `SHA-1` hash as the key and storing all other data as values in a
    list. Notice here an alternative method of accessing the results of the query
    rather than the `fetchall()` function used in previous recipes. After we have
    extracted all of the data from the `Files` table, we return the dictionary back
    to the `main()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Back in the `main()` function, we immediately pass the returned dictionary,
    now referred to as `db_items`, to the `create_files()` method. The dictionary
    we just created is going to be used by the next function to perform lookups on
    the file `SHA-1` hash and determine its real filename, extension, and native file
    path. The `create_files()` function performs these lookups and copies the backed-up
    file to the output folder with the appropriate path, name, and extension.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `else` statement handles situations where there were no backups found by
    the `backup_summary()` function. We remind the user of what the appropriate input
    folder should be and exit the script. This completes the `main()` function; now
    let''s move onto the `create_files()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We start the `create_files()` method by printing a status message to the log:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create a counter to track the number of files found in the manifest
    but not within the backup. We then iterate through each key in the `db_items`
    dictionary generated from the `process_manifest()` function. We first check if
    the associated file name is `None` or an empty string and continue onto the next
    `SHA-1` hash item otherwise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'If the associated file name is present, we create a few variables representing
    the output directory path and the output file path. Notice the output path is
    appended to the name of the backup, `b`, to mimic the backup folder structure
    in the input directory. We use the output directory path, `dirpath`, to first
    check if it exists and create it otherwise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We create a few more path variables, including the location of the backed-up
    file in the input directory. We do this by creating a string with the backup name,
    the first two characters of the `SHA-1` hash key, and the `SHA-1` key itself separated
    by forward slashes. We then join this to the input directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'With all of these paths created, we can now begin to perform a few more validation
    steps and then copy files over to the new output destination. First, we check
    if the output file already exists in the output folder. During development of
    this script, we noticed some files had the same name and were stored in the same
    folder in the output. This caused data to be overwritten and file counts to not
    match up between the backup folder and the output folder. To remedy this, if the
    file already exists in the backup, we append an underscore and an integer, `x`,
    which represents the loop iteration number, which serves as a unique value for
    our purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'With filename collisions sorted out, we use the `shutil.copyfile()` method
    to copy the backed-up file, represented by the path variable, and rename it and
    store it in the output folder, represented by the `filepath` variable. If the
    path variable refers to a file that is not in the backup folder, it will raise
    an `IOError`, which we catch and log to the log file and add to our counter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'We then provide a warning to the user about the number of files that were not
    found in the `Manifest.db`, just in case the user did not enable verbose logging.
    Once we have copied all files in the backup directory, we use the `shutil.copyfile()`
    method to individually copy the non-obfuscated PLIST and database files present
    in the backup folder to the output folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run this code, we can see the following updated file structure in our
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00026.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This script can be further improved. We have provided a recommendation here:'
  prefs: []
  type: TYPE_NORMAL
- en: Add functionality to convert encrypted iTunes backups. Using a third-party library,
    such as `pycrypto`, one can decrypt the backups by supplying the correct password.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Putting Wi-Fi on the map
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recipe Difficulty: Medium'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python Version: 3.5'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operating System: Any'
  prefs: []
  type: TYPE_NORMAL
- en: Without a connection to the outside world, mobile devices are little more than
    an expensive paperweight. Fortunately, open Wi-Fi networks are everywhere, and
    sometimes a mobile device will connect to them automatically. On the iPhone, a
    list of Wi-Fi networks the device has connected to is stored in a binary PLIST
    named `com.apple.wifi.plist`. This PLIST records, among other things, the Wi-Fi
    SSID, BSSID, and connection time. In this recipe, we will show how to extract
    Wi-Fi details from a standard Cellebrite XML report or supply Wi-Fi MAC addresses
    in a newline-delimited file. As the Cellebrite report formats may evolve over
    time, we are basing our XML parsing on a report generated with UFED Physical Analyzer
    version 6.1.6.19.
  prefs: []
  type: TYPE_NORMAL
- en: 'WiGLE is an online searchable repository of, at the time of writing, over 300
    million Wi-Fi networks. We will use the Python `requests` library to access the
    API for WiGLE, to perform automated searches based on Wi-Fi MAC addresses. To
    install the `requests` library, we can use `pip`, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: If a network is found in the WiGLE repository, we can obtain a great deal of
    data about it, including its latitude and longitude coordinates. With this information,
    we can understand where a user's device, and presumably the user itself, has been
    and when that connection was made.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about and use WiGLE, visit the website [https://wigle.net/.](https://wigle.net/)
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe requires an API key from the WiGLE website. To register for a free
    API key, visit [https://wigle.net/account](https://wigle.net/account) and follow
    the instructions to display your API key. There are two API values, the name and
    key. For this recipe, please create a file with a single line where the API name
    value is first, followed by a colon (no spaces), and then the API key. This format
    will be read by the script to authenticate you to the WiGLE API.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, in order to query the WiGLE API you must contribute
    data to the service. This is because the whole site is built on community sourced
    data and this encourages users to share information with others. There are many
    ways to contribute data, as documented on [https://wigle.net](https://wigle.net).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This recipe follows the following steps to accomplish the goal:'
  prefs: []
  type: TYPE_NORMAL
- en: Identify input as either a Cellebrite XML report or a line-separated text file
    of MAC addresses.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Process either type of input into a Python dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Query the WiGLE API using `requests`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Optimize the returned WiGLE results into a more convenient format.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write the processed output to a CSV file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we import the required libraries to handle argument parsing, writing
    spreadsheets, processing XML data, and interacting with the WiGLE API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: This recipe's command-line handler accepts two positional arguments, `INPUT_FILE`
    and `OUTPUT_CSV`, representing the input file with Wi-Fi MAC addresses and the
    desired output CSV, respectively. By default, the script assumes the input file
    is a Cellebrite XML report. The user can specify the type of the input file using
    the optional `-t` flag and choose between `xml` or `txt`. Additionally, we can
    set the path of the file containing our API key. By default, this is set in the
    base of the user's directory and named `.wigle_api`, though you can update this
    value to reflect what is easiest in your environment.
  prefs: []
  type: TYPE_NORMAL
- en: This file holding your API key should have additional protections, through file
    permissions or otherwise, to prevent theft of your key.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'We perform the standard data validation steps and check that the input file
    exists and is a file, exiting the script otherwise. We use `os.path.dirname()`
    to extract the directory path and check if it exists. If it does not already exist,
    we use the `os.makedirs()` function to create the directory. We also read in and
    split the API name and key before calling the `main()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'After we perform argument validation, we pass all arguments to the `main()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `main()` function, we first determine the type of input we are working
    with. By default, the `type` variable is `"xml"` unless otherwise specified by
    the user. Depending on the file type, we send it to the appropriate parser, which
    returns the extracted Wi-Fi data elements in a dictionary. This dictionary is
    then passed, along with the output CSV, to the `query_wigle()` function. This
    function is responsible for querying, processing, and writing the query results
    to a CSV file. First, let''s take a look at the parsers, starting with the `parse_xml()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: We parse the Cellebrite XML report using `xml.etree.ElementTree`, which we have
    imported as `ET`.
  prefs: []
  type: TYPE_NORMAL
- en: "To learn more about the `xml` library, visit [https://docs.python.org/3/library/xml.etree.elementtree.html](https://docs.python.org/3/library/xml.etree.elementtree.html).[\uFEFF\
    ](https://docs.python.org/3/library/xml.etree.elementtree.html)"
  prefs: []
  type: TYPE_NORMAL
- en: Parsing a report generated by a forensic tool can be tricky business. These
    reports may change in format and break your script. Therefore, we cannot assume
    that this script will continue to function with future iterations of Cellebrite's
    Physical Analyzer software. And it is for that reason that we've included an option
    to use this script with a text file containing MAC addresses instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'As with any XML file, we need to first access the file and parse it using the
    `ET.parse()` function. We then use the `getroot()` method to return the root element
    of the XML file. We use this root as the initial foothold in the file as we search
    for the Wi-Fi data tags within the report:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the `iter()` method to iterate through the child elements of the root.
    We check the tag for each child looking for the model tag. If found, we check
    if it has a location type attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'For each location model found, we iterate through each of its field elements
    using the `findall()` method. This element contains metadata about the location
    artifact, such as the timestamp, BSSID, and SSID, of the network. We can check
    if the field has a name attribute with the value of `"Timestamp"` and store its
    value in the `ts` variable. If the value does not have any text content, we continue
    on to the next field:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'In a similar fashion, we check if the field''s name matches `"Description"`.
    This field contains the BSSID and SSID of the Wi-Fi network in a tab-delimited
    string. We attempt to access the text of this value and except an `AttributeError`
    if there is no text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Because there may be other types of `"Location"` artifacts in the Cellebrite
    report, we check that the string `"SSID"` is present in the value''s text. If
    so, we split the string using the tab special character into two variables. These
    strings we extracted from the value''s text contain some unnecessary characters,
    which we remove from the string using string slicing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'After we extract the timestamp, BSSID, and SSID from the report, we can add
    them to the `wifi` dictionary. If the Wi-Fi BSSID is already stored as one of
    the keys, we append the timestamp and SSID to the list. This is so that we can
    capture all historical connections to this Wi-Fi network and any changes to the
    name of the network. If we have not yet added this MAC address to the `wifi` dictionary,
    we create the key/value pairs including the WiGLE dictionary that stores API call
    results. After we have parsed all Location model artifacts, we return the `wifi`
    dictionary to the `main()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'In contrast to the XML parser, the TXT parser is much more straightforward.
    We iterate through each line of the text file and set up each line, which should
    be one MAC address, as a key to an empty dictionary. After we have processed all
    lines in the file, we return the dictionary to the `main()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'With the dictionary of MAC addresses, we can now move onto the `query_wigle()`
    function and use `requests` to make WiGLE API calls. First, we print a message
    to the console informing the user of the current execution status. Next, we iterate
    through each MAC address in the dictionary and use the `query_mac_addr()` function
    to query the site for the BSSID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'The `query_mac_addr()` function takes our MAC address and API key and constructs
    the URL for the request. We use the base URL for the API and insert the MAC address
    at the end of it. This URL is then provided to the `requests.get()` method, along
    with an `auth kwarg` to provide the API name and key. The `requests` library handles
    forming and sending the packet to the API with the correct HTTP basic authentication.
    The `req` object is now ready for us to interpret, so we can call the `json()`
    method to return the data as a dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'With the returned `wigle_results` dictionary, we check the `resultCount` key
    to determine how many results were found in the `Wigle` database. If there are
    no results, we append an empty list to the results key in the `Wigle` dictionary.
    Likewise, if there are results, we directly append the returned `wigle_results`
    dictionary to the dataset. The API does have limits to a number of calls you can
    execute per day. When you reach that limit, a `KeyError` will be generated, which
    we catch and print to the console. We also provide reporting for other errors
    identified in a run, as the API may grow to expand the error reporting. After
    searching for each address and adding the results to the dictionary, we pass it,
    along with the output CSV, to the `prep_output()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: If you haven't noticed, the data is becoming increasingly complicated, which
    makes writing it and working with it a bit more complicated. The `prep_output()`
    method essentially flattens the dictionary into easily writable chunks. The other
    reason we need this function is that we need to create separate rows for each
    instance a particular Wi-Fi network was connected to. While the WiGLE results
    for that network will be the same, the connection timestamp and the network SSID
    may be different.
  prefs: []
  type: TYPE_NORMAL
- en: 'To accomplish this, we start by creating a dictionary for the final processed
    results and a Google Maps-related string. We use this string to create a query
    with the latitude and longitude so the user can easily paste the URL into their
    browser to view geolocation details in Google Maps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: We iterate through each MAC address in the dictionary and create two additional
    loops to iterate through all timestamps and all WiGLE results for the MAC address.
    With these loops, we can now access all of the data we have collected thus far
    and begin to add the data to the new output dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Due to the complexity of the initial dictionary, we create a variable called
    `shortres` to act as a shortcut to a deeper part of the output dictionary. This
    prevents us from unnecessarily writing the entire directory structure each and
    every time we need to access that part of the dictionary. The first use of the
    `shortres` variable can be seen as we extract the latitude and longitude of this
    network from the WiGLE results and append it to the Google Maps query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: In one (rather complicated) line, we add a key and value pair where the key
    is unique based on loop iteration counters and the value is the flattened dictionary.
    We do this by first creating a new dictionary containing the BSSID, SSID, timestamp,
    and the newly created Google Maps URL. Because we want to simplify the output,
    we need to merge the new dictionary and the WiGLE results, stored in the `shortres`
    variable, together.
  prefs: []
  type: TYPE_NORMAL
- en: We could iterate through each key in the second dictionary and add its key and
    value pairs one by one. However, it is much quicker to use a feature introduced
    in Python 3.5 whereby we can merge the two dictionaries by placing two `*` symbols
    before each dictionary. This will combine both dictionaries and, if there are
    any keys with the same name, overwrite data from the first dictionary with the
    second one. In this case, we do not have any key overlap, so this will simply
    combine the dictionaries as desired.
  prefs: []
  type: TYPE_NORMAL
- en: 'See the following StackOverflow post to learn more about dictionary merging:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://stackoverflow.com/questions/38987/how-to-merge-two-python-dictionaries-in-a-single-expression](https://stackoverflow.com/questions/38987/how-to-merge-two-python-dictionaries-in-a-single-expression).'
  prefs: []
  type: TYPE_NORMAL
- en: 'After all of the dictionaries have been merged, we proceed to the `write_csv()`
    function to finally write the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'In this recipe, we reintroduce the `csv.DictWriter` class, which allows us
    to easily write dictionaries to a CSV file. This is preferable over the `csv.writer`
    class we have used previously as it provides us a few benefits, including ordering
    the columns. To take advantage of that, we need to know all of the fields we use.
    Because WiGLE is dynamic and the reported results may change, we elected to dynamically
    find the names of all keys in the output dictionary. By adding them to a set,
    we ensure we only have unique keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have identified all of the keys we have in the output, we can create
    the CSV object. Notice how with the `csv.DictWriter` object we use two keyword
    arguments. The first, as mentioned previously, is a list of all the keys in the
    dictionary that we have sorted. This sorted list is the order of the columns in
    the resulting CSV. If the `csv.DictWriter` encounters a key that is not in the
    supplied `field_list`, which shouldn''t happen in this case due to our precautions,
    it will ignore the error rather than raise an exception due to the configuration
    in the `extrasaction kwarg`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have the writer set up, we can use the `writeheader()` method to automatically
    write the columns based on the supplied field names. After that, it''s a simple
    matter of iterating through each dictionary in the data and writing it to the
    CSV file with the `writerow()` function. While this function is simple, imagine
    the headache we would have if we did not simplify the original data structure
    first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'After running this script, we can see all sorts of useful information in our
    CSV report. The first few columns include the BSSID, Google Maps URL, City and
    County:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00027.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We then see several timestamps such as the first time seen, most recent time
    seen, and more specific locations such as the region and road:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00028.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'And finally, we can learn the SSID, coordinates, and type of network and authentication
    used:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00029.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Digging deep to recover messages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recipe Difficulty: Hard'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python Version: 3.5'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operating System: Any'
  prefs: []
  type: TYPE_NORMAL
- en: Earlier in this chapter, we developed a recipe to identify missing records from
    a database. In this recipe, we will leverage the output from that recipe and identify
    recoverable records and their offset within a database. This is accomplished by
    understanding some internals of SQLite databases and leveraging that understanding
    to our advantage.
  prefs: []
  type: TYPE_NORMAL
- en: For a detailed description of the SQLite file internals, review [https://www.sqlite.org/fileformat.html](https://www.sqlite.org/fileformat.html).
  prefs: []
  type: TYPE_NORMAL
- en: With this technique, we will be able to quickly triage a database and identify
    recoverable messages.
  prefs: []
  type: TYPE_NORMAL
- en: When a row from a database is deleted, similar to a file, the entry is not necessarily
    overwritten. This entry can still persist for some time based on database activity
    and its allocation algorithms. Our chances for data recovery decrease when, for
    example, a `vacuum` command is triggered.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will not get into the weeds discussing SQLite structure; suffice to say
    that each entry is made up of four elements: payload length, the ROWID, the payload
    header, and the payload itself. The previous recipe identifies missing ROWID values,
    which we will use here to find all such occurrences of the ROWID across the database.
    We will use other data, such as known standard payload header values, with the
    iPhone SMS database to validate any hits. While this recipe is focused on extracting
    data from the iPhone SMS database, it can be modified to work for any database.
    We will later point out the few lines of code one would need to change to use
    it for other databases.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All libraries used in this script are present in Python's standard library.
    If you would like to follow along, obtain an iPhone SMS database. If the database
    does not contain any deleted entries, open it with an SQLite connection and delete
    a few. This is a good test to confirm the script works as intended on your dataset.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This recipe is made up of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Connect to the input database.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Query table PRAGMA and identify active entry gaps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert ROWID gaps into their varint representation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Search the raw hex of the database for missing entries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Output results to a CSV file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we import the required libraries to handle argument parsing, manipulating
    hex and binary data, writing spreadsheets, creating tuples of cartesian products,
    searching with regular expression, and interacting with SQLite databases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'This recipe''s command-line handler takes three positional and one optional
    argument. This is largely the same as the *Identifying gaps in SQLite databases*
    recipe earlier in this chapter; however, we have also added an argument for the
    output CSV file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'After we parse the arguments, we pass the supplied arguments to the `main()`
    function. If the optional column argument was supplied by the user, we pass it
    to the `main()` function using the `col` keyword argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: Because this script leverages what we have previously built, the `main()` function
    is largely duplicative of what we have already shown. Rather than repeating the
    comments about the code (there's only so much one can say about a line of code)
    we refer you to the *Identifying gaps in SQLite databases* recipe for an explanation
    of that portion of the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'To refresh everyone''s collective memory, see the following summary of that
    recipe: the `main()` function performs basic input validation, identifies potential
    primary keys from a given table (unless the column was supplied by the user),
    and calls the `find_gaps()` function. The `find_gaps()` function is another holdover
    from the previous script and is almost identical to the previous with the exception
    of one line. Rather than printing all of the identified gaps, this function now
    returns the identified gaps back to the `main()` function. The remainder of the
    `main()` function and all other code covered here on out is new. This is where
    we pick back up the thread as we continue to understand this recipe.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With gaps identified, we call a function called `varint_converter()` to process
    each gap into its varint counterpart. Varints, also known as variable-length integers,
    are big-endian integers between one and nine bytes in size. Varints are used by
    SQLite because they can take up less space than actually storing the ROWID integer
    itself. Therefore, in order to search for the deleted ROWID effectively, we must
    first convert it to a varint as we must search for that instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'For ROWIDs less than or equal to 127, their varint equivalent is simply the
    hex representation of the integer. We use the built-in `hex()` method to convert
    the integer into a hex string and use string slicing to remove the prepended `0x`.
    For example, executing `hex(42)` returns the string `0x2a`; in this case, we remove
    the leading `0x` hex designator as we are only interested in the value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'If the missing ROWID is `128` or greater, we start an infinite `while` loop
    to find the relevant varint. Before starting the loop, we use list comprehension
    to create a list containing numbers `0` through `255`. We also instantiate a counter
    variable with a value of `1`. The first part of the `while` loop creates a list
    of tuples, whose number of elements equals to the `counter` variable, containing
    every combination of the `combos` list. For example, if counter is equal to `2`,
    we see a list of tuples representing all possible 2-byte varints as `[(0, 0),
    (0, 1), (0, 2), ..., (255, 255)]`. After that process completes, we use list comprehension
    again to remove all tuples whose first element is less than or equal to `127`.
    Due to the fact that this part of the `if-else` loop deals with rows greater than
    or equal to `128`, we know the varint cannot be equal to or less than `127` and
    so those values are eliminated from consideration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: After creating the list of n-byte varints, we loop through each combination
    and pass it to the `integer_converter()` function. This function treats these
    numbers as part of a varint and decodes them into the corresponding ROWID. We
    can then check the returned ROWID against the missing ROWID. If it matches, we
    add a key and value pair to the `varints` dictionary where the key is the hexadecimal
    representation of the varint and the value is the missing ROWID. At this point,
    we increment the `i` variable by `1` and try to fetch the next row element. If
    successful, we process that ROWID and so on until we have reached the end of the
    ROWIDs that will generate an `IndexError`. We catch such an error and return the
    `varints` dictionary back to the `main()` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'One important thing to note about this function, because the input was a sorted
    list of ROWIDs, we only need to calculate the n-byte varint combinations once
    as the next ROWID in line can only be bigger not smaller. Additionally, due to
    the fact that we know the next ROWID is at least one greater than the previous,
    we continue looping through the varint combinations we created without restarting
    as it would be impossible for the next ROWID to be smaller. These techniques show
    a great use case for `while` loops as they vastly improve the execution speed
    of the recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: The `integer_converter()` function is relatively straightforward. This function
    makes use of the built-in `bin()` method, similar to the `hex()` method already
    discussed, to convert an integer into its binary equivalent. We iterate through
    each value in the proposed varint, first converting each using `bin()`. This returns
    a string, this time with the binary prefix value `0b` prepended, which we remove
    using string slicing. We again use `zfill()` to ensure the bytes have all bits
    intact as the `bin()` method removes leading `0` bits by default. After that,
    we remove the first bit from every byte. As we iterate through each number of
    our varint, we add the resulting processed bits to a variable called `binary`.
  prefs: []
  type: TYPE_NORMAL
- en: This process may sound a little confusing; however, this is the manual process
    of decoding varints.
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to this blog post on *Forensics from the sausage factory* for more details
    about how to manually convert varints to integers and other SQLite internals:'
  prefs: []
  type: TYPE_NORMAL
- en: "[https://forensicsfromthesausagefactory.blogspot.com/2011/05/analysis-of-record-structure-within.html](https://forensicsfromthesausagefactory.blogspot.com/2011/05/analysis-of-record-structure-within.html).[\uFEFF\
    ](https://forensicsfromthesausagefactory.blogspot.com/2011/05/analysis-of-record-structure-within.html)"
  prefs: []
  type: TYPE_NORMAL
- en: 'After we finish iterating through the list of numbers, we use `lstrip()` to
    strip out any leftmost zero values in the binary string. If the resulting string
    is empty, we return `0`; otherwise, we convert and then return the processed binary
    data back into an integer from the base-2 binary representation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Back in the `main()` function, we pass the `varints` dictionary and the path
    to the database file to the `find_candidates()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'The two candidates we search for are `"350055"` and `"360055"`. As discussed
    before, in a database, following the ROWID for a cell is the payload header length.
    This payload header length is typically one of two values in the iPhone SMS database:
    either `0x35` or `0x36`. Following the payload header length is the payload header
    itself. The first serial type of the payload header will be `0x00` and represents
    a `NULL` value, which the primary key of the database--the first column and hence
    the first serial type--will always be recorded as. Next is the serial type `0x55`
    corresponding to the second column in the table, the message GUID, which is always
    a `21` byte string and therefore will always be represented by the serial type
    `0x55`. Any validated hits are appended to the results list.'
  prefs: []
  type: TYPE_NORMAL
- en: 'By searching for the ROWID varint and these three additional bytes, we can
    greatly reduce the number of false positives. Note that if you are working on
    a database other than the iPhone SMS database, you need to change the value of
    these candidates to reflect any static content proceeding the ROWID in your table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'We open the database in `rb` mode to search its binary content. In order to
    do so, we must first read in the entire database and, using the `binascii.hexlify()`
    function, convert this data into hex. As we have already stored the varints as
    hex, we can now easily search this dataset for the varint and other surrounding
    data. We begin the search process by looping through each varint and creating
    two different search strings to account for either of the two static footholds
    in the iPhone SMS database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'We then use the `re.finditer()` method to iterate through each hit based on
    the `search_a` and `search_b` keywords. For each result, we append a list with
    the ROWID, the search term used, and the offset within the file. We must divide
    by 2 to accurately report the number of bytes rather than the number of hex digits.
    After we finish searching the data, we return the results to the `main()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'For the last time, we are back in the `main()` function. This time we check
    if there are any search results. If there are, we pass them along with the CSV
    output to the `csvWriter()` method. Otherwise, we print a status message to the
    console notifying the user that there were no intact recoverable ROWIDs identified:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'The `write_csv()` method is true to form and simple as always. We open a new
    CSV file and create three columns for the three elements stored in the nested
    list structure. We then use the `writerows()` method to write all rows in the
    results data list to the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'When we look at the exported report we can clearly see our row ID, the searched
    hex value, and the offset within the database the record was found:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00030.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This script can be further improved. We have provided a recommendation here:'
  prefs: []
  type: TYPE_NORMAL
- en: Rather than hard-coding the candidates, accept a text file of such candidates
    or command-line entries to increase the recipe's flexibility
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
