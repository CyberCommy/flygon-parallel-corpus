- en: Scaling Up Your Application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter is a short break from the previous technical chapters, where we
    will go in-depth on the following:'
  prefs: []
  type: TYPE_NORMAL
- en: How serverless autoscaling works
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How Lambda can handle traffic demands during peak-service usage with no capacity
    planning or scheduled scaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How AWS Lambda uses concurrency to create multiple executions in parallel to
    your function's code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How it can impact your cost and application performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter is a follow-up of the previous chapter as it will use the serverless
    API built in the previous one; it's recommended to read the previous chapter first
    before tackling this section.
  prefs: []
  type: TYPE_NORMAL
- en: Load testing and scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this part, we will generate random workloads to see how Lambda acts when
    incoming requests increase. To achieve that, we will use a load-testing tool,
    such as **Apache Bench**. In this chapter, I will be using `hey`, which is a Go-based
    tool, and is very efficient and faster than classic `HTTP` benchmarking tools
    due to Golang''s built-in concurrency. You can download it by installing the following
    `Go` package from your terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Be sure that the `$GOPATH` variable is set to be able to execute the `hey` command
    regardless of your current directory, or you can add the `$HOME/go/bin` folder
    to the `$PATH` variable.
  prefs: []
  type: TYPE_NORMAL
- en: Lambda autoscaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we are ready to run our first harness or load testing by executing the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: If you prefer apache benchmark, the same command can be used by replacing the
    **hey** keyword with **ab.**
  prefs: []
  type: TYPE_NORMAL
- en: 'The command will open 50 connections and send 1,000 requests against the API
    Gateway endpoint URL for the `FindAllMovies` function. At the end of the test,
    **hey** will display information about the total response time and in-depth details
    about each request, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/745a0e79-f00a-40af-a9a0-8bd988ec3b7b.png)'
  prefs: []
  type: TYPE_IMG
- en: Make sure to replace the invocation URL with your own. Also, please note that
    some parts of the screenshot have been cropped to focus only on the useful content.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the total response time, **hey** gives an output of a response-time
    histogram that shows the first requests taking more time (around 2 seconds) to
    respond, which can be explained with the **cold start** since Lambda has to download
    the deployment package and initialize a new container. However, the rest of the
    requests were fast (less than 800 milliseconds), due to the **warm start** and
    the usage of existing containers from previous requests.
  prefs: []
  type: TYPE_NORMAL
- en: From the previous benchmark, we can say that Lambda keeps its promise of autoscaling
    when traffic is raised; while that might be a good thing, it has downsides, which
    we will see in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Downstream resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our example of the Movies API, a DynamoDB table has been used to resolve
    the stateless issue. This table requires the user to define the read and write
    throughput capacity, in advance, to create the necessary underlying infrastructure
    to handle the defined traffic. While creating the table in [Chapter 5](8ed90f5e-1c5d-4b4f-aca7-82b878ca2712.xhtml), *Managing
    Data Persistence with DynamoDB,* we used the default throughput, which is five
    read-capacity units and five write-capacity units. The five read-capacity units
    work as a charm for APIs that aren't read-heavy. In the previous load test, we
    created 50 concurrent executions, that is, 50 reads in parallel to the `movies`
    table. As a result, the table will be suffering from high read throughput, and
    the `Scan` operation will be slower and DynamoDB might start throttling requests.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can verify this by going to the DynamoDB console and clicking on the **Metrics** tab
    from the `movies` table, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3ee6584d-7f97-4382-8ccb-30bf7198d4c6.png)'
  prefs: []
  type: TYPE_IMG
- en: Clearly, the read-capacity graph experienced a high peak, which resulted in
    throttling read requests, and the table was overwhelmed by all of these incoming
    requests.
  prefs: []
  type: TYPE_NORMAL
- en: DynamoDB's throttling requests can be fixed by enabling the autoscaling mechanism
    to increase the provisioned read and write capacity to handle a sudden increase
    in traffic, or by reusing the query's results stored in the in-memory cache engine
    (a solution such as AWS ElastiCache with Redis or Memcached engine can be used)
    to avoid overwhelming the table and cutting several milliseconds off your function's
    execution time. However, you can't limit and protect your database resource from
    being overloaded in response to your Lambda function's scaling event.
  prefs: []
  type: TYPE_NORMAL
- en: Private Lambda functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another issue of concurrency can arise if your Lambda function is running inside
    a private VPC, since it will need to attach an **Elastic Network Interface** (**ENI**)
    to the Lambda container and wait for it to assign itself an IP. AWS Lambda uses
    the ENI to connect securely to internal resources in the VPC.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to poor performance (attaching ENI takes an average of 4 seconds),
    a VPC-enabled Lambda function forces you to maintain and configure a NAT instance
    for internet access and a number of VPC subnets in multiple availability zones
    capable of supporting the ENI scaling requirements of your function, which might
    cause the VPC to run out of IP addresses.
  prefs: []
  type: TYPE_NORMAL
- en: To sum up, the Lambda function's autoscaling is a double-edged sword; it doesn't
    require capacity planning from your side. However, it might result in bad performance
    and surprising monthly bills. That's where the **concurrent execution** model
    comes into play.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrent execution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AWS Lambda dynamically scales capacity in response to increased traffic. However,
    there's a limited number of an executed function's code at any given time. This
    number is called concurrent execution, and it's defined per AWS region. The default
    limit of concurrency is 1,000 per AWS region. So, what happens if your function
    crosses this defined threshold? Read on to find out.
  prefs: []
  type: TYPE_NORMAL
- en: Lambda throttling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Lambda applies throttling (rate limiting) to your function if the concurrent
    execution count is exceeding the limit. Hence, the remaining incoming requests
    won't invoke the function.
  prefs: []
  type: TYPE_NORMAL
- en: The invoking client is responsible for retrying the failed requests due to throttling
    by implementing a back-off strategy based on the `HTTP` code returned (`429` =
    too many requests). It's worth mentioning that Lambda functions can be configured
    to store unprocessed events, after a certain number of retries, to a queue called
    the **dead letter queue**.
  prefs: []
  type: TYPE_NORMAL
- en: Throttling might be useful in some cases, as the concurrent execution capacity
    is shared across all functions (in our example, the `find`, `update`, `insert`,
    and `delete` functions). You may want to ensure that one function doesn't consume
    all the capacity and avoids starvation of the rest of the Lambda functions. This
    situation can happen frequently if one of your functions is used more than others.
    For example, consider the `FindAllMovies` function. Supposing it's the holiday
    season, a lot of customers will use your application to see a list of movies available
    to rent, which might result in several instances of the invocation of the `FindAllMovies` Lambda
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Luckily, AWS has added a new feature that lets you reserve and define, in advance,
    a concurrent execution value per Lambda function. This property allows you to
    specify a number of reserved concurrency for your function so you are sure that
    your function always has enough capacity to handle upcoming events or requests.
    For instance, you could set rate limiting for your functions as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `FindAllMovies` function: 500'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `InsertMovie` function : 100'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `UpdateMovie` function: 50'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The remaining will be shared among the others
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the upcoming section, we will see how to define a reserved concurrency for
    `FindAllMovies` and how it can impact the performance of the API.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can estimate the concurrent execution count with the following formula:
    `events/requests per second * function duration`.'
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency reservation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Navigate to the AWS Lambda Console ([https://console.aws.amazon.com/lambda/home](https://console.aws.amazon.com/lambda/home))
    and click on the FindAllMovies function. Under the Concurrency section, we can
    see that our function is only limited by the total amount of concurrency available
    in the account, which is **1000**,shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/627e3cc6-60f8-43b8-8aab-a245c4a10ba2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will change that by defining 10 in the reserved account''s concurrency field.
    This ensures only 10 parallel executions of the function at any given time. This
    value will be deducted from the unreserved account''s concurrency pool, shown
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/efe5e643-809c-4050-a213-e3d8633c152e.png)'
  prefs: []
  type: TYPE_IMG
- en: The maximum reserved concurrency you can set is 900, as AWS Lambda reserves
    100 for other functions so they can still process requests and events.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, AWS CLI can be used with the `put-function-concurrency` command
    to set a concurrency limit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Once again, generate some workloads using the same command given previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This time, the result will be different, as 171 of 1,000 requests fail with
    the 502 code error, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f4d76c40-6cb2-4b71-a121-7c4d8f6820a0.png)'
  prefs: []
  type: TYPE_IMG
- en: Beyond 10 concurrent executions, a throttling is applied and part of the request
    is refused with the 502 response code.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can confirm this by heading back to the function console; we should see
    a warning message similar to that shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6c89cab8-ef3a-4792-905c-a3c916fb588f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If you open the metrics related to the `movies` table and jump to the read-capacity
    chart, you can see that our read capacity will still be under control and below
    the defined 5 read-units capacity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f413ad7d-8d50-4d52-8aa9-51750286955c.png)'
  prefs: []
  type: TYPE_IMG
- en: Throttling can be used if you're planning maintenance on a Lambda function and
    you want to stop its invocation temporarily, this can be done by setting the function
    concurrency to 0.
  prefs: []
  type: TYPE_NORMAL
- en: Throttle is working as expected and you are now protecting your downstream resources
    from too much load from your Lambda function.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned that Lambda cannot scale infinitely due the execution
    limit set per AWS region. This limit can be raised by contacting the AWS support
    team. We also covered how the concurrency reservation at the function level might
    help you to protect your downstream resources, match the subnet size if you're
    using a VPC-enabled Lambda function, and control your costs during the development
    and testing of your functions.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will build a user-friendly UI on top of the serverless
    API with an S3 static-hosted website feature.
  prefs: []
  type: TYPE_NORMAL
