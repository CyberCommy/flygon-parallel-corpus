- en: Designing Concurrent Data Structures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we touched on the basics of concurrency and multithreading
    in C++. One of the biggest challenges in concurrent code design is properly handling
    data races. Thread synchronization and orchestration is not an easy topic to grasp,
    although we might consider it the most important one. While we can use synchronization
    primitives such as mutexes everywhere that we have the slightest doubt about a
    data race, it's not a best practice that we would advise.
  prefs: []
  type: TYPE_NORMAL
- en: A better way of designing concurrent code is to avoid locks at all costs. That
    would not only increase the performance of the application but also make it much
    safer than before. Easier said than done – lock-free programming is a challenging
    topic that we are introducing in this chapter. In particular, we will go further
    into the fundamentals of designing lock-free algorithms and data structures. This
    is a hard topic being continuously researched by many outstanding developers.
    We will touch on the basics of lock-free programming, which will give you an idea
    of how to construct your code in an efficient way. After reading this chapter,
    you will be better able to picture problems with data races and acquire the basic
    knowledge needed to design concurrent algorithms and data structures. It might also
    be helpful for your general design skills to build fault-tolerant systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding data races and lock-based solutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using atomics in C++ code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing lock-free data structures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The g++ compiler with the `-std=c++2a` option is used to compile the examples
    in this chapter. You can find the source files used in this chapter at [https://github.com/PacktPublishing/Expert-CPP](https://github.com/PacktPublishing/Expert-CPP)
    .
  prefs: []
  type: TYPE_NORMAL
- en: A closer look at data races
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As already stated many times, data races are situations programmers try to avoid
    at all costs. In the previous chapter, we discussed a deadlock and ways to avoid
    it. The last example that we used in the previous chapter was making a thread-safe
    singleton pattern. Let's suppose we use a class for creating database connections
    (a classic example).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a simple implementation of the pattern that tracks down the connections
    to the database. Keeping a separate connection each time we need access to the
    database is not a good practice. Instead, we reuse the existing connection for
    querying the database from different parts of the program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s discuss that example in more detail. In the previous chapter, we incorporated
    locking to protect the `get_instance()` function from data races. Let''s illustrate
    in detail why we did so. To simplify this for the example, here are the four lines
    of interest to us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, imagine that we run a thread that accesses the `get_instance()` function.
    We name it `Thread A` and the first line that it executes is the conditional statement,
    as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'It will execute the instructions line by line. What interests us more is the
    second thread (marked as `Thread B`), which starts executing the function concurrent
    to `Thread A`. The following situation might arise during the concurrent execution
    of the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '`Thread B` gets a positive result when it compares `instance_` against `nullptr`.
    `Thread A` has passed the same check and sets `instance_` to a new object. While
    from the perspective of `Thread A` everything looks fine, it just passed the conditional
    check, resets `instances`, and will move on to the next line to return `instance_`.
    However, `Thread B` compared `instance_` right before it had its value changed.
    Therefore, `Thread B` also moves on to setting the value of `instance_`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The preceding issue is that `Thread B` resets `instance_` after it has already
    been set. Also, we view `get_instance()` as a single operation; it consists of
    several instructions, each of which is executed sequentially by a thread. For
    two threads not to interfere with each other, the operation shouldn't consist
    of more than one instruction.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reason why we are concerned with data races is the gap pictured in the
    code block preceding. That gap between the lines is something that allows threads
    to interfere with each other. When you design a solution using a synchronization
    primitive, such as a mutex, you should picture all the gaps that you miss because
    the solution might not be the correct one. The following modification uses mutex
    and the `double-checked` locking pattern discussed in the previous chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s what happens when two threads try to access the `instance_` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, even when both threads pass the first check, one of them locks the mutex.
    While one of the threads might try to lock the mutex, the other will reset the
    instance. To make sure it''s not already set, we use the second check (that''s
    why it''s called **double-checked locking**):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'When `Thread A` finishes setting `instance_`, it then unlocks the mutex so
    `Thread B` can move on with locking and resetting `instance_`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As a rule of thumb, you should always look between the lines of the code. There
    is always a gap between two statements, and that gap will make two or more threads
    interfere with each other. The next section discusses a classic example of incrementing
    a number in detail.
  prefs: []
  type: TYPE_NORMAL
- en: A synchronized increment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Almost every book touching on the topic of thread synchronization uses the
    classic example of incrementing a number as a data racing example. This book is
    not an exception. The example follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We added a couple more threads to make the example more complex. The preceding
    code does nothing more than increment the `counter` variable using four different
    threads. At first glance, at any point in time, only one of the threads increments
    `counter`. However, as we mentioned in the previous section, we should be attentive
    and look for gaps in the code. The `foo()` function seems to be missing one. The
    increment operator behaves in the following way (as pseudocode):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have discovered gaps where there weren''t supposed to be any. So now,
    at any point in time, only one thread executes one of the three preceding instructions.
    That is, something like the following is possible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'So, for example, `thread B` might modify the value of `counter` while `thread
    A` reads its previous value. That means `thread A` will assign a new increment
    value to `counter` when it has been already done by `thread B`. The confusion
    introduces chaos and, sooner or later, our brains will explode trying to understand
    the ordering of operations. As a classic example, we''ll move on to solving it
    by using thread-locking mechanisms. Here''s a popular solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Whichever thread arrives at `lock_guard` first locks `mutex`, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The problem with using locking is performance. In theory, we use threads to
    speed up program execution, more specifically, data processing. In the case of
    big collections of data, using multiple threads might increase the program's performance
    drastically. However, in a multithreaded environment, we take care of concurrent
    access first because accessing the collection with multiple threads might lead
    to its corruption. For example, let's look at a thread-safe stack implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a thread-safe stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recall the stack data structure adapter from [Chapter 6](92a352fd-cfe4-4d57-8005-ef618534ff74.xhtml),
    *Digging into Data Structures and Algorithms in STL*. We are going to implement
    a thread-safe version of the stack using locks. The stack has two basic operations,
    `push` and `pop`. Both of them modify the state of the container. As you know,
    the stack is not a container itself; it''s an adapter that wraps a container and
    provides an adapted interface to access. We will wrap `std::stack` in a new class
    by incorporating thread-safety. Besides construction and destruction functions,
    `std::stack` provides the following functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`top()`: Accesses the top element of the stack'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`empty()`: Returns true if the stack is empty'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`size()`: Returns the current size of the stack'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`push()`: Inserts a new item into the stack (at the top)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`emplace()`: Constructs an element in place at the top of the stack'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pop()`: Removes the top element of the stack'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`swap()`: Swaps the contents with another stack'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will keep it simple and concentrate on the idea of thread-safety rather
    than making a powerful full-featured stack. The main concerns here are functions
    that modify the underlying data structure. Our interest lies in the `push()` and
    `pop()` functions. Those are functions that might corrupt the data structure if
    several threads interfere with each other. So, the following declaration is the
    class representing a thread-safe stack:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Note that we declared `mutex_` as mutable because we locked it in the `empty()`
    const function. It's arguably a better design choice than removing the const-ness
    of `empty()`. However, you should know by now that using a mutable for any of
    the data members suggests that we have made bad design choices. Anyway, the client
    code for `safe_stack` won't care much about the inner details of the realization;
    it doesn't even know that the stack uses a mutex to synchronize concurrent access.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now look at the implementation of its member functions along with a
    short description. Let''s start with the copy constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Note that we locked the mutex of the other stack. As unfair as it might seem,
    we need to make sure that the underlying data of the other stack won't get modified
    while we make a copy of it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s look at the implementation of the `push()` function. It''s obviously
    simple; we lock the mutex and push the data into the underlying stack:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Almost all functions incorporate thread synchronization in the same way: locking the
    mutex, doing the job, and unlocking the mutex. This ensures that only one thread
    is accessing the data at any one time. That said, to protect data from race conditions,
    we must ensure that the function invariants aren''t broken.'
  prefs: []
  type: TYPE_NORMAL
- en: If you are not a fan of typing long C++ type names such as `std::lock_guard<std::mutex>`,
    use the `using` keyword to make short aliases for types, for example, using `locker
    = std::guard<std::mutex>;`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, moving on to the `pop()` function, we can modify the class declaration
    to make `pop()` directly return the value at the top of the stack. We do this
    mostly because we don''t want someone to access the top of the stack (with a reference)
    and then pop that data from within another thread. So, we will modify the `pop()` function
    to make a shared object and then return the stack element:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Note that the declaration of the `safe_stack` class should also change according
    to the `pop()` function modifications. Also, we don't need `top()` anymore.
  prefs: []
  type: TYPE_NORMAL
- en: Designing lock-free data structures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If at least one thread is guaranteed to make progress, then we say it's a lock-free
    function. Compared to lock-based functions, where one thread can block another
    and they both might wait for some condition before making progress, a lock-free
    state ensures progress is made by at least one of the threads. We say that algorithms
    and data structures using data synchronization primitives are blocking, that is,
    a thread is suspended until another thread performs an action. That means the
    thread can't make progress until the block is removed (typically, unlocking a
    mutex). Our interest lies in data structures and algorithms that don't use blocking
    functions. We call some of them lock-free, although we should make a distinction
    between the types of non-blocking algorithms and data structures.
  prefs: []
  type: TYPE_NORMAL
- en: Using atomic types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Earlier in the chapter, we introduced the gaps between lines of source code
    as the reason for data races. Whenever you have an operation that consists of
    more than one instruction, your brain should alert you about a possible issue.
    However, it doesn't matter how much you strive to make operations independent
    and singular; most of the time, you can't achieve anything without breaking operations
    into steps involving multiple instructions. C++ comes to the rescue by providing
    atomic types.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s understand why the word atomic is used. In general, we understand
    atomic to mean something that can''t be broken down into smaller parts. That is,
    an atomic operation is an operation that can''t be half-done: it''s done or it
    isn''t. An example of an atomic operation might be the simple assignment of an
    integer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: If two threads access this line of code, neither of them can encounter it half-done.
    In other words, there are no gaps between the assignment. Of course, the same
    statement might have a lot of gaps if `num` represents a complex object with a
    user-defined assignment operator.
  prefs: []
  type: TYPE_NORMAL
- en: An atomic operation is an indivisible operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, a non-atomic operation might be seen as half-done. The classic
    example is the increment operation that we discussed earlier. In C++, all operations
    on atomic types are also atomic. That means we can avoid gaps between lines by
    using atomic types. Before using atomics, we could create atomic operations by
    using mutexes. For example, we might consider the following function atomic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The difference between a real atomic operation and the fake one we just made
    is that atomic operations don't require locks. That's actually a big difference,
    because synchronization mechanisms such as mutexes incorporate overhead and performance
    penalties. To be more precise, atomic types leverage lower-level mechanisms to
    ensure the independent and atomic execution of instructions. The standard atomic
    types are defined in the `<atomic>` header. However, standard atomic types might
    also use internal locking. To make sure they don't use internal locking, all atomic
    types in the standard library expose the `is_lock_free()` function.
  prefs: []
  type: TYPE_NORMAL
- en: The only atomic type that doesn't have the `is_lock_free()` member function
    is `std::atomic_flag`. The operations on this type are required to be lock-free.
    It's a Boolean flag and most of the time it is used as a base to implement other
    lock-free types.
  prefs: []
  type: TYPE_NORMAL
- en: 'That said, `obj.is_lock_free()` returns `true` if operations on `obj` are done
    directly with atomic instructions. If it returns false, it means internal locking
    is used. There is more: the `static constexpr` function `is_always_lock_free()`
    returns `true` if the atomic type is lock-free for all supported hardware. As
    the function is `constexpr`, it allows us to define whether the type is lock-free
    at compile time. That''s a big advancement and affects the organization and execution
    of the code in a good way. For example, `std::atomic<int>::is_always_lock_free()`
    returns `true` as `std::atomic<int>` is most probably always lock-free.'
  prefs: []
  type: TYPE_NORMAL
- en: In Greek, a means not and tomo means cut. The word atom comes from the Greek
    atomos, which translates to uncuttable. That is, by atomic we consider indivisible
    smallest units. We use atomic types and operations to avoid gaps between instructions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use specializations for atomic types, for example, `std::atomic<long>`;
    however, you can refer to the following table for more convenient names for atomic
    types. The left-hand column of the table contains the atomic type and the right-hand
    column contains its specialization:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Atomic type** | **Specialization** |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_bool` | `std::atomic<bool>` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_char` | `std::atomic<char>` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_schar` | `std::atomic<signed char>` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_uchar` | `std::atomic<unsigned char>` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_int` | `std::atomic<int>` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_uint` | `std::atomic<unsigned>` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_short` | `std::atomic<short>` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_ushort` | `std::atomic<unsigned short>` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_long` | `std::atomic<long>` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_ulong` | `std::atomic<unsigned long>` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_llong` | `std::atomic<long long>` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_ullong` | `std::atomic<unsigned long long>` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_char16_t` | `std::atomic<char16_t>` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_char32_t` | `std::atomic<char32_t>` |'
  prefs: []
  type: TYPE_TB
- en: '| `atomic_wchar_t` | `std::atomic<wchar_t>` |'
  prefs: []
  type: TYPE_TB
- en: The preceding table represents basic atomic types. The fundamental difference
    between a regular type and, an atomic type is the kind of operations we can apply
    to them. Let's now discuss atomic operations in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Operations on atomic types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recall the gaps we were discussing in the previous section. The goal of atomic
    types is to either eliminate gaps between instructions or provide operations that
    take care of combining several instructions together wrapped as a single instruction.
    The following are operations on atomic types:'
  prefs: []
  type: TYPE_NORMAL
- en: '`load()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`store()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`exchange()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`compare_exchange_weak()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`compare_exchange_strong()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`wait()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`notify_one()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`notify_all()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `load()` operation atomically loads and returns the value of the atomic
    variable. `store()` atomically replaces the value of the atomic variable with
    the provided non-atomic argument.
  prefs: []
  type: TYPE_NORMAL
- en: 'Both `load()` and `store()` are similar to regular read and assign operations
    for non-atomic variables. Whenever we access the value of an object, we execute
    a read instruction. For example, the following code prints the contents of the
    `double` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'In the case of atomic types, a similar read operation is transformed into this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Although the preceding code bears no meaning, we included the example to represent
    the differences in treating atomic types. Accessing atomic variables should be
    done through atomic operations. The following code represents definitions for
    the `load()`, `store()`, and `exchange()` functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, there is the additional parameter named `order` of type `std::memory_order`.
    We will describe it shortly. The `exchange()` function comprises the `store()`
    and `load()` functions in a way that atomically replaces the value with the provided
    argument and atomically obtains the previous value.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `compare_exchange_weak()` and `compare_exchange_strong()` functions work
    similarly to each other. Here''s how they are defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: They compare the first argument (`expected_value`) with the atomic variable
    and if they are equal, replace the variable with the second argument (`target_value`).
    Otherwise, they atomically load the value into the first argument (that's why
    it is passed by reference). The difference between weak and strong exchanges is
    that `compare_exchange_weak()` is allowed to fail falsely (called a **spurious
    failure**), that is, even when `expected_value` is equal to the underlying value,
    the function treats them as not equal. That's done because on some platforms it
    leads to increased performance.
  prefs: []
  type: TYPE_NORMAL
- en: The `wait()`, `notify_one()`, and `notify_all()` functions have been added since
    C++20\. The `wait()` function blocks the thread until the value of the atomic
    object modifies. It takes an argument to compare with the value of the atomic
    object. If the values are equal, it blocks the thread. To manually unblock the
    thread, we can call `notify_one()` or `notify_all()`. The difference between them
    is that `notify_one()` unblocks at least one blocked operation, while `notify_all()`
    unblocks all such operations.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's discuss the memory order that we encountered in the atomic type member
    functions declared previously. `std::memory_order` defines the order of memory
    accesses around atomic operation. When multiple threads simultaneously read and
    write to variables, a thread can read the changes in an order different from the
    order in which another thread stored them. The default order for atomic operations
    is sequentially consistent ordering – that's where `std::memory_order_seq_cst`
    comes in. There are several types of orders, including `memory_order_relaxed`,
    `memory_order_consume`, `memory_order_acquire`, `memory_order_release`, `memory_order_acq_rel`,
    and `memory_order_seq_cst`. In the next section, we'll design a lock-free stack
    that uses atomic types with the default memory order.
  prefs: []
  type: TYPE_NORMAL
- en: Designing a lock-free stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the key things to keep in mind when designing a stack is to ensure that
    a pushed value is safe to return from another thread. Also important is ensuring
    that only one thread returns a value.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous sections, we implemented a lock-based stack that wrapped `std::stack`.
    We know that a stack is not a real data structure but an adapter. Usually, when
    implementing a stack, we choose either a vector or a linked list as its underlying
    data structure. Let's look at an example of a lock-free stack based on a linked
    list. Pushing a new element into the stack involves creating a new list node,
    setting its `next` pointer to the current `head` node, and then setting the `head`
    node to point to the newly inserted node.
  prefs: []
  type: TYPE_NORMAL
- en: If you are confused by the terms head or next pointer, revisit [Chapter 6](92a352fd-cfe4-4d57-8005-ef618534ff74.xhtml),
    *Digging into Data Structures and Algorithms in STL*, where we discussed linked
    lists in detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a single-threaded context, the steps described are fine; however, if there
    is more than one thread modifying the stack, we should start worrying. Let''s
    find the pitfalls of the `push()` operation. Here are the three main steps happening
    when a new element is pushed into the stack:'
  prefs: []
  type: TYPE_NORMAL
- en: '`node* new_elem = new node(data);`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`new_elem->next = head_;`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`head_ = new_elem;`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the first step, we declare the new node that will be inserted into the underlying
    linked list. The second step describes that we are inserting it at the front of
    the list – that's why the new node's `next` pointer points to `head_`. Finally,
    as the `head_` pointer represents the starting point of the list, we should reset
    its value to point to the newly added node, as done in step 3.
  prefs: []
  type: TYPE_NORMAL
- en: 'The node type is the internal struct that we use in the stack for representing
    a list node. Here''s how it is defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The first thing that we suggest you do is look for gaps in the code – not in
    the preceding code, but in the steps we described when pushing a new element into
    the stack. Take a closer look at it. Imagine two threads are adding nodes at the
    same time. One thread at step 2 sets the next pointer of the new element to point
    to `head_`. The other thread makes `head_` point to the other new element. It''s
    already obvious that this might lead to data corruption. It''s crucial for a thread
    to have the same `head_` for both steps 2 and 3. To solve the race condition between
    steps 2 and 3, we should use an atomic compare/exchange operation to guarantee
    that `head_` wasn''t modified when we read its value previously. As we need to
    access the head pointer atomically, here''s how we modify `head_ member` in the `lock_free_stack`
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s how we implement a lock-free `push()` around the atomic `head_` pointer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: We use `compare_exchange_weak()` to ensure that the `head_` pointer has the
    same value as we stored in `new_elem->next`. If it is, we set it to `new_elem`.
    Once `compare_exchange_weak()` succeeds, we are sure the node has been successfully
    inserted into the list.
  prefs: []
  type: TYPE_NORMAL
- en: 'See how we access nodes by using atomic operations. The atomic form of a pointer
    of type `T` - `std::atomic<T*>` - provides the same interface. Besides that, `std::atomic<T*>`
    provides pointer to the arithmetic operations `fetch_add()` and `fetch_sub()`.
    They do atomic addition and subtraction on the stored address. Here''s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: We intentionally named the pointer `old`, because `fetch_add()` adds the number
    to the address of the pointer and returns the `old` value. That's why `old` points
    to the same address that `arr` points to.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next section, we will introduce more operations available on atomic
    types. Now, let''s get back to our lock-free stack. To `pop()` an element, that
    is, to remove a node, we need to read `head_` and set it to the next element of
    `head_`, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, take a good look at the preceding code. Imagine several threads executing
    it concurrently. What if two threads removing items from the stack read the same
    value of `head_`? This and a couple of other race conditions lead us to the following
    implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: We applied almost the same logic in the preceding code as we did with the `push()`
    function. The preceding code isn't perfect; it should be enhanced. We suggest
    you make the effort to modify it to eliminate memory leaks.
  prefs: []
  type: TYPE_NORMAL
- en: We have seen that lock-free implementations rely heavily on atomic types and
    operations. The operations we discussed in the previous section are not final.
    Let's now discover some more atomic operations.
  prefs: []
  type: TYPE_NORMAL
- en: More operations on atomics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, we used `std::atomic<>` on a pointer to a user-defined
    type. That is, we declared the following structure for the list node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The node struct is a user-defined type. Although in the previous section we
    instantiated `std::atomic<node*>`, in the same way, we can instantiate `std::atomic<>`
    for almost any user-defined type, that is, `std::atomic<T>`. However, you should
    note that the interface of `std::atomic<T>` is limited to the following functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`load()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`store()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`exchange()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`compare_exchange_weak()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`compare_exchange_strong()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`wait()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`notify_one()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`notify_all()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's now look at the complete list of operations available on atomic types
    based on the specifics of the underlying type.
  prefs: []
  type: TYPE_NORMAL
- en: '`std::atomic<>` instantiated with an integral type (such as an integer or a
    pointer) has the following operations along with the ones we listed previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '`fetch_add()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fetch_sub()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fetch_or()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fetch_and()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fetch_xor()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Also, besides increment (`++`) and decrement (`--`), the following operators
    are also available: `+=`, `-=`, `|=`, `&=`, and `^=`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, there is a special atomic type called `atomic_flag` with two available
    operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '`clear()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`test_and_set()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You should consider `std::atomic_flag` a bit with atomic operations. The `clear()`
    function clears it, while `test_and_set()` changes the value to `true` and returns
    the previous value.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced a rather simple example of designing a stack.
    There are more complex examples to research and follow. When we discussed designing
    a concurrent stack, we looked at two versions, one of them representing a lock-free
    stack. Compared to lock-based solutions, lock-free data structures and algorithms
    are the ultimate goal for programmers as they provide mechanisms to avoid data
    races without even synchronizing the resources.
  prefs: []
  type: TYPE_NORMAL
- en: We also introduced atomic types and operations that you can use in your projects
    to make sure instructions are indivisible. As you already know, if an instruction
    is atomic, there is no need to worry about its synchronization. We strongly suggest
    you continue researching the topic and build more robust and complex lock-free
    data structures. In the next chapter, we will see how to design world-ready applications.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why did we check the instance twice in the multithreaded singleton implementation?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the implementation of the lock-based stack's copy constructor, we locked
    the mutex of the other stack. Why?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are atomic types and atomic operations?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we use `load()` and `store()` for atomic types?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What additional operations are supported on `std::atomic<T*>`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Concurrent Patterns and Best Practices by Atul Khot*, at[ https://www.packtpub.com/application-development/concurrent-patterns-and-best-practices](https://www.packtpub.com/application-development/concurrent-patterns-and-best-practices)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Mastering C++ Multithreading by Maya Posch*, at [https://www.packtpub.com/application-development/mastering-c-multithreading](https://www.packtpub.com/application-development/mastering-c-multithreading)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
