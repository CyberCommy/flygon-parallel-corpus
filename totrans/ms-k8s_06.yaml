- en: Using Critical Kubernetes Resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will design a massive-scale platform that will challenge
    Kubernetes' capabilities and scalability. The Hue platform is all about creating
    an omniscient and omnipotent digital assistant. Hue is a digital extension of
    you. It will help you do anything, find anything, and, in many cases, will do
    a lot on your behalf. It will obviously need to store a lot information, integrate
    with many external services, respond to notifications and events, and be smart
    about interacting with you.
  prefs: []
  type: TYPE_NORMAL
- en: We will take the opportunity in this chapter to get to know Kubectl and other
    related tools a little better, and will explore in detail resources that we've
    seen before, such as pods, as well as new resources, such as **jobs**. At the
    end of this chapter, you will have a clear picture of how impressive Kubernetes
    is and how it can be used as the foundation for hugely complex systems.
  prefs: []
  type: TYPE_NORMAL
- en: Designing the Hue platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will set the stage and define the scope of the amazing Hue
    platform. Hue is not Big Brother, Hue is Little Brother! Hue will do whatever
    you allow it to do. It will be able to do a lot, but some people might be concerned,
    so you get to pick how much or how little Hue can help you with. Get ready for
    a wild ride!
  prefs: []
  type: TYPE_NORMAL
- en: Defining the scope of Hue
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Hue will manage your digital persona. It will know you better than you know
    yourself. Here is a list of some of the services which Hue can manage and help
    you with:'
  prefs: []
  type: TYPE_NORMAL
- en: Search and content aggregation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Medical
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Smart home
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finance-bank, savings, retirement, investing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Office
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Social
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Travel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wellbeing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Family
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Smart reminders and notifications**: Let''s think of the possibilities. Hue
    will know you, but also know your friends and the aggregate of other users across
    all domains. Hue will update its models in real time. It will not be confused
    by stale data. It will act on your behalf, present relevant information, and learn
    your preferences continuously. It can recommend new shows or books that you may
    like, make restaurant reservations based on your schedule and your family or friends,
    and control your home automation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security, identity, and privacy**: Hue is your proxy online. The ramifications
    of someone stealing your Hue identity, or even just eavesdropping on your Hue
    interaction, are devastating. Potential users may even be reluctant to trust the
    Hue organization with their identity. Let''s devise a non-trust system where users
    have the power to pull the plug on Hue at any time. Here are a few ideas in the
    right direction:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strong identity through a dedicated device with multi-factor authorization,
    including multiple biometric reasons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frequently rotating credentials
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quick service pause and identity re-verification of all external services (will
    require original proof of identity to each provider)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Hue backend will interact with all external services through short-lived
    tokens
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Architecting Hue as a collection of loosely-coupled microservices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hue's architecture will need to support enormous variation and flexibility.
    It will also need to be very extensible where existing capabilities and external
    services are constantly upgraded, and new capabilities and external services are
    integrated into the platform. That level of scale calls for microservices, where
    each capability or service is totally independent of other services except for
    well-defined interfaces through standard and/or discoverable APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Hue components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before embarking on our microservice journey, let's review the types of component
    we need to construct for Hue.
  prefs: []
  type: TYPE_NORMAL
- en: '**User profile**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The user profile is a major component, with lots of sub-components. It is the
    essence of the user, their preferences, history across every area, and everything
    that Hue knows about them.
  prefs: []
  type: TYPE_NORMAL
- en: '**User graph**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The user graph component models networks of interactions between users across
    multiple domains. Each user participates in multiple networks: social networks
    such as Facebook and Twitter, professional networks, hobby networks, and volunteering
    communities. Some of these networks are ad hoc, and Hue will be able to structure
    them to benefit users. Hue can take advantage of the rich profiles it has of user
    connections to improve interactions even without exposing private information.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Identity**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identity management is critical, as mentioned previously, so it deserves a separate
    component. A user may prefer to manage multiple mutually exclusive profiles with
    separate identities. For example, maybe users are not comfortable with mixing
    their health profile with their social profile because of the risk of inadvertently
    exposing personal health information to their friends.
  prefs: []
  type: TYPE_NORMAL
- en: '**Authorizer**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The authorizer is a critical component where the user explicitly authorizes
    Hue to perform certain actions or collect various data on its behalf. This includes
    access to physical devices, accounts of external services, and level of initiative.
  prefs: []
  type: TYPE_NORMAL
- en: '**External service**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hue is an aggregator of external services. It is not designed to replace your
    bank, your health provider, or your social network. It will keep a lot of metadata
    about your activities, but the content will remain with your external services.
    Each external service will require a dedicated component to interact with the
    external service API and policies. When no API is available, Hue emulates the
    user by automating the browser or native apps.
  prefs: []
  type: TYPE_NORMAL
- en: '**Generic sensor**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A big part of Hue's value proposition is to act on the user's behalf. In order
    to do that effectively, Hue needs to be aware of various events. For example,
    if Hue reserved a vacation for you but it senses that a cheaper flight is available,
    it can either automatically change your flight or ask you for confirmation. There
    is an infinite number of things to sense. To reign in sensing, a generic sensor
    is needed. A generic sensor will be extensible, but exposes a generic interface
    that the other parts of Hue can utilize uniformly even as more and more sensors
    are added.
  prefs: []
  type: TYPE_NORMAL
- en: '**Generic actuator**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the counterpart of the generic sensor. Hue needs to perform actions
    on your behalf, such as reserving a flight. To do that, Hue needs a generic actuator
    that can be extended to support particular functions but can interact with other
    components, such as the identity manager and the authorizer, in a uniform fashion.
  prefs: []
  type: TYPE_NORMAL
- en: '**User learner**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the brain of Hue. It will constantly monitor all of your interactions
    (that you authorize) and update its model of you. This will allow Hue to become
    more and more useful over time, predict what you need and what will interest you,
    provide better choices, surface more relevant information at the right time, and
    avoid being annoying and overbearing.
  prefs: []
  type: TYPE_NORMAL
- en: Hue microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The complexity of each of the components is enormous. Some of the components,
    such as the external service, the generic sensor, and generic actuator, will need
    to operate across hundreds, thousands, or more external services that constantly
    change outside the control of Hue. Even the user learner needs to learn the user''s
    preferences across many areas and domains. Microservices address this need by
    allowing Hue to evolve gradually and grow more isolated capabilities without collapsing
    under its own complexity. Each microservice interacts with generic Hue infrastructure
    services through standard interfaces and, optionally, with a few other services
    through well-defined and versioned interfaces. The surface area of each microservice
    is manageable, and the orchestration between microservices is based on standard
    best practices:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Plugins**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Plugins are the key to extending Hue without a proliferation of interfaces.
    The thing about plugins is that you often need plugin chains that cross multiple
    abstraction layers. For example, if we want to add a new integration for Hue with
    YouTube, then you can collect a lot of YouTube-specific information: your channels,
    favorite videos, recommendations, and videos you have watched. To display this
    information to users and allow them to act on it, you need plugins across multiple
    components and eventually in the user interface as well. Smart design will help
    by aggregating categories of actions such as recommendations, selections, and
    delayed notifications to many different services.'
  prefs: []
  type: TYPE_NORMAL
- en: The great thing about plugins is that they can be developed by anyone. Initially,
    the Hue development team will have to develop the plugins, but as Hue becomes
    more popular, external services will want to integrate with Hue and build Hue
    plugins to enable their service.
  prefs: []
  type: TYPE_NORMAL
- en: That will lead, of course, to a whole ecosystem of plugin registration, approval,
    and curation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data stores**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hue will need several types of data store, and multiple instances of each type,
    to manage its data and metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: Relational database
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Graph database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time-series database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In-memory caching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Due to the scope of Hue, each one of these databases will have to be clustered
    and distributed.
  prefs: []
  type: TYPE_NORMAL
- en: '**Stateless microservices**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The microservices should be mostly stateless. This will allow specific instances
    to be started and killed quickly, and migrated across the infrastructure as necessary.
    The state will be managed by the stores and accessed by the microservices with
    short-lived access tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '**Queue-based interactions**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All these microservices need to talk to each other. Users will ask Hue to perform
    tasks on their behalf. External services will notify Hue of various events. Queues
    coupled with stateless microservices provide the perfect solution. Multiple instances
    of each microservice will listen to various queues and respond when relevant events
    or requests are popped from the queue. This arrangement is very robust and easy
    to scale. Every component can be redundant and highly available. While each component
    is fallible, the system is very fault-tolerant.
  prefs: []
  type: TYPE_NORMAL
- en: A queue can be used for asynchronous RPC or request-response style interactions
    too, where the calling instance provides a private queue name and the callee posts
    the response to the private queue.
  prefs: []
  type: TYPE_NORMAL
- en: Planning workflows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hue often needs to support workflows. A typical workflow will get a high-level
    task, such as making a dentist appointment; it will extract the user's dentist
    details and schedule, match it with the user's schedule, choose between multiple
    options, potentially confirm with the user, make the appointment, and set up a
    reminder. We can classify workflows into fully automatic and human workflows where
    humans are involved. Then there are workflows that involve spending money.
  prefs: []
  type: TYPE_NORMAL
- en: Automatic workflows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Automatic workflows don't require human intervention. Hue has full authority
    to execute all the steps from start to finish. The more autonomy the user allocates
    to Hue, the more effective it will be. The user should be able to view and audit
    all workflows, past and present.
  prefs: []
  type: TYPE_NORMAL
- en: Human workflows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Human workflows require interaction with a human. Most often it will be the
    user that needs to make a choice from multiple options or approve an action, but
    it may involve a person on another service. For example, to make an appointment
    with a dentist, you may have to get a list of available times from the secretary.
  prefs: []
  type: TYPE_NORMAL
- en: Budget-aware workflows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some workflows, such as paying bills or purchasing a gift, require spending
    money. While, in theory, Hue can be granted unlimited access to the user's bank
    account, most users will probably be more comfortable with setting budgets for
    different workflows or just making spending a human-approved activity.
  prefs: []
  type: TYPE_NORMAL
- en: Using Kubernetes to build the Hue platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will look at various Kubernetes resources and how they can
    help us build Hue. First, we'll get to know the versatile Kubectl a little better,
    then we will look at running long-running processes in Kubernetes, exposing services
    internally and externally, using namespaces to limit access, launching ad hoc
    jobs, and mixing in non-cluster components. Obviously, Hue is a huge project,
    so we will demonstrate the ideas on a local Minikube cluster and not actually
    build a real Hue Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Using Kubectl effectively
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kubectl is your Swiss Army knife. It can do pretty much anything around the
    cluster. Under the hood, Kubectl connects to your cluster through the API. It
    reads your `.kube/config` file, which contains information necessary to connect
    to your cluster or clusters. The commands are divided into multiple categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Generic commands**: Deal with resources in a generic way: `create`, `get`,
    `delete`, `run`, `apply`, `patch`, `replace`, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cluster management commands**: Deal with nodes and the cluster at large:
    `cluster-info`, `certificate`, `drain`, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Troubleshooting commands**: `describe`, `logs`, `attach`, `exec`, and so
    on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deployment commands**: Deal with deployment and scaling: `rollout`, `scale`,
    `auto-scale`, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Settings commands**: Deal with labels and annotations: `label`, `annotate`,
    and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You can view the configuration with Kubernetes `config view`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the configuration for a Minikube cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Understanding Kubectl resource configuration files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Many Kubectl operations, such as `create`, require complicated hierarchical
    output (since the API requires this output). Kubectl uses YAML or JSON configuration
    files. Here is a JSON configuration file for creating a pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '`apiVersion`: The very important Kubernetes API keeps evolving and can support
    different versions of the same resource through different versions of the API.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kind`: `kind` tells Kubernetes what type of resource it is dealing with, in
    this case, `pod`. This is always required.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metadata`: This is a lot of information that describes the pod and where it
    operates:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`name`: Identifies the pod uniquely within its namespace'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels`: Multiple labels can be applied'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`namespace`: The namespace the pod belongs to'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`annotations`: A list of annotations available for query'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spec`: `spec` is a pod template that contains all of the information necessary
    to launch a pod. It can be quite elaborate, so we''ll explore it in multiple parts:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '`Container spec`: The pod spec''s container is a list of container specs. Each
    container spec has the following structure:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Each container has an image, a command that, if specified, replaces the Docker
    image command. It also has arguments and environment variables. Then, there are,
    of course, the image pull policy, ports, and resource limits. We covered those
    in earlier chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying long-running microservices in pods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Long-running microservices should run in pods and be stateless. Let's look at
    how to create pods for one of Hue's microservices. Later, we will raise the level
    of abstraction and use a deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Creating pods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start with a regular pod configuration file for creating a Hue learner
    internal service. This service doesn't need to be exposed as a public service,
    and it will listen to a queue for notifications and store its insights in some
    persistent storage.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need a simple container that the pod will run in. Here is possibly the simplest
    Docker file ever, which will simulate the Hue learner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: It uses the `busybox` base image, prints to standard output `Started...` and
    then goes into an infinite loop, which is, by all accounts, long-running.
  prefs: []
  type: TYPE_NORMAL
- en: I have built two Docker images tagged as `g1g1/hue-learn:v3.0` and `g1g1/hue-learn:v4.0`
    and pushed them to the Docker Hub registry (`g1g1` is my user name).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Now, these images are available to be pulled into containers inside of Hue's
    pods.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll use YAML here because it''s more concise and human-readable. Here are
    the boilerplate and `metadata` labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The reason I use an annotation for the version and not a label is that labels
    are used to identify the set of pods in the deployment. Modifying labels is not
    allowed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next comes the important `containers` spec, which defines for each container
    the mandatory `name` and `image`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The resources section tells Kubernetes the resource requirements of the container,
    which allows for more efficient and compact scheduling and allocations. Here,
    the container requests `200` milli-cpu units (0.2 core) and `256` MiB:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The environment section allows the cluster administrator to provide environment
    variables that will be available to the container. Here it tells it to discover
    the queue and the store through `dns`. In a testing environment, it may use a
    different discovery method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Decorating pods with labels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Labeling pods wisely is key for flexible operations. It lets you evolve your
    cluster live, organize your microservices into groups that you can operate on
    uniformly, and drill down in an ad hoc manner to observe different subsets.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, our Hue learner pod has the following labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Runtime-environment**: Production'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tier**: Internal-service'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The version annotation can be used to support running multiple versions at the
    same time. If both version 2 and version 3 need to run at the same time, either
    to provide backward compatibility or just temporarily during the migration from
    `v2` to `v3`, then having a version annotation or label allows both scaling pods
    of different versions independently and exposing services independently. The `runtime-environment`
    label allows performing global operations on all pods that belong to a certain
    environment. The `tier` label can be used to query all pods that belong to a particular
    tier. These are just examples; your imagination is the limit here.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying long-running processes with deployments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a large-scale system, pods should never be just created and let loose. If
    a pod dies unexpectedly for whatever reason, you want another one to replace it
    to maintain overall capacity. You can create replication controllers or replica
    sets yourself, but that leaves the door open to mistakes as well as the possibility
    of partial failure. It makes much more sense to specify how many replicas you
    want when you launch your pods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s deploy three instances of our Hue learner microservice with a Kubernetes
    deployment resource. Note that deployment objects became stable at Kubernetes
    1.9:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The `pod spec` is identical to the `spec` section from the pod configuration
    file that we used previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create the deployment and check its status:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: You can get a lot more information about the deployment using the `kubectl describe`
    command.
  prefs: []
  type: TYPE_NORMAL
- en: Updating a deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Hue platform is a large and ever-evolving system. You need to upgrade constantly.
    Deployments can be updated to roll out updates in a painless manner. You change
    the pod template to trigger a rolling update which is fully managed by Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently, all the pods are running with version 3.0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s update the deployment to upgrade to version 4.0\. Modify the image version
    in the deployment file. Don''t modify labels; it will cause an error. Typically,
    you modify the image and some related metadata in annotations. Then we can use
    the `apply` command to upgrade the version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Separating internal and external services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Internal services are services that are accessed directly only by other services
    or jobs in the cluster (or administrators that log in and run ad hoc tools). In
    some cases, internal services are not accessed at all, and just perform their
    function and store their results in a persistent store that other services access
    in a decoupled way.
  prefs: []
  type: TYPE_NORMAL
- en: 'But some services need to be exposed to users or external programs. Let''s
    look at a fake Hue service that manages a list of reminders for a user. It doesn''t
    really do anything, but we''ll use it to illustrate how to expose services. I
    pushed a dummy `hue-reminders` image (the same as `hue-learn`) to Docker Hub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Deploying an internal service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is the deployment, which is very similar to the Hue-learner deployment,
    except that I dropped the `annotations`, `env`, and `resources` sections, kept
    just one label to save space, and added a `ports` section to the container. That''s
    crucial, because a service must expose a port through which other services can
    access it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run the deployment, two Hue `reminders` pods are added to the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'OK, the pods are running. In theory, other services can look up or be configured
    with their internal IP address and just access them directly because they are
    all in the same network space. But this doesn''t scale. Every time a reminders
    pod dies and is replaced by a new one, or when we just scale up the number of
    pods, all the services that access these pods must know about it. Services solve
    this issue by providing a single access point to all the pods. The service is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The service has a selector that selects all the pods that have labels that match
    it. It also exposes a port, which other services will use to access it (it doesn't
    have to be the same port as the container's port).
  prefs: []
  type: TYPE_NORMAL
- en: Creating the hue-reminders service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s create the service and explore it a little bit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The service is up and running. Other pods can find it through environment variables
    or DNS. The environment variables for all services are set at pod creation time.
    That means that if a pod is already running when you create your service, you''ll
    have to kill it and let Kubernetes recreate it with the environment variables
    (you create your pods through a deployment, right?):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'But using DNS is much simpler. Your service DNS name is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Exposing a service externally
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The service is accessible inside the cluster. If you want to expose it to the
    world, Kubernetes provides two ways to do it:'
  prefs: []
  type: TYPE_NORMAL
- en: Configure `NodePort` for direct access
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configure a cloud load balancer if you run it in a cloud environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Before you configure a service for external access, you should make sure it
    is secure. The Kubernetes documentation has a good example that covers all the
    gory details here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/kubernetes/examples/blob/master/staging/https-nginx/README.md](https://github.com/kubernetes/examples/blob/master/staging/https-nginx/README.md).'
  prefs: []
  type: TYPE_NORMAL
- en: We've already covered the principles in [Chapter 5](3dbb475e-3cc2-4943-8441-40a287cccfed.xhtml),
    *Configuring Kubernetes Security, Limits, and Accounts*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the `spec` section of the Hue-reminders service when exposed to the
    world through `NodePort`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Ingress
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`Ingress` is a Kubernetes configuration object that lets you expose a service
    to the outside world and take care of a lot of details. It can do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Provide an externally visible URL to your service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load-balance traffic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Terminate SSL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide name-based virtual hosting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To use `Ingress`, you must have an `Ingress` controller running in your cluster.
    Note that Ingress is still in beta and has many limitations. If you're running
    your cluster on GKE, you're probably OK. Otherwise, proceed with caution. One
    of the current limitations of the `Ingress` controller is that it isn't built
    for scale. As such, it is not a good option for the Hue platform yet. We'll cover
    the `Ingress` controller in greater detail in Chapter 10, *Advanced Kubernetes
    Networking*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is what an `Ingress` resource looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The Nginx `Ingress` controller will interpret this `Ingress` request and create
    a corresponding configuration file for the Nginx web server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: It is possible to create other controllers.
  prefs: []
  type: TYPE_NORMAL
- en: Using namespace to limit access
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Hue project is moving along nicely, and we have a few hundred microservices
    and about 100 developers and DevOps engineers working on it. Groups of related
    microservices emerge, and you notice that many of these groups are pretty autonomous.
    They are completely oblivious to the other groups. Also, there are some sensitive
    areas, such as health and finance, that you will want to control access to more
    effectively. Enter namespaces.
  prefs: []
  type: TYPE_NORMAL
- en: Let's create a new service, Hue-finance, and put it in a new namespace called
    `restricted`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the YAML file for the new `restricted` namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the namespace has been created, we need to configure a context for the
    namespace. This will allow restricting access just to this namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s check our `cluster` configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the current context is `restricted`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, in this empty namespace, we can create our `hue-finance` service, and
    it will be on its own:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: You don't have to switch contexts. You can also use the `--namespace=<namespace>`
    and `--all-namespaces` command-line switches.
  prefs: []
  type: TYPE_NORMAL
- en: Launching jobs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hue has a lot of long-running processes deployed as microservices, but it also
    has a lot of tasks that run, accomplish some goal, and exit. Kubernetes supports
    this functionality through the job resource. A Kubernetes job manages one or more
    pods and ensures that they run until success. If one of the pods managed by the
    job fails or is deleted, then the job will run a new pod until it succeeds.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a job that runs a Python process to compute the factorial of 5 (hint:
    it''s 120):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Note that the `restartPolicy` must be either `Never` or `OnFailure`. The default
    `Always` value is invalid because a job shouldn't restart after successful completion.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start the job and check its status:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The pods of completed tasks are not displayed by default. You must use the
    `--show-all` option:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The `factorial5` pod has a status of `Completed`. Let''s check out its output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Running jobs in parallel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can also run a job with parallelism. There are two fields in the spec, called
    `completions` and `parallelism`. The `completions` are set to `1` by default.
    If you want more than one successful completion, then increase this value. `parallelism`
    determines how many pods to launch. A job will not launch more pods than needed
    for successful completions, even if the parallelism number is greater.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s run another job that just sleeps for `20` seconds until it has three
    successful completions. We''ll use a `parallelism` factor of `6`, but only three
    pods will be launched:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Cleaning up completed jobs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When a job completes, it sticks around - and its pods do, too. This is by design,
    so you can look at logs or connect to pods and explore. But normally, when a job
    has completed successfully, it is not needed anymore. It''s your responsibility
    to clean up completed jobs and their pods. The easiest way is to simply delete
    the `job` object, which will delete all the pods too:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Scheduling cron jobs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes cron jobs are jobs that run for a specified time, once or repeatedly.
    They behave as regular Unix cron jobs, specified in the `/etc/crontab` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Kubernetes 1.4 they were known as a `ScheduledJob`. But, in Kubernetes 1.5,
    the name was changed to `CronJob`. Starting with Kubernetes 1.8, the `CronJob`
    resource is enabled by default in the API server and there no need to pass a `--runtime-config`
    flag anymore, but it''s still in `beta`. Here is the configuration to launch a
    cron job every minute to remind you to stretch. In the schedule, you may replace
    the `*` with `?`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'In the pod spec, under the job template, I added a label called `name`. The
    reason is that cron jobs and their pods are assigned names with a random prefix
    by Kubernetes. The label allows you to easily discover all the pods of a particular
    cron job. See the following command lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that each invocation of a cron job launches a new `job` object with a
    new pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'When a cron job invocation completes, its pod gets into a `Completed` state
    and will not be visible without the `-show-all` or `-a` flags:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'As usual, you can check the output of the pod of a completed cron job using
    the `logs` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: When you delete a cron job, it stops scheduling new jobs and deletes all the
    existing job objects along with all the pods it created.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use the designated label (the name is equal to `STRETCH` in this case)
    to locate all the job objects launched by the cron job. You can also suspend a
    cron job so it doesn''t create more jobs without deleting completed jobs and pods.
    You can also manage previous jobs by setting in the spec history limits: `spec.successfulJobsHistoryLimit`
    and `.spec.failedJobsHistoryLimit`.'
  prefs: []
  type: TYPE_NORMAL
- en: Mixing non-cluster components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most real-time system components in the Kubernetes cluster will communicate
    with out-of-cluster components. These could be completely external third-party
    services which are accessible through some API, but could also be internal services
    running in the same local network that, for various reasons, are not part of the
    Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two categories here: inside-the-cluster-network and outside-the-cluster-network.
    Why is the distinction important?'
  prefs: []
  type: TYPE_NORMAL
- en: Outside-the-cluster-network components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These components have no direct access to the cluster. They can only access
    it through APIs, externally visible URLs, and exposed services. These components
    are treated just like any external user. Often, cluster components will just use
    external services, which pose no security issue. For example, in my previous job
    we had a Kubernetes cluster that reported exceptions to a third-party service
    ([https://sentry.io/welcome/](https://sentry.io/welcome/)). It was one-way communication
    from the Kubernetes cluster to the third-party service.
  prefs: []
  type: TYPE_NORMAL
- en: Inside-the-cluster-network components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These are components that run inside-the-network but are not managed by Kubernetes.
    There are many reasons to run such components. They could be legacy applications
    that have not be Kubernetized yet, or some distributed data store that is not
    easy to run inside Kubernetes. The reason to run these components inside-the-network
    is for performance, and to have isolation from the outside world so that traffic
    between these components and pods can be more secure. Being part of the same network
    ensures low-latency, and the reduced need for authentication is both convenient
    and can avoid authentication overhead.
  prefs: []
  type: TYPE_NORMAL
- en: Managing the Hue platform with Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will look at how Kubernetes can help operate a huge platform
    such as Hue. Kubernetes itself provides a lot of capabilities to orchestrate pods
    and manage quotas and limits, detecting and recovering from certain types of generic
    failures (hardware malfunctions, process crashes, and unreachable services). But,
    in a complicated system such as Hue, pods and services may be up and running but
    in an invalid state or waiting for other dependencies in order to perform their
    duties. This is tricky because if a service or pod is not ready yet, but is already
    receiving requests, then you need to manage it somehow: fail (puts responsibility
    on the caller), retry (*how many times?* *for how long?* *how often?*), and queue
    for later (*who will manage this queue?*).'
  prefs: []
  type: TYPE_NORMAL
- en: It is often better if the system at large can be aware of the readiness state
    of different components, or if components are visible only when they are truly
    ready. Kubernetes doesn't know Hue, but it provides several mechanisms, such as
    liveness probes, readiness probes, and Init Containers, to support the application-specific
    management of your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Using liveness probes to ensure your containers are alive
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kubectl watches over your containers. If a container process crashes, Kubelet
    will take care of it based on the restart policy. But this is not always enough.
    Your process may not crash, but instead run into an infinite loop or a deadlock.
    The restart policy might not be nuanced enough. With a liveness probe, you get
    to decide when a container is considered alive. Here is a pod template for the
    Hue music service. It has a `livenessProbe` section, which uses the `httpGet`
    probe. An HTTP probe requires a scheme (HTTP or HTTPS, default to HTTP, a host
    (which defaults to `PodIp`), a `path`, and a `port`). The probe is considered
    successful if the HTTP status is between `200` and `399`. Your container may need
    some time to initialize, so you can specify an `initialDelayInSeconds`. The Kubelet
    will not hit the liveness check during this period:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: If a liveness probe fails for any container, then the pod's restart policy goes
    into effect. Make sure your restart policy is not *Never*, because that will make
    the probe useless.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two other types of probe:'
  prefs: []
  type: TYPE_NORMAL
- en: '`TcpSocket`: Just check that a port is open'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Exec`: Run a command that returns `0` for success'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using readiness probes to manage dependencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Readiness probes are used for different purpose. Your container may be up and
    running, but it may depend on other services that are unavailable at the moment.
    For example, Hue-music may depend on access to a data service that contains your
    listening history. Without access, it is unable to perform its duties. In this
    case, other services or external clients should not send requests to the Hue music
    service, but there is no need to restart it. Readiness probes address this use
    case. When a readiness probe fails for a container, the container's pod will be
    removed from any service endpoint it is registered with. This ensures that requests
    don't flood services that can't process them. Note that you can also use readiness
    probes to temporarily remove pods that are overbooked until they drain some internal
    queue.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a sample readiness probe. I use the exec probe here to execute a `custom`
    command. If the command exits a non-zero exit code, the container will be torn
    down:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: It is fine to have both a readiness probe and a liveness probe on the same container
    as they serve different purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Employing Init Containers for orderly pod bring-up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Liveness and readiness probes are great. They recognize that, at startup, there
    may be a period where the container is not ready yet, but shouldn't be considered
    failed. To accommodate that there is the `initialDelayInSeconds` setting where
    containers will not be considered failed. But what if this initial delay is potentially
    very long? Maybe, in most cases, a container is ready after a couple of seconds
    and ready to process requests, but because the initial delay is set to five minutes
    just in case, we waste a lot of time when the container is idle. If the container
    is part of a high-traffic service, then many instances can all sit idle for five
    minutes after each upgrade and pretty much make the service unavailable.
  prefs: []
  type: TYPE_NORMAL
- en: Init Containers address this problem. A pod may have a set of Init Containers
    that run to completion before other containers are started. An Init Container
    can take care of all the non-deterministic initialization and let application
    containers with their readiness probe have minimal delay.
  prefs: []
  type: TYPE_NORMAL
- en: 'Init Containers came out of beta in Kubernetes 1.6\. You specify them in the
    pod spec as the `initContainers` field, which is very similar to the `containers`
    field. Here is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Sharing with DaemonSet pods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`DaemonSet` pods are pods that are deployed automatically, one per node (or
    a designated subset of the nodes). They are typically used for keeping an eye
    on nodes and ensuring that they are operational. This is a very important function
    that we covered in Chapter 3, *Monitoring, Logging, and Troubleshooting*, when
    we discussed the node problem detector. But they can be used for much more. The
    nature of the default Kubernetes scheduler is that it schedules pods based on
    resource availability and requests. If you have lots of pods that don''t require
    a lot of resources, many pods will be scheduled on the same node. Let''s consider
    a pod that performs a small task and then, every second, sends a summary of all
    its activities to a remote service. Imagine that, on average, 50 of these pods
    are scheduled on the same node. This means that, every second, 50 pods make 50
    network requests with very little data. How about we cut it down by 50 times to
    just a single network request? With a `DaemonSet` pod, all the other 50 pods can
    communicate with it instead of talking directly to the remote service. The `DaemonSet`
    pod will collect all the data from the 50 pods and, once a second, will report
    it in aggregate to the remote service. Of course, that requires the remote service
    API to support aggregate reporting. The nice thing is that the pods themselves
    don''t have to be modified; they will just be configured to talk to the `DaemonSet`
    pod on localhost instead of the remote service. The `DaemonSet` pod serves as
    an aggregating proxy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The interesting part about this configuration file is that the `hostNetwork`,
    `hostPID`, and `hostIPC` options are set to `true`. This enables the pods to communicate
    efficiently with the proxy, utilizing the fact that they are running on the same
    physical host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Evolving the Hue platform with Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll discuss other ways to extend the Hue platform and service
    additional markets and communities. The question is always, *What Kubernetes features
    and capabilities can we use to address new challenges or requirements?*
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing Hue in enterprises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Enterprises often can't run in the cloud, either due to security and compliance
    reasons, or for performance reasons because the system has work with data and
    legacy systems that are not cost-effective to move to the cloud. Either way, Hue
    for enterprise must support on-premise clusters and/or bare-metal clusters.
  prefs: []
  type: TYPE_NORMAL
- en: While Kubernetes is most often deployed on the cloud, and even has a special
    cloud-provider interface, it doesn't depend on the cloud and can be deployed anywhere.
    It does require more expertise, but enterprise organizations that already run
    systems on their own datacenters have that expertise.
  prefs: []
  type: TYPE_NORMAL
- en: CoreOS provides a lot of material regarding deploying Kubernetes clusters on
    bare-metal lusters.
  prefs: []
  type: TYPE_NORMAL
- en: Advancing science with Hue
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hue is so great at integrating information from multiple sources that it would
    be a boon for the scientific community. Consider how Hue can help multi-disciplinary
    collaborations between scientists from different areas.
  prefs: []
  type: TYPE_NORMAL
- en: A network of scientific communities might require deployment across multiple
    geographically-distributed clusters. Enter cluster federation. Kubernetes has
    this use case in mind and evolves its support. We will discuss it at length in
    a later chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Educating the kids of the future with Hue
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hue can be utilized for education and provide many services to online education
    systems. But privacy concerns may prevent deploying Hue for kids as a single,
    centralized system. One possibility is to have a single cluster, with namespaces
    for different schools. Another deployment option is that each school or county
    has its own Hue Kubernetes cluster. In the second case, Hue for education must
    be extremely easy to operate to cater for schools without a lot of technical expertise.
    Kubernetes can help a lot by providing self-healing and auto-scaling features
    and capabilities for Hue, to be as close to zero-administration as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we designed and planned the development, deployment, and management
    of the Hue platform - an imaginary omniscient and omnipotent service - built on
    microservices architecture. We used Kubernetes as the underlying orchestration
    platform, of course, and delved into many of its concepts and resources. In particular,
    we focused on deploying pods for long-running services, as opposed to jobs for
    launching short-term or cron jobs, explored internal services versus external
    services, and also used namespaces to segment a Kubernetes cluster. Then we looked
    at the management of a large system such as Hue with liveness and readiness probes,
    Init Containers, and DaemonSets.
  prefs: []
  type: TYPE_NORMAL
- en: You should now feel comfortable architecting web-scale systems composed of microservices,
    and understand how to deploy and manage them in a Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look into the super-important area of storage.
    Data is king, but often the least-flexible element of the system. Kubernetes provides
    a storage model, and many options for integrating with various storage solutions.
  prefs: []
  type: TYPE_NORMAL
