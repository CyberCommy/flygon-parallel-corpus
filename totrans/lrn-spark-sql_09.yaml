- en: Developing Applications with Spark SQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will present several examples of developing applications
    using Spark SQL. We will primarily focus on text analysis-based applications,
    including preprocessing pipelines, bag-of-words techniques, computing readability
    metrics for financial documents, identifying themes in document corpuses, and
    using Naive Bayes classifiers. Additionally, we will describe the implementation
    of a machine learning example.
  prefs: []
  type: TYPE_NORMAL
- en: 'More specifically, you will learn about the following in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Spark SQL-based application's development
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocessing textual data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building preprocessing data pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying themes in document corpuses
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Naive Bayes classifiers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing a machine learning application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing Spark SQL applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning, predictive analytics, and related data science topics are
    becoming increasingly popular for solving real-world problems across business
    domains. These applications are driving mission-critical business decision making
    in many organizations. Examples of such applications include recommendation engines,
    targeted advertising, speech recognition, fraud detection, image recognition and
    categorization, and so on. Spark (and Spark SQL) is increasingly becoming the
    platform of choice for these large-scale distributed applications.
  prefs: []
  type: TYPE_NORMAL
- en: With the availability of online data sources for financial news, earning conference
    calls, regulatory filings, social media, and so on, interest in the automated
    and intelligent analysis of textual and other unstructured data available in various
    formats, including text, audio, and video, has proliferated. These applications
    include sentiment analysis from regulatory filings, large-scale automated analysis
    of news articles and stories, twitter analysis, stock price prediction applications,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will present some approaches and techniques for dealing
    with textual data. Additionally, we will present some examples of applying machine
    learning models on textual data to classify documents, derive insights from document
    corpuses, and process textual information for sentiment analysis.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we start our coverage with a few methods that help in converting
    regulatory filings into collections of words. This step allows the use of domain-specific
    dictionaries to classify the tone of the documents, train algorithms to identify
    document characteristics or identify hidden structures as common topics across
    a collection of documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a more detailed survey of textual analysis methods in accounting and finance,
    refer to *Textual Analysis in Accounting and Finance: A Survey* by Tim Loughran
    and Bill McDonald, at [https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2504147](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2504147).'
  prefs: []
  type: TYPE_NORMAL
- en: We will also examine typical issues, challenges, and limitations present in
    implementing textual analysis applications, for example, converting tokens to
    words, disambiguating sentences, and cleansing embedded tags, documents, and other
    noisy elements present in financial disclosure documents. Also, note that the
    use of HTML formatting is a leading source of errors while parsing documents.
    Such parsing depends on the consistency in the structure of the text and the related
    markup language, often resulting in significant errors. Additionally, it is important
    to understand that we are often interested in both the intended and the unintended
    information conveyed by text.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding text analysis applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The inherent nature of language and writing leads to problems of high dimensionality
    while analyzing documents. Hence, some of the most widely used textual methods
    rely on the critical assumption of independence, where the order and direct context
    of a word are not important. Methods, where word sequence is ignored, are typically
    labeled as "bag-of-words" techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Textual analysis  is a lot more imprecise compared to quantitative analysis.
    Textual data requires an additional step of translating the text into quantitative
    measures, which are then used as inputs for various text-based analytics or ML
    methods. Many of these methods are based on deconstructing a document into a term-document
    matrix consisting of rows of words and columns of word counts.
  prefs: []
  type: TYPE_NORMAL
- en: In applications using a bag of words, the approach to normalizing the word counts
    is important as the raw counts directly dependent on the document length. A simple
    use of proportions can solve this problem, however, we might also want to adjust
    a word's weight. Typically, these approaches are based on the rarity of a given
    term in the document, for example, **term frequency-inverse document frequency** (**tf-idf**).
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explore using Spark SQL for the textual analysis
    of financial documents.
  prefs: []
  type: TYPE_NORMAL
- en: Using Spark SQL for textual analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will present a detailed example of typical preprocessing
    required for preparing data for textual analysis (from the accounting and finance
    domain). We will also compute a few metrics for readability (a measure of whether
    the receiver of information can accurately reconstruct the intended message).
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing textual data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will develop a set of functions for preprocessing a `10-K`
    statement. We will be using the "complete submission text file" for a `10-K` filing
    on the EDGAR website as the input text in our example.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more details on the `Regex` expressions used to preprocess `10-K` filings
    refer to *The Annual Report Algorithm: Retrieval of Financial Statements and Extraction
    of Textual Information*, by Jorg Hering at [http://airccj.org/CSCP/vol7/csit76615.pdf](http://airccj.org/CSCP/vol7/csit76615.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import all the packages required in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we read the input file and convert the input rows into a single string
    for our processing. You can download the input file for the following example
    from [https://www.sec.gov/Archives/edgar/data/320193/000119312514383437/0001193125-14-383437-index.html](https://www.sec.gov/Archives/edgar/data/320193/000119312514383437/0001193125-14-383437-index.html):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'As the pre-processing functions are executed, the length of the input string
    progressively reduces as a lot of extraneous or unrelated text/tags are removed
    at each step. We compute the starting length of the original string to track the
    impact of applying specific functions in each of the processing steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In addition, `10-K` files are composed of several exhibits--XBRL, graphics,
    and other documents (file) types embedded in the financial statements; these include
    Microsoft Excel files (file extension `*.xlsx`), ZIP files (file extension `*.zip`),
    and encoded PDF files (file extension `*.pdf`).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following step, we apply additional rules for deleting these embedded
    documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we delete all the metadata included in the core document and the exhibits,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Before deleting all the HTML elements and their corresponding attributes, we
    delete the tables in the document since they normally contain non-textual (quantitative)
    information.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following function uses a set of regular expressions applied to delete
    tables and HTML elements embedded in the financial statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Next, we extract the text in the body section of each of the HTML-formatted
    documents. As the EDGAR system accepts submissions with extended character sets,
    such as `&nbsp; &amp;`, `&reg;`, and so on--they will need to be decoded and/or
    appropriately replaced for textual analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'We show a few examples of these in this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define a function that cleans up excess blanks, line feed, and carriage
    returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In the next code block, we define a function to illustrate the removal of a
    user-specified set of strings. These strings can be read from an input file or
    a database. This step is not required if you can implement `Regex` for extraneous
    text that are, typically, present throughout the document (and vary from document
    to document), but do not have any additional value in textual analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In the next step, we remove all the URLs, filenames, digits, and punctuation
    (except the periods) from the document string. The periods are retained at this
    stage for computing the number of periods (representing the number of sentences)
    in the text (as shown in the following section):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will discuss some metrics typically used to measure
    readability, that is, whether the textual information contained in a `10-K` filing
    is accessible to the user.
  prefs: []
  type: TYPE_NORMAL
- en: Computing readability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Fog Index and the number of words contained in the annual report have been
    widely used as measures of readability for annual reports (that is, `Form 10-Ks`).
    The Fog Index is a function of two variables: average sentence length (in words)
    and complex words (defined as the percentage of words with more than two syllables):'
  prefs: []
  type: TYPE_NORMAL
- en: '*Fog Index = 0.4 * (average number of words per sentence + percentage of complex
    words)*'
  prefs: []
  type: TYPE_NORMAL
- en: The Fog Index equation estimates the number of years of education needed to
    understand the text on a first reading. Thus, a Fog Index value of `16` implies
    that the reader needs sixteen years of education--essentially a college degree--to
    comprehend the text on a first reading. Generally, documents with a Fog Index
    of above eighteen are considered unreadable since more than a master's degree
    is needed to understand the text.
  prefs: []
  type: TYPE_NORMAL
- en: Parsing the `10-K` for computing the average number of words per sentence is
    typically a difficult and error-prone process because these documents contain
    a variety of abbreviations, and use periods to delineate section identifiers or
    as spacers. Additionally, real-world systems will also need to identify the many
    lists contained in such filings (based on punctuation and line spacing). For example,
    such an application will need to avoid counting the periods in section headers,
    ellipses, or other cases where a period is likely not terminating a sentence,
    and then assume that the remaining periods are sentence terminations.
  prefs: []
  type: TYPE_NORMAL
- en: The average words-per-sentence metric is determined by the number of words divided
    by the number of sentence terminations. This is typically done by removing abbreviations
    and other spurious sources of periods and then counting the number of sentence
    terminators and the number of words.
  prefs: []
  type: TYPE_NORMAL
- en: 'We compute the number of periods left in the text, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we remove all the periods (and any other non-alphabetic characters remaining
    in our text) to arrive at an initial set of words contained in our original document.
    Note that all of these words may still not be legitimate words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00269.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following step, we convert the string of words into a DataFrame and
    use the `explode()` function to create a row for each of the words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Next, we read in a dictionary (preferably a domain-specific dictionary). We
    will match our list of words against this dictionary to arrive at our final list
    of words (they should all be legitimate words after this stage).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: For our purposes, we have used the Loughran & McDonold's Master Dictionary as
    it contains words typically found in 10-K statements. You can download the `LoughranMcDonald_MasterDictionary_2014.csv` file
    and the associated documentation from [https://www3.nd.edu/~mcdonald/Word_Lists.html](https://www3.nd.edu/~mcdonald/Word_Lists.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next step, we join our word list DataFrame with the dictionary, and
    compute the number of words in our final list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The average words per sentence are computed by dividing the number of words
    by the number of periods computed earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the `Syllables` column in the dictionary to compute the number of words
    in our word list that have more than two syllables, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we plug in the parameters into our equation to compute the Fog Index,
    as illustrated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The argument against the use of readability measures, such as the Fog Index,
    in financial documents is the observation that a majority of these documents are
    not distinguishable based on the writing style used. Additionally, even though
    the percentage of complex words in these documents may be high, such words, or
    industry jargon, are easily understood by the audience for such documents (for
    example, the investor community).
  prefs: []
  type: TYPE_NORMAL
- en: As a simple proxy for readability of annual reports, Loughran and McDonald suggest
    using the natural log of gross `10-K` file size (complete submission text file).
    Compared to the Fog Index, this measure is a lot easier to obtain and does not
    involve complicated parsing of `10-K` documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following step, we present a function to compute the file (or, more
    specifically, the RDD) size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The file size (in MB) and the log of the file sizes can be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Although file size is a good proxy for the readability of documents such as
    the `10-K` filings, it may be less suitable for text from press releases, newswire
    stories, and earnings conference calls. In such cases, as the length of the text
    does not vary significantly, other approaches that focus more on the content may
    be more suitable.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss using word lists in textual analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Using word lists
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In measuring the tone or sentiment of a financial document, practitioners typically
    count the number of words associated with a particular sentiment scaled by the
    total number of words in the document. Thus, for example, higher proportions of
    negative words in a document indicate a more pessimistic tone.
  prefs: []
  type: TYPE_NORMAL
- en: The use of dictionaries to measure tone has several important advantages. Apart
    from the convenience in computing sentiment at scale, the use of such dictionaries
    promotes standardization by eliminating individual subjectivity. An important
    component of classifying words is identifying the most frequently occurring words
    within each classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following step, we use the word-sentiment indicators contained in the
    dictionary to get a sense of the sentiment or tone of the `10-K` filing. This
    can be computed relative to the past filings by the same organization, or against
    other organizations in the same or different sector(s):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Typically, the use of modal words is also important in such analyses. For example,
    the use of weaker modal words (for example, may, could, and might) could possibly
    signal issues at the firm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following code, we count the number of words for each category of modal
    words. As per the reference documentation of the dictionary used here, a `1` indicates
    "strong modal" (for example, words such as "always", "definitely", and "never"),
    a `2` indicates "moderate modal" (for example, words such as "can", "generally",
    and "usually"), and a `3` indicates "weak modal" (for example, words such as "almost",
    "could", "might", and "suggests"):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will use some of the functions defined in this section
    to create a data preprocessing pipeline for `10-K` filings.
  prefs: []
  type: TYPE_NORMAL
- en: Creating data preprocessing pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will convert some of the data processing functions from
    the previous sections into custom Transformers. These Transformer objects map
    an input DataFrame to an output DataFrame and are typically used to prepare DataFrames
    for machine learning applications.
  prefs: []
  type: TYPE_NORMAL
- en: We create the following classes as `UnaryTransformer` objects that apply transformations
    to one input DataFrame column and produce another by appending a new column (containing
    the processing results of the applied function) to it. These custom Transformer
    objects can then be a part of a processing pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create the four custom `UnaryTransformer` classes that we will use
    in our example, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**TablesNHTMLElemCleaner.scala**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '**AllURLsFileNamesDigitsPunctuationExceptPeriodCleaner.scala**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '**OnlyAlphasCleaner.scala**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '**ExcessLFCRWSCleaner.scala**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the following `build.sbt` file for compiling and packaging the target
    classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the following `SBT` command to compile and package the classes into a JAR
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, restart Spark shell with the preceding JAR file included in the session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The Dataset for the following example, Reuters-21578, Distribution 1.0, can
    be downloaded from [https://archive.ics.uci.edu/ml/datasets/reuters-21578+text+categorization+collection](https://archive.ics.uci.edu/ml/datasets/reuters-21578+text+categorization+collection).
  prefs: []
  type: TYPE_NORMAL
- en: Here, we will take one of the entries demarcated by `<Reuters>...</Reuters>`
    tags in the downloaded SGML files to create a new input file containing a single
    story. This roughly simulates a new story coming into our pipeline. More specifically,
    this story can be coming in via a Kafka queue, and we can create a continuous
    Spark SQL application to process the incoming story text.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we read in the newly created file into a DataFrame, as illustrated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create instances of Transformers using the classes we defined earlier
    in this section. The Transformers are chained together in the pipeline by specifying
    the output columns for each of the Transformer as the input column to the next
    Transformer in the chain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'After processing the text through our cleansing components, we add two more
    stages in order to tokenize and remove stop words from our text, as demonstrated.
    We use the generic stop words list file available at [https://www3.nd.edu/~mcdonald/Word_Lists.html](https://www3.nd.edu/~mcdonald/Word_Lists.html):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00270.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: At this stage, the components for all our processing stages are ready to be
    assembled into a pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: For more details on Spark pipelines, refer to [https://spark.apache.org/docs/latest/ml-pipeline.html](https://spark.apache.org/docs/latest/ml-pipeline.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'We create a pipeline and chain all the Transformers to specify the pipeline
    stages, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The `pipeline.fit()` method is called on the original DataFrame containing
    the raw text document:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use the pipeline model from the preceding step to transform our original
    Dataset to the form required to feed other downstream textual applications. We
    also drop the columns from intermediate processing steps to clean up our DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Furthermore, we can clean up the final output column by removing any rows containing
    empty strings or spaces, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The rest of the processing is similar to what we presented earlier. The following
    steps explode the column containing our words into separate rows, join our final
    list of words with the dictionary, and then compute the sentiment and modal word''s
    usage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00271.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The next set of steps illustrates the use of the preceding pipeline for processing
    another story from our corpus. We can then compare these results to get a relative
    sense of pessimism (reflected by measures of negative sentiment) in the stories:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00272.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the following negative sentiment computation, we can conclude that
    this story is, relatively, more pessimistic than the previous one analyzed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will shift our focus to identifying the major themes
    within a corpus of documents.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding themes in document corpuses
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bag-of-words-based techniques can also be used to classify common themes in
    documents or to identify themes within a corpus of documents. Broadly, these techniques,
    like most, are attempting to reduce the dimensionality of the term-document matrix,
    based on each word's relation to latent variables in this case.
  prefs: []
  type: TYPE_NORMAL
- en: One of the earliest approaches to this type of classification was **Latent Semantic
    Analysis** (**LSA**). LSA can avoid the limitations of count-based methods associated
    with synonyms and terms with multiple meanings. Over the years, the concept of
    LSA has evolved into another model called **Latent Dirichlet Allocation** (**LDA**).
  prefs: []
  type: TYPE_NORMAL
- en: LDA allows us to identify latent thematic structure within a collection of documents.
    Both LSA and LDA use the term-document matrix for reducing the dimensionality
    of the term space and for producing the topic weights. A constraint of both the
    LSA and LDA techniques is that they work best when applied to large documents.
  prefs: []
  type: TYPE_NORMAL
- en: For more detailed explanation of LDA, refer to *Latent Dirichlet Allocation*,by
    David M. Blei, Andrew Y. Ng, and Michael I. Jordan, at [http://ai.stanford.edu/~ang/papers/jair03-lda.pdf](http://ai.stanford.edu/~ang/papers/jair03-lda.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Now, we present an example of using LDA over a corpus of XML documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start the Spark shell with the package for reading XML documents as we will
    be reading an XML-based corpus in this section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define a few constants for the number of topics, maximum number of
    iterations, and the vocabulary size, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The `PERMISSIVE` mode, used as follows, allows us to continue creating a DataFrame
    even while encountering corrupt records during parsing. The `rowTag` parameter
    specifies the XML node to be read. Here, we are interested in the sentences present
    in the document for our topic's analysis using LDA.
  prefs: []
  type: TYPE_NORMAL
- en: The Dataset for this example contains 4,000 Australian legal cases from the
    **Federal Court of Australia** (**FCA**), and can be downloaded from [https://archive.ics.uci.edu/ml/datasets/Legal+Case+Reports](https://archive.ics.uci.edu/ml/datasets/Legal+Case+Reports).
  prefs: []
  type: TYPE_NORMAL
- en: 'We read in all the case files, as demonstrated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we generate document IDs for each of the legal cases, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We use a `CountVectorizer` (and `CountVectorizerModel`) to convert our collection
    of legal documents into vectors of token counts. Here, we do not have a priori
    dictionary available, so the `CountVectorizer` is used as an Estimator to extract
    the vocabulary and generate a `CountVectorizerModel`. The model produces sparse
    representations for the documents over the vocabulary, which is then passed to
    the LDA algorithm. During the fitting process, the `CountVectorizer` will select
    the top `vocabSize` words ordered by term frequency across the corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'We compute the log-likelihood and the log-perplexity of the LDA model, as shown.
    A model with higher likelihood implies a better model. Similarly, lower perplexity
    represents a better model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we use the `describeTopics()` function to display the topics described
    by their top-weighted terms, as illustrated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00273.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00274.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can display the results containing the actual terms corresponding to the
    term indices, as follows. It is quite evident from the words displayed here that
    we are dealing with a legal corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: The next main topic in textual analysis is collocated words. For some words,
    much of their meaning is derived from their collocation with other words. Predicting
    word meaning based on collocation is generally one of the most common extensions
    beyond the simple bag-of-words approach.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will examine the use of Naive Bayes classifier on n-gram.
  prefs: []
  type: TYPE_NORMAL
- en: Using Naive Bayes classifiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Naive Bayes classifiers are a family of probabilistic classifiers based on applying
    the Bayes' conditional probability theorem. These classifiers assume independence
    between the features. Naive Bayes is often the baseline method for text categorization
    with word frequencies as the feature set. Despite the strong independence assumptions,
    the Naive Bayes classifiers are fast and easy to implement; hence, they are used
    very commonly in practice.
  prefs: []
  type: TYPE_NORMAL
- en: While Naive Bayes is very popular, it also suffers from errors that can lead
    to favoring of one class over the other(s). For example, skewed data can cause
    the classifier to favor one class over another. Similarly, the independence assumption
    can lead to erroneous classification weights that favor one class over another.
  prefs: []
  type: TYPE_NORMAL
- en: For specific heuristics for dealing with problems associated with Naive Bayes
    classifers, refer to *Tackling the Poor Assumptions of Naive Bayes Text Classifiers*,
    by Rennie, Shih, et al at [https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf](https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: One of the main advantages of the Naive Bayes approach is that it does not require
    a large training Dataset to estimate the parameters necessary for classification.
    Among the various approaches for word classification using supervised machine
    learning, the Naive Bayes method is extremely popular; for example, which sentences
    in an annual report can be classified as being "negative," "positive," or "neutral".
    Naive Bayes method is also most commonly used with n-grams and **Support Vector
    Machines** (**SVMs**).
  prefs: []
  type: TYPE_NORMAL
- en: N-grams are used for a variety of different tasks. For example, n-grams can
    be used for developing features for supervised machine learning models such as
    SVMs, Maximum Entropy models, and Naive Bayes. When the value of `N` is `1`, the
    n-grams are referred to as unigrams (essentially, the individual words in a sentence;
    when the value of `N` is `2`, they are called bigrams, when `N` is `3`, they are
    called trigrams, and so on. The main idea here is to use tokens such as bigrams
    in the feature space instead of individual words or unigrams.
  prefs: []
  type: TYPE_NORMAL
- en: The Dataset for this example contains approximately 1.69 million Amazon reviews
    for the electronics category, and can be downloaded from [http://jmcauley.ucsd.edu/data/amazon/](http://jmcauley.ucsd.edu/data/amazon/).
  prefs: []
  type: TYPE_NORMAL
- en: For a more detailed explanation of steps used in this example, check out Natural
    Language Processing with Apache Spark ML and Amazon Reviews (Parts 1 & 2), Mike
    Seddon, at [http://mike.seddon.ca/natural-language-processing-with-apache-spark-ml-and-amazon-reviews-part-1/](http://mike.seddon.ca/natural-language-processing-with-apache-spark-ml-and-amazon-reviews-part-1/).
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we read the input JSON file to create our input DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00275.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'You can print the schema, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we print out the count of records for each of the rating values, as follows.
    Note that the number of records is highly skewed in favor of rating five. This
    skew can impact our results in favor of rating five versus the other ratings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'We create a view from the DataFrame, as follows. This step can conveniently
    help us create a more balanced training DataFrame containing an equal number of
    records from each of the rating categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we create our training and test Datasets using the row numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'In the given steps, we tokenize our text, remove stop words, and create bigrams
    and trigrams:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'In the next steps, we define `HashingTF` instances for the unigrams, the bigrams,
    and the trigrams:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we create an instance of the Naive Bayes classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'We assemble our processing pipeline, as illustrated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'We create a parameter grid to be used for cross-validation to arrive at the
    best set of parameters for our model, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: In the next step, note that a k-fold cross-validation performs model selection
    by splitting the Dataset into a set of non-overlapping, randomly partitioned folds
    that are used as separate training and test Datasets; for example, with `k=3`
    folds, K-fold cross-validation will generate three (training, test) Dataset pairs,
    each of which uses `2/3` of the data for training and `1/3` for testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each fold is used as the test set exactly once. To evaluate a particular `ParamMap`,
    `CrossValidator` computes the average evaluation metric for the three models produced
    by fitting the Estimator on the two different (training, test) Dataset pairs.
    After identifying the best `ParamMap`, `CrossValidator` finally refits the Estimator
    using the best `ParamMap` on the entire Dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'The prediction results obtained are not good after executing the previous set
    of steps. Let''s check whether we can improve the results by reducing the number
    of review categories and increase the number of records in our training set, as
    illustrated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00276.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'The label converter can be used to recover the original labels'' text for improved
    readability in case of textual labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create an instance of our Naive Bayes classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'We assemble our pipeline with all the Transformers and the Naive Bayes estimator,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'We use cross-validation to select the best parameters for our model, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Note the significant improvement in our prediction results as a result of clubbing
    ratings into fewer categories and increasing the number of records for training
    our model.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will present an example of machine learning on textual
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Developing a machine learning application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will present a machine learning example for textual analysis.
    Refer to [Chapter 6](part0103.html#3279U0-e9cbc07f866e437b8aa14e841622275c), *Using
    Spark SQL in Machine Learning Applications*, for more details about the machine
    learning code presented in this section.
  prefs: []
  type: TYPE_NORMAL
- en: The Dataset used in the following example contains 1,080 documents of free text
    business descriptions of Brazilian companies categorized into a subset of nine
    categories. You can download this Dataset from [https://archive.ics.uci.edu/ml/datasets/CNAE-9](https://archive.ics.uci.edu/ml/datasets/CNAE-9).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define a schema for the input records:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'We then use the schema to convert the RDD to a DataFrame, as illustrated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00277.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we add an index column to the DataFrame using the `monotonically_increasing_id()`
    function, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00278.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following step, we assemble the feature vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'We split the input Dataset into the training (90 percent of the records) and
    test (10 percent of the records) Datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following step, we create and fit the logistic regression model on our
    training Dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'We can list the parameters of the model, as illustrated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we display the coefficients and intercept values for our logistic regression
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we use the model to make predictions on the test Dataset, as demonstrated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'Select example rows to display the key columns in the predictions DataFrame,
    as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00279.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We use an evaluator to compute the test error, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'The results of applying a logistic regression model to our Dataset are not
    particularly good. Now, we define parameter grid to explore whether a better set
    of parameters can improve the overall prediction results for our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we show the use of a `CrossValidator` to pick the better parameters for
    our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we run the cross-validation to choose the best set of parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'We make predictions on the test Dataset. Here, `cvModel` uses the best model
    found (`lrModel`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00280.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00281.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'Note the significant improvement in the prediction accuracy as a result of
    cross-validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, save the model to the filesystem. We will retrieve it from the filesystem
    in the program later in this section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we build an application, compile, package, and execute it using the code
    and the saved model and test data from our spark-shell session, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '**LRExample.scala**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: Create a `lib` folder inside your root `SBT` directory (the same as the directory
    containing the `build.sbt` file) and copy `scopt_2.11-3.3.0.jar` and `spark-examples_2.11-2.2.1-SNAPSHOT.jar`
    from spark distribution's `examples` directory into it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, compile and package the source using the same `build.sbt` file used in
    an earlier section, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, execute your Spark Scala program using spark-submit, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced a few Spark SQL applications in the textual analysis
    space. Additionally, we provided detailed code examples, including building a
    data preprocessing pipeline, implementing sentiment analysis, using Naive Bayes
    classifier with n-grams, and implementing an LDA application to identify themes
    in a document corpus. Additionally, we worked through the details of implementing
    an example of machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will focus on use cases for using Spark SQL in deep
    learning applications. We will explore a few of the emerging deep learning libraries
    and present examples of implementing deep learning related applications.
  prefs: []
  type: TYPE_NORMAL
