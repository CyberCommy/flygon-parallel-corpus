- en: Chapter 7. Managing the Networking Stack of a Docker Container
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: docker0 bridge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Troubleshooting Docker bridge configuration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring DNS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Troubleshooting communication between containers and the external network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ibnetwork and the Container Network Model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker networking tools based on overlay and underlay networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparison of Docker networking tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring **OpenvSwitch** (**OVS**) to work with Docker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Each Docker container has its own network stack, and this is due to the Linux
    kernel `net` namespace, where a new `net` namespace for each container is instantiated
    and cannot be seen from outside the container or other containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Docker networking is powered by the following network components and services:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Linux bridges**: L2/MAC learning switch built into the kernel to use for
    forwarding'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Open vSwitch**: Advanced bridge that is programmable and supports tunneling'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Network Address Translators (NAT)**: These are immediate entities that translate
    IP address + Ports (SNAT, DNAT)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**IPtables**: Policy engine in the kernel that is used for managing packet
    forwarding, firewall, and NAT features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apparmor/SElinux**: Firewall policies for each application can be defined'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Various networking components can be used to work with Docker, providing new
    ways to access and use Docker-based services. As a result, we see a lot of libraries
    that follow different approaches to networking. Some prominent ones are Docker
    Compose, Weave, Kubernetes, Pipework, and libnetwork. The following diagram depicts
    root ideas of Docker networking:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Docker networking](graphics/image_07_001-2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Docker networking modes
  prefs: []
  type: TYPE_NORMAL
- en: docker0 bridge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**docker0 bridge** is the heart of default networking. When the Docker service
    is started, a Linux bridge is created on the host machine. The interfaces on the
    containers talk to the bridge and the bridge proxies to the external world. Multiple
    containers on the same host can talk to each other through the Linux bridge.'
  prefs: []
  type: TYPE_NORMAL
- en: 'docker0 can be configured via the `--net` flag, and has four modes in general:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--net default`: In this mode, the default bridge is used as the bridge for
    containers to connect to each other'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--net=none`: With this flag, the container created is truly isolated and cannot
    connect to the network'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--net=container:$container2`: With this flag, the container created shares
    its network namespace with the container named `$container2`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--net=host`: In this mode, the container created shares its network namespace
    with the host'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Troubleshooting Docker bridge configuration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will look at how the container ports are mapped to host ports
    and how we can troubleshoot the issue of connecting containers to the external
    world. This mapping can be done either implicitly by the Docker Engine or can
    be specified.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we create two containers-**Container 1** and **Container 2**-both of them
    are assigned an IP address from a private IP address space and also connected
    to **docker0 bridge**, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Troubleshooting Docker bridge configuration](graphics/image_07_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Two containers talking via Docker0 bridge
  prefs: []
  type: TYPE_NORMAL
- en: Both the preceding containers will be able to ping each other as well as reach
    the external world. For external access, their ports will be mapped to a host
    port. As mentioned in the previous section, containers use network namespaces.
    When the first container is created, a new network namespace is created for the
    container.
  prefs: []
  type: TYPE_NORMAL
- en: 'A **Virtual Ethernet** (**vEthernet** or **vEth**) link is created between
    the container and the Linux bridge. Traffic sent from the `eth0` port of the container
    reaches the bridge through the vEth interface and gets switched thereafter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command will be similar to the following one with
    bridge name and the vEth interfaces on the containers it is mapped to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Connecting containers to the external world
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The **iptables NAT** table on the host is used to masquerade all external connections,
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Reaching containers from the outside world
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The port mapping is again done using the iptables NAT option in the host machine,
    as the following diagram shows, where port mapping of **Container 1** is done
    to communicate with the external world. We will look into it in detail in the
    later part of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '![Reaching containers from the outside world](graphics/image_07_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Port mapping of Container 1 to communicate with the external world
  prefs: []
  type: TYPE_NORMAL
- en: 'Docker server, by default, creates a `docker0` bridge inside the Linux kernel
    that can pass packets back and forth between other physical or virtual network
    interfaces so that they behave as a single ethernet network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have one or more containers up and running, we can confirm that Docker
    has properly connected them to the docker0 bridge by running the `brctl` command
    on the host machine and looking at the interfaces column of the output. First,
    install the bridge utilities using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a host with two different containers connected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Docker uses docker0 bridge settings whenever a container is created. It assigns
    a new IP address from the range available on the bridge whenever a new container
    is created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By default, Docker provides a vnet docker0 that has the `172.17.42.1` IP address.
    Docker containers have IP addresses in the range of `172.17.0.0/16`
  prefs: []
  type: TYPE_NORMAL
- en: To change the default settings in Docker, modify the `/etc/default/docker` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Change the default bridge from `docker0` to `br0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command displays the new bridge name and the IP address range
    of the Docker service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Configuring DNS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker provides hostname and DNS configuration for each container without building
    a custom image. It overlays the `/etc` files inside the container with virtual
    files where it can write new information.
  prefs: []
  type: TYPE_NORMAL
- en: This can be seen by running the `mount` command inside the container. Containers
    receive the same `/resolv.conf` as of the host machine when they are created initially.
    If a host's `/resolv.conf` file is modified, it will be reflected in the container's
    `/resolv.conf` file only when the container is restarted.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Docker, you can set the `dns` options in two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Using `docker run --dns=<ip-address>`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the Docker daemon file, add `DOCKER_OPTS="--dns ip-address"`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can also specify the search domain using `--dns-search=<DOMAIN>`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the nameserver being configured in container using
    the `DOCKER_OPTS` setting in the Docker daemon file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Configuring DNS](graphics/image_07_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: DOCKER_OPTS being used to set nameserver setting for Docker container
  prefs: []
  type: TYPE_NORMAL
- en: 'The main DNS files are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the command to add the DNS server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s the command to add the hostname:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Troubleshooting communication between containers and the external network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Packets can only pass between containers if the `ip_forward` parameter is set
    to `1`. Usually, you will simply leave the Docker server at its default setting
    of `--ip-forward=true` and Docker will set `ip_forward` to `1` for you when the
    server starts up. To check the settings, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'By enabling `ip-forward`, users can make communication between containers and
    the external world possible; it will also be needed for inter-container communication
    if you are in a multiple bridge setup:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Troubleshooting communication between containers and the external network](graphics/image_07_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: ip-forward = true forwards all the packets to/from the container to the external
    network
  prefs: []
  type: TYPE_NORMAL
- en: Docker will not delete or modify any pre-existing rules from Docker filter chain.
    This allows users to create rules to restrict access to containers. Docker uses
    docker0 bridge for packet flow between all containers in a single host. It adds
    a rule to the `FORWARD` chain in iptables (blank accept policy) for the packets
    to flow between two containers. The `--icc=false` option will `DROP` all the packets.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the Docker daemon is configured with both `--icc=false` and `--iptables=true` and
    the Docker run is invoked with the `--link=` option, the Docker server will insert
    a pair of iptables `ACCEPT` rules for the new container to connect to the ports
    exposed by the other container-the ports that it mentioned in the `EXPOSE` lines
    of its Dockerfile:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Troubleshooting communication between containers and the external network](graphics/image_07_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: ip-forward = false forwards all the packets to/from the container to external
    network
  prefs: []
  type: TYPE_NORMAL
- en: By default, Docker's forward rule permits all external IPs. To allow only a
    specific IP or network to access the containers, insert a negated rule at the
    top of the Docker filter chain.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, you can restrict external access such that only the source IP
    `10.10.10.10` can access the containers using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**References:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.docker.com/v1.5/articles/networking/](https://docs.docker.com/v1.5/articles/networking/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.docker.com/engine/userguide/networking/](https://docs.docker.com/v1.5/articles/networking/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://containerops.org/](https://docs.docker.com/engine/userguide/networking/)'
  prefs: []
  type: TYPE_NORMAL
- en: Restricting SSH access from one container to another
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To restrict SSH access from one container to another, perform the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create two containers, c1 and c2:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We can test connectivity between the containers using the IP address we've just
    discovered. Let's see this now using the `ping` tool.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s go into the other container, c1, and try to ping c2:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Install `openssh-server` on both the containers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Enable iptables on the host machine. Initially, you will be able to SSH from
    one container to another.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Stop the Docker service and add `DOCKER_OPTS="--icc=false --iptables=true"`
    in the `default docker` file of the host machine. This option will enable the
    iptables firewall and drop all ports between the containers. By default, iptables
    are not enabled on the host:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The Docker Upstart and SysVinit configuration file, customize the location
    of the Docker binary (especially for development testing):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Use `DOCKER_OPTS` to modify the daemon startup options:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Restart the Docker service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Inspect the iptables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The `DROP` rule has been added to the iptables of the host machine, which drops
    connection between the containers. Now you won't be able to SSH between the containers.
  prefs: []
  type: TYPE_NORMAL
- en: Linking containers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can communicate or connect legacy containers using the `--link` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the first container that will act as the server-`sshserver`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a second container that acts like an SSH client:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that there are more rules added to the Docker chain rule:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram explains the communication between containers using the `--link`
    flag:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Linking containers](graphics/image_07_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Docker--link creates private channels between containers
  prefs: []
  type: TYPE_NORMAL
- en: 'You can inspect your linked container with `docker inspect`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you can successfully SSH into the SSH server with its IP:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Using the `--link` parameter, Docker creates a secure channel between the containers
    that doesn't need to expose any ports externally on the containers.
  prefs: []
  type: TYPE_NORMAL
- en: libnetwork and the Container Network Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'libnetwork is implemented in Go for connecting Docker containers. The aim is
    to provide a **Container Network Model** (**CNM**) that helps programmers provide
    the abstraction of network libraries. The long-term goal of libnetwork is to follow
    the Docker and Linux philosophy to deliver modules that work independently. libnetwork
    has the aim for providing a composable need for networking in containers. It also
    aims to modularize the networking logic in the Docker Engine and libcontainer
    to a single, reusable library by doing the following things:'
  prefs: []
  type: TYPE_NORMAL
- en: Replacing the networking module of the Docker Engine with libnetwork
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allowing local and remote drivers to provide networking to containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Providing a `dnet` tool for managing and testing libnetwork-however, this is
    still a work in progress
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Reference:** [https://github.com/docker/libnetwork/issues/45](https://github.com/docker/libnetwork/issues/45)'
  prefs: []
  type: TYPE_NORMAL
- en: libnetwork implements the CNM. It formalizes the steps required to provide networking
    for containers while providing an abstraction that can be used to support multiple
    network drivers. Its endpoint APIs are primarily used for managing the corresponding
    object and bookkeeping them in order to provide the level of abstraction as required
    by the CNM.
  prefs: []
  type: TYPE_NORMAL
- en: CNM objects
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The CNM is built on three main components, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![CNM objects](graphics/image_07_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Network sandbox model of libnetwork
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Reference:** [https://www.docker.com](https://www.docker.com)'
  prefs: []
  type: TYPE_NORMAL
- en: Sandbox
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A sandbox contains the configuration of a container's network stack that includes
    management of routing tables, the container's interface, and DNS settings. The
    implementation of a sandbox can be a Linux network namespace, a FreeBSD jail,
    or another similar concept.
  prefs: []
  type: TYPE_NORMAL
- en: A sandbox may contain many endpoints from multiple networks. It also represents
    a container's network configuration, such as IP address, MAC address, and DNS
    entries.
  prefs: []
  type: TYPE_NORMAL
- en: libnetwork makes use of the OS-specific parameters to populate the network configuration
    represented by a sandbox. It provides a framework to implement a sandbox in multiple
    OSes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Netlink** is used to manage the routing table in namespace and currently
    two implementations of a sandbox exist-`namespace_linux.go` and `configure_linux.go`-to
    uniquely identify the path on the host filesystem. A sandbox is associated with
    a single Docker container.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following data structure shows the runtime elements of a sandbox:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'A new sandbox is instantiated from a network controller (which is explained
    in detail later):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Endpoint
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An endpoint joins a sandbox to a network and provides connectivity for services
    exposed by a container to the other containers deployed in the same network. It
    can be an internal port of Open vSwitch or a similar vEth pair.
  prefs: []
  type: TYPE_NORMAL
- en: An endpoint can belong to only one network and may only belong to one sandbox.
    It represents a service and provides various APIs to create and manage the endpoint.
    It has a global scope but gets attached to only one network.
  prefs: []
  type: TYPE_NORMAL
- en: 'An endpoint is specified by the following struct:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: An endpoint is associated with a unique ID and name. It is attached to a network
    and a sandbox ID. It is also associated with a IPv4 and IPv6 address spaces. Each
    endpoint is associated with an endpoint interface.
  prefs: []
  type: TYPE_NORMAL
- en: Network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A group of endpoints that are able to communicate with each other directly is
    called a **network**. It provides the required connectivity within the same host
    or multiple hosts and whenever a network is created or updated, the corresponding
    driver is notified. An example is a VLAN or Linux bridge that has a global scope
    within a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'A networks are controlled from a network controller, which we will discuss
    in the next section. Every network has a name, address space, ID, and network
    type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Network controller
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A network controller object provides APIs to create and manage a network object.
    It is an entry point to the libnetwork by binding a particular driver to a given
    network, and it supports multiple active drivers, both inbuilt and remote. A network
    controller allows users to bind a particular driver to a given network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Each network controller has reference to the following things:'
  prefs: []
  type: TYPE_NORMAL
- en: One or more drivers in a data structure driver table
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One or more sandboxes in a data structure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A data store
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An ipamTable![Network controller](graphics/image_07_009.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network controller handling the network between Docker container and Docker
    engine
  prefs: []
  type: TYPE_NORMAL
- en: The preceding diagram shows how the network controller sits between the Docker
    Engine, containers, and the networks they are attached to.
  prefs: []
  type: TYPE_NORMAL
- en: CNM attributes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following are the CNM attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Options:** These are not end user visible but are the key-value pairs of
    data to provide a flexible mechanism to pass driver-specific configuration from
    user to driver directly. libnetwork operates on the options only if a key matches
    a well-known label and as a result of this a value is picked up that is represented
    by a generic object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Labels:** These are a subset of options that are end user variable represented
    in the UI using the `--labels` option. Their main function is to perform driver-specific
    operations, and they are passed from the UI.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CNM life cycle
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Consumers of the CNM interact through the CNM objects and its APIs to network
    the containers that they manage; drivers register with a network controller.
  prefs: []
  type: TYPE_NORMAL
- en: Built-in drivers are registered inside libnetwork, while remote drivers register
    with libnetwork using a plugin mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each driver handles a particular network type as explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A network controller object is created using the `libnetwork.New()` API to manage
    the allocation of networks and optionally configure a driver with driver-specific
    options. The network object is created using the controller's `NewNetwork()` API, a
    `name`, and a `NetworkType` is added as a parameter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `NetworkType` parameter helps to choose a driver and binds the created network
    to that driver. All operations on network will be handled by the driver that is
    created using the preceding API.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Controller.NewNetwork()` API takes in an optional options parameter that
    carries driver-specific options and labels, which the driver can use for its purpose.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Network.CreateEndpoint()` is called to create a new endpoint in a given network.
    This API also accepts optional options parameters that vary with the driver.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CreateEndpoint()` can choose to reserve IPv4/IPv6 addresses when an endpoint
    is created in a network. The driver assigns these addresses using the `InterfaceInfo`
    interface defined in `driverapi`. The IPv4/IPv6 addresses are needed to complete
    the endpoint as service definition along with the ports that the endpoint exposes.
    A service endpoint is a network address and the port number that the application
    container is listening on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Endpoint.Join()` is used to attach a container to an endpoint. The `Join`
    operation will create a sandbox, if one doesn''t exist for that container. The
    drivers make use of the sandbox key to identify multiple endpoints attached to
    the same container.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a separate API to create an endpoint and another to join the endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: An endpoint represents a service that is independent of the container. When
    an endpoint is created, it has resources reserved for a container to get attached
    to the endpoint later. It gives a consistent networking behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '`Endpoint.Leave()` is invoked when a container is stopped. The driver can clean
    up the states that it allocated during the `Join()` call. libnetwork deletes the
    sandbox when the last referencing endpoint leaves the network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: libnetwork keeps holding on to IP addresses as long as the endpoint is still
    present. These will be reused when the container (or any container) joins again.
    It ensures that the container's resources are reused when they are stopped and
    started again.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Endpoint.Delete()` deletes an endpoint from a network. This results in deleting
    the endpoint and cleaning up the cached `sandbox.Info`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Network.Delete()` is used to delete a network. Deleting is allowed if there
    are no endpoints attached to the network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker networking tools based on overlay and underlay networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An overlay is a virtual network that is built on top of anunderlying network
    infrastructure (the underlay). The purpose is to implement a network service that
    is not available in the physical network.
  prefs: []
  type: TYPE_NORMAL
- en: Network overlay dramatically increases the number of virtual subnets that can
    be created on top of the physical network, which in turn supports multi-tenancy
    and virtualization features.
  prefs: []
  type: TYPE_NORMAL
- en: Every container in Docker is assigned with an IP address that is used for communication
    with other containers. If a container has to communicate to the external network,
    you set up networking in the host system and expose or map the port from the container
    to the host machine. With this application running inside, containers will not
    be able to advertise their external IP and ports as the information is not available
    to them.
  prefs: []
  type: TYPE_NORMAL
- en: The solution is to somehow assign unique IPs to each Docker container across
    all hosts and have some networking product that routes traffic between the hosts.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are different projects and tools to help with Docker networking, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Flannel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weave
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Project Calico
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flannel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Flannel** gives each container an IP that can be used for container-to-container
    communication. By packet encapsulation, it creates a virtual overlay network over
    host network. By default, flannel provides a `/24` subnet to the hosts, from which
    the Docker daemon will allocate IPs to the containers.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Flannel](graphics/image_07_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Communication between containers using Flannel
  prefs: []
  type: TYPE_NORMAL
- en: Flannel runs an agent, `flanneld`, on each host and is responsible for allocating
    a subnet lease out of a preconfigured address space. Flannel uses `etcd` ([https://github.com/coreos/etcd](https://github.com/coreos/etcd))
    to store the network configuration, allocated subnets, and auxiliary data (such
    as the host's IP).
  prefs: []
  type: TYPE_NORMAL
- en: In order to provide encapsulation, Flannel uses the **Universal TUN/TAP** device
    and creates an overlay network using UDP to encapsulate IP packets. The subnet
    allocation is done with the help of `etcd`, which maintains the overlay subnet
    to host mappings.
  prefs: []
  type: TYPE_NORMAL
- en: Weave
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Weave** creates a virtual network that connects Docker containers deployed
    across hosts/VMs and enables their automatic discovery.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Weave](graphics/image_07_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Weave Network
  prefs: []
  type: TYPE_NORMAL
- en: Weave can traverse firewalls and operate in partially connected networks. Traffic
    can be optionally encrypted, allowing hosts/VMs to be connected across an untrusted
    network.
  prefs: []
  type: TYPE_NORMAL
- en: Weave augments Docker's existing (single host) networking capabilities, such
    as the docker0 bridge, so that these can continue to be used by the containers.
  prefs: []
  type: TYPE_NORMAL
- en: Project Calico
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Project Calico** provides a scalable networking solution for connecting containers,
    VMs, or bare metal. Calico provides connectivity using the scalable IP networking
    principle as a layer 3 approach. Calico can be deployed without overlays or encapsulation.
    The Calico service should be deployed as a container on each node. It provides
    each container with its own IP address and, also, handles all the necessary IP
    routing, security policy rules, and distribution of routes across a cluster of
    nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Calico architecture contains four important components in order to provide
    better networking solutions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Felix**, the Calico worker process, is the heart of the Calico networking
    that primarily routes and provides the desired connectivity to and from the workloads
    on the host. It also provides the interface to the kernel for outgoing endpoint
    traff'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BIRD**, the route ic. BIRD, the route distribution open source BGP, exchanges
    routing information between hosts. The kernel endpoints that are picked up by
    BIRD are distributed to BGP peers in order to provide inter-host routing. Two
    BIRD processes run in the *calico-node* container, IPv4 (bird) and one for IPv6
    (bird6)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**confd**, a templating process to autogenerate configuration for BIRD, monitors
    the `etcd` store for any changes to BGP configuration, such as log levels and
    IPAM information. `confd` also dynamically generates BIRD configuration files
    based on data from `etcd` and is trigger automatically as updates are applied
    to the data. `confd` triggers BIRD to load new files whenever the configuration
    file is changed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**calicoctl** is the command line used to configure and start the Calico service.
    It even allows the data store (`etcd`) to define and apply security policy. The
    tool also provides the simple interface for general management of Calico configuration
    irrespective of whether Calico is running on VMs, containers, or bare metal. The
    following commands are supported at `calicoctl;`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'As per the official GitHub page of the Calico repository ([https://github.com/projectcalico/calico-containers](https://github.com/projectcalico/calico-containers)),
    the following integration of Calico exists:'
  prefs: []
  type: TYPE_NORMAL
- en: Calico as a Docker network plugin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calico without Docker networking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calico with Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calico with Mesos
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calico with Docker Swarm![Project Calico](graphics/image_07_012.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calico Architecture
  prefs: []
  type: TYPE_NORMAL
- en: Configuring an overlay network with the Docker Engine swarm node
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the release of Docker 1.9, the multi-host and overlay network has become
    one of its primary feature. It enables private networks that can be established
    to connect multiple containers. We will be creating the overlay network on a manager
    node running in swarm cluster without an external key-value store. The swarm network
    will make the network available to the nodes in the swarm that require it for
    a service.
  prefs: []
  type: TYPE_NORMAL
- en: When we deploy the service that uses overlay network, the manager automatically
    extends the network to the nodes that are running the service tasks. Multi-host
    networking requires a store for service discovery, so now we will create a Docker
    machine to run this service.
  prefs: []
  type: TYPE_NORMAL
- en: '![Configuring an overlay network with the Docker Engine swarm node](graphics/image_07_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Overlay network across multiple hosts
  prefs: []
  type: TYPE_NORMAL
- en: For the following deployment, we will be using Docker machine application that creates
    the Docker daemon on a virtualization or cloud platform. For the virtualization
    platform, we will be using VMware fusion as the provider.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Docker-machine installation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Mulithost networking requires a store for service discovery, so we will create
    a Docker machine to run that service, creating the new Docker daemon:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To see how to connect your Docker client to the Docker Engine running on this
    virtual machine, run `docker-machine env swarm-consul`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll start the consul container for service discovery:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Create two Docker daemons to run the Docker cluster, the first daemon is the
    swarm node that will automatically run a Swarm container used to coordinate the
    cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Docker is up and running!
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To see how to connect your Docker client to the Docker Engine running on this
    virtual machine, run `docker-machine env swarm-0`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second daemon is the Swarm `secondary` node that will automatically run
    a Swarm container and report the state back to the `master` node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Docker is up and running!
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To see how to connect your Docker client to the Docker Engine running on this
    virtual machine, run `docker-machine env swarm-1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Docker executable will communicate with one Docker daemon. Since we are in
    a cluster, we''ll ensure the communication of the Docker daemon to the cluster
    by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'After this, we''ll create a private `prod` network with an overlay driver:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We will be starting the two dummy `ubuntu:12.04` containers using the `--net
    parameter`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following code snippet, we can see that this Docker container has two
    network interfaces: one connected to the private overlay network and another to
    the Docker bridge:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The other container will also be connected to the `prod` network interface
    existing on the other host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: This is how a private network can be configured across hosts in the Docker Swarm
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Comparison of all multi-host Docker networking solutions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '|  | **Calico** | **Flannel** | **Weave** | **Docker Overlay N/W** |'
  prefs: []
  type: TYPE_TB
- en: '| **Network Model** | Layer-3 Solution | VxLAN or UDP | VxLAN or UDP | VxLAN
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Name Service** | No | No | Yes | No |'
  prefs: []
  type: TYPE_TB
- en: '| **Protocol Support** | TCP,UDP, ICMP & ICMPv6 | All | All | All |'
  prefs: []
  type: TYPE_TB
- en: '| **Distributed Storage** | Yes | Yes | No | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| **Encryption Channel** | No | TLS | NaCI Library | No |'
  prefs: []
  type: TYPE_TB
- en: Configuring OpenvSwitch (OVS) to work with Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Open vSwitch** (**OVS**) is an open source **OpenFlow** capable virtual switch
    that is typically used with hypervisors to interconnect virtual machines within
    a host and between different hosts across networks. Overlay networks need to create
    a virtual data path using supported tunneling encapsulations, such as VXLAN or
    GRE.'
  prefs: []
  type: TYPE_NORMAL
- en: The overlay data path is provisioned between tunnel endpoints residing in the
    Docker host that gives the appearance of all hosts within a given provider segment
    being directly connected to one another.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a new container comes online, the prefix is updated in the routing protocol
    announcing its location via a tunnel endpoint. As the other Docker hosts receive
    the updates, the forwarding is installed into OVS for which tunnel endpoint the
    host resides. When the host is deprovisioned, a similar process occurs and tunnel
    endpoint Docker hosts remove the forwarding entry for the deprovisioned container:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Configuring OpenvSwitch (OVS) to work with Docker](graphics/image_07_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Communication between containers running on multiple hosts through OVS based
    VXLAN tunnels
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By default, Docker uses the Linux docker0 bridge; however, there are cases where
    OVS might be required instead of the Linux bridge. A single Linux bridge can handle
    1,024 ports only; this limits the scalability of Docker as we can only create
    1,024 containers, each with a single network interface.
  prefs: []
  type: TYPE_NORMAL
- en: Troubleshooting OVS single host setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Install OVS on a single host, create two containers, and connect them to an
    OVS bridge:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install OVS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Install the `ovs-docker` utility:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '![Troubleshooting OVS single host setup](graphics/image_07_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Single host OVS
  prefs: []
  type: TYPE_NORMAL
- en: Create an OVS bridge.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here, we will be adding a new OVS bridge and configuring it so that we can
    get the containers connected on a different network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Add a port from the OVS bridge to the Docker container.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create two `ubuntu` Docker containers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Connect the container to the OVS bridge:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Test the connection between the two containers connected using the OVS bridge
    with the `ping` command. First, find out their IP addresses:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we know the IP Address of `container1` and `container2` , we can run
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Troubleshooting OVS multiple host setups
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we will connect Docker containers on multiple hosts using OVS:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us consider our setup, as shown in the following diagram, that contains
    two hosts-`Host1` and `Host2`-running Ubuntu 14.04:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install Docker and OVS on both the hosts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Install the `ovs-docker` utility:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '![Troubleshooting OVS multiple host setups](graphics/image_07_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Multi Host container communication with OVS
  prefs: []
  type: TYPE_NORMAL
- en: Docker chooses a random network to run its containers by default. It creates
    a docker0 bridge and assigns an IP address (`172.17.42.1`) to it. So, both the `Host1`
    and `Host2` docker0 bridge IP addresses are the same, due to which it is difficult
    for containers in both the hosts to communicate. To overcome this, let's assign
    static IP addresses to the network, that is, (`192.168.10.0/24`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To change the default Docker subnet:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the following commands on `Host1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the `br0` OVS bridge:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the tunnel to the other host:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the `br0` bridge to the `docker0` bridge:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following commands on Host2:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the `br0` OVS bridge:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the tunnel to the other host and attach it to the:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the `br0` bridge to the `docker0` bridge:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: The docker0 bridge is attached to another bridge-`br0.` This time, it's an OVS
    bridge, which means that all traffic between the containers is routed through
    `br0` too. Additionally, we need to connect together the networks from both the
    hosts in which the containers are running. A GRE tunnel ([http://en.wikipedia.org/wiki/Generic_Routing_Encapsulation](http://en.wikipedia.org/wiki/Generic_Routing_Encapsulation))
    is used for this purpose. This tunnel is attached to the `br0` OVS bridge and,
    as a result, to `docker0` as well. After executing the preceding commands on both
    the hosts, you should be able to ping the `docker0` bridge addresses from both
    the hosts.
  prefs: []
  type: TYPE_NORMAL
- en: 'On Host1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'On Host2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Create containers on both the hosts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On Host1, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'On Host2, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Now we can ping `container2` from `container1.` In this way, we connect Docker
    containers on multiple hosts using OVS.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learnt how Docker networking is powered with docker0 bridge,
    its troubleshooting issues, and configuration. We also looked at troubleshooting
    the communication issues between Docker networks and the external network. Following
    that, we did some deep dive into libnetwork and the CNM and its life cycle. Then,
    we looked into containers' communication across multiple hosts using different
    networking options, such as Weave, OVS, Flannel, and Docker's latest overlay network,
    with comparison, and the troubleshooting issues involved in their configuration.
  prefs: []
  type: TYPE_NORMAL
- en: We saw that Weave creates a virtual network, OVS uses GRE tunneling technology,
    Flannel provides a separate subnet, and the Docker overlay sets up each host to
    connect containers on multiple hosts. After that, we looked into Docker network
    configuration with OVS and troubleshooting the single host and multiple host setup.
  prefs: []
  type: TYPE_NORMAL
