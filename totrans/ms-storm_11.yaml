- en: Apache Log Processing with Storm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we covered how we can integrate Storm with Redis, HBase,
    Esper and Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are covering the most popular use case of Storm, which is
    log processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following major sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Apache log processing elements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installation of Logstash
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring Logstash to produce the Apache log into Kafka
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Splitting the Apache log file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculating the country name, operating system type, and browser type
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying the search key words of your website
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Persisting the process data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kafka spout and defining the topology
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying the topology
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing the data into Elasticsearch and reporting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache log processing elements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Log processing is becoming a necessity for every organization, as they need
    to collect the business information from log data. In this chapter, we are basically
    working on how we can process the Apache log data using Logstash, Kafka, Storm,
    and Elasticsearch to collect the business information.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates all the elements that we are developing in
    this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00063.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.1: Log processing topology'
  prefs: []
  type: TYPE_NORMAL
- en: Producing Apache log in Kafka using Logstash
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As explained in [Chapter 8](part0137.html#42KT20-6149dd15e07b443593cc93f2eb31ee7b),
    *Integration of Storm and Kafka*, Kafka is a distributed messaging queue and can
    integrate very well with Storm. In this section, we will show you how we can use
    Logstash to read the Apache log file and publish it into the Kafka Cluster. We
    are assuming you already have the Kafka Cluster running. The installation steps
    of the Kafka Cluster are outlined in [Chapter 8](part0137.html#42KT20-6149dd15e07b443593cc93f2eb31ee7b),
    *Integration of Storm and Kafka*.
  prefs: []
  type: TYPE_NORMAL
- en: Installation of Logstash
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before moving on to the installation of Logstash, we are going to answer the
    questions: What is Logstash? Why are we using Logstash?'
  prefs: []
  type: TYPE_NORMAL
- en: What is Logstash?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Logstash is a tool that is used to collect, filter/parse, and emit the data
    for future use. Collect, parse, and emit are divided into three sections, which
    are called input, filter, and output:'
  prefs: []
  type: TYPE_NORMAL
- en: The **input** section is used to read the data from external sources. The common
    input sources are File, TCP port, Kafka, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **filter** section is used to parse the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **output** section is used to emit the data to some external source. The
    common external sources are Kafka, Elasticsearch, TCP, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why are we using Logstash?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We need to read the log data in real time and store it into Kafka before Storm
    starts the actual processing. We are using Logstash as it is very mature in reading
    the log files and pushing the logs data into Kafka.
  prefs: []
  type: TYPE_NORMAL
- en: Installation of Logstash
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We should have JDK 1.8 installed on the Linux box before installing Logstash,
    as we are going to use Logstash 5.4.1 and JDK 1.8 is the minimum requirement for
    this. The following are the steps to install Logstash:'
  prefs: []
  type: TYPE_NORMAL
- en: Download Logstash 5.4.1 from [https://artifacts.elastic.co/downloads/logstash/logstash-5.4.1.zip](https://artifacts.elastic.co/downloads/logstash/logstash-5.4.1.zip).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy the setup on all the machines whose Apache logs you want to publish into
    Kafka.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Extract the setup by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Configuration of Logstash
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we are going to define the Logstash configuration to consume the Apache
    logs and store them into Kafka.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a `logstash.conf` file and add the following lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We should change the following parameters in the preceding configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '`TOPIC_NAME`: Replace with the Kafka topic you want to use for storing the
    Apache log'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`KAFKA_IP` and `KAFKA_PORT`: Specify the comma separated list of all the Kafka
    nodes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PATH_TO_APACHE_LOG`: The location of Apache log file on the Logstash machine'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Go to Logstash home directory and execute the following command to start the
    log reading and publishing into Kafka:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now, the real-time log data is coming into the Kafka topic. In the next section,
    we are writing the Storm topology to consume the log data, process, and store
    the process data into the database.
  prefs: []
  type: TYPE_NORMAL
- en: Why are we using Kafka between Logstash and Storm?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we all know, Storm provides guaranteed message processing, meaning that every
    message enters into the Storm topology and will be processed at least once. In
    Storm, data loss is possible only at the spout end, if the processing capacity
    of Storm spout is less than the producing capacity of Logstash. Hence, to avoid
    the data getting lost at the Storm spout end, we will generally publish the data
    into a messaging queue (Kafka) and Storm spout will use the messaging queue as
    the data source.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting the Apache log line
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we are creating a new topology, which will read the data from Kafka using
    the `KafkaSpout` spout. In this section, we are writing an `ApacheLogSplitter` bolt,
    that has a logic to fetch the IP, status code, referrer, bytes sent, and so on,
    information from the Apache log line. As this is a new topology, we must first
    create the new project.
  prefs: []
  type: TYPE_NORMAL
- en: Create a new Maven project with `groupId` as `com.stormadvance` and `artifactId`
    as `logprocessing`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add the following dependencies in the `pom.xml` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We are creating an `ApacheLogSplitter` class in the `com.stormadvance.logprocessing`
    package. This class contains logic to fetch the different elements such as IP,
    referrer, user-agent, and so on, from the Apache log line.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The input for the `logSplitter(String apacheLog)` method is:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the `logSplitter(String apacheLog)` method is:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will create the `ApacheLogSplitterBolt` class in the `com.stormadvance.logprocessing`
    package. The `ApacheLogSplitterBolt` extends the `org.apache.storm.topology.base.BaseBasicBolt`
    class and passes the set of fields generated by `ApacheLogSplitter` class to the
    next bolt in the topology. The following is the source code of the `ApacheLogSplitterBolt`
    class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The output of the `ApacheLogSplitterBolt` class contains seven fields. These
    fields are `ip`, `dateTime`, `request`, `response`, `bytesSent`, `referrer`, and
    `useragent`.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying country, operating system type, and browser type from the log file
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section explains how we can calculate the user country name, operation
    system type, and browser type by analyzing the Apache log line. By identifying
    the country name, we can easily identify the location where our site is getting
    more attention and the location where we are getting less attention. Let''s perform
    the following steps to calculate the country name, operating system, and browser
    from the Apache log file:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We are using the open source `geoip` library to calculate the country name
    from the IP address. Add the following dependencies in the `pom.xml` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the following repository into the `pom`.`xml` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We are creating an `IpToCountryConverter` class in the `com.stormadvance.logprocessing`
    package. This class contains the parameterized constructor that is taking the
    location of the `GeoLiteCity.dat` file as input. You can find the `GeoLiteCity.dat`
    file in the resources folder of the `logprocessing` project. The location of the
    `GeoLiteCity.dat` file must be the same in all Storm nodes. The `GeoLiteCity.dat`
    file is a database which we are using to calculate the country name from the IP
    address. The following is the source code of the `IpToCountryConverter` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now download the `UserAgentTools` class from [https://code.google.com/p/ndt/source/browse/branches/applet_91/Applet/src/main/java/edu/internet2/ndt/UserAgentTools.java?r=856](https://code.google.com/p/ndt/source/browse/branches/applet_91/Applet/src/main/java/edu/internet2/ndt/UserAgentTools.java?r=856).
    This class contains logic to calculate the operating system and browser type from
    the user agent. You can also find the `UserAgentTools` class in the `logprocessing`
    project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's write the `UserInformationGetterBolt` class in the `com.stormadvance.logprocessing`
    package. This bolt uses the `UserAgentTools` and `IpToCountryConverter` class
    to calculate the country name, operating system, and browser.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The output of the `UserInformationGetterBolt` class contains 10 fields. These
    fields are `ip`, `dateTime`, `request`, `response`, `bytesSent`, `referrer`, `useragent`,
    `country`, `browser`, and `os`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the search keyword
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section explains how we can calculate the search keyword from the referrer
    URL. Suppose a referrer URL is [https://www.google.co.in/#q=learning+storm](https://www.google.co.in/#q=learning+storm).
    We will pass this referrer URL to a class and the output of the class will be
    *learning storm*. By identifying the search keyword, we can easily identify the
    keywords users are searching to reach our site. Let''s perform the following steps
    to calculate the keywords from the referrer URL:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We are creating a `KeywordGenerator` class in the `com.stormadvance.logprocessing`
    package. This class contains logic to generate the search keyword from the referrer
    URL. The following is the source code of the `KeywordGenerator` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: If the input for the `KeywordGenerator` class is: [https://in.search.yahoo.com/search;_ylt=AqH0NZe1hgPCzVap0PdKk7GuitIF?p=india+live+score&toggle=1&cop=mss&ei=UTF-8&fr=yfp-t-704](https://in.search.yahoo.com/search;_ylt=AqH0NZe1hgPCzVap0PdKk7GuitIF?p=india+live+score&toggle=1&cop=mss&ei=UTF-8&fr=yfp-t-704)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, the output of the `KeywordGenerator` class is:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We are creating a `KeyWordIdentifierBolt` class in the `com.stormadvance.logprocessing`
    package. This class calls the `KeywordGenerator` to generate the keyword from
    the referrer URL. The following is the source code of the `KeyWordIdentifierBolt`
    class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The output of the `KeyWordIdentifierBolt` class contains 11 fields. These fields
    are `ip`, `dateTime`, `request`, `response`, `bytesSent`, `referrer`, `useragent`,
    `country`, `browser`, `os`, and `keyword`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Persisting the process data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section will explain how we can persist the process data into a data store.
    We are using MySQL as a data store for the log processing use case. I am assuming
    you have MySQL installed on your centOS machine or you can follow the blog at
    [http://www.rackspace.com/knowledge_center/article/installing-mysql-server-on-centos](http://www.rackspace.com/knowledge_center/article/installing-mysql-server-on-centos)
    to install the MySQL on the centOS machine. Let''s perform the following steps
    to persist the record into MySQL:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following dependency to `pom.xml`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We are creating a `MySQLConnection` class in the `com.stormadvance.logprocessing`
    package. This class contains `getMySQLConnection(String ip, String database, String
    user, String password)` method, which returns the MySQL connection. The following
    is the source code of the `MySQLConnection` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we are creating a `MySQLDump` class in the `com.stormadvance.logprocessing`
    package. This class has a parameterized constructor that is taking MySQL `server
    ip, database name, user, and password` as arguments. This class calls the `getMySQLConnection(ip,database,user,password)`
    method of the MySQLConnection class to get the MySQL connection. The `MySQLDump`
    class contains the `persistRecord(Tuple tuple`) record method, and this method
    persists the input tuple into MySQL. The following is the source code of the `MySQLDump`
    class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create a `PersistenceBolt` class in the `com.stormadvance.logprocessing`
    package. This class implements `org.apache.storm.topology.IBasicBolt`. This class
    calls the `persistRecord(Tuple tuple)` method of the `MySQLDump` class to persist
    the records/events into MySQL. The following is the source code of the `PersistenceBolt`
    class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we have covered how we can insert the input tuple into a data
    store.
  prefs: []
  type: TYPE_NORMAL
- en: Kafka spout and define topology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section will explain how we can read the Apache log from a Kafka topic.
    This section also defines the `LogProcessingTopology` that will chain together
    all the bolts created in the preceding sections. Let''s perform the following
    steps to consume the data from Kafka and define the topology:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following dependency and repository for Kafka in the `pom.xml` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the following `build` plugins in the `pom.xml` file. It will let us execute
    the `LogProcessingTopology` using Maven:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create a `LogProcessingTopology` class in the `com.stormadvance.logprocessing`
    package. This class uses the `org.apache.storm.topology.TopologyBuilder` class
    to define the topology. The following is the source code of the `LogProcessingTopology`
    class with an explanation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This section covered how we can chain the different types of bolts into a topology.
    Also, we have covered how we can consume the data from Kafka. In the next section,
    we will explain how we can deploy the topology.
  prefs: []
  type: TYPE_NORMAL
- en: Deploy topology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section will explain how we can deploy the `LogProcessingTopology`. Perform
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the following command on the MySQL console to define the database schema:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: I am assuming you have already produced some data on the `apache_log` topic
    by using Logstash.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Go to the project home directory and run the following command to build the
    project:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following command to start the log processing topology in local
    mode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, go to MySQL console and check the rows in the `apachelog` table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we have covered how we can deploy the log processing topology.
    The next section will explain how we can generate the statistics from data stored
    in MySQL.
  prefs: []
  type: TYPE_NORMAL
- en: MySQL queries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section will explain how we can analyze or query in store data to generate
    some statistics. We will cover the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the page hit from each country
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculating the count of each browser
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculating the count of each operating system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate the page hit from each country
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Run the following command on the MySQL console to calculate the page hit from
    each country:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Calculate the count for each browser
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Run the following command on the MySQL console to calculate the count for each
    browser:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Calculate the count for each operating system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Run the following command on the MySQL console to calculate the count for each
    operating system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced you to how we can process the Apache log file,
    how we can identify the country name from the IP, how we can identify the user
    operating system and browser by analyzing the log file, and how we can identify
    the search keyword by analyzing the referrer field.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how we can solve machine learning problems
    through Storm.
  prefs: []
  type: TYPE_NORMAL
