- en: Web Scraping Navigation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, this book has focused on retrieving information for a single web page.
    Although this is the basis of web scraping, it does not cover the majority of
    use cases. More than likely, you will need to visit multiple web pages, or websites,
    in order to collect all of the information to fulfill your needs. This may entail
    visiting many known websites directly via a list or URLs, or following links discovered
    in some pages to more unknown places. There are many different ways of navigating
    your scraper through the web.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: How to follow links
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to submit forms with `POST` requests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to track your history to avoid loops
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The difference between breadth-first and depth-first crawling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Following links
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you have seen in many examples throughout this book, there are HTML elements
    denoted by `<a>` tags that contain `href` attributes that reference different
    URLs. These tags, called anchor tags, are how links are generated on a web page.
    In a web browser, these links would typically have a different font color, often
    blue, with an underline. As a user in a web browser, if you wanted to follow a
    link, you would usually just click on it and you would be redirected to the URL.
    As a web scraper, the clicking action is usually not necessary. Instead, you can
    send a `GET` request to the URL in the `href` attribute itself.
  prefs: []
  type: TYPE_NORMAL
- en: If you find that the `href` attribute lacks the `http://` or `https://` prefix
    and the hostname, you must use the prefix and hostname of the current web page.
  prefs: []
  type: TYPE_NORMAL
- en: Example – Daily deals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 4](e6f2de6a-420b-4691-9ba1-969ed6ad32ea.xhtml),* Parsing HTML*,
    we used an example where we retrieved the titles and the prices of the latest
    releases from the Packt Publishing website. You could collect more information
    about each book by following each link to the book''s main web page. In the following
    code example, we will add the navigation to make this possible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we have modified the `Each()` loop to extract the link for every
    product listed in the web page. Each link only contains the relative path to the
    book, so we prefix the [https://www.packtpub.com](https://www.packtpub.com) string
    to each link. Next, we navigate to the page itself by using the link we constructed,
    and scrape the desired information. At the end of each page, we sleep for `1`
    second so that our web scraper does not overburden the servers, observing the
    good etiquette we learned in [Chapter 3](16487efd-3a75-4823-ad19-627a83752cd4.xhtml),
    *Web Scraping Etiquette*.
  prefs: []
  type: TYPE_NORMAL
- en: Submitting forms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up until this point, we have been able to request information from servers using
    only HTTP `GET` requests. These requests cover the vast majority of web scraping
    tasks that you will encounter as you build your own web scraper. However, there
    will come a time where you may need to submit some kind of form data in order
    to retrieve the information you are looking for. This form data could entail search
    queries, or a login screen, or any page that would require you to type into a
    box and click a Submit button.
  prefs: []
  type: TYPE_NORMAL
- en: For simple websites, this is done using an HTML `<form>` element, containing
    one or more `<input>` elements and a Submit button. This `<form>` element usually
    has attributes defining the `action` (where to send the `<form>` data), and a
    `method` (the HTTP method to use). By default, the web page will use an HTTP `GET`
    request to send the form data, but it is very common to see HTTP `POST` requests
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: Example – Submitting searches
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following example, you will see how to simulate a form submission by
    using the properties and elements of an HTML form. We will use the form located
    at the [https://hub.packtpub.com/](https://hub.packtpub.com/) website to discover
    articles written about the Go programming language (commonly referred to as GoLang).
    On the home page of [https://hub.packtpub.com](https://hub.packtpub.com), there
    is a search box in the top-left corner of the page, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f8262a4d-c832-4b8e-80eb-b112ee0eeb3f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'By right-clicking on the Search... box, you should be able to inspect the element
    using your browser''s Developer tools. This reveals the HTML source code for the
    page, showing that this box is located in an HTML form. In Google Chrome, it looks
    similar to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aab70a2c-d612-4f8d-80eb-1911f3711ce1.png)'
  prefs: []
  type: TYPE_IMG
- en: This form uses the HTTP `GET` method, and submits to the [https://hub.packtpub.com/](https://hub.packtpub.com/)
    endpoint. The values for this form are taken from the `<input>` tags using the
    `name` attribute as a key, and the text within the Search box as the value. Because
    this form uses `GET` as a method, the key-value pairs are sent to the server as
    the query part of the URL. For our example, we want to submit GoLang as our search
    query. To do this, when you click the button to submit your query, your browser
    will send a `GET` request to [https://hub.packtpub.com/?s=Golang](https://hub.packtpub.com/?s=Golang).
  prefs: []
  type: TYPE_NORMAL
- en: The resulting page will contain all articles related to Go. You could scrape
    the title, dates, authors, and so on in order to keep an index of Go articles.
    By submitting this query periodically, you could discover new articles as soon
    as they are released.
  prefs: []
  type: TYPE_NORMAL
- en: Example – POST method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The form we used in the previous example used `GET` as a method. Hypothetically,
    if it were to use the `POST` method, there would be a slight difference in how
    the form is submitted. Instead of putting the values in the URL, you would need
    to build a request body instead. In the following example, the same form and search
    query will be structured as a `POST` request:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In Go, you build a form submission using the `url.Values` struct. You can use
    this to set the inputs of the form—`s=Golang` in our case—and submit it using
    the `http.Post()` function. This technique will only help if the form uses `POST`
    as its method.
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding loops
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are building a web scraper that follows links, you might need to be aware
    of which pages you've already visited. It's quite possible that a page you are
    visiting contains a link to a page you have already visited, sending you into
    an infinite loop. Therefore, it is very important to build a tracking system into
    your scraper that records its history.
  prefs: []
  type: TYPE_NORMAL
- en: The simplest data structure for storing a unique collection of items would be
    a set. The Go standard library does not have a set data structure, but it can
    be emulated by using a `map[string]interface{}{}`.
  prefs: []
  type: TYPE_NORMAL
- en: An `interface{}` in Go is a generic object, similar to `java.lang.Object`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Go, you can define a map as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, we would use the visited URL as the key, and anything you want
    as the value. We will just use `nil`, because as long as the key is present, we
    know we have visited the site. Adding a site that we have visited would simply
    insert the URL as the key and `nil` as a value, as given in the following code
    block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'When you try to retrieve a value from a map, using a given key, Go will return
    two values: the value for the key if it exists, and a Boolean, stating whether
    or not the key exists in the map. In our case, we are only concerned about the
    latter.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We would check for a site visit like the one demonstrated in the following
    code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Breadth-first versus depth-first crawling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that you have the ability to navigate to different pages, as well as the
    ability to avoid getting stuck in a loop, you have one more important choice to
    make when crawling a website. In general, there are two main approaches to covering
    all pages by following links: breadth-first, and depth-first. Imagine that you
    are scraping a single web page that contains 20 links. Naturally, you would follow
    the first link on the page. On the second page, there are ten more links. Herein
    lies your decision: follow the first link on the second page, or go back to the
    second link on the first page.'
  prefs: []
  type: TYPE_NORMAL
- en: Depth-first
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you choose to follow the first link on the second page, this would be considered
    depth-first crawling:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/da7a1fd2-dfee-4cd0-9c7d-ce7f8233b4ca.png)'
  prefs: []
  type: TYPE_IMG
- en: Your scraper will continue to follow the links as deeply as possible to collect
    all of the pages. In the case of products, you might be following a path of recommendations
    or similar items. This might take you to products far outside the original starting
    point of your scraper. On the other hand, it may also help build a tighter network
    of related items quickly. On websites containing articles, depth-first crawling
    will send you back in time quickly, as linked pages would most likely be a reference
    to a previously written article. This will help you reach the origins of many
    linked paths quickly.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 6](a1ed8fec-44c3-4798-beab-27a3c08dc151.xhtml),*Protecting Your
    Web Scraper*, we will learn how to avoid some of the pitfalls of depth-first crawling
    by ensuring we have the proper boundaries in place.
  prefs: []
  type: TYPE_NORMAL
- en: Breadth-first
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you choose to follow the second link on the first page, this would be considered
    breadth-first crawling:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f9a154db-834e-47a8-af96-2dddb69ced25.png)'
  prefs: []
  type: TYPE_IMG
- en: Using this technique, you would most likely stay within your original search
    domain for longer. For example, if you were on a website with products and initiated
    a search for shoes, the majority of the links on the page would be related to
    shoes. You would be collecting the links within the same domain first. As you
    move deeper within the website, recommended items may lead you to other types
    of clothing. Breadth-first crawling would help you collect full clusters of pages
    more quickly.
  prefs: []
  type: TYPE_NORMAL
- en: There is no right or wrong technique for how to navigate your scraper; it all
    depends on your specific needs. Depth-first crawling will reveal the origins of
    specific topics, whereas breadth-first crawling will complete a full cluster before
    discovering new content. You might even use a combination of techniques if this
    suits your requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Navigating with JavaScript
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far we have focused on simple web pages where all of the information needed
    is only available in the HTML file. This is not always the case for more modern
    websites, which contain JavaScript code responsible for loading extra information
    after the initial page loads. In many websites, when you perform a search, the
    initial page might display with an empty table and, in the background, make a
    second request to collect the actual results to display. In order to do this,
    custom code written in JavaScript is run by your web browser. Using the standard
    HTTP client would not be sufficient in this case and you would need to use an
    external browser that supports JavaScript execution.
  prefs: []
  type: TYPE_NORMAL
- en: In Go, there are many options for integrating scraper code with web browsers
    thanks to a few standard protocols. The WebDriver protocol is the original standard
    developed by Selenium and is supported by most major browsers. This protocol allows
    programs to send a browser's commands, such as load a web page, wait for an element,
    click a button, and capture the HTML. These commands would be necessary to collect
    results from a web page where items are loaded via JavaScript. One such library
    supporting the WebDriver client protocol is `selenium` by GitHub user `tebeka`.
  prefs: []
  type: TYPE_NORMAL
- en: Example – Book reviews
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: On the Packt Publishing website, book reviews are loaded via JavaScript and
    are not visible when the page is first loaded. This example demonstrates how to
    use the `selenium` package to scrape reviews from a book listing on the Packt
    Publishing site.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `selenium` package relies on four external dependencies in order to function
    properly:'
  prefs: []
  type: TYPE_NORMAL
- en: A Google Chrome or Mozilla Firefox web browser
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A WebDriver that is compatible with Chrome or Firefox, respectively
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Selenium Server binary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Java
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of these dependencies will be downloaded by the `selenium` script during
    installation, except for Java.
  prefs: []
  type: TYPE_NORMAL
- en: Please ensure that you have Java installed on your machine. If not, please download
    and install the official version from [https://www.java.com/en/download/help/download_options.xml](https://www.java.com/en/download/help/download_options.xml).
  prefs: []
  type: TYPE_NORMAL
- en: 'First, install the package via the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This will install `selenium` inside your `GOPATH` at `$GOPATH/src/github.com/tebeka/selenium`.
    This installation script relies on a number of other packages in order to run.
    You can install them using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we install the browsers, drivers, and `selenium` binary that the code
    example needs. Navigate to the `Vendor` folder inside the `selenium` directory
    and complete the installation by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that `selenium` and all of its dependencies are set up, you can create
    a new folder in your `$GOPATH/src` with a `main.go` file. Let''s step through
    the code that you will need to write in order to collect reviews from a book.
    First, let''s look at the `import` statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, our program only relies on the `selenium` package to run the
    example! Next, we can see the beginning of the `main` function and define a few
    important variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we render the constants for the paths to the `selenium` server executable,
    and the path for the Firefox WebDriver, known as the `geckodriver`. If you were
    to run this example with Chrome, you would provide the path to your `chromedriver`
    instead. All of these files were installed by the `init.go` program run earlier
    and your paths will be different from the ones written here. Please be sure to
    change these to suit your environment. The next part of the function initializes
    the `selenium` driver:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '`defer` statements tell Go to run the following command at the end of the function.
    It is good practice to defer your cleanup statements so you don''t forget to put
    them at the end of your function!'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we create the `selenium` driver by providing the paths to the executable
    that it needs, as well as the port through which our code will communicate with
    the `selenium` server. We also obtain a connection to the WebDriver by calling
    `NewRemote()`. The `wd` object is the WebDriver connection that we will use to
    send commands to the Firefox browser, as demonstrated in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We tell the browser to load the web page for *Mastering Go*, by Mihalis Tsoukalos,
    and wait for our CSS query for product reviews to return more than one result.
    This will loop indefinitely until the reviews appear. Once we discover the reviews,
    we print the text for each one.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered the basics of how to navigate your web scraper through
    a website. We looked into the anatomy of a web link, and how to use HTTP `GET`
    requests to simulate following a link. We looked at how HTTP forms, such as search
    boxes, generate HTTP requests. We also saw the difference between HTTP `GET` and
    `POST` requests, and how to send `POST` requests in Go. We also covered how to
    avoid loops by tracking your history. Finally, the differences between breadth-first
    and depth-first web crawling, and their respective trade-offs were covered.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 6](a1ed8fec-44c3-4798-beab-27a3c08dc151.xhtml), *Protecting Your
    Web Scraper*, we will look at ways to ensure your safety as you crawl the web.
  prefs: []
  type: TYPE_NORMAL
