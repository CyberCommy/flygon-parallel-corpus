- en: Chapter 14. Scalable Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: L2 cache                                 7 ns
  prefs: []
  type: TYPE_NORMAL
- en: Main memory                       100 ns
  prefs: []
  type: TYPE_NORMAL
- en: Disk (random seek)              2,000,000 ns
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, Spark provides in-memory processing capabilities, including many
    optimizations that take advantage of the fast caches available (L1/L2/L3 caches).
    Therefore, it can avoid unnecessarily reading from main memory or spilling to
    disk it's important that your analytics take full advantage of these efficiencies. This
    was introduced as part of Project Tungsten, [https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html).
  prefs: []
  type: TYPE_NORMAL
- en: '**Only optimize after observation**: There''s a famous saying by Donald Knuth,
    the legendary computer scientist and author, that *premature optimization is the
    root of all evil*. While this sounds extreme, what he means is that all performance-related
    tweaks or optimizations should be based on empirical evidence rather than preemptive
    intuition. As such predictions very often fail to correctly identify performance
    problems, and instead give rise to poor design choices that are later regretted.
    But contrary to what you might think, the suggestion here is not that you just
    forget about performance until the end, in fact quite the reverse. In an environment
    where the size of the data and hence the length of time any operation takes dictates
    everything, it''s fundamental to begin optimization early in the analytic design
    process. But isn''t this a contradiction of Knuth''s law? Well, no. In terms of
    performance, simplicity is often the key. The approach should be evidence-based
    so start simple, carefully observe the performance of your analytic at runtime
    (through the use of analytic tuning and code profiling, see the next section),
    perform targeted optimizations that correct the problems identified, and repeat.
    Over-engineering is usually as much to blame in poorly performing analytics as
    choosing slow algorithms, but it can be much harder to fix down the line.*   **Start
    small and scale-up**: Start with small data samples. While an analytic may *eventually*
    be required to run over a petabyte of data, starting with a small dataset is definitely
    advisable. Sometimes only a handful of rows are required to determine whether
    an analytic is working as expected. And more rows can be added to prove out the
    various test and edge cases. It''s more about breadth of coverage here rather
    than volume. The analytic design process is extremely iterative and judicious
    use of data sampling will pay dividends during this phase; while even a small
    dataset will allow you to measure the impact on performance as you incrementally
    increase the size of the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bottom line is that writing analytics, particularly over data you are unfamiliar
    with, can take time and there are no shortcuts.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have some guidelines, let's focus on how they apply to Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Spark architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Spark is designed to simplify the laborious, and sometimes error prone
    task of highly-parallelized, distributed computing. To understand how it does
    this, let's explore its history and identify what Spark brings to the table.
  prefs: []
  type: TYPE_NORMAL
- en: History of Spark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Apache Spark implements a type of *data parallelism* that seeks to improve
    upon the MapReduce paradigm popularized by Apache Hadoop. It extended MapReduce
    in four key areas:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Improved programming model**: Spark provides a higher level of abstraction
    through its APIs than Hadoop; creating a programming model that significantly
    reduces the amount of code that must be written. By introducing a fluent, side-effect-free,
    function-oriented API, Spark makes it possible to reason about an analytic in
    terms of its transformations and actions, rather than just sequences of mappers
    and reducers. This makes it easier to understand and debug.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Introduces workflow**: Rather than chaining jobs together (by persisting
    results to disk and using a third-party workflow scheduler, as with traditional
    MapReduce), Spark allows analytics to be decomposed into tasks and expressed as
    **Directed Acyclic Graphs** (**DAGs**). This has the immediate effect of removing
    the need to materialize data, but also means it has much more control over how
    analytics are run, including enabling efficiencies such as cost-based query optimization
    (seen in the catalyst query planner).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Better Memory Utilization**: Spark exploits the memory on each node for in-memory
    caching of datasets. It permits access to caches between operations to improve
    performance over basic MapReduce. This is particularly effective for iterative
    workloads, such as **stochastic gradient descent** (**SGD**), where a significant
    improvement in performance can usually be observed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integrated Approach**: With support for streaming, SQL execution, graph processing,
    machine learning, database integration, and much more, it offers one tool to rule
    them all! Before Spark, specialist tools were needed, for example, Storm, Pig,
    Giraph, Mahout, and so on. Although there are situations where the specialist
    tools can provide better results, Spark''s on-going commitment to integration
    is impressive.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to these general improvements, Spark offers many other features.
    Let's take a look inside the box.
  prefs: []
  type: TYPE_NORMAL
- en: Moving parts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At a conceptual level, there are a number of key components inside Apache Spark,
    many of which you may know already, but let''s review them within the context
    of the scalability principles we''ve outlined:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Moving parts](img/image_14_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Driver
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **Driver** is the main entry point for Spark. It's the program that you
    start, it runs in a single JVM, and it initiates and controls all of the operations
    in your job.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of performance, it's likely that you'll want to avoid bringing large
    datasets back to the driver, as running such operations (such as `rdd.collect`)
    can often cause an `OutOfMemoryError`. This happens when the size of data being
    returned exceeds the JVM heap size of the driver, as specified by `--driver-memory`.
  prefs: []
  type: TYPE_NORMAL
- en: SparkSession
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As the driver is starting, the `SparkSession` class is initialized. The `SparkSession` class
    provides access to all of Spark's services, via the relevant context, such as
    `SQLContext`, `SparkContext`, and `StreamingContext` classes.
  prefs: []
  type: TYPE_NORMAL
- en: It's also the place to tune Spark's runtime performance-related properties.
  prefs: []
  type: TYPE_NORMAL
- en: Resilient distributed datasets (RDDs)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An **Resilient Distributed Dataset** (**RDD**) is the underlying abstraction
    representing a distributed set of homogenous records.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although data may be physically stored over many machines in the cluster, analytics
    are intentionally unaware of their actual location: they deal only with RDDs.
    Under the covers, RDDs consist of partitions, or contiguous blocks of data, like
    slices of cake. Each partition has one or more replicas, or copies, and Spark
    is able to determine the physical location of these replicas in order to decide
    where to run transformation tasks to ensure data locality.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For an example of how the physical location of replicas is determined, see
    `getPreferredLocations` in: [https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala](https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala).'
  prefs: []
  type: TYPE_NORMAL
- en: RDDs are also responsible for ensuring that data is cached appropriately from
    the underlying block storage, for example, HDFS.
  prefs: []
  type: TYPE_NORMAL
- en: Executor
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Executors** are processes that run on the worker nodes of your cluster. When
    launched, each executor connects back to the driver and waits for instructions
    to run operations over data.'
  prefs: []
  type: TYPE_NORMAL
- en: You decide on how many executors your analytic needs and this becomes your maximum
    level of parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unless using dynamic allocation. In which case, the maximum level of parallelism
    is infinity until configured using `spark.dynamicAllocation.maxExecutors`. See
    Spark configuration for details.
  prefs: []
  type: TYPE_NORMAL
- en: Shuffle operation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **shuffle** is the name given to the transfer of data between executors
    that occurs as part of an operation whenever data must be physically moved, in
    order to compute a calculation. It typically occurs when data is grouped so that
    all records with the same key are together on a single machine, but it can also
    be used strategically to repartition data for greater levels of parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: However, as it involves both (i) the movement of data over the network and (ii)
    its persistence to disk, it is generally considered a slow operation. And hence,
    the shuffle is an area of great significance to scalability more on this later.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster Manager
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **Cluster Manager** sits outside of Spark, acting as a resource negotiator
    for the cluster. It controls the initial allocation of physical resources, so
    that Spark is able to start its executors on machines with the requisite number
    of cores and memory.
  prefs: []
  type: TYPE_NORMAL
- en: Although each cluster manager works in a different way, your choice is unlikely
    to have any measurable impact on algorithmic performance.
  prefs: []
  type: TYPE_NORMAL
- en: Task
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A **Task** represents an instruction to run a set of operations over a single
    partition of data. Each task is serialized over to an executor by the driver and,
    is in effect, what is referred to by the expression moving the processing to the
    data.
  prefs: []
  type: TYPE_NORMAL
- en: DAG
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A **DAG** represents the logical execution plan of all transformations involved
    in the execution of an action. Its optimization is fundamental to the performance
    of the analytic. In the case of SparkSQL and Datasets optimization is performed
    on your behalf by the catalyst optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: DAG scheduler
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **DAG scheduler** creates a physical plan, by dividing the DAG into stages
    and, for each stage, creating a corresponding set of tasks (one for each partition).
  prefs: []
  type: TYPE_NORMAL
- en: '![DAG scheduler](img/image_14_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Transformations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Transformations** are a type of operation. They typically apply a user-defined
    function to each record in an RDD. There are two kinds of transformation, *narrow*
    and *wide*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Narrow transformations are operations that are applied locally to partitions
    and as such do not require data to be moved in order to compute correctly. They
    include: `filter`, `map`, `mapValues`, `flatMap`, `flatMapValues`, `glom`, `pipe`,
    `zipWithIndex`, `cartesian`, `union`, `mapPartitionsWithInputSplit`, `mapPartitions`,
    `mapPartitionsWithIndex`, `mapPartitionsWithContext`, `sample`, `randomSplit`.'
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, wide transformations are operations that require data to be moved
    in order to compute correctly. In other words, they require a shuffle. They include: `sortByKey`,
    `reduceByKey`, `groupByKey`, `join`, `cartesian`, `combineByKey`, `partitionBy`,
    `repartition`, `repartitionAndSortWithinPartitions`, `coalesce`, `subtractByKey`,
    `cogroup`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `coalesce`, `subtractByKey` and `cogroup` transformations could be narrow
    depending on where data is physically situated.
  prefs: []
  type: TYPE_NORMAL
- en: In order to write scalable analytics, it's important to be aware of which type
    of transformation you are using.
  prefs: []
  type: TYPE_NORMAL
- en: Stages
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A **stage** represents a group of operations that can be physically mapped
    to a task (one per partition). There are a couple of things to note about stages:'
  prefs: []
  type: TYPE_NORMAL
- en: Any sequence of narrow transformations appearing consecutively in a DAG are
    pipelined together into a single stage. In other words, they execute in order,
    on the same executor and hence against the same partition and do not need a shuffle.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whenever a wide transformation is encountered in a DAG, a stage boundary is
    introduced. Two stages (or more in the case of join, and so on) now exist and
    the second cannot begin until the first has finished (see `ShuffledRDD` class
    for more details).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Actions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Actions** are another type of operation within Spark. They''re typically
    used to perform a parallel write or transfer of data back to the driver. While
    other transformations are lazily evaluated, it is the action that triggers the
    execution of a DAG.'
  prefs: []
  type: TYPE_NORMAL
- en: Upon invoking an action, its parent RDD gets submitted to the `SparkSession` or `SparkContext` classes
    within the driver and the DAG scheduler generates a DAG for execution.
  prefs: []
  type: TYPE_NORMAL
- en: Task scheduler
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **task scheduler** receives a set of tasks determined by the DAG scheduler
    (one task per partition) and schedules each to run on an appropriate executor
    in conjunction with data locality.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have gained an understanding of the Spark architecture, let's prepare
    for writing scalable analytics by introducing some of the challenges, or *gotchas*
    that you might face if you're not careful. Without knowledge of these up-front,
    you could lose time trying to figure them out on your own!
  prefs: []
  type: TYPE_NORMAL
- en: Algorithmic complexity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As well as the obvious effect of the size of your data, the performance of an
    analytic is highly dependent on the nature of the problem you're trying to solve.
    Even some seemingly simple problems, such as a depth first search of a graph,
    do not have well-defined algorithms that perform efficiently in distributed environments.
    This being the case, great care should be taken when designing analytics to ensure
    that they exploit patterns of processing that are readily parallelized. Taking
    the time to understand the nature of your problem in terms of complexity before
    you start, can pay off in the long term. In the next section, we'll show you how
    to do this.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Generally speaking, *NC-complete* problems are parallelizable, whereas P-complete
    problems are not: [https://en.wikipedia.org/wiki/NC_(complexity)](https://en.wikipedia.org/wiki/NC_(complexity)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another thing to note is that distributed algorithms will often be much slower
    than single-threaded applications when run on small data. It''s worth bearing
    in mind that in the scenarios where all of your data fits onto a single machine,
    the overhead of Spark: spawning processes, transferring data, and the latency
    introduced by interprocess communications, will rarely payoff. Investment in this
    approach only really starts to assist in the case where your datasets are large
    enough that they don''t fit comfortably into memory, then you will notice gains
    in throughput, the amount of data you can process in unit time, as a result of
    using Spark.'
  prefs: []
  type: TYPE_NORMAL
- en: Numerical anomalies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When processing large amounts of data, you might notice some strange effects
    with numbers. These oddities relate to the universal number representations of
    modern machines and specifically to the concept of *precision*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate the effect, consider the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Notice how a positive number is turned into a negative number simply by adding
    one. This phenomenon is known as a **number overflow** and it occurs when a calculation
    results in a number that is too large for its type. In this case, an `Int` has
    a fixed-width of 32-bits, so when we attempt to store a 33-bit number, we get
    an overflow, resulting in a negative. This type of behavior can be demonstrated
    for any numeric type, and as a result of any arithmetic operation.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is due to the signed, fixed-width, two's complement number representations
    adopted by most modern processor manufacturers (and hence Java and Scala).
  prefs: []
  type: TYPE_NORMAL
- en: 'Although overflows occur in the course of normal programming, it''s much more
    apparent when dealing with large datasets. It can occur even when performing relatively
    simple calculations, such as summations or means. Let''s consider the most basic
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Datasets are not immune:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Of course, there are strategies for handling this; for example by using alternative
    algorithms, different data types, or changing the unit of measurement. However,
    a plan for tackling these types of issues should always be taken into account
    in your design.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another similar effect is the loss of significance caused by rounding errors
    in calculations limited by their precision. For illustrative purposes, consider
    this really basic (and not very sophisticated!) example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Here, we were expecting the answer `6.310887552645619145394993304824655E-30`,
    but instead we get zero. This is a clear loss of precision and significance, demonstrating
    another type of behavior that you need to be aware of when designing analytics.
  prefs: []
  type: TYPE_NORMAL
- en: 'To cope with these issues, Welford and Chan devised an online algorithm for
    calculating the `mean` and `variance`. It seeks to avoid problems with precision.
    Under the covers, Spark implements this algorithm, and an example can be seen
    in the PySpark StatCounter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a deeper look into how it''s calculating the mean and variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '`delta`: The `delta` is the difference between mu (the current running average)
    and the new value under consideration. It measures the change in value between
    data points and because of this it''s always small. It''s basically a magic number
    that ensures that the calculation never involves summing all the values as this
    would potentially lead to an overflow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mu`: The mu represents the current running average. At any given time, it''s
    the total of the values seen so far, over the count of those values. The `mu`
    is calculated incrementally by continually applying the delta.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`m2`: The `m2` is the sum of the mean squared difference. It assists the algorithm
    in avoiding loss of significance by adjusting the precision during the calculation.
    This reduces the amount of information lost through rounding errors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As it happens, this particular online algorithm is specifically for computing
    statistics, but the online approach may be adopted by the design of any analytic.
  prefs: []
  type: TYPE_NORMAL
- en: Shuffle
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we identified earlier in our section on principles, moving data around is
    expensive and this means that one of the main challenges when writing any scalable
    analytic is that of minimizing the transfer of data. The overhead of management
    and handling of data transfer is still, at this moment in time, a very costly
    operation. We''ll discuss more on how to tackle this later in the chapter, but
    for now we''ll build awareness of the challenges around data locality; knowing
    which operations are OK to use and which should be avoided, whilst also understanding
    the alternatives. Some of the key offenders are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cartesian()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reduce()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PairRDDFunctions.groupByKey()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But be aware, with a little forethought, using these can be avoided altogether.
  prefs: []
  type: TYPE_NORMAL
- en: Data schemes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Choosing a schema for your data will be critical to your analytic design. Obviously,
    often you have no choice about the format of your data; either a schema will be
    imposed on you or your data may not have a schema. Either way, with techniques
    such as "temporary tables" and schema-on-read (see [Chapter 3](ch03.xhtml "Chapter 3. Input
    Formats and Schema"), *Input Formats and Schema* for details), you still have
    control over how data is presented to your analytic - and you should take advantage
    of this. There are an enormous number of options here and selecting the right
    one is part of the challenge. Let''s discuss some common approaches and start
    with some that are not so good:'
  prefs: []
  type: TYPE_NORMAL
- en: '**OOP**: **Object-oriented programming** (**OOP**) is the general concept of
    programming by decomposing problems into classes that model real world concepts.
    Typically, definitions will group both data and behavior, making them a popular
    way to ensure that code is compact and understandable. In the context of Spark,
    however, creating complex object structures, particularly ones that includes rich
    behavior, is unlikely to benefit your analytic in terms of readability or maintenance.
    Instead, it is likely to vastly increase the number of objects requiring garbage
    collection and limit the scope for code reuse. Spark is designed using a *functional
    approach*, and while you should be careful about abandoning objects altogether,
    you should strive to keep them simple and reuse object references where it is
    safe to do so.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**3NF**: For decades, databases have been optimized for certain types of schema
    - relational, star, snowflake, and so on. And techniques such as **3rd Normal
    Form** (**3NF**) work well to ensure the correctness of traditional data models.
    However, within the context of Spark, forcing dynamic table joins, or/and joining
    facts with dimensions, results in shuffles, potentially many shuffles, which is
    ultimately bad for performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Denormalization**: Denormalization is a practical way to ensure that your
    analytic has all the data it needs without having to resort to a shuffle. Data
    can be arranged so that records processed together are also stored together. This
    has the added cost of having to store duplicates of much of the data, but it''s
    often a trade-off that pays off. Particularly as there are techniques and technologies
    that help overcome the cost of duplication, such as columnar-oriented storage,
    column-pruning, and so on. More on this later.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we understand some of the difficulties that you might encounter when
    designing analytics, let's get into the detail of how to apply patterns that address
    these and ensure that your analytics run well.
  prefs: []
  type: TYPE_NORMAL
- en: Plotting your course
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It's easy to overlook planning and preparation when you're preoccupied with
    experimenting on the latest technologies and data! Nevertheless, the *process*
    of how you write scalable algorithms is just as important as the algorithms themselves.
    Therefore, it's crucial to understand the role of planning in your project and
    to choose an operating framework that allows you to respond to the demands of
    your goals. The first recommendation is to adopt an *agile development methodology*.
  prefs: []
  type: TYPE_NORMAL
- en: The distinctive ebb and flow of analytic authoring may mean that there is just
    no natural end to the project. By being disciplined and systematic with your approach,
    you can avoid many pitfalls that lead to an under performing project and poorly
    performing code. Conversely, no amount of innovative, open source software or
    copious corpus will rescue a project with no structure.
  prefs: []
  type: TYPE_NORMAL
- en: As every data science project is slightly different, there's no right or wrong
    answers when it comes to overall management. Here we offer a set of guidelines,
    or best practice, based on experience, that should help navigate the data minefield.
  prefs: []
  type: TYPE_NORMAL
- en: When dealing with large quantities of data, even small mistakes in calculations
    may result in many lost hours - waiting for jobs to process without any certainty
    of when, or whether, they will finish. Therefore, generally speaking, one should
    approach analytic authoring with a similar level of rigor as one would the design
    of an experiment. The emphasis here should be on practicality and every care should
    be taken to anticipate the effect of changes on processing time.
  prefs: []
  type: TYPE_NORMAL
- en: Here are some tips for staying out of trouble during the development process.
  prefs: []
  type: TYPE_NORMAL
- en: Be iterative
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Take an iterative approach to your everyday work and build your analytics incrementally.
    Add functionality as you go, and use unit testing to ensure that you have a solid
    base before adding more features. For each code change you make, consider adopting
    an iterative cycle, such as the one shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Be iterative](img/image_14_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Let's discuss each of these steps in turn.
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As always, the first step is to gain an understanding of the data you'll be
    processing. As discussed previously, it's likely that you'll have to attend to
    all the edge cases present in your corpus. You should consider starting with a
    basic data profile in order to understand whether the data meets your expectations,
    in terms of veracity and quality, where the potential risks are and how you might
    segment it into classes so that it can be processed. An approach to this is described
    in detail in [Chapter 4](ch04.xhtml "Chapter 4. Exploratory Data Analysis"), *Explorative
    Data Analysis*.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to **Exploratory data analysis** (**EDA**), understanding the shape
    of your data will allow you to reason about the design of your analytic and anticipate
    additional demands that you may have to cater for.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, here is a quick data profile to show the completeness of some
    GDELT news article downloads for a given day:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: For this particular day, you'll see here that in fact the majority of GKG records
    surveyed have no associated news article content. Although this could be for a
    variety of reasons, the point to note is that these missing articles form a new
    class of records that will require different processing. We'll have to write an
    alternate flow for these records, and that flow might have different performance
    characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: Scale up slowly
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In terms of data, it is important to *start small and scale up*. Don't be afraid
    to start with a subset of your corpus. Consider choosing a subset identified as
    significant during the data profile stage, or in many cases it's beneficial to
    use a handful of records in each subset. What's important here is that the subset
    you choose is representative enough to prove the particular use case, function
    or feature, yet small enough to allow for *timely iterations*.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding GDELT example, we could temporarily ignore records with no
    content and deal only with the subset containing news articles. In this way, we'll
    filter out any troublesome cases and handle them in later iterations.
  prefs: []
  type: TYPE_NORMAL
- en: Having said that, eventually you'll definitely want to reintroduce all the subsets
    and edge cases present in your corpus. While it's fine to do this in a piecemeal
    way, by including more important classes first and leaving edge cases until later,
    it is necessary to ultimately understand the behavior of every record in your
    dataset, even outliers, because the chances are they won't be one offs. You will
    also need to understand the effect that any data has on your analytic when it
    is seen in production, regardless of how infrequently, in order to avoid an entire
    run failing due to a single rogue record.
  prefs: []
  type: TYPE_NORMAL
- en: Estimate performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As you write each transformation, be aware of the time-cost in terms of complexity.
    For example, it's good to ask yourself, "how would the running time be affected
    if i doubled the input?". When considering this, it's helpful to think in terms
    of the **Big O Notation**. Big O will not give you an exact performance figure;
    it does not take into account practical factors, such as number of cores, available
    memory, or network speed. However, it can be useful as a guide in order to get
    an indicative measure of the processing complexity.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a reminder, here are some common notations, in order of time-complexity
    (preferred-first):'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Notation** | **Description** | **Example Operations** |'
  prefs: []
  type: TYPE_TB
- en: '| O(1) | Constant (Quick)Not dependent on size | `broadcast.value``printSchema`
    |'
  prefs: []
  type: TYPE_TB
- en: '| O(log n) | Logarithmic*Grows with the height of a balanced tree of n nodes*
    | `pregel``connectedComponents` |'
  prefs: []
  type: TYPE_TB
- en: '| O(n) | Linear*Grows proportionally with n (rows)* | `map``filter``count``reduceByKey``reduceGroups`
    |'
  prefs: []
  type: TYPE_TB
- en: '| O(n + m) | Linear*Grows proportionally with n and m (other dataset)* | `join``joinWith``groupWith``cogroup``fullOuterJoin`
    |'
  prefs: []
  type: TYPE_TB
- en: '| O(n²) | Quadratic*Grows as the square of n* | `cartesian` |'
  prefs: []
  type: TYPE_TB
- en: '| O(n²c) | Polynomial (Slow)*Grows with n and c (columns)* | `LogisticRegression.fit`
    |'
  prefs: []
  type: TYPE_TB
- en: Using this kind of notation can assist you when choosing the most efficient
    operation during the design phase of your analytic. For an example of how to replace
    a `cartesian` join [O(n2)] with `connectedComponents` [O(log n)], see [Chapter
    10](ch10.xhtml "Chapter 10. Story De-duplication and Mutation"), *Story De-duplication
    and Mutation*.
  prefs: []
  type: TYPE_NORMAL
- en: It also allows you to estimate your analytics performance characteristics prior
    to executing your job. You can use this information in conjunction with the parallelism
    and configuration of your cluster to ensure that when it's time to do a full-run
    of your job, maximum resources are employed.
  prefs: []
  type: TYPE_NORMAL
- en: Step through carefully
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Spark's fantastic, fluent, function-oriented API is designed to allow the *chaining
    together* of transformations. Indeed, this is one of its main benefits, and as
    we've seen it is especially convenient for building data science pipelines. However,
    it's because of this convenience that it is rather tempting to write a string
    of commands and then execute them all in one run. As you might have already found
    with this approach, if a failure occurs or you're not getting the results you
    expect, all processing up to that point is lost and must be replayed. As the development
    process is characteristically iterative, this results in an overly elongated cycle
    that can too often result in lost time.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid this problem, it's important to be able to **fail fast** during each
    iteration. Therefore, consider getting into the habit of running one step at a
    time on a small sample of data before proceeding. By issuing an action, say a
    count or small take, after each and every transformation, you can check for correctness
    and ensure that each step is successful before moving onto the next step. By investing
    in a little up-front care and attention, you'll make better use of your time and
    your development cycles will tend to be quicker.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to this, and whenever possible during the development life cycle,
    consider persisting intermediate datasets to disk to avoid having to repeatedly
    recalculate, particularly if they are computationally heavy, or potentially reusable.
    This is a form of on-disk caching and it is a similar approach to *checkpointing*
    (as used in spark streaming when storing state). In fact, it's a common trade-off
    when writing CPU-intensive analytics, and it is especially useful when developing
    analytics that run over large datasets. However, it is a trade-off, so to decide
    whether or not it's worthwhile, evaluate the amount time taken to compute the
    dataset from scratch, versus the time taken to read it from disk.
  prefs: []
  type: TYPE_NORMAL
- en: If you decide to persist, be sure to use `ds.write.save` and format as `parquet`
    (default) to avoid a proliferation of bespoke classes and serialization version
    issues. This way you'll preserve the benefits of schema on read.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, as you''re iterating through the analytic development lifecycle,
    writing your own highly-performant functions, it''s a good idea to maintain a
    **regression test pack**. This has a couple of benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: It allows you to ensure that as you introduce new classes of data, you haven't
    broken existing functionality.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It gives you a level of confidence that your code is correct up to the step
    you're working on.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can easily create a regression test pack using unit tests. There are many
    unit testing frameworks out there to aid with this. One popular approach is to
    test each function by comparing the actual results with what you expected. In
    this way, you can build up a pack over time, by specifying tests, along with the
    commensurate data for each of your functions. Let''s explain how to do this with
    a simple example. Suppose we have the following model, taken from the GDELT GKG
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We''d like to test that given a DataFrame of `PersonTone`''s, that the `averageNewsSentiment`
    function correctly computes the average tone for various people taken from all
    articles. For the purposes of writing this unit test, we''re not too interested
    in how the function works, just that it works as *expected*. Therefore, we''ll
    follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required unit test frameworks. In this case, let''s use `ScalaTest`
    and a handy DataFrame-style, parsing framework called `product-collections`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We'll also use a custom extension of the `ScalaTest FunSuite`, called `SparkFunSuite`,
    which we introduced in [Chapter 3](ch03.xhtml "Chapter 3. Input Formats and Schema"),
    *Input Formats and Schema*, which you can find in the code repository.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, mock-up some input data and define the *expected* results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, run the function on the input data using and collect the *actual* result.
    Note: this runs locally and does not require a cluster.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lastly, *verify* that the actual results match the expected results and if they
    don't, fail the test.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The complete unit test looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Tune your analytic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The purpose of analytic tuning is to ensure smooth running and maximum efficiency
    of your analytic within the practical limitations of your cluster. Most of the
    time, this means trying to confirm that memory is being used effectively on all
    machines, that your cluster is fully-utilized, and by ensuring that your analytic
    is not unduly IO-bound, CPU-bound, or network-bound. This can be difficult to
    achieve on a cluster due to the distributed nature of the processing and the sheer
    number of machines involved.
  prefs: []
  type: TYPE_NORMAL
- en: Thankfully, the Spark UI is designed to assist you in this task. It centralizes
    and provides a one-stop shop for useful information about the runtime performance
    and state of your analytic. It can help give pointers to resource bottlenecks
    and even tell you where your code is spending most of its time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a closer look:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input Size or Shuffle Read Size/Records**: Used both for narrow and wide
    transformations, in either case this is the total amount of data read by the task,
    regardless of its source (remote or local). If you''re seeing large input sizes
    or numbers of records, consider repartitioning or increasing the number of executors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Tune your analytic](img/image_14_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Duration**: The amount of time the task has been running. Although entirely
    dependent on the type of computational task underway, if you''re seeing small
    input sizes and long durations, you may be CPU-bound, consider using thread-dump
    to determine what the time is being spent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pay particular attention to any variance in the duration. The Spark UI provides
    figures for the min, 25%, median, 75%, and max displayed on the **Stages** page.
    And from this it is possible to determine the profile of your cluster utilization.
    In other words, whether there is an even distribution of data across your tasks,
    meaning a fair distribution of computing responsibility, or whether you have a
    heavily skewed data distribution, meaning distorted processing with a long tail
    of tasks. If the latter is the case, review the section on handling data distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '**Shuffle Write Size/Records**: The amount of data to be transferred as part
    of the shuffle. It may vary between tasks, but generally you''ll want to ensure
    that the total value is as low as possible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Locality Level**: A measure of data locality appears on the **Stages** page.
    Optimally, this should be PROCESS_LOCAL. However, you will see that it changes
    to any after a shuffle or wide transformation. This usually can''t be helped.
    However, if you''re seeing a lot of `NODE_LOCAL` or `RACK_LOCAL` for narrow transformations:
    consider increasing the number of executors, or in extreme cases confirm your
    storage system block size and replication factor or rebalance your data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GC time**: The amount of time each task spends garbage collecting, that is,
    cleaning-up no longer used objects in memory. It should be no more than around
    10% of the overall time (shown by **Duration**). If it''s excessively high, it''s
    probably an indication of an underlying problem. However, it''s worth reviewing
    the other areas of your analytic relating to data distribution (that is, number
    of executors, JVM heap size, number of partitions, parallelism, skew, and so on)
    before attempting to tune the garbage collector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Thread dump (per executor)**: Shown on the **Executors** page, the thread
    dump option allows you to take a peek at the inner workings of any of your executors,
    at any time. This can be invaluable when trying to gain an understanding of your
    analytic''s behavior. Helpfully, the thread dump is sorted and lists most interesting
    threads at the top of the list look for threads labeled **Executor task launch
    worker** as these are the threads that run your code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By repeatedly refreshing this view, and reviewing the stack trace for a single
    thread, it's possible to get a rough idea of where it's spending time and hence
    identify areas of concern.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Alternatively, you can use a flame graph, for details see [https://www.paypal-engineering.com/2016/09/08/spark-in-flames-profiling-spark-applications-using-flame-graphs/](https://www.paypal-engineering.com/2016/09/08/spark-in-flames-profiling-spark-applications-using-flame-graphs/).
  prefs: []
  type: TYPE_NORMAL
- en: '![Tune your analytic](img/image_14_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Skipped Stages**: The stages that were not required to run. Typically, when
    a stage is shown in this section on the Stages page, it means that a complete
    set of data for this section of the RDD lineage was found in the *cache*, which
    the DAG scheduler did not need to re-compute and instead skipped to the next stage.
    Generally, it is the sign of a good caching strategy.![Tune your analytic](img/image_14_006.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Event Timeline**: Again, shown on the **Stages** page the event timeline
    provides a visual representation of your running tasks. It''s useful to see the
    level of parallelism, and how many tasks are executing on each executor at any
    given time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If after initial investigations, should you need more in-depth information
    than the Spark UI provides, you can use any of the monitoring tools provided by
    your operating system in order to investigate the underlying conditions of your
    infrastructure. The following is a table of a selection of common Linux tools
    for this purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Area Under Consideration** | **Tool** | **Description** | **Example Usage**
    |'
  prefs: []
  type: TYPE_TB
- en: '| General / CPU | htop | Process activity monitor that refreshes to show near
    real-time CPU, memory and swap (among other things) utilization per process |
    htop -p <pid> |'
  prefs: []
  type: TYPE_TB
- en: '|  | dstat | Highly-configurable reporting on system resource utilization |
    dstat -t -l -c -y -i -p -m -g -d -r -n 3 |'
  prefs: []
  type: TYPE_TB
- en: '|  | ganglia | Aggregating system resource monitor designed for use on distributed
    systems | Web-based |'
  prefs: []
  type: TYPE_TB
- en: '| Java Virtual Machine | jvmtop | Statistics about a JVM including resource
    utilization and a real-time view of its threads | jvmtop <pid> |'
  prefs: []
  type: TYPE_TB
- en: '|  | jps | Lists all JVM processes | jps -l |'
  prefs: []
  type: TYPE_TB
- en: '|  | jmap | JVM internal memory map including breakdown of all objects allocated
    on the heap | jmap -histo <pid> &#124; head -20 |'
  prefs: []
  type: TYPE_TB
- en: '|  | jstack | JVM Snapshot including full thread dump | jstack <pid> |'
  prefs: []
  type: TYPE_TB
- en: '| Memory | free | Essential guide to memory utilization | free -m |'
  prefs: []
  type: TYPE_TB
- en: '|  | vmstat | Detailed system resource statistics based on sampling including
    breakdown of memory allocation | vmstat -s |'
  prefs: []
  type: TYPE_TB
- en: '| Disk I/O | iostat | Provides disk I/O statistics, including I/O wait | iostat
    -x 2 5 |'
  prefs: []
  type: TYPE_TB
- en: '|  | iotop | Disk I/O monitor, in a similar style to top. Show''s I/O at a
    process level | iotop |'
  prefs: []
  type: TYPE_TB
- en: '| Network | nettop | Network connection activity monitor including real-time
    I/O | nettop -Pd |'
  prefs: []
  type: TYPE_TB
- en: '|  | wireshark | Interactive network traffic analyzer | wireshark -i <iface>
    -ktshark -i <iface> |'
  prefs: []
  type: TYPE_TB
- en: Design patterns and techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll outline some design patterns and general techniques for
    use when writing your own analytics. These are a collection of hints and tips
    that represent the accumulation of experiences working with Spark. They are offered
    up as guidelines for effective Spark analytic authoring. They also serve as a
    reference for when you encounter the inevitable scalability problems and don't
    know what to do.
  prefs: []
  type: TYPE_NORMAL
- en: Spark APIs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With so many different sets of API's and functions to choose from, it's difficult
    to know which ones are the most performant.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Apache Spark currently has over one thousand contributors, many of whom are
    highly experienced world-class software professionals. It is a mature framework
    having been developed for over six years. Over that time, they have focused on
    refining and optimizing just about every part of the framework from the DataFrame-friendly
    APIs, through the Netty-based shuffle machinery, to the catalyst query plan optimizer.
    The great news is that it all comes for "free" - providing you use the newest
    APIs available in Spark 2.0.
  prefs: []
  type: TYPE_NORMAL
- en: Recent optimizations (introduced by *project tungsten*), such as off-heap explicit
    memory management, cache-miss improvements, and dynamic stage generation, are
    only available with the newer `DataFrame` and `Dataset` APIs, and are not currently
    supported by the RDD API. In addition, the newly-introduced Encoders are significantly
    faster and more space-efficient than Kryo serialization or Java serialization.
  prefs: []
  type: TYPE_NORMAL
- en: For the most part, this means that Datasets usually outperform RDDs.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let''s illustrate using an informal example of a basic count of people mentioned
    in articles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Summary pattern
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: My timeseries analytic must run operationally within strict **service level
    agreements** (**SLAs**) and there is not enough time to compute the required result
    over the entire dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For real-time analytics, or ones with strict SLAs, running lengthy computations
    over large datasets can be impractical. Sometimes it's necessary to design analytics
    using a two-pass algorithm in order to compute results in a timely fashion. To
    do this, we'll need to introduce the concept of the *Summary* pattern.
  prefs: []
  type: TYPE_NORMAL
- en: The Summary pattern is a two-pass algorithm where the end result is reconstructed
    from the aggregation of summaries only. Although only using summaries, and having
    never processed the entire dataset directly, the result of the aggregation is
    the same as if it were run over the entire raw dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic steps are:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculate a summary over the appropriate interval (per minute, per day, per
    week, and so on).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Persist the summary data for later use.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate an aggregate over the larger interval (per month, per year, and so
    on).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is a particularly useful approach when designing incremental or online
    algorithms for streaming analytics.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The GDELT GKG dataset is a great example of a summary dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Certainly, it would be impractical to perform sentiment analysis or named entity
    recognition over say, a month's worth of global media news articles every 15 minutes.
    Fortunately, GDELT produces those 15 minute summaries that we are able to aggregate
    making this entirely possible.
  prefs: []
  type: TYPE_NORMAL
- en: Expand and Conquer Pattern
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: My analytic has a relatively small number of tasks, each with high *Input/Shuffle
    Size (Bytes)*. These tasks take a long time to complete, while sometimes there
    are idle executors.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The *Expand and Conquer* pattern tokenizes records for more efficient parallel
    execution by allowing you to increase parallelism. By decomposing or unpacking
    each record you enable them to be composed in different ways, spread over the
    cluster, and processed by different executors.
  prefs: []
  type: TYPE_NORMAL
- en: In this pattern, `flatMap` is used, usually in conjunction with shuffle or `repartition`,
    to *increase* the number of tasks and decrease the amount of data being processed
    by each task. This gives rise to an optimal situation whereby enough tasks are
    queued so that no executors are ever left idle. It can also help in the scenario
    where you're struggling to process large amounts of data in the memory of one
    machine, and hence receiving *out of memory errors*.
  prefs: []
  type: TYPE_NORMAL
- en: This useful and versatile technique comes in handy in almost every situation
    where you have large datasets. It promotes the use of simple data structures and
    allows you to take full advantage of the distributed nature of Spark.
  prefs: []
  type: TYPE_NORMAL
- en: A word of caution, however, as `flatMap` can also cause performance problems
    because it has the potential to *increase* the time complexity of your analytic.
    By using `flatMap`, you are generating many records for each and every row, hence
    potentially adding another dimension of data that requires processing. Therefore,
    you should always consider the impact of this pattern on algorithmic complexity,
    using the Big O Notation.
  prefs: []
  type: TYPE_NORMAL
- en: Lightweight Shuffle
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The *Shuffle Read Blocked Time* of my analytic is a significant proportion of
    the overall processing time (>5%). What can I do to avoid having to wait for the
    shuffle to finish?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Although Spark''s shuffle is carefully engineered to minimize both network
    and disk I/O by using techniques such as data compression and merge file consolidation,
    it has the following two fundamental problems that mean it will often become a
    performance bottleneck:'
  prefs: []
  type: TYPE_NORMAL
- en: '**It''s I/O Intensive**: The shuffle relies on (i) moving data over a network
    and (ii) writing that data to disk on the target machine. Therefore, it''s much
    slower than local transformations. To illustrate how much slower, here are the
    relative timings for reading 1 MB sequentially from various devices: It''s I/O
    intensive: The shuffle relies on (i) moving data over a network and (ii) writing
    that data to disk on the target machine. Therefore, it''s much slower than local
    transformations. To illustrate how much slower, here are the relative timings
    for reading 1 MB sequentially from various devices:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Memory                0.25ms*'
  prefs: []
  type: TYPE_NORMAL
- en: '*10 GbE              10ms*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Disk                     20ms        *'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, as a shuffle operation uses both network and disk, it would
    be around 120 times slower than one performed on a cached, local partition. Obviously,
    timings will vary depending on physical types and speeds of the devices used,
    figures here are provided as relative guidelines.
  prefs: []
  type: TYPE_NORMAL
- en: '**It''s a synchronization point for concurrency**: Every task in a stage must
    complete before the next stage can begin. Given that stage boundaries involve
    a shuffle (see `ShuffleMapStage`), it marks a point in the execution where tasks
    that otherwise would be ready to start, must wait until all tasks in that stage
    have finished. This gives rise to a synchronization barrier that can have a significant
    impact on performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For these reasons, try to avoid the shuffle where possible, or at least minimize
    its impact.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes it's possible to avoid a shuffle altogether, in fact there are patterns,
    such as *Broadcast Variables* or *Wide Table patterns*, that offer suggestions
    on how to do this, but often it's inevitable and all that can be done is to lessen
    the amount of data that is transferred, and hence the impact of the shuffle.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, try to construct a *Lightweight Shuffle* specifically minimizing
    data transfer - only necessary bytes should be transferred.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once again, if you use the `Dataset` and `DataFrame` API, when the catalyst
    generates a logical query plan it will perform over 50 optimizations, including
    *pruning* any unused columns or partitions automatically (see [https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala](https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala)).
    But if you''re using RDDs, you''ll have to do this yourself. There''s following few
    techniques that you can try:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Use map to reduce data**: Call `map` on your data immediately prior to a
    shuffle in order to get rid of any data that is not used in follow on processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use keys only**: When you have key-value pairs, consider using `rdd.keys`
    instead of `rdd`. For operations such as counts or membership tests, this should
    be sufficient. Similarly, consider using `values` whenever appropriate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adjust order of stages**: Should you join and then `groupBy` or `groupBy`
    and then join? In Spark, this is mainly about the size of the datasets. It should
    be fairly trivial to do cost-based assessments using the number of records before
    and after each transformation. Experiment to find which one is more efficient
    for your datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Filter first**: Generally speaking, filtering rows prior to a shuffle is
    an advantage as it reduces the number of rows transferred. Consider filtering
    as early as possible, provided your revised analytic is functionally equivalent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In some situations, you can also filter out entire partitions, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '**Use CoGroup**: If you have two or more RDDs all grouped by the same key,
    then `CoGroup` might be able to join them without instigating a shuffle. This
    ingenious little trick works because any `RDD[(K,V)]` using the same type `K`
    as a key, and grouped using a `HashPartitioner`, will always settle on the same
    node. Therefore, when joining by key `K`, no data needs to be moved.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Try a different codec**: Another tip for decreasing the amount of bytes transferred
    is to change the compression algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Spark provides three options: `lz4`, `lzf`, and `snappy`. Consider reviewing
    each one to determine which works best for your particular type of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Wide Table pattern
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The one-to-many or many-to-many relationships in my datasets are producing many
    shuffles that ruin all my analytics' performance.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to optimize your data structures, we advocate denormalizing your data
    into a form that's useful for your particular type of processing. The approach,
    described here as the *Wide Table pattern*, involves combining data structures
    that are frequently used together, so that they are composed into a single record.
    This preserves data locality and removes the need to perform expensive joins.
    The more often a relationship is used, the more you benefit from this data locality.
  prefs: []
  type: TYPE_NORMAL
- en: The process involves constructing a data representation, view, or table that
    contains everything you need to do for follow-on processing. You may construct
    this programmatically, or by standard *joins* SparkSQL statements. It is then
    materialized ahead of time and used directly inside your analytics whenever required.
  prefs: []
  type: TYPE_NORMAL
- en: Where necessary, data is duplicated across each row to ensure self-sufficiency.
    You should resist the urge to factor out additional tables, like those found in
    third-normal form or in snowflake designs, and instead rely on columnar data formats,
    such as Parquet and ORC, to provide efficient storage mechanisms without sacrificing
    fast sequential access. They can do this by arranging data by column and compressing
    data within each column, which helps alleviate concerns when duplicating data.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, nested types, classes, or arrays can often be used to good effect
    inside a record to represent children or composite data classes. Again, avoid
    necessary dynamic joins at analytic runtime.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For an example of how to use denormalized data structures, including nested
    types, see *[Chapter 3](ch03.xhtml "Chapter 3. Input Formats and Schema"), Input
    Formats and Schema*.
  prefs: []
  type: TYPE_NORMAL
- en: Broadcast variables pattern
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: My analytic requires many compact reference datasets and dimension tables that,
    despite their smaller size, cause costly shuffles of all data.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While some datasets - such as transaction logs or tweets - are theoretically
    infinitely large, others have natural limits and will never grow beyond a certain
    size. These are known as *bounded datasets*. Although they may change occasionally
    over time, they are reasonably stable and can be said to be held within a finite
    space. For example, the list of all the postcodes in the UK could be considered
    a bounded dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'When joining to a bounded dataset or any small collection, there is an opportunity
    to take advantage of an efficiency pattern that Spark provides. Rather than using
    join as you would normally, which would instigate a shuffle that could potentially
    transfer all data, consider using a broadcast variable instead. Once assigned,
    the broadcast variable will be distributed and made available locally to all the
    executors in your cluster. You can use a broadcast variable like so:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a broadcast variable
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Make sure you collect any data to be broadcast.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing a broadcast variable
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Removing a broadcast variable
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Broadcast variables can be used by the `RDD` API or the `Dataset` API. Also,
    you can still exploit broadcast variables in SparkSQL - it will handle it automatically.
    Just ensure that the threshold is set above the size of the table to join, like
    so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For examples of how to use broadcast variables to implement efficient joins
    and filters, see [Chapter 9](ch09.xhtml "Chapter 9.  News Dictionary and Real-Time
    Tagging System") *, News dictionary and Real-time Tagging System*.
  prefs: []
  type: TYPE_NORMAL
- en: Combiner pattern
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: My analytic is performing an aggregation based on a set of keys, and hence,
    is having to shuffle *all data for all keys*. Consequently, it's very slow.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'At the core of Apache Spark''s shuffling abilities is a powerful and flexible
    pattern, referred to here as the *Combiner* pattern, which offers a mechanism
    for greatly reducing the amount of data in the shuffle. The Combiner pattern is
    so important that examples of it can be found in multiple locations in the Spark
    code - to see it in action here are some of those examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ExternalAppendOnlyMap`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CoGroupedRDD`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DeclarativeAggregate`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ReduceAggregator`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In fact, all high-level API's that use the shuffle operation, such as `groupBy`,
    `reduceByKey`, `combineByKey`, and so on, use this pattern as the core of their
    processing. However, there's some variation in the implementations mentioned previously,
    although the fundamental concept is the same. Let's take a closer look.
  prefs: []
  type: TYPE_NORMAL
- en: The Combiner pattern provides an efficient approach to compute a function across
    sets of records in parallel and then combines their output in order to achieve
    an overall result.
  prefs: []
  type: TYPE_NORMAL
- en: '![Solution](img/B05261_14_04-1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Generally, it consists of three functions that must be provided by the caller:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Initialize ***(e) -> C[0]: *Creates the initial *container*, otherwise known
    as `createCombiner`, `type` constructor, or `zero`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this function, you should create and initialize an instance that will serve
    as the container for all other combined values. Sometimes the first value from
    each key is also provided to pre-populate the container that will eventually hold
    all the combined values for that key. In this case, the function is known as *unit*.
  prefs: []
  type: TYPE_NORMAL
- en: It's worth noting that this function is executed exactly once per key on every
    partition in your dataset. Therefore, it is potentially called multiple times
    for each key and consequently must not introduce any side-effects that would produce
    inconsistent results were the dataset to be distributed differently.
  prefs: []
  type: TYPE_NORMAL
- en: '**Update***(C[0], e) -> C[i]: *Adds an element to the container. Otherwise
    known as `mergeValue`, `bind` *function*, or `reduce`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this function, you should add a record from the originating RDD into the
    container. This usually involves transforming or aggregating the value in some
    way and only the output of this calculation is taken forwards inside the container.
  prefs: []
  type: TYPE_NORMAL
- en: As updates are executed in parallel and in any order, this function must be
    commutative and associative.
  prefs: []
  type: TYPE_NORMAL
- en: '**Merge***(C[i], C[j]) -> C[k]: *Combines together two containers. Otherwise
    known as `mergeCombiners` or `merge`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this function, you should combine the values represented by each container
    to form a new value, which is then taken forwards.
  prefs: []
  type: TYPE_NORMAL
- en: Again, because there are no guarantees on the order of merges, this function
    should be commutative and associative.
  prefs: []
  type: TYPE_NORMAL
- en: You may have noticed a similarity between this pattern and the concept of *monads*.
    If you haven't encountered monads yet, they represent an abstract mathematical
    concept, used in functional programming as a way of expressing functions so that
    they are composable in a general way. They support many features, such as composition,
    side-effect free execution, repeatability, consistency, lazy evaluation, immutability,
    and provide many other benefits. We will not give a full explanation of monads
    here, there are plenty of great introductions already out there - for example
    [http://www.simononsoftware.com/a-short-introduction-to-monads/](http://www.simononsoftware.com/a-short-introduction-to-monads/),
    which takes a practical rather than a theoretical viewpoint. Instead, we will
    explain where the Combiner pattern is different and how it helps to understand
    Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Spark executes the `update` function on every record in your dataset. Due to
    its distributed nature, this can happen in parallel. It also runs the *merge*
    function to combine results from the output of each partition. Again, because
    this function is applied in parallel and therefore could be combined in any order,
    Spark requires these functions to be *commutative*, meaning that the sequence
    in which they are applied should have no impact on the overall answer. It's this
    commutative, merge step that really provides the basis of the definition.
  prefs: []
  type: TYPE_NORMAL
- en: An understanding of this pattern is useful for reasoning about the behavior
    of any distributed aggregations. If you're interested in understanding this pattern
    further, a nice implementation can be found in [https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/expressions/Aggregator.scala](https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/expressions/Aggregator.scala).
  prefs: []
  type: TYPE_NORMAL
- en: In addition to this, it's useful when trying to determine which high-level API
    to use. With so many available, it's sometimes difficult to know which one to
    choose. By applying an understanding of *types* to the preceding descriptions,
    we can decide on the most fitting and performant API. For example, where the types
    of *e* and *C[n]* are the same, you should consider using `reduceByKey`. However,
    where the type of *e* is different to *C[n]*, then an operation such as `combineByKey`
    should be considered.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate, let's consider some different approaches using four of the most
    common operations available on the `RDD` API.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To provide some context, let''s say we have an RDD of key-value pairs representing
    people mentioned in news articles, where the key is the name of the person referred
    to in the article, and the value is a pre-filtered, tokenized, bag-of-words, textual-version
    of the article:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now suppose we want to find some statistics about articles in which a person
    is mentioned, for example, min and max length, most frequently used words (excluding
    stop-words), and so on. In this case, our result would be of the form, `(person:String,stats:ArticleStats)`,
    where `ArticleStats` is a case class designed to hold the required statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s start with the definition of the three combiner functions, as described
    previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'As you might notice, these functions are really just the syntactic sugar of
    our pattern; the real logic is hidden away in the companion class and the semigroup:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: For our purposes, we won't cover these in detail, let's just assume that any
    computation necessary to calculate the statistics are carried out by the supporting
    code - including the logic for finding the extremities of two previously calculated
    metrics - and instead focus on the explanation of our different approaches.
  prefs: []
  type: TYPE_NORMAL
- en: '**GroupByKey approach**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our first approach is by far and away the slowest option because `groupByKey`
    doesn''t use the `update` function. Despite this obvious disadvantage, we can
    still achieve our result - by sandwiching the `groupByKey` between maps where
    the first map is used to convert into the desired type and the last to perform
    the reduce-side aggregation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: However, you will notice that it does not perform any map-side combining for
    efficiency, instead preferring to combine all values on the reduce-side, meaning
    that all values are copied across the network as part of the shuffle.
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, you should always consider the following alternatives before
    resorting to this approach.
  prefs: []
  type: TYPE_NORMAL
- en: '**ReduceByKey approach**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To improve on this, we can use `reduceByKey`. Unlike `groupByKey`, `reduceByKey`
    provides map-side combining for efficiency by making use of the `update` function.
    In terms of performance, it offers an optimum approach. However, it still requires
    each value to be manually converted to the correct type prior to invocation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The result is achieved in two steps by mapping records from the originating
    `RDD` into the desired type.
  prefs: []
  type: TYPE_NORMAL
- en: '**AggregateByKey approach**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, `aggregateByKey` provides the same performance characteristics as `reduceByKey`
    - by implementing map-side combine - but this time as one operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '**CombineByKey Approach**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally, `combineByKey` is thought of as the most flexible key-based operation,
    giving you complete control over all three functions in the Combiner pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: While providing `init` as a *function* rather than just a single value might
    give you more flexibility in select scenarios, in practice for most problems the
    relationship between `init`, `update`, and `merge` is such that you don't really
    gain anything in terms of functionality or performance between either approach.
    And regardless, all three are backed by `combineByKeyWithClassTag`, so in this
    instance feel free to choose whichever one that is a better syntactic fit for
    your problem, or just pick the one you prefer.
  prefs: []
  type: TYPE_NORMAL
- en: Optimized cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: I want to know how to configure my Spark job's executors in order to make full
    use of the resources of my cluster, but with so many options I'm confused.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As Spark is designed to scale horizontally, generally speaking, you should prefer
    having *more* executors over *larger* executors. But with each executor comes
    the overhead of a JVM, so it's advisable to make full use of them by running *multiple*
    tasks inside each executor. As this seems like a bit of a contradiction, let's
    look at how to configure Spark to achieve this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark provides the following options (specified on the command line or in configuration):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Number of executors can be estimated using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '*number of executors = (total cores - cluster overhead) / cores per executor*'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, when using a YARN-based cluster accessing HDFS and running in
    YARN-client mode, the equation would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*((T - (2*N + 6)) / 5)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: '`T`: Total number of cores in the cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: '`N`: Total of nodes in the cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: '`2`: Removes the *per node* overhead of HDFS and YARN.'
  prefs: []
  type: TYPE_NORMAL
- en: Assumes two HDFS processes on each node - `DataNode` and `NodeManager`.
  prefs: []
  type: TYPE_NORMAL
- en: '`6`: Removes the *master process* overhead of HDFS and YARN.'
  prefs: []
  type: TYPE_NORMAL
- en: Assumes the average of six processes - `NameNode`, `ResourceManager`, `SecondaryNameNode`,
    `ProxyServer`, `HistoryServer`, and so on. Obviously, this is an example and in
    reality it depends on what other services are running the cluster, along with
    other factors such as Zookeeper quorum size, HA strategy, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '`5`: Anecdotally, the optimum number of cores for each executor to ensure optimal
    task concurrency without prohibitive disk I/O contention.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Memory allocation can be estimated using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '*mem per executor = (mem per node / number of executors per node) * safety
    fraction*'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, when using a YARN-based cluster running in YARN-client mode with
    64 GB per node, the equation would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*(64 / E)* 0.9 => 57.6 / E*'
  prefs: []
  type: TYPE_NORMAL
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: '`E`:Number of executors per node (as calculated in the previous example).'
  prefs: []
  type: TYPE_NORMAL
- en: '`0.9`: Fraction of actual memory allocated to the heap after subtracting off-heap.'
  prefs: []
  type: TYPE_NORMAL
- en: overhead (`spark.yarn.executor.memoryOverhead`, default 10%).
  prefs: []
  type: TYPE_NORMAL
- en: It's worth noting that while it is generally beneficial to allocate more memory
    to an executor (allowing more space for sorting, caching, and so on) increasing
    the memory also increases *garbage collection pressure*. The GC must sweep the
    entire heap for unreachable object references, therefore the larger the memory
    region it has to analyze, the more resources it must consume and at some point
    this leads to diminishing returns. Whilst there's no absolute figure as to at
    what point this happens, as a general rule of thumb, keep the memory per executor
    under 64 GB to avoid problems.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding equations should provide a good starting-point estimation for
    sizing your cluster. For further tuning, you may wish to experiment by tweaking
    these settings and measuring the effect on performance using the Spark UI.
  prefs: []
  type: TYPE_NORMAL
- en: Redistribution pattern
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: My analytic always runs on the same few executors. How do I increase the level
    of parallelism?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When `Datasets` and `RDDs` are relatively small to begin with, even if you then
    expand them using `flatMap`, any child in the lineage will take the parents number
    of partitions.
  prefs: []
  type: TYPE_NORMAL
- en: So, if some of your executors are idle, calling the `repartition` function could
    improve your level of parallelism. You will incur the immediate cost of moving
    data around, but this could pay-off overall.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following command to determine the number of partitions for your data
    and hence the parallelism:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: If the number of partitions is less than the maximum number of tasks allowable
    on your cluster, then you're not making full use of your executors.
  prefs: []
  type: TYPE_NORMAL
- en: Conversely, if you have a large number of tasks (10,000+) and they aren't running
    for very long then you should probably call `coalesce` to make better use of your
    resources - starting and stopping tasks is relatively expensive!
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here, we increase the parallelism of a `Dataset` to `400`. The physical plan
    will show this as `RoundRobinPartitioning(400)`, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'And here''s the equivalent re-partitioning for an `RDD` performed by simply
    specifying the number of partitions to use in the `reduceByKey` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Salting key pattern
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most of my tasks finish in a reasonable time, but there's always one or two
    that take much longer (>10x) and repartitioning does not seem to have any beneficial
    effect.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you're experiencing having to wait for a handful of slow tasks, then you
    could be suffering from a skew in your data distribution. Symptoms of this are
    that you're seeing some tasks taking far longer than others or that some tasks
    have far more input or output.
  prefs: []
  type: TYPE_NORMAL
- en: If this is the case, the first thing to do is check that the number of keys
    is greater than the number of executors, as coarse-grained grouping can limit
    parallelism. A quick way to find the number of keys in your `RDD` is to use `rdd.keys.count`.
    If this value is lower than the number of executors, then reconsider your key
    strategy. Patterns such as *Expand and Conquer* may be able to help out.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the preceding things are in order, the next thing to review is key distribution.
    Where you find a small number of keys with large numbers of associated values,
    consider the *Salting Key* pattern. In this pattern, popular keys are subdivided
    by appending a random element. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: This results in a more balanced key distribution because during the shuffle,
    the `HashPartitioner` sends the new keys to different executors. You can choose
    the value of n to suit the parallelism you need - greater skew in the data necessitates
    a greater range of salts.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, all this salting does mean that you'll need to re-aggregate back
    onto the old keys to ensure that you ultimately calculate the correct answer.
    But, depending on the amount of skew in your data, a two-phase aggregation may
    still be faster.
  prefs: []
  type: TYPE_NORMAL
- en: You can either apply this salting to all keys, or filter out as in the preceding
    example. The threshold at which you filter, decided by `isPopular` in the example,
    is also entirely your choice.
  prefs: []
  type: TYPE_NORMAL
- en: Secondary sort pattern
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When grouping by keys, my analytic has to explicitly sort the values *after*
    they are grouped. This sorting takes place in memory, therefore large value-sets
    take a long time, and they may involve spilling to disk and sometimes give an
    `OutOfMemoryError`. Here is an example of the problematic approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Instead, when grouping by key, values should be pre-ordered within each key
    for immediate and efficient follow-on processing.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Use the *Secondary Sort* pattern to order the list of items in a group efficiently
    by using the shuffle machinery. This approach will scale when handling even the
    largest of datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to sort efficiently, this pattern utilizes three concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Composite key**: Contains both the elements you want to group by *and* the
    elements you want to sort by.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Grouping partitioner**: Understands which parts of the composite key are
    related to *grouping*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Composite key ordering**: Understands which parts of the composite key are
    related to *ordering*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each of these is injected into Spark so that the final dataset is presented
    as grouped and ordered.
  prefs: []
  type: TYPE_NORMAL
- en: Please note, in order to perform a secondary sort you need to use `RDDs`, as
    the new `Dataset` API is not currently supported. Track the progress on the following
    JIRA [https://issues.apache.org/jira/browse/SPARK-3655](https://issues.apache.org/jira/browse/SPARK-3655).
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Consider the following model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Here we have an entity representing the occasions where people are mentioned
    in news articles containing the person's name, the article they were mentioned
    in, and its publication date.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we want to group together all the mentions of people with the same
    name, and order them by time. Let''s look at the three mechanisms we need:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Composite key**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '*Contains both name and published date.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Grouping partitioner**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '*It''s only grouping by name.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Composite key ordering**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '*It''s only sorting by published date.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have defined these, we can use them in the API, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Here the `SortKey` is used to pair the data, the `GroupingPartitioner` is used
    when partitioning the data, and the `Ordering` is used during the merge and, of
    course, it's found via Scala's `implicit` mechanism, which matches based on type.
  prefs: []
  type: TYPE_NORMAL
- en: Filter overkill pattern
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: My analytic uses a *whitelist* in order to filter relevant data for processing.
    The filter happens early on in the pipeline so that my analytic only ever has
    to process the data I'm interested in, for maximum efficiency. However, the whitelist
    frequently changes meaning my analytic must be executed afresh, each time, against
    the new list.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Contrary to some of the other advice you'll read here, in some scenarios calculating
    results across all the data, by removing filters, can actually *increase* the
    overall efficiency of an analytic.
  prefs: []
  type: TYPE_NORMAL
- en: If you are frequently rerunning your analytic over different segments of the
    dataset, then consider using a popular approach, described here as the *Filter
    Overkill pattern*. This involves omitting all filters in Spark and processing
    over the entire corpus. The results of this one-off processing will be much larger
    that the filtered version, but it can be easily indexed in a tabular data store
    and filtered dynamically at query time. This avoids having to apply different
    filters over multiple runs, and having to re-compute historical data when filters
    change.
  prefs: []
  type: TYPE_NORMAL
- en: Probabilistic algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It takes too long to compute statistics over my dataset because it is too large.
    By the time the response is received, it's out of date or no longer relevant.
    Therefore, it's more important to receive a timely response, or at least provide
    a maximum bound to time-complexity, than a complete or correct answer. In fact,
    a well-timed estimate even with *a small probability of error* would be taken
    in preference to a correct answer where the running time is not known.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Probabilistic algorithms use *randomization* to improve the time complexity
    of their algorithms and guarantee worst case performance. If you are time sensitive
    and just about right is good enough, you should consider using a probabilistic
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, the same can be said for the problem of memory usage. There are
    a set of Probabilistic algorithms that provide estimates inside restricted space-complexity.
    Examples include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bloom Filter **is a membership test that is guaranteed to never miss an element
    in a set, but could give you a false positive, that is, determine an element to
    be a member of a set when it is not. It''s useful for quickly reducing the amount
    of data in a problem-space prior to a more accurate calculation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**HyperLogLog **counts the number of distinct values in a column, providing
    a very reasonable estimate using a fixed memory footprint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CountMinSketch** provides a frequency table used for counting occurrences
    of events in a stream of data. Particularly useful in Spark streaming where a
    fixed memory footprint eliminates the potential for memory overflows.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Spark provides implementations of these in `org.apache.spark.sql.DataFrameStatFunctions`
    and they can be used by accessing `df.stat`. Spark also includes some access via
    the `RDD` API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For an example of how to use a **Bloom Filter** see [Chapter 11](ch11.xhtml
    "Chapter 11. Anomaly Detection on Sentiment Analysis"), *Anomaly Detection on
    Sentiment Analysis*.
  prefs: []
  type: TYPE_NORMAL
- en: Selective caching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: My analytic is caching datasets, but if anything, it's running slower than before.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Caching is key to getting the most performance out of Spark; however, when
    used incorrectly, it can have a detrimental effect. Caching is particularly useful
    whenever you intend to use an RDD more than once. This generally happens when
    you are: (i) using the data across stages, (ii) the data appears in the lineage
    of multiple child datasets, or (iii) during iterative processes, such as stochastic
    gradient descent.'
  prefs: []
  type: TYPE_NORMAL
- en: The problem occurs when you cache indiscriminately without considering reuse.
    This is because the cache adds overhead when it's created, updated and flushed,
    and then must be garbage collected when not used. Therefore, improper caching
    can actually *slow down your job*. So, the easiest way to improve caching is to
    stop doing it (selectively of course).
  prefs: []
  type: TYPE_NORMAL
- en: Another consideration is whether there's enough memory allocated and available
    to efficiently cache your RDD. If your dataset won't fit into memory, Spark will
    either throw an `OutOfMemoryError` or swap data to disk (depending on the storage
    levels, this will be talked about shortly). In the latter case, this could have
    a performance impact due to both (i) the time taken to move extra data in and
    out of memory and (ii) having to wait for the availability of the disk (I/O wait).
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to determine whether you have enough memory allocated to your executors,
    first cache the dataset as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Then, look at the *Storage* page in the Spark UI. For each RDD, this provides
    the fraction cached, its size, and the amount spilled to disk.
  prefs: []
  type: TYPE_NORMAL
- en: '![Solution](img/B05261_14_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This should enable you to adjust the memory allocated to each executor in order
    to ensure that your data fits in memory. There are also the following caching
    options available:'
  prefs: []
  type: TYPE_NORMAL
- en: '**NONE**: No caching (default)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MEMORY**: Used when `cache` is called'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DISK**: Spill to disk'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SER**: Same as MEMORY, but objects are stored in a byte array'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2** (**REPLICATED**): Keep a cached copy on two different nodes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The preceding options can be used in any combination, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: If you're experiencing `OutOfMemoryError` errors, try changing to `MEMORY_AND_DISK`
    to allow spilling of the cache to disk
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you're experiencing high garbage collection times, consider trying one of
    the serialized byte buffer forms of cache, such as `MEMORY_AND_SER`, as this will
    circumvent the GC entirely (at the slight cost of increased serialization)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal here is to ensure that the *Fraction Cached* is at 100%, and where
    possible, minimize the *Size on Disk* to establish effective in-memory caching
    of your datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Garbage collection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The *GC time* of my analytic is a significant proportion of the overall processing
    time (>15%).
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Spark's garbage collector works pretty efficiently out of the box, so you should
    only attempt to adjust it if you're sure that it's the cause and not the *symptom*
    of the problem. Before altering the GC settings, you should ensure that you have
    reviewed all other aspects of your analytic. Sometimes you might see high GC times
    in the Spark UI for reasons other than a poor GC configuration. Most of the time,
    it's worth investigating these first.
  prefs: []
  type: TYPE_NORMAL
- en: If you're seeing frequent or lengthy GC times, the first thing to do is confirm
    that your code is behaving sensibly and make sure that it's not at the root of
    excess/irregular memory consumption. For example, review your caching strategy
    (see the preceding section) or use the `unpersist` function to explicitly remove
    RDDs or Datasets that are no longer required.
  prefs: []
  type: TYPE_NORMAL
- en: Another factor for consideration is the number of objects you allocate within
    your job. Try to minimize the amount of objects you instantiate by (i) simplifying
    your domain model, or (ii) by reusing instances, or (iii) by preferring primitives
    where you can.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, if you're still seeing lengthy GC times, try tuning the GC. There's
    some great information provided by Oracle on how to do this ([https://docs.oracle.com/javase/8/docs/technotes/guides/vm/gctuning/g1_gc_tuning.html](https://docs.oracle.com/javase/8/docs/technotes/guides/vm/gctuning/g1_gc_tuning.html)),
    but specifically there is evidence to suggest that Spark can perform well using
    the G1 GC. It's possible to switch to this GC by adding `XX:UseG1GC` to the Spark
    command line.
  prefs: []
  type: TYPE_NORMAL
- en: 'When tuning the G1 GC, the two main options are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**InitiatingHeapOccupancyPercent: **A threshold percent of how full the heap
    should be before the GC triggers a cycle. The lower the percentage, the more frequently
    the GC runs, but the less work it has to do on each run. Therefore, if you set
    it to *less than* 45% (the default value), you might see fewer pauses. It can
    be configured on the command line using `-XX:InitiatingHeapOccupancyPercent`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ConcGCThread**: The number of concurrent GC threads running in the background.
    The more threads, the quicker the garbage collection can complete. But it''s a
    trade-off as more GC threads means more CPU resource allocation. Can be configured
    on the command line using `-XX:ConcGCThread`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, it's a matter of experimenting with these settings and tuning your
    analytic to find the optimum configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Graph traversal
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: My analytic has an iterative step that only completes when a global condition
    is met, such as all keys report no more values to process, and consequently the
    running time can be slow and difficult to predict.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Generally speaking, the efficiency of graph-based algorithms is such that,
    if you can represent your problem as a standard graph traversal problem, you probably
    should. Examples of problems with graph-based solutions include: shortest-path,
    depth-first search, and page-rank.'
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For an example of how to use the *Pregel* algorithm in `GraphX` and how to interpret
    a problem in terms of graph traversal, see [Chapter 7](ch07.xhtml "Chapter 7. Building
    Communities"), *Building Communities*.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have concluded our journey by discussing aspects of distributed
    computing performance, and what to exploit when writing your own scalable analytics.
    Hopefully, you've come away with a sense of some of the challenges involved, and
    have a better understanding of how Spark works under the covers.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark is a constantly evolving framework and new features and improvements
    are being added every day. No doubt it will become increasingly easier to use
    as continuous tweaks and refinements are intelligently applied into the framework,
    automating much of what must be done manually today.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of what's next, who knows what's round the corner? But with Spark beating
    the competition yet again to win the 2016 CloudSort Benchmark ([http://sortbenchmark.org/](http://sortbenchmark.org/))
    and new versions set to be released every four months, one thing is for sure,
    it's going to be fast-paced. And hopefully, with the solid principles and methodical
    guidelines that you've learned in this chapter, you'll be developing scalable,
    performant algorithms for many years to come!
  prefs: []
  type: TYPE_NORMAL
