- en: Logging and Tracing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we first started with containers and Kubernetes, we showed how we could
    get the log output from any of our individual containers using the `kubectl log` command.
    As we scale the number of containers from which we want to get information, the
    ability to easily find the relevant logs becomes increasingly difficult. In the
    previous chapter, we looked at how to aggregate and collect metrics, and in this
    chapter we extend that same concept, looking at how to aggregate logging and getting
    a better understanding of how containers work together with distributed tracing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Topics for this chapter include:'
  prefs: []
  type: TYPE_NORMAL
- en: A Kubernetes concept—DaemonSet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing Elasticsearch, Fluentd, and Kibana
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Viewing logs using Kibana
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed tracing with Jeager
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An example of adding tracing to your application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Kubernetes concept – DaemonSet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Kubernetes resource that we have now used (through Helm) is DaemonSet. This
    resource is a wrapper around pods very similar to ReplicaSet, but with the purpose
    of running a pod on every node in a cluster. When we installed Prometheus using
    Helm, it created a DaemonSet to run node-collector on each node within the Kubernetes
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two common patterns for running software in a support role with your
    application: the first is using the side-car pattern, and the second is using
    a DaemonSet. A side-car is when you include a container within your pod whose
    sole purpose is to run alongside the primary application and provide some supporting,
    but external, role. An example of a useful side-car might be a cache, or a proxy
    service of some form. Running a side-car application obviously increases the resources
    needed for a pod, and if the number of pods is relatively low or they are sparse
    compared to the size of a cluster, this would be the most efficient way to provide
    that supporting software.'
  prefs: []
  type: TYPE_NORMAL
- en: When the support software you are running will be potentially replicated many
    times on a single node, and the service it is providing is fairly generic (such
    as log aggregation or metric collection), then it can be significantly more efficient
    to run a single pod on each node in a cluster. That is exactly where DaemonSet
    comes in.
  prefs: []
  type: TYPE_NORMAL
- en: Our earlier example of using a DaemonSet was running an instance of node-collector
    on every node within the cluster. The purpose of the node-collector DaemonSet
    is to collect statistics and metrics about the operation of every node. Kubernetes
    also uses DaemonSet to run its own services, such as kube-proxy, which operate
    on every node in the cluster. If you are using an overlay network to connect your
    Kubernetes cluster, such as Weave or Flannel, it is also frequently run using
    a DaemonSet. Another common use case is the one we'll explore more in this chapter,
    collecting and forwarding logging.
  prefs: []
  type: TYPE_NORMAL
- en: The required fields for a DaemonSet specification are similar to a deployment
    or job; in addition to the `apiVersion`, `kind`, and `metadata`, the DaemonSet
    also needs a spec, which includes a template that is used to create the pods on
    each node. In addition, the template can have a `nodeSelector` to match a set
    or subset of the nodes available.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the YAML that Helm created when it installed `prometheus`. You
    can get a sense of how the data for the DaemonSet is laid out. The following output
    is from the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The DaemonSet specification that Helm generated is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This DaemonSet runs a single container with a pod on each node, the image `prom/node-exporter:0.15`,
    which collects metrics from the volume mount points (`/proc` and `/sys` are very
    Linux-specific), and exposes them on port `9100` for `prometheus` to scrape with
    an HTTP request.
  prefs: []
  type: TYPE_NORMAL
- en: Installing and using Elasticsearch, Fluentd, and Kibana
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fluentd is software that's frequently used to collect and aggregate logging.
    Hosted at [https://www.fluentd.org](https://www.fluentd.org), like prometheus
    it is open source software that is managed under the umbrella of the **Cloud Native
    Computing Foundation** (**CNCF**). When it comes to talking about aggregating
    logs, the problem has existed long before containers, and ELK was a frequent acronym
    used to represent a solution, the combination of Elasticsearch, Logstash, and
    Kibana. When using containers, the number of log sources expands, making the problem
    of collecting all the logs even larger, and Fluentd evolved to support the same
    space as Logstash, focusing on structured logging with a JSON format, routing
    it, and supporting plugins to process the logs. Fluentd was written in Ruby and
    C, intending to be faster and more efficient than LogStash, and the same pattern
    is continuing with Fluent Bit ([http://fluentbit.io](http://fluentbit.io)), which
    has an even smaller memory footprint. You may even see references to EFK, which
    stands for the combination of Elasticsearch, Fluentd, and Kibana.
  prefs: []
  type: TYPE_NORMAL
- en: Within the Kubernetes community, one of the more common solutions for capturing
    and aggregating logs is Fluentd, and it is even built into recent versions of
    Minikube as one of the add-ons that you can use.
  prefs: []
  type: TYPE_NORMAL
- en: If you are using Minikube, you can experiment with EFK very easily by enabling
    the Minikube add-on. While Fluentd and Kibana are fairly small in terms of resource
    needs, Elasticsearch has higher resource requirements, even for a small demonstration
    instance. The default VM that Minikube uses to create a single-node Kubernetes
    cluster has 2 GB of memory allocated to it, which is insufficient for running
    EFK and any additional workloads, as ElasticSearch by itself wants to utilize
    2 GB of memory while it is initializing and starting up.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, you can ask Minikube to start up and allocate more memory to the
    VM that it creates. To see how Elasticsearch, Kibana, and Fluentd work together,
    you should start Minikube with at least 5 GB of RAM allocated, which you can do
    with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'You can then see what add-ons are enabled and disabled for Minikube with the
    Minikube add-ons command. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'And enabling EFK is simply done using this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '`enabled` does not mean instantly running. FluentD and Kibana will start fairly
    quickly, but ElasticSearch does take significantly longer. Being an add-on implies
    that software within Kubernetes will manage the containers within the kube-system
    namespace, so getting information about the current state of these services won''t
    be as simple as `kubectl get pods`. You will need to either reference `-n kube-system`
    or use the option `--all-namespaces`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see the Minikube add-on manager loads up EFK as three ReplicaSets,
    each running a single pod, and fronted with a service that is exposed from the
    virtual machine as a NodePort. With Minikube, you can also see the list of services
    using this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Log aggregation with EFK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fluentd starts as the source for collecting logs from all the containers. It
    uses the same underlying sources that the command `kubectl logs` uses. Within
    the cluster, every container that is operating is generating logs that are handled
    in some fashion by the container runtime, the most common of which is Docker,
    which maintains log files for every container on each of the hosts.
  prefs: []
  type: TYPE_NORMAL
- en: The Minikube add-on that sets up Fluentd configures it with a `ConfigMap`, which
    references where to load these log files, and includes additional rules to annotate
    the log data with information from Kubernetes. As Fluentd runs, it keeps track
    of these log files, reading in the data as it is updated from each container,
    parsing the log file output into data structures in JSON format, and adding the
    Kubernetes-specific information. The same configuration also details what to do
    with the output, and in the case of the Minikube add-on, it specifies an endpoint,
    which is the `elasticsearch-logging` service, where it sends this structured JSON
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch is a popular open source data and search index, with corporate
    support from [Elastic.co](https://www.elastic.co/). Although it requires quite
    a bit of resources to run, it scales up extremely well and has a very flexible
    structure for adding a variety of data sources and providing a search interface
    for that data. You can get more details about how ElasticSearch works at [https://github.com/elastic/elasticsearch](https://github.com/elastic/elasticsearch),
    the GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: Kibana is the final part of this trio, and provides the web-based user interface
    for searching the content that is stored in Elasticsearch. Also maintained by
    [Elastic.co](https://www.elastic.co/), it provides some dashboard capabilities
    and an interactive query interface for Elasticsearch. You can find more information
    on Kibana at [https://www.elastic.co/products/kibana](https://www.elastic.co/products/kibana).
  prefs: []
  type: TYPE_NORMAL
- en: While using Minikube, everything in your cluster is on a single node, so there
    are limits and differences to using the same kind of framework in a larger cluster.
    If you are using a remote cluster with several nodes, you may want to look at
    something like Helm to install Elasticsearch, Fluentd, and Kibana. Many service
    providers who are supporting Kubernetes also have something set up to aggregate,
    store, and provide a searchable index of your containers logs. Google Stackdriver,
    Datadog, and Azure are examples that provide a similar mechanism and service,
    specific to their hosted solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Viewing logs using Kibana
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this book, we will explore how to use Kibana, taking advantage of it as
    an add-on to Minikube. After you have enabled it, and when the pods are fully
    available and reporting as Ready, you can access Kibana with this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This will bring up a web page that is backed by the `kibana-logging` service.
    When it is first accessed, the web page will ask you to specify a default index,
    which is used by Elasticsearch to build its search indices:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/055a41ee-3d69-408e-8cca-7be39e12a8e6.png)'
  prefs: []
  type: TYPE_IMG
- en: Click on Create, taking the defaults that are provided. The default index pattern
    of `logstash-*` doesn't mean it has to come from `logstash` as a source, and the
    data that has already been sent to ElasticSearch from Fluentd will all be directly
    accessible.
  prefs: []
  type: TYPE_NORMAL
- en: 'One you have defined a default index, the next page that is displayed will
    show you all the fields that have been added into Elasticsearch as Fluentd has
    taken the data from the container logs and Kubernetes metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/de9d0ee0-d8ed-4ce1-b285-572b2f6ee971.png)'
  prefs: []
  type: TYPE_IMG
- en: You can browse through this list to see what is being captured by field name,
    which will give you a little sense of what is available to browse and search.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see the logs that are flowing from the system, the Discover button on the
    top left of the web page will take you to a view that is built from these indices
    we just created, and by default, will reflect all of the logs that are happening
    that Fluentd is collecting:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/91c5976e-53e7-4849-9fea-fc73fd985733.png)'
  prefs: []
  type: TYPE_IMG
- en: The logging that you are seeing is primarily coming from the Kubernetes infrastructure
    itself. To get a better picture of how to use the logging, spin up the earlier
    examples we created, and we will scale those up to multiple instances to see the
    output.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will grab the two-tier example app of Flask and Redis from [https://github.com/kubernetes-for-developers/kfd-flask](https://github.com/kubernetes-for-developers/kfd-flask):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This will deploy our earlier Python and Redis examples, with a single instance
    of each. Once these pods are active, go back and refresh the browser with Kibana
    active in it, and it should update to show you the latest logging. You can set
    the time period that Kibana is summarizing at the top of the window, and set it
    to auto-refresh on a regular basis, if you desire.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s scale up our Flask deployment to have multiple instances, which
    will make learning how to use Kibana a bit easier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Filtering by app
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The key to using Kibana effectively is to filter to just the data you are interested
    in seeing. The default discovery view is set to give you a sense of how large
    the logs are from specific sources, and we can use filtering to narrow down to
    what we want to see.
  prefs: []
  type: TYPE_NORMAL
- en: As you view the data, scroll down on the left side through the list of fields,
    and each of these can be used as a filter. If you tap on one, Kubernetes.labels.app
    for example, Kibana will give you a summary of what different values this field
    has collected for the timespan that you are viewing.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/cc31781e-ae3c-406b-8345-61bce34cad64.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding example, you can see that the two `app` labels that are within
    the timespan are `flask` and `kubernetes-dashboard`. We can limit it to the Flask
    application by clicking the magnifying glass icon with a plus in it. The result
    constrains to only log items that include those values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/f3188d31-ab4c-43f0-b66b-5ff37bb2e83e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The icon with a magnifying glass with a minus symbol is used to set an exclusion
    filter. Since we used the `kubectl scale` command earlier to create multiple instances,
    you can scroll down to `kubernetes.pod_name` in the list of fields and see the
    pods that are listed and reporting as matching the first filter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/127983ae-ea49-434c-a0bd-adf3a0d47235.png)'
  prefs: []
  type: TYPE_IMG
- en: You can now refine the filter to only include one, or exclude one of those pods,
    to see all the remaining logs. As you add filters, they will appear at the top
    of the screen, and by clicking on that reference you can remove, pin, or temporarily
    disable the filter.
  prefs: []
  type: TYPE_NORMAL
- en: Lucene query language
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can also use the Lucene query language, which is what ElasticSearch uses
    by default, to refine your searches to data within the fields, make more complex
    filters, or otherwise track down the data with more precision. The Lucene query
    language goes beyond what we will cover in this book, but you can get an excellent
    overview in the Kibana documentation at [https://www.elastic.co/guide/en/kibana/current/lucene-query.html.](https://www.elastic.co/guide/en/kibana/current/lucene-query.html)
  prefs: []
  type: TYPE_NORMAL
- en: Lucene's search language is oriented around searching unstructured text data,
    so basic searching for a word is as simple as entering a word. Multiple words
    are treated as individual searches, so if you are searching for a specific phrase,
    put the phrase in quotes. The search parser will also understand explicit OR and
    AND for simple Boolean searches.
  prefs: []
  type: TYPE_NORMAL
- en: 'The default on the query syntax is to search all fields, and you may specify
    a field to search. To do so, name the field, followed by a colon, and then the
    search terms. For example, to search for `error` in the field `log`, you can use
    this search query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This search query also supports wildcard searches, using the character `?`
    to represent any single unknown character, and `*` to represent zero or more characters.
    You can also use regular expressions in your query, by wrapping your query with
    `/` characters, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This will search for `error` or `errors` in the log field.
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE: Because Lucene breaks down the fields, regular expressions are applied
    to each word in a string, and not the string as a whole. Because of this, regular
    expressions are best used when you want to hunt for a composed word, and not a
    phrase or string that includes whitespaces.'
  prefs: []
  type: TYPE_NORMAL
- en: The Lucene query language also includes some advanced search options that can
    accommodate misspellings and slight variations, which can be immensely useful.
    The syntax includes support for fuzzy searches using the `~` character as a wildcard,
    which allows for slight variations in spelling, transposition, and so on. Phrases
    also support using ~ as a variation indicator, and it is used for making proximity
    searches, a maximum distance between two words in a phrase. To get more information
    on how these specific techniques work and how to use them, dig into the [ElasticSearch
    Query DSL documentation](https://www.elastic.co/guide/en/elasticsearch/reference/6.2/query-dsl-query-string-query.html#_fuzziness).
  prefs: []
  type: TYPE_NORMAL
- en: Running Kibana in production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kibana has a variety of other features, including setting up dashboards, making
    data visualizations, and even using simple machine learning to hunt for anomalies
    in the log data. These features are beyond the scope of this book. You can learn
    more about them in the Kibana user's guide at [https://www.elastic.co/guide/en/kibana/current/](https://www.elastic.co/guide/en/kibana/current/).
  prefs: []
  type: TYPE_NORMAL
- en: Running more complex developer support tools, such as Elasticsearch, Fluentd,
    and Kibana, is a more complex task than we can cover in this book. There is some
    documentation around using Fluentd and Elasticsearch as an add-on, as you have
    seen previously with the Minikube example. EFK is its own complex application
    to be managed. There are several Helm charts that might suit your needs, or you
    may want to consider leveraging a cloud provider's solution, rather than take
    on the administration of these components yourself.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed tracing with Jaeger
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you decompose your services into multiple containers, one of the hardest
    things to understand is the flow and path of requests, and how containers are
    interacting. As you expand and use more containers to support components within
    your system, knowing which containers are which and how they're contributing to
    the performance of a request becomes a significant challenge. For simple systems,
    you can often add logging and get a view through the log files. As you move into
    dozens, or even hundreds, of different containers making up a service, that process
    becomes far less tenable.
  prefs: []
  type: TYPE_NORMAL
- en: One solution to this problem is called Distributed Tracing, which is a means
    of tracking the path of requests between containers, much like a profiler can
    track requests within a single application. This involves using libraries or frameworks
    that support a tracing library to create and pass along the information, as well
    as a system external to your application to collect this information and present
    it in a usable form. The earliest examples of this are documented in research
    papers from a Google system called Dapper, and an early open source implementation
    inspired by Dapper was called Zipkin, made by folks working at Twitter. The same
    concept has been repeated several times, and in 2016, a group started to come
    together to collaborate on the various tracing attempts. They formed OpenTracing,
    which is now a part of the Cloud Native Compute Foundation, to specify the format
    for sharing tracing across a variety of systems and languages.
  prefs: []
  type: TYPE_NORMAL
- en: Jaeger is an implementation of the OpenTracing standards, inspired by Dapper
    and Zipkin, created by engineers working at Uber, and donated to the Cloud Native
    Compute Foundation. Full documentation for Jaeger is available at [http://jaeger.readthedocs.io/](http://jaeger.readthedocs.io/).
    Released in 2017, Jaeger is in active development and use.
  prefs: []
  type: TYPE_NORMAL
- en: There are other tracing platforms, notably OpenZipkin ([https://zipkin.io](https://zipkin.io)),
    also available, so Jaeger isn't the only option in this space.
  prefs: []
  type: TYPE_NORMAL
- en: Spans and traces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When looking at distributed tracing, there are two common terms that you will
    see repeatedly: span and trace. A span is the smallest unit that is tracked in
    distributed tracing, and represents an individual process getting a request and
    returning a response. As the process makes requests to other services in order
    to do its work, it passes information along with the request so that the service
    being requested can create its own span and reference the requesting one. Each
    of these spans is collected and exported from each process, gathered up, and can
    then be analyzed. A full collection of spans that are all working together is
    called a trace.'
  prefs: []
  type: TYPE_NORMAL
- en: Adding, gathering, and transferring all this additional information is additional
    overhead for each of the services. While this information is valuable, it can
    also generate a huge amount of information, and if every service interacting always
    created and published every trace, the amount of data processing needed to handle
    the tracing system would exponentially outgrow the service itself. To provide
    value to tracing, tracing systems have implemented sampling so that not every
    request gets traced, but a reasonable volume, you still have enough information
    to get a good representation of the system's overall operation.
  prefs: []
  type: TYPE_NORMAL
- en: Different tracing systems handle this differently, and how much data and what
    data is passed between services is still very much in flux. Additionally, services
    that don't follow the request/response pattern – such as a background queue or
    fan-out processing – aren't easily represented by the current tracing systems.
    The data can still be captured, but presenting a consistent view of the processing
    can be far more complex.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you view the details of a trace, you are often presented with a flame-graph
    style output, which shows a timeline view of how long each trace took and what
    service was processing it. For example, here is an example trace detail view from
    the Jaeger documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/f3ff52a5-e84b-4000-a533-5d4dae94754e.png)'
  prefs: []
  type: TYPE_IMG
- en: Architecture of Jaeger distributed tracing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Like (**Elasticsearch, Fluentd, and Kibana** (**EFK**)), Jaeger is a complex
    system that collects and processes quite a bit of information. It is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/9d20266c-5139-4f29-ada1-597a40a57514.png)'
  prefs: []
  type: TYPE_IMG
- en: This is the architecture of how Jaeger worked at Uber as of 2017\. The configuration
    uses the side-car pattern we mentioned earlier, with each container running a
    nearby container that collects the spans from the instrumentation using UDP, and
    then forwards those spans into a storage system based on Cassandra. Setting up
    a Cassandra cluster, as well as the individual collectors and the query engine,
    is far more than can be easily created in a local development environment.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, Jaeger also has an all-in-one option for experimenting with and
    learning how to use Jaeger and what it can do. The all-in-one option has the agent,
    collector, query engine, and UI in a single container image that doesn't store
    any information persistently.
  prefs: []
  type: TYPE_NORMAL
- en: The Jaeger project has the all-on-one option, as well as Helm charts and variations
    that utilize Elasticsearch for persistence, documented and stored on GitHub at [https://github.com/jaegertracing/jaeger-kubernetes](https://github.com/jaegertracing/jaeger-kubernetes).
    In fact, the Jaeger project tests their development of Jaeger and each of the
    components by leveraging Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Trying out Jaeger
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can try out the current version of Jaeger by using their all-in-one development
    setup. Since they maintain this on GitHub, you can run it directly from there
    with this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This will create a deployment and a number of service frontends:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'And when the `jaeger-deployment` pod is reporting ready, you can use the following
    command to access the Jaeger query interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting web page should appear:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/2aef79e3-c902-4065-bd20-776ff57c1d9a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'By default, the Jaeger system is reporting on its own operations, so as you
    use the query interface, it will also generate its own traces that you can start
    to investigate. The Find Traces panel on the left of the window should show at
    the service jaeger-query, and if you click on the Find Traces button at the bottom,
    it will search based on the default parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aa36c7cb-5d2f-492d-b685-81d5e7875ba0.png)'
  prefs: []
  type: TYPE_IMG
- en: This page shows the times of all the traces found and how long they took, allowing
    you to dig down into them by API endpoint (which is called operation in this user
    interface), limiting the time span, and providing a rough representation of how
    long the queries took to process.
  prefs: []
  type: TYPE_NORMAL
- en: 'These traces are all made up of a single span, so are quite simple. You can
    select one of those spans and look at the trace detail, including expanding the
    information that it captured and passed along with those traces. Looking at that
    detail fully expanded should show you something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/be759bdd-3762-4a4e-8c90-5ce79b4eb911.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's look at how to add tracing to your own application.
  prefs: []
  type: TYPE_NORMAL
- en: Example – adding tracing to your application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are several things we will need to do to enable tracing from our example
    applications:'
  prefs: []
  type: TYPE_NORMAL
- en: Add the libraries and code to generate traces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add a tracing collector side-car to your pod
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look at enabling the tracing side-car first, and we will use the Python
    Flask example that we have been building earlier in the book.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for this example is online at the GitHub project at [https://github.com/kubernetes-for-developers/kfd-flask](https://github.com/kubernetes-for-developers/kfd-flask),
    and the branch for this addition is `0.6.0`. You can get the code for this project
    locally using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Adding a tracing collector to your pod
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The libraries that implement open-tracing typically use a very lightweight
    network connection, in this case UDP, to send traces from our code. UDP does not
    guarantee connections, so this also means that trace information could be lost
    if the network became too congested. OpenTracing and Jaeger minimize that by taking
    advantage of one of the guarantees of Kubernetes: two containers in the same pod
    will be placed on the same node, sharing the same network space. If we run a process
    in another container in our pod that captures the UDP packets, the network connectivity
    will be all on the same node and have very little chance of interference.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Jaeger project has an image that listens to a variety of ports to capture
    these traces and forward them to its storage and query system. The container `jaegertracing/jaeger-agent`
    is published to DockerHub and is maintained as a very small image size (5 MB for
    version 1.2). This small size and the benefit of being close to our application
    makes it ideal for running as a side-car: another container in our pod, supporting
    the main process.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can do this by adding another container to the pod defined in our flask
    deployment (`deploy/flask.yaml`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This example is based off the Jaeger [deployment documentation](https://jaeger.readthedocs.io/en/latest/deployment/),
    which provides an example of how to use this with Docker, but not Kubernetes directly.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to notice the command that we have in this container. By default,
    the container runs `/go/bin/agent-linux`, but without any options. In order to
    send data to our local installation of Jaeger, we will need to tell the collector
    where to send it. The destination is defined by the option `--collector.host-port`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, we installed the Jaeger all-in-one into the default namespace,
    and it includes a service named `jaeger-collector`, so that will be directly available
    to this pod. If you have an installation of Jaeger in your cluster that''s more
    robust, you may also have it defined in a different namespace. For example, the
    Helm installation of Jaeger installs into a namespace, `jaeger-infra`, and in
    those cases the value of the `collector.host-port` option would need to change
    to reflect that: `jaeger-collector.jaeger-infra.svc:14267`.'
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple ports used by Jaeger here as well, intentionally to allow
    the agent to collect from a number of legacy mechanisms used by alternate languages.
    We will be using UDP port `6382` for the `python jaeger-tracing` client library.
  prefs: []
  type: TYPE_NORMAL
- en: Add the libraries and code to generate traces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We start by adding two libraries for the tracing to our project: `jaeger-client`
    and `flask_opentracing`. `flask-opentracing` adds tracing into Flask projects
    so that you can easily have all HTTP endpoints traced automatically. The OpenTracing
    project doesn''t include any collectors, so we also need a library to collect
    and send the trace data somewhere, in this case, jaeger-client.'
  prefs: []
  type: TYPE_NORMAL
- en: The example also adds the requests library, as in this example we will add an
    HTTP endpoint that makes a remote request, processes the responses, and returns
    the values—and add tracing to that sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Importing the libraries and initializing the tracer is fairly straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Jeager recommends you use a method to initialize the tracer indirectly, as shown
    previously. The configuration in this case sets the sampler to forward all requests;
    when you use this in production, you want to consider that carefully as a configuration
    option, as tracing every request can be overwhelming in high volume services.
  prefs: []
  type: TYPE_NORMAL
- en: 'The tracer is initialized right after we create the Flask application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: This works with Flask to instrument all `@app.routes` with tracing, each route
    will be labeled as an operation based on the name of the Python function. You
    can also trace only specific routes with a different configuration setup and add
    tracing annotations on the Flask routes you want to trace.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rebuilding the Flask image and deploying it will immediately start to generate
    traces, and with the jaeger-agent running in a side-car, the local `jaeger dev`
    instance will start showing traces immediately. You should see a service named
    `flask-service`, based on our application name and it should have multiple operations
    listed within it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ec715a4c-c36b-4f60-81f2-fd0e0365c833.png)'
  prefs: []
  type: TYPE_IMG
- en: The operations alive, ready, and metrics are the Flask routes that have been
    enabled to support the liveness and readiness probes, as well as the `prometheus`
    metrics. With this already defined on our example pod, they are getting consistent
    connections, which in turn generates the traces associated with the requests.
  prefs: []
  type: TYPE_NORMAL
- en: 'This alone is useful, but does not yet tell you what within the method took
    more or less time. You can add spans into a trace around methods or sections of
    code that you''re interested in instrumenting, using the `opentracing` library
    that gets installed with `flask-opentracing`. The following code snippet shows
    how to wrap the call to Redis that we use in our readiness probe with a trace
    span, so that it will show up separately:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The keys here are getting the current tracing span that we generated for every
    request with `flask_tracer.get_span()`, and then using that in a `with` statement,
    which adds the span to a discrete bit of code that executes within that context.
    We can also use methods on the span, which is available within that block of code.
    We use the method `set_tag` to add a tag with the value of the result of the ping,
    so that it will be available in the specific trace output.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will go ahead and add a `@app.route` called `/remote` to make a remote HTTP
    request to GitHub, and add tracing around that to see it as sub spans:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'This example is similar to the readiness probe, except we''re wrapping different
    sections of code in different spans and naming them explicitly: `github-api` and
    `parse-json`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'While adding code, you can use commands such as `kubectl delete` and `kubectl
    apply` to recreate the deployment with building it and pushing it to your container
    registry. For these examples, my pattern was to run the following commands from
    the project''s home directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: You will want to replace the image registry reference and Docker tag with values
    from your  project.
  prefs: []
  type: TYPE_NORMAL
- en: 'And then, check the status of the deployment with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Once it''s fully online, you''ll see it reporting as Ready:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The 2/2 shows you that two containers are running for the Flask pod, our main
    code and the jaeger-agent side-car.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are using Minikube, you can also use the service commands to make it
    easy to open these endpoints in a browser:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Any service with a node port setting can be easily opened locally with commands
    such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'With this code added, built, and deployed, you can see the traces in Jaeger.
    Direct your browser to make some requests to `/remote` to generate spans from
    the requests, and in the Jaeger query browser you should see something like the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/a8091b4e-1492-4e78-8a35-e54de9554fa2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The top of the Jaeger query window will show you dots indicating the time of
    the query and the relative duration, and you''ll see the various traces that it
    found listed below – four in our case. If you select a trace, you can get to the
    detailed view, which will include the sub-spans. Click on the spans to get more
    detail from each of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/f21aaceb-4f92-4027-bca9-3ba531a43300.png)'
  prefs: []
  type: TYPE_IMG
- en: With the span detail view, any tags you set on the span within the code will
    be viewable, and you can see that the `github-api` call took the majority of the
    time (265 of 266 ms) in responding to the request to `/remote`.
  prefs: []
  type: TYPE_NORMAL
- en: Considerations for adding tracing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tracing is an immensely powerful tool that also comes with a cost. Every trace
    is (although small) some overhead to process and manage. You may be excited to
    add tracing to every method in your application, or to build it into a library
    that attaches tracing and span creation to every method call. This can be done,
    and you will quickly find your infrastructure overwhelmed with trace information.
  prefs: []
  type: TYPE_NORMAL
- en: Tracing is also a tool that has the most benefit when tied directly to the responsibility
    of running the code. Be very aware that as you add tracing, you are also adding
    a lot of ancillary processing needed to capture, store, and query the data created
    by the traces.
  prefs: []
  type: TYPE_NORMAL
- en: A good way to handle the balancing act of the trade-offs is to add tracing intentionally,
    iteratively, and slowly – building to gain visibility as you need it.
  prefs: []
  type: TYPE_NORMAL
- en: OpenTracing as a standard is supported by a number of vendors. OpenTracing is
    also an evolving standard. While writing this book, there is a lot of conversation
    happening about how to best share and handle the span data (often called "baggage")
    that is carried along with requests between processes. Like tracing itself, adding
    data can add value, but it comes with the cost of larger requests and more processing
    needed to capture and handle the information.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced logging and tracing with Fluentd and Jaeger.
    We showed how to deploy it and use it, capturing and aggregating data from your
    code when it runs at scale. We walked through how to use an Elasticsearch query
    to find data. We also looked at how to view Jaeger traces and how to add tracing
    to your code.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at ways of using Kubernetes to support and
    run integration testing, as well as using it with continuous integration.
  prefs: []
  type: TYPE_NORMAL
