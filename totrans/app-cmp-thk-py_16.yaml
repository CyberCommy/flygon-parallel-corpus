- en: '*Chapter 13*: Using Classification and Clusters'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will use the classification and clustering capabilities
    of the Python programming language. We will use the computational thinking elements
    to define the necessary components for problems when working with clusters and
    classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining training and testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing data clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be able to design algorithms that are the
    best fit for the scenarios presented. You will also be able to identify the Python
    functions that are the most aligned with the problems presented and generalize
    your solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will need the latest version of Python and **Scikit-Learn** to execute the
    code in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will find the code used in this chapter here: [https://github.com/PacktPublishing/Applied-Computational-Thinking-with-Python/tree/master/Chapter13](https://github.com/PacktPublishing/Applied-Computational-Thinking-with-Python/tree/master/Chapter13)'
  prefs: []
  type: TYPE_NORMAL
- en: 'You will find the **Pima Indians Diabetes Database** from **Kaggle** here:
    [https://www.kaggle.com/uciml/pima-indians-diabetes-database](https://www.kaggle.com/uciml/pima-indians-diabetes-database)'
  prefs: []
  type: TYPE_NORMAL
- en: Data training and testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we''re going to learn how to create models for training and
    testing data using Python tools and libraries. When working with data and data
    science, we sometimes want to train the algorithm to continue gathering and learning
    from the data. Data training is then used for data mining and machine learning.
    First, let''s define the **training dataset**:'
  prefs: []
  type: TYPE_NORMAL
- en: It is a sample of data used to fit the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is an actual dataset that is used to train the model (weights and biases,
    in the case of a **neural network**). The training model *sees* and *learns* from
    this data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In computing, a neural network is a system of computing that is created using
    biological neural networks in human and animal brains as inspiration. When using
    training datasets, such as when we are creating machine learning models, the models
    depend heavily on the data. *But what is machine learning?* **Machine learning**,
    or **ML**, is an application of **artificial intelligence** (**AI**) that allows
    a machine to learn automatically using a program without explicitly being programmed
    to do so.
  prefs: []
  type: TYPE_NORMAL
- en: 'Without a top-quality foundation of data training, an algorithm is useless.
    Data training in ML refers to the initial data used to develop models. They find
    relationships and develop understanding and discover patterns and trends. Input
    data is fed into the ML algorithm and also to all the techniques associated to
    produce an output. That output is also fed back into the models as updated feedback,
    which in turn provides feedback data that is used as input again. The process
    is cyclical, continuously adapting and learning. The following diagram shows a
    simple ML graphic with the processes that happen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.1 – Machine Learning and its relationships with input and output'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Figure_13.01_B15413.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.1 – Machine Learning and its relationships with input and output
  prefs: []
  type: TYPE_NORMAL
- en: In ML, data is combined with statistical tools to predict an output. The machines
    receive input data and use an algorithm to construct answers. This is similar
    to **data mining**, where large datasets are used to find anomalies, correlations,
    patterns, and so on to predict an outcome.
  prefs: []
  type: TYPE_NORMAL
- en: In data mining, we extract information but using methods that pull the necessary,
    relevant, error-free data points. That is, with data mining, we extract what we
    need from the dataset without extracting those anomalies while also looking at
    correlations and patterns within our data. The difference between data mining
    and ML is that ML analyzes both input data and output data. So once the output
    is processed, it goes back to the algorithm, where it's fed back to the input
    data, and reprocessed. The cycle is ongoing, as you can see from the preceding
    diagram.
  prefs: []
  type: TYPE_NORMAL
- en: 'Important Note:'
  prefs: []
  type: TYPE_NORMAL
- en: There are four groups of ML algorithms, but in this book, we are only going
    to introduce two. While this is not an ML book, Python's applications in the ML
    arena continue to grow, so it is relevant to our goal of understanding the applications
    of the programming language and how we use computational thinking to solve problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'In ML, we use two types of important techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Supervised learning** maps data pairs, using input data and expected output
    (training data) so that the model can find underlying patterns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unsupervised learning** uses unlabeled training data to make conclusions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In addition, there are two other techniques we won''t go into in this book:
    semi-supervised learning and reinforcement learning.'
  prefs: []
  type: TYPE_NORMAL
- en: In supervised learning, the learning algorithm is presented with a set of inputs
    along with their desired outputs (also called labels). The goal is to discover
    a rule that enables the computer to re-create the outputs, or in other words,
    map the input and output. On the other hand, unsupervised learning allows us to
    approach problems with little or no idea what our results should look like. The
    output variables are unlabeled. With unsupervised learning, an algorithm is presented
    with a set of inputs but no desired outputs, which means the algorithm must find
    structures and patterns on its own.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows a roadmap to follow for supervised and unsupervised
    learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.2 – Types of machine learning'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Figure_13.02_B15413.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.2 – Types of machine learning
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the preceding figure, we have two types of **supervised
    learning**. When we are given the training data and desired outputs, we use **regression**
    or **classification**. With **regression**, we predict a continuous-valued output.
    With **classification**, we get a discrete-valued output (0 or 1). An example
    of regression would be predicting how much rainfall we'd get on a given day, while
    with classification, we'd be looking to know whether it would rain or not.
  prefs: []
  type: TYPE_NORMAL
- en: For **unsupervised learning**, the preceding figure contains the example of
    **clustering**. In **clustering**, we get the training data, but only a few desired
    outputs. An example of clustering is **grouping**, which would take items from
    large collections of data and group them.
  prefs: []
  type: TYPE_NORMAL
- en: We can apply these types of learning styles to **artificial neural networks**.
    The training of a neural network is usually conducted by determining the difference
    between the processed output of the network (often a prediction) and a target
    output. This is the error, so the network then adjusts its weighted associations
    according to a learning rule and using this error value to adapt. Neural networks
    are typically organized in layers. Layers are made up of several interconnected
    nodes that contain an activation function.
  prefs: []
  type: TYPE_NORMAL
- en: 'An activation function is used to make decisions about whether a neuron is
    activated or not. To do so, a weighted sum is calculated and further bias is added.
    We use activation functions to provide outputs of a neuron with non-linearity.
    The three most common activation functions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sigmoid** can be formulated as ![](image/Formula_B15413_13_001.png), where
    the input of the real number is *x*. It returns a value between –1 and 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tanh** is given by *tanh(x)*. It is a hyperbolic tangent function with real
    number input *x*; it returns a value between –1 and 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rectified Linear Unit** (**ReLU**) is a piecewise linear function. Its output
    is the same as the input if the input is positive or 0 otherwise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram shows the graphs associated with each of the aforementioned
    activation functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.3 – Activation functions'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Figure_13.03_B15413.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.3 – Activation functions
  prefs: []
  type: TYPE_NORMAL
- en: 'Patterns are presented to the network via the **input layer**, which communicates
    to one or more **hidden layers**, where the actual processing is done via a system
    of weighted **connections**. The hidden layers then link to an **output layer**
    where the answer is output as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.4 – Artificial Neural Network (ANN) model'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Figure_13.04_B15413.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.4 – Artificial Neural Network (ANN) model
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the summation operator (![](image/Formula_B15413_13_002.png))
    takes the input values and moves through the network and creates an output. They
    must be summed and return a single value when entering a new node. An activation
    function essentially *squashes* the input and transforms it into an output value
    that represents how much a node should contribute (that is, when a node should
    fire). A node is considered to be *fired up* when it is activated. It takes the
    output value and converts it so the next node can take it as input. This is called
    **firing up**.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's take a look at how we classify data using pandas.
  prefs: []
  type: TYPE_NORMAL
- en: Classifying data example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now let''s take a look at an example where we are classifying data. The following
    screenshot shows an example of using supervised learning. To produce the output
    that can be seen in the screenshot, we used an existing dataset from [www.kaggle.com](http://www.kaggle.com).
    The dataset is called **Pima Indians Diabetes Database**. It describes whether
    or not a Pima Indian patient was diagnosed with diabetes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.5 – Sample of unsupervised learning'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Figure_13.05_B15413.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.5 – Sample of unsupervised learning
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, the attributes from the table, also known as the *input variables*
    (*x*), are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Number of times pregnant
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plasma glucose concentration for 2 hours in an oral glucose tolerance test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diastolic blood pressure (mm Hg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Triceps skinfold thickness (mm)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2-hour serum insulin (mu U/ml)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Body mass index ![](image/Formula_B15413_13_003.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diabetes pedigree function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Age (years)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the *output variable* (*y*), we have the class variable (0 or 1). From the
    dataset, each row represents a patient and whether or not that person received
    a diagnosis of diabetes in the past 5 years. As you can see, there are eight input
    variables and one output variable (the last column) as shown in the preceding
    figure.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be using the binary classification model (1 or 0), which maps the rows
    of input variables (*x*) to the output variable (*y*). This will summarize *y
    = f(x)*. The following code snippet uses this information to get our outputs.
    Please note we will be discussing the full file in snippets throughout this example:'
  prefs: []
  type: TYPE_NORMAL
- en: ch13_diabetesA.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from the preceding snippet, we are uploading the dataset called
    `diabetes.csv` (from Kaggle). If you need a reminder of how to save the file and
    locate the path needed, take a look at [*Chapter 12*](B15413_12_Final_SK_ePub.xhtml#_idTextAnchor159),
    *Using Python in Experimental and Data Analysis Problems*, in the *Understanding
    data analysis with Python* section. There are many ways to upload datasets.
  prefs: []
  type: TYPE_NORMAL
- en: As we did in [*Chapter 12*](B15413_12_Final_SK_ePub.xhtml#_idTextAnchor159),
    *Using Python in Experimental and Data Analysis Problems*, we are using the very
    popular **pandas** and importing it as `pd`. Pandas is used for data manipulation
    and analysis. It offers data structures and operations for manipulating numerical
    tables and time series. The `read_csv()` function from pandas deals with importing
    data from **Comma-Separated Values** (**CSV**) values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Important Note:'
  prefs: []
  type: TYPE_NORMAL
- en: We need to find the correct directory. When you're calling the `.csv` file make
    sure you're in the correct directory (where the `.csv` file is located) to avoid
    error codes. Use `os.chdir()` after using `import os`, then use `print('Current
    directory', os.getcwd())`. See [*Chapter 12*](B15413_12_Final_SK_ePub.xhtml#_idTextAnchor159),
    *Using Python in Experimental and Data Analysis Problems*, for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you run the preceding snippet of code, you can look at your variable explorer
    to see the item shown in the following screenshot. Note that the **variable explorer**
    is a tool that allows you to browse and manage objects associated with and used
    in your code. This tool is part of the **Spyder** environment, which runs Python
    with additional functionalities and editing tools, such as the variable explorer.
    The variable explorer is found in Spyder at the top on the right-hand side of
    our environment. The following screenshot shows the view of our database in the
    variable explorer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.6 – Variable explorer sample'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Figure_13.06_B15413.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.6 – Variable explorer sample
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, **Size** describes the dataset. It shows the number of patients,
    **786**, and the total number of variables, **9**. Now we have a better understanding
    of our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'But let''s say you don''t know what type of learning you will be needing. You
    can type this function in the console to get a full picture of the data and outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the information we receive after using the preceding
    line of code in our algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.7 – Display of information after running a description algorithm'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Figure_13.07_B15413.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.7 – Display of information after running a description algorithm
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see from the preceding figure, we are able to get all the numerical
    features and know that there is no categorical data. We want that information,
    so the following line of code can be used to see the correlation between the variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This simple line of code helps us get the information shown in the following
    screenshot. Please note that the following screenshot may look different depending
    on the environment you are using. When running this code, using environments like
    **Spyder** or **Jupyter**, depending on your theme settings and choices, the table
    may look different, with different color schemes (or no color schemes):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.8 – Dataset correlation graphic'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Figure_13.08_B15413.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.8 – Dataset correlation graphic
  prefs: []
  type: TYPE_NORMAL
- en: We can see the correlation between all the variables with the outcome (output
    (*y*)). The preceding screenshot shows us that plasma glucose has the strongest
    correlation with the outcome, and insulin has the lowest.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have a better understanding of the dataset, let''s separate the
    input variables and output variables to put in the model. Let''s take a look at
    the following code snippet from our `ch13_diabetesA.py` file, which exemplifies
    this for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the `print` function to check our values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you have run the preceding snippet of code, the output data will look
    as shown in the following screenshot. Note that the result shows what we''ve defined
    as the variables `x_variables` and `y_variable`, which are in turn defined as
    parts of the dataset, as noted in the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.9 – Training the dataset output for algorithm printing input and
    output values'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Figure_13.09_B15413.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.9 – Training the dataset output for algorithm printing input and output
    values
  prefs: []
  type: TYPE_NORMAL
- en: Now we have to split the data into a training dataset and a test dataset. The
    purpose of the split technique is to evaluate the performance of an ML algorithm.
    It is only meant for any type of supervised learning algorithm. The first set
    (the training dataset) is used for the purpose of fitting the model.
  prefs: []
  type: TYPE_NORMAL
- en: The main goal is to fit it on available data with known inputs and outputs,
    then make predictions on new examples in the future where we do not have the expected
    output or target values.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Scikit-Learn library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another important library when working with data and ML is the `scikit-learn`
    (`sklearn`) library. This library is particularly useful for classification, regression,
    clustering, model selection, dimensionality reduction, and more. You may recall
    from [*Chapter 12*](B15413_12_Final_SK_ePub.xhtml#_idTextAnchor159), *Using Python
    in Experimental and Data Analysis Problems*, under the *Using data libraries in
    Python section*, that you can use `pip install` from your **Command Prompt** window
    to install the required libraries. Once you have the library, you can import it
    into the code, as shown in the following snippet, which uses `sklearn` to split
    the data. As a note, this snippet is part of the larger `ch13_diabetesA.py` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the known parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`x_variable` and `y_variable` as previously defined.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`test_size`: The test size will be 20% of the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`random_state`: It sets a seed to the random generator so your train and test
    splits are always deterministic. If it is set to none, then a randomly initialized
    `RandomState` object is returned.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram shows the process and how each element interacts with
    others in the cycle:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.10 – Data cycle in ML'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Figure_13.10_B15413.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.10 – Data cycle in ML
  prefs: []
  type: TYPE_NORMAL
- en: Note that we'll use a sequential model in the algorithm. In addition, we are
    using the `keras` library, which is used with Python so we can run deep learning
    models with our algorithms. Make sure you have the `keras` library available to
    use for this algorithm. If you have TensorFlow installed, you should already have
    access to the Keras library.
  prefs: []
  type: TYPE_NORMAL
- en: When working with machine learning problems and algorithms, you'll have a choice
    of libraries. We chose Keras for this particular problem. Keras is open source,
    and is useful in creating artificial neural networks. TensorFlow is a platform
    that contains many machine learning components and tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Keras works on top of TensorFlow and makes it easier to interact with it in
    the Python programming language. Keras is a higher-level API, which we'll discuss
    further. Because of its capacity, it can sometimes be slower than usual. **PyTorch**
    is another library used for artificial neural networks. It's a lower-level API,
    and therefore runs faster. Keras is supported by **Google**, while PyTorch is
    supported by **Facebook**. Both are helpful, so deciding on which to use is typically
    a developer's preference. Personally, I prefer the Keras library.
  prefs: []
  type: TYPE_NORMAL
- en: The sequential API allows you to create models layer by layer in a step-by-step
    fashion. There are two other available models, the **functional API** and **model
    sub-classing**. We'll use the sequential API because it's the easiest architecture,
    while the functional API is used for deep learning (complex models) and model
    sub-classing.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few things we should note about using Keras:'
  prefs: []
  type: TYPE_NORMAL
- en: The model class is the root class and is used to define the architecture for
    the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Like Python itself, Keras uses object-oriented programming, which means we can
    add subclasses.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The subclasses in the model are customizable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All that said, it should also be noted that sub-classing is more challenging
    than if we were to use the sequential or functional API. Now let''s take a look
    at an updated algorithm using our Keras library. Remember to include the directory
    of your file or save the `.csv` file to the necessary directory to run the algorithm
    correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: ch13_diabetesB.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: From the preceding snippet, we can see that we added four layers that are densely
    connected.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first layer is built as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 12 neurons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_dim = 8` (that is, input values that are coming into the network)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`activation ''relu''`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As you can see, we have added multiple models and defined them. To compile
    the model, we use the following snippet of code, contained in the same code file.
    We can also set `model.fit` to use our libraries and the following code, which
    is part of our `ch13_diabetesB.py` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code compiles the Adam optimizer. The Adam optimizer is used
    for stochastic gradient descent and updates network weights iteratively using
    the training data. Once we run our code, our output provides the information shown
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.11 – Output of model run using the Keras library'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Figure_13.11_B15413.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.11 – Output of model run using the Keras library
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the accuracy may be different as you run the algorithm. Test the
    algorithm a few times to see changes in your window. After we''ve run our accuracy
    model, we print the model summary using `model.summary()`, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.12 – Model summary of the algorithm'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Figure_13.12_B15413.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.12 – Model summary of the algorithm
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've seen how to run our algorithms using the diabetes data file,
    let's look briefly at some optimization models that will help us evaluate the
    algorithm. We will not be going into these algorithms, but we do want to mention
    a few of the various tools available to us. When we are working with modeling,
    we use optimization models, such as **binary cross-entropy**, the **Adam optimization
    algorithm**, and **gradient descent**.
  prefs: []
  type: TYPE_NORMAL
- en: Defining optimization models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's take a look at the types of models. Note that we are not diving deep into
    the use of these models, but further exploration into their application to our
    algorithm is recommended.
  prefs: []
  type: TYPE_NORMAL
- en: The binary cross-entropy model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In binary classification, we use cross-entropy as the default **loss function**.
    The loss function is a method that helps us evaluate how our algorithm models
    the data. With the loss function, we can use optimization in order to produce
    more accurate results, that is, it helps reduce the **prediction error**. We use
    the loss function when target values are in the binary set.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-entropy is a loss function that calculates the difference between two
    probability distributions. We can use cross-entropy when optimizing classification
    models using **logistic regression** and **artificial neural networks**.
  prefs: []
  type: TYPE_NORMAL
- en: The Adam optimization algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Adam algorithm is a method for **stochastic optimization**. Stochastic optimization
    is used when there is randomness in a function, to maximize or minimize the value
    of the function. The Adam optimization algorithm is appropriate for some simple
    optimization problems that are *non-convex*. It is efficient and uses little memory
    but can be applied to large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: The gradient descent model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The gradient descent algorithm is a first-order optimization algorithm. First-order
    refers to linear local errors. We use gradient descent on functions that can be
    differentiated to find a local minimum.
  prefs: []
  type: TYPE_NORMAL
- en: The confusion matrix model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The confusion matrix is also known as the **error matrix**. The confusion matrix
    is visually helpful, as it presents the performance of the algorithm in a table
    format, which allows better visualization of that performance. It is typically
    used in supervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: As stated, this is just some base information as you start working on the optimization
    of your algorithms. Additional information on ML can be found in other Packt books,
    such as *Python Machine Learning* and *Exploratory Data Analysis with Python*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we move on to an introduction to clusters, let''s do a quick recap of
    what we learned in this section on using the Keras package and model:'
  prefs: []
  type: TYPE_NORMAL
- en: Loading data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining a neural network in Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compiling a Keras model using the efficient numerical backend
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a model on data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating a model on data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making predictions with the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let's move on to data clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing data clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we're going to take a look at how to approach data clustering.
    First, let's define what we mean by **data clustering**. Data clustering refers
    to how we partition data into groups or clusters. Clusters can be meaningful if
    they provide an expanded understanding of domain knowledge. We use clustering
    for many applications, such as medicine, where clustering can help identify how
    a group of patients responds to treatment, or market research, where clustering
    is used to group consumers in order to appeal to that group based on that particular
    group's characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the purpose of this discussion, we are going to look at synthetic clusters
    rather than applied clusters. In [*Chapter 16*](B15413_16_Final_SK_ePub.xhtml#_idTextAnchor219),
    *Advanced Applied Computational Thinking Problems*, you''ll see some examples
    of clusters in context. A **synthetic cluster** is made from a synthetic dataset.
    That is, we generate the dataset using an algorithm. Let''s take a look at the
    following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: ch13_syntheticDataset.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice from the preceding snippet that we establish the number of samples,
    the number of features, and the number of clusters, among other things. In addition,
    we then create a scatterplot of the synthetic data and plot the result. The following
    graph shows the results of our synthetic dataset plotted as a scatterplot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.13 – Synthetic dataset scatterplot'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Figure_13.13_B15413.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.13 – Synthetic dataset scatterplot
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the number of samples for this synthetic dataset was `1800`. Try
    to change the number of samples to see the changes in the scatterplot. Now that
    we have a dataset, we can start applying clustering algorithms. Here are some
    of the more common clustering algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The BIRCH algorithm**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The K-means clustering algorithm**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will look at the aforementioned algorithms in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Using the BIRCH algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Balanced Iterative Reducing and Clustering using Hierarchies** (**BIRCH**)
    is a clustering algorithm that uses the cluster centroids as long as there is
    enough available memory and time. For the BIRCH and K-means clustering algorithms,
    we will share an algorithm and the corresponding plot in order to better understand
    them.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following snippet shows us the BIRCH algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: ch13_BIRCH.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Note that we arbitrarily chose two clusters in this sample, as you can see in
    the line `model = Birch(threshold = 0.01, n_clusters = 2)`. We are sticking with
    our sample of 1800 so that we can compare our output figures. The following screenshot
    shows two sample BIRCH models. The first (*on the left*) shows the algorithm run
    as provided in the preceding snippet. The second (*on the right*) shows the same
    algorithm run but for three clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run the second graph, we changed the model line code to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following figure with both plots shown, `n_clusters = 2`
    and `n_clusters = 3`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.14 – BIRCH models using 2 and 3 clusters, respectively'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Figure_13.14_B15413.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.14 – BIRCH models using 2 and 3 clusters, respectively
  prefs: []
  type: TYPE_NORMAL
- en: Notice that on the *left* of the preceding screenshot, two clusters are clearly
    shown. When compared to *Figure 13.13*, you can see some data points have been
    converted to fit each of the clusters identified. The scatterplot on the *right*
    of the preceding screenshot divides the data into three distinct clusters. To
    get more familiar with the clustering algorithms, change the parameters to see
    what happens when you change the number of clusters, the sample size, and other
    parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's take a look at the K-means clustering algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Using the K-means clustering algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The K-means clustering algorithm is one of the most widely used clustering
    algorithms. The algorithm assigns examples so that the variance is minimized in
    each of the identified clusters. Much like with the BIRCH algorithm, we set the
    number of clusters within the algorithm. Let''s take a look at the K-means code
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: ch13_KMeans.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, notice that we''re using the same number of clusters (`2`) and the number
    of samples (`1800`) so that we can compare our displays. The following screenshot
    shows the K-means scatterplot output resulting from the preceding algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.15 – K-means algorithm output'
  prefs: []
  type: TYPE_NORMAL
- en: '](image/Figure_13.15_B15413.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.15 – K-means algorithm output
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the data is still exactly the same, however, when we compare the
    displays we got from the BIRCH algorithm and the K-means algorithm, you can see
    that our algorithms produced very different results for our clusters.
  prefs: []
  type: TYPE_NORMAL
- en: There are many other clustering algorithms we can use and test. Learning about
    them and comparing the results is imperative in determining which ones to use
    based on real datasets. The results from the K-means algorithm, in this case,
    do not really fit the model well. The BIRCH model seems more suited when using
    two clusters because the variance in the K-means algorithm is unequal.
  prefs: []
  type: TYPE_NORMAL
- en: As we move on from the clustering examples, please note that, as for much of
    data science and ML, the more we use the models and algorithms, the more we understand
    their uses and when the models are appropriate, and we can learn to visually identify
    whether an algorithm fits our data or not.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to use some of the packages available for the
    Python programming language to create models of large sets of data. We used packages,
    such as Keras, to upload data and define neural networks. We trained the model
    and evaluated the model. We used the model to make predictions. We also learned
    about the classification and testing of data and how to work with data clusters.
    After reading this chapter, you can now define data training and how Python is
    used in data training. You can also define and use a clustering algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: We will continue to explore some of the topics discussed here in the next chapter.
    We will also see some of these applications in the examples provided in [*Chapter
    16*](B15413_16_Final_SK_ePub.xhtml#_idTextAnchor219), *Advanced* *Applied Computational
    Thinking Problems*.
  prefs: []
  type: TYPE_NORMAL
