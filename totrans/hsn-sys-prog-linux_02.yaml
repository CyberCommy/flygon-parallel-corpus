- en: Virtual Memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Coming back to this chapter, we will look at the meaning and purpose of **virtual
    memory** (**VM**) and, importantly, why it is a key concept and required one.
    We will cover the meaning and importance of VM, paging and address-translation,
    the benefits of using VM, the memory layout of a process in execution, and the
    internal layout of a process as seen by the kernel. We shall also delve into what
    segments make up the process virtual address space. This knowledge is indispensable
    in difficult-to-debug situations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Virtual memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Process virtual address space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A modern desktop PC or laptop is required; Ubuntu Desktop specifies the following as recommended
    system requirements for installation and usage of the distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: 2 GHz dual core processor or better
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Running on a physical host**: 2 GB or more system memory'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Running as a guest**: The host system should have at least 4 GB RAM (the
    more, the better and smoother the experience)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 25 GB of free hard drive space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Either a DVD drive or a USB port for the installer media
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Internet access is definitely helpful
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We recommend the reader use one of the following Linux distributions (can be
    installed as a guest OSon a Windows or Linux host system, as mentioned):'
  prefs: []
  type: TYPE_NORMAL
- en: Ubuntu 18.04 LTS Desktop (Ubuntu 16.04 LTS Desktop is a good choice too as it
    has long term support as well, and pretty much everything should work)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ubuntu Desktop download link: [https://www.ubuntu.com/download/desktop](https://www.ubuntu.com/download/desktop)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fedora 27 (Workstation)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Download link: [https://getfedora.org/en_GB/workstation/download/](https://getfedora.org/en_GB/workstation/download/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that these distributions are, in their default form, OSS and non-proprietary,
    and free to use as an end user.
  prefs: []
  type: TYPE_NORMAL
- en: There are instances where the entire code snippet isn't included in the book
    . Thus the GitHub URL to refer the codes: [https://github.com/PacktPublishing/Hands-on-System-Programming-with-Linux](https://github.com/PacktPublishing/Hands-on-System-Programming-with-Linux).
  prefs: []
  type: TYPE_NORMAL
- en: Also, for the further reading section, refer to the preceding GitHub link.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Modern operating systems are based on a memory model called VM. This includes
    Linux, Unixes, MS Windows, and macOS. Truly understanding how a modern OS works
    under the hood requires a deep understanding of VM and memory management – not
    topics we delve into in intricate detail in this book; nevertheless, a solid grasp
    of VM concepts is critical for Linux system developers.
  prefs: []
  type: TYPE_NORMAL
- en: No VM – the problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s imagine for a moment that VM, and all the complex baggage it lugs around,
    does not exist. So, we''re working on a (fictional) pure flat physical memory
    platform with, say, 64 MB RAM. This is actually not that unusual – most old OSes
    (think DOS) and even modern **Real-Time Operating Systems** (RTOSes) operate this
    way:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3c80c1e0-d9e3-4f87-8f4a-e887fa0a5ed2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Flat physical address space of 64 MB'
  prefs: []
  type: TYPE_NORMAL
- en: 'Obviously, everything that runs on this machine must share this physical memory
    space: the OS, device drivers, libraries, and applications. We might visualize
    it this way (of course, this is not intended to reflect an actual system – it''s
    just a highly simplified example to help you understand things): one OS, several
    device drivers (to drive the hardware peripherals), a set of libraries, and two
    applications. The physical memory map (not drawn to scale) of this fictional (64
    MB system) platform might look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Object** | **Space taken** | **Address range** |'
  prefs: []
  type: TYPE_TB
- en: '| Operating system (OS) | 3 MB | 0x03d0 0000 - 0x0400 0000 |'
  prefs: []
  type: TYPE_TB
- en: '| Device Drivers | 5 MB | 0x02d0 0000 – 0x0320 0000 |'
  prefs: []
  type: TYPE_TB
- en: '| Libraries | 10 MB | 0x00a0 0000 – 0x0140 0000 |'
  prefs: []
  type: TYPE_TB
- en: '| Application 2 | 1 MB | 0x0010 0000 – 0x0020 0000 |'
  prefs: []
  type: TYPE_TB
- en: '| Application 1 | 0.5 MB | 0x0000 0000 – 0x0008 0000 |'
  prefs: []
  type: TYPE_TB
- en: '| Overall Free Memory | 44.5 MB | <various> |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: The physical memory map'
  prefs: []
  type: TYPE_NORMAL
- en: 'The same fictional system is represented in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c2247e5e-1f97-4fd0-8ef7-410a2ccf8b62.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 2: The physical memory map of our fictional 64 MB system'
  prefs: []
  type: TYPE_NORMAL
- en: Normally, of course, the system will undergo rigorous testing before release
    and will perform as expected; except, there's this thing you might have heard
    of in our industry called bugs. Yes, indeed.
  prefs: []
  type: TYPE_NORMAL
- en: 'But let''s imagine a dangerous bug creeps into Application 1, say, within the
    use of the ubiquitous `memcpy(3)` glibc API, due to either of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Inadvertent programming errors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deliberate malicious intent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As a quick reminder, the usage of the `memcpy` library API is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`void *memcpy(void *dest, const void *src, size_t n).`'
  prefs: []
  type: TYPE_NORMAL
- en: Objective
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This C program snippet as follows intends to copy some memory, say 1,024 bytes,
    using the usual `memcpy(3)` glibc API, from a source location 300 KB into the
    program to a destination location 400 KB into the program. As Application 1 is
    the program at the low end of physical memory (see the preceding memory map),
    it starts at the `0x0` physical offset.
  prefs: []
  type: TYPE_NORMAL
- en: We understand that on a modern OS nothing will start at address `0x0`; that's
    the canonical NULL memory location! Keep in mind that this is just a fictional
    example for learning purposes
  prefs: []
  type: TYPE_NORMAL
- en: First, let's see the correct usage case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to the following pseudocode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The effect of the preceding code is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/abe5252f-75e7-4d7c-bf4a-0aeab82fb356.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 3: Zoomed into App 1: the correct memcpy()'
  prefs: []
  type: TYPE_NORMAL
- en: As can be seen in the preceding diagram, this works! The (big) arrow shows the
    copy path from source to destination, for 1,024 bytes. Great.
  prefs: []
  type: TYPE_NORMAL
- en: Now for the buggy case.
  prefs: []
  type: TYPE_NORMAL
- en: 'All remains the same, except that this time, due to a bug (or malicious intent),
    the `dest`pointer is modified as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The destination location is now around 64 KB (0x03cf0000 – 0x03d00000) into
    the operating system! The best part: the code itself does not fail*.* `memcpy()`
    does its job. Of course, now the OS is probably corrupted and the entire system
    will (eventually) crash.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the intent here is not to debug the cause (we know); the intent here
    is to clearly realize that, in spite of this bug, memcpy succeeds.
  prefs: []
  type: TYPE_NORMAL
- en: How come? This is because we are programming in C – we are free to read and
    write physical memory as we wish; inadvertent bugs are our problem, not the language's!
  prefs: []
  type: TYPE_NORMAL
- en: So what now? Ah, this is one of the key reasons why VMsystems came into existence.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unfortunately, the term **virtual memory** (**VM**) is often misunderstood or
    hazily understood, at best, by a large proportion of engineers. In this section,
    we attempt to clarify what this term and its associated terminologies (such as
    memory pyramid, addressing, and paging) really mean; it's important for developers
    to clearly understand this key area.
  prefs: []
  type: TYPE_NORMAL
- en: First, what is a process?
  prefs: []
  type: TYPE_NORMAL
- en: A process is an instance of a program in execution*.*
  prefs: []
  type: TYPE_NORMAL
- en: 'A program is a binary executable file: a dead, disk object. For example, take
    the `cat` program*:* `$ ls -l /bin/cat`'
  prefs: []
  type: TYPE_NORMAL
- en: '`-rwxr-xr-x 1 root root 36784 Nov 10 23:26 /bin/cat`'
  prefs: []
  type: TYPE_NORMAL
- en: '`$`'
  prefs: []
  type: TYPE_NORMAL
- en: When we run `cat` it becomes a live runtime schedulable entity, which, in the
    Unix universe, we call a process.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to understand deeper concepts clearly, we start with a small, simple,
    and fictional machine. Imagine it has a microprocessor with 16 address lines.
    Thus, it''s easy to see, it will have access to a total potential memory space
    (or address space) of 2^(16) = 65,536 bytes = 64 KB:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a8c1b992-8b5e-4242-a6d8-849724f94961.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 4: Virtual memory of 64 KB'
  prefs: []
  type: TYPE_NORMAL
- en: But what if the physical memory (RAM) on the machine is a lot less, say, 32
    KB?
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, the preceding diagram depicts virtual memory, not physical.
  prefs: []
  type: TYPE_NORMAL
- en: 'Meanwhile, physical memory (RAM) looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/01122e43-c0aa-442b-861e-5f31ad13a0e0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 5: Physical memory of 32 KB'
  prefs: []
  type: TYPE_NORMAL
- en: 'Still, the promise made by the system to every process alive: every single
    process will have available to it the entire virtual address space, that is, 64
    KB. Sounds absurd, right? Yes, until one realizes that memory is more than just
    RAM; in fact, memory is viewed as a hierarchy – what''s commonly referred to as
    the memory pyramid:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ae13549-17b9-41bf-97c9-ec81c261fa88.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 6: The Memory pyramid'
  prefs: []
  type: TYPE_NORMAL
- en: 'As with life, everything''s a trade-off. Toward the apex of the pyramid, we
    gain in **Speed** at the cost of size; toward the bottom of the pyramid, it''s
    inverted: **Size** at the cost of speed. One could also consider CPU registers
    to be at the very apex of the pyramid; as its size is almost insignificant, it
    has not been shown.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Swap* is a filesystem type – a raw disk partition is formatted as swap upon
    system installation. It''s treated as second-level RAM by the OS. When the OS
    runs out of RAM, it uses swap. As a rough heuristic, system administrators sometimes
    configure the size of the swap partition to be twice that of available RAM.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To help quantify this, according to *Computer Architecture*, *A Quantitative
    Approach*, *5th Ed*, by Hennessy & Patterson, fairly typical numbers follow:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Type** | **CPU registers** | **CPU caches** | **RAM** | **Swap/storage**
    |'
  prefs: []
  type: TYPE_TB
- en: '| L1 | L2 | L3 |'
  prefs: []
  type: TYPE_TB
- en: '| Server | 1000 bytes | 64 KB | 256 KB | 2 - 4 MB | 4 - 16 GB | 4 - 16 TB |'
  prefs: []
  type: TYPE_TB
- en: '| 300 ps | 1 ns | 3 - 10 ns | 10 - 20 ns | 50 - 100 ns | 5 - 10 ms |'
  prefs: []
  type: TYPE_TB
- en: '| Embedded | 500 bytes | 64 KB | 256 KB | - | 256 - 512 MB | 4 - 8 GB Flash
    |'
  prefs: []
  type: TYPE_TB
- en: '| 500 ps | 2 ns | 10 - 20 ns | - | 50 - 100 ns | 25 - 50 us |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Memory hierarchy numbers'
  prefs: []
  type: TYPE_NORMAL
- en: 'Many (if not most) embedded Linux systems do not support a swap partition;
    the reason is straightforward: embedded systems mostly use flash memory as the
    secondary storage medium (not a traditional SCSI disk as do laptops, desktops,
    and servers). Writing to a flash chip wears it out (it has limited erase-write
    cycles); hence, embedded-system designers would rather sacrifice swap and just
    use RAM. (Please note that the embedded system can still be VM-based, which is
    the usual case with Linux and Win-CE, for example).'
  prefs: []
  type: TYPE_NORMAL
- en: The OS will do its best to keep the working set of pages as high up the pyramid
    as is possible, optimizing performance.
  prefs: []
  type: TYPE_NORMAL
- en: It's important for the reader to note that, in the sections that follow, while
    this book attempts to explain some of the inner workings of advanced topics such
    as VM and addressing (paging), we quite deliberately do not paint a complete,
    realistic, real-world view.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reason is straightforward: the deep and gory technical details are well
    beyond the scope of this book. So, the reader should keep in mind that several
    of the following areas are explained in concept and not in actuality. The *Further
    reading* section provides references for readers who are interested in going deeper
    into these matters. Refer it on the GitHub repository.'
  prefs: []
  type: TYPE_NORMAL
- en: Addressing 1 – the simplistic flawed approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Okay, now to the memory pyramid; even if we agree that virtual memory is now
    a possibility, a key and difficult hurdle to overcome remains. To explain this,
    note that every single process that is alive will occupy the entire available
    **virtual address space** (**VAS**). Thus, each process overlaps with every other
    process in terms of VAS. But how would this work? It wouldn''t, by itself. In
    order for this elaborate scheme to work, the system has to somehow map every virtual
    address in every process to a physical address! Refer to the following mapping
    of virtual address to physical address:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Process P:virtual address (va) → RAM:physical address (pa)**'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, the situation is something like this now:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/22cf8ed4-3660-4494-9bb3-cbb3b2281b94.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 7: Processes containing virtual addresses'
  prefs: []
  type: TYPE_NORMAL
- en: Processes **P1**, **P2**, and **Pn**, are alive and well in VM. Their virtual
    address spaces cover 0 to 64 KB and overlap each other. Physical memory, RAM,
    of 32 KB is present on this (fictional) system.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, two virtual addresses for each process are shown in the following
    format:'
  prefs: []
  type: TYPE_NORMAL
- en: '**`P''r'':va''n''`**; where `r` is the process number and `n` is 1 and 2.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned earlier, the key now is to map each process''s virtual addresses
    to physical addresses. So, we need to map the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We could have the OS perform this mapping; the OS would then maintain a mapping
    table per process to do so. Diagrammatically and conceptually it looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a7af066e-5f21-4d90-8aee-d155b5ed2b09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 8: Direct mapping virtual addresses to physical RAM addresses'
  prefs: []
  type: TYPE_NORMAL
- en: 'So that''s it, then? Seems quite simple, actually. Well, no, it won''t work
    in reality: to map all the possible virtual addresses per process to physical
    addresses in RAM, the OS would need to maintain a **va**-to-**pa** translation
    entry per address per process! That''s too expensive, as each table would possibly
    exceed the size of physical memory, rendering the scheme useless.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A quick calculation reveals that we have 64KB virtual memory, that is, 65,536
    bytes or addresses. Each of these virtual addresses need to be mapped to a physical
    address. So each process would require:'
  prefs: []
  type: TYPE_NORMAL
- en: 65536 * 2 = 131072 = 128 KB, for a mapping table. per process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It gets worse in reality; the OS would need to store some metadata along with
    each address-translation entry; let''s say 8 bytes of metadata. So now, each process
    would require:'
  prefs: []
  type: TYPE_NORMAL
- en: 65536 * 2 * 8 = 1048576 = 1 MB, for a mapping table. per process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wow, 1 megabyte of RAM per process! That's far too much (think of an embedded
    system); also, on our fictional system, there's a total of 32 KB of RAM. Whoops.
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, we can reduce this overhead by not mapping each byte but mapping each
    word; say, 4 bytes to a word. So now, each process would require:'
  prefs: []
  type: TYPE_NORMAL
- en: (65536 * 2 * 8) / 4 = 262144 = 256 KB, for a mapping table. per process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Better, but not good enough. If there are just 20 processes alive, we'd require
    5 MB of physical memory to store just the mapping metadata. With 32 KB of RAM,
    we can't do that.
  prefs: []
  type: TYPE_NORMAL
- en: Addressing 2 – paging in brief
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To address (pun intended) this tricky issue, computer scientists came up with
    a solution: do not attempt to map individual virtual bytes (or even words) to
    their physical counterpart; it''s far too expensive. Instead, carve up both physical
    and virtual memory space into blocks and map them.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A bit simplistically, there are broadly two ways to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: Hardware-segmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hardware-paging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hardware-segmentation:** Carves up the virtual and physical address space
    into arbitrary-sized chunks called **segments**. The best example is Intel 32-bit
    processors.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hardware-paging:** Carves up the virtual and physical address space into
    equal-sized chunks called **pages**. Most real-world processors support hardware-paging,
    including Intel, ARM, PPC, and MIPS.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Actually it''s not even up to the OS developer to select which scheme to use:
    the choice is dictated by the hardware MMU.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, we remind the reader: the intricate details are beyond the scope of
    this book. See the *Further reading* section on the GitHub repository.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's assume we go with the paging technique. The key takeaway is that we stop
    attempting to map all possible virtual addresses per process to physical addresses
    in RAM, instead, we map virtual pages (just called pages) to physical pages (called
    page frames).
  prefs: []
  type: TYPE_NORMAL
- en: Common Terminology
  prefs: []
  type: TYPE_NORMAL
- en: '**virtual address space** : **VAS**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Virtual page within the process VAS : page'
  prefs: []
  type: TYPE_NORMAL
- en: 'Physical page in RAM : **page frame** (**pf**)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Does NOT work: **virtual address** (**va**) → **physical address** (**pa**)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Does work: (virtual) page → page frame'
  prefs: []
  type: TYPE_NORMAL
- en: The left-to-right arrow represents the mapping.
  prefs: []
  type: TYPE_NORMAL
- en: As a rule of thumb (and the generally accepted norm), the size of a page is
    4 kilobytes (4,096 bytes). Again, it's the processor **Memory Management Unit**
    (**MMU**) that dictates the page size.
  prefs: []
  type: TYPE_NORMAL
- en: So how and why does this scheme help?
  prefs: []
  type: TYPE_NORMAL
- en: 'Think about it for a moment; in our fictional machine, we''ve got: 64 KB of
    VM, that is, 64K/4K =  16 pages, and 32 KB of RAM, that is, 32K/4K = 8 page frames.'
  prefs: []
  type: TYPE_NORMAL
- en: Mapping 16 pages to corresponding page frames requires a table of only 16 entries
    per process; this is viable!
  prefs: []
  type: TYPE_NORMAL
- en: 'As in our earlier calculations:'
  prefs: []
  type: TYPE_NORMAL
- en: 16 * 2 * 8 = 256 bytes, for a mapping table per process.
  prefs: []
  type: TYPE_NORMAL
- en: The very important thing, it bears repeating: we map (virtual) pages to (physical)
    page frames!
  prefs: []
  type: TYPE_NORMAL
- en: 'This is done by the OS on a per-process basis. Thus, each process has its own
    mapping table that translates pages to page frames at runtime; it''s commonly
    called the **Paging Table** (**PT**):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/051c43fb-a2e3-4f6b-8fa6-72d8dfe5f411.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 9: Mapping (virtual) pages to (physical) page frames'
  prefs: []
  type: TYPE_NORMAL
- en: Paging tables – simplified
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Again, in our fictional machine, we''ve got: 64 KB of VM, that is, 64K/4K =
    16 pages, and 32 KB of RAM, that is, 32K/4K = 8 page frames.'
  prefs: []
  type: TYPE_NORMAL
- en: Mapping the 16 (virtual) pages to corresponding (physical) page frames requires
    a table of only 16 entries per process, which makes the whole deal viable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Very simplistically, the OS-created PT of a single process look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **(Virtual) page** | **(Physical) page frame** |'
  prefs: []
  type: TYPE_TB
- en: '| `0` | `3` |'
  prefs: []
  type: TYPE_TB
- en: '| `1` | `2` |'
  prefs: []
  type: TYPE_TB
- en: '| `2` | `5` |'
  prefs: []
  type: TYPE_TB
- en: '| `[...]` | `[...]` |'
  prefs: []
  type: TYPE_TB
- en: '| `15` | `6` |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: OS-created PT'
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, the astute reader will notice that we have a problem: we''ve got
    16 pages and just eight page frames to map them into – what about the remaining
    eight pages?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, consider this:'
  prefs: []
  type: TYPE_NORMAL
- en: In reality, every process will not use every available page for code or data
    or whatever; several regions of the virtual address space will remain empty (sparse),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Even if we do require it, we have a way: don''t forget the memory pyramid.
    When we''re out of RAM, we use swap. So the (conceptual) PT for a process might
    appear like this (as an example, pages 13 and 14 are residing in swap):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| **(Virtual) page** | **(Physical) page frame** |'
  prefs: []
  type: TYPE_TB
- en: '| `0` | `3` |'
  prefs: []
  type: TYPE_TB
- en: '| `1` | `2` |'
  prefs: []
  type: TYPE_TB
- en: '| `2` | `5` |'
  prefs: []
  type: TYPE_TB
- en: '| `[...]` | `[...]` |'
  prefs: []
  type: TYPE_TB
- en: '| `13` | `<swap-address>` |'
  prefs: []
  type: TYPE_TB
- en: '| `14` | `<swap-address>` |'
  prefs: []
  type: TYPE_TB
- en: '| `15` | `6` |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Conceptual PT'
  prefs: []
  type: TYPE_NORMAL
- en: Again, please note that this description of PTs is purely conceptual; actual
    PTs are more complex and highly arch (CPU/MMU) dependent.
  prefs: []
  type: TYPE_NORMAL
- en: Indirection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By introducing paging, we have actually introduced a level of indirection:
    we no longer think of a (virtual) address as an absolute offset from zero, but
    rather as a relative quantity: `va = (page, offset)`.'
  prefs: []
  type: TYPE_NORMAL
- en: We think of each virtual address as associated with a page number and an offset
    from the beginning of that page. This is called using one level of indirection.
  prefs: []
  type: TYPE_NORMAL
- en: So each time a process refers to a virtual address (and of course, note that
    this is happening almost all of the time), the system must translate the virtual
    address to the corresponding physical address based on the PTs for that process.
  prefs: []
  type: TYPE_NORMAL
- en: Address-translation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So, at runtime, the process looks up a virtual address which is, say, 9,192
    bytes from 0, that is, its virtual address: **`va = 9192 = 0x000023E8`**. If each
    page is 4,096 bytes in size, this implies the va address is on the third page
    (page #2), at an offset of 1,000 bytes from the start of that page.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, with one level of indirection, we have: **`va = (page, offset) = (2, 1000)`**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Aha! Now we can see how address-translation works: the OS sees that the process
    wants an address in page 2\. It does a lookup on the PT for that process, and
    finds that page 2 maps to page frame 5\. To calculate the physical address shown
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Voila!
  prefs: []
  type: TYPE_NORMAL
- en: The system now places the physical address on the bus and the CPU performs its
    work as usual. It looks quite simple, but again, it's not realistic—please see
    the information box as follows as well.
  prefs: []
  type: TYPE_NORMAL
- en: Another advantage gained by the paging schema is the OS only needs to store
    a page-to-page-frame mapping. This automatically lets us translate any byte in
    the page to the corresponding physical byte in the page frame by just adding the
    offset, as there is a 1:1 mapping between a page and a page frame (both are of
    identical size).
  prefs: []
  type: TYPE_NORMAL
- en: In reality, it's not the OS that does the actual calculations to perform address-translation.
    This is because doing this in the software would be far too slow (remember, looking
    up virtual addresses is an ongoing activity happening almost all the time). The
    reality is that the address lookup and translation is done by silicon – the hardware **Memory
    Management Unit** (**MMU**) within the CPU!
  prefs: []
  type: TYPE_NORMAL
- en: 'Keep the following in mind:'
  prefs: []
  type: TYPE_NORMAL
- en: • The OS is responsible for creating and maintaining PTs for each process.
  prefs: []
  type: TYPE_NORMAL
- en: • The MMU is responsible for performing runtime address-translation (using the
    OS PTs).
  prefs: []
  type: TYPE_NORMAL
- en: • Beyond this, modern hardware supports hardware accelerators, such as the TLB,
    use of CPU caches, and virtualization extensions, which go a long way toward getting
    decent performance.
  prefs: []
  type: TYPE_NORMAL
- en: Benefits of using VM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At first glance, the sheer overhead introduced due to virtual memory and the
    associated address-translation would seem to warrant not using it. Yes, the overhead
    is high, but the reality is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Modern hardware-acceleration (via TLBs/CPU caches/prefetching) mitigates this
    overhead and provides decent enough performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The benefits one derives from VM outweigh the performance issues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On a VM-based system, we get the following benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: Process-isolation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The programmer need not worry about physical memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory-region protection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It's important to understand these a bit better.
  prefs: []
  type: TYPE_NORMAL
- en: Process-isolation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With virtual memory, every process runs inside a sandbox, which is the extent
    of its VAS. The key rule: it cannot look outside the box.'
  prefs: []
  type: TYPE_NORMAL
- en: So, think about it, it's impossible for a process to peek or poke the memory
    of any other process's VAS. This helps in making the system secure and stable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: we have two processes, A and B. Process A wants to write to the `0x10ea` virtual
    address in process B. It cannot, even if it attempts to write to that address,
    all it''s really doing is writing to its own virtual address, `0x10ea`! The same
    goes for reading.'
  prefs: []
  type: TYPE_NORMAL
- en: So we get process-isolation – each process is completely isolated from every
    other process.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual address X for process A is not the same as virtual address X for process
    B; in all likelihood, they translate to different physical addresses (via their
    PTs).
  prefs: []
  type: TYPE_NORMAL
- en: 'Exploiting this property, the Android system is designed to very deliberately
    use the process model for Android apps: when an Android app is launched, it becomes
    a Linux process, which lives within its own VAS, isolated and thus protected from
    other Android apps (processes)!'
  prefs: []
  type: TYPE_NORMAL
- en: Again, don't make the mistake of assuming that every single (virtual) page within
    a given process is valid for that process itself. A page is only valid if it's
    mapped, that is, it's been allocated and the OS has a valid translation for it
    (or a way to get to it). In fact, and especially true for the enormous 64-bit
    VAS, the process virtual address space is considered to be sparse, that is, scanty.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If process-isolation is as described, then what if process A needs to talk
    to process B? Indeed, this is a frequent design requirement for many, if not most,
    real Linux applications – we need some mechanism(s) to be able to read/write the
    VAS of another process. Modern OSes provide mechanisms to achieve this: **Inter-Process
    Communication** (**IPC**) mechanisms. (A little on IPC can be found in [Chapter
    15](5e7e9c60-48d8-41bd-adef-31bbfd598c78.xhtml), *Multithreading with Pthreads
    Part II - Synchronization.*)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The programmer need not worry about physical memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: On older OSes and even modern RTOSes, the programmer is expected to understand
    the memory layout of the entire system in detail and use memory accordingly (recall
    *Fig 1*). Obviously, this places a major burden on the developer; they have to
    ensure that they work well within the physical constraints of the system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most modern developers working on modern OSes never even think this way: if
    we want, say, 512 Kb of memory, do we not just allocate it dynamically (with `malloc(3)`,
    seen later in detail in [Chapter 4](0b4868f7-a8d0-4ced-831f-20af9929de9f.xhtml),
    *Dynamic Memory Allocation*), leaving the precise details of how and where it''s
    done to the library and OS layers? In fact, we can do this kind of thing dozens
    of times and not worry about stuff such as, "Will there be enough physical RAM?
    Which physical page frames should be used? What about fragmentation/wastage?"'
  prefs: []
  type: TYPE_NORMAL
- en: We get the added benefit that the memory returned to us by the system is guaranteed
    to be contiguous; of course, it's just virtually contiguous, it need not be physically
    contiguous, but that kind of detail is exactly what the VM layers take care of!
  prefs: []
  type: TYPE_NORMAL
- en: All is handled, really efficiently, by the library layer and the underlying
    memory-management system in the OS.
  prefs: []
  type: TYPE_NORMAL
- en: Memory-region protection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Perhaps the most important benefit of VM is this: the ability to define protections
    on virtual memory and have them honored by the OS.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Unix and friends (including Linux), allow four protection or permission values
    on memory pages:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Protection or permission type** | **Meaning** |'
  prefs: []
  type: TYPE_TB
- en: '| None | No permission to do anything on the page |'
  prefs: []
  type: TYPE_TB
- en: '| Read | Page can be read from |'
  prefs: []
  type: TYPE_TB
- en: '| Write | Page can be written to |'
  prefs: []
  type: TYPE_TB
- en: '| Execute | Page (code) can be executed |'
  prefs: []
  type: TYPE_TB
- en: Table 5:  Protection or permission values on memory pages
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider a small example: we allocate four pages of memory in our process
    (numbered 0 to 3). By default, the default permission or protections on the pages
    is **RW** (**Read-Write**), which means the pages can be both read from and written
    to.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With virtual memory OS-level support, the OS exposes APIs (the `mmap(2)` and
    `mprotect(2)` system calls) with which one can change the default page protections! Kindly
    take a look at the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Memory page #** | **Default protections** | **Changed protections** |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | RW- | -none- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | RW- | Read-only (R--) |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | RW- | Write-only (-W-) |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | RW- | Read-Execute (R-X) |'
  prefs: []
  type: TYPE_TB
- en: With powerful APIs such as this, we can set memory protections to the granularity
    of a single page!
  prefs: []
  type: TYPE_NORMAL
- en: 'Applications (and indeed the OS) can, and do, leverage these powerful mechanisms;
    in fact, that''s precisely what is done on particular regions of process address
    space by the OS (as we''ll learn in the next section, *SIDEBAR :: Testing the
    memcpy() ''C'' program*).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, fine, we can set certain protections on certain pages, but what if an
    application disobeys them? For example, after setting page #3 (as seen in the
    preceding table) to read-execute, what if the app (or OS) attempts to write to
    that page?'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is where the real power of virtual memory (and memory management) is seen:
    the reality is that on a VM-enabled system, the OS – more realistically, the MMU –
    is able to trap into every single memory access and determine whether the end
    user process is obeying the rules or not. If it is, the access proceeds successfully;
    if not, the MMU hardware raises an exception (similar, but not identical, to an
    interrupt). The OS now jumps into a code routine called the exception (or fault)
    handler. The OSes exception-handling routine determines whether the access is
    indeed illegal, and if so, the OS immediately kills the process attempting this
    illegal access.'
  prefs: []
  type: TYPE_NORMAL
- en: How's that for memory protection? In fact, this is pretty much exactly what
    a Segmentation Violation or segfault is; more on this in [Chapter 12](657b6be0-ebc8-40dd-81b6-4741b04602b1.xhtml),
    *Signaling - Part II*. The exception-handler routine is called the OSes fault-handler.
  prefs: []
  type: TYPE_NORMAL
- en: 'SIDEBAR :: Testing the memcpy() C program'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we better understand the what and why of a VM system, let''s go back
    to the buggy pseudocode example we considered at the beginning of this chapter:
    the case where we used `memcpy(3)` to copy some memory but specified the wrong
    destination address (and it would have overwritten the OS itself in our fictional
    physical-memory-only system).'
  prefs: []
  type: TYPE_NORMAL
- en: 'A conceptually similar C program, but which runs on Linux—a full-fledged virtual-memory-enabled
    OS—is shown and tried out here. Let''s see how the buggy program works on Linux:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The `malloc(3)` API will be covered in detail in the next chapter; for now,
    just understand that it is used to dynamically allocate 256 KB of memory to the
    process. Also, of course, `memcpy(3)` is used to copy memory from a source to
    a destination pointer, for n bytes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The interesting part is that we have a variable called `arbit_addr`; it's set
    to an arbitrary invalid (virtual) address. As you can see from the code, we set
    the destination pointer to `arbit_addr` when the user passes any argument to the
    program, making it the buggy test case. Let's try running the program for both
    the correct and buggy cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the correct case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: It runs fine, with no errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the buggy case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: It crashes! As described earlier, the buggy memcpy causes the MMU to fault;
    the OSes fault-handling code realizes that this is indeed a bug and it kills the
    offending process! The process dies because it's at fault, not the system. Not
    only is this correct, the segfault caused also alerts the developer to the fact
    that their code is buggy and must be fixed.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. What's a core dump anyway?
  prefs: []
  type: TYPE_NORMAL
- en: A core dump is a snapshot of certain dynamic regions (segments) of the process
    at the time it crashed (technically, it's a snapshot of minimally the data and
    stack segments). The core dump can be analyzed postmortem using debuggers such
    as GDB. We do not cover these areas in this book.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Hey, it says (core dumped) but I don't see any core file?
  prefs: []
  type: TYPE_NORMAL
- en: Well, there can be several reasons why the core file isn't present; the details
    lie beyond the scope of this book. Please refer to the man page on `core(5)` for
    details: [https://linux.die.net/man/5/core](https://linux.die.net/man/5/core).
  prefs: []
  type: TYPE_NORMAL
- en: 'Think about what has happened here in a bit more detail: the destination pointer''s
    value is `0xffffffffff601000;` on the x86_64 processor, this is actually a kernel
    virtual address. Now we, a user mode process, are trying to write some memory
    to this destination region, which is protected against access from userspace.
    Technically, it''s in the kernel virtual address space, which is not available
    to user mode processes (recall our discussion of *CPU privilege levels* in [Chapter
    1](c17af8c2-a426-4ab6-aabb-aa1374e56cc4.xhtml), *Linux System Architecture*).
    So when we – a user mode process – attempt to write to kernel virtual address
    space, the protection mechanism spins up and prevents us from doing this, killing
    us in the bargain.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Advanced: How does the system know that this region is protected and what kind
    of protection it has? These details are encoded into the **Paging Table Entry**
    (**PTEs**) for the process, and are checked by the MMU on every access!'
  prefs: []
  type: TYPE_NORMAL
- en: 'This kind of advanced memory protection would be impossible without support
    in both hardware and software:'
  prefs: []
  type: TYPE_NORMAL
- en: Hardware support via the MMU found in all modern microprocessors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Software support via the operating system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many more benefits that VM provides, including (but not limited to)
    making powerful technologies, such as demand paging, **copy-on-write** (**COW**)
    handling, defragmentation, memory overcommit, memory-compaction, **Kernel Samepage
    Merging** (**KSM**), and **Transcendent Memory** (**TM**), possible.  Within this
    book's scope, we will cover a couple of these at later points.
  prefs: []
  type: TYPE_NORMAL
- en: Process memory layout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A process is an instance of a program in execution. It is seen as a live, runtime
    schedulable entity by the OS. In other words, it's the process that runs when
    we launch a program.
  prefs: []
  type: TYPE_NORMAL
- en: The OS, or kernel, stores metadata about the process in a data structure in
    kernel memory; on Linux, this structure is often called the **process descriptor**—though
    the term *task structure* is a more accurate one. Process attributes are stored
    in the task structure; the process **PID** (**process identifier**) – a unique
    integer identifying the process, process credentials, open-file information, signaling
    information, and a whole lot more, reside here.
  prefs: []
  type: TYPE_NORMAL
- en: From the earlier discussion,* Virtual memory*, we understand that a process
    has, among many other attributes, a VAS*.* The VAS is the sum-total space potentially
    available to it. As in our earlier example, with a fictional computer with 16
    address lines, the VAS per process would be 2^16 = 64 KB.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s consider a more realistic system: a 32-bit CPU with 32 lines for
    addressing. Clearly, each process has a VAS of 2^32, a fairly large quantity of
    4 GB.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 GB in hexadecimal format is `0x100000000;` so the VAS spans from the low address
    of `0x0` to the high address of `4GB - 1 = 0xffff ffff.`
  prefs: []
  type: TYPE_NORMAL
- en: 'However, we have yet to learn more details (see the *Advanced: VM split*) regarding
    the exact usage of the high end of the VAS. Therefore, for the time being at least,
    let''s just refer to this as the high address and not put a particular numerical
    value to it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is its diagrammatic representation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3cfd1f33-c180-4c72-bfe4-3a6c9dbfc9f2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 10: Process virtual address space (VAS)'
  prefs: []
  type: TYPE_NORMAL
- en: So, the thing to understand for now is that on a 32-bit Linux, every process
    alive has this image:**0x0** to 0xffff ffff = 4 GB of virtual address space*.*
  prefs: []
  type: TYPE_NORMAL
- en: Segments or mappings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When a new process is created (details in [Chapter 10](607ad988-406d-4736-90a4-3a318672ab6e.xhtml),
    *Process Creation*), its VAS must be set up by the OS. All modern OSes divide
    up the process VAS into homogeneous regions called **segments** (don't confuse
    these segments with the hardware-segmentation approach mentioned in the, *Addressing
    2 – paging in brief *section).
  prefs: []
  type: TYPE_NORMAL
- en: 'A segment is a homogeneous or uniform region of the process VAS; it consists
    of virtual pages. The segment has attributes, such as start and end addresses,
    protections (RWX/none), and mapping types. The key point for now: all pages belonging
    to a segment share the same attributes.'
  prefs: []
  type: TYPE_NORMAL
- en: Technically, and more accurately from the OS viewpoint, the segment is called
    a **mapping**.
  prefs: []
  type: TYPE_NORMAL
- en: From now on, when we use the word segment, we also mean mapping and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: 'Briefly, from the lower to high end, every Linux process will have the following
    segments (or mappings):'
  prefs: []
  type: TYPE_NORMAL
- en: Text (code)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Library (or other)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/22ba5179-fea4-4a15-a907-e45cb70fa355.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 11: Overall view of the process VAS with segments'
  prefs: []
  type: TYPE_NORMAL
- en: Read on for more details about each of these segments.
  prefs: []
  type: TYPE_NORMAL
- en: Text segment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Text is code: the actual opcodes and operands that make up the machine instructions
    that are fed to the CPU to consume. Readers may recall the `objdump --source ./hello_dbg`
    we did in [Chapter 1](c17af8c2-a426-4ab6-aabb-aa1374e56cc4.xhtml), *Linux System
    Architecture*, showing C code translated into assembly and machine language. This
    machine code resides within the process VAS in a segment called **text**. For
    example, let''s say a program has 32 KB of text; when we run it, it becomes a
    process and the text segment takes 32 KB of virtual memory; that''s 32K/4K = 8
    (virtual) pages.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For optimization and protection, the OS marks, that is, protects, all these
    eight pages of text as **read-execute** (**r-x**). This makes sense: code will
    be read from memory and executed by the CPU, not written to it.'
  prefs: []
  type: TYPE_NORMAL
- en: The text segment on Linux is always toward the low end of the process VAS. Note
    that it will never start at the `0x0` address.
  prefs: []
  type: TYPE_NORMAL
- en: As a typical example, on the IA-32, the text segment usually starts at `0x0804
    8000`. This is very arch-specific though  and changes in the presence of Linux
    security mechanisms like **Address Space Layout Randomization** (**ASLR**).
  prefs: []
  type: TYPE_NORMAL
- en: Data segments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Immediately above the text segment is the data segment, which is the place where
    the process holds the program's global and static variables (data).
  prefs: []
  type: TYPE_NORMAL
- en: 'Actually, it''s not one mapping (segment); the data segment consists of three
    distinct mappings. In order from the low address, it consists of: the initialized
    data segment, the uninitialized data segment, and the heap segment.'
  prefs: []
  type: TYPE_NORMAL
- en: We understand that, in a C program, uninitialized global and static variables
    are automatically initialized to zero. What about initialized globals? The initialized
    data segment is the region of address space where explicitly initialized global
    and static variables are stored.
  prefs: []
  type: TYPE_NORMAL
- en: 'The uninitialized data segment is the region of address space where, of course,
    uninitialized globals and static variables reside. The key point: these are implicitly
    initialized to zero (they''re actually memset to zero). Also, older literature
    often refers to this region as the BSS. BSS is an old assembler directive – Block
    Started by Symbol – that can be ignored; today, the BSS region or segment is nothing
    but the uninitialized data segment of the process VAS.'
  prefs: []
  type: TYPE_NORMAL
- en: The heap should be a term familiar to most C programmers; it refers to the memory
    region reserved for dynamic memory allocations (and subsequent free's). Think
    of the heap as a free gift of memory pages made available to the process at startup.
  prefs: []
  type: TYPE_NORMAL
- en: 'A key point: the text, initialized data, and uninitialized data segments are
    fixed in size; the heap is a dynamic segment – it can grow or shrink in size at
    runtime. It''s important to note that the heap segment grows toward higher virtual
    addresses. Further details on the heap and its usage can be found in the next
    chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: Library segments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When linking a program, we have two broad choices:'
  prefs: []
  type: TYPE_NORMAL
- en: Static linking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dynamic linking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Static linking implies that any and all library text (code) and data is saved
    within the program's binary executable file (hence it's larger, and a bit faster
    to load up).
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic linking implies that any and all shared library text (code) and data
    is not saved within the program's binary executable file; instead, it is shared
    by all processes and mapped into the process VAS at runtime (hence the binary
    executable is a lot smaller, though it might take a bit longer to load up). Dynamic
    linking is always the default.
  prefs: []
  type: TYPE_NORMAL
- en: 'Think about the `Hello, world` C program. You invoked `printf(3)`, but did
    you write the code for it? No, of course not; we understand that it''s within
    glibc and will be linked into our process at runtime. That''s exactly what happens
    with dynamic linking: at process load time, all the library text and data segments
    that the program depends upon (uses) are *memory-mapped* (details in [Chapter
    18](cf0e96e2-0e5d-4fb2-abc2-742bc93b61d0.xhtml), *Advanced File I/O* )into the
    process VAS. Where? In the region between the top of the heap and the bottom of
    the stack: the library segments (refer to the preceding diagram).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another thing: other mappings (besides library text and data) may find their
    way into this region of address space. A typical case is explicit memory mappings
    made by the developer (using the `mmap(2)` system call), implicit mappings such
    as those made by IPC mechanisms, such as shared memory mappings, and the malloc
    routines (refer to [Chapter 4](0b4868f7-a8d0-4ced-831f-20af9929de9f.xhtml), *Dynamic
    Memory Allocation*).'
  prefs: []
  type: TYPE_NORMAL
- en: Stack segment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section explains the process stack: what, why, and how.'
  prefs: []
  type: TYPE_NORMAL
- en: What is stack memory?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You probably remember being taught that stack memory is just memory but with
    a special push/pop semantic; the memory you push last resides at the top of the
    stack, and if you perform a pop operation, that memory gets popped off – removed
    from – the stack.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pedagogical example of visualizing a stack of dinner plates is a good one:
    the plate you place last is at the top, and you take the top plate off to give
    it to your dinner guest (of course, you could insist that you give them the plate
    from the middle or bottom of the stack, but we think that the plate on the very
    top would be the easiest one to pop off).'
  prefs: []
  type: TYPE_NORMAL
- en: Some literature also refers to this push/pop behavior as **Last In First Out**
    (**LIFO**). Fair enough.
  prefs: []
  type: TYPE_NORMAL
- en: The high end of the process VAS is used for the stack segment (refer to *Fig
    11*). Okay, fine, but what exactly is it for? How does it help?
  prefs: []
  type: TYPE_NORMAL
- en: Why a process stack?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''re taught to write nice modular code: divide your work into subroutines,
    and implement them as small, easily readable, and maintainable C functions. That''s
    great.'
  prefs: []
  type: TYPE_NORMAL
- en: The CPU, though, does not really understand how to invoke a C function, how
    to pass parameters, store local variables, and return a result to the calling
    function. Our savior, the compiler, takes over, converting C code into an assembly
    language that is capable of making this whole function thing work.
  prefs: []
  type: TYPE_NORMAL
- en: The compiler generates assembly code to invoke a function, passes along parameters,
    allocates space for local variables, and finally, emits a return result back to
    the caller. To do this, it uses the stack! So, similar to the heap, the stack
    is also a dynamic segment.
  prefs: []
  type: TYPE_NORMAL
- en: Every time a function is called, memory is allocated in the stack region (or
    segment or mapping) to hold metadata that has the function call, parameter passing
    and the function return mechanism work. This metadata region for each function
    is called the stack frame*.*
  prefs: []
  type: TYPE_NORMAL
- en: The stack frame holds the metadata necessary to implement the function call-parameter
    use-return value mechanism. The exact layout of a stack frame is highly CPU (and
    compiler) dependent; it's one of the key areas addressed by the CPU ABI document.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the IA-32 processor, the stack frame layout essentially is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`[ <-- high address`'
  prefs: []
  type: TYPE_NORMAL
- en: '`  [ Function Parameters ... ]`'
  prefs: []
  type: TYPE_NORMAL
- en: '`  [ RET address ]`'
  prefs: []
  type: TYPE_NORMAL
- en: '`  [ Saved Frame Pointer ] (optional)`'
  prefs: []
  type: TYPE_NORMAL
- en: '`  [ Local Variables ... ]`'
  prefs: []
  type: TYPE_NORMAL
- en: '`]  <-- SP: lowest address  `'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider some pseudocode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The call graph is quite obvious:'
  prefs: []
  type: TYPE_NORMAL
- en: '`main --> foo --> bar --> jail`'
  prefs: []
  type: TYPE_NORMAL
- en: The arrow drawn like --> means calls; so, main calls foo, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The thing to understand: every function invocation is represented at runtime
    by a stack frame in the process''s stack.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If the processor is issued a push or pop instruction, it will go ahead and
    perform it. But, think about it, how does the CPU know where exactly – at which
    stack memory location or address – it should push or pop memory? The answer: we
    reserve a special CPU register, the **stack pointer** (usually abbreviated to **SP**),
    for precisely this purpose: the value in SP always points to the top of the stack.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The next key point: the stack segment grows toward lower virtual addresses. This
    is often referred to as stack-grows-down semantics. Also note that the direction
    of stack growth is a CPU-specific feature dictated by the ABI for that CPU; most
    modern CPUs (including Intel, ARM, PPC, Alpha, and Sun SPARC) follow the stack-grows-down semantic.'
  prefs: []
  type: TYPE_NORMAL
- en: The SP always points to the top of the stack; as we use a downward-growing stack,
    this is the lowest virtual address on the stack!
  prefs: []
  type: TYPE_NORMAL
- en: 'For clarity, let''s check out a diagram that visualizes the process stack just
    after the call to `main()` (`main()` is invoked by a `__libc_start_main()` glibc
    routine):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5a15d9e1-29d7-4985-bc59-b7c57dceefe5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Process stack after main() is called'
  prefs: []
  type: TYPE_NORMAL
- en: 'The process stack upon entry to the `jail()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b190513f-f14d-4254-bf61-10212c725059.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Process stack after jail() is called'
  prefs: []
  type: TYPE_NORMAL
- en: Peeking at the stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can take a peek into the process stack (technically, the stack of `main()`)
    in different ways. Here, we show two possibilities:'
  prefs: []
  type: TYPE_NORMAL
- en: Automatically via the `gstack(1)` utility
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manually with the GDB debugger
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peek at the usermode stack, first, via `gstack(1)`:'
  prefs: []
  type: TYPE_NORMAL
- en: WARNING! Ubuntu users, you might face an issue here. At the time of writing
    (Ubuntu 18.04), gstack does not seem to be available for Ubuntu (and its alternative, pstack,
    does not work well either!). Please use the second method (via GDB), as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a quick example, we look up the stack of `bash` (the parameter is the PID
    of the process):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The stack frame number appears on the left preceded by the `#` symbol; note
    that frame `#0` is the top of the stack, (the lowest frame). Read the stack in
    a bottom-up fashion, that is, from frame `#6` (the frame for the `main()` function)
    up to frame `#0` (the frame for the `waitpid()` function). Also note that, if
    the process is multithreaded, `gstack` will show the stack of *each* thread.
  prefs: []
  type: TYPE_NORMAL
- en: Peek at the Usermode Stack, next, via GDB.
  prefs: []
  type: TYPE_NORMAL
- en: The **G****NU Deb****ugger** (**GDB**) is a renowned, very powerful debug tool
    (if you don't already use it, we highly recommend you learn how to; check out
    the link in the *Further reading* section). Here, we'll use GDB to attach to a
    process and, once attached, peek at its process stack.
  prefs: []
  type: TYPE_NORMAL
- en: 'A small test C program, that makes several nested function calls, will serve
    as a good example. Essentially, the call graph will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `pause(2)` system call is a great example of a blocking call – it puts the
    calling process to sleep, waiting (or blocking) on an event; the event it's blocking
    upon here is the delivery of any signal to the process. (Patience; we'll learn
    more in [Chapter 11](99fafa09-8972-4d9f-b241-46caf9de98f3.xhtml), *Signaling -
    Part I*, and [Chapter 12](657b6be0-ebc8-40dd-81b6-4741b04602b1.xhtml), *Signaling
    - Part II*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the relevant code `(ch2/stacker.c)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Note that, for GDB to see the symbols (names of functions, variables, line numbers),
    one must compile the code with the `-g` switch (produces debug information).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we run the process in the background:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, open GDB; within GDB, attach to the process (the PID is displayed in
    the preceding code), and view its stack with the **backtrace** ( **bt**) command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: On Ubuntu, due to security, GDB will not allow one to attach to any process;
    one can overcome this by running GDB as root; then it works well.
  prefs: []
  type: TYPE_NORMAL
- en: 'How about looking up the same process via `gstack` (at the time of writing,
    Ubuntu users, you''re out of luck). Here it is on a Fedora 27 box:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Guess what? It turns out that `gstack` is really a wrapper shell script that
    invokes GDB in a non-interactive fashion and it issues the very same `backtrace` command
    we just used!
  prefs: []
  type: TYPE_NORMAL
- en: As a quick learning exercise, check out the `gstack` script.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced – the VM split
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What we have seen so far is actually not the complete picture; in reality, this
    address space needs to be shared between user and kernel space.
  prefs: []
  type: TYPE_NORMAL
- en: This section is considered advanced. We leave it to the reader to decide whether
    to dive into the details that follow. While they're very useful, especially from
    a debug viewpoint, it's not strictly required for following the rest of this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall what we mentioned in the *Library segments* section: if a `Hello, world`
    application is to work, it needs to have a mapping to the `printf(3)` glibc routine.
    This is achieved by having the dynamic or shared libraries memory-mapped into
    the process VAS at runtime (by the loader program).'
  prefs: []
  type: TYPE_NORMAL
- en: 'A similar argument could be made for any and every system call issued by the
    process: we understood from [Chapter 1](c17af8c2-a426-4ab6-aabb-aa1374e56cc4.xhtml), *Linux
    System Architecture*, that the system call code is actually within the kernel
    address space. Thus, if issuing a system call were to succeed, we would need to
    re-vector the CPU''s **Instruction Pointer** (**IP **or PC register) to the address
    of the system call code, which, of course, is within kernel address space. Now,
    if the process VAS consists of just text, data, library, and stack segments, as
    we have been so far suggesting, how would it work? Recall the fundamental rule
    of virtual memory: you cannot look outside the box (available address space).'
  prefs: []
  type: TYPE_NORMAL
- en: In order for this whole scheme to succeed, therefore, even kernel virtual address
    space—yes, please note, even the kernel address space is considered virtual – must
    somehow be mapped into the process VAS.
  prefs: []
  type: TYPE_NORMAL
- en: As we saw earlier, on a 32-bit system, the total VAS available to a process
    is 4 GB. So far, the implicit assumption is that the top of the process VAS on
    32-bit is therefore 4 GB. That's right. As well, again, the implicit assumption
    is that the stack segment (consisting of stack frames) lies here—at the 4 GB point
    at the top. Well, that's incorrect (please refer to *Fig 11*).
  prefs: []
  type: TYPE_NORMAL
- en: 'The reality is this: the OS creates the process VAS, and arranges for the segments
    within it; however, it reserves some amount of virtual memory at the top end for
    the kernel or OS-mapping (meaning, the kernel code, data structures, stacks, and
    drivers). By the way, this segment, which contains kernel code and data, is usually
    referred to as the kernel segment.'
  prefs: []
  type: TYPE_NORMAL
- en: How much VM is kept for the kernel segment? Ah, that's a tunable or a configurable
    that is set by kernel developers (or the system administrator) at kernel-configuration
    time; it's called **VMSPLIT**. This is the point in the VAS where we split the
    address space between the OS kernel and user mode memory – the text, data, library,
    and stack segments!
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, for clarity, let''s reproduce Fig 11 (as Fig 14), but this time, explicitly
    reveal the VM Split:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/59969b43-5ec2-487f-95a1-407310bffe05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: The process VM Split'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s not get into the gory details here: suffice it to say that on an IA-32
    (Intel x86 32-bit), the splitting point is typically the 3 GB point. So, we have
    a ratio: *userspace VAS : kernel VAS  ::  3 GB : 1 GB    ; on the IA-32*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember, this is tunable. On other systems, such as a typical ARM-32 platform,
    the split might be like this instead:* userspace VAS : kernel VAS  ::  2 GB :
    2 GB   ; on the ARM-32*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'On an x86_64 with a gargantuan `2^64` VAS (that''s a mind-boggling 16 Exabytes!),
    it would be: *userspace VAS : kernel VAS  ::  128 TB : 128 TB   ; on the x86_64*.'
  prefs: []
  type: TYPE_NORMAL
- en: Now one can clearly see why we use the term monolithic to describe the Linux
    OS architecture – each process is indeed like a single, large piece of stone!
  prefs: []
  type: TYPE_NORMAL
- en: 'Each process contains both of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Userspace mappings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text (code)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Initialized data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uninitialized data (BSS)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Heap
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Library mappings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other mappings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kernel segments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every process alive maps into the kernel VAS (or kernel segment, as it's usually
    called), in its top end.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a crucial point. Let''s look at a real-world case: on the Intel IA-32
    running the Linux OS, the default value of `VMSPLIT` is 3 GB (which is `0xc0000000`).
    Thus, on this processor, the VM layout for each process is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**0x0** to **0xbfffffff** : userspace mappings, that is, text, data, library
    and stack.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**0xc0000000** to **0xffffffff** : kernel space or the kernel segment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is made clear in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fc38a5e3-3fce-441a-8903-4c0dcd6f0bbb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 15: Full process VAS on the IA-32'
  prefs: []
  type: TYPE_NORMAL
- en: Notice how the top gigabyte of VAS for every process is the same – the kernel
    segment. Also keep in mind that this layout is not the same on all systems – the
    VMSPLIT and the size of user and kernel segments varies with the CPU architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Since Linux 3.3 and especially 3.10 (kernel versions, of course), Linux supports
    the `prctl(2)` system call. Looking up its man page reveals all kinds of interesting,
    though non-portable (Linux-only), things one could do. For example, `prctl(2)`,
    used with the `PR_SET_MM` parameter, lets a process (with root privileges) essentially
    specify its VAS layout, its segments, in terms of start and end virtual addresses
    for text, data, heap, and stack. This is certainly not required for normal applications.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter delved into an explanation of VM concepts, why VM matters, and
    its many benefits to modern operating systems and the applications running on
    them. We then covered the layout of the process virtual address space on the Linux
    OS, including some information on the text, (multiple) data, and stack segments.
    The true reasons for the stack, and its layout, were covered as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, the reader will learn about per-process resource limits:
    why they are required, how they work, and of course, the programmer interfaces
    required to work with them.'
  prefs: []
  type: TYPE_NORMAL
