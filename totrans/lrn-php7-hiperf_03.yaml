- en: Chapter 3. Improving PHP 7 Application Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'PHP 7 has been completely rewritten from the ground up based on the **PHP Next
    Generation** (**phpng** or **PHPNG**) targeting performance. However, there are
    always more ways to improve the performance of the application, including writing
    high performance code, using best practices, web server optimizations, caching,
    and so on. In this chapter, we will discuss such optimizations listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: NGINX and Apache
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HTTP server optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Content Delivery Network (CDN)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JavaScript/CSS optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Full page caching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Varnish
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The infrastructure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NGINX and Apache
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are too many HTTP server software available, and each one has its pros
    and cons. The two most popular HTTP servers used are NGINX and Apache. Let's have
    a look at both of them and note which one is better for our needs.
  prefs: []
  type: TYPE_NORMAL
- en: Apache
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Apache is the most widely used HTTP server and is loved by most administrators.
    It is selected by administrators because of its flexibility, widespread support,
    power, and modules for most of the interpreted languages, such as PHP. As Apache
    can process a vast number of interpreted languages, it does not need to communicate
    with other software to fulfill the request. Apache can process requests in prefork
    (the processes are spawned across thread), worker (threads are spawned across
    processes), and event-driven (same as worker process, but it sets dedicated threads
    for *keep-alive* connections and separate threads for active connections); thus,
    it provides much more flexibility.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed earlier, each request will be processed by a single thread or process,
    so Apache consumes too many resources. When it comes to high-traffic applications,
    Apache may slow down the application as it does not provide good support for concurrent
    processing.
  prefs: []
  type: TYPE_NORMAL
- en: NGINX
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: NGINX was built to solve the concurrency problems with high-traffic applications.
    NGINX provides asynchronous, event-driven, and nonblocking request handling. As
    requests are processed asynchronously, NGINX does not wait for a request to be
    completed to block the resource.
  prefs: []
  type: TYPE_NORMAL
- en: NGINX creates worker processes, and each individual worker process can handle
    thousands of connections. So, a few processes can handle high traffic at once.
  prefs: []
  type: TYPE_NORMAL
- en: NGINX does not provide any built-in support for any interpreted languages. It
    relies on external resources for this. This is also good because the processing
    is made outside NGINX, and NGINX only processes the connections and requests.
    Mostly, NGINX is considered faster than Apache. In some situations, such as with
    static content (serving images, `.css` and `.js` files, and so on), this can be
    true, but in current high performance servers, Apache is not the problem; PHP
    is the bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Both Apache and NGINX are available for all kinds of operations systems. For
    the purpose of this book, we will use Debian and Ubuntu, so all file paths will
    be mentioned according to these OSes
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned before, we will use NGINX for this book.
  prefs: []
  type: TYPE_NORMAL
- en: HTTP server optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Each HTTP server provides certain features that can be used to optimize request
    handling and serving content. In this section, we will share some techniques for
    both Apache and NGINX that can be used to optimize the web server and provide
    the best performance and scalability. Mostly, when these optimizations are applied,
    a restart for Apache or NGINX is required.
  prefs: []
  type: TYPE_NORMAL
- en: Caching static files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Mostly, static files, such as images, `.css`, `.js`, and fonts don't change
    frequently. So, it is best practice to cache these static files on the end user
    machine. For this purpose, the web server adds special headers to the response,
    which tells the user browser to cache the static content for a certain amount
    of time. The following is the configuration code for both Apache and NGINX.
  prefs: []
  type: TYPE_NORMAL
- en: Apache
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s have a look at the Apache configuration to cache the following static
    content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code that has to be placed in a `.htaccess` file, we used the
    Apache `FilesMatch` directive to match the extensions of files. If a desired extension
    file is requested, Apache sets the headers to cache control for seven days. The
    browser then caches these static files for seven days.
  prefs: []
  type: TYPE_NORMAL
- en: NGINX
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following configuration can be placed in `/etc/nginx/sites-available/your-virtual-host-conf-file`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we used the NGINX `Location` block with a case-insensitive
    modifier (`~*`) to set `Expires` for seven days. This code will set the cache-control
    header for seven days for all the defined file types.
  prefs: []
  type: TYPE_NORMAL
- en: 'After making these settings, the response headers for a request will be as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![NGINX](graphics/B05225_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding figure, it can be clearly seen that the `.js` file is loaded
    from cache. Its cache-control header is set to seven days or 604,800 seconds.
    The expiry date can also be noted clearly in the `expires` headers. After the
    expiry date, the browser will load this `.js` file from the server and cache it
    again for the duration defined in the cache-control headers.
  prefs: []
  type: TYPE_NORMAL
- en: HTTP persistent connection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In HTTP persistent connection, or HTTP keep-alive, a single TCP/IP connection
    is used for multiple requests or responses. It has a huge performance improvement
    over the normal connection as it uses only a single connection instead of opening
    and closing connections for each and every single request or response. Some of
    the benefits of the HTTP keep-alive are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The load on the CPU and memory is reduced because fewer TCP connections are
    opened at a time, and no new connections are opened for subsequent requests and
    responses as these TCP connections are used for them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduces latency in subsequent requests after the TCP connection is established.
    When a TCP connection is to be established, a three-way handshake communication
    is made between a user and the HTTP server. After successfully handshaking, a
    TCP connection is established. In case of keep-alive, the handshaking is performed
    only once for the initial request to establish a TCP connection, and no handshaking
    or TCP connection opening/closing is performed for the subsequent requests. This
    improves the performance of the requests/responses.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network congestion is reduced because only a few TCP connections are opened
    to the server at a time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Besides these benefits, there are some side effects of keep-alive. Every server
    has a concurrency limit, and when this concurrency limit is reached or consumed,
    there can be a huge degradation in the application's performance. To overcome
    this issue, a time-out is defined for each connection, after which the HTTP keep-alive
    connection is closed automatically. Now, let's enable HTTP keep-alive on both
    Apache and NGINX.
  prefs: []
  type: TYPE_NORMAL
- en: Apache
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Apache, keep-alive can be enabled in two ways. You can enable it either in
    the `.htaccess` file or in the Apache config file.
  prefs: []
  type: TYPE_NORMAL
- en: 'To enable it in the `.htaccess` file, place the following configuration in
    the `.htaccess` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding configuration, we set the Connection header to keep-alive in
    the `.htaccess` file. As the `.htaccess` configuration overrides the configuration
    in the config files, this will override whatever configuration is made for keep-alive
    in the Apache config file.
  prefs: []
  type: TYPE_NORMAL
- en: 'To enable the keep-alive connection in the Apache config file, we have to modify
    three configuration options. Search for the following configuration and set the
    values to the ones in the example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding configuration, we turned on the keep-alive configuration by
    setting the value of `KeepAlive` to `On`.
  prefs: []
  type: TYPE_NORMAL
- en: The next is `MaxKeepAliveRequests`, which defines the maximum number of keep-alive
    connections to the web server at the time. A value of 100 is the default in Apache,
    and it can be changed according to the requirements. For high performance, this
    value should be kept high. If set to 0, it will allow unlimited keep-alive connections,
    which is not recommended.
  prefs: []
  type: TYPE_NORMAL
- en: The last configuration is `KeepAliveTimeout`, which is set to 100 seconds. This
    defines the number of seconds to wait for the next request from the same client
    on the same TCP connection. If no request is made, then the connection is closed.
  prefs: []
  type: TYPE_NORMAL
- en: NGINX
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'HTTP keep-alive is part of the `http_core` module and is enabled by default.
    In the NGINX configuration file, we can edit a few options, such as timeout. Open
    the `nginx` config file, edit the following configuration options, and set its
    values to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `keepalive_requests` config defines the maximum number of requests a single
    client can make on a single HTTP keep-alive connection.
  prefs: []
  type: TYPE_NORMAL
- en: The `keepalive_timeout` config is the number of seconds that the server needs
    to wait for the next request until it closes the keep-alive connection.
  prefs: []
  type: TYPE_NORMAL
- en: GZIP compression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Content compression provides a way to reduce the contents' size delivered by
    the HTTP server. Both Apache and NGINX provide support for GZIP compression, and
    similarly, most modern browsers support GZIP. When the GZIP compression is enabled,
    the HTTP server sends compressed HTML, CSS, JavaScript, and images that are small
    in size. This way, the contents are loaded fast.
  prefs: []
  type: TYPE_NORMAL
- en: A web server only compresses content via GZIP when the browser sends information
    about itself that it supports GZIP compression. Usually, a browser sends such
    information in *Request* headers.
  prefs: []
  type: TYPE_NORMAL
- en: The following are codes for both Apache and NGINX to enable GZIP compression.
  prefs: []
  type: TYPE_NORMAL
- en: Apache
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following code can be placed in the `.htaccess` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we used the Apache `deflate` module to enable compression.
    We used filter by type to compress only certain types of files, such as `.html`,
    plain text, `.xml`, `.css`, and `.js`. Also, before ending the module, we set
    a case to not compress the images because compressing images can cause image quality
    degradation.
  prefs: []
  type: TYPE_NORMAL
- en: NGINX
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As mentioned previously, you have to place the following code in your virtual
    host conf file for NGINX:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, GZIP compression is activated by the `gzip on;` line.
    The `gzip_vary on;` line is used to enable varying headers. The `gzip_types` line
    is used to define the types of files to be compressed. Any file types can be added
    depending on the requirements. The `gzip_com_level 4;` line is used to set the
    compression level, but be careful with this value; you don't want to set it too
    high. Its range is from 1 to 9, so keep it in the middle.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s check whether the compression really works. In the following screenshot,
    the request is sent to a server that does not have GZIP compression enabled. The
    size of the final HTML page downloaded or transferred is 59 KB:'
  prefs: []
  type: TYPE_NORMAL
- en: '![NGINX](graphics/B05225_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'After enabling GZIP compression on the web server, the size of the transferred
    HTML page is reduced up to 9.95 KB, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![NGINX](graphics/B05225_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Also, it can be noted that the time to load the contents is also reduced. So,
    the smaller the size of your contents, the faster the page will load.
  prefs: []
  type: TYPE_NORMAL
- en: Using PHP as a separate service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Apache uses the `mod_php` module for PHP. This way, the PHP interpreter is integrated
    to Apache, and all processing is done by this Apache module, which eats up more
    server hardware resources. It is possible to use PHP-FPM with Apache, which uses
    the FastCGI protocol and runs in a separate process. This enables Apache to worry
    about HTTP request handlings, and the PHP processing is made by the PHP-FPM.
  prefs: []
  type: TYPE_NORMAL
- en: NGINX, on the other hand, does not provide any built-in support or any support
    by module for PHP processing. So, with NGINX, PHP is always used in a separate
    service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s take a look at what happens when PHP runs as a separate service:
    the web server does not know how to process the dynamic content request and forwards
    the request to another external service, which reduces the processing load on
    the web server.'
  prefs: []
  type: TYPE_NORMAL
- en: Disabling unused modules
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Both Apache and NGINX come with lots of modules built into them. In most cases,
    you won't need some of these modules. It is good practice to disable these modules.
  prefs: []
  type: TYPE_NORMAL
- en: It is good practice to make a list of the modules that are enabled, disable
    those modules one by one, and restart the server. After this, check whether your
    application is working or not. If it works, go ahead; otherwise, enable the module(s)
    after which the application stopped working properly again.
  prefs: []
  type: TYPE_NORMAL
- en: This is because you may see that a certain module may not be required, but some
    other useful module depends on this module. So, it's best practice it to make
    a list and enable or disable the modules, as stated before.
  prefs: []
  type: TYPE_NORMAL
- en: Apache
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To list all the modules that are loaded for Apache, issue the following command
    in the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This command will list all the loaded modules, as can be seen in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Apache](graphics/B05225_03_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now, analyze all the loaded modules, check whether they are needed for the application,
    and disable them, as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open up the Apache config file and find the section where all the modules are
    loaded. A sample is included here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The modules that have a `#` sign in front of them are not loaded. So, to disable
    a module in the complete list, just place a `#` sign. The `#` sign will comment
    out the line, and the module won't be loaded anymore.
  prefs: []
  type: TYPE_NORMAL
- en: NGINX
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To check which modules NGINX is compiled with, issue the following command
    in the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This will list complete information about the NGINX installation, including
    the version and modules with which NGINX is compiled. Have a look at the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![NGINX](graphics/B05225_03_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Normally, NGINX enables only those modules that are required for NGINX to work.
    To enable any other module that is compiled with NGINX installed, we can place
    a little configuration for it in the `nginx.conf` file, but there is no single
    way to disable any NGINX module. So, it is good to search for this specific module
    and take a look at the module page on the NGINX website. There, we can find information
    about this specific module, and if available, we can find information about how
    to disable and configure this module.
  prefs: []
  type: TYPE_NORMAL
- en: Web server resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Each web server comes with its own optimum settings for general use. However,
    these settings may be not optimum for your current server hardware. The biggest
    problem on the web server hardware is the RAM. The more RAM the server has, the
    more the web server will be able to handle requests.
  prefs: []
  type: TYPE_NORMAL
- en: NGINX
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: NGINX provides two variables to adjust the resources, which are `worker_processes`
    and `worker_connections`. The `worker_processes` settings decide how many NGINX
    processes should run.
  prefs: []
  type: TYPE_NORMAL
- en: Now, how many `worker_processes` resources should we use? This depends on the
    server. Usually, it is one worker processes per processor core. So, if your server
    processor has four cores, this value can be set to 4.
  prefs: []
  type: TYPE_NORMAL
- en: 'The value of `worker_connections` shows the number of connections per `worker_processes`
    setting per second. Simply speaking, `worker_connections` tells NGINX how many
    simultaneous requests can be handled by NGINX. The value of `worker_connections`
    depends on the system processor core. To find out the core''s limitations on a
    Linux system (Debian/Ubuntu), issue the following command in the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This command will show you a number that should be used for `worker_connections`.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's say that our processor has four cores, and each core's limitation
    is 512\. Then, we can set the values for these two variables in the NGINX main
    configuration file. On Debian/Ubuntu, it is located at `/etc/nginx/nginx.conf`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, find out these two variables and set them as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The preceding values can be high, specially `worker_connections`, because server
    processor cores have high limitations.
  prefs: []
  type: TYPE_NORMAL
- en: Content Delivery Network (CDN)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Content Delivery Network is used to host static media files, such as images,
    `.css` and `.js` files, and audio and video files. These files are stored on a
    geographical network whose servers are located in different locations. Then, these
    files are served to requests from a specific server, depending on the request
    location.
  prefs: []
  type: TYPE_NORMAL
- en: 'CDN provides the following features:'
  prefs: []
  type: TYPE_NORMAL
- en: As the contents are static, which don't change frequently, CDN caches them in
    memory. When a request comes for a certain file, CDN sends the file directly from
    cache, which is faster than loading the file from disk and sending it to the browser.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CDN servers are located in different locations. All the files are stored in
    each location, depending on your settings in CDN. When a browser request arrives
    to CDN, CDN sends the requested contents from the nearest location available to
    the requested location. For example, if the CDN has servers in London, New York,
    and Dubai and a request comes from Middle East, the CDN will send content from
    the Dubai server. This way, as a CDN delivers the contents from the nearest location,
    the response time is reduced.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each browser has limitations for sending simultaneous requests to a domain.
    Mostly, it's three requests. When a response arrives for a request, the browser
    sends more requests to the same domain, which causes a delay in complete page
    loading. CDN provides subdomains (either their own subdomains or your main domain's
    subdomains, using your main domain's DNS settings), which enables browsers to
    send more parallel requests for the same contents loading from different domains.
    This enables the browser to load the page content fast.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generally, there is a small amount of requests for dynamic content and more
    requests for static content. If your application's static content is hosted on
    a separate CDN server, this will reduce the load on your server tremendously.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using CDN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So, how do you use CDN in your application? In best practice, if your application
    has high traffic, creating different subdomains at your CDN for each content type
    is the best. For example, a separate domain for CSS and JavaScript files, a subdomain
    for images, and another separate subdomain for audio/videos files can be created.
    This way, the browser will send parallel requests for each content type. Let''s
    say, we have the following URLs for each content type:'
  prefs: []
  type: TYPE_NORMAL
- en: '**For CSS and JavaScript**: `http://css-js.yourcdn.com`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**For images**: `http://images.yourcdn.com`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**For other** **media**: `http://media.yourcdn.com`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, most open source applications provide settings at their admin control panel
    to set up CDN URLs, but in case you happened to use an open source framework or
    a custom-build application, you can define your own setting for CDN by placing
    the previous URLs either in the database or in a configuration file loaded globally.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our example, we will place the preceding URLs in a config file and create
    three constants for them, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'If we need to load a CSS file, it can be loaded as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'For a JavaScript file, it can be loaded as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'If we load images, we can use the previous way in the `src` attribute of the
    `img` tag, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding examples, if we don't need to use CDN or want to change the
    CDN URLs, it will be easy to change in just one place.
  prefs: []
  type: TYPE_NORMAL
- en: Most famous JavaScript libraries and templating engines host their static resources
    on their own personal CDN. Google hosts query libraries, fonts, and other JavaScript
    libraries on its own CDN, which can be used directly in applications.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, we may not want to use CDN or be able to afford them. For this, we
    can use a technique called domain sharing. Using domain sharding, we can create
    subdomains or point out other domains to our resources' directories on the same
    server and application. The technique is the same as discussed earlier; the only
    difference is that we direct other domains or subdomains to our media, CSS, JavaScript,
    and image directories ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: This may seem be fine, but it won't provide us with CDN's best performance.
    This is because CDN decides the geographical availability of content depending
    on the customer's location, extensive caching, and files optimization on the fly.
  prefs: []
  type: TYPE_NORMAL
- en: CSS and JavaScript optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Every web application has CSS and JavaScript files. Nowadays, it is common that
    most applications have lots of CSS and JavaScript files to make the application
    attractive and interactive. Each CSS and JavaScript file needs a browser to send
    a request to the server to fetch the file. So, the more the CSS and JavaScript
    files you have, the more requests the browser will need to send, thus affecting
    its performance.
  prefs: []
  type: TYPE_NORMAL
- en: Each file has a content size, and it takes time for the browser to download
    it. For example, if we have 10 CSS files of 10 KB each and 10 JavaScript files
    of 50 KB each, the total content size of the CSS files is 100 KB, and for JavaScript
    it is 500 KB—600 KB for both types of files. This is too much, and the browser
    will take time to download them.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Performance plays a vital role in web applications. Even Google counts performance
    in its indexing. Don't think of a file that has a few KBs and takes a 1 ms to
    download because when it comes to performance, each millisecond is counted. The
    best thing is to optimize, compress, and cache everything.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will discuss two ways to optimize our CSS and JS, which
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Merging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Minifying
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Merging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the merging process, we can merge all the CSS files into a single file, and
    the same process is carried out with JavaScript files, thus creating a single
    file for CSS and JavaScript. If we have 10 files for CSS, the browser sends 10
    requests for all these files. However, if we merge them in a single file, the
    browser will send only one request, and thus, the time taken for nine requests
    is saved.
  prefs: []
  type: TYPE_NORMAL
- en: Minifying
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the minifying process, all the empty lines, comments, and extra spaces are
    removed from the CSS and JavaScript files. This way, the size of the file is reduced,
    and the file loads fast.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s say you have the following CSS code in a file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'After minifying the file, we will have CSS code similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly for JavaScript, let''s consider that we have the following code in
    a JavaScript file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, if the preceding file is minified, we will have the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: It can be noted in the preceding examples that all the unnecessary white spaces
    and new lines are removed. Also, it places the complete file code in one single
    line. All code comments are removed. This way, the file size is reduced, which
    helps the file be loaded fast. Also, this file will consume less bandwidth, which
    is useful if the server resources are limited.
  prefs: []
  type: TYPE_NORMAL
- en: Most open source applications, such as Magento, Drupal, and WordPress, provide
    either built-in support or support the application by third-party plugins/modules.
    Here, we won't cover how to merge CSS or JavaScript files in these applications,
    but we will discuss a few tools that can merge CSS and JavaScript files.
  prefs: []
  type: TYPE_NORMAL
- en: Minify
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Minify is a set of libraries completely written in PHP. Minify supports both
    merging and minifying for both CSS and JavaScript files. Its code is completely
    object-oriented and namespaced, so it can be embedded into any current or proprietary
    framework.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Minify homepage is located at [http://minifier.org](http://minifier.org).
    It is also hosted on GitHub at [https://github.com/matthiasmullie/minify](https://github.com/matthiasmullie/minify).
    It is important to note that the Minify library uses a path converter library,
    which is written by the same author. The path converter library can be downloaded
    from [https://github.com/matthiasmullie/path-converter](https://github.com/matthiasmullie/path-converter).
    Download this library and place it in the same folder as the minify libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s create a small project that we will use to minify and merge CSS
    and JavaScript files. The folder structure of the project will be as in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Minify](graphics/B05225_03_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding screenshot, the complete project structure is shown. The project
    name is `minify`. The `css` folder has all of our CSS files, including the minified
    or merged ones. Similarly, the `js` folder has all our JavaScript files, including
    the minified or merged ones. The `libs` folder has the `Minify` library along
    with the `Converter` library. `Index.php` has our main code to minify and merge
    CSS and JavaScript files.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `data` folder in the project tree is related to JavaScript minification.
    As JavaScript has keywords that require a space before and after them, these `.txt`
    files are used to identify these operators.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s start by minifying our CSS and JavaScript files using the following
    code in `index.php`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code is simple. First, we included all our required libraries.
    Then, in the `Minify CSS` block, we created two path variables: `$cssSourcePath`,
    which has the path to the CSS file that we need to minify, and `$cssOutputPath`,
    which has path to the minified CSS file that will be generated.'
  prefs: []
  type: TYPE_NORMAL
- en: After this, we instantiated an object of the `CSS.php` class and passed the
    CSS file that we need to minify. Finally, we called the minify method of the `CSS`
    class and passed the output path along with the filename, which will generate
    the required file for us.
  prefs: []
  type: TYPE_NORMAL
- en: The same explanation goes for the JS minifying process.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we run the preceding PHP code, all the files are in place, and everything
    goes fine, then two new filenames will be created: `styles.min.css` and `app.min.js`.
    These are the new minified versions of their original files.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s use Minify to merge multiple CSS and JavaScript files. First, add
    some CSS and JavaScript files to the respective folders in the project. After
    this, we just need to add a little code to the current code. In the following
    code, I will skip including all the libraries, but these files have to be loaded
    whenever you need to use Minify:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Now, take a look at the highlighted code. In the CSS part, we saved the minified
    and merged file as `style.min.merged.css`, but naming is not important; it is
    all up to our own choice.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will simply use the add method of the `$cssMinifier` and `$jsMinifier`
    objects to add new files and then call `minify`. This causes all the additional
    files to be merged in the initial file and then minified, thus generating a single
    merged and minified file.
  prefs: []
  type: TYPE_NORMAL
- en: Grunt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: According to its official website, Grunt is a JavaScript task runner. It automates
    certain repetitive tasks so that you don't have to work repeatedly. It is an awesome
    tool and is widely used among web programmers.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Grunt is very easy. Here, we will install it on MAC OS X, and the
    same method is used for most Linux systems, such as Debian and Ubuntu.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Grunt requires Node.js and npm. Installing and configuring Node.js and npm is
    out of the scope of this book, so for this book, we will assume that these tools
    are installed on your machine or that you can search for them and figure out how
    to install them.
  prefs: []
  type: TYPE_NORMAL
- en: 'If Node.js and npm are installed on your machine, just fire up the following
    command in your terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This will install Grunt CLI. If everything goes fine, then the following command
    will show you the version the of Grunt CLI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The output of the preceding command is `grunt-cli v0.1.13;` as of writing this
    book, this version is available.
  prefs: []
  type: TYPE_NORMAL
- en: Grunt provides you with a command-line, which enables you to run a Grunt command.
    A Grunt project requires two files in your project file tree. One is `package.json`,
    which is used by `npm` and lists Grunt and the Grunt plugins that the project
    needs as DevDependencies.
  prefs: []
  type: TYPE_NORMAL
- en: The second file is the `GruntFile`, which is stored as `GruntFile.js` or `GruntFile.coffee`
    and is used to configure and define Grunt tasks and load Grunt plugins.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will use the same preceding project, but our folder structure will
    be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Grunt](graphics/B05225_03_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, open the terminal in your project root and issue the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This will generate the `package.json` file by asking a few questions. Now,
    open the `package.json` file and modify it so that the contents of the final `package.json`
    files look similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: I added comments to different parts of the `package.json` file so that it is
    easy to understand. Note that for the final file, we will remove the comments
    from this file.
  prefs: []
  type: TYPE_NORMAL
- en: It can be seen that in the `DevDependencies` section, we added three Grunt plugins
    used for different tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to add `GruntFile`. Let''s create a file called `GruntFile.js`
    in our project root similar to the `package.json` file. Place the following contents
    in `GruntFile`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code is simple and self-explanatory, and the comments are added
    whenever needed. At the top, we loaded our `package.json` file, and after this,
    we defined different tasks along with their src and destination files. Remember
    that every task's src and destination syntax is different, and it depends on the
    plugin. After `initConfig` block, we loaded different plugins and npm tasks and
    then registered them with GRUNT.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's run our tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s combine CSS and JavaScript files and store them in their respective
    destinations defined in our tasks list in GruntFile via the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: After running the preceding command in your terminal, if you see a message such
    as `Done, without errors`, then the task is completed successfully.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the same way, let''s minify our css file using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will minify our JavaScript file using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Now, it may seem like a lot of work to use Grunt, but it provides some other
    features that can make a developer's life easy. For example, what if you need
    to change your JavaScript and CSS files? Should you run all the preceding commands
    again? No, Grunt provides a watch plugin, which activates and executes all the
    files in the destination paths in the tasks, and if any changes occur, it runs
    the tasks automatically.
  prefs: []
  type: TYPE_NORMAL
- en: For a more detailed learning, take a look at Grunt's official website at [http://gruntjs.com/](http://gruntjs.com/).
  prefs: []
  type: TYPE_NORMAL
- en: Full page caching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In full page caching, the complete page of the website is stored in a cache,
    and for the next requests, this cached page is served. Full page cache is more
    effective if your website content does not change too often; for example, on a
    blog with simple posts, new posts are added on a weekly basis. In this case, the
    cache can be cleared after new posts are added.
  prefs: []
  type: TYPE_NORMAL
- en: What if you have a website that has pages with dynamic parts, such as an e-commerce
    website? In this case, a complete page caching will create problems because the
    page is always different for each request; as a user is logged in, he/she may
    add products to the shopping cart and so on. In this case, using full page caching
    may not be that easy.
  prefs: []
  type: TYPE_NORMAL
- en: Most popular platforms provide either built-in support for full page cache or
    through plugins and modules. In this case, the plugin or module takes care of
    the dynamic blocks of the page for each request.
  prefs: []
  type: TYPE_NORMAL
- en: Varnish
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Varnish, as mentioned on its official website, makes your website fly; and this
    is true! Varnish is an open source web application accelerator that runs in front
    of your web server software. It has to be configured on port 80 so that each request
    comes to it.
  prefs: []
  type: TYPE_NORMAL
- en: Now, the Varnish configuration file (called VCL files with the `.vcl` extenstion)
    has a definition for backends. A backend is the web server (Apache or NGINX) configured
    on another port (let's say 8080). Multiple backends can be defined, and Varnish
    will take care of the load balancing too.
  prefs: []
  type: TYPE_NORMAL
- en: When a request comes to Varnish, it checks whether the data for this request
    in available at its cache or not. If it finds the data in its cache, this cached
    data is returned to the request, and no request is sent to the web server or backend.
    If Varnish does not find any data in its cache, it sends a request to the web
    server and requests the data. When it receives data from the web server, it first
    caches this data and then sends it back to the request.
  prefs: []
  type: TYPE_NORMAL
- en: As it is clear in the preceding discussion, if Varnish finds the data in the
    cache, there is no need for a request to the web server and, therefore, for processing
    in there, and the response is sent back very fast.
  prefs: []
  type: TYPE_NORMAL
- en: Varnish also provides features such as load balancing and health checks. Also,
    Varnish has no support for SSL and cookies. If Varnish receives cookies from the
    web server or backend, this page is not cached. There are different ways to overcome
    these issues easily.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ve done enough theory; now, let''s install Varnish on a Debian/Ubuntu server
    via the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, add the Varnish repositories to the `sources.list` file. Place the following
    line in the file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'After this, issue the following command to update the repositories:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, issue the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: This will download and install Varnish. Now, the first thing to do is configure
    Varnish to listen at port 80 and make your web server listen at another port,
    such as 8080\. We will configure it here with NGINX.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, open the Varnish configuration file location at `/etc/default/varnish`
    and change it so that it looks similar to the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Save the file and restart Varnish by issuing the following command in the terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Now our Varnish runs on port `80`. Let''s make NGINX run on port `8080`. Edit
    the NGINX `vhost` file for the application and change the listen port from `80`
    to `8080`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, restart NGINX by issuing the following command in the terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to configure the Varnish VCL file and add a backend that will
    communicate with our backend on port `8080`. Edit the Varnish VCL file located
    at `/etc/varnish/default.vcl`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding configuration, our backend host is located at the same server
    on which Varnish runs, so we entered the local IP. We can also enter a localhost
    in this case. However, if our backend runs on a remote host or another server,
    the IP of this server should be entered.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we are done with Varnish and web server configuration. Restart both Varnish
    and NGINX. Open your browser and enter the IP or hostname of the server. The first
    response may seem slow, which is because Varnish is fetching data from the backend
    and then caching it, but other subsequent responses will be extremely fast, as
    Varnish cached them and is now sending back the cached data without communicating
    with the backend.
  prefs: []
  type: TYPE_NORMAL
- en: 'Varnish provides a tool in which we can easily monitor the Varnish cache status.
    It is a real-time tool and updates its contents in real time. It is called varnishstat.
    To start varnishstat, just issue the following command in the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command will display a session similar to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Varnish](graphics/B05225_03_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As can be seen in the preceding screenshot, it displays very useful information,
    such as the running time and the number of requests made at the beginning, cache
    hits, cache misses, all backends, backend reusages, and so on. We can use this
    information to tune Varnish for its best performance.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A complete Varnish configuration is out of the scope of this book, but a good
    documentation can be found on the Varnish official website at [https://www.varnish-cache.org](https://www.varnish-cache.org).
  prefs: []
  type: TYPE_NORMAL
- en: The infrastructure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We discussed too many topics on increasing the performance of our application.
    Now, let's discuss the scalability and availability of our application. With time,
    the traffic on our application can increase to thousands of users at a time. If
    our application runs on a single server, the performance will be hugely effected.
    Also, it is not a good idea to keep the application running at a single point
    because in case this server goes down, our complete application will be down.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make our application more scalable and better in availability, we can use
    an infrastructure setup in which we can host our application on multiple servers.
    Also, we can host different parts of the application on different servers. To
    better understand, take a look at the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The infrastructure](graphics/B05225_03_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This is a very basic design for the infrastructure. Let's talk about its different
    parts and what operations will be performed by each part and server.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is possible that only the Load Balancer (LB) will be connected to the public
    Internet, and the rest of the parts can be connected to each through a private
    network in a Rack. If a Rack is available, this will be very good because all
    the communication between all the servers will be on a private network and therefore
    secure.
  prefs: []
  type: TYPE_NORMAL
- en: Web servers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the preceding diagram, we have two web servers. There can be as many web
    servers as needed, and they can be easily connected to LB. The web servers will
    host our actual application, and the application will run on NGINX or Apache and
    PHP 7\. All the performance tunings we will discuss in this chapter can be used
    on these web servers. Also, it is not necessary that these servers should be listening
    at port 80\. It is good that our web server should listen at another port to avoid
    any public access using browsers.
  prefs: []
  type: TYPE_NORMAL
- en: The database server
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The database server is mainly used for the database where the MySQL or Percona
    Server can be installed. However, one of the problems in the infrastructure setup
    is to store session data in a single place. For this purpose, we can also install
    the Redis server on the database server, which will handle our application's session
    data.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding infrastructure design is not a final or perfect design. It is
    just to give the idea of a multiserver application hosting. It has room for a
    lot of improvement, such as adding another local balancer, more web servers, and
    servers for the database cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancer (LB)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first part is the **load balancer** (**LB**). The purpose of the load balancer
    is to divide the traffic among the web servers according to the load on each web
    server.
  prefs: []
  type: TYPE_NORMAL
- en: For the load balancer, we can use HAProxy, which is widely used for this purpose.
    Also, HAProxy checks the health of each web server, and if a web server is down,
    it automatically redirects the traffic of this down web server to other available
    web servers. For this purpose, only LB will be listening at port 80.
  prefs: []
  type: TYPE_NORMAL
- en: We don't want to place a load on our available web servers (in our case, two
    web servers) of encrypting and decrypting the SSL communication, so we will use
    the HAProxy server to terminate SSL there. When our LB receives a request with
    SSL, it will terminate SSL and send a normal request to one of the web servers.
    When it receives a response, HAProxy will encrypt the response and send it back
    to the client. This way, instead of using both the servers for SSL encryption/decryption,
    only a single LB server will be used for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Varnish can be also used as a load balancer, but this is not a good idea because
    the whole purpose of Varnish is HTTP caching.
  prefs: []
  type: TYPE_NORMAL
- en: HAProxy load balancing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the preceding infrastructure, we placed a load balancer in front of our web
    servers, which balance load on each server, check the health of each server, and
    terminate SSL. We will install HAProxy and configure it to achieve all the configurations
    mentioned before.
  prefs: []
  type: TYPE_NORMAL
- en: HAProxy installation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will install HAProxy on Debian/Ubuntu. As of writing this book, HAProxy
    1.6 is the latest stable version available. Perform the following steps to install
    HAProxy:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, update the system cache by issuing the following command in the terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, install HAProxy by entering the following command in the terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: This will install HAProxy on the system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, confirm the HAProxy installation by issuing the following command in the
    terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '![HAProxy installation](graphics/B05225_03_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If the output is as in the preceding screenshot, then congratulations! HAProxy
    is installed successfully.
  prefs: []
  type: TYPE_NORMAL
- en: HAProxy load balancing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now, it''s time to use HAProxy. For this purpose, we have the following three
    servers:'
  prefs: []
  type: TYPE_NORMAL
- en: The first is a load balancer server on which HAProxy is installed. We will call
    it LB. For this book's purpose, the IP of the LB server is 10.211.55.1\. This
    server will listen at port 80, and all HTTP requests will come to this server.
    This server also acts as a frontend server as all the requests to our application
    will come to this server.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second is a web server, which we will call Web1\. NGINX, PHP 7, MySQL, or
    Percona Server are installed on it. The IP of this server is 10.211.55.2\. This
    server will either listen at port 80 or any other port. We will keep it to listen
    at port 8080.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third is a second web server, which we will call Web2, with the IP 10.211.55.3\.
    This has the same setup as of the Web1 server and will listen at port 8080.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Web1 and Web2 servers are also called backend servers. First, let's configure
    the LB or frontend server to listen at port 80.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the `haproxy.cfg` file located at `/etc/haproxy/` and add the following
    lines at the end of the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we set HAProxy to listen at the HTTP port 80 on any IP
    address, either the local loopback IP 127.0.0.1 or the public IP. Then, we set
    the default backend.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will add two backend servers. In the same file, at the end, place the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding configuration, we added two servers into the web backend. The
    reference name for the backend is `web-backend`, which is used in the frontend
    configuration too. As we know, both our web servers listen at port 8080, so we
    mentioned that it is the definition of each web server. Also, we used `check`
    at the end of the definition of each web server, which tells HAProxy to check
    the server's health.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, restart HAProxy by issuing the following command in the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To start HAProxy, we can use the `sudo service haproxy start` command. To stop
    HAProxy, we can use the `sudo service haproxy stop` command.
  prefs: []
  type: TYPE_NORMAL
- en: Now, enter the IP or hostname of the LB server in the browser, and our web application
    page will be displayed either from Web1 or Web2.
  prefs: []
  type: TYPE_NORMAL
- en: Now, disable any of the web servers and then reload the page again. The application
    will still work fine, because HAProxy automatically detected that one of web servers
    is down and redirected the traffic to the second web server.
  prefs: []
  type: TYPE_NORMAL
- en: 'HAProxy also provides a stats page, which is browser-based. It provides complete
    monitoring information about the LB and all the backends. To enable stats, open
    `haprox.cfg`, and place the following code at the end of the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The stats are enabled at port `1434`, which can be set to any port. The URL
    of the page is `stats uri`. It can be set to any URL. The `auth` section is for
    basic HTTP authentication. Save the file and restart HAProxy. Now, open the browser
    and enter the URL, such as `10.211.55.1:1434/haproxy-stats`. The stats page will
    be displayed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![HAProxy load balancing](graphics/B05225_03_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding screenshot, each backend web server can be seen, including
    frontend information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, if a web server is down, HAProxy stats will highlight the row for this
    web server, as can be seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![HAProxy load balancing](graphics/B05225_03_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For our test, we stopped NGINX at our Web2 server and refreshed the stats page,
    and the Web2 server row in the backend section was highlighted.
  prefs: []
  type: TYPE_NORMAL
- en: 'To terminate SSL using HAProxy, it is pretty simple. To terminate SSL using
    HAProxy, we will just add the SSL port 443 binding along with the SSL certificate
    file location. Open the `haproxy.cfg` file, edit the frontend block, and add the
    highlighted code in it, as in the following block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Now, HAProxy also listens at 443, and when an SSL request is sent to it, it
    processes it there and terminates it so that no HTTPS requests are sent to backend
    servers. This way, the load of SSL encryption/decryption is removed from the web
    servers and is managed by the HAProxy server only. As SSL is terminated at the
    HAProxy server, there is no need for web servers to listen at port 443, as regular
    requests from HAProxy server are sent to the backend.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed several topics starting from NGINX and Apache
    to Varnish. We discussed how we can optimize our web server's software settings
    for the best performance. Also, we discussed CDNs and how to use them in our customer
    applications. We discussed two ways to optimize JavaScript and CSS files for the
    best performance. We briefly discussed full page cache and Varnish installation
    and configuration. At the end, we discussed multiserver hosting or infrastructure
    setup for our application to be scalable and the best in availability.
  prefs: []
  type: TYPE_NORMAL
- en: In next chapter, we will look into the ways of increasing the performance of
    our database. We will discuss several topics, including the Percona Server, different
    storage engines for the database, query caching, Redis, and Memcached.
  prefs: []
  type: TYPE_NORMAL
