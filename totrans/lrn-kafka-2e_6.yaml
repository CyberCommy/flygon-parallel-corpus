- en: Chapter 6. Kafka Integrations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Consider a use case for a website where continuous security events, such as
    user authentication and authorization to access secure resources, need to be tracked,
    and decisions need to be taken in real time for any security breach. Using any
    typical batch-oriented data processing systems, such as Hadoop, where all the
    data needs to be collected first and then processed to reveal patterns, will make
    it too late to decide whether there is any security threat to the web application
    or not. Hence, this is the classical use case for real-time data processing.
  prefs: []
  type: TYPE_NORMAL
- en: Let's consider another use case, where raw clickstreams generated by customers
    through website usage are captured and preprocessed. Processing these clickstreams
    provides valuable insight into customer preferences and these insights can be
    coupled later with marketing campaigns and recommendation engines to offer an
    analysis of consumers. Hence, we can simply say that this large amount of clickstream
    data stored on Hadoop will get processed by Hadoop MapReduce jobs in batch mode,
    not in real time.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we shall be exploring how Kafka can be integrated with the
    following technologies to address different use cases, such as real-time processing
    using Storm, as Spark Streaming, and batch processing using Hadoop:'
  prefs: []
  type: TYPE_NORMAL
- en: Kafka integration with Storm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kafka integration with Hadoop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So let's start.
  prefs: []
  type: TYPE_NORMAL
- en: Kafka integration with Storm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Processing small amounts of data in real-time was never a challenge using technologies
    such as **Java Messaging Service** (**JMS**); however, these processing systems
    show performance limitations when dealing with large volumes of streaming data.
    Also, these systems are not good horizontally scalable solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Storm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Storm** is an open source, distributed, reliable, and fault-tolerant system
    for processing streams of large volumes of data in real-time. It supports many
    use cases, such as real-time analytics, online machine learning, continuous computation,
    and the **Extract Transformation Load** (**ETL**) paradigm.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are various components that work together for streaming data processing,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Spout**: This is a continuous stream of log data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bolt**: The spout passes the data to a component called **bolt**. A bolt
    consumes any number of input streams, does some processing, and possibly emits
    new streams. For example, emitting a stream of trend analysis by processing a
    stream of tweets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram shows spout and bolt in the Storm architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Introducing Storm](img/3090OS_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can assume a Storm cluster to be a chain of bolt components, where each
    bolt performs some kind of transformation on the data streamed by the spout. Other
    than spout and bolts, a few other components are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tuple: This is the native data structure (name list values of any data type)
    used by Storm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stream: This represents a sequence of tuples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Workers: These represent the Storm process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Executors: A Storm thread launched by a Storm worker. Here, workers may run
    one or more executors and executors may run one or more Storm job(s) from a spout
    or bolt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next in the Storm cluster, jobs are typically referred to as **topologies**;
    the only difference is that these topologies run forever. For real-time computation
    on Storm, topologies that are nothing but graphs of computation are created. Typically,
    topologies define how data will flow from spouts through bolts. These topologies
    can be transactional or non-transactional in nature.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Complete information about Storm can be found at [http://storm-project.net/](http://storm-project.net/).
  prefs: []
  type: TYPE_NORMAL
- en: The following section is useful if you have worked with Storm or have working
    knowledge of Storm.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating Storm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have already learned in the previous chapters that Kafka is a high-performance
    publisher-subscriber-based messaging system with highly scalable properties. Kafka
    spout is available for integrating Storm with Kafka clusters.
  prefs: []
  type: TYPE_NORMAL
- en: The Kafka spout is a regular spout implementation that reads the data from a
    Kafka cluster. This Kafka spout, which was available earlier at [https://github.com/wurstmeister/storm-kafka-0.8-plus](https://github.com/wurstmeister/storm-kafka-0.8-plus),
    is now merged into the core Storm project version 0.9.2-incubating and can be
    found at [https://github.com/apache/storm/tree/master/external/storm-kafka](https://github.com/apache/storm/tree/master/external/storm-kafka).
    This storm-kafka spout provides the key features such as support for dynamic discovery
    of Kafka brokers and "exactly once" tuple processing. Apart from the regular Storm
    spout for Kafka, it also provides the Trident spout implementation for Kafka.
    In this section, our focus will remain on the regular storm-kafka spout.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Trident is a high-level abstraction for doing real-time computing on top of
    Storm. It allows us to seamlessly intermix high throughput (millions of messages
    per second), stateful stream processing with low-latency distributed querying.
    For more information [https://storm.apache.org/documentation/Trident-tutorial.html](https://storm.apache.org/documentation/Trident-tutorial.html).
  prefs: []
  type: TYPE_NORMAL
- en: Both spout implementations use the `BrokerHost` interface that tracks Kafka
    broker host-to-partition mapping and `KafkaConfig` parameters. Two implementations,
    `ZkHosts` and `StaticHosts`, are provided for the `BrokerHost` interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `ZkHosts` implementation is used for dynamically tracking Kafka broker-to-partition
    mapping with the help of Kafka''s zookeeper''s entries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The preceding constructors are used to create the instance of `ZkHosts`. Here,
    `brokerZkStr` can be `localhost:9092` and `brokerZkPath` is the root directory
    under which all the topic and partition information is stored. The default value
    of `brokerZkPath` is `/brokers`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `StaticHosts` implementation is used for static partitioning information
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'For creating the `StaticHosts` instance, the first instance of `GlobalPartitionInformation`
    is created as shown in the preceding code. Next, the `KafkaConfig` instance needs
    to be created for constructing the Kafka spout as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding constructors take the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: A list of Kafka brokers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The topic name used to read the message
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Client ID, used as a part of the Zookeeper path where the spout as a consumer
    stores the current consumption offset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `KafkaConfig` class also has a bunch of public variables for controlling
    the application''s behavior and how spout fetches messages from the Kafka cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Spoutconfig` class extends the `KafkaConfig` class to support two additional
    values as `zkroot` and `id`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding constructor additionally takes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The root path in Zookeeper, where spout stores the consumer offset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The unique identity of the spout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code sample shows the `KafkaSpout` class instance initialization
    with the previous parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram shows the high-level integration view of what a Kafka
    Storm working model will look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Integrating Storm](img/3090OS_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The Kafka spout uses the same Zookeeper instance that is used by Apache Storm,
    to store the states of the message offset and segment consumption tracking if
    it is consumed. These offsets are stored at the root path specified for the Zookeeper.
    The Kafka spout uses these offsets to replay tuples in the event of a downstream
    failure or timeout. Although it also has a provision to rewind to a previous offset
    rather than starting from the last saved offset, Kafka chooses the latest offset
    written around the specified timestamp:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Here the value `-1` forces the Kafka spout to restart from the latest offset
    and `-2` forces the spout to restart from the earliest offset.
  prefs: []
  type: TYPE_NORMAL
- en: This storm-kafka spout also has a as it has no support for Kafka 0.7x brokers
    and only supports Kafka 0.8.1.x onwards.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To run Kafka with Storm, clusters for both Storm and Kafka need to be set up
    and should be running. A Storm cluster setup is beyond the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Kafka integration with Hadoop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Resource sharing, stability, availability, and scalability are a few of the
    many challenges of distributed computing. Nowadays, an additional challenge is
    to process extremely large volumes of data in TBs or PBs.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Hadoop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hadoop is a large-scale distributed batch-processing framework that parallelizes
    data processing across many nodes and addresses the challenges for distributed
    computing, including big data.
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop works on the principle of the MapReduce framework (introduced by Google),
    which provides a simple interface for the parallelization and distribution of
    large-scale computations. Hadoop has its own distributed data filesystem called
    **Hadoop Distributed File System** (**HDFS**). In any typical Hadoop cluster,
    HDFS splits the data into small pieces (called **blocks**) and distributes it
    to all the nodes. HDFS also replicates these small pieces of data and stores them
    to make sure that, if any node is down, the data is available from another node.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the high-level view of a multi-node Hadoop cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Introducing Hadoop](img/3090OS_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Hadoop has the following main components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Name node**: This is a single point of interaction for HDFS. A name node
    stores information about the small pieces (blocks) of data that are distributed
    across the node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Secondary name node**: This node stores edit logs, which are helpful to for
    restoring the latest updated state of HDFS in the case of a name node failure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data node**: These nodes store the actual data distributed by the name node
    in blocks and also store the replicated copy of data from other nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Job tracker**: This is responsible for splitting the MapReduce jobs into
    smaller tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Task tracker**: The task tracker is responsible for the execution of tasks
    split by the job tracker.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data nodes and the task tracker share the same machines and the MapReduce
    job split; execution of tasks is done based on the data store location information
    provided by the name node.
  prefs: []
  type: TYPE_NORMAL
- en: Now before we discuss the Kafka integration with Hadoop let's quickly set up
    a single node Hadoop cluster in pseudo distributed mode.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Hadoop clusters can be set up in three different modes:'
  prefs: []
  type: TYPE_NORMAL
- en: Local mode
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pseudo distributed mode
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fully distributed mode
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Local mode and pseudo distributed mode work on single-node cluster. In local
    mode, all the Hadoop main components run in the single JVM instance; whereas,
    in pseudo distributed mode, each component runs in a separate JVM instance on
    the single node. Pseudo distributed mode is primarily used as a development environment
    by developers. In fully distributed mode, all the components run on separate nodes
    and are used in test and production environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the steps used for creating pseudo distributed mode cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: Install and configure Java. Refer to the *Installing Java 1.7 or higher* section
    in [Chapter 1](ch01.html "Chapter 1. Introducing Kafka"), *Introducing Kafka*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download the current stable Hadoop distribution from [http://www.apache.org/dyn/closer.cgi/hadoop/common/](http://www.apache.org/dyn/closer.cgi/hadoop/common/).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Unpack the downloaded Hadoop distribution in `/opt` and add Hadoop''s `bin`
    directory to the path as:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the following configurations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up ssh to the localhost without a passphrase:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'If ssh-to-localhost does not work without a passphrase, execute the following
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Format the filesystem:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Start the NameNode daemon and DataNode daemon:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Once the Hadoop cluster is set up successfully, browse the web interface for
    the NameNode at `http://localhost:50070/`.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating Hadoop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section is useful if you have worked with Hadoop or have a working knowledge
    of Hadoop.
  prefs: []
  type: TYPE_NORMAL
- en: For real-time publish-subscribe use cases, Kafka is used to build a pipeline
    that is available for real-time processing or monitoring and to load the data
    into Hadoop, NoSQL, or data warehousing systems for offline processing and reporting.
  prefs: []
  type: TYPE_NORMAL
- en: Kafka provides the source code for both the Hadoop producer and consumer, under
    its `contrib` directory.
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop producers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A Hadoop producer provides a bridge for publishing the data from a Hadoop cluster
    to Kafka, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hadoop producers](img/3090OS_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For a Kafka producer, Kafka topics are considered as URIs and, to connect to
    a specific Kafka broker, URIs are specified as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The Hadoop producer code suggests two possible approaches for getting the data
    from Hadoop:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Using the Pig script and writing messages in Avro format**: In this approach,
    Kafka producers use Pig scripts for writing data in a binary Avro format, where
    each row signifies a single message. For pushing the data into the Kafka cluster,
    the `AvroKafkaStorage` class (it extends Pig''s `StoreFunc` class) takes the Avro
    schema as its first argument and connects to the Kafka URI. Using the `AvroKafkaStorage`
    producer, we can also easily write to multiple topics and brokers in the same
    Pig-script-based job. While writing Pig scripts, required Kafka JAR files also
    need to be registered. The following is the sample Pig script:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding script, the Pig `StoreFunc` class makes use of `AvroStorage`
    in Piggybank to convert from Pig's data model to the specified Avro schema.
  prefs: []
  type: TYPE_NORMAL
- en: '**Using the Kafka OutputFormat class for jobs**: In this approach, the Kafka
    `OutputFormat` class (it extends Hadoop''s `OutputFormat` class) is used for publishing
    data to the Kafka cluster. Using the 0.20 MapReduce API, this approach publishes
    messages as bytes and provides control over output by using low-level methods
    of publishing. The Kafka `OutputFormat` class uses the `KafkaRecordWriter` class
    (it extends Hadoop''s `RecordWriter` class) for writing a record (message) to
    a Hadoop cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For Kafka producers, we can also configure Kafka producer parameters by prefixing
    them with `kafka.output` in the job configuration. For example, to change the
    compression codec, add the `kafka.output.compression.codec` parameter (for example,
    `SET kafka.output.compression.codec 0` in Pig script for no compression). Along
    with these values, Kafka broker information (`kafka.metadata.broker.list`), the
    topic (`kafka.output.topic`), and the schema (`kafka.output.schema`) are injected
    into the job's configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop consumers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A Hadoop consumer is a Hadoop job that pulls data from the Kafka broker and
    pushes it into HDFS. The following diagram shows the position of a Kafka consumer
    in the architecture pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hadoop consumers](img/3090OS_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A Hadoop job performs parallel loading from Kafka to HDFS, and the number of
    mappers for loading the data depends on the number of files in the input directory.
    The output directory contains data coming from Kafka and the updated topic offsets.
    Individual mappers write the offset of the last consumed message to HDFS at the
    end of the map task. If a job fails and jobs get restarted, each mapper simply
    restarts from the offsets stored in HDFS.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ETL example provided in the `Kafka-0.8.1.1-src/contrib/hadoop-consumer`
    directory demonstrates the extraction of Kafka data and loading it to HDFS. It
    requires the following inputs from a configuration file, for example, `test/test.properties`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kafka.etl.topic`: The topic to be fetched.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kafka.server.uri`: The Kafka server URI.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input`: Input directory containing topic offsets that can be generated by
    `DataGenerator`. The number of files in this directory determines the number of
    mappers in the Hadoop job.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output`: Output directory containing Kafka data and updated topic offsets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kafka.request.limit`: It is used to limit the number events fetched.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the Kafka consumer, the `KafkaETLRecordReader` instance is a record reader
    associated with `KafkaETLInputFormat`. It fetches Kafka data from the server starting
    from the provided offsets (specified by `input`) and stops when it reaches the
    largest available offsets or the specified limit (specified by `kafka.request.limit`).
    `KafkaETLJob` also contains some helper functions to initialize job configuration
    and `SimpleKafkaETLJob` sets up job properties and submits the Hadoop job. Once
    the job is started `SimpleKafkaETLMapper` dumps Kafka data into HDFS (specified
    by `output`).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have primarily learned how Kafka can be integrated with
    existing open source frameworks in the area of real-time/batch data processing.
    In the real-time data processing area, Kafka is integrated with Storm using the
    existing Storm spout. As for batch data processing, Kafka brings Hadoop-based
    data producers and consumes, so that data can be published onto the HDFS, processed
    using MapReduce, and later consumed.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, which is also the last chapter of this book, we will look
    at some of the other important facts about Kafka.
  prefs: []
  type: TYPE_NORMAL
