- en: Twitter Tweet Collection and Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we covered how we can create a log processing application
    with Storm and Kafka.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are covering another important use case of Storm machine
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the major topics covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Kafka producer to store the tweets in a Kafka cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Kafka Spout to read the data from Kafka
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Storm Bolt to filter the tweets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Storm Bolt to calculate the sentiments of tweets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment of topologies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning is a branch of applied computer science in which we build models
    of real-world phenomenon based on existing data available for analysis, and then
    using that model, predicting certain characteristics of data never seen before
    by the model. Machine learning has become a very important component of real-time
    applications as decisions need to be made in real time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Graphically, the process of machine learning can be represented by the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00064.gif)'
  prefs: []
  type: TYPE_IMG
- en: The process of building the model from data is called **training** in machine
    learning terminology. Training can happen in real time on a stream of data or
    it can be done on historical data. When the training is done in real time, the
    model evolves over time with the changed data. This kind of learning is referred
    to as *online* learning, and when the model is updated every once in a while,
    by running the training algorithm on a new set of data, it is called *offline*
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: When we talk about machine learning in the context of Storm, more often than
    not we are talking about online learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some of the real-world applications of machine learning:'
  prefs: []
  type: TYPE_NORMAL
- en: Online ad optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: New article clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spam detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computer vision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentiment analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Twitter sentiment analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will be dividing the sentiments use case into two parts:'
  prefs: []
  type: TYPE_NORMAL
- en: Collecting tweets from Twitter and storing them in Kafka
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading the data from Kafka, calculating the sentiments, and storing them in
    HDFS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00065.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Using Kafka producer to store the tweets in a Kafka cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to cover how we can stream the tweets from Twitter
    using the twitter streaming API. We are also going to cover how we can store the
    fetched tweets in Kafka for later processing through Storm.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are assuming you already have a twitter account, and that the consumer key
    and access token are generated for your application. You can refer to: [https://bdthemes.com/support/knowledge-base/generate-api-key-consumer-token-access-key-twitter-oauth/](https://bdthemes.com/support/knowledge-base/generate-api-key-consumer-token-access-key-twitter-oauth/)
    to generate a consumer key and access token. Take the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a new maven project with `groupId`, `com.stormadvance` and `artifactId`,
    `kafka_producer_twitter`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add the following dependencies to the `pom.xml` file. We are adding the Kafka
    and Twitter streaming Maven dependencies to `pom.xml` to support the Kafka Producer
    and the streaming tweets from Twitter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Now, we need to create a class, `TwitterData`, that contains the code to consume/stream
    data from Twitter and publish it to the Kafka cluster. We are assuming you already
    have a running Kafka cluster and topic, `twitterData`, created in the Kafka cluster.
    Please refer to [Chapter 8](part0137.html#42KT20-6149dd15e07b443593cc93f2eb31ee7b),
    *Integration of Storm and Kafka*, for information on the installation of the Kafka
    cluster and the creation of a Kafka topic if they do not exist.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The class contains an instance of the `twitter4j.conf.ConfigurationBuilder`
    class; we need to set the access token and consumer keys in configuration, as
    mentioned in the source code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `twitter4j.StatusListener` class returns the continuous stream of tweets
    inside the `onStatus()` method. We are using the Kafka Producer code inside the
    `onStatus()` method to publish the tweets in Kafka. The following is the source
    code for the `TwitterData` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Use valid Kafka properties before executing the `TwitterData` class.
  prefs: []
  type: TYPE_NORMAL
- en: After executing the preceding class, the user will have a real-time stream of
    Twitter tweets in Kafka. In the next section, we are going to cover how we can
    use Storm to calculate the sentiments of the collected tweets.
  prefs: []
  type: TYPE_NORMAL
- en: Kafka spout, sentiments bolt, and HDFS bolt
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we are going to write/configure a Kafka spout to consume the
    tweets from the Kafka cluster. We are going to use the open source Storm spout
    connectors for consuming the data from Kafka:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a new maven project with the `groupID` as `com.stormadvance` and `artifactId` as `Kafka_twitter_topology`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add the following maven dependencies to the `pom.xml` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `StormHDFSTopology` class inside the `com.stormadvance.Kafka_twitter_topology.topology`
    package and add the following dependencies to specify that the Kafka spout consumes
    the data from the `twitterData` topic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `JSONParsingBolt` class inside the package''s `com.stormadvance.Kafka_twitter_topology.bolt`
    class to extract the tweet text from the JSON twitter tweet that the JSON received
    from Twitter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `SentimentBolt` class inside the package''s `com.stormadvance.Kafka_twitter_topology.sentiments`
    class to create the sentiments of each tweet. We are using a dictionary file to
    find out if the words used in tweets are positive or negative and calculate the
    sentiments of an entire tweet. The following is the source code of the class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to store the sentiments in an HDFS for generating charts or feature
    analysis. Next, we add the following code inside the `StormHDFSTopology` class
    to chain the spout and bolts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the complete code of the `StormHDFSTopology` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can create the JAR for the entire project and deploy it on a Storm cluster
    as defined in [Chapter 2](part0034.html#10DJ40-6149dd15e07b443593cc93f2eb31ee7b),
    *Storm Deployment, Topology Development, and Topology Options* in this book.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we covered how we can read Twitter tweets using the Twitter
    streaming API, how we can process the tweets to calculate the tweet text from
    inputted JSON records, calculate the sentiments of the tweets, and store the final
    output in HDFS.
  prefs: []
  type: TYPE_NORMAL
- en: With this, we come to the end of this book. Over the course of this book, we
    have come a long way from taking our first steps with Apache Storm to developing
    real-world applications with it. Here, we would like to summarize everything that
    we have learned.
  prefs: []
  type: TYPE_NORMAL
- en: We introduced you to the basic concepts and components of Storm, and covered
    how we can write and deploy/run the topology in both local and clustered mode.
    We also walked through the basic commands of Storm, and covered how we can modify
    the parallelism of the Storm topology at runtime. We also dedicated an entire
    chapter to monitoring Storm, which is an area often neglected during development,
    but is a critical part of any production setting. You also learned about Trident,
    which is an abstraction of the low-level Storm API that can be used to develop
    more complex topologies and maintain the application state.
  prefs: []
  type: TYPE_NORMAL
- en: No enterprise application can be developed in a single technology, and so our
    next step was to see how we could integrate Storm with other big data tools and
    technologies. We saw a specific implementation of Storm with Kafka, Hadoop, HBase,
    and Redis. Most of the big data applications use Ganglia as a centralized monitoring
    tool, hence we also covered how we could monitor the Storm cluster through JMX
    and Ganglia.
  prefs: []
  type: TYPE_NORMAL
- en: You also learned about various patterns used to integrate diverse data sources
    with Storm. Finally, in both [Chapter 11](part0182.html#5DI6C0-6149dd15e07b443593cc93f2eb31ee7b),
    *Apache Log Processing with Storm*, and this chapter, we implemented two case
    studies in Apache Storm that can serve as a starting point for developing more
    complex applications.
  prefs: []
  type: TYPE_NORMAL
- en: We hope that reading this book has been a fruitful journey for you, and that
    you developed a basic understanding of Storm and, in general, the various aspects
    of developing a real-time stream processing application. Apache Storm is turning
    into a de-facto standard for stream processing, and we hope that this book will
    act as a catalyst for you to jumpstart the exciting journey of building a real-time
    stream processing application.
  prefs: []
  type: TYPE_NORMAL
