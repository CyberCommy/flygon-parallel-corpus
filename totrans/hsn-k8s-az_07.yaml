- en: 5\. Handling common failures in AKS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes is a distributed system with many working parts. AKS abstracts most
    of it for you, but it is still your responsibility to know where to look and how
    to respond when bad things happen. Much of the failure handling is done automatically
    by Kubernetes; however, you will encounter situations where manual intervention
    is required.
  prefs: []
  type: TYPE_NORMAL
- en: There are two areas where things can go wrong in an application that is deployed
    on top of AKS. Either the cluster itself has issues, or the application deployed
    on top of the cluster has issues. This chapter focuses specifically on cluster
    issues. There are several things that can go wrong with a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The first thing that can go wrong is a node in the cluster can become unavailable.
    This can happen either due to an Azure infrastructure outage or due to an issue
    with the virtual machine itself, such as an operating system crash. Either way,
    Kubernetes monitors the cluster for node failures and will recover automatically.
    You will see this process in action in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: A second common issue in a Kubernetes cluster is out-of-resource failures. This
    means that the workload you are trying to deploy requires more resources than
    are available on your cluster. You will learn how to monitor these signals and
    how you can solve them.
  prefs: []
  type: TYPE_NORMAL
- en: Another common issue is problems with mounting storage, which happens when a
    node becomes unavailable. When a node in Kubernetes becomes unavailable, Kubernetes
    will not detach the disks attached to this failed node. This means that those
    disks cannot be used by workloads on other nodes. You will see a practical example
    of this and learn how to recover from this failure.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will look into the following failure modes in depth in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Handling node failures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving out-of-resource failures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling storage mount issues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, you will learn about common failure scenarios, as well as solutions
    to those scenarios. To start, we will introduce node failures.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Refer to *Kubernetes the Hard Way* ([https://github.com/kelseyhightower/kubernetes-the-hard-way](https://github.com/kelseyhightower/kubernetes-the-hard-way)),
    an excellent tutorial, to get an idea about the blocks on which Kubernetes is
    built. For the Azure version, refer to *Kubernetes the Hard Way – Azure Translation*
    ([https://github.com/ivanfioravanti/kubernetes-the-hard-way-on-azure](https://github.com/ivanfioravanti/kubernetes-the-hard-way-on-azure)).
  prefs: []
  type: TYPE_NORMAL
- en: Handling node failures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Intentionally (to save costs) or unintentionally, nodes can go down. When that
    happens, you don''t want to get the proverbial 3 a.m. call that your system is
    down. Kubernetes can handle moving workloads on failed nodes automatically for
    you instead. In this exercise, we are going to deploy the guestbook application
    and are going to bring a node down in our cluster and see what Kubernetes does
    in response:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ensure that your cluster has at least two nodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This should generate an output as shown in *Figure 5.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Executing the kubectl get nodes command displays an output withtwo nodes.
    The status of these two nodes is Ready.](image/Figure_5.1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1: Ensure you have two nodes running in your cluster'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'If you don''t have two nodes in your cluster, look for your cluster in the
    Azure portal, navigate to **Node pools**, and click on **Node count**. You can
    scale this to **2** nodes as shown in *Figure 5.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Click on the Node pools tab in the Navigation pane located on the left side
    of the Azure portal. This will open show you several options. Go to the Node count
    option. Click on it to scale this count to two nodes.](image/Figure_5.2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.2: Scaling the cluster'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As an example application in this chapter, we will work with the guestbook
    application. The YAML file to deploy this has been provided in the source code
    for this chapter (`guestbook-all-in-one.yaml`). To deploy the guestbook application,
    use the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We will give the service a public IP again, as we did in the previous chapters.
    To start the edit, execute the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This will open a `vi` environment. Navigate to the line that now says `type:
    ClusterIP` (line 27) and change that to `type: LoadBalancer`. To make that change,
    hit the *I*button to enter insert mode, type your changes, then hit the *Esc*
    button, type `:wq!`, and hit *Enter* to save the changes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once the changes are saved, you can watch the `service` object until the public
    IP becomes available. To do this, type the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This will take a couple of minutes to show you the updated IP. *Figure 5.3*
    represents the service's public IP. Once you see the right public IP, you can
    exit the watch command by hitting *Ctrl* + *C* (*command* + *C* on Mac):![The
    output displays frontend service changing its External-IP from <pending> to an
    actual IP.](image/Figure_5.3.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.3: Get the service''s public IP'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Go to `http://<EXTERNAL-IP>` as shown in *Figure 5.4*:![Once you enter the public
    IP in the address bar of your browser, it will open a white screen with the word
    Guestbook written in bold. This is a sign that your application is now running.](image/Figure_5.4.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.4: Ensure the application is running'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let''s see where the Pods are currently running using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This will generate an output as shown in *Figure 5.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![When you execute the kubectl get pods -o wide command, the output displayed
    will show that the Pods are spread between nodes 0 and 1.](image/Figure_5.5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.5: Our Pods are spread between node 0 and node 1'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This shows us that we have the workload spread between node 0 and node 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we want to demonstrate how Kubernetes handles a node failure.
    To demonstrate this, we will shut down a node in the cluster. In this case, we
    are going for maximum damage, so let''s shut down node 1 (you can choose whichever
    node you want – for illustration purposes, it doesn''t really matter):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To shut down this node, look for `Virtual machine scale sets` back in our cluster
    in the Azure search bar, as shown in *Figure 5.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Type vmss in the Azure search bar to shut down the node.](image/Figure_5.6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.6: Looking for the VMSS hosting your cluster'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'After navigating to the blade for the scale set, go the **Instances** view,
    select the instance you want to shut down, and then hit the **Deallocate** button,
    as shown in *Figure 5.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Click on the Instances tab in the Navigation pane in the Azure portal. This
    will display the number of instances. Select the one you want to shutdown. Click
    on the deallocate icon that appears in the address bar.](image/Figure_5.7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.7: Shutdown node 1'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This will shut down our node. To see how Kubernetes will react with your Pods,
    you can watch the Pods in your cluster via the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'To verify whether your application can continue to run, you can optionally
    run the following command to hit the guestbook front end every 5 seconds and get
    the HTML. It''s recommended to open this in a new Cloud Shell window:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The preceding command will keep calling your application till you press *Ctrl*
    + *C* (*command* + *C* on Mac). There might be intermittent times where you don't
    get a reply, which is to be expected as Kubernetes takes a couple of minutes to
    rebalance the system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add some guestbook entries to see what happens to them when you cause the node
    to shut down. This will display an output as shown in *Figure 5.8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Use the guestbook application to write a couple of messages.](image/Figure_5.8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.8: Writing a couple of messages in the guestbook'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: What you can see is that all your precious messages are gone! This shows the
    importance of having **PersistentVolumeClaims** (**PVCs**) for any data that you
    want to survive in the case of a node failure, which is not the case in our application
    here. You will see an example of this in the last section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: After a while, the output of watching the Pods should have shown additional
    output, showing you that the Pods got rescheduled on the healthy host, as shown
    in *Figure 5.9*:![This will display an output that shows that the Pods from the
    failed node arerecreated.](image/Figure_5.9.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.9: The Pods from the failed node getting recreated'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'What you see here is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: One of the front end Pods running on host 1 got terminated as the host became
    unhealthy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A new front end Pod got created, on host 0\. This went through the stages **Pending**,
    **ContainerCreating**, and then **Running**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Kubernetes picked up that the host was unhealthy before it rescheduled the Pods.
    If you were to do `kubectl get nodes`, you would see node 1 in a `NotReady` state.
    There is a configuration in Kubernetes called pod-eviction-timeout that defines
    how long the system will wait to reschedule Pods on a healthy host. The default
    is 5 minutes.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you learned how Kubernetes automatically handles node failures
    by recreating Pods on healthy nodes. In the next section, you will learn how you
    can diagnose and solve out-of-resource issues.
  prefs: []
  type: TYPE_NORMAL
- en: Solving out-of-resource failures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another common issue that can come up with Kubernetes clusters is the cluster
    running out of resources. When the cluster doesn't have enough CPU power or memory
    to schedule additional Pods, Pods will become stuck in a `Pending` state.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes uses `requests` to calculate how much CPU power or memory a certain
    Pod requires. Our guestbook application has requests defined for all the deployments.
    If you open the `guestbook-all-in-one.yaml` file, you''ll see the following for
    the `redis-slave` deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This section explains that every Pod for the `redis-slave` deployment requires
    `100m` of a CPU core (100 milli or 10%) and 100MiB (Mebibyte) of memory. In our
    1 CPU cluster (with node 1 shut down), scaling this to 10 Pods will cause issues
    with the available resources. Let''s look into this:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In Kubernetes, you can use either the binary prefix notation or the base 10
    notation to specify memory and storage. Binary prefix notation means using KiB
    (kibibyte) to represent 1,024 bytes, MiB (mebibyte) to represent 1,024 KiB, and
    Gib (gibibyte) to represent 1,024 MiB. Base 10 notation means using kB (kilobyte)
    to represent 1,000 bytes, MB (megabyte) to represent 1,000 kB, and GB (gigabyte)
    represents 1,000 MB.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by scaling the `redis-slave` deployment to 10 Pods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This will cause a couple of new Pods to be created. We can check our Pods using
    the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This will generate an output as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using thekubectl get pods command, you can check your Pods. The output will
    generate some Pods whose status isPending.](image/Figure_5.10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.10: If the cluster is out of resources, Pods will go into the Pending
    state'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Highlighted here is one of the Pods that are in the `Pending` state.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can get more information about these pending Pods using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This will show you more details. At the bottom of the `describe` command, you
    should see something like what''s shown in *Figure 5.11*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The output displays an event with a FailedSchedulingmessage from the default-scheduler.
    A detailed message shows "0/2 nodes are available: 1 insufficient CPU, 1 node(s)
    had taints that the pod didn''t tolerate."](image/Figure_5.11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.11: Kubernetes is unable to schedule this Pod'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'It explains two things to us:'
  prefs: []
  type: TYPE_NORMAL
- en: One of the nodes is out of CPU resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the nodes has a taint that the Pod didn't tolerate. This means that the
    node that is `NotReady` can't accept Pods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can solve this capacity issue, by starting up node 1 as shown in *Figure
    5.12*. This can be done in a way similar to the shutdown process:![You can start
    the node by using the same procedure that you used for the shutdown. Click on
    the Instances tab in the Navigation blade. Select the node that you want to start.
    Lastly, click on the Start button in the toolbar.](image/Figure_5.12.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.12: Start node 1 again'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It will take a couple of minutes for the other node to become available again
    in Kubernetes. If we re-execute the `describe` command on the previous Pod, we'll
    see an output like what's shown in *Figure 5.13*:![The events output from describing
    the pod shows that after some time, the scheduler assigned the pod to the new
    node, and shows the process of pulling an image, as well as creating and starting
    the container.](image/Figure_5.13.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.13: When the node is available again, the other Pods get started on
    the new node'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This shows that after node 1 became available, Kubernetes scheduled our Pod
    on that node, and then started the container.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned how to diagnose out-of-resource errors. We were
    able to solve the error by adding another node to the cluster. Before we move
    on to the final failure mode, we will clean up the guestbook deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In *Chapter 4*, *Scaling your Application*, we discussed the cluster autoscaler.
    The cluster autoscaler will monitor out-of-resource errors and add new nodes to
    the cluster automatically.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s clean up by running the following `delete` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: So far, we have discussed two failure modes for nodes in a Kubernetes cluster.
    First, we discussed how Kubernetes handles a node going offline and how the system
    reschedules Pods to a working node. After that, we saw how Kubernetes uses requests
    to schedule Pods on a node, and what happens when a cluster is out of resources.
    In the next section, we'll cover another failure mode in Kubernetes, namely what
    happens when Kubernetes moves Pods with PVCs attached.
  prefs: []
  type: TYPE_NORMAL
- en: Fixing storage mount issues
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Earlier in this chapter, you noticed how the guestbook application lost data
    when the Redis master was moved to another node. This happened because that sample
    application didn't use any persistent storage. In this section, we'll cover an
    example of how PVCs can be used to prevent data loss when Kubernetes moves a Pod
    to another node. We will show you a common error that occurs when Kubernetes moves
    Pods with PVCs attached, and we will show you how to fix this.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this, we will reuse the WordPress example from the previous chapter. Before
    we start, let''s make sure that the cluster is in a clean state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This shows us just the one Kubernetes service, as shown in *Figure 5.14*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Executing the kubectl get all command generates an output showing only one
    Kubernetes service running for now.](image/Figure_5.14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.14: You should only have the Kubernetes service running for now'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let''s also ensure that both nodes are running and `Ready`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This should show us both nodes in a `Ready` state, as shown in *Figure 5.15*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![You should now see the statuses of the two nodes are Ready.](image/Figure_5.15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.15: You should have two nodes available in your cluster'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the previous example, under the *Handling node failures* section, we saw
    that the messages stored in `redis-master` were lost if the Pod gets restarted.
    The reason for this is that `redis-master` stores all data in its container, and
    whenever it is restarted, it uses the clean image without the data. In order to
    survive reboots, the data has to be stored outside. Kubernetes uses PVCs to abstract
    the underlying storage provider to provide this external storage.
  prefs: []
  type: TYPE_NORMAL
- en: To start this example, we'll set up the WordPress installation.
  prefs: []
  type: TYPE_NORMAL
- en: Starting the WordPress installation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s start by installing WordPress. We will demonstrate how it works and
    then verify that storage is still present after a reboot:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Begin reinstallation by using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This will take a couple of minutes to process. You can follow the status of
    this installation by executing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'After a couple of minutes, this should show us Pods with a status of `Running`
    and with a ready status of `1/1` for both Pods, as shown in *Figure 5.16*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using kubectl get pods -w, you will see the pods transition from ContainerCreating
    to Runningstatus, and you will see the numberof Ready pods change from 0/1 to
    1/1.](image/Figure_5.16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.16: All Pods will have the status of Running after a couple of minutes'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this section, we saw how to install the WordPress. Now, we will see how to
    avoid data loss using the persistent volumes.
  prefs: []
  type: TYPE_NORMAL
- en: Using persistent volumes to avoid data loss
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A **persistent volume** (**PV**) is the way to store persistent data in the
    cluster with Kubernetes. We explained PVs in more detail in *Chapter 3*, *Application
    deployment on AKS*. Let''s explore the PVs created for our WordPress deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, run the following `describe nodes` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Scroll through the output until you see a section that is similar to *Figure
    5.17*. In our case, both WordPress Pods are running on node 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '![When you execute the kubectl describe nodes command, you will see the information
    that indicates both the Pods are running on node 0.](image/Figure_5.17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.17: In our case, both WordPress Pods are running on node 0'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Your Pod placement might vary.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next thing we can check is the status of our PVCs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This will generate an output as shown in *Figure 5.18*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The output displays two PVCs. Alongside their names, you can also see the
    Status, Volume, Capacity, and the Access mode of these PVCs.](image/Figure_5.18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.18: Two PVCs are created by the WordPress deployment'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The following command shows the actual PV that is bound to the Pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This will show you the details of both volumes. We will show you one of those
    in *Figure 5.19*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using the kubectl describe pvc command, you can see the two PVCs in detail.
    The picture highlights the claim as default/data-wp-mariadb-0, and it highlights
    the diskURI in Azure.](image/Figure_5.19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.19: The details of one of the PVCs'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Here, we can see which Pod has claimed this volume and what the **DiskURI**
    is in Azure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Verify that your site is actually working:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This will show us the public IP of our WordPress site, as shown in *Figure
    5.20*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The output screen displays the External-IP for the wp-WordPress service only.](image/Figure_5.20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.20: Get the service''s public IP'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'If you remember from *Chapter 3*, *Application deployment of AKS*, Helm showed
    us the commands we need to get the admin credentials for our WordPress site. Let''s
    grab those commands and execute them to log on to the site as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This will show you the `username` and `password`, as displayed in *Figure 5.21*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The output shows how to get the username and password through Helm. The username
    in the screenshot is user, and the password in the screenshot is lcsUSJTk8e. The
    password in your case will differ.](image/Figure_5.21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.21: Getting the username and password for the WordPress application'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You might notice that the commands we use in the book are slightly different
    than the ones that `helm` returned. The command that `helm` returned for the password
    didn't work for us, and we provided you with the working commands.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can log in to our site via the following address: `http://<external-ip>/admin`.
    Log in here with the credentials from the previous step. Then you can go ahead
    and add a post to your website. Click the **Write your first blog post** button,
    and then create a short post, as shown in *Figure 5.22*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![You will see a dashboard that welcomes you to WordPress. Here, you will see
    a button that says- Write your first blog post. Click on it to begin writing.](image/Figure_5.22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.22: Writing your first blog post'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Type a little text now and hit the **Publish** button, as shown in *Figure
    5.23*. The text itself isn''t important; we are writing this to verify that data
    is indeed persisted to disk:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Let''s say you typed the word "test" at random. Once you''ve written the
    text, click on the Publish button,which is located in the top right corner of
    the screen.](image/Figure_5.23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.23: Publishing a post with random text'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you now head over to the main page of your website at `http://<external-ip>`,
    you'll see your test post as shown in *Figure 5.23*. We will verify whether this
    post survives a reboot in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: '**Handling Pod failure with PVC involvement**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first test we''ll do with our PVC is to kill the Pods and verify whether
    the data has indeed persisted. To do this, let''s do two things:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Watch our Pods in our application**. To do this, we''ll use our current Cloud
    Shell and execute the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '**Kill the two Pods that have the PVC mounted**. To do this, we''ll create
    a new Cloud Shell window by clicking on the icon shown in *Figure 5.24*:![Click
    on the icon located to the left side of the curly braces icon on the toolbar.
    Click on it to open a new Cloud Shell.](image/Figure_5.24.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 24: Opening a new Cloud Shell instance'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Once you open a new Cloud Shell, execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'If you follow along with the `watch` command, you should see an output like
    what''s shown in *Figure 5.25*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Running kubectl get pods -w shows you that the old pods get terminated and
    new pods are created. The new pods transition from the Pending state to ContainerCreating
    to Running.](image/Figure_5.25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.25: After deleting the Pods, Kubernetes will automatically recreate
    both Pods'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see, Kubernetes quickly started creating new Pods to recover from
    the Pod outage. The Pods went through a similar life cycle as the original ones,
    going from **Pending** to **ContainerCreating** to **Running**.
  prefs: []
  type: TYPE_NORMAL
- en: If you head on over to your website, you should see that your demo post has
    been persisted. This is how PVCs can help you prevent data loss, as they persist
    data that would not have been persisted in the container itself.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Figure 5.26* shows that the blog post was persisted even though the Pod was
    recreated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![You can see that your data is persisted and the blog post with the word "test"
    is still available.](image/Figure_5.26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.26: Your data is persisted, and your blog post is still there'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'One final interesting data point to watch here is the Kubernetes event stream.
    If you run the following command, you can see events related to volumes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This will generate an output as shown in *Figure 5.27*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Output screen will show a FailedAttachVolumewarning. Below it, it will show
    that the state is now normal with a SuccessfulAttachVolumemessage. This shows
    that Kubernetes was initially unable to mount the Volume, but was able to mount
    it on the next try.](image/Figure_5.27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.27: Kubernetes dealt with a FailedAttachVolume error'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This shows you the events related to volumes. There are two interesting messages
    to explain in more detail: `FailedAttachVolume` and `SuccessfulAttachVolume`.
    This shows us how Kubernetes handles volumes that have a read-write-once configuration.
    Since the characteristic is to only read and write from a single Pod, Kubernetes
    will only mount the volume to the new Pod when it is successfully unmounted from
    the current Pod. Hence, initially, when the new Pod was scheduled, it showed the
    `FailedAttachVolume` message as the volume was still attached to the Pod that
    was being deleted. Afterward, the Pod could successfully mount the volume and
    showed this with the message `SuccessfulAttachVolume`.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we've learned how PVCs can help when Pods get recreated on
    the same node. In the next section, we'll see how PVCs are used when a node has
    a failure.
  prefs: []
  type: TYPE_NORMAL
- en: '**Handling node failure with PVC involvement**'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous example, we saw how Kubernetes can handle Pod failures when
    those Pods have a PV attached. In this example, we''ll look at how Kubernetes
    handles node failures when a volume is attached:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first check which node is hosting our application, using the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We found that, in our cluster, node 1 was hosting MariaDB, and node 0 was hosting
    the WordPress site, as shown in *Figure 5.28*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Running kubectl get pods -o wide shows you which pod is running on which
    node. In the screenshot,one pod is running on each host.](image/Figure_5.28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.28: We have two Pods running in our deployment – one on node 1, one
    on node 0'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We are going to introduce a failure and stop the node that could cause the most
    damage by shutting down node 0 on the Azure portal. We'll do this the same way
    as in the earlier example. First, look for the scale set backing our cluster,
    as shown in *Figure 5.29*:![Typing vmss in the search bar on the Azure portal
    will show you the full name of the VMSS hosting your AKS cluster.](image/Figure_5.29.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.29: Finding the scale set backing our cluster'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Then shut down the node as shown in *Figure 5.30*:![To shutdown the node, click
    on the Instances tab located in the Navigation pane in the Azure portal. You will
    see two nodes. Select the first node and click on the Deallocate button on the
    toolbar.](image/Figure_5.30.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.30: Shutting down the node'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'After this action, we''ll once again watch our Pods to see what is happening
    in the cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'As in the previous example, it is going to take 5 minutes before Kubernetes
    will start taking action against our failed node. We can see that happening in
    *Figure 5.31*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![When you execute the kubectl get pods -o wide -w command, you will see that
    there are Pods with a status ofPending that are not assigned to a node.](image/Figure_5.31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.31: A Pod in a pending state'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We are seeing a new issue here. Our new Pod is stuck in a `Pending` state and
    has not been assigned to a new node. Let''s figure out what is happening here.
    First, we''ll `describe` our Pod:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'You will get an output as shown in *Figure 5.32*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![You will see the details of why this Pod is stuck in Pending state, which
    is due to insufficient CPU.](image/Figure_5.32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.32: Output displaying the Pod in a pending state'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This shows us that we don''t have sufficient CPU resources in our cluster to
    host the new Pod. We can fix this by using the `kubectl edit deploy/...` command
    to fix any insufficient CPU/memory errors. We''ll change the CPU requests from
    300 to 3 for our example to continue:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This brings us to a `vi` environment. We can quickly find the section referring
    to the CPU by typing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Once there, move the cursor over the two zeros and hit the `x` key twice to
    delete the zeros. Finally, type `:wq!` to save our changes so that we can continue
    our example.
  prefs: []
  type: TYPE_NORMAL
- en: 'This will result in a new ReplicaSet being created with a new Pod. We can get
    the name of the new Pod by entering the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Look for the Pod with the status `ContainerCreating`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The output screen displays four Pods with different statuses. Look for the
    Pod with the ContainerCreatingstatus.](image/Figure_5.33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.33: The new Pod is stuck in a ContainerCreating state'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let''s have a look at the details for that Pod with the `describe` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `Events` section of this `describe` output, you can see an error message
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Executing kubectl describe on the pod with the ContainerCreating status shows
    a detailed error message with the reason FailedMount. The message says that Kubernetes
    was unable to mount the volume.](image/Figure_5.34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.34: The new Pod has a new error message, describing volume mounting
    issues'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This tells us that the volume that our new Pod wants to mount is still mounted
    to the Pod that is stuck in a `Terminating` state. We can solve this by manually
    detaching the disk from the node we shut down and forcefully removing the Pod
    stuck in the `Terminating` state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The behavior of the Pod stuck in the `Terminating` state is not a bug. This
    is default Kubernetes behavior. The Kubernetes documentation states the following:
    "Kubernetes (versions 1.5 or newer) will not delete Pods just because a Node is
    unreachable. The Pods running on an unreachable Node enter the `Terminating` or
    `Unknown` state after a timeout. Pods may also enter these states when the user
    attempts the graceful deletion of a Pod on an unreachable Node." You can read
    more at [https://kubernetes.io/docs/tasks/run-application/force-delete-stateful-set-pod/](https://kubernetes.io/docs/tasks/run-application/force-delete-stateful-set-pod/).'
  prefs: []
  type: TYPE_NORMAL
- en: To do that, we'll need the scale set's name and the resource group name. To
    find those, look for the scale set in the portal as shown in *Figure 5.35*:![Type
    vmss in the Azure search bar to look for the ScaleSet that is backing your cluster](image/Figure_5.35.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.35: Look for the scale set backing your cluster'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In the scale set view, copy and paste the scale set name and the resource group.
    Edit the following command to detach the disk from your failed node and then run
    this command in Cloud Shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'This will detach the disk from node 0\. The second step required here is the
    forceful removal of the Pod from the cluster, while it is stuck in the terminating
    state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'This will bring our new Pod to a healthy state. It will take a couple of minutes
    for the system to pick up the changes and then mount and schedule the new Pod.
    Let''s get the details of the Pod again using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This will generate an output as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The output will display the event types for both the Pods as Normal. The
    reason is SuccessfulAttachVolume and Pulled for the second Pod.](image/Figure_5.36.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.36: Our new Pod is now attaching the volume and pulling the container
    image'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This shows us that the new Pod successfully got the volume attached and that
    the container image got pulled. Let''s verify that the Pod is actually running:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'This will show the Pods running as shown in *Figure 5.37*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The status of the two Pods is Running.](image/Figure_5.37.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.37: Both Pods are successfully running'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: And this should make your WordPress website available again.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before continuing, let''s clean up our deployment using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s also restart the node that was shut down, as shown in *Figure 5.38*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![To restart the node that was shutdown, click on the Instances tab in the
    Navigation pane. Select the node that is deallocated/stopped. Click on the Start
    button in the toolbar alongside the search bar. Your node should now be restarted.](image/Figure_5.38.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.38: Starting node 1 again'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this section, we covered how you can recover from a node failure when PVCs
    aren't mounted to new Pods. We needed to manually unmount the disk and then forcefully
    delete the Pod that was stuck in a `Terminating` state.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, you learned about common Kubernetes failure modes and how you
    can recover from these. We started this chapter with an example on how Kubernetes
    automatically detects node failures and how it will start new Pods to recover
    the workload. After that, you scaled out your workload and had your cluster run
    out of resources. You recovered from that situation by starting the failed node
    again to add new resources to the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you saw how PVs are useful to store data outside of a Pod. You shut down
    all Pods on the cluster and saw how the PV ensured that no data was lost in your
    application. In the final example in this chapter, you saw how you can recover
    from a node failure when PVs are attached. You were able to recover the workload
    by unmounting the disk from the node and forcefully deleting the terminating Pod.
    This brought your workload back to a healthy state.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter has explained common failure modes in Kubernetes. In the next chapter,
    we will introduce HTTPS support to our services and introduce authentication with
    Azure Active Directory.
  prefs: []
  type: TYPE_NORMAL
