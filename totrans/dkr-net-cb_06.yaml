- en: Chapter 6. Securing Container Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Enabling and disabling ICC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disabling outbound masquerading
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing netfilter to Docker integration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating custom iptables rules
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exposing services through a load balancer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you move toward container-based applications, one of the items you'll want
    to put some serious consideration toward is network security. Containers, in particular,
    can lead to a proliferation in the number of network endpoints that need to be
    secured. Granted, not all endpoints are fully exposed to the network. However,
    those that aren't, by default, talk directly to each other, which can cause other
    concerns. There are many ways to tackle network security when it comes to container-based
    applications, and this chapter doesn't aim to address all possible solutions.
    Rather, this chapter aims to review configuration options and relevant network
    topologies that can be combined in a number of different ways based on your own
    network security requirements. We'll discuss in detail some features that we were
    exposed to in earlier chapters such as ICC mode and outbound masquerading. In
    addition, we'll cover a couple of different techniques to limit the network exposure
    of your containers.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling and disabling ICC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In earlier chapters, we were exposed to the concept of ICC mode, but didn't
    have much information on the mechanics of how it worked. ICC is a Docker-native
    way of isolating all containers connected to the same network. The isolation provided
    prevents containers from talking directly to each other while still allowing their
    exposed ports to be published as well as allowing outbound connectivity. In this
    recipe, we'll review our options for ICC-based configuration in both the default
    `docker0` bridge context as well as with user-defined networks.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We'll be using two Docker hosts in this recipe to demonstrate how ICC works
    in different network configurations. It is assumed that both Docker hosts used
    in this lab are in their default configuration. In some cases, the changes we
    make may require you to have root-level access to the system.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'ICC mode can be configured on both the native `docker0` bridge as well as any
    user-defined networks that utilize the bridge driver. In this recipe, we''ll review
    how to configure ICC mode on the `docker0` bridge. As we''ve seen in earlier chapters,
    settings related to the `docker0` bridge need to be made at the service level.
    This is because the `docker0` bridge is created as part of service initialization.
    This also means that, to make changes to it, we''ll need to edit the Docker service
    configuration and then restart the service for them to take effect. Before we
    make any changes, let''s take the opportunity to review the default ICC configuration.
    To do this, let''s first view the `docker0` bridge configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It's important to remember that the `docker network` subcommand is used to manage
    all Docker networks. A common misconception is that it can only be used to manage
    user-defined networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, the `docker0` bridge is configured for ICC mode on (`true`).
    This means that Docker will not interfere or prevent containers connected to this
    bridge to talk directly to one another. To prove this out, let''s start two containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that we didn''t specify the `-P` flag, which tells Docker to not publish
    any of the containers exposed ports. Now, let''s get each container''s IP address,
    so we can validate connectivity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we know the IP addresses, we can verify that each container can access
    the other on any service in which the container is listening:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on these tests, we can assume that the containers are allowed to talk
    to each other on any protocol that is listening. This is the expected behavior
    when ICC mode is enabled. Now, let''s change the service level setting and recheck
    our configuration. To do this, set the following configuration in your systemd
    drop in file for the Docker service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now reload the systemd configuration, restart the Docker service, and check
    the ICC setting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we''ve confirmed that ICC is disabled, let''s start up our two containers
    once again and run the same connectivity tests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, we have no connectivity between the two containers. However,
    the Docker host itself is still able to access the services:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We can inspect the netfilter rules that are used to implement ICC by looking
    at the `iptables` rules `FORWARD` chain of the filter table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding bolded rule is what prevents container–to-container communication
    on the `docker0` bridge. If we had inspected this `iptables` chain before disabling
    ICC, we would have seen this rule set to `ACCEPT` as shown following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'As we saw earlier, linking containers allowed you to bypass this rule and allow
    a source container to access a target container. If we remove the two containers
    we can restart them with a link as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now if we examine the rules with `iptables`, we can see two new rules added
    to the filter table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: These two new rules allow `web2` to access `web1` on any exposed port. Notice
    how the first rule defines the access from `web2` (`172.17.0.3`) to `web1` (`172.17.0.2`)
    with a destination port of `80`. The second rule flips the IPs and specifies port
    `80` as the source port, allowing the traffic to return to `web2`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Earlier, when we discussed user-defined networks, you saw that we could pass
    the ICC flag to a user-defined bridge. However, disabling ICC mode is not currently
    supported with the overlay driver.
  prefs: []
  type: TYPE_NORMAL
- en: Disabling outbound masquerading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By default, containers are allowed to access the outside network by masquerading
    or hiding their real IP address behind that of the Docker host. This is accomplished
    through netfilter `masquerade` rules that hide container traffic behind the Docker
    host interface referenced in the next hop. We saw a detailed example of this in
    [Chapter 2](ch02.html "Chapter 2. Configuring and Monitoring Docker Networks"),
    *Configuring and Monitoring Docker Networks*, when we discussed container-to-container
    connectivity across hosts. While this type of configuration is ideal in many respects,
    there are some cases when you might prefer to disable the outbound masquerading
    capability. For instance, if you prefer to not allow your containers to have outbound
    connectivity at all, disabling masquerading would prevent containers from talking
    to the outside network. This, however, only prevents outbound traffic due to lack
    of return routing. A better option might be to treat containers like any other
    individual network endpoint and use existing security appliances to define network
    policy. In this recipe, we'll discuss how to disable IP masquerading as well as
    how to provide containers with unique IP addressing as they traverse the outside
    network.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We'll be using a single Docker host in this example. It is assumed that the
    Docker host used in this lab is in its default configuration. You'll also need
    access to change Docker service-level settings. In some cases, the changes we
    make may require you to have root-level access to the system. We'll also be making
    changes to the network equipment to which the Docker host connects.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You''ll recall that IP masquerading in Docker is handled through a netfilter
    `masquerade` rule. On a Docker host in its default configuration, we can see this
    rule by examining the ruleset with `iptables`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This rule specifies the source of the traffic as the `docker0` bridge subnet
    and only NAT traffic can be headed off the host. The `MASQUERADE` target tells
    the host to source NAT the traffic to the Docker host''s next hop interface. That
    is, if the host has multiple IP interfaces, the container''s traffic will source
    NAT to whichever interface is used as the next hop. This means that container
    traffic could potentially be hidden behind different IP addresses based on the
    Docker host interface and routing table configuration. For instance, consider
    a Docker host with two interfaces, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](graphics/B05453_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the left-hand side example, traffic is taking the default route since the
    destination of `4.2.2.2` doesn't match a more specific prefix in the host's routing
    table. In this case, the host performs a source NAT and changes the source of
    the traffic from `172.17.0.2` to `10.10.10.101` as it traverses the Docker host
    to the outside network. However, if the destination falls into `172.17.0.0/16`,
    the container traffic will instead be hidden behind the `192.168.10.101` interface,
    as shown in the example on the right.
  prefs: []
  type: TYPE_NORMAL
- en: 'The default behavior of Docker can be changed by manipulating the `--ip-masq`
    Docker option. By default, the option is considered to be `true` and can be overridden
    by specifying the option and setting it to `false`. We can do this by specifying
    the option in our Docker systemd drop in file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now reload the systemd configuration, restart the Docker service, and check
    the ICC setting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that the `masquerade` rule is now gone. Traffic generated from a container
    on this host would attempt to route out through the Docker host with its actual
    source IP address. A `tcpdump` on the Docker host would capture this traffic exiting
    the host''s `eth0` interface with the original container IP address:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Since the outside network doesn't know where `172.17.0.0/16` is, this request
    will never receive a response effectively preventing the container from communicating
    to the outside world.
  prefs: []
  type: TYPE_NORMAL
- en: While this may be a useful means to prevent communication to the outside world,
    it's not entirely ideal. For starters, you're still allowing the traffic out;
    the response just won't know where to go as it attempts to return to the source.
    Also, you've impacted all of the containers, from all networks, on the Docker
    host. If the `docker0` bridge had a routable subnet allocated to it, and the outside
    network knew where that subnet lived, you could use existing security tooling
    to make security policy decisions.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, let''s assume that the `docker0` bridge were to be allocated
    a subnet of `172.10.10.0/24` and we left IP masquerading disabled. We could do
    this by changing the Docker options to also specify a new bridge IP address:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'As before, traffic leaving a container and destined for the outside network
    would be unchanged as it traversed the Docker host. Let''s assume a small network
    topology, as the one shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](graphics/B05453_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s assume a flow from the container to `4.2.2.2`. In this case, egress
    traffic should work inherently:'
  prefs: []
  type: TYPE_NORMAL
- en: Container generates traffic toward `4.2.2.2` and uses its default gateway that
    is the `docker0` bridge IP address
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Docker host does a route lookup, fails to find a specific prefix match,
    and forwards the traffic to its default gateway that is the switch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The switch does a route lookup, fails to find a specific prefix match, and forwards
    the traffic to its default route that is the firewall.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The firewall does a route lookup, fails to find a specific prefix match, ensures
    that the traffic is allowed in the policy, performs a hide NAT to a public IP
    address, and forwards the traffic to its default route that is the Internet.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So without any additional configuration, egress traffic should reach its destination.
    The problem is with the return traffic. When the response from the Internet destination
    gets back to the firewall, it will attempt to determine how to route back to the
    source. This route lookup will likely fail causing the firewall to drop the traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In some cases, edge network equipment (the firewall in this case) routes all
    private IP addressing back to the inside (the switch in this case). In those scenarios,
    the firewall might forward the return traffic to the switch, but the switch won't
    have a specific return route causing the same problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order for this to work, the firewall and the switch need to know how to
    return the traffic to the specific container. To do this, we need to add specific
    routes on each device pointing the `docker0` bridge subnet back to the `docker1`
    host:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](graphics/B05453_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Once these routes are in place, containers spun up on the Docker host should
    have connectivity to outside networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'A `tcpdump` on the Docker host will show that the traffic is leaving with the
    original container IP address:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This type of configuration offers the ability to use existing security appliances
    to decide whether containers can reach certain resources on the outside networks.
    However, this is also a function of how close the security appliance is to your
    Docker host. For instance, in this configuration the containers on the Docker
    host would be able to reach any other network endpoints connected to the switch.
    The enforcement point (the firewall, in this example) only allows you to limit
    the container's connectivity to the Internet. In addition, assigning routable
    IP space each Docker host might introduce IP assignment constraints if you have
    large scale.
  prefs: []
  type: TYPE_NORMAL
- en: Managing netfilter to Docker integration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By default, Docker performs most of the netfilter configuration for you. It
    takes care of things such as publishing ports and outbound masquerading, as well
    as allows you to block or allow ICC. However, this is all optional and you can
    tell Docker not to modify or add to any of your existing `iptables` rules. If
    you do this, you'll need to generate your own rules to provide similar functionality.
    This may be appealing to you if you're already using `iptables` rules extensively
    and don't want Docker to automatically make changes to your configuration. In
    this recipe we'll discuss how to disable automatic `iptables` rule generation
    for Docker and show you how to manually create similar rules.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We'll be using a single Docker host in this example. It is assumed that the
    Docker host used in this lab is in its default configuration. You'll also need
    access to change Docker service-level settings. In some cases, the changes we
    make may require you to have root-level access to the system.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we''ve already seen, Docker takes care of a lot of the heavy lifting for
    you when it comes to network configuration. It also allows you the ability to
    configure these things on your own if need be. Before we look at doing it ourselves,
    let''s confirm what Docker is actually configuring on our behalf with regard to
    `iptables` rules. Let''s run the following containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Running these containers would yield the following topology:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](graphics/B05453_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The examples given later will not use the host's `eth1` interface directly.
    It is displayed to illustrate how the rules generated by Docker are written in
    a manner that encompasses all physical interfaces on the Docker host.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we''ve mentioned before, Docker uses `iptables` to handle the following
    items:'
  prefs: []
  type: TYPE_NORMAL
- en: Outbound container connectivity (masquerading)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inbound port publishing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Container–to-container connectivity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since we''re using the default configuration and we have published ports on
    both containers, we should be able to see all three of these items configured
    in `iptables`. Let''s take a look at the NAT table to start with:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In most cases, I prefer to print the rules and interpret them rather than have
    them listed in formatted columns. There are trade-offs to each approach, but if
    you prefer the list mode, you can replace the `-S` with `-vL`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s review the importance of each of the bolded lines in the preceding output.
    The first bolded line takes care of the outbound hide NAT or `MASQUERADE`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The rule is looking for traffic that matches two characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: The source IP address must match the IP address space of the `docker0` bridge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The traffic is not exiting through the `docker0` bridge. That is, it's leaving
    through another interface such as `eth0` or `eth1`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The jump statement at the end specifies a target of `MASQUERADE`, which will
    source NAT the container traffic to one of the host's IP interfaces based on the
    routing table.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next two bolded lines provide similar functionality and provide the NAT
    required for publishing ports on each respective container. Let''s examine one
    of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The rule is looking for traffic that matches three characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: The traffic is not entering through the `docker0` bridge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The traffic is TCP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The traffic has a destination port of `32768`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The jump statement at the end specifies a target of `DNAT` and a destination
    of the container with its real service port (`80`). Notice that both of these
    rules are generic in terms of the Docker host's physical interfaces. As we saw
    earlier, both port publishing and outbound masquerading can occur on any interface
    on the host unless we 'specifically limit the scope.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next table we want to review is the filter table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Again, you'll note that the chain policy is set to `ACCEPT` for the default
    chains. In the case of the filter table, it has a more drastic impact on functionality.
    This means that everything is being allowed unless specifically denied in a rule.
    In other words, if there were no rules defined everything would still work. Docker
    inserts these rules in case your default policy is not set to `ACCEPT`. Later
    on, when we manually create the rules, we'll set the default policy to `DROP`
    so that you can see the impact the rules have. The preceding rules require a little
    more explaining, especially if you aren't familiar with how `iptables` rules work.
    Let's review the bolded lines one at a time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first bolded line takes care of allowing traffic from the outside network
    back into the containers. In this case, the rule is specific to instances where
    the container itself is generating traffic toward, and expecting a response, from
    the outside network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The rule is looking for traffic that matches two characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: The traffic is exiting through the `docker0` bridge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The traffic has a connection state of `RELATED` or `ESTABLISHED`. This would
    include sessions that are part of an existing flow or related to it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The jump statement at the end references a target of `ACCEPT`, which will allow
    the flow through.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second bolded line allows the container''s connectivity to the outside
    network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The rule is looking for traffic that matches two characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: The traffic is entering through the `docker0` bridge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The traffic is not exiting through the `docker0` bridge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is a very generic way of identifying traffic that came from the containers
    and is leaving through any other interface than the `docker0` bridge. The jump
    statement at the end references a target of `ACCEPT`, which will allow the flow
    through. This rule, in conjunction with the first rule, will allow a flow generated
    from a container toward the outside network to work.
  prefs: []
  type: TYPE_NORMAL
- en: 'The third bolded line allows inter-container connectivity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The rule is looking for traffic that matches two characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: The traffic is entering through the `docker0` bridge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The traffic is exiting through the `docker0` bridge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is another generic means to identify traffic that is originated from a
    container on the `docker0` bridge as well as destined for a target on the `docker0`
    bridge. The jump statement at the end references a target of `ACCEPT`, which will
    allow the flow through. This is the same rule that's turned into a `DROP` target
    when you disable ICC mode as we saw in earlier chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last two bolded lines allow the published ports to reach the containers.
    Let''s examine one of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The rule is looking for traffic that matches five characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: The traffic is destined to the container whose port was published
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The traffic is not entering through the `docker0` bridge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The traffic is exiting through the `docker0` bridge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The protocol is TCP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The port number is `80`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This rule specifically allows the published port to work by allowing access
    to the container's service port (`80`). The jump statement at the end references
    a target of `ACCEPT`, which will allow the flow through.
  prefs: []
  type: TYPE_NORMAL
- en: Manually creating the required iptables rules
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we''ve seen how Docker automatically handles rule generation, let''s
    walk through an example of how to build this connectivity on our own. To do this,
    we first need to instruct Docker to not create any `iptables` rules. To do this,
    we set the `--iptables` Docker option to `false` in the Docker systemd drop in
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll need to reload the systemd drop in file and restart the Docker service
    for Docker to reread the service parameters. To ensure that you start with a blank
    slate, if possible, restart the server or flush all the `iptables` rules out manually
    (if you''re not comfortable with managing the `iptables` rules, the best approach
    is just to reboot the server to clear them out). We''ll assume for the rest of
    the example that we''re working with an empty ruleset. Once Docker is restarted,
    you can restart your two containers and ensure that there are no `iptables` rules
    present on the system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, there are no `iptables` rules currently defined. We can also
    see that our default chain policy in the filter table is set to `ACCEPT`. Let''s
    now change the default policy in the filter table to `DROP` for each chain. Along
    with that, let''s also include a rule to allow SSH to and from the host so as
    not to break our connectivity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now check the filter table once again to make sure that the rules were
    accepted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, the containers `web1` and `web2` will no longer be able to reach
    each other:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Depending on your operating system, you might notice that `web1` actually is
    able to ping `web2` at this point. The most likely reason for this is that the
    `br_netfilter` kernel module has not been loaded. Without this module bridged
    packets will not be inspected by netfilter. To resolve this, you can manually
    load the module by using the `sudo modprobe br_netfilter` command. To make the
    module load at each boot, you could add it to the `/etc/modules` file as well.
    When Docker is managing the `iptables` ruleset, it takes care of loading the module
    for you.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s start building the ruleset to recreate the connectivity that Docker
    previously built for us automatically. The first thing we want to do is allow
    containers inbound and outbound access. We''ll do that with these two rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Although these two rules will allow containers to generate and receive traffic
    from the outside network, the connectivity still won''t work at this point. In
    order for this to work, we need to apply the `masquerade` rule so that the container
    traffic will be hidden behind an interface on the `docker0` host. If we don''t
    do this, the traffic will never get returned as the outside network knows nothing
    about the `172.17.0.0/16` network in which the containers live:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'With this in place, the containers will now be able to reach network endpoints
    on the outside network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'However, the containers still cannot communicate directly with each other:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to add one final rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Since traffic between containers both enters and leaves the `docker0` bridge,
    this will allow the inter-container connectivity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The only configuration remaining is to provide a mechanism to publish ports.
    We can do that by first provisioning a destination NAT on the Docker host itself.
    Even though Docker is not provisioning the NAT rules, it''s still keeping track
    of the port allocations on your behalf. At container runtime if you choose to
    publish a port, Docker will allocate a port mapping for you even though it is
    not handling the publishing. It is wise to use the port Docker allocates to prevent
    overlaps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the ports Docker allocated, we can define an inbound NAT rule for each
    container that translates inbound connectivity to an external port on the Docker
    host to the real container IP and service port. Finally, we just need to allow
    inbound traffic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Once these rules are configured, we can now test the connectivity from outside
    the Docker host on the published ports:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Manually creating the required iptables rules](graphics/B05453_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Creating custom iptables rules
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, we covered how Docker handles `iptables` rules for the
    most common container networking needs. However, there may be cases where you
    wish to extend the default `iptables` configuration to either allow more access
    or limit the scope of connectivity. In this recipe, we'll walk through a couple
    of examples of how to implement custom `iptables` rules. We'll focus on limiting
    the scope of sources connecting to services running on your containers as well
    as allowing the Docker host itself to connect to those services.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The examples provided later are designed to demonstrate the options you have
    to configure `iptables` rulesets. The way they are implemented in these examples
    may or may not make sense in your environment and can be deployed in different
    ways and places based on your security needs.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We'll be using the same Docker host with the same configuration from the previous
    recipe. The Docker service should be configured with the `--iptables=false` service
    option, and there should be two containers defined—`web1` and `web2`. If you are
    unsure how to get to this state, please see the previous recipe. In order to define
    a new `iptables` policy, we'll also need to flush all the existing `iptables`
    rules out of the NAT and the FILTER table. The easiest way to do this is to reboot
    the host.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Flushing the `iptables` rules when your default policy is deny will disconnect
    any remote administration sessions. Be careful not to accidentally disconnect
    yourself if you are managing the system without console access!
  prefs: []
  type: TYPE_NORMAL
- en: 'If you prefer not to reboot, you can change the default filter policy back
    to `allow`. Then, flush the filter and NAT table as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At this point, you should once again have a Docker host with two containers
    running and an empty default `iptables` policy. To begin, let''s once again change
    the default filter policy to `deny` while ensuring that we still allow our management
    connection over SSH:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Because we''ll be focusing on policy around the filter table, let''s put in
    the NAT policy unchanged from the previous recipe. These NATs cover both outbound
    masquerading and inbound masquerading for the destination NATs for the service
    in each container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'One of the items you might be interested in configuring is limiting the scope
    of what the containers can access on the outside network. You''ll notice that,
    in previous examples, the containers were allowed to talk to anything externally.
    This was allowed since the filter rule was rather generic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'This rule allows the containers to talk to anything out of any interface besides
    `docker0`. Rather than allowing this, we can specify only the ports we want to
    allow outbound. So for instance, if we publish port `80`, we can then define a
    reverse or outbound rule, which only allows that specific return traffic. Let''s
    first recreate the inbound rules we used in the previous example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can easily replace the more generic outbound rule with specific rules
    that only allow the return traffic on port `80`. For example, let''s put in a
    rule that allows the container `web1` to return traffic only on port `80`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'If we check, we should see that from the outside network we can get to the
    service on `web1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](graphics/B05453_06_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'However, the container `web1` is not able to talk to anything on the outside
    network except on port `80` at this point because we didn''t use the generic outbound
    rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'To fix this, we can add specific rules to allow things like ICMP sourced from
    the `web1` container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: The above rule coupled with the state-aware return rule from the previous recipe
    will allow the web1 container to initiate and receive return ICMP traffic.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'In the case of the `web2` container, its web server can still not be accessed
    from the outside network. If we wish to limit the source of traffic which can
    talk to the web server we could do that by altering the inbound port `80` rule,
    or by specifying a destination in the outbound port `80` rule. We could for instance
    limit the source of the traffic to a single device on the outside network by specifying
    a destination in the egress rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Now if we try from a lab device on the outside network with the IP address
    of `10.20.30.13`, we should be able to access the web server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'But if we try from a different lab server with a different IP address, the
    connection will fail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Again, this rule could be implemented either as an inbound or outbound rule.
  prefs: []
  type: TYPE_NORMAL
- en: 'When managing the `iptables` rules in this manner, you might have noticed that
    the Docker host itself no longer has connectivity to the containers and the services
    they are hosting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'This is because all of the rules we''ve been writing in the filter table have
    been in the forward chain. The forward chain only applies to traffic the host
    is forwarding, not traffic that is originated or destined to the host itself.
    To fix this, we can put rules in the `INPUT` and `OUTPUT` chains of the filter
    table. To allow ICMP to and from the containers, we can specify rules like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The rule being added to the output chain looks for traffic headed out of the
    `docker0` bridge (toward the containers), that is of protocol ICMP, and is a new
    or established flow. The rule being added to the input chain looks for traffic
    headed into the `docker0` bridge (toward the host), that is of protocol ICMP,
    and is an established flow. Since the traffic is being originated from the Docker
    host, these rules will match and allow the ICMP traffic to the container to work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'However, this still will not allow the containers themselves to ping the default
    gateway. This is because the rule we added to the input chain matching traffic
    coming into the `docker0` bridge is only looking for established sessions. To
    have this work bidirectionally, you''d need to add the `NEW` flag to the second
    rule so that it would also match new flows generated by the containers toward
    the host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the rule we added to the output chain already specifies new or established
    flows, ICMP connectivity from the containers to the host will now also work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Exposing services through a load balancer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another way to isolate your containers is to frontend them with a load balancer.
    This mode of operation offers several advantages. First, the load balancer can
    provide intelligent load balancing to multiple backend nodes. If a container dies,
    the load balancer can remove it from the load balancing pool. Second, you're effectively
    hiding your containers behind a load balancing **Virtual IP** (**VIP**) address.
    Clients believe that they are interacting directly with the application running
    in the container while they are actually interacting with the load balancer. In
    many cases, a load balancer can provide or offload security features, such as
    SSL and web application firewall that make scaling a container-based application
    easier to accomplish in a secure fashion. In this recipe, we'll learn how this
    can be done and some of the features available in Docker that make this easier
    to do.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We'll be using multiple Docker hosts in the following examples. We'll also be
    using a user-defined overlay network. It will be assumed that you know how to
    configure the Docker hosts for overlay networking. If you do not, please see the
    *Creating a user-defined overlay network* recipe in [Chapter 3](ch03.html "Chapter 3. User-Defined
    Networks"), *User-Defined Networks*.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Load balancing is not a new concept and is one that is well-understood in the
    physical and virtual machine space. However, load balancing with containers adds
    in an extra layer of complexity, which can make things drastically more complicated.
    To start with, let''s look how load balancing typically works without containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](graphics/B05453_06_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In this case, we have a simple load balancer configuration where the load balancer
    is providing VIP for a single backend pool member (`192.168.50.150`). The flow
    works like this:'
  prefs: []
  type: TYPE_NORMAL
- en: The client generates a request toward the VIP (10.10.10.150) hosted on the load
    balancer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The load balancer receives the request, ensures that it has VIP for that IP,
    and then generates a request to the backend pool member(s) for that VIP on behalf
    of the client
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The server receives the request sourced from the load balancer and responds
    directly back to the load balancer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The load balancer then responds back to the client
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In most cases, the conversation involves two distinct sessions, one between
    the client and the load balancer and another between the load balancer and the
    server. Each of these is a distinct TCP session.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s show an example of how this might work in the container space.
    Examine the topology shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](graphics/B05453_06_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In this example, we''ll be using both container-based application servers as
    backend pool members as well as a container-based load balancer. Let''s make the
    following assumptions:'
  prefs: []
  type: TYPE_NORMAL
- en: The hosts `docker2` and `docker3` will provide hosting for many different web
    presentation containers that support many different VIPs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will use one load balancer container (`haproxy` instance) for each VIP we
    wish to define
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each presentation server exposes port `80`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given this, we can assume that host network mode is out of the question for
    both the load balancer host (`docker1`) as well as the hosting hosts (`docker2`
    and `docker3`) since it would require containers exposing services on a large
    number of ports. Before the introduction of user-defined networks, this would
    leave us with having to deal with port mapping on the `docker0` bridge.
  prefs: []
  type: TYPE_NORMAL
- en: 'That would quickly become a problem both to manage as well as troubleshoot.
    For instance, the topology might really look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](graphics/B05453_06_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In this case, the load balancer VIP would be a published port on the host `docker1`,
    that is, `32769`. The web servers themselves are also publishing ports to expose
    their web servers. Let''s walk through what a load balancing request might look
    like:'
  prefs: []
  type: TYPE_NORMAL
- en: A client from the outside network generates a request to `http://docker1.lab.lab:32769`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `docker1` host receives the request and translates the packet through the
    published port on the `haproxy` container. This changes the destination IP and
    port to `172.17.0.2:80`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `haproxy` container receives the request and determines that the VIP being
    accessed has a backend pool containing `docker2:23770` and `docker3:32771`. It
    selects the `docker3` host for this session and sends a request towards `docker3:32771`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As the request traverses the host `docker1`, it performs an outbound `MASQUERADE`
    and hides the container behind the host's IP interface.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The request is sent to the host's default gateway (the MLS), which, in turn,
    forwards the request down to the host `docker3`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `docker3` host receives the request and translates the packet through the
    published port on the `web2` container. This changes the destination IP and port
    to `172.17.0.3:80`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `web2` container receives the request and responds back toward `docker1`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `docker3` host receives the reply and translates the packet back through
    the inbound published port.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The request is received at `docker1` translated back through the outbound `MASQUERADE`,
    and is delivered at the `haproxy` container.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `haproxy` container then responds back to the client. The `docker1` host
    translates the `haproxy` container's response back to its own IP address on port
    `32769` and the response makes its way back to the client.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While doable, it's a lot to keep track of. In addition, the load balancer node
    needs to be aware of the published port and IP address of each backend container.
    If a container gets restarted, the published port can change effectively making
    it unreachable. Troubleshooting this with a large backend pool would be a headache
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'So while this is certainly doable, the introduction of user-defined networks
    can make this much more manageable. For instance, we could leverage an overlay
    type network for the backend pool members and completely remove the need for much
    of the port publishing and outbound masquerading. That topology would look more
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](graphics/B05453_06_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s see what it would take to build this kind of configuration. The first
    thing we need to do is to define a user-defined overlay type network on one of
    the nodes. We''ll define it on `docker1` and call it `presentation_backend`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that how I passed the `--internal` flag when I created this network. You'll
    recall from [Chapter 3](ch03.html "Chapter 3. User-Defined Networks"), *User-Defined
    Networks*, that this means that only containers connected to this network will
    be able to access it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next thing we want to do is to create the two web containers which will
    serve as the backend pool members for the load balancer. We''ll do that on hosts
    `docker2` and `docker3`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'The remaining component left to deploy is the load balancer. As mentioned,
    `haproxy` has a container image of their load balancer, so we''ll use that for
    this example. Before we run the container we need to come up with a configuration
    that we can pass into the container for `haproxy` to use. This is done through
    mounting a volume into the container as we''ll see shortly. The configuration
    file is named `haproxy.cfg` and my example configuration looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'A couple of items are worth pointing out in the preceding configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: We bind the `haproxy` service to all interfaces on port `80`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any request hitting the container on port `80` will get load balanced to a pool
    named `pres_containers`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `pres_containers` pool load balances in a round-robin method between two
    servers:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`web1` on port `80`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`web2` on port `80`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the interesting items here is that we can define the pool members by
    name. This is a huge advantage that comes along with user-defined networks and
    means that we don't need to worry about tracking container IP addressing.
  prefs: []
  type: TYPE_NORMAL
- en: 'I put this config file in a folder in my home directory named `haproxy`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the configuration file is in pace, we can run the container as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: You might be wondering why I'm specifying a port mapping when connecting the
    container to an `internal` type network. Recall from earlier chapters that port
    mappings are global across all network types. In other words, even though I'm
    not using it currently, it's still a characteristic of the container. So if I
    ever connect a network type to the container that can use the port mapping, it
    will. In this case, I first need to connect the container to the overlay network
    to ensure that it has reachability to the backend web servers. If the `haproxy`
    container is unable to resolve the pool member names when it starts, it will fail
    to load.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, the `haproxy` container has reachability to its pool members,
    but we have no way to access the `haproxy` container externally. To do that, we''ll
    connect another interface to the container that can use the port mapping. In this
    case, that will be the `docker0` bridge:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, the `haproxy` container should be available externally at the
    following URLs:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load balanced VIP: `http://docker1.lab.lab`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'HAProxy stats: `http://docker1.lab.lab/lbstats`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If we check the stats page, we should see that the `haproxy` container can
    reach each backend web server across the overlay. We can see that the health check
    for each is coming back with a `200 OK` status:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](graphics/B05453_06_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now if we check VIP itself and hit refresh a couple of times, we should see
    the web page presented from each container:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](graphics/B05453_06_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This type of topology provides us several notable advantages over the first
    concept we had around container load balancing. The use of the overlay-based network
    not only provided name-based resolution of containers but also significantly reduced
    the complexity of the traffic path. Granted, the traffic took the same physical
    path in either case, but we didn't need to rely on so many different NATs for
    the traffic to work. It also made the entire solution far more dynamic. This type
    of design can be easily replicated to provide load balancing for many different
    backend overlay networks.
  prefs: []
  type: TYPE_NORMAL
