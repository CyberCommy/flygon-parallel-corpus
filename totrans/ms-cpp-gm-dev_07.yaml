- en: Advanced Rendering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Often, the first impression that a player will get off your game is from the
    visuals on the screen. Having a strong understanding of creating advanced rendering
    techniques is crucial in building a compelling and immersive experience. In this
    chapter, we look at how we can create some advanced rendering effects by implementing
    shader techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to shaders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lighting techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using shaders to create effects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to shaders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Simply put, a shader is a computer program that is used to do image processing
    such as special effects, color effects, lighting, and, well, shading. The position,
    brightness, contrast, hue, and other effects on all pixels, vertices, or textures
    used to produce the final image on the screen can be altered during runtime, using
    algorithms constructed in the shader program(s). These days, most shader programs
    are built to run directly on the **Graphical Processing Unit** (**GPU**). Shader
    programs are executed in parallel. This means, for example, that a shader might
    be executed once per pixel, with each of the executions running simultaneously
    on different threads on the GPU. The amount of simultaneous threads depends on
    the graphics card specific GPU, with modern cards sporting processors in the thousands.
    This all means that shader programs can be very performant and provide developers
    with lots of creative flexibility. In this section, we are going to get acquainted
    with shaders and implement our own shader infrastructure for the example engine.
  prefs: []
  type: TYPE_NORMAL
- en: Shader languages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With advances in graphics card technology, more flexibility has been added to
    the rendering pipeline. Where at one time developers had little control over concepts
    such as fixed-function pipeline rendering, new advancements have allowed programmers
    to take deeper control of graphics hardware for rendering their creations. Originally,
    this deeper control was achieved by writing shaders in assembly language, which
    was a complex and cumbersome task. It wasn't long before developers yearned for
    a better solution. Enter the shader programming languages. Let's take a brief
    look at a few of the more common languages in use.
  prefs: []
  type: TYPE_NORMAL
- en: '**C for graphics** (**Cg**) is a shading language originally developed by the
    Nvidia graphics company. Cg is based on the C programming language and, although
    they share the same syntax, some features of C were modified and new data types
    were added to make Cg more suitable for programming GPUs. Cg compilers can output
    shader programs supported by both DirectX and OpenGL. While Cg was mostly deprecated,
    it has seen a resurgence in a new form with its use in the Unity game engine.'
  prefs: []
  type: TYPE_NORMAL
- en: '**High-Level Shading Language** (**HLSL**) is a shading language developed
    by the Microsoft Corporation for use with the DirectX graphics API. HLSL is again
    modeled after the C programming language and shares many similarities to the Cg
    shading language. HLSL is still in development and continues to be the shading
    language of choice for DirectX. Since the release, DirectX 12 the HLSL language
    supports even lower level hardware control and has seen dramatic performance improvements.'
  prefs: []
  type: TYPE_NORMAL
- en: '**OpenGL Shading Language** (**GLSL**) is a shading language that is also based
    on the C programming language. It was created by the **OpenGL Architecture Review
    Board** (**OpenGL ARB**) to give developers more direct control of the graphics
    pipeline without having to use ARB assembly language or other hardware-specific
    languages. The language is still in open development and will be the language
    we will focus on in our examples.'
  prefs: []
  type: TYPE_NORMAL
- en: Building a shader program infrastructure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Most modern shader programs are composed of up to five different types of shader
    files: fragment or pixel shaders, vertex shaders, geometry shaders, compute shaders,
    and tessellation shaders. When building a shader program, each of these shader
    files must be compiled and linked together for use, much like how a C++ program
    is compiled and linked. Next, we are going to walk you through how this process
    works and see how we can build an infrastructure to allow for easier interaction
    with our shader programs.'
  prefs: []
  type: TYPE_NORMAL
- en: To get started, let's look at how we compile a GLSL shader. The GLSL compiler
    is part of the OpenGL library itself, and our shaders can be compiled within an
    OpenGL program. We are going to build an architecture to support this internal
    compilation. The whole process of compiling a shader can be broken down into some
    simple steps. First, we have to create a shader object, then provide the source
    code to the shader object. We can then ask the shader object to be compiled. These
    steps can be represented in the following three basic calls to the OpenGL API.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create the shader object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We create the shader object using the `glCreateShader()` function. The argument
    we pass in is the type of shader we are trying to create. The types of shaders
    can be `GL_VERTEX_SHADER`, `GL_FRAGMENT_SHADER`, `GL_GEOMETRY_SHADER`, `GL_TESS_EVALUATION_SHADER`,
    `GL_TESS_CONTROL_SHADER`, or `GL_COMPUTE_SHADER`. In our example case, we are
    trying to compile a vertex shader, so we use the `GL_VERTEX_SHADER` type.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we copy the shader source code into the shader object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here we are using the `glShaderSource()` function to load our shader source
    to memory. This function accepts an array of strings, so before we call `glShaderSource()`,
    we create a pointer to the start of the `shaderCode` array object using a still-to-be-created
    method. The first argument to `glShaderSource()` is the handle to the shader object.
    The second is the number of source code strings that are contained in the array.
    The third argument is a pointer to an array of source code strings. The final
    argument is an array of `GLint` values that contains the length of each source
    code string in the previous argument.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we compile the shader:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The last step is to compile the shader. We do this by calling the OpenGL API
    method, `glCompileShader()`, and passing the handle to the shader that we want
    compiled.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, because we are using memory to store the shaders, we should know
    how to clean up when we are done. To delete a shader object, we can call the `glDeleteShader()`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: Deleting a Shader `ObjectShader` objects can be deleted when no longer needed
    by calling `glDeleteShader()`. This frees the memory used by the shader object.
    It should be noted that if a shader object is already attached to a program object,
    as in linked to a shader program, it will not be immediately deleted, but rather
    flagged for deletion. If the object is flagged for deletion, it will be deleted
    when it is detached from the linked shader program object.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have compiled our shaders, the next step we need to take before we can
    use them in our program is to link them together into a complete shader program.
    One of the core aspects of the linking step involves making the connections between
    input variables from one shader to the output variables of another and making
    the connections between the input/output variables of a shader to appropriate
    locations in the OpenGL program itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Linking is much like compiling the shader. We create a new shader program and
    attach each shader object to it. We then tell the shader program object to link
    everything together. The steps to accomplish this in the OpenGL environment can
    be broken down into a few calls to the API, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create the shader program object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: To start, we call the `glCreateProgram()` method to create an empty program
    object. This function returns a handle to the shader program object which, in
    this example, we are storing in a variable named `shaderProgram`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we attach the shaders to the program object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: To load each of the shaders into the shader program, we use the `glAttachShader()`
    method. This method takes two arguments. The first argument is the handle to the
    shader program object, and the second is the handle to the shader object to be
    attached to the shader program.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we link the program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'When we are ready to link the shaders together we call the `glLinkProgram()`
    method. This method has only one argument: the handle to the shader program we
    want to link.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s important that we remember to clean up any shader programs that we are
    not using anymore. To remove a shader program from the OpenGL memory, we call
    `glDeleteProgram()` method. The `glDeleteProgram()` method takes one argument:
    the handle to the shader program that is to be deleted. This method call invalidates
    the handle and frees the memory used by the shader program. It is important to
    note that if the shader program object is currently in use, it will not be immediately
    deleted, but rather flagged for deletion. This is similar to the deletion of shader
    objects. It is also important to note that the deletion of a shader program will
    detach any shader objects that were attached to the shader program at linking
    time. This, however, does mean the shader object will be deleted immediately unless
    those shader objects have already been flagged for deletion by a previous call
    to the `glDeleteShader()` method.'
  prefs: []
  type: TYPE_NORMAL
- en: So those are the simplified OpenGL API calls required to create, compile, and
    link shader programs. Now we are going to move onto implementing some structure
    to make the whole process much easier to work with. To do this, we are going to
    create a new class called `ShaderManager`. This class will act as the interface
    for compiling, linking, and managing the cleanup of shader programs. To start
    with, let's look at the implementation of the `CompileShaders()` method in the
    `ShaderManager.cpp` file. I should note that I will be focusing on the important
    aspects of the code that pertain to the implementation of the architecture. The
    full source code for this chapter can be found in the `Chapter07` folder in the
    GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'To begin, for this example we are focusing on two of the shader types, so our
    `ShaderManager::CompileShaders()` method accepts two arguments. The first argument
    is the file path location of the vertex shader file, and the second is the file
    path location to the fragment shader file. Both are strings. Inside the method
    body, we first create the shader program handle using the `glCreateProgram()`
    method and store it in the `m_programID` variable. Next, we create the handles
    for the vertex and fragment shaders using the `glCreateShader()` command. We check
    for any errors when creating the shader handles, and if we find any we throw an
    exception with the shader name that failed. Once the handles have been created,
    we then call the `CompileShader()` method, which we will look at next. The `CompileShader()`
    function takes two arguments: the first is the path to the shader file, and the
    second is the handle in which the compiled shader will be stored.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the full `CompileShader()` function. It handles the look and
    loading of the shader file from storage, as well as calling the OpenGL compile
    command on the shader file. We will break it down chunk by chunk:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'To start the function, we first use an `ifstream` object to open the file with
    the shader code in it. We also check to see if there were any issues loading the
    file and if, there were, we throw an exception notifying us that the file failed
    to open:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to parse the shader. To do this, we create a string variable
    called `fileContents` that will hold the text in the shader file. We then create
    another string variable named line; this will be a temporary holder for each line
    of the shader file we are trying to parse. Next, we use a `while` loop to step
    through the shader file, parsing the contents line by line and saving each loop
    into the `fileContents` string. Once all the lines have been read into the holder
    variable, we call the close method on the `shaderFile` `ifstream` object to free
    up the memory used to read the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'You might remember from earlier in the chapter that I mentioned that when we
    are using the `glShaderSource()` function, we have to pass the shader file text
    as a pointer to the start of a character array. In order to meet this requirement,
    we are going to use a neat trick where we use the C string conversation method
    built into the string class to allow us to pass back a pointer to the start of
    our shader character array. This, in case you are unfamiliar, is essentially what
    a string is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have a pointer to the shader text, we can call the `glShaderSource()`
    method to tell OpenGL that we want to use the contents of the file to compile
    our shader. Then, finally, we call the `glCompileShader()` method with the handle
    to the shader as the argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'That handles the compilation, but it is a good idea to provide ourselves with
    some debug support. We implement this compilation debug support by closing out
    the `CompileShader()` function by first checking to see if there were any errors
    during the compilation process. We do this by requesting information from the
    shader compiler through `glGetShaderiv()` function, which, among its arguments,
    takes an enumerated value that specifies what information we would like returned.
    In this call, we are requesting the compile status:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we check to see if the returned value is `GL_FALSE`, and if it is, that
    means we have had an error and should ask the compiler for more information about
    the compile issues. We do this by first asking the compiler what the max length
    of the error log is. We use this max length value to then create a vector of character
    values called errorLog. Then we can request the shader compile log by using the
    `glGetShaderInfoLog()` method, passing in the handle to the shader file the number
    of characters we are pulling, and where we want to save the log:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have the log file saved, we go ahead and delete the shader using the
    `glDeleteShader()` method. This ensures we don''t have any memory leaks from our
    shader:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we first print the error log to the console window. This is great
    for runtime debugging. We also throw an exception with the shader name/file path,
    and the message that it failed to compile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'That really simplifies the process of compiling our shaders by providing a
    simple interface to the underlying API calls. Now, in our example program, to
    load and compile our shaders we use a simple line of code similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Having now compiled the shaders, we are halfway to a useable shader program.
    We still need to add one more piece, linking. To abstract away some of the processes
    of linking the shaders and to provide us with some debugging capabilities, we
    are going to create the `LinkShaders()` method for our `ShaderManager` class.
    Let''s take a look and then break it down:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'To start our `LinkShaders()` function, we call the `glAttachShader()` method
    twice, using the handle to the previously created shader program object, and the
    handle to each shader we wish to link, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we perform the actual linking of the shaders into a usable shader program
    by calling the `glLinkProgram()` method, using the handle to the program object
    as its argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then check to see if the linking process has completed without any errors
    and provide ourselves with any debug information that we might need if there were
    any errors. I am not going to go through this code chunk line by line since it
    is nearly identical to what we did with the `CompileShader()` function. Do note,
    however, that the function to return the information from the linker is slightly
    different and uses `glGetProgram*` instead of the `glGetShader*` functions from
    before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, if we are successful in the linking process, we need to clean it up
    a bit. First, we detach the shaders from the linker using the `glDetachShader()`
    method. Next, since we have a completed shader program, we no longer need to keep
    the shaders in memory, so we delete each shader with a call to the `glDeleteShader()`
    method. Again, this will ensure we do not leak any memory in our shader program
    creation process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have a simplified way of linking our shaders into a working shader program.
    We can call this interface to the underlying API calls by simply using one line
    of code, similar to the following one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: So that handles the process of compiling and linking our shaders, but there
    is another key aspect to working with shaders, which is the passing of data to
    and from the running program/the game and the shader programs running on the GPU.
    We will look at this process and how we can abstract it into an easy-to-use interface
    for our engine next.
  prefs: []
  type: TYPE_NORMAL
- en: Working with shader data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the most important aspects of working with shaders is the ability to
    pass data to and from the shader programs running on the GPU. This can be a deep
    topic, and much like other topics in this book has had its own dedicated books.
    We are going to stay at a higher level when discussing this topic and again will
    focus on the two needed shader types for basic rendering: the vertex and fragment
    shaders.'
  prefs: []
  type: TYPE_NORMAL
- en: To begin with, let's take a look at how we send data to a shader using the vertex
    attributes and **Vertex Buffer Objects** (**VBO**). A vertex shader has the job
    of processing the data that is connected to the vertex, doing any modifications,
    and then passing it to the next stage of the rendering pipeline. This occurs once
    per vertex. In order for the shader to do its thing, we need to be able to pass
    it data. To do this, we use what are called vertex attributes, and they usually
    work hand in hand with what is referred to as VBO.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the vertex shader, all per-vertex input attributes are defined using the
    keyword `in`. So, for example, if we wanted to define a vector 3 input attribute
    named VertexColour, we could write something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Now, the data for the `VertexColour` attribute has to be supplied by the program/game.
    This is where VBO come in. In our main game or program, we make the connection
    between the input attribute and the vertex buffer object, and we also have to
    define how to parse or step through the data. That way, when we render, the OpenGL
    can pull data for the attribute from the buffer for each call of the vertex shader.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look a very simple vertex shader:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: In this example, there are just two input variables for this vertex shader,
    `VertexPosition` and `VertexColor`. Our main OpenGL program needs to supply the
    data for these two attributes for each vertex. We will do so by mapping our polygon/mesh
    data to these variables. We also have one output variable named Colour, which
    will be sent to the next stage of the rendering pipeline, the fragment shader.
    In this example, Colour is just an untouched copy of `VertexColour`. The `VertexPosition`
    attribute is simply expanded and passed along to the OpenGL API output variable
    `gl_Position` for more processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s take a look at a very simple fragment shader:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: In this fragment shader example, there is only one input attribute, `Colour`.
    This input corresponds to the output of the previous rendering stage, the vertex
    shader's `Colour` output. For simplicity's sake, we are just expanding the `Colour`
    and outputting it as the variable `FragColour` for the next rendering stage.
  prefs: []
  type: TYPE_NORMAL
- en: That sums up the shader side of the connection, so how do we compose and send
    the data from inside our engine? We can accomplish this in basically four steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create a **Vertex Array Object** (**VAO**) instance to hold our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create and populate the VBO for each of the shaders'' input attributes.
    We do this by first creating a VBO variable, then, using the `glGenBuffers()`
    method, we generate the memory for the buffer objects. We then create handles
    to the different attributes we need buffers for, assigning them to elements in
    the VBO array. Finally, we populate the buffers for each attribute by first calling
    the `glBindBuffer()` method, specifying the type of object being stored. In this
    case, it is a `GL_ARRAY_BUFFER` for both attributes. Then we call the `glBufferData()`
    method, passing the type, size, and handle to bind. The last argument for the
    `glBufferData()` method is one that gives OpenGL a hint about how the data will
    be used so that it can determine how best to manage the buffer internally. For
    full details about this argument, take a look at the OpenGL documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The third step is to create and define the VAO. This is how we will define
    the relationship between the input attributes of the shader and the buffers we
    just created. The VAO contains this information about the connections. To create
    a VAO, we use the `glGenVertexArrays()` method. This gives us a handle to our
    new object, which we store in our previously created VAO variable. Then, we enable
    the generic vertex attribute indexes 0 and 1 by calling the `glEnableVertexAttribArray()`
    method. By making the call to enable the attributes, we are specifying that they
    will be accessed and used for rendering. The last step makes the connection between
    the buffer objects we have created and the generic vertex attribute indexes the
    match too:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, in our `Draw()` function call, we bind to the VAO and call `glDrawArrays()`
    to perform the actual render:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we move on to another way to pass data to the shader, there is one more
    piece of this attribute connection structure we need to discuss. As mentioned,
    the input variables in a shader are linked to the generic vertex attribute we
    just saw, at the time of linking. When we need to specify the relationship structure,
    we have a few different choices. We can use what are known as layout qualifiers
    within the shader code itself. The following is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Another choice is to just let the linker create the mapping when linking, and
    then query for them afterward. The third and the one I personally prefer is to
    specify the relationship prior to the linking process by making a call to the
    `glBindAttribLocation()` method. We will see how this is implemented shortly when
    we discuss how to abstract these processes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have described how we can pass data to a shader using attributes, but there
    is another option: uniform variables. Uniform variables are specifically used
    for data that changes infrequently. For example, matrices are great candidates
    for uniform variables. Within a shader, a uniform variable is read-only. That
    means the value can only be changed from outside the shader. They can also appear
    in multiple shaders within the same shader program. They can be declared in one
    or more shaders within a program, but if a variable with a given name is declared
    in more than one shader, its type must be the same in all shaders. This gives
    us insight into the fact that the uniform variables are actually held in a shared
    namespace for the whole of the shader program.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To use a uniform variable in your shader, you first have to declare it in the
    shader file using the uniform identifier keyword. The following is what this might
    look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We then need to provide the data for the uniform variable from inside our game/program.
    We do this by first finding the location of the variable using the `glGetUniformLocation()`
    method. Then we assign a value to the found location using one of the `glUniform()`
    methods. The code for this process could look something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: We then assign a value to the uniform variable's location using the `glUniformMatrix4fv()`
    method. The first argument is the uniform variable's location. The second argument
    is the number of matrices that are being assigned. The third is a GL `bool` type
    specifying whether or not the matrix should be transposed. Since we are using
    the GLM library for our matrices, a transpose is not required. If you were implementing
    the matrix using data that was in row-major order, instead of column-major order,
    you might need to use the `GL_TRUE` type for this argument. The last argument
    is a pointer to the data for the uniform variable.
  prefs: []
  type: TYPE_NORMAL
- en: Uniform variables can be any GLSL type, and this includes complex types such
    as structures and arrays. The OpenGL API provides a `glUniform()` function with
    the different suffixes that match each type. For example, to assign to a variable
    of type `vec3`, we would use `glUniform3f()` or `glUniform3fv()` methods. (the
    *v* denotes multiple values in the array).
  prefs: []
  type: TYPE_NORMAL
- en: So, those are the concepts and techniques for passing data to and from our shader
    programs. However, as we did for the compiling and linking of our shaders, we
    can abstract these processes into functions housed in our `ShaderManager` class.
    We are going to focus on working with attributes and uniform variables. We do
    have a great class that abstracts the creation of VAO and VBO for models/meshes,
    that we walked through in great detail back in [Chapter 4](9379b574-a962-466b-9efe-d21b410c51c0.xhtml),
    *Building Gameplay Systems*, when we discussed building an asset pipeline. To
    see how that was constructed, either flip back to [Chapter 4](https://cdp.packtpub.com/mastering_c___game_development/wp-admin/post.php?post=325&action=edit#post_245), *Building
    Gameplay Systems,* or check out the implementation in the `Mesh.h` and `Mesh.cpp`
    files of the `BookEngine` solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will look at the abstraction of adding attribute bindings using the
    `AddAttribute()` function of the `ShaderManger` class. This function takes one
    argument, the attribute''s name, to be bound as a string. We then call the `glBindAttribLocation()`
    function, passing the program''s handle and the current index or number of attributes,
    which we increase on call, and finally the C string conversion of the `attributeName`
    string, which provides a pointer to the first character in the string array. This
    function must be called after compilation, but before the linking of the shader
    program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'For the uniform variables, we create a function that abstracts looking up the
    location of the uniform in the shader program, the `GetUniformLocation()` function.
    This function again takes only one variable which is a uniform name in the form
    of a string. We then create a temporary holder for the location and assign it
    the returned value of the `glGetUniformLocation()` method call. We check to make
    sure the location is valid, and if not we throw an exception letting us know about
    the error. Finally, we return the valid location if found:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the abstraction for binding our data, but we still need to assign
    which shader should be used for a certain draw call, and to activate any attributes
    we need. To accomplish this, we create a function in the `ShaderManager` called
    `Use()`. This function will first set the current shader program as the active
    one using the `glUseProgram()` API method call. We then use a for loop to step
    through the list of attributes for the shader program, activating each one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Of course, since we have an abstracted way to enable the shader program, it
    only makes sense that we should have a function to disable the shader program.
    This function is very similar to the `Use()` function, but in this case, we are
    setting the program in use to 0, effectively making it `NULL`, and we use the
    `glDisableVertexAtrribArray()` method to disable the attributes in the for loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The net effect of this abstraction is we can now set up our entire shader program
    structure with a few simple calls. Code similar to the following would create
    and compile the shaders, add the necessary attributes, link the shaders into a
    program, locate a uniform variable, and create the VAO and VBO for a mesh:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, in our `Draw` loop, if we want to use this shader program to draw, we
    can simply use the abstracted functions to activate and deactivate our shader,
    similar to the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: This makes it much easier for us to work with and test out advanced rendering
    techniques using shaders. We will be using this structure to build out the examples
    in the rest of this chapter and in fact the rest of the book.
  prefs: []
  type: TYPE_NORMAL
- en: Lighting effects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most common uses for shaders is creating lighting and reflection
    effects. Lighting effects achieved from the use of shaders help provide a level
    of polish and detail that every modern game strives for. In the next section,
    we will look at some of the well-known models for creating different surface appearance
    effects, with examples of shaders you can implement to replicate the discussed
    lighting effect.
  prefs: []
  type: TYPE_NORMAL
- en: Per-vertex diffuse
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To start with, we will look at one of the simpler lighting vertex shaders, the
    diffuse reflection shader. Diffuse is considered simpler since we assume that
    the surface we are rendering appears to scatter the light in all directions equally.
    With this shader, the light makes contact with the surface and slightly penetrates
    before being cast back out in all directions. This means that some of the light's
    wavelength will be at least partially absorbed. A good example of what a diffuse
    shader looks like is to think of matte paint. The surface has a very dull look
    with no shine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a quick look at the mathematical model for a diffuse reflection.
    This reflection model takes two vectors. One is the direction of the surface contact
    point to the initial light source, and the second is the normal vector of that
    same surface contact point. This would look something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/61cd3123-d51e-46a5-9e49-fe95a5c7b232.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It''s worth noting that the amount of light that strikes the surface is partially
    dependent on the surface in relation to the light source and that the amount of
    light that reaches a single point will be at its maximum along the normal vector,
    and its lowest when perpendicular to the normal vector. Dusting off our physics
    knowledge toolbox, we are able to express this relationship given the amount of
    light making contact with a point by calculating the dot product of the point
    normal vector and incoming light vector. This can be expressed by the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Light Density(Source Vector) Normal Vector*'
  prefs: []
  type: TYPE_NORMAL
- en: The source and normal vector in this equation are assumed to be normalized.
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned before, some of the light striking the surface will be absorbed
    before it is re-cast. To add this behavior to our mathematical model, we can add
    a reflection coefficient, also referred to as the diffuse reflectivity. This coefficient
    value becomes the scaling factor for the incoming light. Our new formula to specify
    the outgoing intensity of the light will now look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Outgoing Light = (Diffuse Coefficient x Light Density x Source Vector) Normal
    Vector*'
  prefs: []
  type: TYPE_NORMAL
- en: With this new formula, we now have a lighting model that represents an omnidirectional,
    uniform scattering.
  prefs: []
  type: TYPE_NORMAL
- en: 'OK, now that we know the theory, let''s take a look at how we can implement
    this lighting model in a GLSL shader. The full source for this example can be
    found in the `Chapter07` folder of the GitHub repository, starting with the Vertex
    Shader shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: We'll go through this shader block by block. To start out, we have our attributes,
    `vertexPosition_modelspace`, `vertexUV`, and `vertexNormal`. These will be set
    by our game application, which we will look at after we go through the shader.
    Then we have our out variables, UV and `LightIntensity`. These values will be
    calculated in the shader itself. We then have our uniforms. These include the
    needed values for our reflection calculation, as we discussed. It also includes
    all the necessary matrices. Like the attributes, these uniform values will be
    set via our game.
  prefs: []
  type: TYPE_NORMAL
- en: Inside of the main function of this shader, our diffuse reflection is going
    to be calculated in the camera relative coordinates. To accomplish this, we first
    normalize the vertex normal by multiplying it by the normal matrix and storing
    the results in a vector 3 variable named `tnorm`. Next, we convert the vertex
    position that is currently in model space to camera coordinates by transforming
    it with the model view matrix. We then calculate the incoming light direction,
    normalized, by subtracting the vertex position in the camera coordinates from
    the light's position. Next, we calculate the outgoing light intensity by using
    the formula we went through earlier. A point to note here is the use of the max
    function. This is a situation when the light direction is greater than 90 degrees,
    as in the light is coming from inside the object. Since in our case we don't need
    to support this situation, we just use a value of `0.0` when this arises. To close
    out the shader, we store the model view projection matrix, calculated in clip
    space, in the built-in outbound variable `gl_position`. We also pass along the
    UV of the texture, unchanged, which we are not actually using in this example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the shader in place, we need to provide the values needed
    for the calculations. As we learned in the first section of this chapter, we do
    this by setting the attributes and uniforms. We built an abstraction layer to
    help with this process, so let''s take a look at how we set these values in our
    game code. Inside the `GamePlayScreen.cpp` file, we are setting these values in
    the `Draw()` function. I should point out this is for the example, and in a production
    environment, you would only want to set the changing values in a loop for performance
    reasons. Since this is an example, I wanted to make it slightly easier to follow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'I won''t go through each line since I am sure you can see the pattern. We first
    use the shader manager''s `GetUniformLocation()` method to return the location
    for the uniform. Next, we set the value for this uniform using the OpenGL `glUniform*()`
    method that matches the value type. We do this for all uniform values needed.
    We also have to set our attributes, and as discussed in the beginning of the chapter,
    we do this in between the compilation and linking processes. In this example case,
    we are setting these values in the `OnEntry()` method of the `GamePlayScreen()`
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'That takes care of the vertex shader and passed in values needed, so next,
    let''s look at the fragment shader for this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: For this example, our fragment shader is extremely simple. To begin, we have
    the in values for our UV and `LightIntensity`, and we will only use the `LightIntensity`
    this time. We then declare our out color value, specified as a vector 3\. Next,
    we have the `sampler2D` uniform that we use for texturing, but again we won't
    be using this value in the example. Finally, we have the main function. This is
    where we set the final output color by simply passing the `LightIntensity` through
    to the next stage in the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you run the example project, you will see the diffuse reflection in action.
    The output should look like the following screenshot. As you can see, this reflection
    model works well for surfaces that are very dull but has limited use in a practical
    environment. Next, we will look at a reflection model that will allow us to depict
    more surface types:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8e83e885-ac5e-4c00-91b7-03998dd9380e.png)'
  prefs: []
  type: TYPE_IMG
- en: Per-vertex ambient, diffuse, and specular
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **ambient**, **diffuse**, **and specular** (**ADS**) reflection model, also
    commonly known as the **Phong reflection model**, provides a method of creating
    a reflective lighting shader. This technique models the interaction of light on
    a surface using a combination of three different components. The ambient component
    models the light that comes from the environment; this is intended to model what
    would happen if the light was reflected many times, where it appears as though
    it is emanating from everywhere. The diffuse component, which we modeled in our
    previous example, represents an omnidirectional reflection. The last component,
    the specular component, is meant to represent the reflection in a preferred direction,
    providing the appearance of a light *glare* or bright spot.
  prefs: []
  type: TYPE_NORMAL
- en: 'This combination of components can be visualized using the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/45633898-8754-47a0-b4b8-f102297cca8f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Wikipedia'
  prefs: []
  type: TYPE_NORMAL
- en: This process can be broken down into separate components for discussion. First,
    we have the ambient component that represents the light that will illuminate all
    of the surfaces equally and reflect uniformly in all directions. This lighting
    effect does not depend on the incoming or the outgoing vectors of the light since
    it is uniformly distributed and can be expressed by simply multiplying the light
    source's intensity with the surface reflectivity. This is shown in the mathematical
    formula *I[a] = L[a]K[a]*.
  prefs: []
  type: TYPE_NORMAL
- en: The next component is the diffuse component we discussed earlier. The diffuse
    component models a dull or rough surface that scatters light in all directions.
    Again, this can be expressed with the mathematical formula *I[d] = L[d]K[d](sn)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final component is the specular component, and it is used to model the
    *shininess* of the surface. This creates a *glare* or bright spot that is common
    on surfaces that exhibit glossy properties. We can visualize this reflection effect
    using the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/46525544-c9ce-4200-b03c-bf4de5fc5118.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For the specular component, ideally, we would like the reflection to be at
    is most apparent when viewed aligned with the reflection vector, and then to fade
    off as the angle is increased or decreased from this alignment. We can model this
    effect using the cosine of the angle between our viewing vector and the reflection
    angle, which is then raised by some power, as shown in this equation: *(r v) ^p*.
    In this equation, *p* represents the specular highlight, the *glare* spot. The
    larger the value input for *p*, the smaller the spot will appear, and the *shinier* the
    surface will look. After adding the values to represent the reflectiveness of
    the surface and the specular light intensity, the formula for calculating the
    specular effect for the surface looks like so: *I[s] = L[s]K[s](r v) ^p*.'
  prefs: []
  type: TYPE_NORMAL
- en: So, now, if we take all of our components and put them together in a formula,
    we come up with *I = I[a] + I[d] + I[s]* or breaking it down more, *I = L[a]K[a]
    + L[d]K[d](sn) + L[s]K[s](r v) ^p* .
  prefs: []
  type: TYPE_NORMAL
- en: 'With our theory in place, let''s see how we can implement this in a per-vertex
    shader, beginning with our vertex shader as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Let's take a look at the what is different to start with. In this shader, we
    are introducing a new concept, the uniform struct. We are declaring two `struct`,
    one to describe the light, `LightInfo`, and one to describe the material, `MaterialInfo`.
    This is a very useful way of containing values that represent a portion in the
    formula as a collection. We will see how we can set the values of these `struct`
    elements from the game code shortly. Moving on to the main function of the function.
    First, we start as we did in the previous example. We calculate the `tnorm`, `CameraCoords`,
    and the light source vector(s). Next, we calculate the vector in the direction
    of the viewer/camera (v), which is the negative of the normalized `CameraCoords`.
    We then calculate the direction of the *pure* reflection using the provided GLSL
    method, reflect. Then we move on to calculating the values of our three components.
    The ambient is calculated by multiplying the light ambient intensity and the surface's
    ambient reflective value. The `diffuse` is calculated using the light intensity,
    the surface diffuse reflective value of the surface, and the result of the dot
    product of the light source vector and the `tnorm`, which we calculated just before
    the ambient value. Before computing the specular value, we check the value of
    `sDotN`. If `sDotN` is zero, then there is no light reaching the surface, so there
    is no point in computing the specular component. If `sDotN` is greater than zero,
    we compute the specular component. As in the previous example, we use a GLSL method
    to limit the range of values of the dot product to between `1` and `0`. The GLSL
    function `pow` raises the dot product to the power of the surface's shininess
    exponent, which we defined as `p` in our shader equation previously.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we add all three of our component values together and pass their sum
    to the fragment shader in the form of the out variable, `LightIntensity`. We end
    by transforming the vertex position to clip space and passing it off to the next
    stage by assigning it to the `gl_Position` variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the setting of the attributes and uniforms needed for our shader, we handle
    the process just as we did in the previous example. The main difference here is
    that we need to specify the elements of the `struct` we are assigning when getting
    the uniform location. An example would look similar to the following, and again
    you can see the full code in the example solution in the `Chapter07` folder of
    the GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The fragment shader used for this example is the same as the one we used for
    the diffuse example, so I won't cover it again here.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you run the ADS example from the `Chapter07` code solution of the GitHub
    repository, you will see our newly created shader in effect, with an output looking
    similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1b255e28-0a5a-4456-8efc-86158d5d6514.png)'
  prefs: []
  type: TYPE_IMG
- en: In this example, we calculated the shading equation within the vertex shader;
    this is referred to as a per-vertex shader. One issue that can arise from this
    approach is that our
  prefs: []
  type: TYPE_NORMAL
- en: '*glare* spots, the specular highlights, might appear to warp or disappear.
    This is caused by the shading being interpolated and not calculated for each point
    across the face. For example, a spot that was set near the middle of the face
    might not appear due to the fact that the equation was calculated at the vertices
    where the specular component was near to zero. In the next example, we will look
    at a technique that can eliminate the issue by calculating the reflection in the
    fragment shader.'
  prefs: []
  type: TYPE_NORMAL
- en: Per-fragment Phong interpolation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous examples, we have been using the vertex shaders to handle the
    lighting calculations. One issue when using a vertex shader to evaluate the color
    of each vertex, as mentioned in the last example, is that color is then interpolated
    across the face. This can cause some less than favorable effects. There is another
    way to accomplish this same lighting effect, but with improved accuracy. We can
    move the calculation to the fragment shader instead. In the fragment shader, instead
    of interpolating across the face, we interpolate normal and position and use these
    values to calculate at each fragment instead. This technique is often called **Phong
    interpolation**. The results of this techniques are much more accurate than when
    using a per-vertex implementation. However, since this per-fragment implementation
    evaluates each fragment, as opposed to just the vertices, this implementation
    will run slower than the per-vertex technique.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start our look at the shader implementation by looking at the vertex
    shader for this example first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Since this technique uses the fragment shader to perform the calculations, our
    vertex shader is considerably light. For the most part, we are doing a few simple
    equations to calculate the normal and the position, and then passing the values
    along to the next stage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will look at the core of this technique''s implementation in the fragment
    shader. Following is the complete fragment shader, and we will cover the differences
    from the previous examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: This fragment shader should look very familiar, as this is nearly identical
    to the vertex shader from our previous examples. The big difference, besides the
    fact that this will run per fragment, not per vertex, is that we have cleaned
    up the shader by implementing a function to handle the Phong model calculation.
    We are also going to pass through a texture this time, to give our texture back
    to the gnome. The Phong model calculation is exactly the same as we have seen
    before, so I won't cover it again. The reason we moved it out into a function
    is mostly for readability, as it keeps the main function uncluttered. Creating
    a function in GLSL is nearly the same as in C++ and C. You have a return type,
    a function name followed by arguments, and a body. I highly recommend using functions
    in any shader more complex than a few lines.
  prefs: []
  type: TYPE_NORMAL
- en: To connect our shaders to the values from our game, we follow the same technique
    as before, where we set the needed attributes and uniform values. For this example,
    we must supply the values for Ka, Kd, Ks, Material Shininess, `LightPosition`,
    and `LightIntensity`. These values match up with the previously described ADS
    equation. We also need to pass in the usual matrices values. The full code can
    again be found in the `Chapter07` folder of the GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we run the `Phong_Example` from the `Chapter07` solution, we will see the
    new shader in action, complete with texture and a more accurate reflection representation.
    The following is a screenshot of the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5fcd5f76-14f7-4684-a7a6-45e392f66a21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will end our discussion on lighting techniques here, but I encourage you
    to continue your research on the topic. There are many interesting lighting effects
    that you can achieve with shaders, and we have only really begun to scratch the
    surface. In the next section, we will look at another common use for shaders:
    rendering effects.'
  prefs: []
  type: TYPE_NORMAL
- en: Using Shaders to create effects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Shaders are not just limited to creating lighting effects. You can create many
    different visual effects using different shader techniques. In this section, we
    will cover a couple of interesting effects that you can achieve, including using
    the discard keyword to *throw away* pixels, and using shaders to create a simple
    particle effect system.
  prefs: []
  type: TYPE_NORMAL
- en: Discarding fragments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the use of fragment shader tools, we are able to create some cool effects.
    One of these tools is the use of the discard keyword. The discard keyword, like
    its name suggests, removes or throws away fragments. When the discard keyword
    is used, the shader immediately stops its execution and skips the fragment, not
    writing any data to the output buffer. The created effect is holes in the polygon
    faces without using a blending effect. The discard keyword can also be combined
    with the use of alpha maps to allow textures to specify what fragments should
    be discarded. This can be a handy technique when modeling effects such as damage
    to an object.
  prefs: []
  type: TYPE_NORMAL
- en: For this example, we are going to create a fragment shader that will use the
    discard keyword to remove certain fragments based on the UV texture coordinates.
    The effect will be a lattice or perforated look for our gnome model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s begin with looking at the vertex shader for this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we are moving our lighting calculation back to the vertex shader.
    You may have noticed that this vertex shader is very similar to the previous example,
    with some slight changes. The first change to note is that we are using the UV
    texture coordinates in this example. We use the texture coordinates to determine
    the fragments to throw away, and we are not going to render the texture of the
    model this time. Since we are going to be discarding some fragments of the gnome
    model, we will be able to see through the model to the other and inside. This
    means we will need to calculate the lighting equation for both the front and back
    of the face. We accomplish this by calculating the Phong model for each side,
    changing the normal vector being passed in. We then store those values for each
    vertex in the `FrontColor` and `BackColor` variables to be passed along to the
    fragment shader. To again make our main class slightly easier to read, we also
    move the camera space transformation to a function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s look at the fragment shader for this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: In our fragment shader, we are calculating which fragment to discard to give
    us the desired perforated effect. To accomplish this, we first scale the UV coordinate
    using our scaling factor. This scaling factor represents the number of perforated
    rectangles per texture coordinate. Next, we calculate the fractional part of the
    texture coordinate components by using the GLSL function `fract()`. We then compare
    each *x* and *y* component to the float value of 0.2 using another GLSL function,
    `greaterThan()`.
  prefs: []
  type: TYPE_NORMAL
- en: If both the *x* and *y* components of the vector in the `toDiscard` variable
    evaluate to true, this means the fragment lies within the perforated rectangle's
    frame, and we want to discard it. We can use the GLSL function to help us perform
    this check. The function call will return true if all of the components of the
    parameter vector are true. If the function returns true, we execute the `discard`
    statement to throw away that fragment.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we have an `else` block where we color the fragment depending on whether
    it is a back-facing or front-facing polygon. To help us, we use the `gl_FronFacing()`
    function to return true or false based on the polygon's normal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just as we have in the previous examples, we must again make sure to set the
    attributes and uniform variables needed for the shader in our game program. To
    see the full implementation of the example, see the `Chapter07`, `DiscardExample`,
    project. If we run this example program, you will see our gnome model looking
    as if he was made of lattice. The following is a screenshot of the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e90cca70-3782-4848-8bee-a2e50889ee8d.png)'
  prefs: []
  type: TYPE_IMG
- en: Generating particles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another effect that you can achieve through the use of shaders is what is commonly
    referred to as particle effects. You can think of a particle system as a group
    of objects that are used in unison to create the visual appearance of smoke, fire,
    explosions, and so on. A single particle in the system is considered to be a point
    object with a position, but no size. To render these point objects, the `GL_POINTS`
    primitive is usually the most common method. You can, however, render particles
    just like any other object, using triangles or quads.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our example, we are going to implement a simple particle system that will
    have a fountain appearance. Each particle in our system will follow these rules.
    It will have a limited lifetime, it will be created and animated based on defined
    criteria, and will then terminate. In some particle systems, you could then recycle
    the particle, but for simplicity''s sake, our example here will not. The animation
    criteria for a particle is most often based on kinematic equations which define
    the movement of the particle based on gravitational acceleration, wind, friction,
    and other factors. Again, to keep our example simple, we will animate our particles
    using the standard kinematics calculation for objects under constant acceleration.
    The following equation describes the position of a particle at a given time *t*,
    where *P[0]* is the initial position, *V[0]t* is the initial velocity, and *a*
    represents the acceleration:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(t) = P[0]+ V­[0]t + ½at²*'
  prefs: []
  type: TYPE_NORMAL
- en: In our example, we will define the initial position of the particles to be at
    the origin (0,0,0). The initial velocity will be calculated randomly within a
    range. Since each particle will be created at a different time interval in our
    equation, time will be relative to the creation time of that particle.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the initial position is the same for all particles, we won''t need to
    provide it as an attribute to our shader. We will only have to provide two vertex
    attributes: the initial velocity and the start time for the particles. As mentioned
    previously, we will render each particle using `GL_POINTS`. The cool thing about
    using `GL_POINTS` is it is easy to apply a texture to a point sprite because OpenGL
    will automatically generate texture coordinates and pass them to the fragment
    shader via the GLSL variable `gl_PointCoord`. To give the appearance of the particle
    fading away, we will also increase the transparency of the point object linearly
    over the lifetime of the particle.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s begin with a look at the vertex shader for this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Our shader begins with the two required input attributes, the initial velocity
    of the particle, `VertexInitVel`, and the start time of the particle, `StartTime`.
    We then have the output variable `Transp` which will hold the calculation of the
    particle''s transparency to be passed to the next shader stage. Next, we have
    our uniform variables: time, the animation runtime, gravity, used to calculate
    the constant acceleration, and `ParticleLifetime`, which specifies the maximum
    amount of time a particle can stay active. In the main function, we first set
    the initial position of the particle to be the origin, in this case (0,0,0). We
    then set the transparency to 0\. Next, we have a conditional that checks if the
    particle has been activated yet. If the current time is greater than the start
    time, the particle is active, or else the particle is not active. If the particle
    is not active, the position is left at the origin and the particle is rendered
    with full transparency. Then, if the particle is alive, we determine the current
    *age* of the particle by subtracting the start time from the current time, and
    we store the result in a float value `t`. We then check `t` against the `ParticleLiftime`
    value, and if `t` is greater than the lifetime value for the particle, the particle
    has already run through its lifetime animation and is then rendered fully transparent.
    If `t` is not greater than the lifetime value, the particle is in an active state
    and we animate the particle. We accomplish this animation using the equation we
    discussed previously. The transparency is determined by interpolation based on
    the runtime or *age* of the particle.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s look at the fragment shader for this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Our fragment shader for this example is pretty basic. Here, we are setting the
    color of the fragment based on its texture lookup value. As mentioned previously,
    because we are using the `GL_POINT` primitive, the texture coordinates are automatically
    calculated by OpenGL's `gl_PointCoord` variable. To wrap up, we multiply the alpha
    value of the fragment's final color by the `Transp` input variable. This will
    give us the fade away effect as our particle's runtime elapses.
  prefs: []
  type: TYPE_NORMAL
- en: In our game code, we need to create two buffers. The first buffer will store
    the initial velocity for each of the particles. The second buffer will store the
    start time for each particle. We will also have to set the uniform variables needed,
    including the `ParticleTex` for the particle texture, the `Time` variable for
    the amount of time that has elapsed since the animation beginning, the `Gravity`
    variable for representing the acceleration constant, and the `ParticleLifetime`
    variable for defining how long a particle will run its animation for. For brevity's
    sake, I will not go through the code here, but you can see the implementation
    of `Chapter07` folder's particle example project.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before testing our example, we also need to make sure that depth test is off
    and that enable alpha blending is on. You can do this with the following lines
    of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'You may also want to change the point object size to be a more reasonable value.
    You can set the value to 10 pixels using the following line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'If we now run our example project, we will see a particle effect similar to
    a fountain. A couple of captured frames can be seen as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/245c79e2-56a5-44c1-9414-d1040716729a.png)![](img/e5b65410-4810-4b6b-94d4-6aeba4eca42e.png)'
  prefs: []
  type: TYPE_IMG
- en: While this is a simple example, it has room for a lot of performance and flexibility
    increases, and it should provide you with a good starting point for implementing
    GPU based particle systems. Feel free to experiment with different input values,
    maybe even adding more factors to the particles animation calculation. Experimentation
    can lead to a lot of interesting outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered the basics of working with shaders. We learned how
    we can build a compiler and link abstraction layers to save us time. We gained
    knowledge about lighting technique theories and how we can implement them in shader
    language. Finally, we closed out the chapter by looking at other uses for shaders
    such as creating particle effects. In the next chapter, we will expand our example
    game framework further by creating advanced gameplay systems.
  prefs: []
  type: TYPE_NORMAL
