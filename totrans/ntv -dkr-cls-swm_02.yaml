- en: Chapter 2. Discover the Discovery Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 1](ch01.html "Chapter 1. Welcome to Docker Swarm"), *Welcome to
    Docker Swarm* we created a simple yet well functioning local Docker Swarm cluster
    using the `nodes://` mechanism. This system is not very practical, except for
    learning the Swarm fundamentals.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, it is just a flat model that does not contemplate any true master-slave
    architecture, not to mention the high-level services, such as nodes discovery
    and auto-configuration, resilience, leader elections, and failover (high availability).
    In practice, it's not suitable for a production environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from `nodes://`, Swarm v1 officially supports four discovery services;
    however, one of them, Token, is a trivial non-production one. Basically, with
    Swarm v1 you need to integrate a discovery service manually, while with Swarm
    Mode (from Docker 1.12), a discovery service, Etcd, is already integrated. In
    this chapter we''re going to cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Discovery services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A test-grade discovery service: Token'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raft theory and Etcd
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zookeeper and Consul
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before exploring these services in depth, lets us discuss what is a discovery
    service?
  prefs: []
  type: TYPE_NORMAL
- en: A discovery service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Imagine you''re running a Swarm cluster on a static configuration, similar
    to the one in [Chapter 1](ch01.html "Chapter 1. Welcome to Docker Swarm"), *Welcome
    to Docker Swarm*, networking is flat and every container is assigned a specific
    task, for example a MySQL database. It''s easy to locate the MySQL container because
    you assigned it a defined IP address or you run some DNS server. It''s easy to
    notify whether this single container is working or not and it''s a known fact
    that it won''t change its port (`tcp/3336`). Moreover, it''s not necessary that
    our MySQL container announces its availability as a database container with its
    IP and port: We, of course, already know that.'
  prefs: []
  type: TYPE_NORMAL
- en: This is the pet model, mocked-up manually by a system administrator. However,
    since we're more advanced operators, we want to drive a cattle instead.
  prefs: []
  type: TYPE_NORMAL
- en: So, imagine you're running a Swarm made of hundreds of nodes, hosting several
    applications running a certain number of services (web servers, databases, key-value
    stores, caches, and queues). These applications run on a massive number of containers
    that may dynamically change their IP address, either because you restart them,
    you create new ones, you spin up replicas, or some high availability mechanism
    starts new ones for you.
  prefs: []
  type: TYPE_NORMAL
- en: How can you find the MySQL services acting of your Acme app? How do you ensure
    that your load balancer knows the address of your 100 Nginx frontends so that
    their functionalities don't break? How do you notify if a service has moved away
    with a different configuration?
  prefs: []
  type: TYPE_NORMAL
- en: '*You use a discovery service.*'
  prefs: []
  type: TYPE_NORMAL
- en: A so called discovery service is a mechanism with many features. There are different
    services you can choose from, with more or less similar qualities, with their
    pros and their cons, but basically all discovery services target distributed systems,
    hence they must be distributed on all cluster nodes, be scalable, and fault-tolerant.
    The main goal of a discovery service is to help services to find and talk to one
    another. In order to do that, they need to save (register) information related
    to where each service is located, by announcing themselves, and they usually do
    that by acting as a key-value store. Discovery services existed way before of
    the rise of Docker, but the problem has become a lot more difficult with the advent
    of containers and container orchestration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Summarizing again, through a discovery service:'
  prefs: []
  type: TYPE_NORMAL
- en: You can locate single services in the infrastructure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can notify a service configuration change
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Services register their availability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And more
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Typically, a discovery service is made as a key-value store. Docker Swarm v1,
    officially, supports the following discovery services. However, you can integrate
    your own using `libkv` abstraction interface, you can integrate your own one as
    shown in the following site:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/docker/docker/tree/master/pkg/discovery](https://github.com/docker/docker/tree/master/pkg/discovery).'
  prefs: []
  type: TYPE_NORMAL
- en: Token
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consul 0.5.1+
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Etcd 2.0+
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ZooKeeper 3.4.5+
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, the Etcd library has been integrated into the Swarm mode as its built-in
    discovery service.
  prefs: []
  type: TYPE_NORMAL
- en: Token
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker Swarm v1 includes an out-of-the-box discovery service, called Token.
    Token is integrated into the Docker Hub; hence, it requires all the Swarm nodes
    to be connected to the Internet and able to reach the Docker Hub. This is the
    main limitation of Token but, you will soon see, Token will allow us to make some
    practice in handling clusters.
  prefs: []
  type: TYPE_NORMAL
- en: In a nutshell, Token requires you to generate a UUID called, in fact, token.
    With this UUID, you can create a manager, act like a master, and join slaves to
    the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Re-architecting the example of Chapter 1 with token
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If we want to keep it practical, it's time to take a look at an example. We'll
    use token to re-architect the example of [Chapter 1](ch01.html "Chapter 1. Welcome
    to Docker Swarm"), *Welcome to Docker Swarm*. As a novelty, the cluster will be
    not flat anymore, but it will consist of 1 master and 3 slaves and each node will
    have security enabled by default.
  prefs: []
  type: TYPE_NORMAL
- en: The master node will be the node exposing Swarm port `3376`. We'll connect specifically
    to it in order to be able to drive the entire cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '![Re-architecting the example of Chapter 1 with token](images/B05661_02_01-1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can create 4 nodes with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Now, we have four machines running the last version of the Engine, with the
    TLS enabled. This means, as you remember, that the Engine is exposing port `2376`
    and not `2375`.
  prefs: []
  type: TYPE_NORMAL
- en: '![Re-architecting the example of Chapter 1 with token](images/image_02_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We will now create the cluster, starting from the master. Pick up one of the
    nodes, for example `node0`, and source its variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We now generate the cluster token and the unique ID. For this purpose, we use
    the `swarm create` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![Re-architecting the example of Chapter 1 with token](images/image_02_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As a result, the swarm container outputs the token, and the protocol that we''ll
    be using in this example will be invoked as shown: `token://3b905f46fef903800d51513d51acbbbe`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Take note of this token ID, for example assigning it to a shell variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We now create a master and try to meet, at least, some of the basic standard
    security requirements, how is, we'll enable TLS encryption. As we'll see in a
    moment, the `swarm` command accepts TLS options as arguments. But how do we pass
    keys and certificates to a container? For this, we'll use the certificates generated
    by Docker Machine and placed in `/var/lib/boot2docker` on the host.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, we mount a volume from the Docker host to the container on the
    Docker host. All remotely and controlled thanks to the environment variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the `node0` variables already sourced, we start the Swarm master with
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: To begin, we run the container in an interactive mode to observe the swarm output.
    Then, we mount the node `/var/lib/boot2docker` directory to the `/certs` directory
    inside the swarm container. We redirect the `3376` Swarm secure ports from node0
    to the swarm container. We execute the `swarm` command in the manage mode by binding
    it to `0.0.0.0:3376`. Then we specify some certificates options and file paths
    and finally describe that the discovery service in use is token, with our token.
  prefs: []
  type: TYPE_NORMAL
- en: '![Re-architecting the example of Chapter 1 with token](images/image_02_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'With this node running, let''s open another terminal and join a node to this
    Swarm. Let''s start by sourcing the `node1` variables. Now, we need swarm to use
    the `join` command, in order to join the cluster whose master is `node0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Here we specify the host (itself) at address `192.168.99.101` to join the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '![Re-architecting the example of Chapter 1 with token](images/image_02_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If we jump back to the first terminal, we'll see that the master has noticed
    that a node has joined the cluster. So, at this point of time we have a Swarm
    cluster composed of one master and one slave.
  prefs: []
  type: TYPE_NORMAL
- en: '![Re-architecting the example of Chapter 1 with token](images/image_02_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Since we now understand the mechanism, we can stop both the `docker` commands
    in the terminals and rerun them with the `-d` option. So, to run containers in
    daemon mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Master**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**Node**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now proceed by joining the other two nodes to the cluster, source their
    variables, and repeat the last command as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, if we open a third terminal, source the `node0` variables, and
    specifically connect to port `3376` (Swarm) instead of `2376` (Docker Engine),
    we can see some fancy output coming from the `docker info` command. For example,
    there are three nodes in a cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Re-architecting the example of Chapter 1 with token](images/image_02_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: So, we have created a cluster with one master, three slaves, and with TLS enabled
    and ready to accept containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can ensure that from the master and list the nodes in the cluster. We will
    now use the `swarm list` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![Re-architecting the example of Chapter 1 with token](images/image_02_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Token limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Token is not deprecated yet, but probably it will be deprecated very soon.
    The standard requirement that every node in the Swarm should have internet connectivity
    is not very convenient. Moreover, the access to the Docker Hub makes this technique
    depend on Hub availability. In practice, it has the Hub as a single point of failure.
    However, using token, we were able to understand what''s behind the scenes a little
    bit better and we met the Swarm v1 commands: `create`, `manage`, `join`, and `list`.'
  prefs: []
  type: TYPE_NORMAL
- en: Now it's time to proceed further and get acquainted with real discovery services
    and the consensus algorithm, a cardinal principle in fault-tolerant systems.
  prefs: []
  type: TYPE_NORMAL
- en: Raft
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Consensus is an algorithm in distributed systems that forces agents in the system
    to agree on consistent values and elect a leader.
  prefs: []
  type: TYPE_NORMAL
- en: Some well-known consensus algorithms are Paxos and Raft. Paxos and Raft deliver
    similar performances but Raft is less complex, easier to understand, and therefore
    becoming very popular in distributed store implementations.
  prefs: []
  type: TYPE_NORMAL
- en: As consensus algorithms, Consul and Etcd implement Raft while ZooKeeper implements
    Paxos. The CoreOS Etcd Go library, implementing Raft, is included into SwarmKit
    and Swarm Mode as a dependency (in `vendor/`), so in this book we'll focus more
    on it.
  prefs: []
  type: TYPE_NORMAL
- en: Raft is described in detail in the Ongaro, Ousterhout paper, and it is available
    at [https://ramcloud.stanford.edu/raft.pdf](https://ramcloud.stanford.edu/raft.pdf).
    In the upcoming section we'll summarize its basic concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Raft theory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Raft was designed with simplicity in mind and compared to Paxos, it truly achieves
    this goal (there are even academic publications demonstrating this). For our purpose,
    the main difference between Raft and Paxos is that in Raft, messages and logs
    are sent only by the cluster leader to its peers, making the algorithm more understandable
    and easier to implement. The sample library that we'll use, in the theory section,
    is the Go one delivered by CoreOS Etcd, available at [https://github.com/coreos/etcd/tree/master/raft](https://github.com/coreos/etcd/tree/master/raft).
  prefs: []
  type: TYPE_NORMAL
- en: 'A Raft cluster is made of nodes that must maintain a replicated state machine
    in a consistent manner, no matter what: new nodes can join, old nodes can crash
    or become unavailable, but this state machine must be kept in sync.'
  prefs: []
  type: TYPE_NORMAL
- en: To achieve this failure-aware goal, typically Raft clusters consist of an odd
    number of nodes, such as three or five to avoid split-brains. A split-brain occurs
    when the remaining node(s) split themselves in groups that can't agree on a leader
    election. If there is an odd number of nodes, they can finally agree on a leader
    with a majority. With an even number, the election can close with a 50%-50% result,
    which should not happen.
  prefs: []
  type: TYPE_NORMAL
- en: Back to Raft, a Raft cluster is defined as a type raft struct in `raft.go` and
    includes information such as the leader UUID, the current term, a pointer to the
    log, and utilities to check the status of quorum and elections. Let's illustrate
    all these concepts step-by-step by decomposing the definition of the cluster component,
    a Node. A Node is defined as an interface in `node.go`, which is implemented canonically
    in this library as a `type node struct`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Every node keeps a tick (incremented by `Tick()`), denoting the term or period
    of time or epoch, of an arbitrary length, which is the current running instant.
    At every term, a node can be in one of the following StateType:'
  prefs: []
  type: TYPE_NORMAL
- en: Leader
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Candidate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Follower
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Under normal conditions, there is only one leader and all other nodes are followers.
    The leader, in order to make us respect its authority, sends heartbeat messages
    to its followers at regular intervals. When followers note that heartbeat messages
    are not arriving any longer, they understand that the leader is not available
    anymore and therefore they increment their values and become candidates and then
    attempt to become a leader by running `Campaign()`. They start with voting for
    themselves and trying to reach a quorum. When a node achieves this, a new leader
    is elected.
  prefs: []
  type: TYPE_NORMAL
- en: '`Propose()` is a method of proposal to append data to the log. A log is the
    data structure used in Raft to synchronize the cluster status and it''s another
    crucial concept in Etcd. It''s saved in a stable storage (memory), which has the
    ability to compact the log when it becomes huge to save space (snapshotting).
    The leader ensures that log is always in a consistent state and commits new data
    to append to its log (a master-log) only when it''s sure that this information
    has been replicated through the majority of its followers, so there is an agreement.
    There is a `Step()` method, which advances the state machine to the next step.'
  prefs: []
  type: TYPE_NORMAL
- en: '`ProposeConfChange()` is a method that allows us to change the cluster configuration
    at runtime. It is demonstrated to be safe under any condition, thanks to its two-phase
    mechanism that ensures that there is an agreement on this change from every possible
    majority. `ApplyConfChange()`applies this change to the current node.'
  prefs: []
  type: TYPE_NORMAL
- en: Then there is `Ready()`. In the Node interface, this function returns a read-only
    channel that returns encapsulated specification of messages that are ready to
    be read, saved to storage, and committed. Usually, after invoking Ready and applying
    its entries, a client must call `Advance()`, to notify that a progress in Ready
    has been made. In practice, `Ready()` and `Advance()` are parts of the method
    through which Raft keeps a high level of coherency, by avoiding inconsistencies
    in the log, its content, and status synchronization.
  prefs: []
  type: TYPE_NORMAL
- en: This is how Raft implementation looks like in CoreOS' Etcd.
  prefs: []
  type: TYPE_NORMAL
- en: Raft in practice
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you want to put your hands and practice Raft, a good idea is to use the `raftexample`
    from Etcd and start a three-member cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since Docker Compose YAML files are self-describing, the following example
    is of a compose file ready to be run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This template creates three Raft services (`raftexample1`, `raftexample2`, and
    `raftexample3`). Each runs an instance of raftexample, by exposing the APIs with
    `--port` and using a static cluster configuration with `--cluster`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can start this on a Docker host with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Now you can play, for example by killing the leader, observe new elections,
    set some values via API to one of the containers, remove the container, update
    the value, restart the container, retrieve this value, and note that it was correctly
    upgraded.
  prefs: []
  type: TYPE_NORMAL
- en: 'Interactions with the APIs can be done via curl, as described at [https://github.com/coreos/etcd/tree/master/contrib/raftexample](https://github.com/coreos/etcd/tree/master/contrib/raftexample):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We leave this exercise to the more enthusiastic readers.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When you're trying to adopt a Raft implementation, choose Etcd's Raft library
    for highest performance and choose Consul (from Serf library) for ready-to-use
    and easier implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Etcd
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Etcd is a highly available, distributed, and consistent key-value store that
    is used for shared configuration and service discovery. Some notable projects
    that use Etcd are SwarmKit, Kubernetes, and Fleet.
  prefs: []
  type: TYPE_NORMAL
- en: Etcd can gracefully manage master elections in case of network splits and can
    tolerate node failure, including the master. Applications, in our case Docker
    containers and Swarm nodes, can read and write data into Etcd's key-value storage,
    for example the location of services.
  prefs: []
  type: TYPE_NORMAL
- en: Re architecting the example of Chapter 1 with Etcd
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We once again create an example with one manager and three nodes, this time
    by illustrating Etcd.
  prefs: []
  type: TYPE_NORMAL
- en: 'This time, we''ll need a real discovery service. We can simulate a non-HA system
    by running the Etcd server inside Docker itself. We create a cluster made of four
    hosts, with the following names:'
  prefs: []
  type: TYPE_NORMAL
- en: '`etcd-m` will be the Swarm master and will host also the Etcd server'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`etcd-1`: The first Swarm node'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`etcd-2`: The second Swarm node'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`etcd-3`: The third Swarm node'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The operator, by connecting to `etcd-m:3376`, will operate Swarm on the three
    nodes, as usual.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by creating the hosts with Machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Now we will run the Etcd master on `etcd-m`. We use the `quay.io/coreos/etcd`
    official image from CoreOS, following the documentation available at [https://github.com/coreos/etcd/blob/master/Documentation/op-guide/clustering.md](https://github.com/coreos/etcd/blob/master/Documentation/op-guide/clustering.md).
  prefs: []
  type: TYPE_NORMAL
- en: 'First, in a terminal, we source the `etcd-m` shell variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we run the Etcd master in a single-host mode (that is, no fault-tolerance,
    and so on):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'What we do here is start the Etcd image in the daemon (`-d`) mode and expose
    ports `2379` (Etcd client communication), `2380` (Etcd server communication),
    `4001` (), and specify the following Etcd options:'
  prefs: []
  type: TYPE_NORMAL
- en: '`name`: The name of the node, in this case we select etcd-m, as the name of
    the node hosting this container'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initial-advertise-peer-urls` in this static configuration is the address:port
    of the cluster'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`listen-peer-urls`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`listen-client-urls`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`advertise-client-urls`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initial-cluster-token`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initial-cluster`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initial-cluster-state`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can ensure that this one-node Etcd cluster is healthy, using the `etcdctl
    cluster-health` command-line utility:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![Re architecting the example of Chapter 1 with Etcd](images/image_02_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This indicates that Etcd is at least up and running, so we can use it to set
    up a Swarm v1 cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'We create the Swarm manager on the same `etcd-m` host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This exposes the usual `3376` port from host to container, but this time starts
    the manager using the `etcd://` URL for the discovery service.
  prefs: []
  type: TYPE_NORMAL
- en: We now join the nodes, `etcd-1`, `etcd-2`, and `etcd-3`.
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, we can source and command machines, one, for each terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: With join `-advertise`, we order the local node to join the Swarm cluster, using
    the Etcd service running and exposed on `etcd-m`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now go to `etcd-m` and see the nodes of our cluster, by invoking the Etcd
    discovery service:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Re architecting the example of Chapter 1 with Etcd](images/image_02_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We have the three hosts already joined to the cluster as expected.
  prefs: []
  type: TYPE_NORMAL
- en: ZooKeeper
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ZooKeeper is another widely used and high-performance coordination service for
    distributed applications. Apache ZooKeeper was originally a subproject of Hadoop
    but is now a top-level project. It is a highly consistent, scalable, and reliable
    key-value store that can be used as a discovery service for a Docker Swarm v1
    cluster. As mentioned previously, ZooKeeper uses Paxos, rather than Raft.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to Etcd, when ZooKeeper forms a nodes cluster with a quorum, it has
    one leader and the remaining nodes are followers. Internally, ZooKeeper uses its
    own ZAB, ZooKeeper Broadcasting Protocol, to maintain consistency and integrity.
  prefs: []
  type: TYPE_NORMAL
- en: Consul
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last discovery service we're going to see here is Consul, a tool for discovering
    and configuring services. It provides an API that allows clients to register and
    discover services. Similar to Etcd and ZooKeeper, Consul is a key-value store
    with a REST API. It can perform health checks to determine service availability
    and uses the Raft consensus algorithm via the Serf library. Similar to Etcd and
    Zookeeper, of course, Consul can form a high availability quorum with leader election.
    Its member management system is based on `memberlist`, an efficient Gossip protocol
    implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Re architecting the example of Chapter 1 with Consul
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will now create another Swarm v1, but in this section we create machines
    on a cloud provider, DigitalOcean. To do so, you need an access token. However,
    if you don't have a DigitalOcean account, you can replace `--driver digitalocean`
    with `--driver virtualbox` and run this example locally.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by creating the Consul master:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We start the first agent here. Although we call it an agent, we are actually
    going to run it in the Server mode. We use the server mode (`-server`) and make
    it into the bootstrap node (`-bootstrap`). With these options, Consul will not
    perform the leader selection as it will force itself to be the leader.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: In case of a quorum for HA, the second and the third must be start with `-botstrap-expect
    3` to allow them to form a high availability cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can use `curl`  command  to test if our Consul quorum started successfully.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: If it's silent without showing any error, then Consul works correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we're going to create another three nodes on DigitalOcean.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s start the master and use Consul as a discovery mechanism:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s what we get when running the `swarm list` command: All nodes joined
    the Swarm, so the example is running.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Towards a decentralized discovery service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The limitation of a Swarm v1 architecture is that it uses a centralized and
    external discovery service. This approach makes every agent to talk to the external
    discovery service and the discovery service servers may see their load growing
    exponentially. From our experiments, for a 500-node cluster, we recommend to form
    an HA discovery service with at least three machines with medium-high specification,
    say 8 cores with 8 GB of RAM.
  prefs: []
  type: TYPE_NORMAL
- en: To properly address this problem, the discovery service used by SwarmKit and
    by Swarm Mode has been designed with decentralization in mind. Swarm mode uses
    the same discovery service codebase, Etcd, on all nodes, with no single point
    of failure.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we got familiar with the concept of consensus and discovery
    service. We understood that they play an essential role in orchestration clusters,
    as they provide services such as fault-tolerance and safe configurations. We analyzed
    a consensus algorithm, such as Raft in detail, before looking to two concrete
    Raft discovery services implementations, Etcd and Consul, putting things in practice
    and re-architecting basic examples with them. In the next chapter we're now going
    to start exploring SwarmKit and Swarm that use the embedded Etcd library.
  prefs: []
  type: TYPE_NORMAL
