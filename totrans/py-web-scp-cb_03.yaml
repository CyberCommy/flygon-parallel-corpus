- en: Processing Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Working with CSV and JSON data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing data using AWS S3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing data using MySQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing data using PostgreSQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing store data using Elasticsearch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to build robust ETL pipelines with AWS SQS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will introduce the use of data in JSON, CSV, and XML formats.
    This will include the means of parsing and converting this data to other formats,
    including storing that data in relational databases, search engines such as Elasticsearch,
    and cloud storage including AWS S3\. We will also discuss the creation of distributed
    and large-scale scraping tasks through the use of messaging systems including
    AWS Simple Queue Service (SQS).  The goal is to provide both an understanding
    of the various forms of data you may retrieve and need to parse, and an instruction
    the the various backends where you can store the data you have scraped.  Finally,
    we get a first introduction to one and Amazon Web Service (AWS) offerings.  By
    the end of the book we will be getting quite heavy into AWS and this gives a gentle
    introduction.
  prefs: []
  type: TYPE_NORMAL
- en: Working with CSV and JSON data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Extracting data from HTML pages is done using the techniques in the previous
    chapter, primarily using XPath through various tools and also with Beautiful Soup.
    While we will focus primarily on HTML, HTML is a variant of XML (eXtensible Markup
    Language).  XML one was the most popular for  of expressing data on the web, but
    other have become popular, and even exceeded XML in popularity.
  prefs: []
  type: TYPE_NORMAL
- en: Two common formats that you will see are JSON (JavaScript Object Notation) and
    CSV (Comma Separated Values).  CSV is easy to create and a common form for many
    spreadsheet applications, so many web sites provide data in that for, or you will
    need to convert scraped data to that format for further storage or collaboration.
    JSON really has become the preferred format, due to its easy within programming
    languages such as JavaScript (and Python), and many database now support it as
    a native data format.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe let's examine converting scraped data to CSV and JSON, as well
    as writing the data to files and also reading those data files from remote servers.
    The tools we will examine are the Python CSV and JSON libraries. We will also
    examine using `pandas` for these techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Also implicit in these examples is the conversion of XML data to CSV and JSON,
    so we won't have a dedicated section for those examples.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will be using the planets data page and converting that data into CSV and
    JSON files. Let''s start by loading the planets data from the page into a list
    of python dictionary objects. The following code (found in (`03/get_planet_data.py`)
    provides a function that performs this task, which will be reused throughout the
    chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the script gives the following output (briefly truncated):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'It may be required to install csv, json and pandas.  You can do that with the
    following three commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will start by converting the planets data into a CSV file.
  prefs: []
  type: TYPE_NORMAL
- en: 'This will be performed using `csv`.  The following code writes the planets
    data to a CSV file (the code is in`03/create_csv.py`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The output file is put into the www folder of our project.  Examining it we
    see the following content::'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We wrote this file into the www directory so that we can download it with our
    web server.
  prefs: []
  type: TYPE_NORMAL
- en: 'This data can now be used in applications that support CSV content, such as
    Excel:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/a00f3815-56b8-4bfb-bcd7-e9dbd035caa9.png)The File Opened in Excel'
  prefs: []
  type: TYPE_NORMAL
- en: 'CSV data can also be read from a web server using the `csv` library and by
    first retrieving the content with `requests` .  The following code is in the `03/read_csv_from_web.py`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The following is a portion of the output
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'One thing to point our is that the CSV writer left a trailing blank like would
    add an empty list item if not handled. This was handled by slicing the rows: This
    following statement returned all lines except the last one:'
  prefs: []
  type: TYPE_NORMAL
- en: '`lines = [line for line in reader][:-1]`'
  prefs: []
  type: TYPE_NORMAL
- en: 'This can also be done quite easily using pandas. The following constructs a
    DataFrame from the scraped data. The code is in `03/create_df_planets.py`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this gives the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'And the `DataFrame` can be saved to a CSV file with a simple call to `.to_csv()`
    (code is in `03/save_csv_pandas.py`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'A CSV file can be read in from a `URL` very easily with `pd.read_csv()` - no
    need for other libraries.  You can use the code in`03/read_csv_via_pandas.py`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Converting data to JSON is also quite easy. Manipulation of JSON with Python
    can be done with the Python `json` library.  This library can be used to convert
    Python objects to and from JSON. The following converts the list of planets into
    JSON and prints it to the console:prints the planets data as JSON (code in `03/convert_to_json.py`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Executing this script produces the following output (some of the output is
    omitted):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'And this can also be used to easily save JSON to a file (`03/save_as_json.py`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Checking the output using `!head -n 13 ../../www/planets.json` shows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'JSON can be read from a web server with `requests` and converted to a Python
    object (`03/read_http_json_requests.py`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'pandas also provides JSON capabilities to save to CSV (`03/save_json_pandas.py`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Unfortunately, there is not currently a way to pretty-print the JSON that is
    output from `.to_json()`. Also note the use of `orient='records'` and the use
    of `rest_index()`. This is necessary for reproducing an identical JSON structure
    to the JSON written using the JSON library example.
  prefs: []
  type: TYPE_NORMAL
- en: 'JSON can be read into a DataFrame using `.read_json()`, as well as from HTTP
    and files (`03/read_json_http_pandas.py`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `csv` and `json` libraries are a standard part of Python, and provide a
    straightforward means of reading and writing data in both formats.
  prefs: []
  type: TYPE_NORMAL
- en: pandas does not come as standard in some Python distributions and you will likely
    need to install it. The pandas functions for both CSV and JSON are also a much
    higher level in operation, with many powerful data operations available, and also
    with support for accessing data from remote servers.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The choice of csv, json, or pandas libraries is yours to make but I tend to
    like pandas and we will examine its use in scraping more throughout the book,
    although we won't get too deep into its usage.
  prefs: []
  type: TYPE_NORMAL
- en: For an in-depth understanding of pandas, check out `pandas.pydata.org`, or pick
    up my other book From Packt,  Learning pandas, 2ed.
  prefs: []
  type: TYPE_NORMAL
- en: For more info on the csv library, see [https://docs.python.org/3/library/csv.html](https://docs.python.org/3/library/csv.html)
  prefs: []
  type: TYPE_NORMAL
- en: For more on the json library, see [https://docs.python.org/3/library/json.html](https://docs.python.org/3/library/json.html)
  prefs: []
  type: TYPE_NORMAL
- en: Storing data using AWS S3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many cases where we just want to save content that we scrape into
    a local copy for archive purposes, backup, or later bulk analysis. We also might
    want to save media from those sites for later use. I've built scrapers for advertisement
    compliance companies, where we would track and download advertisement based media
    on web sites to ensure proper usage, and also to store for later analysis, compliance
    and transcoding.
  prefs: []
  type: TYPE_NORMAL
- en: The storage required for these types of systems can be immense, but with the
    advent of cloud storage services such as AWS S3 (Simple Storage Service), this
    becomes much easier and more cost effective than managing a large SAN (Storage
    Area Network) in your own IT department. Plus, S3 can also automatically move
    data from hot to cold storage, and then to long-term storage, such as a glacier,
    which can save you much more money.
  prefs: []
  type: TYPE_NORMAL
- en: We won't get into all of those details, but simply look at storing our `planets.html`
    file into an S3 bucket. Once you can do this, you can save any content you want
    to year hearts desire.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To perform the following example, you will need an AWS account and have access
    to secret keys for use in your Python code. They will be unique to your account.
     We will use the `boto3` library for S3 access. You can install this using `pip
    install boto3`.  Also, you will need to have environment variables set to authenticate. 
    These will look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`AWS_ACCESS_KEY_ID=AKIAIDCQ5PH3UMWKZEWA`'
  prefs: []
  type: TYPE_NORMAL
- en: '`AWS_SECRET_ACCESS_KEY=ZLGS/a5TGIv+ggNPGSPhGt+lwLwUip7u53vXfgWo`'
  prefs: []
  type: TYPE_NORMAL
- en: These are available in the AWS portal under IAM (Identity Access Management)
    portion of the portal.
  prefs: []
  type: TYPE_NORMAL
- en: It's a good practice to put these keys in environment variables.  Having them
    in code can lead to their theft.  During the writing of this book, I had this
    hard coded and accidentally checked them in to GitHub.  The next morning I woke
    up to critical messages from AWS that I had thousands of servers running!  There
    are GitHub scrapers looking for these keys and they will get found and use for
    nefarious purposes.  By the time I had them all turned off, my bill was up to
    $6000, all accrued overnight. Thankfully, AWS waived these fees!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We won''t parse the data in the `planets.html` file, but simply retrieve it
    from the local web server using requests:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code, (found in `03/S3.py`), reads the planets web page and stores
    it in S3:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This app will give you output similar to the following, which is S3 info telling
    you various facts about the new item.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This output shows us that the object was successfully created in the bucket.
    At this point, you can navigate to the S3 console and see your bucket:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/29fbd119-7ee5-43eb-8b2f-9bc34998ff53.png)The Bucket in S3'
  prefs: []
  type: TYPE_NORMAL
- en: 'Inside the bucket you will see the `planet.html` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/49cc32c4-5ac3-4177-a397-35385afbcf4e.png)The File in the Bucket'
  prefs: []
  type: TYPE_NORMAL
- en: 'By clicking on the file you can see the property and URL to the file within
    S3:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/6c5b035d-009f-4878-9806-034b4db8e500.png)The Properties of the File
    in S3'
  prefs: []
  type: TYPE_NORMAL
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The boto3 library wraps the AWS S3 API in a Pythonic syntax. The .`client()`
    call authenticates with AWS and gives us an object to use to communicate with
    S3\. Make sure you have your keys in environment variables, as otherwise this
    will not work.
  prefs: []
  type: TYPE_NORMAL
- en: The bucket name must be globally unique. At the time of writing, this bucket
    is available, but you will likely need to change the name. The `.create_bucket()`
    call creates the bucket and sets its ACL. `put_object()` uses the `boto3` upload
    manager to upload the scraped data into the object in the bucket.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There a lot of details to learn for working with S3\. You can find API documentation
    at: [http://docs.aws.amazon.com/AmazonS3/latest/API/Welcome.html](http://docs.aws.amazon.com/AmazonS3/latest/API/Welcome.html).
    Boto3 documents can be found at: [https://boto3.readthedocs.io/en/latest/](https://boto3.readthedocs.io/en/latest/).'
  prefs: []
  type: TYPE_NORMAL
- en: While we only saved a web page, this model can be used to store any type of
    file based data in S3.
  prefs: []
  type: TYPE_NORMAL
- en: Storing data using MySQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MySQL is a freely available, open source Relational Database Management System
    (RDBMS).  In this example, we will read the planets data from the website and
    store it into a MySQL database.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will need to have access to a MySQL database. You can install one locally
    installed, in the cloud, within a container.  I am using a locally installed MySQL
    server and have the `root` password set to `mypassword`. You will also need to
    install the MySQL python library.  You can do this with `pip install mysql-connector-python`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing to do is to connect to the database using the `mysql` command
    at the terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can create a database that will be used to store our scraped information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now use the new database:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'And create a Planets table in the database to store our data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Now we are ready to scrape data and put it into the MySQL database.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following code (found in `03/store_in_mysql.py`) will read the planets
    data and write it to MySQL:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Using MySQL Workbench we can see the the records were written to the database
    (you could use the mysql command line also):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/c8a2c090-dce7-40f2-b0b3-0c6d72ff3885.png)Records displayed using
    MySQL Workbench'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code can be used to retrieve the data (`03/read_from_mysql.py`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Accessing a MySQL database using the `mysql.connector` involves the use of
    two classes from the library: `connect` and `cursor`. The `connect` class opens
    and manages a connection with the database server.  From that connection object,
    we can create a cursor object.  This cursor is used for reading and writing data
    using SQL statements.'
  prefs: []
  type: TYPE_NORMAL
- en: In the first example, we used the cursor to insert nine records into the database.
    Those records are not written to the database until the `commit()` method of the
    connection is called.  This executes the writes of all the rows to the database.
  prefs: []
  type: TYPE_NORMAL
- en: Reading data uses a similar model except that we execute an SQL query (`SELECT`)
    using the cursor and iterate across the rows that were retrieved. Since we are
    reading and not writing, there is no need to call `commit()` on the connection.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can learn more about MySQL and install it from:  `https://dev.mysql.com/doc/refman/5.7/en/installing.html`.
    Information on MySQL Workbench is available at:  `https://dev.mysql.com/doc/workbench/en/`.
  prefs: []
  type: TYPE_NORMAL
- en: Storing data using PostgreSQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe we store our planet data in PostgreSQL. PostgreSQL is an open
    source relational database management system (RDBMS). It is developed by a worldwide
    team of volunteers, is not controlled by any corporation or other private entity,
    and the source code is available free of charge.  It has a lot of unique features
    such as hierarchical data models.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First make sure you have access to a PostgreSQL data instance.  Again, you can
    install one locally, run one in a container, or get an instance in the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: As with MySQL, we need to first create a database. The process is almost identical
    to that of MySQL but with slightly different commands and parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the terminal execute the psql command at the terminal.  This takes you
    into the psql command processor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now create the scraping database:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Then switch to the new database:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can create the Planets table. We first need to create a sequence table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'And now we can create the table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: To access PostgreSQL from Python we will use the `psycopg2` library, so make
    sure it is installed in your Python environment using `pip install psycopg2`.
  prefs: []
  type: TYPE_NORMAL
- en: We are now ready to write Python to store the planets data in PostgreSQL.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We proceed with the recipe as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code will read the planets data and write it to the database
    (code in `03/save_in_postgres.py`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'If successful you will see the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Using GUI tools such as pgAdmin you can examine the data within the database:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/e1060188-c3d3-4a2d-aaf4-4f9124294d9e.png)Records Displayed in pgAdmin'
  prefs: []
  type: TYPE_NORMAL
- en: 'The data can be queried with the following Python code (found in `03/read_from_postgresql.py`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'And results in the following output (truncated a little bit:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Accessing a PostgreSQL database using the `psycopg2` library as we did involves
    the use of two classes from the library: `connect` and `cursor`. The `connect`
    class opens and manages a connection with the database server. From that connection
    object, we can create a `cursor` object. This cursor is used for reading and writing
    data using SQL statements.'
  prefs: []
  type: TYPE_NORMAL
- en: In the first example, we used the cursor to insert nine records into the database.
     Those records are not written to the database until the `commit()` method of
    the connection is called.  This executes the writes of all the rows to the database.
  prefs: []
  type: TYPE_NORMAL
- en: Reading data uses a similar model, except that we execute an SQL query (`SELECT`)
    using the cursor and iterate across the rows that were retrieved.  Since we are
    reading and not writing, there is no need to call `commit()` on the connection.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Information on PostgreSQL is available at `https://www.postgresql.org/`.  pgAdmin
    can be obtained at: `https://www.pgadmin.org/`  Reference materials for `psycopg`
    are at: `http://initd.org/psycopg/docs/usage.html`'
  prefs: []
  type: TYPE_NORMAL
- en: Storing data in Elasticsearch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Elasticsearch is a search engine based on Lucene. It provides a distributed,
    multitenant-capable, full-text search engine with an HTTP web interface and schema-free
    JSON documents. It is a non-relational database (often stated as NoSQL), focusing
    on the storage of documents instead of records. These documents can be many formats,
    one of which is useful to us: JSON.  This makes using Elasticsearch very simple
    as we do not need to convert our data to/from JSON.  We will use Elasticsearch
    much more later in the book'
  prefs: []
  type: TYPE_NORMAL
- en: For now, let's go and store our planets data in Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will access a locally installed Elasticsearch server. To do this from Python,
    we will use the `Elasticsearch-py` library. It is most likely that you will need
    to install this using pip: `pip install elasticsearch`.'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike PostgreSQL and MySQL, we do not need to create tables in Elasticsearch
    ahead of time. Elasticsearch does not care about structured data schemas (although
    it does have indexes), so we don't have to go through this procedure.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Writing data to Elasticsearch is really simple. The following Python code performs
    this task with our planets data (`03/write_to_elasticsearch.py`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Executing this results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The output shows the result of each insertion, giving us information such as
    the `_id` assigned to the document by elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have logstash and kibana installed too, you can see the data inside
    of Kibana:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/d9054e7d-acb5-4324-8bcd-33c00accd7c8.png)Kibana Showing and Index'
  prefs: []
  type: TYPE_NORMAL
- en: 'And we can query the data with the following Python code. This code retrieves
    all of the documents in the ''planets'' index and prints the name, mass, and radius
    of each planet (`03/read_from_elasticsearch.py`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Elasticsearch is both a NoSQL database and a search engine. You give documents
    to Elasticsearch and it parses the data in the documents and creates search indexes
    for that data automatically.
  prefs: []
  type: TYPE_NORMAL
- en: During the insertion process, we used the `elasticsearch` libraries' `.index()`
    method and specified an index, named "planets", a document type, `planets_info`,
    and the finally the body of the document, which is our planet Python object. The
    `elasticsearch` library that object to JSON and sends it off to Elasticsearch
    for storage and indexing.
  prefs: []
  type: TYPE_NORMAL
- en: The index parameter is used to inform Elasticsearch how to create an index,
    which it will use for indexing and which we can use to specify a set of documents
    to search for when we query. When we performed the query, we specified the same
    index "planets" and executed a query to match all of the documents.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can find out much more about elasticsearch at: `https://www.elastic.co/products/elasticsearch`.
     Information on the python API can be found at: `http://pyelasticsearch.readthedocs.io/en/latest/api/`'
  prefs: []
  type: TYPE_NORMAL
- en: We will also come back to Elasticsearch in later chapters of the book.
  prefs: []
  type: TYPE_NORMAL
- en: How to build robust ETL pipelines with AWS SQS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scraping a large quantity of sites and data can be a complicated and slow process. 
    But it is one that can take great advantage of parallel processing, either locally
    with multiple processor threads, or distributing scraping requests to report scrapers
    using a message queue system. There may also be the need for multiple steps in
    a process similar to an Extract, Transform, and Load pipeline (ETL). These pipelines
    can also be easily built using a message queuing architecture in conjunction with
    the scraping.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using a message queuing architecture gives our pipeline two advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Robustness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scalability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The processing becomes robust, as if processing of an individual message fails,
    then the message can be re-queued for processing again. So if the scraper fails,
    we can restart it and not lose the request for scraping the page, or the message
    queue system will deliver the request to another scraper.
  prefs: []
  type: TYPE_NORMAL
- en: It provides scalability, as multiple scrapers on the same, or different, systems
    can listen on the queue. Multiple messages can then be processed at the same time
    on different cores or, more importantly, different systems. In a cloud-based scraper,
    you can scale up the number of scraper instances on demand to handle greater load.
  prefs: []
  type: TYPE_NORMAL
- en: 'Common message queueing systems that can be used include: Kafka, RabbitMQ,
    and Amazon SQS. Our example will utilize Amazon SQS, although both Kafka and RabbitMQ
    are quite excellent to use (we will see RabbitMQ in use later in the book). We
    use SQS to stay with a model of using AWS cloud-based services as we did earlier
    in the chapter with S3.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As an example, we will build a vary simple ETL process that will read the main
    planets page and store the planets data in MySQL. It will also pass a single message
    for each *more info* link in the page to a queue, where 0 or more processes can
    receive those requests and perform further processing on those links.
  prefs: []
  type: TYPE_NORMAL
- en: To access SQS from Python, we will revisit using the `boto3` library.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it - posting messages to an AWS queue
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `03/create_messages.py` file contains code to read the planets data and
    to post the URL in the MoreInfo property to an SQS queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the code in a terminal and you will see output similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Now go into the AWS SQS console. You should see the queue has been created
    and that it holds 9 messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/2ad3b7c1-9f39-4d02-ac61-d23619a9c409.png)The Queue in SQS'
  prefs: []
  type: TYPE_NORMAL
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code connects to the given account and the us-west-2 region of AWS. A queue
    is then created if one does not exist. Then, for each planet in the source content,
    the program sends a message which consists of the *more info* URL for the planet.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, there is no one listening to the queue, so the messages will
    sit there until eventually read or they expire. The default life for each message
    is 4 days.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it - reading and processing messages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To process the messages, run the `03/process_messages.py` program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the script using `python process_messages.py`.  You will see output similar
    to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The program connects to SQS and opens the queue. Opening the queue for reading
    is also done using `sqs.create_queue`, which will simply return the queue if it
    already exists.
  prefs: []
  type: TYPE_NORMAL
- en: Then, it enters a loop calling `sqs.receive_message`, specifying the URL of
    the queue, the number of messages to receive in each read, and the maximum amount
    of time to wait in seconds if there are no messages available.
  prefs: []
  type: TYPE_NORMAL
- en: If a message is read, the URL in the message is retrieved and scraping techniques
    are used to read the page at the URL and extract the planet's name and information
    about its albedo.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we retrieve the receipt handle of the message. This is needed to delete
    the message from the queue. If we do not delete the message, it will be made available
    in the queue after a period of time.   So if our scraper crashed and didn't perform
    this acknowledgement, the messages will be made available again by SQS for another
    scraper to process (or the same one when it is back up).
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can find more information about S3 at: `https://aws.amazon.com/s3/`.  Specifics
    on the details of the API are available at: `https://aws.amazon.com/documentation/s3/`.'
  prefs: []
  type: TYPE_NORMAL
