- en: Testing Apache Spark Jobs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will test Apache Spark jobs and learn how to separate logic
    from the Spark engine.
  prefs: []
  type: TYPE_NORMAL
- en: We will first cover unit testing of our code, which will then be used by the
    integration test in SparkSession. Later, we will be mocking data sources using
    partial functions, and then learn how to leverage ScalaCheck for property-based
    testing for a test as well as types in Scala. By the end of this chapter, we will
    have performed tests in different versions of Spark.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will be covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Separating logic from Spark engine-unit testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration testing using SparkSession
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mocking data sources using partial functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using ScalaCheck for property-based testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing in different versions of Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Separating logic from Spark engine-unit testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start by separating logic from the Spark engine.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a component with logic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unit testing of that component
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the case class from the model class for our domain logic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look at the logic first and then the simple test.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we have a `BonusVerifier` object that has only one method, `quaifyForBonus`,
    that takes our `userTransaction` model class. According to our login in the following
    code, we load user transactions and filter all users that are qualified for a
    bonus. First, we need to test it to create an RDD and filter it. We need to create
    a SparkSession and also create data for mocking an RDD or DataFrame, and then
    test the whole Spark API. Since this involves logic, we will test it in isolation.
    The logic is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We have a list of super users with the `A`, `X`, and `100-million` user IDs.
    If our `userTransaction.userId` is within the `superUsers` list, and if the `userTransaction.amount`
    is higher than `100`, then the user qualifies for a bonus; otherwise, they don't.
    In the real world, the qualifier for bonus logic will be even more complex, and
    thus it is very important to test the logic in isolation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows our test using the `userTransaction` model. We know
    that our user transaction includes `userId` and `amount`. The following example
    shows our domain model object, which is shared between a Spark execution integration
    test and our unit testing, separated from Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to create our `UserTransaction` for user ID `X` and the amount `101`,
    as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then pass `userTransaction` to `qualifyForBonus` and the result should
    be `true`. This user should qualify for a bonus, as shown in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/49908d22-b2a3-4b0b-9150-e533a6cda546.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s write a test for the negative use case, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we have a user, `X`, that spends `99` for which our results should be
    false. When we validate our code, we can see, from the following output, that
    our test has passed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fee898ff-e179-4e5b-941a-a434a8c812c9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We have covered two cases, but in real-world scenarios, there are many more.
    For example, if we want to test the case where we are specifying `userId`, which
    is not from this superuser list, and we have `some_new_user` that spends a lot
    of money, in our case, `100000`, we get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s assume that it should not qualify, and so such logic is a bit complex.
    Therefore, we are testing it in a unit test way:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/028fcf7e-9829-424e-a479-5497fc29c89d.png)'
  prefs: []
  type: TYPE_IMG
- en: Our tests are very fast and so we are able to check that everything works as
    expected without introducing Spark at all. In the next section, we'll be changing
    the logic with integration testing using SparkSession.
  prefs: []
  type: TYPE_NORMAL
- en: Integration testing using SparkSession
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's now learn about integration testing using SparkSession.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging SparkSession for integration testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a unit tested component
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here, we are creating the Spark engine. The following line is crucial for the
    integration test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: It is not a simple line just to create a lightweight object. SparkSession is
    a really heavy object and constructing it from scratch is an expensive operation
    from the perspective of resources and time. Tests such as creating SparkSession
    will take more time compared to the unit testing from the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: For the same reason, we should use unit tests often to convert all edge cases
    and use integration testing only for the smaller part of the logic, such as the
    capital edge case.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example shows the array we are creating:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The following example shows the RDD we are creating:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This is the first time that Spark has been involved in our integration testing.
    Creating an RDD is also a time-consuming operation. Compared to just creating
    an array, it is really slow to create an RDD because that is also a heavy object.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now use our `data.filter` to pass a `qualifyForBonus` function, as
    shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This function was already unit tested, so we don't need to consider all edge
    cases, different IDs, different amounts, and so on. We are just creating a couple
    of IDs with some amounts to test whether or not our whole chain of logic is working
    as expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'After we have applied this logic, our output should be similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s start this test and check how long it takes to execute a single integration
    test, as shown in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e492184d-4a2b-4069-bb7f-86937d6c0393.png)'
  prefs: []
  type: TYPE_IMG
- en: It took around `646 ms` to execute this simple test.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to cover every edge case, the value will be multiplied by a factor
    of hundreds compared to the unit test from the previous section. Let''s start
    this unit test with three edge cases, as shown in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4cb08050-118a-499c-8694-469a568f3a1e.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that our test took only `18 ms`, which means that it was 20 times
    faster, even though we covered three edge cases, compared to integration tests
    with only one case.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we have covered a lot of logic with hundreds of edge cases, and we can
    conclude that it is really wise to have unit tests at the lowest possible level.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will be mocking data sources using partial functions.
  prefs: []
  type: TYPE_NORMAL
- en: Mocking data sources using partial functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Spark component that reads data from Hive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mocking the component
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing the mock component
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s assume that the following code is our production line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we are using the `UserDataLogic.loadAndGetAmount` function, which needs
    to load our user data transaction and get the amount of the transaction. This
    method takes two arguments. The first argument is a `sparkSession` and the second
    argument is the `provider` of `sparkSession`, which takes `SparkSession` and returns
    `DataFrame`, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'For production, we will load user transactions and see that the `HiveDataLoader`
    component has only one method, `sparkSession.sql`, and `("select * from transactions")`,
    as shown in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This means that the function goes to Hive to retrieve our data and returns a
    DataFrame. According to our logic, it executes the `provider` that is returning
    a DataFrame and from a DataFrame, it is only selecting `amount`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This logic is not simple we can test because our SparkSession `provider` is
    interacting with the external system in production. So, we can create a function
    such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see how to test such a component. First, we will create a DataFrame
    of user transactions, which is our mock data, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: However, we need to save the data to Hive, embed it, and then start Hive.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we are using the partial functions, we can pass a partial function as
    a second argument, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The first argument is `spark`, but it is not used in our method this time. The
    second argument is a method that is taking SparkSession and returning DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: However, our execution engine, architecture, and code do not consider whether
    this SparkSession is used or if the external call is made; it only wants to return
    DataFrame. We can `_` our first argument because it's ignored and just return
    DataFrame as the return type.
  prefs: []
  type: TYPE_NORMAL
- en: And so our `loadAndGetAmount` will get a mock DataFrame, which is the DataFrame
    that we created.
  prefs: []
  type: TYPE_NORMAL
- en: 'But, for the logic shown, it is transparent and doesn''t consider whether the
    DataFrame comes from Hive, SQL, Cassandra, or any other source, as shown in the
    following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In our example, `df` comes from the memory that we created for the purposes
    of the test. Our logic continues and it selects the amount.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we show our columns, `res.show()` , and that logic should end up with
    one column amount. Let''s start this test, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8dca11c2-8423-432f-9233-897f3fba401b.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see from the preceding example that our resulting DataFrame has one column
    amount in `100` and `200` values. This means it worked as expected, without the
    need to start an embedding Hive. The key here is to use a provider and not embed
    our select start within our logic.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll be using ScalaCheck for property-based tests.
  prefs: []
  type: TYPE_NORMAL
- en: Using ScalaCheck for property-based testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Property-based testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a property-based test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look at a simple property-based test. We need to import a dependency before
    we define properties. We also need a dependency for the ScalaCheck library, which
    is a library for property-based tests.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous section, every test extended `FunSuite`. We used functional
    tests, but we had to provide arguments explicitly. In this example, we''re extending
    `Properties` from the ScalaCheck library and testing a `StringType`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Our ScalaCheck will generate a random string for us. If we create a property-based
    test for a custom type, then that is not known to the ScalaCheck. We need to provide
    a generator that will generate instances of that specific type.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s define the first property of our string type in the following
    way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '`forAll` is a method from the ScalaCheck property. We will pass an arbitrary
    number of arguments here, but they need to be of the type that we are testing.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's assume that we want to get two random strings, and in those strings, the
    invariants should be perceived.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we are adding the length of string `a` to the length of string `b`, the
    sum of that should be greater or equal to `a.length,` because if `b` is `0` then
    it will be equal, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: However, this is an invariant of `string` and for every input string, it should
    be `true`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second property that we are defining is a bit more complex, as shown in
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we asked the ScalaCheck runtime engine to share three
    strings this time, that is, `a`, `b`, and `c`. We will test this when we create
    a list of strings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we are creating a list of strings, that is, `a`, `b`, `c`, as shown in
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: When we map every element to `length`, the sum of those elements should be equal
    to adding everything by length. Here, we have `a.length + b.length + c.length` and
    we will test the collections API to check if the map and other functions work
    as expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start this property-based test to check if our properties are correct,
    as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9319cfda-c10b-40bf-8d3b-9e41fa26fe7f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see that the `StringType.length` property of `string` passed and executed
    `100` tests. It could be surprising that `100` tests were executed, but let''s
    try to see what was passed as the arguments by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We will print the `a` argument and `b` argument, and retry our property by
    testing the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a7df281d-ce0d-408d-a174-96b691f2a45b.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that a lot of weird strings were generated, so this is an edge case
    that we were not able to create up-front. Property-based testing will create a
    very weird unique code that isn't a proper string. So, this is a great tool for
    testing whether our logic is working as expected for a specific type.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll be testing in different versions of Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Testing in different versions of Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Changing the component to work with Spark pre-2.x
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mock testing pre-2.x
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RDD mock testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start with the mocking data sources from the third section of this chapter—*Mocking
    data sources using partial functions*.
  prefs: []
  type: TYPE_NORMAL
- en: Since we were testing `UserDataLogic.loadAndGetAmount`, notice that everything
    operates on the DataFrame and thus we had a SparkSession and DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s compare it to the Spark pre-2.x. We can see that this time, we
    are unable to use DataFrames. Let''s assume that the following example shows our
    logic from the previous Sparks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We can see that we are not able to use DataFrames this time.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous section, `loadAndGetAmount` was taking `spark` and DataFrame,
    but the DataFrame in the following example is an RDD, not a DataFrame anymore,
    and so we are passing an `rdd`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'However, we need to create a different `UserDataLogicPre2` for Spark that takes
    SparkSession and returns an RDD after mapping an RDD of an integer, as shown in
    the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we can see that the `provider` is executing our provider
    logic, mapping every element, getting it as an `int`. Then, we get the amount.
    `Row` is a generic type that can have a variable number of arguments.
  prefs: []
  type: TYPE_NORMAL
- en: In Spark pre-2.x, we do not have `SparkSession` and therefore we need to use `SparkContext` and
    change our login accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we first learned how to separate logic from the Spark engine.
    We then looked at a component that was well-tested in separation without the Spark
    engine, and we carried out integration testing using SparkSession. For this, we created
    a SparkSession test by reusing the component that was already well-tested. By
    doing that, we did not have to cover all edge cases in the integration test and
    our test was much faster. We then learned how to leverage partial functions to
    supply mocked data that's provided at the testing phase. We also covered ScalaCheck
    for property-based testing. By the end of this chapter, we had tested our code
    in different versions of Spark and learned how to change our DataFrame mock test
    to RDD.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to leverage the Spark GraphX API.
  prefs: []
  type: TYPE_NORMAL
