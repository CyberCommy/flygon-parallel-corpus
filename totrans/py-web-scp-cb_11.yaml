- en: Making the Scraper as a Service Real
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating and configuring an Elastic Cloud trial account
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessing the Elastic Cloud cluster with curl
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connecting to the Elastic Cloud cluster with Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing an Elasticsearch query with the Python API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Elasticsearch to query for jobs with specific skills
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modifying the API to search for jobs by skill
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing configuration in the environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating an AWS IAM user and a key pair for ECS
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Docker to authenticate with ECR
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pushing containers into ECR
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating an ECS cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a task to run our containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Starting and accessing the containers in AWS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will first add a feature to search job listings using Elasticsearch
    and extend the API for this capability. Then will move Elasticsearch functions
    to Elastic Cloud, a first step in cloud-enabling our cloud based scraper.  Then,
    we will move our Docker containers to Amazon **Elastic Container Repository**
    (**ECR**), and finally run our containers (and scraper) in Amazon **Elastic Container
    Service** (**ECS**).
  prefs: []
  type: TYPE_NORMAL
- en: Creating and configuring an Elastic Cloud trial account
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe we will create and configure an Elastic Cloud trial account so
    that we can use Elasticsearch as a hosted service.  Elastic Cloud is a cloud service
    offered by the creators of Elasticsearch, and provides a completely managed implementation
    of Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: While we have examined putting Elasticsearch in a Docker container, actually
    running a container with Elasticsearch within AWS is very difficult due to a number
    of memory requirements and other system configurations that are complicated to
    get working within ECS.  Therefore, for a cloud solution, we will use Elastic
    Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll proceed with the recipe as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open your browser and navigate to [https://www.elastic.co/cloud/as-a-service/signup](https://www.elastic.co/cloud/as-a-service/signup).
    You will see a page similar to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/db8027f8-ba93-421c-a026-fb2cedd5ddcb.png)The Elastic Cloud signup
    page'
  prefs: []
  type: TYPE_NORMAL
- en: 'Enter your email and press the Start Free Trial button.  When the email arrives,
    verify yourself.  You will be taken to a page to create your cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/c08434c1-5481-4c1f-8582-674152731121.png)Cluster creation page'
  prefs: []
  type: TYPE_NORMAL
- en: 'I''ll be using AWS (not Google) in the Oregon (us-west-2) region in other examples,
    so I''ll pick both of those  for this cluster.  You can pick a cloud and region
    that works for you.  You can leave the other options as it is, and just press
    create.  You will then be presented with your username and password.  Jot those
    down. The following screenshot gives an idea of how it displays the username and
    password:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/c8d98099-d766-43d2-9825-62dc4ddd9b9c.png)The credentials info for
    the Elastic Cloud accountWe won''t use the Cloud ID in any recipes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, you will be presented with your endpoints.  The Elasticsearch URL is
    what''s important to us:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/054dab3f-ed34-43a7-9cab-099198e4bc8a.png)'
  prefs: []
  type: TYPE_IMG
- en: And that's it - you are ready to go (at least for 14 days)!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Accessing the Elastic Cloud cluster with curl
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Elasticsearch is fundamentally accessed via a REST API.  Elastic Cloud is no
    different and is actually an identical API.  We just need to be able to know how
    to construct the URL properly to connect.  Let's look at that.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We proceed with the recipe as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'When you signed up for Elastic Cloud, you were given various endpoints and
    variables, such as username and password.  The URL was similar to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Depending on the cloud and region, the rest of the domain name, as well as the
    port, may differ.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll use a slight variant of the following URL to communicate and authenticate
    with Elastic Cloud:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Currently, mine is (it will be disabled by the time you read this):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Basic authentication and connectivity can be checked with curl:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: And we are up and talking!
  prefs: []
  type: TYPE_NORMAL
- en: Connecting to the Elastic Cloud cluster with Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now let's look at how to connect to Elastic Cloud using the Elasticsearch Python
    library.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code for this recipe is in the `11/01/elasticcloud_starwars.py` script. 
    This script will scrape Star Wars character data from the swapi.co API/website
    and put it into the Elastic Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We proceed with the recipe as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the file as a Python script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This will loop through up to 20 characters and drop them into the `sw` index
    with a document type of `people`.  The code is straightforward (replace the URL
    with yours):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The connection is made using the URL with the username and password added to
    it.  The data is pulled from swapi.co using a GET request and then with a call
    to `.index()` on the Elasticsearch object.  You''ll see output similar to the
    following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When you signed up for Elastic Cloud, you were also given a URL to Kibana. 
    Kibana is a powerful graphical frontend to Elasticsearch:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the URL in your browser.  You''ll see see a login page:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/8cba8c2e-8870-4c2c-a721-4a1787387ad8.png)The Kibana login page'
  prefs: []
  type: TYPE_NORMAL
- en: 'Enter your username and password and you''ll be taken to the main dashboard:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/6aff28f8-f90a-4018-a809-990fceae344a.png)Creating an index pattern'
  prefs: []
  type: TYPE_NORMAL
- en: 'We''re being asked to create an index pattern for the one index that was created
    by our app: sw.  In the index pattern textbox, enter `sw*` and then press Next
    step.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll be asked to select a time filter field name.  Select I don''t want to
    use the Time Filter and press the Create Index Pattern button. A few moments later,
    you will see a confirmation of the index that was created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/951c8ef3-95f8-454a-b76c-7315b6f68b47.png)The index that was created'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now click the Discover menu item, and you''ll be taken to the interactive data
    explorer, where you will see the data we just entered:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/3c49c59b-7f1d-4f48-b3bc-081f342a95b8.png)The data added to our index'
  prefs: []
  type: TYPE_NORMAL
- en: Here you can navigate through the data and see just how effectively Elasticsearch
    stored and organized this data.
  prefs: []
  type: TYPE_NORMAL
- en: Performing an Elasticsearch query with the Python API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now let's look at how we can search Elasticsearch using the Elasticsearch Python
    library.  We will perform a simple search on the Star Wars index.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Make sure to modify the connection URL in the samples to your URL.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code for the search is in the `11/02/search_starwars_by_haircolor.py` script,
    and can be run by simply executing the script.  This is a fairly simple search
    to find the characters whose hair color is `blond`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The main portion of the code is:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'A search is performed by constructing a dictionary that expresses an Elasticsearch
    DSL query.  In this case, our query asks for all documents where the `"hair_color"`
    property is `"blond"`.  This object is then passed as the body parameter of the
    `.search` method.  The result of this method is a diction describing what was
    found (or not).  In this case:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The results give us some metadata about the search execution and then the results
    in the `hits` property.  Each hit returns the actual document as well as the index
    name, document type, document ID, and a score.  The score is a lucene calculation
    of the relevance of the document to the search query.  While this query uses an
    exact match of a property to a value, you can see that these two documents still
    have different scores.  I'm not sure why in this case, but searching can also
    be less exact and based on various built-in heuristics to find items "like" a
    certain sentence, that is, such as when you enter text into a Google search box.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Elasticsearch search DSL, and the search engine itself, is very powerful
    and expressive.  We'll only look at this example and one more in the next recipe,
    so we don't go into it in much detail.  To find out more about the DSL, you can
    start with the official documentation at [https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl.html](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl.html).
  prefs: []
  type: TYPE_NORMAL
- en: Using Elasticsearch to query for jobs with specific skills
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we move back to using the crawler that we created to scrape
    and store job listings from StackOverflow in Elasticsearch.  We then extend this
    capability to query Elasticsearch to find job listings that contain one or more
    specified skills.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The example we will use is coded to use a local Elastic Cloud engine and not
    a local Elasticsearch engine.  You can change that if you want.  For now, we will
    perform this process within a single python script that is run locally and not
    inside a container or behind an API.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We proceed with the recipe as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for the recipe is in the `11/03/search_jobs_by_skills.py` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The first part of this code defines four job listings to be put into Elasticsearch,
    if they already are not available.  It iterates through this job's ID, and if
    not already available, retrieves them and puts them in Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: The remainder of this defines a query to be executed against Elasticsearch,
    and follows the same pattern for executing the search. The only difference is
    in the definition of the search criteria.  Ultimately, we want to match a list
    of job skills to those in the job listings.
  prefs: []
  type: TYPE_NORMAL
- en: This query simply matches a single skill to those in the skills field in our
    job listings documents. The sample specifies that we want to match to the JSON.skills
    property in the target documents. The skills in those documents are just beneath
    the root of the document, so in this syntax we preface it with JSON.
  prefs: []
  type: TYPE_NORMAL
- en: This property in Elasticsearch is an array, and the query value we have will
    match the document if any of the values in that property array are `"c#"`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Running this search with just those four documents in Elasticsearch results
    in the following (the output here just shows the results and not the complete
    contents of the four documents returned):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Each of the jobs placed in Elasticsearch has C# for a skill (I randomly picked
    these documents, so this is a little bit of a coincidence).
  prefs: []
  type: TYPE_NORMAL
- en: 'The results of these searches return the entire contents of each of the documents
    that are identified.  If we don''t want the entire document returned for each
    hit, we can change the query to make this happen. Let''s modify the query to only
    return the ID in the hits. Change the `search_definition` variable to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Including the `"_source"` property tells Elasticsearch to return the specified
    document properties in the result.  Executing this query results in the following
    output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Each of the hits now only returns the ID property of the document. This will
    help control the size of the result if there are a lot of hits.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get to the ultimate goal of this recipe, identifying documents that
    have multiple skills.  This is actually a very simple change to `search_defintion`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This states that we only want documents where the skills contain both `"c#"`
    and `"sql"`.  The result from running the script is then the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The result set is now cut down to two hits, and if you check, these are the
    only two with those values in the skills.
  prefs: []
  type: TYPE_NORMAL
- en: Modifying the API to search for jobs by skill
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will modify our existing API to add a method to enable searching
    for jobs with a set of skills.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will be extending the API code.  We will make two fundamental changes to
    the implementation of the API.  The first is that we will add an additional Flask-RESTful 
    API implementation for the search capability, and the second is that we will make
    addresses for both Elasticsearch and our own microservice configurable by environment
    variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'The API implementation is in `11/04_scraper_api.py`.   By default, the implementation
    attempts to connect to Elasticsearch on the local system.  If you are using Elastic
    Cloud, make sure to change the URL (and make sure you have documents in the index):'
  prefs: []
  type: TYPE_NORMAL
- en: 'The API can be started by simply executing the script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'To make a search request, we make a POST to the `/joblistings/search` endpoint,
    passing data in the form of `"skills=<skills separated with a space>"`.  The following
    performs a search for jobs with C# and SQL:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: And we get the results that we saw in the previous recipe.  We've now made our
    search capabilities accessible over the internet with REST!
  prefs: []
  type: TYPE_NORMAL
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This works by adding another Flask-RESTful class implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This class implements a post method as a resource mapped to `/joblistings/search`. 
    The reason for the POST operation is that we are passing a string consisting of
    multiple words.  While this could be URL-encoded in a GET operation, a POST allows
    us to pass this in as a keyed value.  And while we only have the one key, skills,
    future expansion to other keys to support other search parameters can be simply
    added.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The decision to perform the search from within the API implementation is one
    that should be considered as a system evolves.  It is my opinion, and just mine
    (but I think others would agree), that like how the API calls a microservice for
    the actual scraping, it should also call a microservice that handles the search
    (and that microservice would then interface with Elasticsearch).  This would also
    be the case for storing the document returned from the scraping microservice,
    as well as accessing Elasticsearch to check for a cached document.  But for our
    purposes here, we'll try and keep it simple.
  prefs: []
  type: TYPE_NORMAL
- en: Storing configuration in the environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe points out a change made in the code of the API in the previous
    recipe to support one of the *factors* of a **12-Factor** application.  A 12-Factor
    app is defined as an app that is designed to be run as a software as a service. 
    We have been moving our scraper in this direction for a while now, breaking it
    into components that can be run independently, as scripts, or in containers, and
    as we will see soon, in the cloud.  You can learn all about 12-Factor apps at [https://12factor.net/](https://12factor.net/).
  prefs: []
  type: TYPE_NORMAL
- en: Factor-3 states that we should pass in configuration to our application through
    environment variables.  While we definitely don't want to hardcode things, such
    as URLs, to external services, it also isn't best practice to use configuration
    files.  When deploying to various environments, such as containers or the cloud,
    a config file will often get fixed in an image and not be able to be changed on-demand
    as the application is dynamically deployed to different environments.
  prefs: []
  type: TYPE_NORMAL
- en: The best way to fix this is to always look in environment variables for configuration
    settings that can change based on how the application is run.  Most tools for
    running 12-Factor apps allow the setting of environment variables based on how
    and where the environment decides the app should be run.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In our job listings implementation, we used the following code to determine
    the host for Elasticsearch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: It's a straightforward and simple thing to do, but it's very important for making
    our app incredibly portable to different environments.  This defaults to using
    localhost, but lets us define a different host with the `ES_HOST` environment
    variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'The implementation of the skills search also makes a similar change to allow
    us to change a default of localhost for our scraping microservice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We will see Factor-3 in use in the upcoming recipes, as we move this code to
    AWS's Elastic Container Service.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an AWS IAM user and a key pair for ECS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will create an Identity and Access Management (IAM) user
    account to allow us to access the AWS Elastic Container Service (ECS).  We need
    this as we are going to package our scraper and API up in Docker containers (we've
    done this already), but now we are going to move these containers into and run
    them from AWS ECS, making our scraper a true cloud service.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This assumes that you have already created an AWS account, which we used earlier
    in the book when we looked at SQS and S3.  You don't need a different account,
    but we need to create a non-root user that has permissions to use ECS.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Instructions for creating an IAM user with ECS permissions and a key pair can
    be found at [https://docs.aws.amazon.com/AmazonECS/latest/developerguide/get-set-up-for-amazon-ecs.html](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/get-set-up-for-amazon-ecs.html).
  prefs: []
  type: TYPE_NORMAL
- en: There are a lot of instructions on this page, such as setting up a VPC and security
    groups.  Just focus now on creating the user, assigning permissions, and creating
    the key pair.
  prefs: []
  type: TYPE_NORMAL
- en: 'One thing I want to highlight are the permissions for the IAM account you create. 
    There are detailed instructions on doing this at [https://docs.aws.amazon.com/AmazonECS/latest/developerguide/instance_IAM_role.html](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/instance_IAM_role.html). 
    I''ve seen this not done properly.  Just make sure that when you examine the permissions
    for the user you just created that the following permissions are assigned:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/f67ea806-ca40-49f7-a2f1-fd561116c372.png)AWS IAM credentials'
  prefs: []
  type: TYPE_NORMAL
- en: I attached these directly to the account I use for ECS instead of through the
    group.  If this isn't assigned, you will get cryptic authentication errors when
    pushing containers to ECR.
  prefs: []
  type: TYPE_NORMAL
- en: 'One more thing: we will need the access key ID and the associated secret key. 
    This will be presented to you during the creation of the user.  If you didn''t
    record it, you can create another one in the security credentials tab of the user''s
    account page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/9fa87343-5f4c-439a-abc8-f79339dfcd9b.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that you can't get the secret for an already existing access key ID.  You
    will have to make another.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Docker to authenticate with ECR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will configure docker to be able to push our containers to
    the Elastic Container Repository (ECR).
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A key element of Docker is docker container repositories.  We have previously
    used Docker Hub to pull containers.  But we can also push our containers to Docker
    Hub, or any Docker-compatible container repository, such as ECR. But this is not
    without its troubles. The docker CLI does not naturally know how to authenticate
    with ECR, so we have to jump through a few hoops to get it to work.
  prefs: []
  type: TYPE_NORMAL
- en: 'Make sure that the AWS command line tools are installed.  These are required
    to get Docker authenticated to work with ECR.  Good instructions are found at [https://docs.aws.amazon.com/cli/latest/userguide/installing.html](https://docs.aws.amazon.com/cli/latest/userguide/installing.html).  Once
    the install is verified, you will need to configure the CLI to use the account
    created in the previous recipe.  This can be done using the `aws configure` command,
    which will prompt you for four items:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Swap the keys to be the ones you retrieved earlier, and set your default region
    and data type.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We proceed with the recipe as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the following command. This returns a command to authenticate Docker
    with ECR:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This output is a command that you need to execute to get your docker CLI to
    authenticate with ECR!  This secret is only valid for a few hours (twelve I believe). 
    You can copy all this from the where it starts with `docker login` through the
    end of the URL at the end of the secret.
  prefs: []
  type: TYPE_NORMAL
- en: 'On a Mac (and Linux), I generally shorten this to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Much easier. At this point, we can use the docker command to push containers
    to ECR.
  prefs: []
  type: TYPE_NORMAL
- en: This is an area where I've seen a couple of problems.  I've found the URL at
    the end of the secret can still be the root user and not the user you created
    for ECR (this login HAS to be for that user).  If that is the case, later commands
    will get weird authentication issues.  The fix is to delete all the AWS CLI configuration
    files and reconfigure.  This fix doesn't always work. Sometimes, I've had to use
    a fresh system/VM, go through the AWS CLI install/ config, and then generate this
    secret to get it to work.
  prefs: []
  type: TYPE_NORMAL
- en: Pushing containers into ECR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe we will rebuild our API and microservice containers and push
    them to ECR.  We will also push a RabbitMQ container to ECR.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bear with this, as this can get tricky. In addition to our container images,
    we also need to push our RabbitMQ container to ECR. ECS doesn't talk to Docker
    Hub and and can't pull that image. it would be immensely convenient, but at the
    same time it's probably also a security issue.
  prefs: []
  type: TYPE_NORMAL
- en: Pushing these containers to ECR from a home internet connection can take a long
    time. I create a Linux image in EC2 in the same region as my ECR, pulled down
    the code from github, build the containers on that EC2 system, and then push to
    ECR.  The push takes a matter of minutes, if not seconds.
  prefs: []
  type: TYPE_NORMAL
- en: First, let's rebuild our API and microservice containers on our local system. 
    I've included the Python files, two docker files, and a configuration file for
    the microservice  in the `11/05` recipe folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with the build of the API container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: This docker file is similar to the previous API Docker file with the modification
    to copy files from the `11/05` folder.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Then build the container for the scraper microservice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This Dockerfile is slightly different from the one for the microservice.  Its
    contents are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Now we are ready to work with configuring ECR to store our containers for use
    by ECS.
  prefs: []
  type: TYPE_NORMAL
- en: We now run the microservice using python and not with the "nameko run" command. 
    This is due to an issue with sequencing the launch of containers in ECS.  The
    "nameko run" command does not perform well if the RabbitMQ server is not already
    running, which is not guaranteed in ECS.  So, we start this with python.  Because
    of this, the implementation has a startup that essentially copies the code for
    "nameko run" and wraps it with a while loop and exception handlers as it retries
    connections until the container is stopped.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We proceed with the recipe as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'When signed in to the account that we created for ECS, we get access to the
    Elastic Container Repository.  This service can hold our containers for use by
    ECS.  There are a number of AWS CLI commands that you can use to work with ECR. 
    Let''s start with the following that lists the existing repositories:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Right now we don''t have any repositories, so let''s create some.  We will
    create three repositories, one for each of the different containers: scraper-rest-api,
    scraper-microservice, and one for a RabbitMQ container, which we will call `rabbitmq`. 
    Each repository maps to one container by its name, but can have multiple tags
    (up to 1,000 different versions/tags for each). Let''s create the three repositories:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Note the data returned.  We will need the repository URL for each in the following
    step(s).
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to *tag* our local container images so their docker knows that when
    we *push* them, they should go to a specific repository in our ECR.  At this point,
    you should have the following images in docker:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Tag using the `<image-id> <ECR-repository-uri>` docker tag.  Let''s tag all
    three (we don''t need to do the python image):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The list of docker images now shows the tagged images along with the originals:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we finally push the images into ECR:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now check that the images made it to the repository  The following shows this
    for `scraper-rest-api`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: With our containers now stored in ECR, we can go on and create a cluster to
    run our containers.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an ECS cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Elastic Container Service (ECS) is an AWS service that runs your Docker containers
    in the cloud.  There is a lot of power (and detail) in using ECS.  We will look
    at a simple deployment that runs our containers on a single EC2 virtual machine. 
    Our goal is to get our scraper to the cloud.  Extensive detail on using ECS to
    scale out the scraper is for another time (and book).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We start by creating an ECR cluster using the AWS CLI.  The we will create one
    EC2 virtual machine in the cluster to run our containers.
  prefs: []
  type: TYPE_NORMAL
- en: I've included a shell file, in the `11/06` folder, names `create-cluster-complete.sh`,
    which runs through all of these commands in one run.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are number of steps to getting this configured but they are all fairly
    simple.  Let''s walk through them:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following creates an ECR cluster named scraper-cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Wow, that was easy!  Well, there's a bit of detail to take care of yet.  At
    this point, we don't have any EC2 instances to run the containers.  We also need
    to set up key pairs, security groups, IAM policies, phew!  It seems like a lot,
    but we'll get through it quickly and easily.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a key pair. Every EC2 instance needs one to launch, and it is needed
    to remote into the instance (if you want to).  The following creates a key pair,
    puts it in a local file, and then confirms with AWS that it was created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we create security groups.  A security group allows us to open ports to
    the cluster instance from the Internet, and hence allows us to access the apps
    running in our containers.  We will create a security group with ports 22 (ssh)
    and 80 (http), and the two ports for RabbitMQ (5672 and 15672) opened.  We need
    80 open to talk to the REST API (we''ll map 80 to the 8080 containers in the next
    recipe).  We don''t need 15672 and 5672 open, but they help with debugging the
    process by allowing you to connect into RabbitMQ from outside AWS. The following
    four commands create the security group and the rules in that group:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: You can confirm the contents of the security group using the aws ec2 describe-security-groups
    --group-names ScraperClusterSG command. This will output a JSON representation
    of the group.
  prefs: []
  type: TYPE_NORMAL
- en: 'To launch an EC2 instance into an ECS cluster, it needs to have an IAM policy
    put in place to allow it to connect.  It also needs to have various abilities
    with ECR, such as pulling containers.  These are defined in the two files included
    in the recipe directory, `ecsPolicy.json` and `rolePolicy.json`. The following
    commands will register these policies with IAM (output is omitted):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: We need to do one more thing before we launch the instance.  We need to have
    a file to pass user data to the instance that tells the instance which cluster
    to connect to. If we don't do this, it will connect to a cluster named `default`
    instead of `scraper-cluster`.  This file is `userData.txt` in the recipe directory. 
    There is no real action here as I provided the file.
  prefs: []
  type: TYPE_NORMAL
- en: 'New we launch an instance in our cluster.  We need to use an ECS-optimized
    AMI or create an AMI with the ECS container agent.  We will use a prebuilt AMI
    with this agent.  The following kicks off the instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: This will spit out a bit of JSON describing your instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'After a few minutes, you can check that this instance is running in the container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Awesome!  Now we need to define tasks to run on the container instances.
  prefs: []
  type: TYPE_NORMAL
- en: This is an m4.large instance.  It's a bit larger than the t2.micro that fits
    within the free-tier.  So, make sure you don't leave this running if you want
    to keep things cheap.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a task to run our containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will create an ECS task.  A task tells the ECR cluster manager
    which containers to run. A task is a description of which containers in ECR  to
    run and the parameters required for each.  The task description will feel a lot
    like that which we have done with Docker Compose.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The task definition can be built with the GUI or started by submitting a task
    definition JSON file.  We will use the latter technique and examine the structure
    of the file, `td.json`, which describes how to run our containers together.  This
    file is in the `11/07` recipe folder.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following command registers the task with ECS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The output is the definition as filled out by ECS and acknowledges receipt of
    the task definition.
  prefs: []
  type: TYPE_NORMAL
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The task definition consists of two primary sections.  The first gives some
    general information about the tasks as a whole, such as how much memory and CPU
    is allowed for the containers as a whole.  It then consists of a section that
    defines the three containers we will run.
  prefs: []
  type: TYPE_NORMAL
- en: 'The file begins with a few lines that define the overall settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: The actual name of the task is defined by the `"family"` property.  We state
    the our containers require EC2 (tasks can be run without EC2 - ours needs it).
    Then we state that we want to constrain the entire task to the specified amount
    of CPU and memory, and we are not attaching any volumes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s look at the section where the containers are defined.  It starts
    with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s examine each container definition.  The following is the definition
    for the `rabbitmq` container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The first line defines the name of the container, and this name also participates
    in DNS resolution of the name of this container by the API and scraper containers. 
    The image tag defines the ECR repository URI to pull for the container.
  prefs: []
  type: TYPE_NORMAL
- en: Make sure to change the image URL for this and the other two containers to that
    of your repositories.
  prefs: []
  type: TYPE_NORMAL
- en: Next are a definition of maximum CPU (0 is unlimited) and memory to be allowed
    for this container. The port mapping defines the mappings between the container
    host (the EC2 instance we created in the cluster) and the container.  We map the
    two RabbitMQ ports.
  prefs: []
  type: TYPE_NORMAL
- en: The essential tag states that this container must remain running.  If it fails,
    the entire task will be stopped.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next container defined is the scraper microservice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: This differs in that it has an environment variable and links defined.  The
    environment variable is the URL for the `rabbitmq` container.  ECS will ensure
    that the environment variable is set to this value within this container (implementing
    Factor-3).  While this is the same URL as when we ran this locally on docker compose,
    it could be a different URL if the `rabbitmq` container was named differently
    or on another cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The links settings needs a little explanation.  Links are a deprecated feature
    of Docker but still used in ECS.  They are required in ECS to have the container
    resolve DNS names for other containers in the same cluster network.  This tells
    ECS that when this container tries to resolve the `rabbitmq` hostname (as defined
    in the environment variable), it should return the IP address assigned to that
    container.
  prefs: []
  type: TYPE_NORMAL
- en: 'The remainder of the file defines the API container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: In this definition, we define the port mapping to allow HTTP into the container,
    and set the environment variables for the API to use to talk to Elastic Cloud
    and the `rabbitmq` server (which passed the requests to the `scraper-microservice`
    container).  This also defines a link to `rabbitmq` as that needs to also be resolved.
  prefs: []
  type: TYPE_NORMAL
- en: Starting and accessing the containers in AWS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will start our scraper as a service by telling ECS to run
    our task definition.  Then we will check hat it is running by issuing a curl to
    get contents of a job listing.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We need to do one quick thing before running the task.  Tasks in ECS go through
    revisions.  Each time you register a task definition with the same name ("family"),
    ECS defines a new revision number.  You can run any of the revisions.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run the most recent one, we need to list the task definitions for that family
    and find the most recent revision number.  The following lists all of the task
    definitions in the cluster.  At this point we only have one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Notice my revision number is at 17.  While this is my only currently registered
    version of this task, I have registered (and unregistered) 16 previous revisions.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We proceed with the recipe as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can run our task. We do this with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: The output gives us a current status of the task.  The very first time this
    is run, it will take a little time to get going, as the containers are being copied
    over to the EC2 instance.  The main culprit of that delayu is the `scraper-microservice`
    container with all of the NLTK data.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can check the status of the task with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: You will need to change the task GUID to match guid in the `"taskArn"` property
    of the output from running the task.  When all the containers are running, we
    are ready to test the API.
  prefs: []
  type: TYPE_NORMAL
- en: 'To call our service, we will need to find the IP address or DNS name for our
    cluster instance. you can get this from the output when we created the cluster,
    through the portal, or with the following commands.  First, describe the cluster
    instances:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'With the GUID for our EC2 instance, we can query its info and pull the EC2
    instance ID with the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'With that instance ID, we can get the DNS name:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'And with that DNS name, we can make a curl to get a job listing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: And we get the following familiar result!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Our scraper is now running in the cloud!
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our scraper is running on an `m4.large` instance, so we would like to shut it
    down to we don't exceed our free-tier usage.  This is a two-step process. First,
    the EC2 instances in the cluster need to be terminated, and the cluster deleted. 
    Note that deleting the cluster DOES NOT terminate the EC2 instances.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can terminate the EC2 instance using the following (and the instance ID
    we just got from interrogating the cluster):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'And the cluster can be deleted with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
