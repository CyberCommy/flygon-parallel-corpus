- en: Creating a Simple Data API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a REST API with Flask-RESTful
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating the REST API with scraping code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding an API to find the skills for a job listing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing data in Elasticsearch as the result of a scraping request
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checking Elasticsearch for a listing before scraping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have now reached an exciting inflection point in our learning about scraping. 
    From this point on, we will learn about making scrapers as a service using several
    APIs, microservice, and container tools, all of which will allow the running of
    the scraper either locally or in the cloud, and to give access to the scraper
    through standardized REST APIs.60;
  prefs: []
  type: TYPE_NORMAL
- en: We will start this new journey in this chapter with the creation of a simple
    REST API using Flask-RESTful which we will eventually use to make requests to
    the service to scrape pages on demand.  We will connect this API to a scraper
    function implemented in a Python module that reuses the concepts for scraping
    StackOverflow jobs, as discussed in [Chapter  7](90200c61-a13c-4384-925a-e9adbac1eaf0.xhtml),
    *Text Wrangling and Analysis*.
  prefs: []
  type: TYPE_NORMAL
- en: The final few recipes will focus on using Elasticsearch as a cache for these
    results, storing documents we retrieve from the scraper, and then looking for
    them first within the cache.  We will examine more elaborate uses of ElasticCache,
    such as performing searches for jobs with a given set of skills, later in [Chapter
    11](05a687e5-e5d7-4ffb-aa28-6078fbab5fea.xhtml), *Making the Scraper as a Service
    Real*.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a REST API with Flask-RESTful
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We start with the creation of a simple REST API using Flask-RESTful.  This initial
    API will consist of a single method that lets the caller pass an integer value
    and which returns a JSON blob.  In this recipe, the parameters and their values,
    as well as the return value, are not important at this time as we want to first
    simply get an API up and running using Flask-RESTful.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Flask is a web microframework that makes creating simple web application functionality
    incredibly easy.  Flask-RESTful is an extension to Flask which does the same for
    making REST APIs just as simple.  You can get Flask and read more about it at
    `flask.pocoo.org`. Flask-RESTful can be read about at `https://flask-restful.readthedocs.io/en/latest/`. Flask
    can be installed into your Python environment using `pip install flask`. and Flask-RESTful
    can also be installed with `pip install flask-restful`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The remainder of the recipes in the book will be in a subfolder of the chapter''s
    directory.  This is because most of these recipes either require multiple files
    to operate, or use the same filename (ie: `apy.py`).'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The initial API is implemented in `09/01/api.py`.  The API itself and the logic
    of the API is implemented in this single file: `api.py`.  The API can be run in
    two manners, the first of which is by simply executing the file as a Python script.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The API can then be launched with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'When run, you will initially see output similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This program exposes a REST API on `127.0.0.1:5000`, and we can make requests
    for job listings using a `GET` request to the path `/joblisting/<joblistingid>`. 
    We can try this with curl:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of this command will be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: And just like that, we have a REST API up and running.  Now let's see how it
    is implemented.
  prefs: []
  type: TYPE_NORMAL
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There really isn't a lot of code, which is the beauty of Flask-RESTful.  The
    code begins with importing of `flask` and `flask_restful`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'These are followed with code to set up the initial configuration of Flask-RESTful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Next comes a definition of a class which represents the implementation of our
    API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: What we will have Flask-RESTful do is map HTTP requests to methods in this class. 
    Specifically, by convention `GET` requests will be mapped to member functions
    named `get`.  There will be a mapping of the values passed as part of the URL
    to the `jobListingId` parameter of the function.  This function then returns a
    Python dictionary, which Flask-RESTful converts to JSON for us.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next line of code tells Flask-RESTful how to map portions of the URL to
    our class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This defines that URLs with paths beginning with `/joblisting` will be mapped
    to our `JobListing` class, and that the next portion of the URL represents a string
    to be passed to the `jobListingId` parameter of the `get` method.  The GET HTTP
    verb is implied as no other verb has been defined in this mapping.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we have code that specifies that when the file is run as a script that
    we simply execute `app.run()` (passing in this case a parameter to give us debug
    output).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Flask-RESTful then finds our class and sets of the mappings, starts listening
    on `127.0.0.1:5000` (the default), and forwarding requests to our class and method.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The default for Flask-RESTful is to run on port `5000`.  This can be changed
    using alternate forms of `app.run()`.  We will be fine with leaving it at 5000
    for our recipes. Ultimately, you would run this service in something like a container
    and front it with a reverse proxy such as NGINX and perform a public port mapping
    to the internal service port.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating the REST API with scraping code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will integrate code that we wrote for scraping and getting
    a clean job listing from StackOverflow with our API.  This will result in a reusable
    API that can be used to perform on-demand scrapes without the client needing any
    knowledge of the scraping process.  Essentially, we will have created a *scraper
    as a service*, a concept we will spend much time with in the remaining recipes
    of the book.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first part of this process is to create a module out of our preexisting
    code that was written in [Chapter 7](90200c61-a13c-4384-925a-e9adbac1eaf0.xhtml), *Text
    Wrangling and Analysis* so that we can reuse it.  We will reuse this code in several
    recipes throughout the remainder of the book.  Let's briefly examine the structure
    and contents of this module before going and integrating it with the API.
  prefs: []
  type: TYPE_NORMAL
- en: The code for the module is in the `sojobs` (for StackOverflow Jobs) module in
    the project's modules folder.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/c192716d-ac40-476d-936a-36091d78b2fb.png)The sojobs folder'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the most part, these files are copied from those used in [Chapter 7](90200c61-a13c-4384-925a-e9adbac1eaf0.xhtml), *Text
    Wrangling and Analysis*.  The primary file for reuse is `scraping.py`, which contains
    several functions that facilitate scraping.  The function that we will use in
    this recipe is `get_job_listing_info`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Heading back to the code in [Chapter 7](90200c61-a13c-4384-925a-e9adbac1eaf0.xhtml), *Text
    Wrangling and Analysis*, you can see that this code is reused code that we created
    in those recipes. A difference is that instead of reading a single local `.html`
    file, this function is passed the identifier for a job listing, then constructs
    the URL for that job listing, reads the content with requests, performs several
    analyses, and then returns the results.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the function returns a Python dictionary that consists of the requested
    job ID, the original HTML, the text of the listing, and the list of cleaned words.
    This API is returning to the caller an aggregate of these results, with the `ID`
    so it is easy to know the requested job, as well as all of the other results that
    we did to perform various clean ups.  Hence, we have created a value-added service
    for job listings instead of just getting the raw HTML.
  prefs: []
  type: TYPE_NORMAL
- en: Make sure that you either have your PYTHONPATH environment variable pointing
    to the modules directory, or that you have set up your Python IDE to find modules
    in this directory.  Otherwise, you will get errors that this module cannot be
    found.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We proceed with the recipe as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code of the API for this recipe is in `09/02/api.py`.  This extends the
    code in the previous recipe to make a call to this function in the `sojobs` module. 
    The code for the service is the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Note that the main difference is the import of the function from the module,
    and the call to the function and return of the data from the result.
  prefs: []
  type: TYPE_NORMAL
- en: The service is run by executing the script with Python `api.py`.  We can then
    test the API using `curl`.  The following requests the SpaceX job listing we have
    previously examined.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in quite a bit of output.  The following is some of the beginning
    of the response:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Adding an API to find the skills for a job listing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we add an additional operation to our API which will allow us
    to request the skills associated with a job listing.  This demonstrates a means
    of being able to retrieve only a subset of the data instead of the entire content
    of the listing.  While we will only do this for the skills, the concept can be
    easily extended to any other subsets of the data, such as the location of the
    job, title, or almost any other content that makes sense for the user of your
    API.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first thing that we will do is add a scraping function to the `sojobs`
    module.  This function will be named `get_job_listing_skills`.  The following
    is the code for this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This function retrieves the job listing, extracts the JSON provided by StackOverflow,
    and then only returns the `skills` property of the JSON.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's see how to add a method to the REST API to call it.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We proceed with the recipe as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code of the API for this recipe is in `09/03/api.py`. This script adds
    an additional class, `JobListingSkills`, with the following implementation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This implementation is similar to that of the previous recipe, except that it
    calls the new function for getting skills.
  prefs: []
  type: TYPE_NORMAL
- en: We still need to add a statement to inform Flask-RESTful how to map URLs to
    this classes' `get` method.  Since we are actually looking at retrieving a sub-property
    of the larger job listing, we will extend our URL scheme to include an additional
    segment representing the sub-property of the overall job listing resource.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can retrieve just the skills using the following curl:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Which gives us the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Storing data in Elasticsearch as the result of a scraping request
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we extend our API to save the data we received from the scraper
    into Elasticsearch.  We will use this later (in the next recipe) to be able to
    optimize requests by using the content in Elasticsearch as a cache so that we
    do not repeat the scraping process for jobs listings already scraped.  Therefore,
    we can play nice with StackOverflows servers.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Make sure you have Elasticsearch running locally, as the code will access Elasticsearch
    at `localhost:9200`.  There a good quick-start available at  [https://www.elastic.co/guide/en/elasticsearch/reference/current/_installation.html](https://www.elastic.co/guide/en/elasticsearch/reference/current/_installation.html),
    or you can check out the docker Elasticsearch recipe in [Chapter 10](ae07f922-9f75-4b94-9902-e5a5ebeec167.xhtml), *Creating
    Scraper Microservices with Docker* if you'd like to run it in Docker.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once installed, you can check proper installation with the following `curl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'If installed properly, you will get output similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: You will also need to have elasticsearch-py installed.  This is available at [https://www.elastic.co/guide/en/elasticsearch/client/python-api/current/index.html](https://www.elastic.co/guide/en/elasticsearch/client/python-api/current/index.html),
    but can be quickly installed using `pip install elasticsearch`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will make a few small changes to our API code.  The code from the previous
    recipe has been copied into `09/04/api.py`, with the few modifications made.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we add an import for elasticsearch-py:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we make a quick modification to the `get` method of the `JobListing` class
    (I''ve done the same in JobListingSkills, but it''s omitted here for brevity):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The two new lines create an `Elasticsearch` object, and then insert the resulting
    document into ElasticSearch.  Before the first time of calling the API, we can
    see that there is no content, nor a `''joblistings''` index, by using the following
    curl:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Given we just installed Elasticsearch, this will result in the following error.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Now start up the API by using `python api.py`.  Then issue the `curl` to get
    the job listing (`curl localhost:5000/joblisting/122517`).  This will result in
    output similar to the previous recipes. The difference now is that this document
    will be stored in Elasticsearch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now reissue the previous curl for the index:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'And now you will get the following result (only the first few lines shown):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: There has been an index created, named `joblistings`, and this result demonstrates
    the index structure that Elasticsearch has identified by examining the document.
  prefs: []
  type: TYPE_NORMAL
- en: While Elasticsearch is schema-less, it does examine the documents submitted
    and build indexes based upon what it finds.
  prefs: []
  type: TYPE_NORMAL
- en: 'The specific document that we just stored can be retrieved by using the following
    curl:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Which will give us the following result (again, just the beginning of the content
    shown):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: And just like that, with two lines of code, we have the document stored in our
    Elasticsearch database.  Now let's briefly examine how this worked.
  prefs: []
  type: TYPE_NORMAL
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The storing of the document was performed using the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Let's examine what each of these parameters does relative to storing this document.
  prefs: []
  type: TYPE_NORMAL
- en: The `index` parameter specifies which Elasticsearch index we want to store the
    document within. That is named `joblistings`.  This also becomes the first portion
    of the URL used to retrieve the documents.
  prefs: []
  type: TYPE_NORMAL
- en: Each Elasticsearch index can also have multiple document 'types', which are
    logical collections of documents that can represent different types of documents
    within the index.  We used `'job-listing'`, and that value also forms the second
    part of our URL for retrieving a specific document.
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch does not require that an indentifier be specified for each document,
    but if we provide one we can look up specific documents without having to do a
    search.  We will use the job listing ID for the document ID.
  prefs: []
  type: TYPE_NORMAL
- en: The final parameter, body, specifies the actual content of the document.  This
    code simply passed the result received from the scraper.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's look briefly at a few more facets of what Elasticsearch has done for us
    by looking at the results of this document retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we can see the index, document type, and ID in the first few lines of
    the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: This makes a retrieval of a document very efficient when using these three values
    as we did in this query.
  prefs: []
  type: TYPE_NORMAL
- en: There is also a version stored for each document, which in this case is 1.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: If we do this same query with the code remaining as it is, then this document
    will be stored again with the same index, doc type, and ID, and hence will have
    the version incremented.  Trust me, do the curl on the API again, and you will
    see this increment to 2.
  prefs: []
  type: TYPE_NORMAL
- en: Now examine the content of the first few properties of the `"JSON"` property.
    We assigned this property of the result from the API to be the JSON of the job
    description returned by StackOverflow embedded within the HTML.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: This is some of the beauty of a web site like StackOverflow giving us structured
    data, and with using a tools like Elasticsearch as we get nicely structured data. 
    We can, and will, leverage this for great effect with very small amounts of code. 
    We can easily perform queries using Elasticsearch to identify job listing based
    upon specific skills (we'll do this in an upcoming recipe), industry, job benefits,
    and other attributes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The result of our API also returned a property ''`CleanedWords`'', which was
    the result of several of our NLP processes extracting high-value words and terms. 
    The following is an excerpt of the values that ended up in Elasticsearch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: And again, we will be able to use these to perform rich queries that can help
    us find specific matches based upon these specific words.
  prefs: []
  type: TYPE_NORMAL
- en: Checking Elasticsearch for a listing before scraping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now lets leverage Elasticsearch as a cache by checking to see if we already
    have stored a job listing and hence do not need to hit StackOverflow again. We
    extend the API for performing a scrape of a job listing to first search Elasticsearch,
    and if the result is found there we return that data.  Hence, we optimize the
    process by making Elasticsearch a job listings cache.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We proceed with the recipe as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for this recipe is within `09/05/api.py`.  The `JobListing` class
    now has the following implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Before calling the scraper code, the API checks to see if the document already
    exists in Elasticsearch.  This is performed by the appropriately named '`exists`'
    method, which we pass the index, doc type and ID we are trying to get.
  prefs: []
  type: TYPE_NORMAL
- en: If that returns true, then the document is retrieved using the `get` method
    of the Elasticsearch object,  which is also given the same parameters.  This returns
    a Python dictionary representing the Elasticsearch document, not the actual data
    that we stored.  That actual data/document is referenced by accessing the `'_source'`
    key of the dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `JobListingSkills` API implementation follows a slightly different pattern.
    The following is its code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: This implementation only uses ElasticSearch to the extent of checking if the
    document already is in ElasticSearch.  It does not try to save a newly retrieved
    document from the scraper.  That is because the result of the `get_job_listing`
    scraper is only a list of the skills and not the entire document.  So, this implementation
    can use the cache, but it adds no new data.  This is one of the design decisions
    of having different a method for scraping which returns only a subset of the actual
    document that is scraped.
  prefs: []
  type: TYPE_NORMAL
- en: A potential solution to this is to have this API method call `get_job_listing_info`
    instead, then save the document, and finally only return the specific subset (in
    this case the skills).  Again, this is ultimately a design consideration around
    what types of methods your users of the sojobs module need.  For purposes of these
    initial recipes, it was considered better to have two different functions at that
    level to return the different sets of data.
  prefs: []
  type: TYPE_NORMAL
