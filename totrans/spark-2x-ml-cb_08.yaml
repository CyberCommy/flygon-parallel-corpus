- en: Unsupervised Clustering with Apache Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Building a KMeans classification system in Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bisecting KMeans, the new kid on the block in Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Gaussian Mixture and Expectation Maximization (EM) in Spark 2.0 to classify
    data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying the vertices of a graph using Power Iteration Clustering (PIC) in
    Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Latent Dirichlet Allocation (LDA) to classify documents and text into
    topics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming KMeans to classify data in near real time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unsupervised machine learning is a type of learning technique in which we try
    to draw inferences either directly or indirectly (through latent factors) from
    a set of unlabeled observations. In simple terms, we are trying to find the hidden
    knowledge or structures in a set of data without initially labeling the training
    data.
  prefs: []
  type: TYPE_NORMAL
- en: While most machine learning library implementation break down when applied to
    large datasets (iterative, multi-pass, a lot of intermediate writes), the Apache
    Spark Machine Library succeeds by providing machine library algorithms designed
    for parallelism and extremely large datasets using memory for intermediate writes
    out of the box.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the most abstract level, we can think of unsupervised learning as:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Clustering systems**: Classify the inputs into categories either using hard
    (only belonging to a single cluster) or soft (probabilistic membership and overlaps)
    categorization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dimensionality reduction systems**: Find hidden factors using a condensed
    representation of the original data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following figure shows the landscape of machine learning techniques. In
    the previous chapters, we focused on supervised machine learning techniques. In
    this chapter, we concentrate on unsupervised machine learning techniques ranging
    from clustering to latent factor models using Spark''s ML/MLIB library API:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00156.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The clusters are often modeled using intra-cluster similarity measurement, such
    as Euclidian or probabilistic techniques. Spark provides a complete and high-performing
    set of algorithms which lend themselves to parallel implementation at scale. They
    not only provide APIs, but also provide full source code which is very helpful
    for understanding bottlenecks and resolving them (forking to GPU) to fit your
    needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The applications of machine learning are vast and as limitless as you can imagine.
    Some of the most widely known examples and use cases are:'
  prefs: []
  type: TYPE_NORMAL
- en: Fraud detection (finance, law enforcement)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network security (intrusion detection, traffic analysis)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pattern recognition (marketing, intelligence community, banking)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recommendation systems (retail, entertainment)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Affinity marketing (e-commerce, recommenders, deep personalization)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Medical informatics (disease detection, patient care, asset management)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image processing (object/sub-object detection, radiology)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A word of caution on ML versus MLIB usage and future direction in Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: While the MLIB is and will remain viable for the time being, there is a gradual
    movement towards Spark's ML library for future development rather than MLIB in
    Spark. The `org.apache.spark.ml.clustering` is a high-level machine learning package
    and the API is more focused on the DataFrame. The `org.apache.spark.mllib.clustering`
    is a lower-level machine learning package and the API is directly on RDD. While
    both packages will get the benefit of Spark's high performance and scalability,
    the main difference is the DataFrame. The `org.apache.spark.ml` will be the preferred
    method going forward.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we encourage the developer to look at why the introduction of KMeans
    classifying system exists in both ML and MLLIB: `org.apache.spark.ml.clustering`
    and `org.apache.spark.mllib.clustering`
  prefs: []
  type: TYPE_NORMAL
- en: Building a KMeans classifying system in Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will load a set of features (for example, x, y, z coordinates)
    using a LIBSVM file and then proceed to use `KMeans()` to instantiate an object.
    We will then set the number of desired clusters to three and then use `kmeans.fit()`
    to action the algorithm. Finally, we will print the centers for the three clusters
    that we found.
  prefs: []
  type: TYPE_NORMAL
- en: It is really important to note that Spark *does not* implement KMeans++, contrary
    to popular literature, instead it implements KMeans || (pronounced as KMeans Parallel).
    See the following recipe and the sections following the code for a complete explanation
    of the algorithm as it is implemented in Spark.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for Spark context to get access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Spark''s Session object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We create a training dataset from a file in the `libsvm` format and display
    the file on the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console, you will see:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00157.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'The following formula visualizes the data via contour maps that depict each
    feature vector (each row) versus the three unique features in both a 3D and flat
    contour map:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00158.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We then create a KMeans object and set some key parameters to the KMeans model
    and set parameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this case, we set the `K` value to `3` and set the *feature* column as column
    "features", which was defined in the previous step. This step is subjective and
    the optimal value would vary based on specific datasets. We recommend that you
    experiment with values from 2 to 50 and examine the cluster centers for a final
    value.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also set the maximum iteration count to `10`. Most of the values have a
    default setting as the comments as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We then train the dataset. The `fit()` function will then run the algorithm
    and perform the calculations. It is based on the dataset created in the previous
    steps. These steps are common among Spark''s ML and do not usually vary from algorithm
    to algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We also display the model''s prediction on the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00159.gif)'
  prefs: []
  type: TYPE_IMG
- en: We then calculate the cost, using the included `computeCost(x)` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The KMeans Cost is calculated **Within Set Sum of Squared Errors** (**WSSSE**).
    The value will be printed out in the program''s console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The console output will show the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We then print out the cluster''s center based on the calculation of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The console output will show the following information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Based on the setting of the KMeans clustering, we set the `K` value to `3`;
    the model will calculate three centers based on the training dataset that we fit
    in.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then close the program by stopping the Spark context:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We read a LIBSVM file with a set of coordinates (can be interpreted as a tuple
    of three numbers) and then created a `KMean()` object, but changed the default
    number of clusters from 2 (out of the box) to 3 for demonstration purposes. We
    used the `.fit()` to create the model and then used `model.summary.predictions.show()`
    to display which tuple belongs to which cluster. In the last step, we printed
    the cost and the center of the three clusters. Conceptually, it can be thought
    of as having a set of 3D coordinates as data and then assigning each individual
    coordinate to one of the three clusters using KMeans algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: KMeans is a form of unsupervised machine learning algorithm, with its root in
    signal processing (vector quantization) and compression (grouping similar vectors
    of items together to achieve a higher compression rate). Generally speaking, the
    KMeans algorithm attempts to group a series of observations {X[1,] X[2], ....
    , X[n]} into a series of clusters {C[1,] C[2 .....] C[n]} using a form of distance
    measure (local optimization) that is optimized in an iterative manner.
  prefs: []
  type: TYPE_NORMAL
- en: There are three main types of KMeans algorithm that are in use. In a simple
    survey, we found 12 specialized variations of the KMeans algorithm. It is important
    to note that Spark implements a version called KMeans || (KMeans Parallel) and
    *not* KMeans++ or standard KMeans as referenced in some literature or videos.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure depicts KMeans in a nutshell:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00160.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Spark documentation'
  prefs: []
  type: TYPE_NORMAL
- en: KMeans (Lloyd Algorithm)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The steps for basic KMeans implementation (Lloyd algorithm) are:'
  prefs: []
  type: TYPE_NORMAL
- en: Randomly select K datacenters from observations as the initial centroids.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Keep iterating till the convergence criteria is met:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Measure the distance from a point to each centroid
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Include each data point in a cluster which is the closest centroid
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate new cluster centroids based on a distance formula (proxy for dissimilarity)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update the algorithm with new center points
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The three generations are depicted in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00161.gif)'
  prefs: []
  type: TYPE_IMG
- en: KMeans++ (Arthur's algorithm)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The next improvement over standard KMeans is the KMeans++ proposed by David
    Arthur and Sergei Vassilvitskii in 2007\. Arthur's algorithm improves the initial
    Lloyd's KMeans by being more selective during the seeding process (the initial
    step).
  prefs: []
  type: TYPE_NORMAL
- en: KMeans++, rather than picking random centres (random centroids) as starting
    points, picks the first centroid randomly and then picks the data points one by
    one and calculates `D(x)`. Then it chooses one more data point at random and,
    using proportional probability distribution `D(x)2`, it then keeps repeating the
    last two steps until all *K* numbers are picked. After the initial seeding, we
    finally run the KMeans or a variation with the newly seeded centroid. The KMeans++
    algorithm is guaranteed to find a solution in an *Omega= O(log k)* complexity.
    Even though the initial seeding takes extra steps, the accuracy improvements are
    substantial.
  prefs: []
  type: TYPE_NORMAL
- en: KMeans|| (pronounced as KMeans Parallel)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: KMeans || is optimized to run in parallel and can result in one-two orders of
    magnitude improvement over Lloyd's original algorithm. The limitation of KMeans++
    is that it requires K-passes over the dataset, which can severely limit the performance
    and practicality of running KMeans with large or extreme datasets. Spark's KMeans||
    parallel implementation runs faster because it takes fewer passes (a lot less)
    over the data by sampling m points and oversampling in the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The core of the algorithm and the math is depicted in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00162.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In a nutshell, the highlight of the KMeans || (Parallel KMeans) is the course-grain
    sampling which repeats in *log(n)* rounds and at the end we are left with *k *
    log(n)* remaining points that are a C (constant) distance away from the optimal
    solution! This implementation is also less sensitive to outlier data points that
    can skew the clustering results in KMeans and KMeans++.
  prefs: []
  type: TYPE_NORMAL
- en: For a deeper understanding of the algorithm, the reader can access the paper
    by Bahman Bahmani at [http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf](http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is also a streaming version of KMeans implementation in Spark that allows
    you to classify the features on the fly. The streaming version of KMeans is covered
    in more detail in [Chapter 13](part0538.html#G12EK0-4d291c9fed174a6992fd24938c2f9c77), *Spark
    Streaming and Machine Learning Library*.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is also a class that helps you to generate RDD data for KMeans. We found
    this to be very useful during our application development process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This call uses Spark context to create RDDs while allowing you to specify the
    number of points, clusters, dimensions, and partitions.
  prefs: []
  type: TYPE_NORMAL
- en: 'A useful related API is: `generateKMeansRDD()`. Documentation for `generateKMeansRDD`
    can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.util.KMeansDataGenerator$](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.util.KMeansDataGenerator%24) for
    generate an RDD containing test data for KMeans.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We need two pieces of objects to be able to write, measure, and manipulate
    the parameters of the KMeans || algorithm in Spark. The details of these two pieces
    of objects can be found at the following websites:'
  prefs: []
  type: TYPE_NORMAL
- en: '`KMeans()`: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.KMeans](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.KMeans)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`KMeansModel()`: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.KMeansModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.KMeansModel)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bisecting KMeans, the new kid on the block in Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will download the glass dataset and try to identify and label
    each glass using a bisecting KMeans algorithm. The Bisecting KMeans is a hierarchical
    version of the K-Mean algorithm implemented in Spark using the `BisectingKMeans()`
    API. While this algorithm is conceptually like KMeans, it can offer considerable
    speed for some use cases where the hierarchical path is present.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset we used for this recipe is the Glass Identification Database. The
    study of the classification of types of glass was motivated by criminological
    research. Glass could be considered as evidence if it is correctly identified.
    The data can be found at NTU (Taiwan), already in LIBSVM format.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We downloaded the prepared data file in LIBSVM from: [https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/glass.scale](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/glass.scale)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The dataset contains 11 features and 214 rows.
  prefs: []
  type: TYPE_NORMAL
- en: The original dataset and data dictionary is also available at the UCI website: [http://archive.ics.uci.edu/ml/datasets/Glass+Identification](http://archive.ics.uci.edu/ml/datasets/Glass+Identification)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'ID number: 1 to 214'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RI: Refractive index'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Na: Sodium (unit measurement: weight percent in corresponding oxide, as are
    attributes 4-10)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mg: Magnesium'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Al: Aluminum'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Si: Silicon'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'K: Potassium'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ca: Calcium'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ba: Barium'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fe: Iron'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Type of glass: Will find our class attributes or clusters using `BisectingKMeans()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`building_windows_float_processed`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`building_windows_non-_float_processed`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vehicle_windows_float_processed`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vehicle_windows_non-_float_processed` (none in this database)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Containers`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Tableware`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Headlamps`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Spark''s Session object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We create a dataset from a file in the libsvm format and display the dataset
    on the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console, you will see:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00163.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We then split the dataset randomly into two parts in the ratio of 80% and 20%:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console output (total count is 214):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We then create a `BisectingKMeans` object and set some key parameters to the
    model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this case, we set the `K` value to `6` and set the `Feature` column as column
    "features", which was defined in the previous step. This step is subjective and
    the optimal value will vary based on specific datasets. We recommend you experiment
    with values from 2 to 50 and examine the cluster centers for a final value.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also set the maximum iteration count to `65`. Most of the values have a
    default setting, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We then train the dataset. The `fit()` function will then run the algorithm
    and do the calculations. It is based on the dataset created in the previous steps.
    We also print out the model parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We then calculate the cost, using the included computeCost(x) function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The console output will show the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we print out the cluster''s center based on the calculation of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The console output will show the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00164.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'We then use the trained model to make a prediction on the testing dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00165.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We then close the program by stopping the Spark context:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this session, we explored the Bisecting KMeans model, which is new in Spark
    2.0\. We utilized the glass dataset in this session and tried to assign a glass
    type using `BisectingKMeans()`, but changed k to 6 so we have sufficient clusters.
    As usual, we loaded the data into a dataset with Spark's libsvm loading mechanism.
    We split the dataset randomly into 80% and 20%, with 80% used to train the model
    and 20% used for testing the model.
  prefs: []
  type: TYPE_NORMAL
- en: We created the `BiSectingKmeans()` object and used the `fit(x)` function to
    create the model. We then used the `transform(x)` function for the testing dataset
    to explore the model prediction and printed out the result in the console output.
    We also output the cost of computing the clusters (sum of error squared) and then
    displayed the cluster centers. Finally, we printed the features with their assigned
    cluster number and stop operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Approaches to hierarchical clustering include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Divisive**: Top down approach (Apache Spark implementation)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Agglomerative**: Bottom up approach'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'More about the Bisecting KMeans can be found at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.BisectingKMeans](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.BisectingKMeans)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.BisectingKMeansModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.BisectingKMeansModel)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We use clustering to explore the data and get a feel for what the outcome looks
    like as clusters. The bisecting KMeans is an interesting case of hierarchical
    analysis versus KMeans clustering.
  prefs: []
  type: TYPE_NORMAL
- en: The best way to conceptualize it is to think of bisecting KMeans as a recursive
    hierarchical KMeans. The bisecting KMeans algorithm divides the data using similarity
    measurement techniques like KMeans, but uses a hierarchical scheme to increase
    accuracy. It is particularly prevalent in text mining where a hierarchical approach
    will minimize the intra-cluster dependencies of the corpus body among documents.
  prefs: []
  type: TYPE_NORMAL
- en: The Bisecting KMeans algorithm starts by placing all observations in a single
    cluster first, but then breaks up the cluster into n partition (K=n) using the
    KMeans method. It then proceeds to select the most similar cluster (the highest
    inner cluster score) as the parent (the root cluster) while recursively splitting
    the other clusters untill the target number of clusters is derived in a hierarchical
    manner.
  prefs: []
  type: TYPE_NORMAL
- en: The Bisecting KMeans is a powerful tool used in text analytics to reduce the
    dimensionality of feature vectors for intelligent text/subject classification.
    By using this clustering technique, we end up grouping similar words/text/document/evidence
    into similar groups. Ultimately, if you start exploring text analytics, topic
    propagation, and scoring (for example, what article would go viral?), you are
    bound to encounter this technique in the early stages of your journey.
  prefs: []
  type: TYPE_NORMAL
- en: A white paper describing the use of Bisecting KMeans for text clustering is
    available at: [http://www.ijarcsse.com/docs/papers/Volume_5/2_February2015/V5I2-0229.pdf](http://www.ijarcsse.com/docs/papers/Volume_5/2_February2015/V5I2-0229.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two approaches to implementing hierarchical clustering--Spark uses
    a recursive top-down approach in which a cluster is chosen and then splits are
    performed in the algorithm as it moves down the hierarchy:'
  prefs: []
  type: TYPE_NORMAL
- en: Details about the hierarchical clustering approach can be found at [https://en.wikipedia.org/wiki/Hierarchical_clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark 2.0 documentation for Bisecting K-Mean can be found at [http://spark.apache.org/docs/latest/ml-clustering.html#bisecting-k-means](http://spark.apache.org/docs/latest/ml-clustering.html#bisecting-k-means)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A paper describing how to use Bisecting KMeans to classify web logs can be found
    at [http://research.ijcaonline.org/volume116/number19/pxc3902799.pdf](http://research.ijcaonline.org/volume116/number19/pxc3902799.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Gaussian Mixture and Expectation Maximization (EM) in Spark to classify
    data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will explore Spark's implementation of **expectation maximization**
    (**EM**) `GaussianMixture()`*,* which calculates the maximum likelihood given
    a set of features as input. It assumes a Gaussian mixture in which each point
    can be sampled from K number of sub-distributions (cluster memberships).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for vector and matrix manipulation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Spark''s session object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Let us take a look at the dataset and examine the input file. The Simulated
    SOCR Knee Pain Centroid Location Data represents the centroid location for the
    hypothetical knee-pain locations for 1,000 subjects. The data includes the X and
    Y coordinates of the centroids.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This dataset can be used to illustrate the Gaussian Mixture and Expectation
    Maximization. The data is available at: [http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_KneePainData_041409](http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_KneePainData_041409)
  prefs: []
  type: TYPE_NORMAL
- en: 'The sample data looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**X**: The *x* coordinate of the centroid location for one subject and one
    view.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Y**: The *y* coordinate of the centroid location for one subject and one
    view.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: X, Y
  prefs: []
  type: TYPE_NORMAL
- en: 11 73
  prefs: []
  type: TYPE_NORMAL
- en: 20 88
  prefs: []
  type: TYPE_NORMAL
- en: 19 73
  prefs: []
  type: TYPE_NORMAL
- en: 15 65
  prefs: []
  type: TYPE_NORMAL
- en: 21 57
  prefs: []
  type: TYPE_NORMAL
- en: 26 101
  prefs: []
  type: TYPE_NORMAL
- en: 24 117
  prefs: []
  type: TYPE_NORMAL
- en: 35 106
  prefs: []
  type: TYPE_NORMAL
- en: 37 96
  prefs: []
  type: TYPE_NORMAL
- en: 35 147
  prefs: []
  type: TYPE_NORMAL
- en: 41 151
  prefs: []
  type: TYPE_NORMAL
- en: 42 137
  prefs: []
  type: TYPE_NORMAL
- en: 43 127
  prefs: []
  type: TYPE_NORMAL
- en: 41 206
  prefs: []
  type: TYPE_NORMAL
- en: 47 213
  prefs: []
  type: TYPE_NORMAL
- en: 49 238
  prefs: []
  type: TYPE_NORMAL
- en: 40 229
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure depicts a knee-pain map based on the SOCR dataset from
    `wiki.stat.ucla`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00166.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We place the data file in a data directory (you can copy the data file to any
    location you prefer).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The data file contains 8,666 entries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We then load the data file into RDD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We now create a GaussianMixture model, and set the parameters for the model.
    We set the K value to 4, since the data was collected by four views: **Left Front**
    (**LF**), **Left Back** (**LB**), **Right Front** (**RF**), and **Right Back**
    (**RB**). We set the convergence to the default value of 0.01, and the maximum
    iteration count to 100:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We run the model algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We print out the key values for the GaussianMixture model after the training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we set the K value to 4, we will have four sets of values printed out
    in the console logger:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00167.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'We also print out the first 50 cluster-labels based on the GaussianMixture
    model predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The sample output in the console will show the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We then close the program by stopping the Spark context:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, we observed that KMeans can discover and allocate membership
    to one and only one cluster based on an iterative method using similarity (Euclidian,
    and so on). One can think of KMeans as a specialized version of a Gaussian mixture
    model with EM models in which a discrete (hard) membership is enforced.
  prefs: []
  type: TYPE_NORMAL
- en: 'But there are cases that have overlap, which is often the case in medicine
    or signal processing, as depicted in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00168.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In such cases, we need a probability density function that can express the membership
    in each sub-distribution. The Gaussian Mixture models with **Expectation Maximization**
    (**EM**) is the algorithm `GaussianMixture()` available in Spark that can deal
    with this use case.
  prefs: []
  type: TYPE_NORMAL
- en: Here is Spark's API for implementing Gaussian Mixture with Expectation Maximization
    (the maximization of log likelihood).
  prefs: []
  type: TYPE_NORMAL
- en: New GaussianMixture()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This constructs a default instance. The default parameters that control the
    behavior of the model are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00169.gif)'
  prefs: []
  type: TYPE_IMG
- en: The Gaussian Mixture models with Expectation Maximization are a form of soft
    clustering in which a membership can be inferred using a log maximum likelihood
    function. In this scenario, a probability density function with mean and covariance
    is used to define the membership or likelihood of a membership to K number of
    clusters. It is flexible in the sense that the membership is not quantified which
    allows for overlapping membership based on probability (indexed to multiple sub-distributions).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure is a snapshot of the EM algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00170.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here are the steps to the EM algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Assume *N* number of Gaussian distribution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Iterate until we have convergence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each point Z drawn with conditional probability of being drawn from distribution
    Xi written as *P (Z | Xi)*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adjust the parameter's mean and variance so that they fit the points that are
    assigned to the sub-distribution
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For a more mathematical explanation, including detailed work on maximum likelihood,
    see the following link: [http://www.ee.iisc.ernet.in/new/people/faculty/prasantg/downloads/GMM_Tutorial_Reynolds.pdf](http://www.ee.iisc.ernet.in/new/people/faculty/prasantg/downloads/GMM_Tutorial_Reynolds.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following figure provides a quick reference point to highlight some of
    the differences between hard versus soft clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00171.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for constructor GaussianMixture can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.GaussianMixture](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.GaussianMixture)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for constructor GaussianMixtureModel can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.GaussianMixtureModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.GaussianMixtureModel)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying the vertices of a graph using Power Iteration Clustering (PIC) in
    Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is a classification method for the vertices of a graph given their similarities
    as defined by their edges. It uses the GraphX library which is ships out of the
    box with Spark to implement the algorithm. Power Iteration Clustering is similar
    to other Eigen Vector/Eigen Value decomposition algorithms, but without the overhead
    of matrix decomposition. It is suitable when you have a large sparse matrix (for
    example, graphs depicted as a sparse matrix).
  prefs: []
  type: TYPE_NORMAL
- en: GraphFrames will be the replacement/interface proper for the GraphX library
    going forward ([https://databricks.com/blog/2016/03/03/introducing-graphframes.html](https://databricks.com/blog/2016/03/03/introducing-graphframes.html)).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for Spark context to get access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the logger level to ERROR only to reduce the output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Spark''s configuration and SQL context so we can have access to the
    cluster and be able to create and use a DataFrame as needed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We create a training dataset with a list of datasets and use the Spark `sparkContext.parallelize()`
    function to create Spark RDD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'We create a `PowerIterationClustering` object and set the parameters. We set
    the `K` value to `3` and max iteration count to `15`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'We then let the model run:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We print out the cluster assignment based on the model for the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The console output will show the following information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00172.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'We also print out the model assignment data in a collection for each cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The console output will display the following information (in total, we have
    three clusters which were set in the preceding parameters):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'We then close the program by stopping the Spark context:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We created a list of edges and vertices for a graph and then proceeded to create
    the object and set the parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step was the model of training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: The clusters were then outputted for inspection. The code near the end prints
    out the model assignment data in a collection for each cluster using Spark transformation
    operators.
  prefs: []
  type: TYPE_NORMAL
- en: At the core **PIC** (**Power Iteration Clustering**) is an eigenvalue class
    algorithm which avoids matrix decomposition by producing an Eigen Value plus an
    Eigen Vector to satisfy *Av* = λ*v.* Because PIC avoids the decomposition of the
    matrix A, it is suitable when the input matrix A (describing a graph in the case
    of Spark's PIC) is a large sparse matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of PIC in image processing (post enhanced for paper) is depicted
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00173.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The Spark implementation of the PIC algorithm is an improvement over the previous
    common implementation (NCut) by computing a pseudo Eigen Vector of the similarities
    defined as edges given N number of vertices (like an affinity matrix).
  prefs: []
  type: TYPE_NORMAL
- en: 'The input as depicted in the following figure is a trinary tuple of RDDs describing
    the graph. The output is a model with a cluster assignment for each node. The
    algorithm similarities (edges) are assumed to be positive and symmetrical (not
    shown):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00174.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For a more detailed mathematical treatment of the subject (power iteration),
    see the following white paper from Carnegie Mellon University: [http://www.cs.cmu.edu/~wcohen/postscript/icml2010-pic-final.pdf](http://www.cs.cmu.edu/~wcohen/postscript/icml2010-pic-final.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for the constructor `PowerIterationClustering()` can be found
    at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.PowerIterationClustering](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.PowerIterationClustering)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for the constructor `PowerIterationClusteringModel()` can be found
    at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.PowerIterationClusteringModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.PowerIterationClusteringModel)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Latent Dirichlet Allocation (LDA) to classify documents and text into topics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will explore the **Latent Dirichlet Allocation** (**LDA**)
    algorithm in Spark 2.0\. The LDA we use in this recipe is completely different
    from linear discrimination analysis. Both Latent Dirichlet Allocation and linear
    discrimination analysis are referred to as LDA, but they are extremely different
    techniques. In this recipe, when we use the LDA, we refer to Latent Dirichlet
    Allocation. The chapter on text analytics is also relevant to understanding the
    LDA.
  prefs: []
  type: TYPE_NORMAL
- en: LDA is often used in natural language processing which tries to classify a large
    body of document (for example, emails from the Enron fraud case) into a discrete
    number of topics or themes so it can be understood. LDA is also a good candidate
    for selecting articles based on one's interest (for example, as you turn a page
    and spend time on a specific topic) in a given magazine article or page.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'We set up the necessary Spark Session to gain access to the cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'We have a sample LDA dataset, which is located at the following relative path
    (you can use an absolute path). The sample file is provided with any Spark distribution
    and can be found under the home directory of Spark inside the data directory (see
    the following). Assume the input is a set of features for input to the LDA method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00175.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In this step, we read the file and create the necessary dataset from the input
    file and show the top five rows in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00176.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'We create the LDA object and set the parameters for the object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'We then run the model using the high-level API from the package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: We get the topics distribution from the LDA model for each set of features,
    and show the topics.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We set the `maxTermsPerTopic` value as `3`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'On the console, the output will show the following information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00177.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We also transform the training dataset from the LDA model, and show the result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will display the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00178.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'If the preceding method is changed to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'The result will be displayed as truncated:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00179.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We close the Spark context to end the program:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LDA assumes that the document is a mixture of different topics with Dirichlet
    prior distribution. The words in the document are assumed to have an affinity
    towards a specific topic which allows LDA to classify the overall document (compose
    and assign a distribution) that best matches a topic.
  prefs: []
  type: TYPE_NORMAL
- en: A topic model is a generative latent model for discovering abstract themes (topics)
    that occur in the body of documents (often too large for humans to handle). The
    models are a pre-curser to summarize, search, and browse a large set of unlabeled
    documents and their contents. Generally speaking, we are trying to find a cluster
    of features (words, sub-images, and so on) that occur together.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure depicts the overall LDA scheme:'
  prefs: []
  type: TYPE_NORMAL
- en: Please be sure to refer to the white paper cited here for completeness [http://ai.stanford.edu/~ang/papers/nips01-lda.pdf](http://ai.stanford.edu/~ang/papers/nips01-lda.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00180.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The steps for the LDA algorithm are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize the following parameters (controls concentration and smoothing):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Alpha parameter (high alpha makes documents more similar to each other and contain
    similar topics )
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Beta parameter ( high beta means each topic is most likely to contain a mix
    of most of the words)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomly initialize the topic assignment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Iterate:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each document.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each word in the document.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Resample the topic for each word.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With respect to all other words and their current assignment (for the current
    iteration).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get the result.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Model evaluation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In statistics, Dirichlet distribution Dir(alpha) is a family of continuous multivariate
    probability distributions parameterized by a vector α of positive real numbers.
    For a more in-depth treatment of LDA, see the original paper in the
  prefs: []
  type: TYPE_NORMAL
- en: Journal of Machine Learning at: [http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: The LDA does not assign any semantics to a topic and does not care what the
    topics are called. It is only a generative model that uses distribution of fine-grained
    items (for example, words about cats, dogs, fish, cars) to assign an overall topic
    that scores the best. It does not know, care, or understand about topics called
    dogs or cats.
  prefs: []
  type: TYPE_NORMAL
- en: We often have to tokenize and vectorize the document via TF-IDF prior to input
    to an LDA algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following figure depicts the LDA in a nutshell:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00181.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'There are two approaches to document analysis. We can simply use matrix factorization
    to decompose a large matrix of datasets to a smaller matrix (topic assignments)
    times a vector (topics themselves):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00182.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**LDA**: documentation for a constructor can be found at: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDA](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDA)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**LDAModel**: documentation for a constructor can be found at: ](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDA)[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDAModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDAModel)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'See also, via Spark''s Scala API, documentation links for the following:'
  prefs: []
  type: TYPE_NORMAL
- en: DistributedLDAModel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EMLDAOptimizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LDAOptimizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LocalLDAModel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OnlineLDAOptimizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming KMeans to classify data in near real-time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark streaming is a powerful facility which lets you combine near real time
    and batch in the same paradigm. The streaming KMeans interface lives at the intersection
    of ML clustering and Spark streaming, and takes full advantage of the core facilities
    provided by Spark streaming itself (for example, fault tolerance, exactly once
    delivery semantics, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the necessary packages for streaming KMeans:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`package spark.ml.cookbook.chapter8`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary packages for streaming KMeans:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: We set up the following parameters for the streaming KMeans program. The training
    directory will be the directory to send the training data file. The KMeans clustering
    model utilizes the training data to run algorithms and calculations. The `testDirectory`
    will be the test data for predictions. The `batchDuration` is a number in seconds
    for a batch run. In the following case, the program will check every 10 seconds
    to see if there is any new data files for recalculations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The cluster is set to `2`, and the data dimensions will be `3`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'With the preceding settings, the sample training data will contain data like
    the following (in the format of [*X[1], X[2], ...X[n]*], where *n* is `numDimensions`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[0.0,0.0,0.0]'
  prefs: []
  type: TYPE_NORMAL
- en: '[0.1,0.1,0.1]'
  prefs: []
  type: TYPE_NORMAL
- en: '[0.2,0.2,0.2]'
  prefs: []
  type: TYPE_NORMAL
- en: '[9.0,9.0,9.0]'
  prefs: []
  type: TYPE_NORMAL
- en: '[9.1,9.1,9.1]'
  prefs: []
  type: TYPE_NORMAL
- en: '[9.2,9.2,9.2]'
  prefs: []
  type: TYPE_NORMAL
- en: '[0.1,0.0,0.0]'
  prefs: []
  type: TYPE_NORMAL
- en: '[0.2,0.1,0.1]'
  prefs: []
  type: TYPE_NORMAL
- en: '....'
  prefs: []
  type: TYPE_NORMAL
- en: 'The test data file will contain data like the following (in the format of (*y,
    [X1,X2, .. Xn]*), where *n* is `numDimensions` and `y` is an identifier):'
  prefs: []
  type: TYPE_NORMAL
- en: (7,[0.4,0.4,0.4])
  prefs: []
  type: TYPE_NORMAL
- en: (8,[0.1,0.1,0.1])
  prefs: []
  type: TYPE_NORMAL
- en: (9,[0.2,0.2,0.2])
  prefs: []
  type: TYPE_NORMAL
- en: (10,[1.1,1.0,1.0])
  prefs: []
  type: TYPE_NORMAL
- en: (11,[9.2,9.1,9.2])
  prefs: []
  type: TYPE_NORMAL
- en: (12,[9.3,9.2,9.3])
  prefs: []
  type: TYPE_NORMAL
- en: 'We set up the necessary Spark context to gain access to the cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the streaming context and micro-batch window:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code will create data by parsing the data file in the preceding
    two directories into `trainingData` and `testData RDDs`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'We create the `StreamingKMeans` model and set the parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'The program will train the model using the training dataset and predict using
    the test dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'We start the streaming context, and the program will run the batch every 10
    seconds to see if a new dataset is available for training, and if there is any
    new test dataset for prediction. The program will exit if a termination signal
    is received (exit the batch running):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'We copy the `testKStreaming1.txt` data file into the preceding `testDir` set
    and see the following printed out in the console logs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00183.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: For a Windows machine, we copied the `testKStreaming1.txt` file into the directory: `C:\spark-2.0.0-bin-hadoop2.7\data\sparkml2\chapter8\testDir\`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can also check the SparkUI for more information: `http://localhost:4040/`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The job panel will display the streaming jobs, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00184.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'As shown in the following figure, the streaming panel will show the preceding
    Streaming KMeans matrix as the matrix displayed, the batch job running every 10
    seconds in this case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00185.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'You can get more details on the streaming batch by clicking on any of the batches,
    as shown in following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00186.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In certain situations, we cannot use batch methods to load and capture the events
    and then react to them. We can use creative methods of capturing events in the
    memory or a landing DB and then rapidly marshal that over to another system for
    processing, but most of these systems fail to act as streaming systems and often
    are very expensive to build.
  prefs: []
  type: TYPE_NORMAL
- en: Spark provides a near real time (also referred to as subjective real time) that
    can receive incoming sources, such as Twitter feeds, signals, and so, on via connectors
    (for example, a Kafka connector) and then process and present them as an RDD interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the elements needed to build and construct streaming KMeans in Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the streaming context as opposed to the regular Spark context used so far:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Select your connector to connect to a data source and receive events:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Twitter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kafka
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Third party
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ZeroMQ
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TCP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '........'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Create your streaming KMeans model; set the parameters as needed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Train and predict as usual:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Have in mind that K cannot be changed on the fly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Start the context and await for the termination signal to exit:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`ssc.start()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ssc.awaitTermination()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Streaming KMeans are special cases of KMeans implementation in which the data
    can arrive at near real time and be classified into a cluster (hard classification)
    as needed. The applications are vast and can vary from near real-time anomaly
    detection (fraud, crime, intelligence, monitoring, and surveillance) to fine-grain
    micro-sector rotation visualization with Voronoi diagrams in finance. [Chapter
    13](part0538.html#G12EK0-4d291c9fed174a6992fd24938c2f9c77), *Spark Streaming and
    Machine Learning Library* provides a more detailed coverage for streaming.
  prefs: []
  type: TYPE_NORMAL
- en: For a reference to Voronoi diagrams, see the following URL: [https://en.wikipedia.org/wiki/Voronoi_diagram](https://en.wikipedia.org/wiki/Voronoi_diagram)
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently there are other algorithms besides streaming KMeans in the Spark
    Machine Library, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00187.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for Streaming KMeans can be found at: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeans](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeans)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for Streaming KMeans Model can be found at: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.stat.test.StreamingTest](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.stat.test.StreamingTest)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: "Documentation for Streaming Test--very useful for data generation--can be found\
    \ at: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeansModel\uFEFF\
    ](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeansModel)"
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
