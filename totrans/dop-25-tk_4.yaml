- en: Debugging Issues Discovered Through Metrics and Alerts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you eliminate the impossible, whatever remains, however improbable, must
    be the truth.
  prefs: []
  type: TYPE_NORMAL
- en: '- *Spock*'
  prefs: []
  type: TYPE_NORMAL
- en: So far, we explored how to gather metrics and how to create alerts that will
    notify us when there is an issue. We also learned how to query metrics and dig
    for information we might need when trying to find the cause of a problem. We'll
    expand on that and try to debug a simulated issue.
  prefs: []
  type: TYPE_NORMAL
- en: Saying that an application does not work correctly should not be enough by itself.
    We should be much more precise. Our goal is to be able to pinpoint not only which
    application is malfunctioning, but also which part of it is the culprit. We should
    be able to blame a specific function, a method, a request path, and so on. The
    more precise we are in detecting which part of an application is causing a problem,
    the faster we will find the cause of an issue. As a result, it should be easier
    and faster to fix the issue through a new release (a hotfix), scaling, or any
    other means at our disposal.
  prefs: []
  type: TYPE_NORMAL
- en: Let's get going. We'll need a cluster (unless you already have one) before we
    simulate a problem that needs to be solved.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `vfarcic/k8s-specs` ([https://github.com/vfarcic/k8s-specs](https://github.com/vfarcic/k8s-specs))
    repository will continue being our source of Kubernetes definitions we'll use
    for our examples. We'll make sure that it is up-to-date by pulling the latest
    version.
  prefs: []
  type: TYPE_NORMAL
- en: All the commands from this chapter are available in the `04-instrument.sh` ([https://gist.github.com/vfarcic/851b37be06bb7652e55529fcb28d2c16](https://gist.github.com/vfarcic/851b37be06bb7652e55529fcb28d2c16))
    Gist. Just as in the previous chapter, it contains not only the commands but also
    Prometheus' expressions. They are all commented (with `#`). If you're planning
    to copy and paste the expressions from the Gist, please exclude the comments.
    Each expression has `# Prometheus expression` comment on top to help you identify
    it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Given that we learned how to install a fully operational Prometheus and the
    rest of the tools from its chart, and that we'll continue using them, I moved
    it to the Gists. Those that follow are copies of those we used in the previous
    chapter, with the addition of environment variables `PROM_ADDR` and `AM_ADDR`,
    and the steps for the installation of the **Prometheus Chart**. Please create
    a cluster that meets (or exceeds) the requirements specified in the Gists that
    follow, unless you already have a cluster that satisfies them.
  prefs: []
  type: TYPE_NORMAL
- en: '`gke-instrument.sh`: **GKE** with 3 n1-standard-1 worker nodes, **nginx Ingress**,
    **tiller**, **Prometheus** Chart, and environment variables **LB_IP**, **PROM_ADDR**,
    and **AM_ADDR** ([https://gist.github.com/675f4b3ee2c55ee718cf132e71e04c6e](https://gist.github.com/675f4b3ee2c55ee718cf132e71e04c6e)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eks-instrument.sh`: **EKS** with 3 t2.small worker nodes, **nginx Ingress**,
    **tiller**, **Metrics Server**, **Prometheus** Chart, and environment variables
    **LB_IP**, **PROM_ADDR**, and **AM_ADDR** ([https://gist.github.com/70a14c8f15c7ffa533ea7feb75341545](https://gist.github.com/70a14c8f15c7ffa533ea7feb75341545)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aks-instrument.sh`: **AKS** with 3 Standard_B2s worker nodes, **nginx Ingress**,
    and **tiller**, **Prometheus** Chart, and environment variables **LB_IP**, **PROM_ADDR**,
    and **AM_ADDR** ([https://gist.github.com/65a0d5834c9e20ebf1b99225fba0d339](https://gist.github.com/65a0d5834c9e20ebf1b99225fba0d339)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`docker-instrument.sh`: **Docker for Desktop** with **2 CPUs**, **3 GB RAM**,
    **nginx Ingress**, **tiller**, **Metrics Server**, **Prometheus** Chart, and environment
    variables **LB_IP**, **PROM_ADDR**, and **AM_ADDR** ([https://gist.github.com/1dddcae847e97219ab75f936d93451c2](https://gist.github.com/1dddcae847e97219ab75f936d93451c2)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minikube-instrument.sh`: **minikube** with **2 CPUs**, **3 GB RAM**, **ingress,
    storage-provisioner**, **default-storageclass**, and **metrics-server** addons
    enabled, **tiller**, **Prometheus** Chart, and environment variables **LB_IP**,
    **PROM_ADDR**, and **AM_ADDR** ([https://gist.github.com/779fae2ae374cf91a5929070e47bddc8](https://gist.github.com/779fae2ae374cf91a5929070e47bddc8)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now we're ready to face our first simulated issue that might require debugging.
  prefs: []
  type: TYPE_NORMAL
- en: Facing a disaster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's explore one disaster scenario. Frankly, it's not going to be a real disaster,
    but it will require us to find a solution to an issue.
  prefs: []
  type: TYPE_NORMAL
- en: We'll start by installing the already familiar `go-demo-5` application.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We declared `GD5_ADDR` with the address through which we'll be able to access
    the application. We used it as `ingress.host` variable when we installed the `go-demo-5`
    Chart. To be on the safe side, we waited until the app rolled out, and all that's
    left, from the deployment perspective, is to confirm that it is running by sending
    an HTTP request.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The output is the developer's favorite message `hello, world!`.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll simulate an issue by sending twenty slow requests with up to ten
    seconds duration. That will be our simulation of a problem that might need to
    be fixed.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Since we already have Prometheus' alerts, we should receive a notification on
    Slack stating that the application is too slow. However, many readers might be
    using the same channel for those exercises, and it might not be clear whether
    the message comes from us. Instead, we'll open Prometheus' alerts screen to confirm
    that there is a problem. In the "real" setting, you wouldn't be checking Prometheus
    alerts, but wait for notifications on Slack, or whichever other notifications
    tool you chose.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: A few moments later (don't forget to refresh the screen), the `AppTooSlow` alert
    should fire, letting us know that one of our applications is slow and that we
    should do something to remedy the problem.
  prefs: []
  type: TYPE_NORMAL
- en: True to the promise that each chapter will feature outputs and screenshots from
    a different Kubernetes flavor, this time it's minikube's turn.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/fb08d36b-cba3-4acb-8385-f3836d1a1f1d.png)Figure 4-1: One of Prometheus''
    alerts in the firing state'
  prefs: []
  type: TYPE_NORMAL
- en: We'll imagine that we did not generate slow requests intentionally, so we'll
    try to find out what the issue is. Which application is too slow? What useful
    information can we pass to the team so that they can fix the problem as soon as
    possible?
  prefs: []
  type: TYPE_NORMAL
- en: The first logical debugging step is to execute the same expression as the one
    used by the alert. Please expand the `AppTooSlow` alert and click the link of
    the expression. You'll be redirected to the graph screen with the expression already
    pre-populated. Click the Execute button, and switch to the *Graph* tab.
  prefs: []
  type: TYPE_NORMAL
- en: We can see, from the graph, that there was a surge in the number of slow requests.
    The alert was fired because less than ninety-five percent of responses are within
    0.25 seconds bucket. Judging from my Graph (screenshot following), zero percent
    of responses were inside the 0.25 seconds bucket or, in other words, all were
    slower than that. A moment later, that improved slightly by jumping to only six
    percent of fast requests.
  prefs: []
  type: TYPE_NORMAL
- en: All in all, we have a situation in which too many requests are getting slow
    responses, and we should fix that. The main question is how to find out what the
    cause of that slowness is?
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/8052c5cf-a627-4785-a91b-28e759a53a1c.png)Figure 4-2: The graph with
    the percentage of requests with fast responses'
  prefs: []
  type: TYPE_NORMAL
- en: How about executing different expressions. We can, for example, output the rate
    of request durations for that `ingress` (application).
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows, and press the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: That graph shows us the history of request durations, but it does not get us
    any closer to revealing the cause of the issue or, to be more precise, to the
    part of the application that is slow. We could try using other metrics, but they
    are, more or less, equally generic and are probably not going to get us anywhere.
    We need more detailed application-specific metrics. We need data that comes from
    inside the `go-demo-5` app.
  prefs: []
  type: TYPE_NORMAL
- en: Using instrumentation to provide more detailed metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We shouldn't just say that the `go-demo-5` application is slow. That would not
    provide enough information for us to quickly inspect the code in search of the
    exact cause of that slowness. We should be able to do better and deduce which
    part of the application is misbehaving. Can we pinpoint a specific path that produces
    slow responses? Are all methods equally slow, or the issue is limited only to
    one? Do we know which function produces slowness? There are many similar questions
    we should be able to answer in situations like that. But we can't, with the current
    metrics. They are too generic, and they can usually only tell us that a specific
    Kubernetes resource is misbehaving. The metrics we're collecting are too broad
    to answer application-specific questions.
  prefs: []
  type: TYPE_NORMAL
- en: The metrics we explored so far are a combination of exporters and instrumentations.
    Exporters are in charge of taking existing metrics and converting them into the
    Prometheus-friendly format. An example would be Node Exporter ([https://github.com/prometheus/node_exporter](https://github.com/prometheus/node_exporter))
    that is taking "standard" Linux metrics and converting them into Prometheus' time-series
    format. Another example is kube-state-metrics ([https://github.com/kubernetes/kube-state-metrics](https://github.com/kubernetes/kube-state-metrics))
    that listens to Kube API server and generates metrics with the state of the resources.
  prefs: []
  type: TYPE_NORMAL
- en: Instrumented metrics are baked into applications. They are an integral part
    of the code of our apps, and they are usually exposed through the `/metrics` endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: The easiest way to add metrics to your applications is through one of the Prometheus
    client libraries. At the time of this writing, Go ([https://github.com/prometheus/client_golang](https://github.com/prometheus/client_golang)),
    Java and Scala ([https://github.com/prometheus/client_java](https://github.com/prometheus/client_java)),
    Python ([https://github.com/prometheus/client_python](https://github.com/prometheus/client_python)),
    and Ruby ([https://github.com/prometheus/client_ruby](https://github.com/prometheus/client_ruby))
    libraries are officially provided.
  prefs: []
  type: TYPE_NORMAL
- en: On top of those, the community supports Bash ([https://github.com/aecolley/client_bash](https://github.com/aecolley/client_bash)),
    C++ ([https://github.com/jupp0r/prometheus-cpp](https://github.com/jupp0r/prometheus-cpp)),
    Common Lisp ([https://github.com/deadtrickster/prometheus.cl](https://github.com/deadtrickster/prometheus.cl)),
    Elixir ([https://github.com/deadtrickster/prometheus.ex](https://github.com/deadtrickster/prometheus.ex)),
    Erlang ([https://github.com/deadtrickster/prometheus.erl](https://github.com/deadtrickster/prometheus.erl)),
    Haskell ([https://github.com/fimad/prometheus-haskell](https://github.com/fimad/prometheus-haskell)),
    Lua for Nginx ([https://github.com/knyar/nginx-lua-prometheus](https://github.com/knyar/nginx-lua-prometheus)),
    Lua for Tarantool ([https://github.com/tarantool/prometheus](https://github.com/tarantool/prometheus)),
    .NET / C# ([https://github.com/andrasm/prometheus-net](https://github.com/andrasm/prometheus-net)),
    Node.js ([https://github.com/siimon/prom-client](https://github.com/siimon/prom-client)),
    Perl ([https://metacpan.org/pod/Net::Prometheus](https://metacpan.org/pod/Net::Prometheus)),
    PHP ([https://github.com/Jimdo/prometheus_client_php](https://github.com/Jimdo/prometheus_client_php)),
    and Rust ([https://github.com/pingcap/rust-prometheus](https://github.com/pingcap/rust-prometheus)).
    Even if you code in a different language, you can easily provide Prometheus-friendly
    metrics by outputting results in a text-based exposition format ([https://prometheus.io/docs/instrumenting/exposition_formats/](https://prometheus.io/docs/instrumenting/exposition_formats/)).
  prefs: []
  type: TYPE_NORMAL
- en: Overhead of collecting metrics should be negligible and, since Prometheus' pulls
    them periodically, outputting them should have a tiny footprint as well. Even
    if you choose not to use Prometheus, or to switch to something else, the format
    is becoming the standard, and your next metrics collector tool is likely to expect
    the same data.
  prefs: []
  type: TYPE_NORMAL
- en: All in all, there is no excuse not to bake metrics into your applications, and,
    as you'll see soon, they provide invaluable information that we cannot obtain
    from outside.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at an example of the instrumented metrics in `go-demo-5`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The application is written in Go. Don't worry if that's not your language of
    choice. We'll just take a quick look at a few examples as a way to understand
    the logic behind instrumentation, not the exact implementation.
  prefs: []
  type: TYPE_NORMAL
- en: The first interesting part is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We defined a variable that contains a Prometheus Histogram Vector with a few
    options. The `Sybsystem` and the `Name` form the base metric `http_server_resp_time`.
    Since it is a histogram, the final metrics will be created by adding `_bucket`,
    `_sum`, and `_count` suffixes.
  prefs: []
  type: TYPE_NORMAL
- en: Please consult *histogram* ([https://prometheus.io/docs/concepts/metric_types/#histogram](https://prometheus.io/docs/concepts/metric_types/#histogram))
    documentation for more info about that Prometheus' metric type.
  prefs: []
  type: TYPE_NORMAL
- en: The last part is a string array (`[]string`) that defines all the labels we'd
    like to add to the metric. In our case, those labels are `service`, `code`, `method`,
    and `path`. Labels can be anything we need, just as long as they provide the sufficient
    information we might require when querying those metrics.
  prefs: []
  type: TYPE_NORMAL
- en: The next point of interest is the `recordMetrics` function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: I created that as a helper function that can be called from different locations
    in the code. It accepts `start` time, the `Request`, and the return `code` as
    arguments. The function itself calculates `duration` by subtracting the current
    `time` with the `start` time. That `duration` is used in the `Observe` function
    and provides the value of the metric. There are also the labels that will help
    us fine-tune our expressions later on.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we'll take a look at one of the examples where the `recordMetrics`
    is invoked.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The `HelloServer` function is the one that returns the `hello, world!` response
    you already saw quite a few times. The details of that function are not important.
    In this context, the only part that matters is the line `defer func() { recordMetrics(start,
    req, http.StatusOK) }()`. In Go, `defer` allows us to execute something at the
    end of the function where it resides. In our case, that something is the invocation
    of the `recordMetrics` function that will record the duration of a request. In
    other words, before the execution leaves the `HelloServer` function, it'll record
    the duration by invoking the `recordMetrics` function.
  prefs: []
  type: TYPE_NORMAL
- en: I won't go further into the code that contains instrumentation since that would
    assume that you are interested in intricacies behind Go and I'm trying to keep
    this book language-agnostic. I'll let you consult the documentation and examples
    from your favorite language. Instead, we'll take a look at the `go-demo-5` instrumented
    metrics in action.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We created a Pod based on the `appropriate/curl` image, and we sent a request
    through the Service using the address `go-demo-5.go-demo-5:8080/metrics`. The
    first `go-demo-5` is the name of the Service, and the second is the Namespace
    where it resides. As a result, we got output with all the instrumented metrics
    available in that application. We won't go through all of them, but only those
    created by the `http_server_resp_time` histogram.
  prefs: []
  type: TYPE_NORMAL
- en: The relevant parts of the output are as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the Go library we used in the application code created quite
    a few metrics from the `http_server_resp_time` histogram. We got one for each
    of the twelve buckets (`http_server_resp_time_bucket`), one with the sum of the
    durations (`http_server_resp_time_sum`), and one with the count (`http_server_resp_time_count`).
    We would have much more if we made requests that would have different labels.
    For now, those fourteen metrics are all coming from requests that responded with
    the HTTP code `200`, that used `GET` method, that were sent to the `/demo/hello`
    path, and that are coming from the `go-demo` service (application). If we'd create
    requests with different methods (for example, `POST`) or to different paths, the
    number of metrics would increase. Similarly, if we'd implement the same instrumented
    metric in other applications (but with different `service` labels), we'd have
    metrics with the same key (`http_server_resp_time`) that would provide insights
    into multiple apps. That raises the question of whether we should unify metric
    names across all the apps, or not.
  prefs: []
  type: TYPE_NORMAL
- en: I prefer having instrumented metrics of the same type with the same name across
    all the applications. For example, all those that collect response times can be
    called `http_server_resp_time`. That simplifies querying data in Prometheus. Instead
    of learning about instrumented metrics from each individual application, learning
    those from one provides knowledge about all. On the other hand, I am in favor
    of giving each team full control over their applications. That includes the decisions
    which metrics to implement, and how to call them.
  prefs: []
  type: TYPE_NORMAL
- en: All in all, it depends on the structure and responsibilities of the teams. If
    a team is entirely in charge of their applications and they debug problems specific
    to their apps, there is no inherent need for standardization of the names of instrumented
    metrics. On the other hand, if monitoring is centralized and the other teams might
    expect help from experts in that area, creating naming conventions is a must.
    Otherwise, we could easily end up with thousands of metrics with different names
    and types, even though most of them are providing the same information.
  prefs: []
  type: TYPE_NORMAL
- en: For the rest of this chapter, I will assume that we did agree to have `http_server_resp_time`
    histogram in all applications, where that's applicable.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's see how we can tell Prometheus that it should pull the metrics from
    the `go-demo-5` application. It would be even better if we could tell Prometheus
    to pull data from all the apps that have instrumented metrics. Actually, now when
    I think about it, we did not yet discuss how did Prometheus find Node Exporter
    and Kube State Metrics in the previous chapter. So, let's go briefly through the
    discovery process.
  prefs: []
  type: TYPE_NORMAL
- en: A good starting point is the Prometheus' targets screen.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The most interesting group of targets is the `kubernetes-service-endpoints`.
    If we take a closer look at the labels, we'll see that each has `kubernetes_name`
    and that three of the targets have it set to `go-demo-5`. Prometheus somehow found
    that we have three replicas of the application and that metrics are available
    through the port `8080`. If we look further, we'll notice that `prometheus-node-exporter`
    is there as well, one for each node in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The same goes for `prometheus-kube-state-metrics`. There might be others in
    that group.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/a36dd797-57ba-4cf7-bc6c-8da9161fd59c.png)Figure 4-3: kubernetes-service-endpoints
    Prometheus'' targets'
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus discovered all the targets through Kubernetes Services. It extracted
    the port from each of the Services, and it assumed that data is available through
    the `/metrics` endpoint. As a result, every application we have in the cluster,
    that is accessible through a Kubernetes Service, was automatically added to the
    `kubernetes-service-endpoints` group of Prometheus' targets. There was no need
    for us to fiddle with Prometheus' configuration to add `go-demo-5` to the mix.
    It was just discovered. Pretty neat, isn't it?
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, some of the metrics will not be accessible, and that target will
    be marked as red. As an example, `kube-dns` in minikube is not reachable from
    Prometheus. That's common, and it's not a reason to be alarmed, just as long as
    that's not one of the metric sources we do need.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll take a quick look at a few expressions we can write using the instrumented
    metrics coming from `go-demo-5`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Please type the expression that follows, press the Execute button, and switch
    to the *Graph* tab.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We can see three lines that correspond to three replicas of `go-demo-5`. That
    should not come as a surprise since each of those is pulled from instrumented
    metrics coming from each of the replicas of the application. Since those metrics
    are counters that can only increase, the lines of the graph are continuously going
    up.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/17add48b-f7c5-4e29-90e6-43dcf3919682.png)Figure 4-4: The graph with
    the http_server_resp_time_count counter'
  prefs: []
  type: TYPE_NORMAL
- en: That wasn't very useful. If we were interested in the rate of the count of requests,
    we'd envelop the previous expression inside the `rate()` function. We'll do that
    later. For now, we'll write the simplest expression that will give us the average
    response time per request.
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows, and press the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The expression itself should be easy to understand. We are taking the sum of
    all the requests and dividing it with the count. Since we already discovered that
    the problem is somewhere inside the `go-demo-5` app, we used the `kubernetes_name`
    label to limit the results. Even though that is the only application with that
    metric currently running in our cluster, it is a good idea to get used to the
    fact that there might be others at some later date when we extend instrumentation
    to other applications.
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the average request duration increased for a while, only to
    drop close to the initial values a while later. That spike coincides with the
    twenty slow requests we sent a while ago. In my case (screenshot following), the
    peak is close to the average response time of 0.1 seconds, only to drop to around
    0.02 seconds a while later.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/e4efb795-db45-4d39-87be-b5982c518722.png)Figure 4-5: The graph with
    the cumulative average response time'
  prefs: []
  type: TYPE_NORMAL
- en: Please note that the expression we just executed is deeply flawed. It shows
    the cumulative average response time, instead of displaying the `rate`. But, you
    already knew that. That was only a taste of the instrumented metric, not its "real"
    usage (that comes soon).
  prefs: []
  type: TYPE_NORMAL
- en: You might notice that even the spike is very low. It is certainly lower than
    what we'd expect from sending only twenty slow requests through `curl`. The reason
    for that lies in the fact that we were not the only ones making those requests.
    The `readinessProbe` and the `livenessProbe` are sending requests as well, and
    they are very fast. Unlike in the previous chapter when we were measuring only
    the requests coming through Ingress, this time we're capturing all the requests
    that enter the application, and that includes health-checks.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we saw a few examples of the `http_server_resp_time` metric that is
    generated inside our `go-demo-5` application, we can use that knowledge to try
    to debug the simulated issue that led us towards instrumentation.
  prefs: []
  type: TYPE_NORMAL
- en: Using internal metrics to debug potential issues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We'll re-send requests with slow responses again so that we get to the same
    point where we started this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We sent twenty requests that will result in responses with random duration (up
    to ten seconds). Further on, we opened Prometheus' alerts screen.
  prefs: []
  type: TYPE_NORMAL
- en: A while later, the `AppTooSlow` alert should fire (remember to refresh your
    screen), and we have a (simulated) problem that needs to be solved. Before we
    start panicking and do something hasty, we'll try to find the cause of the issue.
  prefs: []
  type: TYPE_NORMAL
- en: Please click the expression of the `AppTooSlow` alert.
  prefs: []
  type: TYPE_NORMAL
- en: We are redirected to the graph screen with the pre-populated expression from
    the alert. Feel free to click the Expression button, even though it will not provide
    any additional info, apart from the fact that the application was fast, and then
    it slowed down for some inexplicable reason.
  prefs: []
  type: TYPE_NORMAL
- en: You will not be able to gather more details from that expression. You will not
    know whether it's slow on all methods, whether only a specific path responds slow,
    nor much of any other application-specific details. Simply put, the `nginx_ingress_controller_request_duration_seconds`
    metric is too generic. It served us well as a way to notify us that the application's
    response time increased, but it does not provide enough information about the
    cause of the issue. For that, we'll switch to the `http_server_resp_time` metric
    Prometheus is retrieving directly from `go-demo-5` replicas.
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows, and press the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Switch to the *Graph* tab, if you're not there already.
  prefs: []
  type: TYPE_NORMAL
- en: That expression is very similar to the queries we wrote before when we were
    using the `nginx_ingress_controller_request_duration_seconds_sum` metric. We are
    dividing the rate of requests in the 0.1 seconds bucket with the rate of all the
    requests.
  prefs: []
  type: TYPE_NORMAL
- en: In my case (screenshot following), we can see that the percentage of fast responses
    dropped twice. That coincides with the simulated slow requests we sent earlier.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ab8b8a7e-6ecd-45ec-a77a-3dd0ffd7cfdd.png)Figure 4-6: The graph with
    the percentage of fast requests measured with instrumented metrics'
  prefs: []
  type: TYPE_NORMAL
- en: So far, using the instrumented metric `http_server_resp_time_count` did not
    provide any tangible benefit when compared with `nginx_ingress_controller_request_duration_seconds_sum`.
    If that would be all, we could conclude that the effort to add instrumentation
    was a waste. However, we did not yet include labels into our expressions.
  prefs: []
  type: TYPE_NORMAL
- en: Let's say that we'd like to group requests by the `method` and the `path`. That
    could give us a better idea whether slowness is global, or limited only to a specific
    type of requests. If it's latter, we'll know where the problem is and, hopefully,
    would be able to find the culprit quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows, and press the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: That expression is almost the same as the one before. The only difference is
    the addition of the `by (method, path)` statements. As a result, we are getting
    a percentage of fast responses, grouped by the `method` and the `path`.
  prefs: []
  type: TYPE_NORMAL
- en: The output does not represent a "real" world use case. Usually, we'd see many
    different lines, one for each method and path that was requested. But, since we
    only made requests to `/demo/hello` using HTTP GET, our graph is a bit boring.
    You'll have to imagine that there are many other lines.
  prefs: []
  type: TYPE_NORMAL
- en: By studying the graph, we discover that all but one line (we're still imagining
    many) are close to hundred percent of fast responses. The one with the sharp drop
    would be the one with the `/demo/hello` path and the `GET` method. However, if
    that would indeed be a real-world scenario, we would probably have too many lines
    in the graph, and we might not be able to distinguish them easily. Our expression
    could benefit with an addition of a threshold.
  prefs: []
  type: TYPE_NORMAL
- en: Please type the expression that follows, and press the Execute button.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The only addition is the `< 0.99` threshold. As a result, our graph excludes
    all the results (all paths and methods) but those that are below 99 percent (0.99).
    We removed all the noise and focused only on the cases when more than one percent
    of all requests are slow (or less than 99 percent are fast). The result is now
    clear. The problem is in the function that handles `GET` requests on the path
    `/demo/hello`. We know that through the labels available below the graph.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/f862b0b0-9ab7-4d89-93f9-4349c9692d69.png)Figure 4-7: The graph with
    the percentage of fast requests measured with instrumented metrics limited to
    results below ninety-nine percent'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know (almost) the exact location of the problem, all that's left
    is to fix the issue, push the change to our Git repository, and wait until our
    continuous deployment process upgrades the software with the new release.
  prefs: []
  type: TYPE_NORMAL
- en: In a relatively short period, we managed to find (debug) the issue or, to be
    more precise, to narrow it to a specific part of the code.
  prefs: []
  type: TYPE_NORMAL
- en: Or, maybe we discovered that the problem is not in the code, but that our application
    needs to scale up. In either case, without instrumented metrics, we would only
    know that the application is slow and that could mean that any part of the app
    is misbehaving. Instrumentation gave us more detailed metrics that we used to
    be more precise and reduce the time we'd typically require to find the issue and
    act accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, we'd have many other instrumented metrics, and our "debugging" process
    would be more complicated. We'd execute other expressions and dig through different
    metrics. Nevertheless, the point is that we should combine generic metrics with
    more detailed ones coming directly from our applications. Those in the former
    group are often used to detect that there is an issue, while the latter type is
    useful when looking for the cause of the problem. Both types of metrics have their
    place in monitoring, alerting, and debugging our clusters and applications. With
    instrumented metrics, we have more application-specific details. That allows us
    to narrow down the locations and causes of a problem. The more confident we are
    about the exact cause of an issue, the better we are equipped to react.
  prefs: []
  type: TYPE_NORMAL
- en: What now?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I don't believe that we need many other examples of instrumented metrics. They
    are not any different than those we are collecting through exporters. I'll leave
    it up to you to start instrumenting your applications. Start small, see what works
    well, improve and extend.
  prefs: []
  type: TYPE_NORMAL
- en: Yet another chapter is finished. Destroy your cluster and start the next one
    fresh, or keep it. If you choose the latter, please execute the commands that
    follow to remove the `go-demo-5` application.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Before you leave, remember the point that follows. It summarizes instrumentation.
  prefs: []
  type: TYPE_NORMAL
- en: Instrumented metrics are baked into applications. They are an integral part
    of the code of our apps, and they are usually exposed through the `/metrics` endpoint.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
