- en: Good Test Habits for New and Legacy Systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Something is better than nothing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coverage isn't everything
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be willing to invest in text fixtures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you aren't convinced about the value of testing, your team won't be either
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Harvesting metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Capturing a bug in an automated test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Separating algorithms from concurrency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pausing to refactor when a test suite takes too long to run
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cashing in on your confidence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be willing to throw away an entire day's changes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead of shooting for 100 percent coverage, try to have steady growth
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Randomly breaking up your application can lead to better code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I hope you have enjoyed the previous chapters of this book. Up to this point,
    we have explored a lot of areas of automated testing, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Unit testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nose testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doctest testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Behavior-driven development
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Acceptance testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous integration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Smoke and load testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will do something different. Instead of providing lots of
    code samples for various tips and tricks, I want to share some ideas I have picked
    up in my career as a software engineer.
  prefs: []
  type: TYPE_NORMAL
- en: All of the previous recipes in this book had very detailed steps on how to write
    the code, run it, and review its results. Hopefully, you have taken those ideas,
    expanded on them, improvised them, and, ultimately, applied them to help solve
    your own software problems.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, let's explore some of the bigger ideas behind testing and how
    they can empower our development of quality systems.
  prefs: []
  type: TYPE_NORMAL
- en: Something is better than nothing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Don't get caught up in the purity of total isolation or worry about obscure
    test methods. The first thing to do is to start testing.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You have just been handed an application that was developed by others who are
    no longer with your company. Been there before? We all have, and probably on several
    occasions. Can we predict some of the common symptoms? Well, they could be similar
    to these:'
  prefs: []
  type: TYPE_NORMAL
- en: There are few (if any) automated tests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is little documentation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are chunks of code that have been commented out.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are either no comments in the code, or there are comments that were written
    ages ago and are no longer correct.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And here is the fun part—we don't know about all of these issues up front. We
    are basically told where to check the source tree, and to get cracking. For example,
    it's only when we run into an issue and seek  documentation that we discover what
    does (or does not) exist.
  prefs: []
  type: TYPE_NORMAL
- en: Maybe I didn't catch everything you have encountered in that list, but I bet
    you've experienced a lot of those things. I don't want to sound like an embittered
    software developer, because I'm not. Not every project is like this. But I'm sure
    we have all had to deal with this at one time or another. So, what do we do? We
    start testing.
  prefs: []
  type: TYPE_NORMAL
- en: But the devil is in the details. Do we write a unit test? What about a thread
    test or an integration test? You know what? It doesn't matter what type of test
    we write. In fact, it doesn't even matter whether we use the right name.
  prefs: []
  type: TYPE_NORMAL
- en: When it's just you and the code sitting in a cubicle, terminology doesn't matter.
    Writing a test is what matters. If you can pick out one small unit of code and
    write a test, then go for it! But what if you picked up a jumbled piece of spaghetti
    code that doesn't come with nicely isolated units?
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a system where the smallest unit you can get hold of is a module that
    parses an electronic file and then stores the parsed results in a database. The
    parsed results aren''t handed back through the API. They just silently, mysteriously
    end up in the database. How do we automate that? Well, we can do the following
    things:'
  prefs: []
  type: TYPE_NORMAL
- en: Write a test that starts by emptying all the tables relevant to the application.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find one of your users who has one of these files and get a copy of it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add code to the test that invokes the top-level API to ingest the file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add some more code that pulls data out of the database and checks the results.
    (You may have to grab that user to make sure the code is working correctly.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Congratulations! You just wrote an automated test! It probably didn't qualify
    as a unit test. In fact, it may look kind of ugly to you, but so what? Maybe it
    took five minutes to run, but isn't that better than no test at all?
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since the database is the place where we can assert results, we need to have
    a cleaned-out version before every run of our test. This will definitely require
    coordination if other developers are using some of the same tables. We may need
    our own schema allocated to us so that we can empty tables at will.
  prefs: []
  type: TYPE_NORMAL
- en: The modules probably suffer from a lack of cohesion and too much tight coupling.
    While we can try to identify why the code is bad, it doesn't advance our cause
    of building automated tests.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, we must recognize that if we try to jump immediately into the unit
    level test, we will have to refactor the modules to support us. With little or
    no safety net, the risk is incredibly high, and we can feel it! If we tried to
    stick to a textbook unit test, then we would probably give up and consider automated
    testing an impossibility.
  prefs: []
  type: TYPE_NORMAL
- en: So, we have to take the first step and write an expensive, end-to-end, automated
    test to build the first link of the chain. That test may take a long time to run
    and may not be very comprehensive in what we can assert, but it's a start, and
    that is what's important. Hopefully, after making steady progress writing more
    tests like this, we will build up a safety net that will prevent us from having
    to go back and refactor this code.
  prefs: []
  type: TYPE_NORMAL
- en: That can't be everything!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Does *just write the test* sound a little too simple? Well, the concept is simple,
    but the work is going to be hard—very hard.
  prefs: []
  type: TYPE_NORMAL
- en: You will be forced to crawl through lots of APIs and find out exactly how they
    work. And, guess what? You probably won't be handed lots of intermediate results
    to assert. Understanding the API is just so that you can track down where the
    data travels to.
  prefs: []
  type: TYPE_NORMAL
- en: When I described the data of our situation as *mysteriously ending up in the
    database*, I was referring to the likelihood that the APIs you have probably weren't
    designed with lots of return values aimed at testability.
  prefs: []
  type: TYPE_NORMAL
- en: Just don't let anyone tell you that you are wasting your time building a long-running
    test case. An automated test suite that takes an hour to run and is exercised
    at least once a day probably instills more confidence than clicking through the
    screens manually. Something is better than nothing.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *Cash in on your confidence* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coverage isn't everything
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You've figured out how to run coverage reports. However, don't assume that more
    coverage is automatically better. Sacrificing test quality in the name of coverage
    is a recipe for failure.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Coverage reports** provide good feedback. They tell us what is getting exercised
    and what is not. But, just because a line of code is exercised, that doesn''t
    mean it is doing everything it is meant to do.'
  prefs: []
  type: TYPE_NORMAL
- en: Are you ever tempted to brag about coverage percentage scores in the break room?
    Taking pride in good coverage isn't unwarranted, but when it leads to comparing
    different projects using these statistics, we are wandering into risky territory.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Coverage reports are meant to be read in the context of the code they were run
    against. The reports show us what was covered and what was not, but this isn't
    where things stop. Instead, it's where they begin. We need to look at what was
    covered and analyze how well the tests exercised the system.
  prefs: []
  type: TYPE_NORMAL
- en: It's obvious that 0% coverage of a module indicates we have work to do. But
    what does it mean when we have 70% coverage? Do we need to code tests that go
    after the other 30%? Sure we do! But there are two different schools of thought
    on how to approach this. One is right, and one is wrong.
  prefs: []
  type: TYPE_NORMAL
- en: The first approach is to write the new tests specifically targeting the uncovered
    parts while trying to avoid overlapping the original 70%. Redundantly testing
    code already covered in another test is an inefficient use of resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second approach is to write the new tests so that they target scenarios
    the code is expected to handle, but that we haven't tackled yet. What was not
    covered should give us a hint about which scenarios haven't been tested yet.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The right approach is the second one. OK, I admit I wrote that in a leading
    fashion. But the point is that it's very easy to look at what wasn't hit and write
    a test that aims to close the gap as fast as possible.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Python gives us incredible power to monkey patch, inject alternate methods,
    and do other tricks to exercise the uncovered code. But doesn''t this sound a
    little suspicious? Here are some of the risks we are setting ourselves up for:'
  prefs: []
  type: TYPE_NORMAL
- en: The new tests may be more brittle when they aren't based on sound scenarios.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A major change to our algorithms may require us to totally rewrite these tests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ever written mock-based tests? It's possible to mock the target system out of
    existence and end up just testing the mocks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even though some (or even most) of our tests may have good quality, the low-quality
    ones will cast our entire test suite as low quality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The coverage tool may not let us *get away* with some of these tactics if we
    do things that interfere with line counting mechanisms. But whether or not the
    coverage tool counts the code should not be the gauge by which we determine the
    quality of tests.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, we need to look at our tests and see whether they are trying to exercise
    real-use cases we should be handling. When we are merely looking for ways to get
    more coverage percentage, we stop thinking about how our code is meant to operate,
    and that is not good.
  prefs: []
  type: TYPE_NORMAL
- en: Are we not supposed to increase coverage?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are supposed to increase coverage by improving our tests, covering more scenarios,
    and removing code that's no longer supported. These things all lead us toward
    overall better quality. Increasing coverage for the sake of coverage doesn't lend
    itself to improving the quality of our system.
  prefs: []
  type: TYPE_NORMAL
- en: But I want to brag about the coverage of my system!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I think it's all right to celebrate good coverage. Sharing a coverage report
    with your manager is all right. But don't let it consume you.
  prefs: []
  type: TYPE_NORMAL
- en: If you start to post weekly coverage reports, double check your motives. The
    same goes if your manager requests postings as well.
  prefs: []
  type: TYPE_NORMAL
- en: If you find yourself comparing the coverage of your system to another system,
    then watch out! Unless you are familiar with the code of both systems, and really
    know more than the bottom line of the reports, you will probably wander into risky
    territory. You may be headed into the faulty competition that could drive your
    team to write brittle tests.
  prefs: []
  type: TYPE_NORMAL
- en: Be willing to invest in test fixtures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spend time working on some test fixtures. You may not get a lot of tests written
    at first, but this investment will pay off.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it....
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we start building a new greenfield project, it's a lot easier to write
    test-oriented modules, but when dealing with legacy systems, it may take more
    time to build a working test fixture. This may be tough to go through, but it's
    a valuable investment.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, in the section *Something is better than nothing*, we talked
    about a system that scanned electronic files and put the parsed results into database
    tables. What steps would our test fixture require? Perhaps we should consider
    the following issues:'
  prefs: []
  type: TYPE_NORMAL
- en: Set up steps to clean out the appropriate tables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quite possibly, we may need to use code or a script to create a new database
    schema to avoid collisions with other developers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It may be necessary to place the file in a certain location so the parser can
    find it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are all steps that take time to build a working test case. More complex
    legacy systems may require even more steps to gear up for a test run.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All of this can become intimidating and may push us to drop automated testing
    and just continue with clicking through the screens to verify things. But taking
    the time to invest in coding this fixture will begin to pay off as we write more
    test cases that use our fixture.
  prefs: []
  type: TYPE_NORMAL
- en: Have you ever built a test fixture and had to alter it for certain scenarios?
    After having developed enough test cases using our fixture, we will probably encounter
    another use case we need to test that exceeds the limits of our fixture. Since
    we are now familiar with it, it is probably easier to create another fixture.
  prefs: []
  type: TYPE_NORMAL
- en: This is another way that coding the first fixture pays off. Future fixtures
    have a good chance of being easier to code. However, this isn't a cut-and-dried
    guarantee of improvement. Often, the first variation of our test fixture is a
    simple one.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will probably run into a situation where we need another test fixture that
    is totally different than what we've built. At this point, investing in the first
    test fixture doesn't have the same payoff. But, by this time, we will have become
    more seasoned test writers and have a better handle on what works and what doesn't
    when it comes to testing the system.
  prefs: []
  type: TYPE_NORMAL
- en: All the work done up to this point will have sharpened our skill set and that,
    in and of itself, is a great payoff for investing in the test fixture.
  prefs: []
  type: TYPE_NORMAL
- en: Is this just about setting up a database?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is not just about setting up a database. If our system interacts extensively
    with an LDAP server, we may need to code a fixture that cleans out the directory
    structure and loads it up with test data.
  prefs: []
  type: TYPE_NORMAL
- en: If the legacy system is flexible enough, we can put this whole test structure
    into a sub node in the hierarchy. But it's just as likely that it expects the
    data to exist at a certain location. In that situation, we may have to develop
    a script that spins up a separate, empty LDAP server, and then shuts it down after
    the test is complete.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up and tearing down an LDAP server may not be the fastest, nor the most
    efficient test fixture. But if we invest time into building this fixture to empower
    ourselves to write automated tests, we will eventually be able to refactor the
    original system to decouple it from a live LDAP server. And this whole process
    will sharpen our skill set. That is why creating the original test fixture truly
    is an investment.
  prefs: []
  type: TYPE_NORMAL
- en: If you aren't convinced about the value of testing, your team won't be either
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Test-bitten developers exhibit zeal; they are excited to run their test suite
    and see things
  prefs: []
  type: TYPE_NORMAL
- en: complete with 100% success. This sort of emotion and pride tends to rub off
    on their fellow developers.
  prefs: []
  type: TYPE_NORMAL
- en: But the reverse is also true. If you aren't excited by all this and don't spread
    the word, none of your teammates will either. The idea of adding automated tests
    to your system will die a sad death.
  prefs: []
  type: TYPE_NORMAL
- en: This isn't just confined to my own personal experience. At the 2010 DevLink
    conference, I attended an open-space discussion about testing, and saw this sort
    of reaction among a dozen other developers I don't work with (`pythontestingcookbook.posterous.com`/
  prefs: []
  type: TYPE_NORMAL
- en: '`greetings-programs`). The testers showed a certain type of excitement as they
    relayed'
  prefs: []
  type: TYPE_NORMAL
- en: their experiences with testing. The ones that were on the fence about embracing
    automated testing were listening with glee, drinking it in. Those not interested
    simply weren't there for the discussion.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are reading this book (which of course you are), there is a fair chance
    you are the only person on your team seriously interested in automated testing.
    Your teammates may have heard of it, but aren''t as bitten by the idea as you.
    To add it to your system will require a lot of investment by you, but don''t confine
    yourself to just sharing the code; consider the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Demonstrate the excitement you feel as you make progress, and tackle thorny
    issues.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Share your test results by posting them on your walls, where others can see
    them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Talk about your accomplishments while chatting with co-workers in the break
    room.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing isn't a cold, mechanical process; it's an exciting, fiery area of development.
    Test-bitten developers can't wait to share it with others. If you look for ways
    to spread the fire of automated testing, eventually others will warm up to it,
    and you will find yourself talking about new testing techniques with them.
  prefs: []
  type: TYPE_NORMAL
- en: Harvesting metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a spreadsheet that shows lines, code, a number of tests, the total test
    execution time, and the number of bugs, and track this with every release. The
    numbers will defend your investment.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'These high-level steps show how to capture metrics over time:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a spreadsheet to track the number of test cases, the time taken to run
    the test suite, the date of the test run, any bugs, and the average time per test.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check the spreadsheet into your code base as another controlled artifact.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add some graphs to show the curve of the test time versus the test quantity.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a new row of data at least each time you do a release. If you can capture
    data more often, such as once a week or even once a day, that is better.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you write more tests, the test suite will take longer to run. But you will
    also find that the number of bugs tends to decrease. The more testing you do,
    and the more often you do it, the better your code will be. Capturing the metrics
    of your testing can act as hard evidence that the time spent writing and running
    tests is a well-placed investment.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why do I need this document? Don't I already know that testing works? Think
    of it as a backup for your assertion of quality. Months down the road, you may
    be challenged by management to speed things up. Maybe they need something faster,
    and they think you are simply spending too much time on this *testing stuff*.
  prefs: []
  type: TYPE_NORMAL
- en: If you can pull out your spreadsheet and show how bugs decreased with testing
    effort, they will have little to argue with. But if you don't have this, and simply
    argue that *testing makes things better*, you may lose the argument.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics aren't just for defending yourself to management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I personally enjoyed seeing the tests grow and the bugs decline. It was a personal
    way to track myself and keep a handle on how much progress was made. And, to be
    honest, my last manager gave me full support for automated testing. He had his
    own metrics of success, so I never had to pull out mine.
  prefs: []
  type: TYPE_NORMAL
- en: Capturing a bug in an automated test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before you fix that one-line bug you spotted, write an automated test instead,
    and make sure it's repeatable. This helps to build up insulation from our system,
    regressing back into failures we fixed in the past.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These high-level steps capture the workflow of capturing bugs in automated tests
    before
  prefs: []
  type: TYPE_NORMAL
- en: 'we fix them:'
  prefs: []
  type: TYPE_NORMAL
- en: When a new bug is discovered, write a test case that recreates it. It doesn't
    matter if the test case is long-running, complex, or integrates with lots of components.
    The critical thing is to reproduce the bug.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add the bug to your suite of tests.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fix the bug.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Verify that the test suite passes before checking your changes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The simplest way to introduce automated testing to an application that never
    had it before is to test one bug at a time. This method ensures that newly discovered
    bugs won't sneak back into the system later on.
  prefs: []
  type: TYPE_NORMAL
- en: The tests may have a loose-knit feel instead of a comprehensive one, but that
    doesn't matter. What does matter is that, over time, you will slowly develop a
    solid safety net of test cases that verify that the system performs as expected.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I didn't say this would be easy. Writing an automated test for software that
    wasn't built with testability in mind is hard work. As mentioned in the recipe
    *Something is better than nothing*, the first test case is probably the hardest.
    But, over time, as you develop more tests, you will gain the confidence to go
    back and refactor things. You will definitely feel empowered by knowing that you
    can't break things without realizing.
  prefs: []
  type: TYPE_NORMAL
- en: When the time comes to add a completely new module, you will be ready for it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This approach of capturing a bug with a test case is useful, but slow. But that's
    OK, because slowly adding testing will give you time to grow your testing skills
    at a comfortable pace.
  prefs: []
  type: TYPE_NORMAL
- en: 'Where does this pay off? Well, eventually, you will need to add a new module
    to your system. Doesn''t this always happen? By that time, your investment in
    testing and test fixtures should already be paying dividends in the improvement
    of the quality of existing code, but you will also have a head start on testing
    the new module. Consider the following:'
  prefs: []
  type: TYPE_NORMAL
- en: You will not just know, but *really understand*, the meaning of *test-oriented
    code.*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will be able to write both the code and its tests at the same time in a
    very effective way.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The new module will have a head start of higher quality and will not require
    as much effort to *catch up* as the legacy parts of your system did.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Don't give into the temptation to skip testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As I stated earlier, the first test case will be very hard to write. And the
    next few after that won't be much easier. This makes it very tempting to throw
    up your hands and skip automated testing. But, if you stick with it and write
    something that works, you can continue building on that successful bit of effort.
  prefs: []
  type: TYPE_NORMAL
- en: This may sound like a cliché, but if you stick with it for about a month, you
    will start to see some results from your work. This is also a great time to start
    **harvesting metrics**. Capturing your progress and being able to reflect on it
    can provide positive encouragement.
  prefs: []
  type: TYPE_NORMAL
- en: Separating algorithms from concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Concurrency is very hard to test, but most algorithms are not when decoupled.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Herb Sutter wrote an article in 2005 entitled *The Free Lunch Is Over*, where
    he pointed out how microprocessors are approaching a physical limitation in serial
    processing, which will be forcing developers to turn towards concurrent solutions
    ([http://www.gotw.ca/publications/concurrency-ddj.htm](http://www.gotw.ca/publications/concurrency-ddj.htm)).
  prefs: []
  type: TYPE_NORMAL
- en: Newer processors come with multiple cores. To build scalable applications, we
    can no longer just wait for a faster chip. Instead, we must use alternate, concurrent
    techniques. This issue is being played out in a whole host of languages. Erlang
    was one of the first languages on the scene that allowed a telecommunications
    system to be built with nine 9's of availability, which means about one second
    of downtime every 30 years.
  prefs: []
  type: TYPE_NORMAL
- en: One of its key features is the use of immutable data sent between actors. This
    provides nice isolation and allows multiple units to run across the CPU cores.
    Python has libraries that provide a similar style of decoupled, asynchronous message
    passing. The two most common ones are Twisted and Kamaelia.
  prefs: []
  type: TYPE_NORMAL
- en: 'But, before you dive into using either of these frameworks, there is something
    important to keep in mind: it''s very hard to test concurrency while also testing
    algorithms. To use these libraries, you will register code that issues messages
    and also registers handlers to process messages.'
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It's important to decouple the algorithms from the machinery of whatever concurrency library
    you pick. This will make it much easier to test the algorithms, but it doesn't
    mean that you shouldn't conduct load tests or try to overload your system with
    live data playback scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: What it does mean is that starting with large volume test scenarios is the wrong
    priority. Your system needs to correctly handle one event in an automated test
    case before it can handle a thousand events.
  prefs: []
  type: TYPE_NORMAL
- en: Research test options provided by your concurrency frameworks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A good concurrency library should provide sound testing options. Seek them out
    and try to use them to their fullest. But don't forget to verify that your custom
    algorithms work in simple, serial fashion as well. Testing both sides will give
    you great confidence that the system is performing as expected under light and
    heavy loads.
  prefs: []
  type: TYPE_NORMAL
- en: Pause to refactor when a test suite takes too long to run
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you start to build a test suite, you may notice the runtime getting quite
    long. If it's so long that you aren't willing to run it at least once a day, you
    need to stop coding and focus on speeding up the tests, whether it involves the
    tests themselves or the code being tested.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This assumes you have started to build a test suite using some of the following
    practices:'
  prefs: []
  type: TYPE_NORMAL
- en: Something is better than nothing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be willing to invest in test fixtures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Capturing a bug in an automated test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are slow-starting steps to start adding tests to a system that was originally
    built without any automated testing. One of the trade-offs to get moving on automated
    testing involves writing relatively expensive tests. For instance, if one of your
    key algorithms is not adequately decoupled from the database, you will be forced
    to write a test case that involves setting up some tables, processing the input
    data, and then making queries against the state of the database afterward.
  prefs: []
  type: TYPE_NORMAL
- en: As you write more tests, the time to run the test suite will certainly grow.
    At some point, you will feel less inclined to spend the time waiting for your
    test suite to run. Since a test suite is only good when used, you must pause development
    and pursue refactoring either the code or the test cases themselves to speed things
    up.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a problem I ran into: my test suite initially took about 15 minutes
    to run. It eventually grew to take one-and-a-half hours to run all the tests.
    I reached a point where I would only run it once a day, and even skipped some
    days. One day, I tried to do a massive code edit. When most of the test cases
    failed, I realized that I had not run the test suite often enough to detect which
    step broke things. I was forced to throw away all the code edits and start over.
    Before proceeding further, I spent a few days refactoring the code as well as
    the tests, bringing the run time of the test suite back down to a tolerable 30
    minutes.'
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'That is the key measurement: when you feel hesitant to run the test suite more
    than *once **a day*, this may be a sign that things need to be cleaned up. Test
    suites are meant to be run multiple times a day.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is because we have competing interests: *writing code* and *running tests*.
    It''s important to recognize these things:'
  prefs: []
  type: TYPE_NORMAL
- en: To run tests, we must suspend our coding efforts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To write more code, we must suspend testing efforts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When testing takes a big chunk of our daily schedule, we must start choosing
    which is more important. We tend to migrate toward writing more code, and this
    is probably the key reason people abandon automated testing and consider it unsuitable
    for their situation.
  prefs: []
  type: TYPE_NORMAL
- en: It's tough, but if we can resist taking the easy way out, and instead do some
    refactoring of either the code or our tests, we will be encouraged to run the
    tests more often.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It''s less science and more voodoo when it comes to what to refactor. It''s
    important to seek out opportunities that give us a good yield. It''s important
    to understand that this can be either our test code, our production code, or a
    combination of both that needs to be refactored. Consider the following points:'
  prefs: []
  type: TYPE_NORMAL
- en: Performance analysis can show us where the hotspots are. Refactoring or rewriting
    these chunks can improve tests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tight coupling often forces us to put in more parts of the system than we want,
    such as database usage. If we can look for ways to decouple the code from the
    database and replace it with mocks or stubs, that sets us up to update the relevant
    tests to come up with a faster running test suite.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coverage obtained from tests can help. All of these approaches have positive
    consequences for our code's quality. More efficient algorithms lead to better
    performance, and looser coupling helps to keep our long-term maintenance costs
    down.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Be willing to throw away an entire day''s changes*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cash in on your confidence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After building up enough tests, you will feel confident enough to rewrite a
    big chunk of code or conduct shotgun surgery that touches almost every file. Go
    for it!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you build more tests and run them several times a day, you will start to
    get a feel for what you know and don't know about the system. Even more so, when
    you've written enough expensive, long-running tests about a particular part of
    the system, you will feel a strong desire to rewrite that module.
  prefs: []
  type: TYPE_NORMAL
- en: What are you waiting for? This is the point of building a runnable safety net
    of tests. Understanding the ins and outs of a module gives you the knowledge to
    attack it. You may rewrite it, be able to better decouple its parts, or whatever
    else is needed to make it work better, as well as being able to better support
    tests.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While you may feel a strong desire to attack the code, there may be an equal
    and opposing feeling to resist making such changes. This is risk aversion, and
    we all have to deal with it. We want to avoid diving in head first to a situation
    that could have drastic consequences.
  prefs: []
  type: TYPE_NORMAL
- en: Assuming we have built an adequate safety net, it's time to engage the code
    and start cleaning it up. If we run the test suite frequently while making these
    changes, we can safely move through the changes we need to make. This will improve
    the quality of the code and will possibly speed up the runtime of the test suite.
  prefs: []
  type: TYPE_NORMAL
- en: '**While making changes, we don''t have to go all in**'
  prefs: []
  type: TYPE_NORMAL
- en: Cashing in on our confidence means we move in and make changes to the code base,
    but it doesn't mean we go into areas of code where the tests are shallow and inadequate.
    There may be several areas we want to clean up, but we should only go after the
    parts we are most confident about. There will be future opportunities to get the
    other parts as we add more tests in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Be willing to throw away an entire day's changes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Have you ever worked for a whole day making changes, only to discover that half
    the tests failed because you forgot to run the test suite more often? Be ready
    to throw away the changes. This is what automated testing lets us do… back up
    to when everything ran perfectly. It will hurt, but next time you will remember
    to run the test suite more often.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe assumes you are using version control and are making regular commits.
    This idea is no good if you haven't made a commit for two weeks.
  prefs: []
  type: TYPE_NORMAL
- en: If you run your test suite at least once a day, and when it passes, you commit
    the changes you have made, then it becomes easy to back up to some previous point,
    such as the beginning of the day.
  prefs: []
  type: TYPE_NORMAL
- en: I have done this many times. The first time was the hardest. It was a new idea
    to me, but I realized the real value of software was now resting on my automated
    test suite. In the middle of the afternoon, I ran the test suite for the first
    time that day after having edited half the system. Over half of the tests failed.
  prefs: []
  type: TYPE_NORMAL
- en: I tried to dig in and fix the issue. The trouble was, I couldn't figure out
    where the issue stemmed from. I spent a couple of hours trying to track it down.
    It began to dawn on me that I wasn't going to figure it out without wasting loads
    of time.
  prefs: []
  type: TYPE_NORMAL
- en: But I remembered that everything had passed with flying colors the previous
    day. I finally decided to throw away my changes, run the test suite, verifying
    everything passed, and then grudgingly go home for the day.
  prefs: []
  type: TYPE_NORMAL
- en: The next day, I attacked the problem again. Only, this time, I ran the tests
    more often. I was able to get it coded successfully. Looking back at the situation,
    I realize that this issue only cost me one lost day. If I had tried to ride it
    out, I could have spent a week and *still* probably ended up throwing things away.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Depending on how your organization manages source control, you may have to
    do the following things:'
  prefs: []
  type: TYPE_NORMAL
- en: Simply do it yourself by deleting a branch or canceling your checkouts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contact your CM team to delete the branch or the commits you made for the day
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This isn't really a technical issue. The source control system makes it easy
    to do this regardless of who is in charge of branch management. The hard part
    is making the decision to throw away the changes. We often feel the desire to
    fix what is broken. The more our efforts cause it to break further, the more we
    want to fix it. At some point, we must realize that it is more costly to move
    forward rather than to back up and start again.
  prefs: []
  type: TYPE_NORMAL
- en: There is an axis of agility that stretches from classic waterfall software production
    to heavily agile processes. Agile teams tend to work in smaller sprints and commit
    in smaller chunks. This makes it more palatable to throw away a day of work. The
    bigger the task and the longer the release cycle, the greater the odds are that
    your changes haven't been checked since you started a task two weeks ago.
  prefs: []
  type: TYPE_NORMAL
- en: Believe me, throwing away two weeks' work is totally different than throwing
    away one day's worth. I would never advocate throwing out two weeks' work.
  prefs: []
  type: TYPE_NORMAL
- en: The core idea is to *not* go home without your test suite passing. If that means
    you have to throw things away to make it happen, then that is what you must do.
    It really drives the point home of *code a little/test a little* until a new feature
    is ready for release.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We also need to reflect on why didn't we run the test suite often enough. It
    may be because the test suite is taking too long to run, and you are hesitating
    to use up that time. It may be time to *pause to refactor when the test suite
    takes too long to run*. The time I really learned this lesson was when my test
    suite took one-and-a-half hours to run. After I got through this whole issue,
    I realized that I needed to speed things up and spent probably a week or two cutting
    it down to a tolerable 30 minutes.
  prefs: []
  type: TYPE_NORMAL
- en: How does this mesh with "Something is better than nothing"
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Earlier in this chapter, we talked about writing a test case that may be quite
    expensive to run to get automated testing in action. What if our testing becomes
    so expensive that it is time prohibitive? After all, couldn't what we just said
    lead to the situation we are dealing with?
  prefs: []
  type: TYPE_NORMAL
- en: '*Code a little/test a little* may seem to be a very slow way to proceed. This
    is probably the reason many legacy systems never embrace automated testing. The
    hill we must climb is steep. But if we can hang in there, start building the tests,
    make sure they run at the end of the day, and then eventually pause to refactor
    our code and tests, we can eventually reach a happy balance of better code quality
    and system confidence.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Something is better than nothing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pause to refactor when a test suite takes too long
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead of shooting for 100 percent coverage, try to have a steady growth
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You won't know how you're doing without coverage analysis. However, don't aim
    too high. Instead, focus on a gradual increase. You will find that your code gets
    better over time—maybe even drops in volume—while the quality and coverage steadily
    improve.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you start with a system that has no tests, don't get focused on a ridiculously
    high number. I worked on a system that had 16% coverage when I picked it up. A
    year later, I had worked it up to 65%. This was nowhere near 100%, but the quality
    of the system had grown in leaps and bounds due to capturing a bug in an automated
    test and harvesting metrics.
  prefs: []
  type: TYPE_NORMAL
- en: At one time, I was discussing the quality of my code with my manager, and he
    showed me a report he had developed. He had run a code-counting tool on every
    release of every application he was overseeing. He said my code counts had a unique
    shape. All the
  prefs: []
  type: TYPE_NORMAL
- en: other tools had a constant increase in lines of code. Mine had grown, peaked,
    and then started to decrease and were still on the decline.
  prefs: []
  type: TYPE_NORMAL
- en: This happened despite the fact that my software did more than ever. It's because
    I started throwing away unused features, bad code, and clearing out cruft during
    refactorings.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By slowly building an automated test suite, you will gradually cover more of
    your code. By keeping a focus on building quality code with corresponding tests,
    the coverage will grow naturally. When we shift to focusing on coverage reports,
    the numbers may grow more quickly, but it will tend to be more artificial.
  prefs: []
  type: TYPE_NORMAL
- en: From time to time, as you cash in on your confidence and rewrite chunks, you
    should feel empowered to throw away old junk. This will also grow your coverage
    metrics in a healthy way.
  prefs: []
  type: TYPE_NORMAL
- en: All of these factors will lead to increased quality and efficiency. While your
    code may eventually peak and then decrease, it isn't unrealistic for it to eventually
    grow again due to new features.
  prefs: []
  type: TYPE_NORMAL
- en: By that time, the coverage will probably be much higher, because you will be
    building completely new features, hand in hand with tests, instead of just maintaining
    legacy parts.
  prefs: []
  type: TYPE_NORMAL
- en: Randomly breaking up your app can lead to better code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"*The best way to avoid failure is to fail constantly.*"– Netflix'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Netflix has built a tool they call **Chaos Monkey**. Its job is to randomly
    kill instances and services. This forces developers to make sure their system
    can fail smoothly and safely. To build our own version of this, some of the things
    we would need it to do are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Randomly kill processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inject faulty data at interface points
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shut down network interfaces between distributed systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Issue shutdown commands to subsystems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create denial-of-service attacks by overloading interface points with too much
    data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is a starting point. The idea is to inject errors wherever you can imagine
    them happening. This may require writing scripts, Cron jobs, or any means necessary
    to cause these errors to happen.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given that there is a chance of a remote system being unavailable in production,
    we should introduce ways for this to happen in our development environment. This
    will encourage us to code higher fault tolerance into our system.
  prefs: []
  type: TYPE_NORMAL
- en: Before we introduce a random-running Chaos Monkey such as Netflix has, we need
    to ensure that our system can handle these situations manually. For example, if
    our system includes communication between two servers, a fair test is unplugging
    the network cable to one box, simulating network failure. When we verify that
    our system can continue working with acceptable means, then we can add scripts
    to do this automatically and, eventually, randomly.
  prefs: []
  type: TYPE_NORMAL
- en: Audit logs are valuable tools to verify that our system is handling these random
    events. If we can read a log entry showing a forced network shutdown and then
    see log entries of similar timestamps, we can easily evaluate whether the system
    handled the situation.
  prefs: []
  type: TYPE_NORMAL
- en: After building that in, we can work on the next error to randomly introduce
    into the system. By following this cycle, we can build up the robustness of our
    system.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This doesn't exactly fit into the realm of automated testing. This is also very
    high level. It's hard to go into much more detail, because the type of faulty
    data to inject requires an intimate understanding of the actual system.
  prefs: []
  type: TYPE_NORMAL
- en: How does this compare to fuzz testing?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Fuzz testing** is a style of testing where invalid, unexpected, and random
    data is injected into input points of our software ([http://en.wikipedia.org/wiki/Fuzz_testing](http://en.wikipedia.org/wiki/Fuzz_testing)).
    If the application fails, this is considered a failure. If it doesn''t, then it
    has passed. This type of testing goes in a similar direction, but the blog article
    written by Netflix appears to go much further than simply injecting different
    data. It talks about killing instances and interrupting distributed communications.
    Basically, anything you can think of that could happen in production, we should
    try to replicate in a test bed. **Fusil** ([https://bitbucket.org/haypo/fusil](https://bitbucket.org/haypo/fusil))
    is a Python tool that aims to provide fuzz testing. You may want to investigate
    whether it is useful for your project needs.'
  prefs: []
  type: TYPE_NORMAL
- en: Are there any tools to help with this?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Jester** (for Java), **Pester** (for Python), and **Nester** (for C#) are
    used to conduct mutation testing ([http://jester.sourceforge.net/](http://jester.sourceforge.net/)).
    These tools find out what code is not covered by test cases, alter the source
    code, and rerun the test suites. Finally, they give a report on what was changed,
    what passed, and what didn''t pass. It can illuminate what is and is not covered
    by our test suites in ways coverage tools can''t.'
  prefs: []
  type: TYPE_NORMAL
- en: This isn't a complete Chaos Monkey, but it provides one area of assistance by
    trying to *break the system* and force us to improve our test regime. To really
    build a full-blown system probably wouldn't fit inside a test project, because
    it requires writing custom scripts based on the environment it's meant to run
    in.
  prefs: []
  type: TYPE_NORMAL
