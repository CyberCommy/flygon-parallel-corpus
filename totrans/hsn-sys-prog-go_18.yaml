- en: Implementing Concurrency Patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will be about concurrency patterns and how to use them to build
    robust system applications. We have already looked at all the tools that are involved
    in concurrency (goroutines and channels, `sync` and `atomic`, and context) so
    now we will look at some common ways of combining them in patterns so that we
    can use them in our programs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Beginning with generators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequencing with pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Muxing and demuxing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resource leaking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter requires Go to be installed and your favorite editor to be set
    up. For more information, please refer to [Chapter 3](602a92d5-25f7-46b8-83d4-10c6af1c6750.xhtml),
    *An Overview of Go*.
  prefs: []
  type: TYPE_NORMAL
- en: Beginning with generators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A generator is a function that returns the next value of a sequence each time
    it is called. The biggest advantage of using generators is the lazy creation of
    new values of the sequence. In Go, this can either be expressed with an interface
    or with a channel. One of the upsides of generators when they're used with channels
    is that they produce values concurrently, in a separate goroutine, leaving the
    main goroutine capable of executing other kinds of operations on them.
  prefs: []
  type: TYPE_NORMAL
- en: 'It can be abstracted with a very simple interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The return type of the interface will depend on the use case, which is `int64`
    in our case. The basic implementation of it could be something such as a simple
    counter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This implementation is not thread-safe, so if we try to use it with goroutines,
    we could lose some elements on the way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: A simple way to make a generator concurrent is to execute atomic operations
    on the integer.
  prefs: []
  type: TYPE_NORMAL
- en: 'This will make the concurrent generator thread-safe, with very few changes
    to the code needing to happen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This will avoid race conditions in the application. However, there is another
    implementation that''s possible, but this requires the use of channels. The idea
    is to produce the value in a goroutine and then pass it in a shared channel to
    the next method, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The loop will go on forever, and will be blocking in the send operation when
    the generator user stops requesting new values with the `Next` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code was structured this way because we were trying to implement the interface
    we defined at the beginning. We could also just return a channel and use it for
    receiving:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The main advantage of using the channel directly is the possibility of including
    it in `select` statements in order to choose between different channel operations.
    The following shows a `select` between two different generators:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Avoiding leaks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is a good idea to allow the loop to end in order to avoid goroutine and
    resource leakage. Some of these issues are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: When a goroutine hangs without returning, the space in memory remains used,
    contributing to the application's size in memory. The goroutine and the variables
    it defines in the stack will get collected by the GC only when the goroutine returns
    or panics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a file remains open, this can prevent other processes from executing operations
    on it. If the number of files that are open reaches the limit imposed by the OS,
    the process will not be able to open other files (or accept network connections).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An easy solution to this problem is to always use `context.Context` so that
    you have a well-defined exit point for the goroutine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This can be used to generate values until there is a need for them and cancel
    the context when there''s no need for new values. The same pattern can be applied
    to a version that returns a channel. For instance, we could use the `cancel` function
    directly or set a timeout on the context:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The generator will produce numbers until the context that's provided expires.
    At this point, the generator will close the channel.
  prefs: []
  type: TYPE_NORMAL
- en: Sequencing with pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A pipeline is a way of structuring the application flow, and is obtained by
    splitting the main execution into stages that can talk with one another using certain
    means of communication. This could be either of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: External, such as a network connection or a file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Internal to the application, like Go's channels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first stage is often referred to as the producer, while the last one is
    often called the consumer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The set of concurrency tools that Go offers allows us to efficiently use multiple
    CPUs and optimize their usage by blocking input or output operations. Channels
    in particular are the perfect tools for internal pipeline communication. They
    can be represented by functions that receive an inbound channel and return an
    outbound one. The base structure would look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We create a channel of the same type as the input channel and return it. In
    a separate goroutine, we receive data from the input channel, perform an operation
    on the data, and we send it to the output channel.
  prefs: []
  type: TYPE_NORMAL
- en: 'This pattern can be further improved by the use of `context.Context` so that
    we have greater control of the application flow. It would look something like
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'There are a couple of general rules that we should follow when designing pipelines:'
  prefs: []
  type: TYPE_NORMAL
- en: Intermediate stages will receive an inbound channel and return another one.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The producer will not receive any channel, but return one.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The consumer will receive a channel without returning one.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each stage will close the channel on creation when it's done sending messages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each stage should keep receiving from the input channel until it's closed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s create a simple pipeline that filters lines from a reader using a certain
    string and prints the filtered lines, highlighting the search string. We can start
    with the first stage – the source – which will not receive any channel in its
    signature but will use a reader to scan lines. We take a context in for reacting
    to an early exit request (context cancellation) and a `bufio` scanner to read
    line by line. The following code shows this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We can split the remaining operations into two phases: a filtering phase and
    a writing phase. The filtering phase will simply filter from a source channel
    to an output channel. We are still passing context in order to avoid sending extra
    data if the context is already complete. This is the text filter implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we have the final stage, the consumer, which will print the output
    in a writer and will also use a context for early exit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The use of this function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: With this approach, we learned how to split a complex operation into a simple
    task that's executed by stages and connected using channels.
  prefs: []
  type: TYPE_NORMAL
- en: Muxing and demuxing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we are familiar with pipelines and stages, we can introduce two new
    concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Muxing (multiplexing) or fan-out**: Receiving from one channel and sending
    to multiple channels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Demuxing (demultiplexing) or fan-in**: Receiving from multiple channels and
    sending through one channel'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This pattern is very common and allows us to use the power of concurrency in
    different ways. The most obvious way is to distribute data from a channel that
    is quicker than its following step, and create more than one instance of such
    steps to make up for the difference in speed.
  prefs: []
  type: TYPE_NORMAL
- en: Fan-out
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The implementation of multiplexing is pretty straightforward. The same channel
    needs to be passed to different stages so that each one will be reading from it.
  prefs: []
  type: TYPE_NORMAL
- en: Each goroutine is competing for resources during the runtime schedule, so if
    we want to reserve more of them, we can use more than one goroutine for a certain
    stage of the pipeline, or for a certain operation in our application.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create a small application that counts the occurrence of words which
    appear in a piece of text using such an approach. Let''s create an initial producer
    stage that reads from a writer and returns a slice of words for that line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now define another stage that will count the occurrence of these words.
    We will use this stage for the fan-out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to use the first stage as a source for more than one instance of the
    second stage, we just need to create more than one counting stage with the same
    input channel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Fan-in
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Demuxing is a little bit more complicated because we don''t need to receive
    data blindly in one goroutine or another – we actually need to synchronize a series
    of channels. A good approach to avoid race conditions is to create another channel
    where all the data from the various input channels will be received. We also need
    to make sure that this merge channel gets closed once all the channels are done.
    We also have to keep in mind that the channel will be closed if the context is
    cancelled. We are using `sync.Waitgroup` here to wait for all the channels to
    finish:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The problem is that we have two possible triggers for closing the channel:
    regular transmission ending and context cancelling.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have to make sure that if the context ends, no message is sent to the outbound
    channel. Here, we are collecting the values from the input channels and sending
    them to the merge channel, but only if the context isn''t complete. We do this
    in order to avoid a send operation being sent to a closed channel, which would
    make our application panic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can focus on the last operation, which uses the merge channel to
    execute our final word count:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The application''s `main` function, with the addition of the fan-in, will look
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the fan-in is the most complex and critical part of the application.
    Let''s recap the decisions that helped build a fan-in function that is free from
    panic or deadlock:'
  prefs: []
  type: TYPE_NORMAL
- en: Use a merge channel to collect values from the various input.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have `sync.WaitGroup` with a counter equal to the number of input channels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use it in a separate goroutine and wait for it to close the channel.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each input channel, create a goroutine that transfers the values to the
    merge channel.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure that you send the record only if the context is not complete.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the wait group's `done` function before exiting such a goroutine.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Following the preceding steps will allow us to use the merge channel with a
    simple `range`. In our example, we are also checking whether the context is complete
    before receiving from the channel in order to allow for an early exit from the
    goroutine.
  prefs: []
  type: TYPE_NORMAL
- en: Producers and consumers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Channels allow us to easily handle a scenario in which multiple consumers receive data
    from one producer and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: 'The case with a single producer and one consumer, as we have already seen,
    is pretty straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The full example is available here: [https://play.golang.org/p/hNgehu62kjv](https://play.golang.org/p/hNgehu62kjv).
  prefs: []
  type: TYPE_NORMAL
- en: Multiple producers (N * 1)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Having multiple producers or consumers can be easily handled using wait groups. In
    the case of multiple producers, all the goroutines will share the same channel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The full example is available here: [https://play.golang.org/p/4DqWKntl6sS](https://play.golang.org/p/4DqWKntl6sS).
  prefs: []
  type: TYPE_NORMAL
- en: They will use `sync.WaitGroup` to wait for each producer to finish before closing
    the channel.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple consumers (1 * M)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The same reasoning applies with multiple consumers – they all receive from
    the same channel in different goroutines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The full example is available here: [https://play.golang.org/p/_SWtw54ITFn](https://play.golang.org/p/_SWtw54ITFn).
  prefs: []
  type: TYPE_NORMAL
- en: In this case, `sync.WaitGroup` is used to wait for the application to end.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple consumers and producers (N*M)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last scenario is where we have an arbitrary number of producers (`N`) and
    another arbitrary number of consumers (`M`).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, we need two waiting groups: one for the producer and another
    for the consumer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This will be followed by a series of producers and consumers, each one in their
    own goroutine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The final step is to wait for the `WaitGroup` producer to finish its work in
    order to close the channel.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we can wait for the consumer channel to let all the messages be processed
    by the consumers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Other patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we've looked at the most common concurrency patterns that can be used.
    Now, we will focus on some that are less common but are worth mentioning.
  prefs: []
  type: TYPE_NORMAL
- en: Error groups
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The power of `sync.WaitGroup` is that it allows us to wait for simultaneous
    goroutines to finish their jobs. We have already looked at how sharing context
    can allow us to give the goroutines an early exit if it''s used correctly. The
    first concurrent operation, such as send or receive from a channel, is in the `select`
    block, together with the context completion channel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: An improvement on this scenario is offered by the experimental `golang.org/x/sync/errgroup` package.
  prefs: []
  type: TYPE_NORMAL
- en: The built-in goroutines are always of the `func()` type, but this package allows
    us to execute `func() error` concurrently and return the first error that's received
    from the various goroutines.
  prefs: []
  type: TYPE_NORMAL
- en: This is very useful in scenarios where you launch more goroutines together and
    receive the first error. The `errgroup.Group` type can be used as a zero value,
    and its `Do` method takes `func() error` as an argument and launches the function
    concurrently.
  prefs: []
  type: TYPE_NORMAL
- en: The `Wait` method either waits for all the functions to finish successfully
    and returns `nil`, or it returns the first error that comes from any of the functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create an example that defines a URL visitor, that is, a function that
    gets a URL string and returns `func() error`, which makes the call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use it directly with the `Go` method and wait. This will return the
    error that was caused by the invalid URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The error group also allows us to create a group, along with a context, with
    the `WithContext` function. This context gets cancelled when the first error is
    received. The context's cancellation enables the `Wait` method to return right
    away, but it also allows an early exit in the goroutines in your functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create a similar `func() error` creator that will send values into a
    channel until the context is closed. We will introduce a small chance (1%) of
    raising an error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We will generate an error group and a context with the dedicated function and
    use it to launch several instances of the function. We will receive this in a
    separate goroutine while we wait for the group. After the wait is over, we will
    make sure that there are no more values being sent to the channel (this would
    cause a panic) by waiting an extra second:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: As expected, thanks to the `select` statement within the context, the application
    runs seamlessly and does not panic.
  prefs: []
  type: TYPE_NORMAL
- en: Leaky bucket
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We saw how to build a rate limiter using ticker in the previous chapters: by
    using `time.Ticker` to force a client to await its turn in order to get served.
    There is another take on rate limiting of services and libraries that''s known
    as the **leaky bucket**. The name evokes an image of a bucket with a few holes
    in it. If you are filling it, you have to be careful to not put too much water
    into the bucket, otherwise it''s going to overflow. Before adding more water,
    you need to wait for the level to drop – the speed at which this happens will
    depend on the size of the bucket and the number of the holes it has. We can easily
    understand what this concurrency pattern does by taking a look at the following
    analogy:'
  prefs: []
  type: TYPE_NORMAL
- en: The water going through the holes represents requests that have been completed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The water that's overflowing from the bucket represents the requests that have
    been discarded.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The bucket will be defined by two attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Rate**: The ideal amount of requests per time if the frequency of requests
    is lower.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Capacity**: The number of requests that can be done at the same time before
    the resource turns unresponsive temporarily.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bucket has a maximum capacity, so when requests are made with a frequency
    higher than the rate specified, this capacity starts dropping, just like when
    you're putting too much water in and the bucket starts to overflow. If the frequency
    is zero or lower than the rate, the bucket will slowly gain its capacity, and
    so the water will be slowly drained.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data structure of the leaky bucket will have a capacity and a counter for
    the requests that are available. This counter will be the same as the capacity
    on creation, and will drop each time requests are executed. The rate specifies
    how often the status needs to be reset to the capacity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'When creating a new bucket, we should also take care of the status reset. We
    can use a goroutine for this and use a context to terminate it correctly. We can
    create a ticker using the rate and then use these ticks to reset the status. We
    need to use the atomic package to ensure it is thread-safe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'When we''re adding to the bucket, we can check the status and act accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: If the status is `0`, we cannot add anything.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the amount to add is higher than the availability, we add what we can.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We add the full amount otherwise:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: We are using a loop to try atomic swap operations until they succeed to ensure
    that what we get with the `Load` operation doesn't change when we are doing a **compare
    and swap** (**CAS**).
  prefs: []
  type: TYPE_NORMAL
- en: 'The bucket can be used in a client that will try to add a random amount to
    the bucket and will log its result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use more clients concurrently so that having concurrent access to resources
    will have the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: Some goroutines will be adding what they expect to the bucket.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One goroutine will finally fill the bucket by adding a quantity that is equal
    to the remaining capacity, even if the amount that they are trying to add is higher.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The other goroutines will not be able to add to the bucket until the capacity
    is reset:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Sequencing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In concurrent scenarios with multiple goroutines, we may need to have a synchronization
    between goroutines, such as in a scenario where each goroutine needs to wait for
    its turn after sending.
  prefs: []
  type: TYPE_NORMAL
- en: A use case for this scenario could be a turn-based application wherein different
    goroutines are sending messages to the same channel, and each one of them has
    to wait until all the others have finished before they can send it again.
  prefs: []
  type: TYPE_NORMAL
- en: 'A very simple implementation of this scenario can be obtained using private
    channels between the main goroutine and the senders. We can define a very simple
    structure that carries both messages and a `Wait` channel. It will have two methods –
    one for marking the transaction as done and another one that waits for such a
    signal – when it uses a channel underneath. The following method shows this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We can create a source of messages with a generator. We can use a random delay
    with the `send` operation. After each send, we wait for the signal that is obtained
    by calling the `Done` method. We always use context to keep everything free from
    leaks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use a fan-in to put all of the channels into one, singular channel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The main application will be receiving from the merged channel until it's closed.
    When it receives one message from each channel, the channel will be blocked, waiting
    for the `Done` method signal to be called by the main goroutine.
  prefs: []
  type: TYPE_NORMAL
- en: 'This specific configuration will allow the main goroutine to receive just one
    message from each channel. When the message count reaches the number of goroutines,
    we can call `Done` from the main goroutine and reset the list so that the other
    goroutines will be unlocked and be able to send messages again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Running the application will result in all the goroutines sending a message
    to the main one once. Each of them will be waiting for everyone to send their
    message. Then, they will start sending messages again. This results in messages
    being sent in rounds, as expected.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at some specific concurrency patterns for our applications.
    We learned that generators are functions that return channels, and also feed such
    channels with data and close them when there is no more data. We also saw that
    we can use a context to allow the generator to exit early.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we focused on pipelines, which are stages of execution that use channels
    for communication. They can either be source, which doesn't require any input;
    destination, which doesn't return a channel; or intermediate, which receives a
    channel as input and returns one as output.
  prefs: []
  type: TYPE_NORMAL
- en: Another pattern is the multiplexing and demultiplexing one, which consists of
    spreading a channel to different goroutines and combining several channels into
    one. It is often referred to as *fan-out fan-in*, and it allows us to execute
    different operations concurrently on a set of data.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we learned how to implement a better version of the rate limiter called
    **leaky bucket**, which limits the number of requests in a specific amount of
    time. We also looked at the sequencing pattern, which uses a private channel to
    signal to all of the sending goroutines when they are allowed to send data again.
  prefs: []
  type: TYPE_NORMAL
- en: In this next chapter, we are going to introduce the first of two extra topics
    that were presented in the *Sequencing* section. It is here that we will demonstrate
    how to use reflection to build generic code that adapts to any user-provided type.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is a generator? What are its responsibilities?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How could you describe a pipeline?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What type of stage gets a channel and returns one?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between fan-in and fan-out?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
