- en: Obtaining, Processing, and Preparing Data with Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning is an extremely broad field, and these days, applications can
    be found across areas that include web and mobile applications, the Internet of
    Things and sensor networks, financial services, healthcare, and various scientific
    fields, to name just a few.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the range of data available for potential use in machine learning
    is enormous. In this book, we will focus mostly on business applications. In this
    context, the data available often consists of data internal to an organization
    (such as transactional data for a financial services company) as well as external
    data sources (such as financial asset price data for the same financial services
    company).
  prefs: []
  type: TYPE_NORMAL
- en: For example, you'll recall from [Chapter 3](fbb4c025-a861-4b26-8284-a8ae5f0f0d88.xhtml),
    *Designing a Machine Learning System*, that the main internal source of data for
    our hypothetical internet business, Movie Stream, consists of data on the movies
    available on the site, the users of the service, and their behavior. This includes
    data about movies and other content (for example, title, categories, description,
    images, actors, and directors), user information (for example, demographics, location,
    and so on), and user activity data (for example, web page views, title previews
    and views, ratings, reviews, and social data such as *likes*, *shares*, and social
    network profiles on services, including Facebook and Twitter).
  prefs: []
  type: TYPE_NORMAL
- en: External data sources in this example may include weather and geolocation services,
    third-party movie ratings, and review sites such as *IMDB* and *Rotten Tomatoes*,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Generally speaking, it is quite difficult to obtain data of an internal nature
    for real-world services and businesses, as it is commercially sensitive (in particular,
    data on purchasing activity, user or customer behavior, and revenue) and of great
    potential value to the organization concerned. This is why it is also often the
    most useful and interesting data on which to apply machine learning--a good machine
    learning model that can make accurate predictions can be highly valuable (witness
    the success of machine learning competitions, such as *Netflix Prize* and *Kaggle*).
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we will make use of datasets that are publicly available to illustrate
    concepts around data processing and training of machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will:'
  prefs: []
  type: TYPE_NORMAL
- en: Briefly cover the types of data typically used in machine learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide examples of where to obtain interesting datasets, often publicly available
    on the internet. We will use some of these datasets throughout the book to illustrate
    the use of the models we introduce.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discover how to process, clean, explore, and visualize our data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduce various techniques to transform our raw data into features that can
    be used as input to machine learning algorithms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn how to normalize input features using external libraries as well as Spark's
    built-in functionality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessing publicly available datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Fortunately, while commercially sensitive data can be hard to come by, there
    are still a number of useful datasets available publicly. Many of these are often
    used as benchmark datasets for specific types of machine learning problems. Examples
    of common data sources include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**UCI Machine Learning Repository**: This is a collection of almost 300 datasets
    of various types and sizes for tasks, including classification, regression, clustering,
    and recommender systems. The list is available at [http://archive.ics.uci.edu/ml/](http://archive.ics.uci.edu/ml/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon AWS public datasets**: This is a set of often very large datasets
    that can be accessed via Amazon S3\. These datasets include the Human Genome Project,
    the Common Crawl web corpus, Wikipedia data, and Google Books Ngrams. Information
    on these datasets can be found at [http://aws.amazon.com/publicdatasets/](http://aws.amazon.com/publicdatasets/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kaggle**: This is a collection of datasets used in machine learning competitions
    run by Kaggle. Areas include classification, regression, ranking, recommender
    systems, and image analysis. These datasets can be found under the Competitions
    section at [http://www.kaggle.com/competitions](http://www.kaggle.com/competitions).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**KDnuggets**: This has a detailed list of public datasets, including some
    of those mentioned earlier. The list is available at [http://www.kdnuggets.com/datasets/index.html](http://www.kdnuggets.com/datasets/index.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many other resources to find public datasets depending on the specific
    domain and machine-learning task. Hopefully, you might also have exposure to some
    interesting academic or commercial data of your own!
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate a few key concepts related to data processing, transformation,
    and feature extraction in Spark, we will download a commonly used dataset for
    movie recommendations; this dataset is known as the **MovieLens** dataset. As
    it is applicable to recommender systems as well as potentially other machine learning
    tasks, it serves as a useful example dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The MovieLens 100k dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The MovieLens 100k dataset is a set of 100,000 data points related to ratings
    given by a set of users to a set of movies. It also contains movie metadata and
    user profiles. While it is a small dataset, you can quickly download it and run
    Spark code on it. This makes it ideal for illustrative purposes.
  prefs: []
  type: TYPE_NORMAL
- en: You can download the dataset from [http://files.grouplens.org/datasets/movielens/ml-100k.zip](http://files.grouplens.org/datasets/movielens/ml-100k.zip).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have downloaded the data, unzip it using your terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This will create a directory called `ml-100k`. Change into this directory and
    examine the contents. The important files are `u.user` (user profiles), `u.item`
    (movie metadata), and `u.data` (the ratings given by users to movies):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `README` file contains more information on the dataset, including the variables
    present in each data file. We can use the head command to examine the contents
    of the various files.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we can see that the `u.user` file contains the user ID, age, gender,
    occupation, and ZIP code fields, separated by a pipe (`|` character):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The `u.item` file contains the movie ID, title, release data, and IMDB link
    fields and a set of fields related to movie category data. It is also separated
    by a `|` character:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous data listed has the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The last 19 fields are the genres, a 1 indicates the movie is of that genre,
    a 0 indicates it is not; movies can be in several genres at once.
  prefs: []
  type: TYPE_NORMAL
- en: 'The movie IDs are the ones used in the `u.data` dataset. It contains100000
    ratings by 943 users on 1682 items. Each user has rated at least 20 movies. Users
    and items are numbered consecutively from 1\. The data is randomly ordered. This
    is a tab separated list of following fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The time stamps are Unix seconds since 1/1/1970 UTC.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at some data from the u.data file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Exploring and visualizing your data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Source code for the chapter can be found at `PATH/spark-ml/Chapter04`:'
  prefs: []
  type: TYPE_NORMAL
- en: Python code is available at `/MYPATH/spark-ml/Chapter_04/python`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scala code is available at`/MYPATH/spark-ml/Chapter_04/scala`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pythons Samples are available for both version 1.6.2 and 2.0.0; we will focus
    on 2.0.0 in this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Scala samples are structured as shown in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Scala 2.0.0 Samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Go to the following directory and run the following commands to run the samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Exploring the user dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we will analyze the characteristics of MovieLens users.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use a `custom_schema` to load the `|` delimited data into a DataFrame. This
    Python code is in `com/sparksamples/Util.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This function is called from `user_data.py` as show following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see output similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Code-listing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/python/2.0.0/com/sparksamples/user_data.py](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/python/2.0.0/com/sparksamples/user_data.py)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/python/2.0.0/com/sparksamples/util.py](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/python/2.0.0/com/sparksamples/util.py)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Similar code in Scala for loading data into a DataFrame is listed as follows.
    This code is in `Util.scala`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see output similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The code listing is at : [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/UserData.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/UserData.scala)
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, this is the first line of our user data file, separated by the
    `"|"` character.
  prefs: []
  type: TYPE_NORMAL
- en: The `first` function is similar to `collect`, but it only returns the first
    element of the RDD to the driver. We can also use `take(k)` to collect only the
    first *k* elements of the RDD to the driver.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the DataFrame created earlier and use the `groupBy` function followed
    by `count()` and `collect()` to calculate number of users, genders, ZIPcodes,
    and occupations. We will then count the number of users, genders, occupations,
    and ZIP codes. We can achieve this by running the following code. Note that we
    do not cache the data, as it is unnecessary for this small size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Similarly, we can implement the logic of getting number of users, genders, occupations,
    and zip codes using Scala.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Find the code listing at: [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/UserData.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/UserData.scala)
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will create a histogram to analyze the distribution of user ages.
  prefs: []
  type: TYPE_NORMAL
- en: In Python, first we get the `DatFrame` into variable `user_data`. Next, we'll
    call `select('age')` and collect the result into List of Row object. Then, we
    iterate and extract age parameter and populate `user_ages_list`.
  prefs: []
  type: TYPE_NORMAL
- en: We will be using Python matplotlib library's `hist` function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Find the code listing at:[ https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/python/2.0.0/com/sparksamples/plot_user_ages.py](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/python/2.0.0/com/sparksamples/plot_user_ages.py)
  prefs: []
  type: TYPE_NORMAL
- en: We passed in the `user_ages_list`, together with the number of bins for our
    histogram (20 in this case), to the `hist` function. Using the `normed=True` argument,
    we also specified that we want the histogram to be normalized so that each bucket
    represents the percentage of the overall data that falls into that bucket.
  prefs: []
  type: TYPE_NORMAL
- en: You will see an image containing the histogram chart, which looks something
    like the one shown here. As we can see, the ages of MovieLens users are somewhat
    skewed toward younger viewers. A large number of users are between the ages of
    about 15 and 35.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_04_001.png)'
  prefs: []
  type: TYPE_IMG
- en: Distribution of user ages
  prefs: []
  type: TYPE_NORMAL
- en: For Scala Histogram Chart, we are using a JFreeChart-based library. We divided
    the data into 16 bins to the show the distribution.
  prefs: []
  type: TYPE_NORMAL
- en: We are using the [https://github.com/wookietreiber/scala-chart](https://github.com/wookietreiber/scala-chart)
    library to create a barchart from the Scala map `m_sorted`.
  prefs: []
  type: TYPE_NORMAL
- en: First we extract the `ages_array` from `userDataFrame` using the `select("age")`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we populate the `mx` Map, which is the bins for displaying. We sort the
    mx Map to create a `ListMap`, which is then used to populate `DefaultCategorySet
    ds`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The complete code can be found in `UserAgesChart.scala`file and is listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: Find the code listing at: [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/UserAgesChart.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/UserAgesChart.scala)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_04_002.png)'
  prefs: []
  type: TYPE_IMG
- en: Count by occupation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We count a number of the various occupations of our users.
  prefs: []
  type: TYPE_NORMAL
- en: The following steps were implemented to get the occupation DataFrame and populate
    the list, which was displayed using Matplotlib.
  prefs: []
  type: TYPE_NORMAL
- en: Get `user_data`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract occupation count using `groupby("occupation")` and calling `count()`
    on it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract list of `tuple("occupation","count")` from the list of rows.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a `numpy` array of values in `x_axis` and `y_axis`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a plot of type bar.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Display the chart.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The complete code listing can be found following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The image you have generated should look like the one here. It appears that
    the most prevalent occupations are **student**, **other**, **educator**, **administrator**,
    **engineer**, and **programmer**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_04_003.png)'
  prefs: []
  type: TYPE_IMG
- en: Distribution of user occupations
  prefs: []
  type: TYPE_NORMAL
- en: Find the code listing at: [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/python/2.0.0/com/sparksamples/plot_user_occupations.py](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/python/2.0.0/com/sparksamples/plot_user_occupations.py)
  prefs: []
  type: TYPE_NORMAL
- en: 'In Scala, we follow the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: First get the `userDataFrame`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We extract occupation column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Group the rows by occupation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Sort the rows by count:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Populate Default category set ds from: `occupation_groups_collection`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Display the Jfree Bar Chart
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The complete code listing is given following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of this code is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_04_004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following figure shows the JFreeChart generated from the previous source
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_04_005.png)'
  prefs: []
  type: TYPE_IMG
- en: Find the code listing at: [https://github.com/ml-resources/spark-ml/blob/branched2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/UserOccupationChart.scala](https://github.com/ml-resources/spark-ml/blob/branched2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/UserOccupationChart.scala)
  prefs: []
  type: TYPE_NORMAL
- en: Movie dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, we will investigate a few properties of the movie catalog. We can inspect
    a row of the movie data file, as we did for the user data earlier, and then count
    the number of movies:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will create a DataFrame of movie data by parsing using the format `com.databrick.spark.csv`
    and giving a `|` delimiter. Then, we use a `CustomSchema` to populate the DataFrame
    and return it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This method is then called from the `MovieData` Scala Object.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps are implemented to filter the date and format it into a
    `Year`:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a Temp View.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Register the function `Util.convertYear` as a UDF with `SparkSession`.`Util.spark`
    (this is our custom class).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Execute SQLon this `SparkSession` as shown following.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Group the resulting DataFrame by `Year` and call `count()` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The complete code listing for the logic is given here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be similar as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Find the code listing at: [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/MovieData.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/MovieData.scala)
  prefs: []
  type: TYPE_NORMAL
- en: Next, we draw the graph of ages of movies collection created earlier. We use
    JFreeChart in Scala and populate `org.jfree.data.category.DefaultCategoryDataset`
    from the collection created by `MovieData.getMovieYearsCountSorted()`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that most of the movies are from 1996\. The graph created is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_04_006.png)'
  prefs: []
  type: TYPE_IMG
- en: Distribution of movie ages
  prefs: []
  type: TYPE_NORMAL
- en: Find the code listing at: [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/MovieAgesChart.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/MovieAgesChart.scala)
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the rating dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s now take a look at the rating data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code is located under `RatingData`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: There are 100,000 ratings, and unlike the user and movie datasets, these records
    are split with a tab character (`"t"`). As you might have guessed, we'd probably
    want to compute some basic summary statistics and frequency histograms for the
    rating values. Let's do this now.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data is separated. As you might have guessed, we''d probably want to compute
    some basic summary statistics and frequency histograms for the rating values.
    Let''s do this now:). As you might have guessed, we probably want to compute some
    basic summary statistics and frequency histograms for the rating values. Let''s
    do this now:'
  prefs: []
  type: TYPE_NORMAL
- en: We will calculate the max, min, and average ratings. We will also calculate
    ratings per user and ratings per movie. We are using Spark SQL to extract the
    max, min, and average value of movie ratings.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Find the code listing at: [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/RatingData.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/RatingData.scala)
  prefs: []
  type: TYPE_NORMAL
- en: Rating count bar chart
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Looking at the results, the average rating given by a user to a movie is around
    3.5, so we might expect that the distribution of ratings will be skewed towards
    slightly higher ratings. Let's see whether this is true, by creating a bar chart
    of rating values using a similar procedure as we did for occupations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for plotting ratings versus count is shown here. This is available
    in the file `CountByRatingChart.scala`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'On executing the previous code, you will get the bar chart as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_04_007.png)'
  prefs: []
  type: TYPE_IMG
- en: Distribution of number ratings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can also look at the distribution of the number of ratings made by each user.
    Recall that we previously computed the `rating_data` RDD used in the preceding
    code by splitting the ratings with the tab character. We will now use the `rating_data`
    variable again in the following code.
  prefs: []
  type: TYPE_NORMAL
- en: Code resides in the class `UserRatingChart`. We will create a DataFrame from
    `u.data` file which is tab separated and then `groupbyuser_id` and sort by the
    count of ratings given by each user in ascending order.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Let us first try to show the ratings.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: After showing the data textually, let's show it using JFreeChart by loading
    `DefaultCategorySet` with data from the `rating_nos_by_user DataFrame`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/image_04_008.png)'
  prefs: []
  type: TYPE_IMG
- en: In the previous graph, x-axis is the user ID and y-axis is the number of ratings,
    which varies from minimum rating of 20 to a maximum of 737.
  prefs: []
  type: TYPE_NORMAL
- en: Processing and transforming your data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to make the raw data usable in a machine learning algorithm, we first
    need to clean it up and possibly transform it in various ways before extracting
    useful features from the transformed data. The transformation and feature extraction
    steps are closely linked, and in some cases, certain transformations are themselves
    a case of feature extraction.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have already seen an example of the need to clean data in the movie dataset.
    Generally, real-world datasets contain bad data, missing data points, and outliers.
    Ideally, we would correct bad data; however, this is often not possible, as many
    datasets derive from some form of collection process that cannot be repeated (this
    is the case, for example, in web activity data and sensor data). Missing values
    and outliers are also common and can be dealt with in a manner similar to bad
    data. Overall, the broad options are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Filter out or remove records with bad or missing values**: This is sometimes
    unavoidable; however, this means losing the good part of a bad or missing record.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fill in bad or missing data**: We can try to assign a value to bad or missing
    data based on the rest of the data we have available. Approaches can include assigning
    a zero value, assigning the global mean or median, interpolating nearby or similar
    data points (usually, in a time-series dataset), and so on. Deciding on the correct
    approach is often a tricky task and depends on the data, situation, and one''s
    own experience.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apply robust techniques to outliers**: The main issue with outliers is that
    they might be correct values, even though they are extreme. They might also be
    errors. It is often very difficult to know which case you are dealing with. Outliers
    can also be removed or filled in, although fortunately, there are statistical
    techniques (such as robust regression) to handle outliers and extreme values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apply transformations to potential outliers**: Another approach for outliers
    or extreme values is to apply transformations, such as a logarithmic or Gaussian
    kernel transformation, to features that have potential outliers, or display large
    ranges of potential values. These types of transformations have the effect of
    dampening the impact of large changes in the scale of a variable and turning a
    nonlinear relationship into one that is linear.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filling in bad or missing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's take a looks at the year of movie review and clean it up.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have already seen an example of filtering out bad data. Following on from
    the preceding code, the following code snippet applies the fill-in approach to
    the bad release date record by assigning a value which is Empty String as 1900
    (this will be later replaced by the Median):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous code, we used the `replaceEmtryStr` function described here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we extract filtered years which are not 1900, replace `Array[Row]` with
    `Array[int]` and calculate various metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: Total sum of Entries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Total No. of Entries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean value of Year
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Median value of Year
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Total Years after conversion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Count of 1900
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The output of the previous code is listed in the following; values with `1900`
    after replacement with median shows that our processing has been successful
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Find the code listing at: [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/MovieDataFillingBadValues.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/MovieDataFillingBadValues.scala)
  prefs: []
  type: TYPE_NORMAL
- en: We computed both the mean and the median year of release here. As can be seen
    from the output, the median release year is considerably higher, because of the
    skewed distribution of the years. While it is not always straightforward to decide
    on precisely which fill-in value to use for a given situation, in this case, it
    is certainly feasible to use the median due to this skew.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the preceding code example is, strictly speaking, not very scalable,
    as it requires collecting all the data to the driver. We can use Spark's `mean`
    function for numeric RDDs to compute the mean, but there is no median function
    available currently. We can solve this by creating our own or by computing the
    median on a sample of the dataset created using the `sample` function (we will
    see more of this in the upcoming chapters).
  prefs: []
  type: TYPE_NORMAL
- en: Extracting useful features from your data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once we are done with the cleaning of our data, we are ready to get down to
    the business of extracting actual features from the data, with which our machine
    learning model can be trained.
  prefs: []
  type: TYPE_NORMAL
- en: '**Features** refer to the variables that we use to train our model. Each row
    of data contains information that we would like to extract into a training example.'
  prefs: []
  type: TYPE_NORMAL
- en: Almost all machine learning models ultimately work on numerical representations
    in the form of a vector; hence, we need to convert raw data into numbers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Features broadly fall into a few categories, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Numerical features**: These features are typically real or integer numbers,
    for example, the user age that we used in an example earlier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Categorical features**: These features refer to variables that can take one
    of a set of possible states at any given time. Examples from our dataset might
    include a user''s gender or occupation or movie categories.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text features**: These are features derived from the text content in the
    data, for example, movie titles, descriptions, or reviews.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Other features**: Most other types of features are ultimately represented
    numerically. For example, images, video, and audio can be represented as sets
    of numerical data. Geographical locations can be represented as latitude and longitude
    or geohash data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here we will cover numerical, categorical, and text features.
  prefs: []
  type: TYPE_NORMAL
- en: Numerical features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is the difference between any old number and a numerical feature? Well,
    in reality, any numerical data can be used as an input variable. However, in a
    machine learning model, you learn about a vector of weights for each feature.
    The weights play a role in mapping feature values to an outcome or target variable
    (in the case of supervised learning models).
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we want to use features that make sense, that is, where the model can
    learn the relationship between feature values and the target variable. For example,
    age might be a reasonable feature. Perhaps there is a direct relationship between
    increasing age and a certain outcome. Similarly, height is a good example of a
    numerical feature that can be used directly.
  prefs: []
  type: TYPE_NORMAL
- en: We will often see that numerical features are less useful in their raw form
    but can be turned into representations that are more useful. The location is an
    example of such a case.
  prefs: []
  type: TYPE_NORMAL
- en: Using raw locations (say, latitude and longitude) might not be that useful unless
    our data is very dense indeed since our model might not be able to learn about
    a useful relationship between the raw location and an outcome. However, a relationship
    might exist between some aggregated or binned representation of the location (for
    example, a city or country) and the outcome.
  prefs: []
  type: TYPE_NORMAL
- en: Categorical features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Categorical features cannot be used as input in their raw form, as they are
    not numbers; instead, they are members of a set of possible values that the variable
    can take. In the example mentioned earlier, user occupation is a categorical variable
    that can take the value of student, programmer, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: To transform categorical variables into a numerical representation, we can use
    a common approach known as **1-of-k** encoding. An approach such as 1-of-k encoding
    is required to represent.
  prefs: []
  type: TYPE_NORMAL
- en: An approach such as 1-of-k encoding is required to represent nominal variables
    in a way that makes sense for machine learning tasks. Ordinal variables might
    be used in their raw form but are often encoded in the same way as nominal variables.
  prefs: []
  type: TYPE_NORMAL
- en: Assume that there are k possible values that the variable can take. If we assign
    each possible value an index from the set of 1 to k, then we can represent a given
    state of the variable using a binary vector of length k; here, all entries are
    zero, except the entry at the index that corresponds to the given state of the
    variable. This entry is set to one.
  prefs: []
  type: TYPE_NORMAL
- en: e.g. student is [0], programmer is [1]
  prefs: []
  type: TYPE_NORMAL
- en: 'so values of:'
  prefs: []
  type: TYPE_NORMAL
- en: student become [ 1, 0]
  prefs: []
  type: TYPE_NORMAL
- en: programmer become [0,1]
  prefs: []
  type: TYPE_NORMAL
- en: 'Extract the binary encoding for two occupations, followed by binary feature
    vector creation with a length of 21:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the previous `println` statements is listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the previous commands, which show the binary feature vector and
    length of binary vector is listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Find the code listing at: [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/RatingData.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/RatingData.scala)
  prefs: []
  type: TYPE_NORMAL
- en: Derived features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we mentioned earlier, it is often useful to compute a derived feature from
    one or more available variables. We hope that the derived feature can add more
    information than only using the variable in its raw form available variables.
    We hope that the derived feature can add more information than only using the
    variable in its raw form.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, we can compute the average rating given by each user to all the
    movies they rated. This would be a feature that could provide a *user-specific*
    intercept in our model (in fact, this is a commonly used approach in recommendation
    models). We have taken the raw rating data and created a new feature that can
    allow us to learn a better model.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of features derived from raw data include computing average values,
    median values, variances, sums, differences, maximums or minimums, and counts.
    We have already seen a case of this when we created a new `movie age` feature
    from the year of release of the movie and the current year. Often, the idea behind
    using these transformations is to summarize the numerical data in some way that
    might make it easier for a model to learn features, for example, by binning features.
    Common examples of these include variables such as age, geolocation, and time.
  prefs: []
  type: TYPE_NORMAL
- en: Transforming timestamps into categorical features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Extract time of Day
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To illustrate how to derive categorical features from numerical data, we will
    use the times of the ratings given by users to movies. Extract the date and time
    from the timestamp and, in turn, extract the `hour` of the day.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will need a function to extract a `datetime` representation of the rating
    timestamp (in seconds); we will create this function now: extract the date and
    time from the timestamp and, in turn, extract the `hour` of the day. This will
    result in an RDD of the hour of the day for each rating.'
  prefs: []
  type: TYPE_NORMAL
- en: Scala
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we define a function which extracts `currentHour` from a date string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Relevant code listing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Find the code listing at: [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/RatingData.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/RatingData.scala)
  prefs: []
  type: TYPE_NORMAL
- en: Extract time of day
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have transformed the raw time data into a categorical feature that represents
    the hour of the day in which the rating was given.
  prefs: []
  type: TYPE_NORMAL
- en: Now, say that we decide this is too coarse a representation. Perhaps we want
    to further refine the transformation. We can assign each hour-of-the-day value
    into a defined bucket that represents a time of day.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we can say that morning is from 7 a.m. to 11 a.m., while lunch
    is from 11 a.m. to 1 a.m., and so on. Using these buckets, we can create a function
    to assign a time of day, given the hour of the day as input.
  prefs: []
  type: TYPE_NORMAL
- en: Scala
  prefs: []
  type: TYPE_NORMAL
- en: 'In Scala, we define a function which takes inputer as absolute HR in 24 HR
    format and returns time of day: `morning`, `lunch`, `afternoon`, `evening`, or
    `night`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: We register this function as a UDF and call it on temp view timestamps within
    a select call.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Find the code listing at: [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/RatingData.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/RatingData.scala)
  prefs: []
  type: TYPE_NORMAL
- en: We have now transformed the timestamp variable (which can take on thousands
    of values and is probably not useful to a model in its raw form) into hours (taking
    on 24 values) and then into a time of day (taking on five possible values). Now
    that we have a categorical feature, we can use the same 1-of-k encoding method
    outlined earlier to generate a binary feature vector.
  prefs: []
  type: TYPE_NORMAL
- en: Text features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In some ways, text features are a form of categorical and derived features.
    Let's take the example of the description for a movie (which we do not have in
    our dataset). Here, the raw text could not be used directly, even as a categorical
    feature, since there are virtually unlimited possible combinations of words that
    could occur if each piece of text was a possible value `true`, since there are
    virtually unlimited possible combinations of words that could occur if each piece
    of text was a possible value. Our model would almost never see two occurrences
    of the same feature and would not be able to learn effectively. Therefore, we
    would like to turn raw text into a form that is more amenable to machine learning,
    since there are virtually unlimited possible combinations of words that could
    occur if each piece of text was a possible value.
  prefs: []
  type: TYPE_NORMAL
- en: There are numerous ways of dealing with text, and the field of natural language
    processing is dedicated to processing, representing, and modeling textual content.
    A full treatment is beyond the scope of this book, but we will introduce a simple
    and standard approach for text-feature extraction; this approach is known as the
    bag-of-words representation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The bag-of-words approach treats a piece of text content as a set of the words,
    and possibly numbers, in the text (these are often referred to as terms). The
    process of the bag-of-words approach is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tokenization**: First, some form of tokenization is applied to the text to
    split it into a set of tokens (generally words, numbers, and so on). An example
    of this is simple whitespace tokenization, which splits the text on each space
    and might remove punctuation and other characters that are not alphabetical or
    numerical.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stop word removal**: Next, it is usual to remove very common words such as
    "the", "and", and "but" (these are known as stop words).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stemming**: The next step can include stemming, which refers to taking a
    term and reducing it to its base form or stem. A common example is plural terms
    becoming singular (for example, dogs becomes dog, and so on). There are many approaches
    to stemming, and text-processing libraries often contain various stemming algorithms,
    for example, OpenNLP, NLTK and so on. Covering stemming in detail is outside the
    scope of this book, but feel free to explore these libraries on your own.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vectorization**: The final step is turning the processed terms into a vector
    representation. The simplest form is, perhaps, a binary vector representation,
    where we assign a value of one if a term exists in the text and zero if it does
    not. This is essentially identical to the categorical 1-of-k encoding we encountered
    earlier. Like 1-of-k encoding, this requires a dictionary of terms mapping a given
    term to an index number. As you might gather, there are potentially millions of
    individual possible terms (even after stop word removal and stemming). Hence,
    it becomes critical to use a sparse vector representation compute `time.computetime.computetime.compute`
    time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In [Chapter 10](789e8b8c-28e8-444d-92a6-aace3a4dfdd6.xhtml), *Advanced Text
    Processing with Spark*, we will cover more complex text processing and feature
    extraction, including methods to weight terms; these methods go beyond the basic
    binary encoding we saw earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Simple text feature extraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To show an example of extracting textual features in the binary vector representation,
    we can use the movie titles that we have available.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will create a function to strip away the year of release for each
    movie, if the year is present, leaving only the title of the movie.
  prefs: []
  type: TYPE_NORMAL
- en: We will use a regular expression, to search for the year between parentheses
    in the movie titles. If we find a match with this regular expression, we will
    extract only the title up to the index of the first match (that is, the index
    in the title string of the opening parenthesis).
  prefs: []
  type: TYPE_NORMAL
- en: Scala
  prefs: []
  type: TYPE_NORMAL
- en: First, we create a function which takes the input string and filters the output
    with a regular expression.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Extract DataFrame with only the raw title and create a temp view `titles`. Register
    the function created above with Spark and then run it on the DataFrame within
    the `select` statement.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, to apply our function to the raw titles and apply a tokenization scheme
    to the extracted titles to convert them to terms, we will use the simple whitespace
    tokenization we covered earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we split the `titles` into single words
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Applying this simple tokenization gives the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Then, we convert the rdd of words and find the total number of words--we get
    the collection of total words and index of `"Dead"` and `"Rooms"`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also achieve the same result more efficiently using Spark''s `zipWithIndex`
    function. This function takes an RDD of values and merges them together with an
    index to create a new RDD of key-value pairs, where the key will be the term and
    the value will be the index in the term dictionary. We will use `collectAsMap`
    to collect the key-value RDD to the driver as a Python `dict` method:'
  prefs: []
  type: TYPE_NORMAL
- en: Scala
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Sparse Vectors from Titles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The final step is to create a function that converts a set of terms into a
    sparse vector representation. To do this, we will create an empty sparse matrix
    with one row and a number of columns equal to the total number of terms in our
    dictionary. We will then step through each term in the input list of terms and
    check whether this term is in our term dictionary. If it is, we assign a value
    of `1` to the vector at the index that corresponds to the term in our dictionary
    mapping:'
  prefs: []
  type: TYPE_NORMAL
- en: 'extracted terms:'
  prefs: []
  type: TYPE_NORMAL
- en: Scala
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then inspect the first few records of our new RDD of sparse vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Find the code listing at: [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/exploredataset/explore_movies.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/exploredataset/explore_movies.scala)
  prefs: []
  type: TYPE_NORMAL
- en: We can see that each movie title has now been transformed into a sparse vector.
    We can see that the titles where we extracted two terms have two non-zero entries
    in the vector, titles where we extracted only one term have one non-zero entry,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Note the use of Spark's `broadcast` method in the preceding example code to
    create a broadcast variable that contains the term dictionary. In real-world applications,
    such term dictionaries can be extremely large, so using a broadcast variable is
    not advisable.
  prefs: []
  type: TYPE_NORMAL
- en: Normalizing features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once the features have been extracted into the form of a vector, a common preprocessing
    step is to normalize the numerical data. The idea behind this is to transform
    each numerical feature in a way that scales it to a standard size. We can perform
    different kinds of normalization, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Normalize a feature: This is usually a transformation applied to an individual
    feature across the dataset, for example, subtracting the mean (centering the feature)
    or applying the standard normal transformation (such that the feature has a mean
    of zero and a standard deviation of 1).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Normalize a feature vector: This is usually a transformation applied to all
    features in a given row of the dataset such that the resulting feature vector
    has a normalized length. That is, we will ensure that each feature in the vector
    is scaled such that the vector has a norm of 1 (typically, on an L1 or L2 norm).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will use the second case as an example. We can use the `norm` function of
    `numpy` to achieve the vector normalization by first computing the L2 norm of
    a random vector and then dividing each element in the vector by this norm to create
    our normalized vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Using ML for feature normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark provides some built-in functions for feature scaling and standardization
    in its machine learning library. These include `StandardScaler`, which applies
    the standard normal transformation, and `Normalizer,` which applies the same feature
    vector normalization we showed you in our preceding example code.lization we showed
    you in our preceding example code.lization, we showed you in our preceding example
    code.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will explore the use of these methods in the upcoming chapters, but for
    now, let''s simply compare the results of using MLlib''s `Normalizer` to our own
    results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: After importing the required class, we will instantiate `Normalizer` (by default,
    it will use the L2 norm as we did earlier). Note that, as in most situations in
    Spark, we need to provide `Normalizer` with an RDD as input (it contains `numpy`
    arrays or MLlib vectors); hence, we will create a single-element RDD from our
    vector `x` for illustrative purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will then use the `transform` function of `Normalizer` on our RDD. Since
    the RDD only has one vector in it, we will return our vector to the driver by
    calling `first` and finally by calling the `toArray` function to convert the vector
    back into a `numpy` array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can print out the same details as we did previously, comparing
    the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'You will end up with exactly the same normalized vector as we did with our
    own code. However, using MLlib''s built-in methods is certainly more convenient
    and efficient than writing our own functions! Equivalent Scala implementation
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is listed following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Using packages for feature extraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While from these common tasks each and every time. Certainly, we can create
    our own reusable code libraries for this purpose; however, fortunately, we can
    rely on the existing tools and packages. Since Spark supports Scala, Java, and
    Python bindings, we can use packages available in these languages that provide
    sophisticated tools to process and extract features and represent them as vectors.
    A few examples of packages for feature extraction include `scikit-learn`, `gensim`,
    `scikit-image`, `matplotlib`, and `NLTK` in Python, `OpenNLP` in Java, and `Breeze`
    and `Chalk` in Scala. In fact, `Breeze` has been part of Spark MLlib since version
    1.0, and we will see how to use some Breeze functionality for linear algebra in
    the later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: TFID
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**tf-idf** is short term for **term frequency-inverse document frequency**.
    It is a numerical statistic that is intended to reflect how important a word is
    to a document in a collection or corpus. It is used as a weighting factor in information
    retrieval and text mining. The tf-idf value increases in proportion to the number
    of times a word appears in a document. It is offset by the frequency of the word
    in the corpus, that helps to adjust for some words which appear more frequently
    in general.'
  prefs: []
  type: TYPE_NORMAL
- en: tf-idf is used by search engines or text processing engines as a tool in scoring
    and ranking a document's relevance for a user query.
  prefs: []
  type: TYPE_NORMAL
- en: The simplest ranking functions are computed by summing the tf-idf for each query
    term; more sophisticated ranking functions are variants of this simple model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the term frequency `tf(t,d)` calculation, one choice is to use the raw frequency
    of a term in a document: the number of times term t occurs in document `d`. If
    raw frequency of `t` is `f(t,d)`, then the simple `tf` scheme is `tf(t,d) = ft,d`.'
  prefs: []
  type: TYPE_NORMAL
- en: Spark's implementation of `tf(t.d)` uses the hashing. A raw word is mapped into
    an index (term) by applying a hash function.The term frequencies are calculated
    using the mapped indices.
  prefs: []
  type: TYPE_NORMAL
- en: 'References:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://spark.apache.org/docs/1.6.0/api/scala/index.html#org.apache.spark.mllib.feature.HashingTF](https://spark.apache.org/docs/1.6.0/api/scala/index.html#org.apache.spark.mllib.feature.HashingTF)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/Tf%E2%80%93idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://spark.apache.org/docs/1.6.0/mllib-feature-extraction.html](https://spark.apache.org/docs/1.6.0/mllib-feature-extraction.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IDF
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **inverse document frequency**(**IDF**)represents how much information
    the word provides: is the term common or rare across the corpus. It is the log
    scaled inverse fraction of the documents containing the word, calculated by division
    of the total number of documents by the number of documents containing the term**TF-IDF**'
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF is calculated by multiplying TF and IDF.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_04_009.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following example calculates TFIDF for each term in the Apache Spark `README.md`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: Find the code listing at: [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala2.0.0/src/main/scala/org/sparksamples/featureext/TfIdfSample.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala2.0.0/src/main/scala/org/sparksamples/featureext/TfIdfSample.scala)
  prefs: []
  type: TYPE_NORMAL
- en: Word2Vector
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Word2Vec tools take text data as input and produce the word vectors as output.
    This tool constructs a vocabulary from the training text data and learns vector
    representation of words. The resulting word vector file can be used as features
    for many natural language processing and machine learning applications.
  prefs: []
  type: TYPE_NORMAL
- en: The easiest way to investigate the learned representations is to find the closest
    words for a user-specified word.
  prefs: []
  type: TYPE_NORMAL
- en: Word2Vec implementation in Apache Spark computes distributed vector representation
    of words. Apache Spark's implementation is a more scalable approach as compared
    to single machine Word2Vec implementations provided by Google).
  prefs: []
  type: TYPE_NORMAL
- en: ([https://code.google.com/archive/p/word2vec/](https://code.google.com/archive/p/word2vec/))
  prefs: []
  type: TYPE_NORMAL
- en: 'Word2Vec can be implemented using two learning algorithms: continuous bag-of-words
    and continuous skip-gram.'
  prefs: []
  type: TYPE_NORMAL
- en: Skip-gram model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The training objective of the skip-gram model is to find word representations
    useful for predicting the surrounding words in a document or a sentence. Given
    a sequence of words *w1,* *w2, w3, . . , wT*, skip-gram model maximizes the average
    log probability shown as following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_04_010.png)'
  prefs: []
  type: TYPE_IMG
- en: '*c* is the size of the training context (which can be a function of the center
    word *wt*). Larger *c* results in more training examples leading to a higher accuracy,
    at the expense of the training time. The basic skip-gram formulation defines *p(wt+j
    |wt)* using the `softmax` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_04_011.png)'
  prefs: []
  type: TYPE_IMG
- en: '*v[w]*, *v'' *and, *w* are the *input* and *output* vector representations
    of *w*, and *W* is the number of words in the vocabulary'
  prefs: []
  type: TYPE_NORMAL
- en: In Spark Hierarchical soft-max approach is used to predicting word *wi* given
    word *wj*.
  prefs: []
  type: TYPE_NORMAL
- en: The following example shows how to create word vectors using Apache Spark.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: Find the code listing at: [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/featureext/ConvertWordsToVectors.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/featureext/ConvertWordsToVectors.scala)
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: Standard scalar
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Standard scalar standardizes features of the data set by scaling to unit variance
    and removing the mean (optionally) using column summary statistics on the samples
    in the training set.
  prefs: []
  type: TYPE_NORMAL
- en: This process is a very common pre-processing step.
  prefs: []
  type: TYPE_NORMAL
- en: Standardization improves the convergence rate during the optimization process.
    It also prevents features with large variances from exerting an overly large influence
    during model training.
  prefs: []
  type: TYPE_NORMAL
- en: '`StandardScaler` class has the following parameters in the constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: 'new StandardScaler(withMean: Boolean, withStd: Boolean)'
  prefs: []
  type: TYPE_NORMAL
- en: '`withMean`: `False` by default. Centers the data with mean before scaling.
    It will build a dense output, does not work on sparse input and will raise an
    exception.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`withStd`: `True` by default. It Scales the data to unit standard deviation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Annotations
  prefs: []
  type: TYPE_NORMAL
- en: Available @Since("1.1.0" )
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: Find the code listing at: [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/featureext/StandardScalarSample.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_04/scala/2.0.0/src/main/scala/org/sparksamples/featureext/StandardScalarSample.scala)
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw how to find common, publicly available datasets that
    can be used to test various machine learning models. You learned how to load,
    process, and clean data, as well as how to apply common techniques to transform
    raw data into feature vectors that can be used as training examples for our models.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn the basics of recommender systems and explore
    how to create a recommendation model, use the model to make predictions, and evaluate
    the model.
  prefs: []
  type: TYPE_NORMAL
