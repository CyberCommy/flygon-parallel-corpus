- en: 3\. Hash Tables and Bloom Filters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Identify lookup-related problems easily in any large-scale application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate whether a problem is suited for a deterministic or non-deterministic
    lookup solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement an efficient lookup solution based on a scenario
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement generic solutions provided as part of C++ STL in large applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we'll look at the problem of fast lookup. We will learn about
    the various approaches to solving this problem and understand which one can be
    used for a given situation.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Lookup is nothing but checking whether an element is present in a container
    or finding the corresponding value for a key in the container. In the student
    database system and the hospital management system examples that we mentioned
    in the previous chapters, a common operation is to fetch a particular record from
    the vast amount of data stored in the system. A similar problem also presents
    itself while getting the meaning of a word from a dictionary, checking whether
    a person is allowed to enter a certain facility based on a set of records (access
    control), and many more applications.
  prefs: []
  type: TYPE_NORMAL
- en: For most of these scenarios, just going through all the elements linearly and
    matching the values would be extremely time-consuming, especially considering
    the vast amount of records that are stored. Let's take a simple example of looking
    up a word in a dictionary. There are roughly 170,000 words in the English dictionary.
    One of the simplest ways to do this is to traverse the dictionary linearly and
    compare the given word with all the words in the dictionary until we've found
    the word, or we reach the end of the dictionary. But this is too slow, and it
    will have a time complexity of *O(n)*, where n is the number of words in the dictionary,
    which is not only huge but is also increasing day by day.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, we need more efficient algorithms to allow for lookup that works much
    faster. We'll look at a couple of efficient structures in this chapter, that is,
    hash tables and bloom filters. We'll implement both of them and compare their
    pros and cons.
  prefs: []
  type: TYPE_NORMAL
- en: Hash Tables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's look at the very basic problem of searching in a dictionary. There are
    about 170,000 words in the Oxford English Dictionary. As we mentioned in the Introduction,
    a linear search will take *O(n)* time, where *n* is the number of words. A better
    way to store the data is to store it in a height-balanced tree that has similar
    properties to a BST. This makes it much faster than linear search as it has a
    time complexity of only *O(log n)*. But for applications that require tons of
    such queries, this is still not a good enough improvement. Think about the time
    it will take for data containing millions or billions of records, such as neuroscientific
    data or genetic data. It would take days to find something in the data. For these
    situations, we need something much faster, such as a **hash table**.
  prefs: []
  type: TYPE_NORMAL
- en: One of the integral parts of hash tables is **hashing**. The idea behind this
    is to represent each value with a possibly unique key and, later on, use the same
    key to check for the presence of the key or to retrieve a corresponding value,
    depending on the use case. The function that derives a unique key from the given
    data is called a hash function. Let's look at how we can store and retrieve data
    by looking at some examples, and let's learn why we need such a function.
  prefs: []
  type: TYPE_NORMAL
- en: Hashing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s take one simple example before jumping into hashing. Let''s say we have
    a container storing integers, and we want to know if a particular integer is part
    of the container or not as quickly as possible. The simplest way is to have a
    Boolean array with each bit representing a value that''s the same as its index.
    When we want to insert an element, we''ll set the Boolean value corresponding
    to that element to *0*. To insert *x*, we simply set *data[x] = true*. Checking
    whether a particular integer, *x*, is inside the container is just as simple —
    we simply check whether *data[x]* is *true*. Thus, our insertion, deletion, and
    search functions become *O(1)*. A simple hash table for storing integers numbered
    from *0* to *9* would look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1: A simple hash table](img/C14498_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1: A simple hash table'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'However, there are some problems with this approach:'
  prefs: []
  type: TYPE_NORMAL
- en: What if the data is floating-point numbers?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What if the data is not just a number?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What if the range of the data is too high? That is, if we have a billion numbers,
    then we need a Boolean array that's one billion in size, and that is not always
    feasible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To resolve this problem, we can implement a function that will map any value
    of any data type to an integer in the desired range. We can choose the range so
    that its Boolean array will have a feasible size. This function is called a **hash
    function**, as we mentioned in the previous section. It will take one data element
    as input and provide a corresponding output integer within the provided range.
  prefs: []
  type: TYPE_NORMAL
- en: The simplest hashing function for integers in a large range is the modulo function
    (denoted by *%*), which divides the element by a specified integer (*n*) and returns
    the remainder. So, we'll simply have an array of size *n*.
  prefs: []
  type: TYPE_NORMAL
- en: If we want to insert a given value, *x*, we can apply the modulo function on
    it (*x % n*), and we will always get a value between *0* and (*n – 1*), both inclusive.
    Now, *x* can be inserted at position *(x % n)*. Here, the number that's obtained
    by applying the hash function is called the **hash value**.
  prefs: []
  type: TYPE_NORMAL
- en: A major problem we may encounter with this is that two elements may have the
    same output from the modulo function. An example is (*9 % 7*) and (*16 % 7*),
    which both result in a hash value of *2*. Thus, if the slot corresponding to *2*
    is *TRUE* (or *1* for Boolean), we would have no idea which of *2*, *9*, *16*,
    or any other integer that returns *x % 7 = 2* is present in our container. This
    problem is known as collision, because multiple keys have the same values instead
    of unique values after applying the hash function.
  prefs: []
  type: TYPE_NORMAL
- en: If we store the actual value instead of a Boolean integer in our hash table,
    we will know which value we have, but we still cannot store multiple values with
    the same hash value. We will look at how to deal with this in the following section.
    But first, let's look at the implementation of a basic dictionary for a bunch
    of integers in the following exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 13: Basic Dictionary for Integers'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we shall implement a basic version of a hash map for unsigned
    integers. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s include the required headers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s add the `hash_map` class. We''ll alias `unsigned int` to avoid
    writing a long name:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s add a constructor for this, which will take the size of the data
    or hash map:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As shown here, we're using `–1` to indicate the absence of an element. This
    is the only negative value we'll use as data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s add the `insert` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, we are not really checking whether there was a value already
    present with the same hash value. We're simply overwriting if any value is already
    present. So, for a given hash value, only the latest inserted value will be stored.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s write a lookup function to see whether an element is present in the
    map or not:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We'll simply check whether the value is present at the index calculated based
    on the hash value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s implement a `remove` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s write a small lambda function in `main` to print the status of the lookup:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s use the `insert` and `erase` functions on the map:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s the output of the program:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, we are able to find most of the values we inserted earlier, as
    expected, except for the last case, where `100` is overwritten by `0` because
    they have the same hash value. This is called a collision, as we described previously.
    In the upcoming sections, we'll see how we can avoid this kind of problem to make
    our results more accurate.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figures, which demonstrate the different functions from the previous
    exercise, should make this clearer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2: Basic operations in a hash table](img/C14498_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: Basic operations in a hash table'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Figure 3.3: Basic operations in a hash table (continued)](img/C14498_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: Basic operations in a hash table (continued)'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As shown in the preceding figures, we can't insert two elements with the same
    hash value; we have to drop one of them.
  prefs: []
  type: TYPE_NORMAL
- en: Now, as we mentioned earlier, one major use of hash tables is to find a value
    corresponding to a key, and not just checking whether the key exists. This can
    be simply achieved by storing a key-value pair instead of just the key in the
    data. So, our insertion, deletion, and lookup functions will still calculate the
    hash value based on our key, but once we find the position in the array, we'll
    have our value as the second parameter of the pair.
  prefs: []
  type: TYPE_NORMAL
- en: Collisions in Hash Tables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous sections, we took a look at how hash tables can help us store
    a lot of keys in a way that makes it easy to look up any required key. However,
    we also encountered a problem where multiple keys had the same hash value, also
    known as a **collision**. In *Exercise 13*, *Basic Dictionary for Integers*, we
    handled this issue by simply rewriting the key and retaining the latest key corresponding
    to a given hash value. However, this does not allow us to store all the keys.
    In the following subtopics, we shall take a look at a couple of approaches that
    help us overcome this problem and allow us to retain all of our key values in
    the hash table.
  prefs: []
  type: TYPE_NORMAL
- en: Close Addressing – Chaining
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, we've only been storing a single element for any hash value. If we already
    have an element for a particular hash value, we have no option but to discard
    either the new value or the old value. The method of `push_back` for new elements)
    is to enable the fast removal of elements from any position. Let's implement this
    in the following exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 14: Hash Table with Chaining'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we shall implement a hash table and use chaining to handle
    collisions. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s include the required headers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s add the `hash_map` class. We''ll alias `unsigned int` to avoid
    writing a long name:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s add a constructor for `hash_map` that will take the size of the
    data or hash map:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s add an `insert` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, we are always inserting the value in the data. One alternative
    could be to search for the value and insert it only if the value is not present.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s write the lookup function to see whether an element is present in the
    map:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, our lookup seems faster than conventional methods, but not as
    fast as it was earlier. This is because now, it is also dependent on the data,
    as well as the value of `n`. We'll come back to this point again after this exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s implement a function to remove elements:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s write the same `main` function as in the previous exercise and look
    at the difference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s use the `insert` and `erase` functions on `map`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s the output of our program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the values are not overwritten because we can store any number
    of values in the list. Hence, our output is completely accurate and reliable.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following images illustrate how different operations are performed on a
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4: Basic operations on a hash table with chaining](img/C14498_03_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.4: Basic operations on a hash table with chaining'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Figure 3.5: Basic operations on a hash table with chaining (continued)](img/C14498_03_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.5: Basic operations on a hash table with chaining (continued)'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As we can see, we are appending elements with the same hash value in a list
    placed in the node instead of a single element.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's consider the time complexity of these operations. As we saw, the
    insertion function is still *O(1)*. Although `push_back` may be a bit slower than
    just setting a value, it is not significantly slower. Considering the problem
    that this approach solves, it is a small price to pay. But lookup and deletion
    may be significantly slower, depending on our hash table's size and dataset. For
    example, if all the keys have the same hash value, the time required for the search
    will be O(n), as it will simply become a linear search in a linked list.
  prefs: []
  type: TYPE_NORMAL
- en: If the hash table is very small compared to the number of keys to be stored,
    there will be a lot of collisions, and the lists will be longer on average. On
    the other hand, if we keep a very big hash table, we may end up having very sparse
    data and end up wasting memory. So, the hash table's size should be optimized
    based on the application's context and scenario. We can define these things mathematically
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **load factor** indicates the average number of keys present per list in
    our hash table. It can be computed using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6: Load factor](img/C14498_03_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.6: Load factor'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the number of keys is equal to our hash table size, the load factor will
    be *1*. This is an ideal scenario; we'll get close to *O(1)* for all the operations,
    and all the space will be utilized properly.
  prefs: []
  type: TYPE_NORMAL
- en: If the value is less than *1*, this means that we are not storing even one key
    per list (assuming we want a list at every index) and essentially wasting some
    space.
  prefs: []
  type: TYPE_NORMAL
- en: If the value is more than *1*, this implies that the average length of our lists
    is more than 1, and hence our find and removal functions will be a bit slower
    on average.
  prefs: []
  type: TYPE_NORMAL
- en: The value of the load factor can be computed in *O(1)* at any time. Some advanced
    hash table implementations make use of this value to modify the hash function
    (also known as rehashing) if the value crosses certain thresholds on either side
    of 1\. The hash function is modified so that the load factor is moved closer to
    1\. Then, the size of the hash table can be updated according to our load factor
    and the values redistributed based on the updated hash function. Rehashing is
    an expensive operation, and hence should not be performed too frequently. But
    if it is applied with a proper strategy, we can achieve really good results in
    terms of average time complexity.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the load factor is not the only factor determining the performance
    of this technique. Consider the following scenario: We have a hash table of size
    *7* and it has seven elements. However, all of them have the same hash value,
    and hence all of them are present in a single bucket. So, the search will always
    take *O(n)* instead of *O(1)* time. However, the load factor will be 1, which
    is an absolutely ideal value. Here, the actual problem is the hash function. The
    hash function should be designed in such a way that different keys are distributed
    as evenly as possible across all the possible indexes. Basically, the difference
    between the minimum bucket size and the maximum bucket size should not be very
    high (which is seven in this case). If the hash function is designed in a way
    that all seven elements get different hash values, then all the search function
    calls will result in *O(1)* complexity and instant results. This is because the
    difference between the min and max bucket size will be *0*. However, this is usually
    not done in hash table implementation. It is supposed to be taken care of by the
    hash function itself because the hash table is not dependent on the implementation
    of the hash function.'
  prefs: []
  type: TYPE_NORMAL
- en: Open Addressing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another method for resolving collisions is **open addressing**. In this method,
    we store all the elements inside the hash table instead of chaining the elements
    to the hash table. Hence, to accommodate all the elements, the size of the hash
    table must be greater than the number of elements. The idea is to probe if a cell
    corresponding to a particular hash value is already occupied. There are multiple
    ways we can probe the value, as we shall see in the following subtopics.
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear probing**'
  prefs: []
  type: TYPE_NORMAL
- en: This is a simple probing technique. If there is a collision at a particular
    hash value, we can simply look at the subsequent hash value for an empty cell
    and insert our element once we find room for it. If the cell at *hash(x)* is full,
    then we need to check whether the cell at *hash(x + 1)* is empty. If it is also
    full, look at *hash(x + 2)*, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure illustrates how linear probing works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7: Basic operations on a hash table with linear probing](img/C14498_03_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.7: Basic operations on a hash table with linear probing'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Figure 3.8: Unable to insert elements after hash table fills up](img/C14498_03_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.8: Unable to insert elements after hash table fills up'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As we can see, we are inserting an element in the next available slot if the
    position corresponding to its hash value is already occupied. After inserting
    the first three elements, we can see that they are clustered together. If more
    elements are inserted in the same range, all of them will go at the end of the
    cluster consecutively, thus making the cluster grow. Now, when we try to search
    for a value that is not present at the location first calculated by the hash function,
    but is present at the end of a big cluster, we have to search through all the
    keys in the cluster linearly. And therefore, the search becomes drastically slow.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we have a major problem if the data is densely clustered. We can say that
    the data is densely clustered if the data is distributed in such a way that there
    are some groups around which the frequency of values is very high. For example,
    let's say that if there are a lot of keys with a hash value of *3* to *7* in a
    hash table of *100*. All the keys will be probed to some values consecutively
    after that, and it will slow down our searching drastically.
  prefs: []
  type: TYPE_NORMAL
- en: '**Quadratic probing**'
  prefs: []
  type: TYPE_NORMAL
- en: As we saw, the major problem with linear probing was clustering. The reason
    behind this was that we were going linearly in the case of collisions. This problem
    can be resolved to a large extent by using a quadratic equation instead of a linear
    one. And that's what quadratic probing provides.
  prefs: []
  type: TYPE_NORMAL
- en: First, we try to insert the value *x* at the position *hash(x)*. If that position
    is already occupied, we go to the position *hash(x + 1**2**)*, and then *hash(x
    + 2**2**)*, and so on. So, we increase the offset in a quadratic fashion and thus
    decrease the probability of creating small clusters of data.
  prefs: []
  type: TYPE_NORMAL
- en: There is one more advantage of both probing techniques – the position of an
    element can be affected by other elements that don't have the same hash value.
    So, basically, even if there's just one key with a certain hash value, it can
    collide because of some other element is present in that location, which was not
    the case with chaining. For example, in linear probing, if we have two keys with
    a hash value of 4, one of them will be inserted at position 4 and the other will
    be inserted at position 5\. Next, if we need to insert a key with a hash value
    of 5, it will need to be inserted at 6\. This key was affected even though it
    did not have the same hash value as any other key.
  prefs: []
  type: TYPE_NORMAL
- en: Perfect Hashing – Cuckoo Hashing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As the heading suggests, **cuckoo hashing** is one of the perfect hashing techniques.
    The methods we mentioned previously don't provide a guarantee of *O(1)* time complexity
    in the worst case, but cuckoo hashing can achieve that if implemented properly.
  prefs: []
  type: TYPE_NORMAL
- en: In cuckoo hashing, we keep two hash tables of the same size, each with their
    own unique hash function. Any element can be present in either of the hash tables,
    and its position is based on the corresponding hash function.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two main ways in which cuckoo hashing differs from our previous hashing
    techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: Any element can be present in any of the two hash tables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any element can be moved to another location in the future, even after insertion.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Earlier hashing techniques did not allow elements to be moved after insertion
    unless we did a complete rehashing, but this is not the case with cuckoo hashing
    because any element can have two possible locations. We can still increase the
    degree by increasing the number of possible locations for any element so that
    we gain better results and have less frequent rehashing. However, in this chapter,
    we'll only look at the version with two possible locations (hash tables) because
    it's easier to understand.
  prefs: []
  type: TYPE_NORMAL
- en: For lookup, we only need to look at two positions to determine whether the element
    is present or not. Hence, a lookup always requires *O(1)* time.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, an insertion function can take a longer time. An insertion function,
    in this case, first checks whether it is possible to insert the new element, let''s
    say *A*, in the first hash table. If so, it inserts the element there, and we
    are done. But if that position is occupied by a preexisting element, let''s say
    *B*, we still go ahead with inserting *A* and move *B* to the second hash table.
    If this new position in the second hash table is also occupied, let''s say by
    element *C*, we again insert *B* there and move *C* to the first table. We can
    carry this on recursively until we are able to find empty slots for all the elements.
    This process is illustrated in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9: Cuckoo hashing](img/C14498_03_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.9: Cuckoo hashing'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'One major problem is that we could end up in a cycle and the recursion may
    lead to an infinite loop. For the example in the previous paragraph, consider
    that there is an element, *D*, where we wish to insert *C*, but if we try to move
    *D*, it goes to the location of *A*. Thus, we are in an infinite cycle. The following
    figure should help you visualize this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.10: A cycle formed during cuckoo hashing](img/C14498_03_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.10: A cycle formed during cuckoo hashing'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To address this, once we've identified the cycle, we need to rehash everything
    with new hash functions. The hash tables that were created with new hash functions
    may still have the same problems, so we may have to rehash and try out different
    hash functions. However, with smart strategies and wisely chosen hash functions,
    we can achieve a performance of amortized *O(1)* with high probability.
  prefs: []
  type: TYPE_NORMAL
- en: Just like open addressing, we can't store more elements than the combined size
    of the hash tables. To ensure good performance, we should make sure that our load
    factor is less than 50%, that is, the number of elements should be less than half
    of the available capacity.
  prefs: []
  type: TYPE_NORMAL
- en: We'll take a look at the implementation of cuckoo hashing in the following exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 15: Cuckoo Hashing'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we''ll implement cuckoo hashing to create a hash table and
    insert various elements in it. We shall also get a trace of how the operation
    proceeds, which will allow us to take a look at how insertion works. Let''s get
    started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by including the required headers, as usual:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s add a class for the hash map. We''ll also store size separately this
    time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, we use two tables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s add the corresponding hash functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have kept both functions very simple, but these functions can be adapted
    as per the requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s add a constructor that will set our data for initialization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, we are simply initializing both the data tables as empty (indicated
    by `–1`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s write a `lookup` function first:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We are trying to find the key in both tables and return the relevant iterator
    if one is found. We don't always need the iterator, but we'll use it in the deletion
    function to make things easier. We are returning the end of the `data2` table
    if the element is not found. As we can see, the lookup will have a time complexity
    of *O(1)* and will be performed pretty quickly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s implement a delete function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, most of the job is done by calling the `lookup` function. We
    just need to validate the result and reset the value to remove it from the table.
  prefs: []
  type: TYPE_NORMAL
- en: 'For insertion, we shall implement the actual logic in a different function
    because it will be recursive. One more thing we want to do is avoid cycles. However,
    keeping a record of all the values that are visited can be costly. To avoid that,
    we will simply stop the function once it is called more than n times. Since the
    threshold of the recursion depth of n is dependent on our memory (or hash table
    size), this gives good performance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the implementation takes three parameters – the key, the table
    in which we want to insert the key, and the count of the recursion call stack
    to keep track of the number of elements we have changed the positions of.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s write a utility function to print the data inside the hash tables.
    Although this is not really necessary and shouldn''t be exposed, we will do that
    so that we can get a better understanding of how our insert function is managing
    the data internally:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s write the `main` function so that we can use this hash map:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the output is showing the complete trace of how both the tables
    are maintained internally. We have printed the internal steps because some values
    are being moved around. The last insertion of `14` leads to a cycle, as we can
    see from the trace. The depth of insertion has gone beyond `7`. Simultaneously,
    we can also see that both the tables are almost full. We've filled `11` elements
    out of `14`, and hence the chance of replacing values is increasing at each step.
    We printed the table just before the cycle as well.
  prefs: []
  type: TYPE_NORMAL
- en: Also, the deletion of an element takes *O(1)* time here because it just uses
    the `lookup` function and deletes the element, if found. So, the only function
    that is costly is insertion. Hence, this is an ideal implementation if the number
    of insertions is quite a bit lower than the number of lookups in any application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use the following visual aids so that we can understand this better:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.11: Inserting elements in a hash table that uses cuckoo hashing](img/C14498_03_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.11: Inserting elements in a hash table that uses cuckoo hashing'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Figure 3.12: Handling collisions in a hash table using cuckoo hashing](img/C14498_03_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.12: Handling collisions in a hash table using cuckoo hashing'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Figure 3.13: Handling collisions in a hash table using cuckoo hashing (continued)](img/C14498_03_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.13: Handling collisions in a hash table using cuckoo hashing (continued)'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Figure 3.14: Finding values in a hash table that uses cuckoo hashing](img/C14498_03_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.14: Finding values in a hash table that uses cuckoo hashing'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Figure 3.15: Erasing values in a hash table that uses cuckoo hashing](img/C14498_03_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.15: Erasing values in a hash table that uses cuckoo hashing'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As we can see from the preceding series of figures, first, we try to insert
    elements in the first table. If there's already another element, we overwrite
    it and insert the preexisting element in the other table. We repeat this until
    it is safe to insert the last element.
  prefs: []
  type: TYPE_NORMAL
- en: C++ Hash Tables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we mentioned previously, the lookup operation is quite frequent in most applications.
    However, we may not always encounter positive integers, which are quite easy to
    hash. You are likely to encounter strings most of the time. Consider the example
    of an English language dictionary that we considered earlier. We can store the
    dictionary data by using the words as keys and the word definitions as the values.
    Another example is the hospital records database we considered in *Chapter 1*,
    *Lists, Stacks, and Queues*, where the patients' names may be used as keys, and
    other related information could be stored as values.
  prefs: []
  type: TYPE_NORMAL
- en: The simple modulo function we used earlier to calculate the hash values of integers
    does not work for strings. An easy option is to calculate the modulo of the sum
    of the ASCII values of all the characters. However, all the permutations of characters
    in a string would be quite vast, and this would create a lot of collisions.
  prefs: []
  type: TYPE_NORMAL
- en: C++ provides a function called `std::hash<std::string>(std::string)` that we
    can use to generate hash values of string. It has a built-in algorithm to take
    care of the hashing function. Similarly, C++ provides such functions for all the
    basic types of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, looking at the hash table we implemented in *Exercise 14*, *Hash Table
    with Chaining*, it seems obvious that we can simply templatize it based on the
    data type and make a generic solution to provide a hash function for any given
    type of data. STL provides a couple of solutions for this: `std::unordered_set<Key>`
    and `std::unordered_map<Key, Value>`. An unordered set can only store a set of
    keys, whereas an unordered map can store the keys and their values. So, each unique
    key will have a corresponding value in the container.'
  prefs: []
  type: TYPE_NORMAL
- en: Both of these containers are implemented in the same way – using hash tables
    with chaining. Each row in the hash table is a vector that stores the keys (and
    the values for the map). The rows are known as **buckets**. So, after calculating
    the hash value for a key, it will be placed into one of the buckets. Each bucket
    is also a list to support chaining.
  prefs: []
  type: TYPE_NORMAL
- en: By default, these containers have a maximum load factor of *1*. As soon as the
    number of elements exceeds the size of the hash table, the hash function will
    be changed, the hash values will be recalculated (rehashing), and a larger hash
    table will be rebuilt to bring down the load factor. We can also use the `rehash`
    function to do this manually. This default maximum limit of *1* for the load factor
    can be changed using the `max_load_factor(float)` function. The values will be
    rehashed once the load factor exceeds the defined maximum limit.
  prefs: []
  type: TYPE_NORMAL
- en: These containers provide commonly useful functions such as `find`, `insert`,
    and `erase`. They also provide iterators to iterate over all the elements, as
    well as constructors to create an unordered set and map using other containers,
    such as vectors and arrays. An unordered map also provides `operator[]` so that
    it can return the value for a known key.
  prefs: []
  type: TYPE_NORMAL
- en: We'll look at the implementation of unordered sets and maps in the following
    exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 16: Hash Tables Provided by STL'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we shall implement unordered sets and maps and apply operations
    such as insertion, deletion, and find on these containers. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Include the required headers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s write some simple `print` functions to make our `main` function
    more readable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, add wrappers over the `find` functions to keep the code neat:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, write the `main` function so that we can use `unordered_set` and `unordered_map`,
    and then perform various operations on it. We shall find, insert, and erase the
    elements:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'One of the possible outputs of this program is as follows. The order of elements
    in a set and a map can be different, and hence is called an *unordered* set/map:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, we can insert, find, and erase elements from both containers.
    These operations are working as expected. If we benchmark these operations against
    other containers, such as vector, list, array, deque, and so on, performance will
    be much faster here.
  prefs: []
  type: TYPE_NORMAL
- en: We can store key-value pairs and access the value for any given key using `operator[]`,
    as shown in this exercise. It returns a reference and hence also allows us to
    set the value, and not just retrieve it.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since `operator[]` returns a reference, if the key is not found, it will add
    the default value to the entry.
  prefs: []
  type: TYPE_NORMAL
- en: In the last line, we are getting `map[100] = 0`, even if `100` was never inserted
    in the map. This is because `operator[]` is returning the default value.
  prefs: []
  type: TYPE_NORMAL
- en: If we want to keep track of the number of buckets as they change based on rehashing,
    we can do that using the `bucket_count()` function. There are other functions
    for getting details about other internal parameters as well, such as `load_factor`,
    `max_bucket_count`, and so on. We can also rehash manually using the `rehash`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: Since these containers are implemented using chaining, they are actually storing
    the key/value pairs in different buckets. So, while searching the keys in any
    bucket, we need to compare them for equality. Hence, we need to define the equality
    operator for the key type. Alternatively, we can pass it as another template parameter.
  prefs: []
  type: TYPE_NORMAL
- en: As we can have seen in this exercise, unordered sets and maps do not allow duplicate
    keys. If we need to store duplicate values, we can use `unordered_multiset` or
    `unordered_multimap`. To support multiple values, the insert function does not
    check whether the key already exists in the container. Also, it supports some
    extra functions to retrieve all the items with a particular key. We won't look
    at any more details regarding these containers as it is out of the scope of this
    book.
  prefs: []
  type: TYPE_NORMAL
- en: 'STL provides hash functions for all the basic data types supported by C++.
    So, if we want a custom class or struct as the key type for any of the aforementioned
    containers, we need to implement a hash function inside the `std` namespace. Alternatively,
    we can pass it as a template parameter. However, writing a hash function on our
    own every time is not a good idea because the performance heavily depends on it.
    Designing a hash function requires quite a bit of research and understanding of
    the problem at hand, as well as mathematical skills. Hence, we are leaving it
    out of the scope of this book. For our purposes, we can simply use the `hash_combine`
    function provided in the `boost` library, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, we've defined a hashing struct with `operator()`, which will
    be used by unordered containers. We have also defined the comparator struct with
    `operator()` to support relevant functions. We have passed these structs as template
    parameters. This also allows us to have different types of comparators and hashers
    for different objects.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from simple hash functions such as modulo, there are some complex hash
    functions, known as cryptographic hash functions, such as MD5, SHA-1, and SHA-256\.
    These algorithms are very complex, and they can take any kind of data — even a
    file — as the input value. An important characteristic of cryptographic functions
    is that it is very difficult to determine the actual data from a given hash value
    (also known as reverse hashing), and hence they are used in some of the most secure
    systems. For example, the Bitcoin blockchain uses the SHA-256 algorithm to store
    an important proof of authenticity of the transaction records. Each *block* in
    the blockchain contains an SHA-256 hash value of its previous linked block, and
    the current block's hash is included in the subsequent block. Illegally modifying
    any block invalidates the entire blockchain from that block onwards, since now
    the modified block's hash value will not match with the value that was stored
    in the next block. Even with some of the fastest supercomputers in the world,
    it would take hundreds of years to break this and create forged transaction records.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 6: Mapping Long URLs to Short URLs'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this activity, we'll create a program to implement a service similar to [https://tinyurl.com/](https://tinyurl.com/).
    It can take a very long URL and map it to a small URL that is easy and convenient
    to share. Whenever we enter the short URL, it should retrieve the original URL.
  prefs: []
  type: TYPE_NORMAL
- en: 'We want the following functionalities:'
  prefs: []
  type: TYPE_NORMAL
- en: Store the original and corresponding smaller URL provided by the user efficiently
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieve the original URL based on the given smaller URL if found; otherwise,
    return an error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These high-level steps should help you solve this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a class that contains `unordered_map` as the main data member.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add a function to insert values. This function should take two parameters:
    the original URL and the smaller version of it.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a function to find the actual URL based on a given small URL if present.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution to this activity can be found on page 498.
  prefs: []
  type: TYPE_NORMAL
- en: Bloom Filters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bloom filters are extremely space-efficient compared to hash tables, but at
    the cost of deterministic answers; that is, we get an answer that is unsure. It
    only guarantees that there won't be any false negatives, but there may be false
    positives. In other words, if we get a positive hit, the element may or may not
    be present; but if we get a negative, then the element is definitely not present.
  prefs: []
  type: TYPE_NORMAL
- en: Just like cuckoo hashing, we will use multiple hash functions here. However,
    we'll keep three functions, as two functions cannot achieve decent accuracy. The
    fundamental idea is that instead of storing the actual values, we store an array
    of Booleans indicating whether or not a value is (maybe) present.
  prefs: []
  type: TYPE_NORMAL
- en: To insert an element, we compute the value of all the hash functions and set
    the bits corresponding to all three hash values in the array to *1*. For lookup,
    we compute the value of all the hash functions and check whether all the corresponding
    bits are set to *1*. If so, we return *true*; otherwise, we return *false* (the
    element is not present).
  prefs: []
  type: TYPE_NORMAL
- en: The obvious question is – why is lookup indeterministic? The reason is that
    any bit can be set by multiple elements. So, there is a relatively significant
    probability that all the relevant bits for a particular value (call it *x*) are
    set to *1* because of some other elements that were inserted earlier, although
    *x* was not inserted at all. In that case, the lookup function will still return
    *true*. Hence, we can expect some false positives. The more elements we insert,
    the higher the chances of false positives. However, if one of the bits for *x*
    is not set, then we can say that the element is not present with confidence. So,
    false negatives cannot be a possibility.
  prefs: []
  type: TYPE_NORMAL
- en: The array will be saturated when all the bits in the Boolean array are set to
    *1*. So, the lookup function will always return *true*, and the insertion function
    will not have any effect at all since all the bits are already set to *1*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagrams make this clearer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.16: Inserting elements in a bloom filter](img/C14498_03_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.16: Inserting elements in a bloom filter'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Figure 3.17: Finding elements in a bloom filter](img/C14498_03_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.17: Finding elements in a bloom filter'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Figure 3.18: Finding elements in a bloom filter (continued)](img/C14498_03_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.18: Finding elements in a bloom filter (continued)'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As shown in the preceding diagrams, we are setting the relevant bits based on
    the hash functions, and for insertion, we're doing a bitwise `AND` for lookup
    of the element, as we explained earlier.
  prefs: []
  type: TYPE_NORMAL
- en: We'll implement a Bloom filter in C++ in the following exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 17: Creating Bloom Filters'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we shall create a Bloom filter and try out some basic operations.
    We shall also test for false positives in lookup. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s include the required headers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s create a class for our Bloom filter and add the required data members:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s add the required hash functions. Again, we''ll use very basic hash
    functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we're using single functions, with a parameter called `num`
    determining the hash function, to avoid unnecessary `if`-`else` blocks in other
    functions. This is also easy to expand; we just need to add a case for every hash
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s add a constructor for the Bloom filter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s add a `lookup` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The `lookup` function is really simple, as expected. It checks whether all the
    required bits are set to `1`. If there are a variable number of hash functions,
    we can always loop over all of them to check whether all the corresponding bits
    are set to `1`. To make our words more accurate, we are also saying that a key
    *may be present* due to the possibility of false positives. On the other hand,
    we are completely sure that a key is not present if `lookup` returns negative.
  prefs: []
  type: TYPE_NORMAL
- en: 'Even the insertion function is equally simple:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s add the `main` function so that we can use this class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, there are a couple of false positives, but no false negatives.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the previous techniques, this structure only required 11 bits to store
    this information, as we can see from the constructor of the Bloom filter. Thus,
    we can easily increase the size of the filter and also update the hash functions
    accordingly to achieve much better results. For example, we can increase the size
    of the array to 1,000 (1,023 is used frequently as it is a prime number), and
    we'll still be using less than 130 bytes, which is much less than most other techniques.
    With the increase in the size of the hash table, our hash functions will also
    become *%1023* or similar and will provide better results and a better distribution
    of numbers.
  prefs: []
  type: TYPE_NORMAL
- en: One important point to note here is that since we are not storing the actual
    data in the container, we can use this as a heterogeneous structure; that is,
    as long as our hash functions are good enough, we can insert different types of
    data, such as integers, strings, and doubles, simultaneously in the same Bloom
    filter.
  prefs: []
  type: TYPE_NORMAL
- en: There are some really good use cases of this in real life, especially when the
    amount of data is too huge to search even with hash tables, and some false positives
    would be acceptable. For example, when creating a new email address with an email
    provider such as Gmail or Outlook, there is a check to see whether the email address
    already exists. There are billions of email addresses present in the database,
    and an accurate check for such a basic and frequent query would be very expensive.
    Fortunately, even if the email address is not already taken, it is okay to sometimes
    say that it is taken as it doesn't do any harm. The user will simply choose something
    else. In such cases, using a Bloom filter is a feasible option. We'll see this
    in action in *Activity 7*, *Email Address Validator*.
  prefs: []
  type: TYPE_NORMAL
- en: Another example is the recommendation algorithm for showing new ads that are
    used by services, such as Facebook ads. It will show you a new ad every time you
    check your feed. It can simply store the IDs of ads you have watched in a Bloom
    filter. Then, the ID of a particular ad can be checked against it before showing
    it on your feed. If the check returns that you have watched a particular ad even
    though you haven't (false positive), it will not show that ad. However, this is
    fine since you wouldn't know about it as you haven't seen that ad anyway. This
    way, you can get new ads every time with a very fast lookup.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 7: Email Address Validator'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this activity, we'll create a validator for emails, similar to what we find
    in a lot of email service provides (such as Gmail and Outlook) while signing up.
    We'll use a Bloom filter to check whether an email address has already been taken
    by someone else.
  prefs: []
  type: TYPE_NORMAL
- en: 'These high-level steps should help you complete this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a `BloomFilter` class that can take a number of hash functions and the
    size of the Bloom.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For hashing, use the MD5 algorithm from the OpenSSL library to generate a hash
    value of a given email. MD5 is a 128-bit hashing algorithm. For multiple hash
    functions, we can use each byte as a separate hash value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To add an email in the Bloom filter, we need to set all the bits to *true* that
    are coming from each byte of the hash value we calculated in *step 2*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To find any email, we need to check whether all the relevant bits are *true*
    based on the hash value we calculated in *step 2*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution to this activity can be found on page 503.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we mentioned in the introduction, the lookup problem is encountered in most
    applications in one way or the other. We can use deterministic as well as probabilistic
    solutions as per our needs. In this chapter, we implemented and saw how we can
    use both of them. In the end, we also looked at an example of built-in containers
    for hashing in C++. These containers are extremely useful while we''re writing
    applications as we don''t need to implement them ourselves every time and for
    every type. A simple rule of thumb is this: if we can see a lot of function calls
    to the `find` function for the container, we should go for a lookup-based solution.'
  prefs: []
  type: TYPE_NORMAL
- en: So far, we've seen how we can store data in various types of data structures
    and perform some basic operations. In the upcoming chapters, we'll look at various
    types of algorithm design techniques so that we can optimize those operations,
    starting with divide and conquer.
  prefs: []
  type: TYPE_NORMAL
