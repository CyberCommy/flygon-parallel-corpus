- en: Chapter 8. Spark Databricks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Creating a big data analytics cluster, importing data, and creating ETL streams
    to cleanse and process the data are hard to do, and also expensive. The aim of
    Databricks is to decrease the complexity and make the process of cluster creation,
    and data processing easier. They have created a cloud-based platform, based on
    Apache Spark that automates cluster creation, and simplifies data import, processing,
    and visualization. Currently, the storage is based upon AWS but, in the future,
    they plan to expand to other cloud providers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The same people who designed Apache Spark are involved in the Databricks system.
    At the time of writing this book, the service was only accessible via registration.
    I have been offered a 30-day trial period. Over the next two chapters, I will
    examine the service, and its components, and offer some sample code to show how
    it works. This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Installing Databricks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS configuration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Account management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The menu system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Notebooks and folders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Importing jobs via libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Development environments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Databricks tables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Databricks DbUtils package
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given that this book is provided in a static format, it will be difficult to
    fully examine functionality such as streaming.
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Databricks service, available at the [https://databricks.com/](https://databricks.com/)
    website, is based upon the idea of a cluster. This is similar to a Spark cluster,
    which has already been examined and used in previous chapters. It contains a master,
    workers, and executors. However, the configuration and the size of the cluster
    are automated, depending upon the amount of memory that you specify. Features
    such as security, isolation, process monitoring, and resource management are all
    automatically managed for you. If you have an immediate requirement for a Spark-based
    cluster using 200 GB of memory, for a short period of time, this service can be
    used to dynamically create it, and process your data. You can terminate the cluster
    to reduce your costs when the processing is finished.
  prefs: []
  type: TYPE_NORMAL
- en: Within a cluster, the idea of a Notebook is introduced, along with a location
    for you to create scripts and run programs. Folders can be created within Notebooks,
    which can be based upon Scala, Python, or SQL. Jobs can be created to execute
    the functionality, and can be called from the Notebook code or the imported libraries.
    Notebooks can call Notebook functionality. Also, the functionality is provided
    to schedule jobs, based on time or event.
  prefs: []
  type: TYPE_NORMAL
- en: This provides you with a feel of what the Databricks service provides. The following
    sections will explain each major item that has been introduced. Please keep in
    mind that what is presented here is new and evolving. Also, I used the AWS US
    East (North Virginia) region for this demonstration, as the Asia Sydney region
    currently has limitations that caused the Databricks install to fail.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Databricks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to create this demonstration, I used the AWS offer of a year''s free
    access, which was available at [http://aws.amazon.com/free/](http://aws.amazon.com/free/).
    This has limitations such as 5 GB of S3 storage, and 750 hours of **Amazon Elastic
    Compute Cloud** (**EC2**), but it allowed me low-cost access and reduced my overall
    EC2 costs. The AWS account provides the following:'
  prefs: []
  type: TYPE_NORMAL
- en: An account ID
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An access Key ID
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A secret access Key
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These items of information are used by Databricks to access your AWS storage,
    install the Databricks systems, and create the cluster components that you specify.
    From the moment of the install, you begin to incur AWS EC2 costs, as the Databricks
    system uses at least two running instances without any clusters. Once you have
    successfully entered your AWS and billing information, you will be prompted to
    launch the Databricks cloud.
  prefs: []
  type: TYPE_NORMAL
- en: '![Installing Databricks](img/B01989_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Having done this, you will be provided with a URL to access your cloud, an
    admin account, and password. This will allow you to access the Databricks web-based
    user interface, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Installing Databricks](img/B01989_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This is the welcome screen. It shows the menu bar at the top of the image, which,
    from left to right, contains the menu, search, help, and account icons. While
    using the system, there may also be a clock-faced icon that shows the recent activity.
    From this single interface, you may search through help screens, and usage examples
    before creating your own clusters and code.
  prefs: []
  type: TYPE_NORMAL
- en: AWS billing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Please note that, once you have the Databricks system installed, you will start
    incurring the AWS EC2 storage costs. Databricks attempts to minimize your costs
    by keeping EC2 resources active for a full charging period. For instance if you
    terminate a Databricks cluster the cluster-based EC2 instances will still exist
    for the hour in which AWS bills for them. In this way, Databricks can reuse them
    if you create a new cluster. The following screenshot shows that, although I am
    using a free AWS account, and though I have carefully reduced my resource usage,
    I have incurred AWS EC2 costs in a short period of time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![AWS billing](img/B01989_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: You need to be aware of the Databricks clusters that you create, and understand
    that, while they exist and are used, AWS costs are being incurred. Only keep the
    clusters that you really require, and terminate any others.
  prefs: []
  type: TYPE_NORMAL
- en: In order to examine the Databricks data import functionality, I also created
    an AWS S3 bucket, and uploaded data files to it. This will be explained later
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Databricks menus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By selecting the top-left menu icon on the Databricks web interface, it is possible
    to expand the menu system. The following screenshot shows the top-level menu options,
    as well as the **Workspace** option, expanded to a folder hierarchy of `/folder1/folder2/`.
    Finally, it shows the actions that can be carried out on `folder2`, that is, creating
    a notebook, creating a dashboard, and more.
  prefs: []
  type: TYPE_NORMAL
- en: '![Databricks menus](img/B01989_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: All of these actions will be expanded in future sections. The next section will
    examine account management, before moving on to clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Account management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Account management is quite simplified within Databricks. There is a default
    Administrator account and subsequent accounts can be created, but you need to
    know the Administrator password to do so. Passwords need to be more than eight
    characters long; they should contain at least one digit, one upper case character,
    and one non-alphanumeric character. **Account** options can be accessed from the
    top-right menu option, shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Account management](img/B01989_08_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This also allows the user to logout. By selecting the account setting, you
    can change your password. By selecting the **Accounts** menu option, an **Accounts**
    list is generated. There, you will find an option to **Add Account**, and each
    account can be deleted via an **X** option on each account line, as shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Account management](img/B01989_08_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'It is also possible to reset the account passwords from the accounts list.
    Selecting the **Add Account** option creates a new account window that requires
    an email address, a full name, the administrator password, and the user''s password.
    So, if you want to create a new user, you need to know your Databricks instance
    Administrator password. You must also follow the rules for new passwords, which
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Minimum of eight characters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Must contain at least one digit in the range: 0-9'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Must contain at least one upper case character in the range: A-Z'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Must contain at least one non-alphanumeric character: !@#$%![Account management](img/B01989_08_07.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next section will examine the **Clusters** menu option, and will enable
    you to manage your own Databricks Spark clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Selecting the **Clusters** menu option provides a list of your current Databricks
    clusters and their status. Of course, currently you have none. Selecting the **Add
    Cluster** option allows you to create one. Note that the amount of memory you
    specify determines the size of your cluster. There is a minimum of 54 GB required
    to create a cluster with a single master and worker. For each additional 54 GB
    specified, another worker is added.
  prefs: []
  type: TYPE_NORMAL
- en: '![Cluster management](img/B01989_08_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The following screenshot is a concatenated image, showing a new cluster called
    `semclust1` being created and in a **Pending** state. While **Pending**, the cluster
    has no dashboard, and the cluster nodes are not accessible.
  prefs: []
  type: TYPE_NORMAL
- en: '![Cluster management](img/B01989_08_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Once created the cluster memory is listed and it''s status changes from **Pending**
    to **Running**. A default dashboard has automatically been attached, and the Spark
    master and worker user interfaces can be accessed. It is important to note here
    that Databricks automatically starts and manages the cluster processes. There
    is also an **Option** column to the right of this display that offers the ability
    to **Configure**, **Restart**, or **Terminate** a cluster as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cluster management](img/B01989_08_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: By reconfiguring a cluster, you can change its size. By adding more memory,
    you can add more workers. The following screenshot shows a cluster, created at
    the default size of 54 GB, having its memory extended to `108` GB.
  prefs: []
  type: TYPE_NORMAL
- en: '![Cluster management](img/B01989_08_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Terminating a cluster removes it, and it cannot be recovered. So, you need to
    be sure that deletion is the correct course of action. Databricks prompts you
    to confirm your action before the termination actually takes place.
  prefs: []
  type: TYPE_NORMAL
- en: '![Cluster management](img/B01989_08_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'It takes time for a cluster to be both, created and terminated. During termination,
    the cluster is marked with an orange banner, and a state of **Terminating**, as
    shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cluster management](img/B01989_08_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note that the cluster type in the previous screenshot is shown to be **On-demand**.
    When creating a cluster, it is possible to select a check box called **Use spot
    instances to create a spot cluster**. These clusters are cheaper than the on-demand
    clusters, as they bid for a cheaper AWS spot price. However, they can be slower
    to start than the on-demand clusters.
  prefs: []
  type: TYPE_NORMAL
- en: The Spark user interfaces are the same as those you would expect on a non-Databricks
    Spark cluster. You can examine workers, executors, configuration, and log files.
    As you create clusters, they will be added to your cluster list. One of the clusters
    will be used as the cluster where the dashboards are run. This can be changed
    by using the **Make Dashboard Cluster** option. As you add libraries and Notebooks
    to your cluster, the cluster details entry will be updated with a count of the
    numbers added.
  prefs: []
  type: TYPE_NORMAL
- en: The only thing that I would say about the Databricks Spark user interface option
    at this time, because it is familiar, is that it displays the Spark version that
    is used. The following screenshot, extracted from the master user interface, shows
    that the Spark version being used (1.3.0) is very up-to-date. At the time of writing,
    the latest Apache Spark release was 1.3.1, dated 17 April, 2015.
  prefs: []
  type: TYPE_NORMAL
- en: '![Cluster management](img/B01989_08_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The next section will examine Databricks Notebooks and folders—how to create
    them, and how they can be used.
  prefs: []
  type: TYPE_NORMAL
- en: Notebooks and folders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Notebook is a special type of Databricks folder that can be used to create
    Spark scripts. Notebooks can call the Notebook scripts to create a hierarchy of
    functionality. When created, the type of Notebook must be specified (Python, Scala,
    or SQL), and a cluster can then specify that the Notebook functionality can be
    run against it. The following screenshot shows the Notebook creation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Notebooks and folders](img/B01989_08_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that a menu option, to the right of a Notebook session, allows the type
    of Notebook that is to be changed. The following example shows that a Python notebook
    can be changed to **Scala**, **SQL**, or **Markdown**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Notebooks and folders](img/B01989_08_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note that a Scala Notebook cannot be changed to Python, and a Python Notebook
    cannot be changed to Scala. The terms Python, Scala, and SQL are well understood
    as the development languages, however, **Markdown** is new. Markdown allows formatted
    documentation to be created from formatted commands in text. A simple reference
    can be found at [https://forums.databricks.com/static/markdown/help.html](https://forums.databricks.com/static/markdown/help.html).
  prefs: []
  type: TYPE_NORMAL
- en: This means that formatted comments can be added to the Notebook session as scripts
    are created. Notebooks are further subdivided into cells, which contain the commands
    to be executed. Cells can be moved within a Notebook by hovering over the top-left
    corner, and dragging them into position. New cells can be inserted into a cell
    list within a Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, using the `%sql` command, within a Scala or Python Notebook cell, allows
    SQL syntax to be used. Typically, the key combination of *Shift* + *Enter* causes
    text blocks in a Notebook or folder to be executed. Using the `%md` command allows
    Markdown comments to be added within a cell. Also, comments can be added to a
    Notebook cell. The menu options available at the top-right section of a Notebook
    cell, shown in the following screenshot, shows comment, as well as the minimize
    and maximize options:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Notebooks and folders](img/B01989_08_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Multiple web-based sessions may share a Notebook. The actions that occur within
    the Notebook will be populated to each web interface viewing it. Also, the Markdown
    and comment options can be used to enable communication between users to aid the
    interactive data investigation between a distributed group.
  prefs: []
  type: TYPE_NORMAL
- en: '![Notebooks and folders](img/B01989_08_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The previous screenshot shows the header of a Notebook session for **notebook1**.
    It shows the Notebook name and type (**Scala**). It also shows the option to lock
    the Notebook to make it read only, as well as the option to detach it from its
    cluster. The following screenshot shows the creation of a folder within a Notebook
    workspace:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Notebooks and folders](img/B01989_08_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'A drop-down menu, from the **Workspace** main menu option, allows for the creation
    of a folder—in this case, named `folder1`. The later sections will describe other
    options in this menu. Once created and selected, a drop-down menu from the new
    folder called `folder1` shows the actions associated with it in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Notebooks and folders](img/B01989_08_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: So, a folder can be exported to a DBC archive. It can be locked, or cloned to
    create a copy. It can also be renamed, or deleted. Items can be imported into
    it; for instance, files, which will be explained by example later. Also, new notebooks,
    dashboards, libraries, and folders can be created within it.
  prefs: []
  type: TYPE_NORMAL
- en: In the same way as actions can be carried out against a folder, a Notebook has
    a set of possible actions. The following screenshot shows the actions available
    via a drop-down menu for the Notebook called `notebook1`, which is currently attached
    to the running cluster called `semclust1`. It is possible to rename, delete, lock,
    or clone a Notebook. It is also possible to detach it from its current cluster,
    or attach it if it is detached. It is also possible to export the Notebook to
    a file, or a DBC archive.
  prefs: []
  type: TYPE_NORMAL
- en: '![Notebooks and folders](img/B01989_08_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: From the folder **Import** option, files can be imported to a folder. The following
    screenshot shows the file drop-option window that is invoked if this option is
    selected. It is possible to either drop a file onto the upload pane from the local
    server, or click on this pane to open a navigation browser to search the local
    server for files to upload.
  prefs: []
  type: TYPE_NORMAL
- en: '![Notebooks and folders](img/B01989_08_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note that the files that are uploaded need to be of a specific type. The following
    screenshot shows the supported file types. This is a screenshot taken from the
    file browser when browsing for a file to upload. It also makes sense. The supported
    file types are Scala, SQL, and Python; as well as DBC archives and JAR file libraries.
  prefs: []
  type: TYPE_NORMAL
- en: '![Notebooks and folders](img/B01989_08_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Before leaving this section, it should also be noted that Notebooks and folders
    can be dragged and dropped to change their position. The next section will examine
    Databricks jobs and libraries via simple worked examples.
  prefs: []
  type: TYPE_NORMAL
- en: Jobs and libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Within Databricks, it is possible to import JAR libraries and run the classes
    in them on your clusters. I will create a very simple piece of Scala code to print
    out the first 100 elements of the Fibonacci series as `BigInt` values, locally
    on my Centos Linux server. I will compile my class into a JAR file using SBT,
    run it locally to check the result, and then run it on my Databricks cluster to
    compare the results. The code looks as following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Not that the most elegant piece of code, or the best way to create Fibonacci,
    but I just want a sample JAR and class to use with Databricks. When run locally,
    I get the first 100 terms, which look as follows (I''ve clipped this data to save
    space):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The library that has been created is called `data-bricks_2.10-1.0.jar`. From
    my folder menu, I can create a new Library using the menu drop-down option. This
    allows me to specify the library source as a JAR file, name the new library, and
    load the library JAR file from my local server. The following screenshot shows
    an example of this process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Jobs and libraries](img/B01989_08_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'When the library has been created, it can be attached to the cluster called
    `semclust1`, my Databricks cluster, using the **Attach** option. The following
    screenshot shows the new library in the process of attaching:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Jobs and libraries](img/B01989_08_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following example, a job called **job2** has been created by selecting
    the **jar** option on the **Task** item. For the job, the same JAR file has been
    loaded and the class `db_ex1` has been assigned to run in the library. The cluster
    has been specified as on-demand, meaning that a cluster will be created automatically
    to run the job. The **Active runs** section shows the job running in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Jobs and libraries](img/B01989_08_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Once run, the job is moved to the **Completed runs** section of the display.
    The following screenshot, for the same job, shows that it took `47` seconds to
    run, that it was launched manually, and that it succeeded.
  prefs: []
  type: TYPE_NORMAL
- en: '![Jobs and libraries](img/B01989_08_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: By selecting the run named **Run 1** in the previous screenshot, it is possible
    to see the run output. The following screenshot shows the same result as the local
    run, displayed from my local server execution. I have clipped the output text
    to make it presentable and readable on this page, but you can see that the output
    is the same.
  prefs: []
  type: TYPE_NORMAL
- en: '![Jobs and libraries](img/B01989_08_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: So, even from this very simple example, it is obvious that it is possible to
    develop applications remotely, and load them onto a Databricks cluster as JAR
    files in order to execute. However, each time a Databricks cluster is created
    on AWS EC2 storage, the Spark URL changes, so the application must not hard-code
    details such as the Spark master URL. Databricks will automatically set the Spark
    URL.
  prefs: []
  type: TYPE_NORMAL
- en: When running the JAR file classes in this way, it is also possible to define
    class parameters. The jobs may be scheduled to run at a given time, or periodically.
    The job timeouts, and alert email addresses may also be specified.
  prefs: []
  type: TYPE_NORMAL
- en: Development environments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It has been shown that scripts can be created in Notebooks in Scala, Python,
    or SQL, but it is also possible to use an IDE such as IntelliJ or Eclipse to develop
    code. By installing an SBT plugin into this development environment, it is possible
    to develop code for your Databricks environment. The current release of Databricks,
    as I write this book, is 1.3.2d. The **Release Notes** link, under **New Features**
    on the start page, contains a link to the IDE integration, which is `https://dbc-xxxxxxx-xxxx.cloud.databricks.com/#shell/1547`.
  prefs: []
  type: TYPE_NORMAL
- en: The URL will be of this form, with the section starting with `dbc` changed to
    match the URL for the Databricks cloud that you will create. I won't expand on
    this here, but leave it to you to investigate. In the next section, I will investigate
    the Databricks table data processing functionality.
  prefs: []
  type: TYPE_NORMAL
- en: Databricks tables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Databricks **Tables** menu option allows you store your data in a tabular
    form with an associated schema. The **Tables** menu option allows you to both
    create a table, and refresh your tables list, as the following screenshot shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Databricks tables](img/B01989_08_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Data import
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can create tables via data import, and specify the table structure, at the
    same time, in terms of column names and types. If the data that is being imported
    has a header, then the column names can be taken from that, although all the column
    types are assumed to be strings. The following screenshot shows a concatenated
    view of the data import options and form, available when creating a table. The
    import file location options are **S3**, **DBFS**, **JDBC**, and **File**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Data import](img/B01989_08_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The previous screenshot shows **S3** selected. In order to browse my **S3**
    bucket for a file to import to a table, I will need to enter the **AWS Key ID**,
    the **Secret Access Key**, and the **AWS S3 Bucket Name**. Then, I could browse,
    select the file, and create a table via preview. In the following screenshot,
    I have selected the **File** option:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Data import](img/B01989_08_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'I can either drop my file to import into the upload frame in the following
    screenshot, or click on the frame to browse the local server to select a file
    to upload. Once a file is selected, it is then possible to define the data column
    delimiter, and whether the data contains a header row. It is possible to preview
    the data, and change the column names and data types. It is also possible to specify
    the new table name, and the file type. The following screenshot shows a sample
    file data load to create the table called `shuttle`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Data import](img/B01989_08_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Once created, the menu table list can be refreshed and the table schema viewed
    to confirm the column names and types. In this way, a sample of the table data
    can also be previewed. The table can now be viewed and accessed from an SQL session.
    The following screenshot shows that the **shuttle** table is visible using the
    `show tables` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Data import](img/B01989_08_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Once imported, the data in this table can also be accessed via an SQL session.
    The following screenshot shows a simple SQL session statement to show the data
    extracted from the new **shuttle** table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Data import](img/B01989_08_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: So, this provides the means to import multiple tables from a variety of data
    sources, and create a complex schema in order to filter and join the data by columns
    and rows, just as you would in a traditional, relational database. It provides
    a familiar approach to big data processing.
  prefs: []
  type: TYPE_NORMAL
- en: This section has described the process by which tables can be created via data
    import, but what about creating tables programmatically, or creating tables as
    external objects? The following sections will provide examples of this approach
    to table management.
  prefs: []
  type: TYPE_NORMAL
- en: External tables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Databricks allows you to create tables against external resources, such as AWS
    S3 files, or local file system files. In this section, I will create an external
    table against an S3-based bucket, path, and a set of files. I will also examine
    both the permissions required in AWS and the access policy used. The following
    screenshot shows an AWS S3 bucket called **dbawss3test2** being created. Permissions
    have been granted to everyone to access the list. I am not suggesting that you
    do this, but ensure that your group can access your bucket.
  prefs: []
  type: TYPE_NORMAL
- en: '![External tables](img/B01989_08_35.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Also, a policy has been added to aid access. In this case, anonymous users
    have been granted read-only access to the bucket and sub contents. You can create
    a more complex policy to limit the access to your group and assorted files. The
    following screenshot shows the new policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![External tables](img/B01989_08_36.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'With an access policy, and a bucket created with the correct access policy,
    I can now create folders and upload files for use with a Databricks external table.
    As the following screenshot shows, I have done just that. The uploaded file has
    ten columns in CSV file format:'
  prefs: []
  type: TYPE_NORMAL
- en: '![External tables](img/B01989_08_37.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that the AWS S3 resources have been set up, they need to be mounted to
    Databricks, as the Scala-based example shows next. I have removed my AWS and secret
    keys from the script for security purposes. Your mounted directory will need to
    start with `/mnt` and any of the `/` characters, and your secret key value will
    need to be replaced with `%2F`. The `dbutils.fs` class is being used to create
    the mount and the code executes within a second, as the following result shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![External tables](img/B01989_08_38.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now, an external table can be created against this mounted path and the files
    that it contains using a Notebook-based SQL session, as the following screenshot
    shows. The table called `s3test1` has been created against the files that the
    mounted directory contains, and a delimiter is specified as a comma, in order
    to parse the CSV-based content.
  prefs: []
  type: TYPE_NORMAL
- en: '![External tables](img/B01989_08_39.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The **Tables** menu option now shows that the **s3test1** table exists, as
    shown in the following screenshot. So, it should be possible to run some SQL against
    this table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![External tables](img/B01989_08_40.jpg)'
  prefs: []
  type: TYPE_IMG
- en: I have run a `SELECT` statement in an SQL-based Notebook session to get a row
    count from the external table, using the `COUNT(*)` function, as shown in the
    following screenshot. It can be see that the table contains **14500** rows.
  prefs: []
  type: TYPE_NORMAL
- en: '![External tables](img/B01989_08_41.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'I will now add another file to the S3-based folder. In this case, it is just
    a copy of the first file in CSV format, so the row count in the external table
    should double. The following screenshot shows the file that is added:'
  prefs: []
  type: TYPE_NORMAL
- en: '![External tables](img/B01989_08_42.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Running the same `SELECT` statement against the external table does indeed
    provide a doubled row count of **29000** rows. The following screenshot shows
    the SQL statement, and the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![External tables](img/B01989_08_43.jpg)'
  prefs: []
  type: TYPE_IMG
- en: So, it is easily possible to create external tables within Databricks, and run
    SQL against content that is dynamically changed. The file structure will need
    to be uniform, and the S3 bucket access must be defined if using AWS. The next
    section will examine the DbUtils package provided with Databricks.
  prefs: []
  type: TYPE_NORMAL
- en: The DbUtils package
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The previous Scala-based script, which uses the DbUtils package, and creates
    the mount in the last section, only uses a small portion of the functionality
    of this package. In this section, I would like to introduce some more features
    of the DbUtils package, and the **Databricks File System** (**DBFS**). The help
    option within the DbUtils package can be called within a Notebook connected to
    a Databricks cluster, to learn more about its structure and functionality. As
    the following screenshot shows, executing `dbutils.fs.help()` in a Scala Notebook
    provides help on fsutils, cache, and the mount-based functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The DbUtils package](img/B01989_08_44.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'It is also possible to obtain help on individual functions, as the text in
    the previous screenshot shows. The example in the following screenshot explains
    the **cacheTable** function, providing descriptive text and a sample function
    call with the parameter and return types:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The DbUtils package](img/B01989_08_45.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The next section will briefly examine the DBFS before moving on to examining
    more of the `dbutils` functionality.
  prefs: []
  type: TYPE_NORMAL
- en: Databricks file system
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The DBFS can be accessed using URL's of the `dbfs:/*` form, and using the functions
    available within `dbutils.fs`.
  prefs: []
  type: TYPE_NORMAL
- en: '![Databricks file system](img/B01989_08_46.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The previous screenshot shows the `/mnt` file system being examined using the
    `ls` function, and then showing mount directories—`s3data` and `s3data1`. These
    were the directories created during the previous Scala S3 mount example.
  prefs: []
  type: TYPE_NORMAL
- en: Dbutils fsutils
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `fsutils` group of functions, within the `dbutils` package, covers functions
    such as `cp`, `head`, `mkdirs`, `mv`, `put`, and `rm`. The help calls, shown previously,
    can provide more information about them. You can create a directory on DBFS using
    the `mkdirs` call, as shown next. Note that I have created a number of directories
    under `dbfs:/`, named as `data*` in this session. The following example has created
    the directory called `data2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dbutils fsutils](img/B01989_08_47.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The previous screenshot shows by executing an `ls` that there are many default
    directories that already exist on DBFS. For instance, see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`/tmp` is a temporary area'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`/mnt` is a mount point for remote directories—that is, S3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`/user` is a user storage area that currently contains Hive'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`/mount` is an empty directory'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`/FileStore` is a storage area for tables, JARs, and job JARs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`/databricks-datasets` is datasets provided by Databricks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `dbutils` copy command, shown next, allows a file to be copied to a DBFS
    location. In this instance, the `external1.txt` file had been copied to the `/data2`
    directory, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dbutils fsutils](img/B01989_08_48.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The `head` function can be used to return the first maxBytes characters from
    the head of a file on DBFS. The following example shows the format of the `external1.txt`
    file. This is useful, as it tells me that this is a CSV file, and so shows me
    how to process it.
  prefs: []
  type: TYPE_NORMAL
- en: '![Dbutils fsutils](img/B01989_08_49.jpg)'
  prefs: []
  type: TYPE_IMG
- en: It is also possible to move files within DBFS. The following screenshot shows
    the `mv` command being used to move the `external1.txt` file from the directory
    `data2` to the directory called `data1`. The `ls` command is then used to confirm
    the move.
  prefs: []
  type: TYPE_NORMAL
- en: '![Dbutils fsutils](img/B01989_08_50.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, the remove function (`rm`) is used to remove the file called `external1.txt`,
    which was just moved. The following `ls` function call shows that the file no
    longer exists within the `data1` directory, because there is no `FileInfo` record
    in the function output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dbutils fsutils](img/B01989_08_51.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The DbUtils cache
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The cache functionality, within DbUtils, provides the means to cache (and uncache)
    both tables and files to DBFS. Actually, the tables are saved as files also to
    the DBFS directory called `/FileStore`. The following screenshot shows that the
    cache functions are available:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The DbUtils cache](img/B01989_08_52.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The DbUtils mount
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The mount functionality allows you to mount remote file systems, refresh mounts,
    display mount details, and unmount specific mounted directories. An example of
    an S3 mount was already given in the previous sections, so I won't repeat it here.
    The following screenshot shows the output from the `mounts` function. The `s3data`
    and `s3data1` mounts have been created by me. The other two mounts for root and
    datasets already existed. The mounts are listed in a sequence of the `MountInfo`
    objects. I have rearranged the text to be more meaningful, and to be better presented
    on the page.
  prefs: []
  type: TYPE_NORMAL
- en: '![The DbUtils mount](img/B01989_08_53.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter has introduced Databricks. It shows how the service can be accessed,
    and also shows how it uses AWS resources. Remember that, in the future, the people
    who invented Databricks plan to support other cloud-based platforms, such as Microsoft
    Azure. I thought that it was important to introduce Databricks, because the same
    people who were involved in the development of Apache Spark are involved in this
    system. The natural progression seems to be Hadoop, Spark, then Databricks.
  prefs: []
  type: TYPE_NORMAL
- en: I will continue the Databricks investigation in the next chapter, because important
    features, such as visualization, have not yet been examined. Also, the major Spark
    functionality modules called GraphX, streaming, MLlib, and SQL have not been introduced
    in Databricks terms. How easy is it to use these modules within Databricks to
    process real data? Read on to find out.
  prefs: []
  type: TYPE_NORMAL
