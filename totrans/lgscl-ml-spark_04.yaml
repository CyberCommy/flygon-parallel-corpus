- en: Chapter 4. Extracting Knowledge through Feature Engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Which features should be used to create a predictive model is not only a vital
    question, but also a difficult question that may require deep knowledge of the
    problem domain to be answered. It is possible to automatically select those features
    in data that are most useful or most relevant for the problem someone is working
    on. Considering these questions, this chapter covers Feature Engineering in detail,
    explaining the reasons why to apply it along with some best practices in feature
    engineering.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to this, we will provide the theoretical descriptions and examples
    of feature extraction, transformations, and selection applied in large scale machine
    learning techniques, using both Spark MLlib and Spark ML APIs. Furthermore, this
    chapter also covers the basic idea of advanced feature engineering (also known
    as extreme feature engineering).
  prefs: []
  type: TYPE_NORMAL
- en: Please note that you will require having R and RStudio installed on your machine
    prior to proceeding with this chapter since an example towards exploratory data
    analysis will be shown using R.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, the following topics will be covered throughout this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: The state of the art of feature engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practices in feature engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature engineering with Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced feature engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The state of the art of feature engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Even though feature engineering is an informal topic, however, it is considered
    as an essential part in applied machine learning. Andrew Ng, who is one of the
    leading scientists in the area of machine learning, defined the term feature engineering
    in his book *Machine Learning and AI via Brain simulations* (see also, feature
    engineering defined at: [https://en.wikipedia.org/wiki/Andrew_Nghttps://en.wikipedia.org/wiki/Andrew_Ng](https://en.wikipedia.org/wiki/Andrew_Nghttps://en.wikipedia.org/wiki/Andrew_Ng))
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Coming up with features is difficult, time-consuming, requires expert knowledge.
    Applied machine learning is basically feature engineering.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Based on the preceding definition, we can argue that feature engineering is
    actually human intelligence, not artificial intelligence. Moreover, we will explain
    what feature engineering is from other perspectives. Feature engineering also
    can be defined as the process of converting raw data into useful features (also
    often called feature vectors). The features help you in better representation
    of the underlying problem to the predictive models eventually; so that the predictive
    modeling can be applied to new data types to avail high predictive accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, we can define the term feature engineering as a software engineering
    process of using or reusing someone's advanced domain knowledge about the underlying
    problem and the available data to create features, which makes the machine learning
    algorithms work with ease.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is how we define the term feature engineering. If you read it carefully,
    you will see four dependencies in these definitions:'
  prefs: []
  type: TYPE_NORMAL
- en: The problem itself
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The raw data you will be working with to find out useful patterns or features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The type of the machine learning problem or classes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The predictive models you'll be using
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now based on these four dependencies, we can conclude a workflow out of this.
    First, you have to understand your problem itself, then you have to know your
    data and if it is in good order, if not, process your data to find a certain pattern
    or features so that you can build your model.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have identified the features, you need to know which categories your
    problem falls under. In other words, you have to be able to identify if it is
    a classification, clustering, or a regression problem based on the features. Finally,
    you will build the model to make a prediction on the test set or validation set
    using a well-known method such as random forest or **Support Vector Machine**
    (**SVMs**).
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this chapter, you will see and argue that feature engineering is
    an art that deals with uncertain and often unstructured data. It's also true that
    there are many well-defined procedures of applying the classification, clustering,
    regression model, or methods such as SVMs that are both methodical and provable;
    however, the data is a variable and comes often with a variety of characteristics
    at different times.
  prefs: []
  type: TYPE_NORMAL
- en: Feature extraction versus feature selection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You will get to know when and how you might be good at deciding which procedures
    to be followed by practice from the empirical apprenticeship. The main tasks involved
    in feature engineering are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data exploration and feature extraction**: This is the process of uncovering
    the hidden treasure in the raw data. Generally, this process does not vary much
    by algorithms consuming the features. However, a better understanding of the hands-on
    experience, business domain, and intuition play a vital role in this regard.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature selection**: This is the process for deciding which features to be
    selected based on the machine learning problem you are dealing with. You can use
    diverse techniques for selecting the features; however, it may vary in algorithms
    and using the features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Importance of feature engineering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When the ultimate goal is to achieve the most accurate and reliable results
    from a predictive model, you have to invest your best in what you have. The best
    investment, in this case, would be the three parameters: time and patience, data
    and availability, and best algorithm. However, *how do you get the most valuable
    treasures out of your data for the predictive modeling?* is the problem that the
    process and practice of feature engineering solves in an emerging way.'
  prefs: []
  type: TYPE_NORMAL
- en: In fact, the success of most of the machine learning algorithms depends on how
    you properly and intelligently utilize value and present your data. It is often
    agreed that the hidden treasure (that is, features or patterns) out of your data
    will directly stimulate the results of the predictive model.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, better features (that is, what you extract and select from the datasets)
    mean better results (that is, the results you will achieve from the model). However,
    please remember one thing before you generalize the earlier statement for your
    machine learning model, you need a great feature which is true nonetheless with
    the properties that describe the structures inherent in your data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, better features signify three pros: flexibility, tuning, and better
    results:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Better features (better flexibility)**: If you are successful in extracting
    and selecting the better features, you will get better results for sure, even
    if you choose a non-optimal or wrong model. In fact, optimal or most suitable
    models can be selected or picked up based on the good structure of the original
    data you have. In addition to this, good features will allow you to use less complex
    but efficient, faster, easily understandable, and easy to maintain models eventually.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Better features (better tuning)**: As we already stated, if you do not choose
    your machine learning model intelligently or if your features are not in good
    shape, you are more likely to get worse results out of the ML model. However,
    even if you choose some wrong parameters during building the model and if you
    do have some well-engineered features, still you can expect better results out
    of the model. Furthermore, you don''t need to worry much or even work harder to
    choose the most optimal models and related parameters. The reason is simple, which
    is the good feature, you have actually understood the problem well and ready to
    use the better represented by all the data by characterizing the problem itself.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Better features (better results)**: You are most likely to get better results
    even if you spent most of your efforts in feature engineering towards better features
    selections.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We also suggest readers not to be overconfident with only the features. The
    preceding statements are often true; however, sometimes they are misleading. We
    would like to clear the preceding statements further. Actually, if you receive
    the best predictive results from a model, it is actually of three factors: the
    model you selected, the data you had, and the features you had prepared.'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, if you have enough time and computational resources, always try to
    use the standard model since often the simplicity does not imply better accuracy.
    Nonetheless, better features will contribute the most out of these three factors.
    One thing you should know is that, unfortunately, even if you master feature engineering
    emanates with many hands-on practices, and research what others are doing well
    in the state of the arts, some machine learning projects fail at the very end.
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering and data exploration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Very often, an intelligent choice for both training and test samples out of
    better features leads to better solutions. Although in the previous section we
    argued that there are two tasks in the feature engineering: feature extraction
    from the raw data and feature selection. However, there is no definite or fixed
    path for feature engineering.'
  prefs: []
  type: TYPE_NORMAL
- en: Conversely, the whole step in feature engineering is very much directed by the
    available raw data. If the data is well-structured you would be feeling lucky.
    Nonetheless, the reality is often that the raw data comes from diverse sources
    in multiple formats. Therefore, exploring this data is very important before you
    proceed to feature extraction and feature selection.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We suggest you to figure out the data skewness and kurtosis using the histogram
    and outliers using the box-plot and bootstrapping the data using Data Sidekick
    techniques (introduced by Abe Gong) in the literature (see at: [https://curiosity.com/paths/abe-gong-building-for-resilience-solid-2014-keynote-oreilly/#abe-gong-building-for-resilience-solid-2014-keynote-oreilly](https://curiosity.com/paths/abe-gong-building-for-resilience-solid-2014-keynote-oreilly/#abe-gong-building-for-resilience-solid-2014-keynote-oreilly)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following questions need to be answered and known by means of data exploration
    before applying the feature engineering:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the percentage of the total data being present or not having null or
    missing values for all the available fields? Then try to handle those missing
    values and interpret them well without losing the data semantics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the correlation between the fields? What is the correlation of each
    field with the predicted variable? What values do they take (that is, categorical
    or non-categorical, numerical or alpha-numerical, and so on)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then find out if the data distribution is skewed or not. You can identify the
    skewness by seeing the outliers or long tail (slightly skewed to the right or
    positively skewed, slightly skewed to the left or negatively skewed, as shown
    in *Figure 1*). Now identify if the outliers contribute towards making the prediction
    or not.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After that, observe the data kurtosis. More technically, check if your kurtosis
    is mesokurtic (less than but almost equal to 3), leptokurtic (more than 3), or
    platykurtic (less than 3). Note, the kurtosis of any univariate normal distribution
    is considered to be 3.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now play with the tail and observe (do the predictions get better?) what happens
    when you remove the long tail?![Feature engineering and data exploration](img/00063.jpeg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Figure 1: Skewness of the data distribution (x-axis = data, y-axis = density).'
  prefs: []
  type: TYPE_NORMAL
- en: You can use simple visualization tools such as density plots for doing this,
    as explained by the following example.
  prefs: []
  type: TYPE_NORMAL
- en: Example 1\. Suppose you are interested in fitness walking and you walked at
    a sports ground or countryside in the last four weeks (excluding the weekends).
    You spent the following time (in minutes to finish a 4 KM walking track):15, 16,
    18, 17.16, 16.5, 18.6, 19.0, 20.4, 20.6, 25.15, 27.27, 25.24, 21.05, 21.65, 20.92,
    22.61, 23.71, 35, 39, and 50\. Now let's compute and interpret the skewness and
    kurtosis of these values using R.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will show how to configure and work with SparkR in [Chapter 10](part0079_split_000.html#2BASE2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 10.  Configuring and Working with External Libraries"), *Configuring
    and Working with External Libraries* and show how to execute the same code on
    SparkR. The reason behind this is some plotting packages such as `ggplot2` are
    still not implemented in the current version of Spark used for SparkR directly.
    However, the `ggplot2` is available as a combined package named `ggplot2.SparkR`
    on GitHub, which can be installed and configured using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '**`devtools::install_github("SKKU-SKT/ggplot2.SparkR")`**'
  prefs: []
  type: TYPE_NORMAL
- en: However, there are numerous dependencies that need to be ensured before and
    during the configuration process. Therefore, we should resolve this issue in a
    later chapter instead. For the time being, we assume you have basic knowledge
    of using R and if you have R installed and configured on your computer then please
    use the following steps. However, a step-by-step example on how to install and
    configure SparkR using RStudio will be shown in [Chapter 10](part0079_split_000.html#2BASE2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 10.  Configuring and Working with External Libraries"), *Configuring
    and Working with External Libraries*.
  prefs: []
  type: TYPE_NORMAL
- en: Now just copy the following code snippets and try to execute to make sure you
    have the correct value of the Skewness and Kurtosis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the `moments` package for calculating Skewness and Kurtosis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `moments` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Make a vector for the time you have taken during the workout:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert the time into DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now calculate the `skewness`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now calculate the `kurtosis`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '**Interpretation of the result**: The skewness of your workout time is 1.769592,
    which means your data is skewed to the right or positively skewed. The kurtosis,
    on the other hand, is 5.650427, which means the distribution of the data is leptokurtic.
    Now to check the outliers or tails check the following histogram. Again, for simplicity,
    we will use R to plot the density plot that will interpret your workout time.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install `ggplot2package` for plotting the histogram:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `moments` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now plot the histogram using the `qplot()` method of `ggplot2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![Feature engineering and data exploration](img/00144.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. Histogram of the workout time (right-skewed).
  prefs: []
  type: TYPE_NORMAL
- en: 'The interpretation presented in *Figure 2* of the distribution of data (workout
    times) shows the density plot is skewed to the right so is leptokurtic. Besides
    the density plot, you can also look at the box-plots for each individual feature.
    Where the box plot displays the data distribution based on five-number summaries:
    **minimum**, **first quartile**, median, **third quartile**, and **maximum**,
    as shown in *Figure 3*, where we can look for outliers beyond three (3) **Inter-Quartile
    Range** (**IQR**):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature engineering and data exploration](img/00120.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3\. Histogram of the workout time (figure courtesy of Box Plot: Display
    of Distribution, [http://www.physics.csbsju.edu/stats/box2.htmlhttp://www.physics.csbsju.edu/stats/box2.html](http://www.physics.csbsju.edu/stats/box2.htmlhttp://www.physics.csbsju.edu/stats/box2.html)).'
  prefs: []
  type: TYPE_NORMAL
- en: Bootstrapping the datasets also sometimes offers insights on outliers. If the
    data volume is too large (that is, big data) doing the Data Sidekick, evaluations
    and predictions are also useful. The idea of Data Sidekick is to use a small part
    of the available data to figure out what insights can be concluded from the datasets
    and it is also commonly referred to as *using small data to multiply the value
    of big data*.
  prefs: []
  type: TYPE_NORMAL
- en: It is very useful for large-scale text analytics. For example, suppose you have
    a huge corpus of text, and of course you can use a small portion of it to test
    various sentiment analysis models and choose the one which gives the best results
    in terms of performance (computation time, memory usage, scalability, and throughput).
  prefs: []
  type: TYPE_NORMAL
- en: Now we would like to draw your attention to the other aspects of feature engineering.
    Moreover, converting continuous variables into categorical variables (with a certain
    combination of features) results in better predictor variables.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In statistical language, a variable in your data either represents measurements
    on some continuous scale, or on some categorical or discrete characteristics.
    For example, weight, height, and age of an athlete would represent the continuous
    variables. Alternatively, the survival or failure in terms of time is also considered
    as continuous variables. A person's gender, occupation, or marital status, on
    the other hand, is categorical or discrete variables. Statistically, some variables
    could be considered in either way. For example, a movie viewer's rating of a move
    on a 10 point scale may be considered a continuous variable, or we may consider
    it as a discrete variable with 10 categories. Time series data or real-time streaming
    data are usually collected for continuous variables until a certain time.
  prefs: []
  type: TYPE_NORMAL
- en: In parallel, considering the square or cube or even using the non-linear models
    of the features can also provide better insights. Also, consider the forward selection
    or backwards selection wisely since both of them are computationally expensive.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, when the number of features becomes significantly large it is a wise
    decision to use the **Principal Component Analysis** (**PCA**) or **Singular Value
    Decomposition** (**SVD**) technique to find the right combination of features.
  prefs: []
  type: TYPE_NORMAL
- en: Feature extraction – creating features out of data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Feature extraction is the automatic way of constructing new features from the
    raw data you have or will be collecting. During the feature extraction process,
    reducing the dimensionality of complex raw data is usually done by making the
    observation into a much smaller set automatically that can be modeled into later
    stages. Projection methods such as PCA and unsupervised clustering methods are
    used for tabular data in TXT, CSV, TSV, or RDB format. However, feature extraction
    from another data format is very complex. Specially parsing many data formats
    such as XML and SDRF is a tedious process if the number of fields to extract is
    huge.
  prefs: []
  type: TYPE_NORMAL
- en: For multimedia data such as image data, the most common type of technique includes
    line or edge detection or image segmentation. However, subject to the domain and
    image, video and audio observations advance themselves to many of the same types
    of **Digital Signal Processing** (**DSP**) methods where typically the analogue
    observations are stored in digital formats.
  prefs: []
  type: TYPE_NORMAL
- en: The most positive pros and the key to feature extraction are that the methods
    that are developed and available are automatic; therefore, thay can solve the
    problem of unmanageable high dimensional data. As we stated in [Chapter 3](part0031_split_000.html#TI1E2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 3. Understanding the Problem by Understanding the Data"), *Understanding
    the Problem by Understanding the Data* that more data exploration and better feature
    extraction eventually increases the performance of your ML model (since feature
    extraction also involves feature selection). The reality is more data will provide
    more insights towards the performance of the predictive models eventually. However,
    the data has to be useful and dumping unwanted data will kill your valuable time;
    therefore, think of the meaning of the statement before collecting your data.
  prefs: []
  type: TYPE_NORMAL
- en: There are several steps involved in the feature extraction process; including
    the data transformation and feature transformation. As we stated several times,
    a machine learning model is likely to provide a better result if the model is
    well trained with better features out of the raw data. Optimized for learning
    and generalization is a key characteristic of good data. Therefore, the process
    of putting together the data in this optimal format is achieved through some data
    processing steps such as cleaning, missing values handling, and some intermediate
    transformation like from a text document to words transformation.
  prefs: []
  type: TYPE_NORMAL
- en: The methods that help to create new features as predictor variables are called
    feature transformation, which is actually a group of methods. Feature transformation
    is essentially required for the dimension reduction. Usually, when the transformed
    features have a descriptive dimension, it is likely to have better order compared
    to the original features.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, less descriptive features can be dropped from the training or test
    samples when building the machine learning models. The most common tasks included
    in the feature transformation are non-negative matrix factorization, principal
    component analysis, and factor analysis using scaling, decomposition, and aggregation
    operations.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of feature extraction include the extraction of contours in images,
    extraction of diagrams from a text, extraction of phonemes from the recording
    of spoken text, and so on. Feature extraction involves a transformation of the
    features, which is often not reversible because some information is lost eventually
    in the process of dimensionality reduction.
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection – filtering features from data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Feature selection is a process for preparing the training datasets or validation
    dataset for predictive modeling and analytics. Feature selection has practical
    implication in most of the machine learning problem types including classification,
    clustering, dimensionality reduction, collaborative filtering, regression, and
    so on.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the ultimate goal is to select a subset from the large collection
    of features from the original data set. And often dimensionality reduction algorithms
    are applied, such as **Singular Value Decomposition** (**SVD**) and **Principal
    Component Analysis** (**PCA**).
  prefs: []
  type: TYPE_NORMAL
- en: An interesting power of the feature selection technique is that a minimal feature
    set can be applied to represent the maximum amount of variance in the available
    data. In other words, the minimal subset of the feature is enough to train your
    machine learning model quite efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: This subset of features is used to train the model. There are two types of feature
    selection techniques, namely forward selection and backwards selection. The forward
    selection starts with the strongest feature and keeps adding more features. On
    the contrary, the backwards selection starts with all the features and removes
    the weakest features. However, both techniques are computationally expensive.
  prefs: []
  type: TYPE_NORMAL
- en: Importance of feature selection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since not all the features are equally important; consequently, you will find
    some features with more importance than others for making the model more accurate.
    Therefore, those attributes can be treated as irrelevant to the problem. As a
    result, you need to remove those features before preparing the training and test
    sets. Sometimes, the same technique might be applied to the validation sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'In parallel to importance, you will always find some features that will be
    redundant in the context of other features. Feature selection is not only involved
    with removing irrelevant or redundant features, it also serves other purposes
    that are important to increase the model''s accuracy, as stated here:'
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection increases the predictive accuracy of the model you are using
    by eliminating irrelevant, null/missing, and redundant features. It also deals
    with highly correlated features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature selection techniques make the model training process more robust and
    faster by decreasing the number of features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature selection versus dimensionality reduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although by using the feature selection technique it is quietly possible to
    reduce the number of features by selecting certain features in the dataset. And
    later on, the subset is used to train the model. However, the entire process usually,
    cannot be used interchangeably with the term **dimensionality reduction**.
  prefs: []
  type: TYPE_NORMAL
- en: The reality is that the feature selection methods are used to extract a subset
    from the total set in the data without changing their underlying properties.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, the dimensionality reduction method, on the other hand, employs
    already engineered features that can transform the original features into corresponding
    feature vectors by reducing the number of variables under certain considerations
    and requirements of the machine learning problem.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, it actually modifies the underlying data, extracts the original features
    from raw and noisy features by compressing the data, but maintains the original
    structure and most of the time is irreversible. Typical examples of dimensionality
    reduction methods include **Principal Component Analysis** (**PCA**), **Canonical
    Correlation Analysis** (**CCA**), and **Singular Value Decomposition** (**SVD**).
  prefs: []
  type: TYPE_NORMAL
- en: Other feature selection techniques use the filter-based, wrapper methods and
    embedded methods feature selection by evaluating the correlation between each
    feature and the target attribute in a supervised context. These methods apply
    some statistical measures to assign a score to each feature also known as filter
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: The features are then ranked based on the scoring system that can help to eliminate
    the specific features. Examples of such techniques are information gain, correlation
    coefficient scores, and Chi-squared test. An example of wrapper methods, which
    is a feature selection process as a search problem, is the recursive feature elimination
    algorithm. On the other hand, **Least Absolute Shrinkage and Selection Operator**
    (**LASSO**), Elastic Net, and Ridge Regression are typical examples of embedded
    methods of feature selection, which is also known as regularizations methods.
  prefs: []
  type: TYPE_NORMAL
- en: The current implementation of Spark MLlib provides the support for dimensionality
    reduction on the `RowMatrix` class only for the SVD and PCA. On the other hand,
    some typical steps from raw data collection to feature selection are feature extractions,
    feature transformation, and feature selection.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Interested readers are suggested to read the API documentation for the feature
    selection and dimensionality reduction at: [http://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html](http://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html).
  prefs: []
  type: TYPE_NORMAL
- en: Best practices in feature engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we have figured out some good practices while performing the
    feature engineering on your available data. Some best practices of machine learning
    were described in [Chapter 2](part0023_split_000.html#LTSU2-5afe140a04e845e0842b44be7971e11a
    "Chapter 2. Machine Learning Best Practices"), *Machine Learning Best Practices*.
    However, those were too general for the overall machine learning state of the
    arts. Those best practices, of course, would be useful in the feature engineering,
    too. Moreover, we will provide more concrete examples concerning feature engineering
    in the following sub-sections.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Although the term feature engineering is more technical, however, it is an
    art that helps you to understand where the features come from. Now some vital
    questions evolve too, which need to be answered before understanding the data:'
  prefs: []
  type: TYPE_NORMAL
- en: What are the provenances of those features? Is the data real-time or coming
    from the static sources?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are the features continuous, discrete, or none?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the distribution of the features? Does the distribution largely depend
    on what subset of examples is being considered?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do these features contain missing values (that is, NULL)? If so, is it possible
    to handle those values? Is it possible to eliminate them in the present, future,
    or upcoming data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is there duplicate or redundant entries?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Should we go for manual feature creation that proves to be useful? If so, how
    hard would it be to incorporate those features in the model training stage?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are there features that can be used as standard features?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knowing the answers to the preceding questions is important. Since data provenance
    would help you to prepare your feature engineering techniques a bit faster. You
    need to know if your features are discrete or continuous or if the requests are
    a real-time response or not. Moreover, you need to know the data distribution
    along with their skewness and kurtosis to handle the outliers.
  prefs: []
  type: TYPE_NORMAL
- en: You need to be prepared for the missing or null values whether they could be
    removed or need to be filled with alternative values. Besides, you need to remove
    duplicates entries in the first place, which is extremely important, since duplicate
    data points might significantly affect the results of model validation if not
    properly excluded. Finally, you need to know your machine learning problem itself
    since knowing the problem type would help you to label your data accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Innovative way of feature extraction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Be innovative while extracting and selecting the features. Here we provide eight
    tips altogether that will help you to generalize the same during your machine
    learning application development.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Create the input by rolling up existing data fields to a broader level or category.
  prefs: []
  type: TYPE_NORMAL
- en: To be more specific, let's give you some examples. Obviously, you can categorize
    your colleagues based on their title into strategic or tactical. For instance,
    you can code the employee with *Vice President or VP* or above as strategic and
    the *Director* and below could be encoded as tactical.
  prefs: []
  type: TYPE_NORMAL
- en: Collating several industries into a higher-level industry could be another example
    of such categorization. Collate oil and gas companies with commodity companies;
    gold, silver, or platinum as precious metal companies; high-tech giants and telecommunications
    industries as *technology*; define the companies with more than $1B revenue as
    *large* and *small* with net asset $1M for instance.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Split data into separate categories or bins.
  prefs: []
  type: TYPE_NORMAL
- en: To be more specific, let's give you some examples. Suppose you are doing some
    analytics on the companies with an annual that ranges from $50 M to over $1 B.
    Therefore, obviously, you can split the revenue into some sequential bins, such
    as $50-$200M, $201-$500M, $501M-$1B, and $1B+, for instance. Now how do you represent
    the features in a presentable format? It's so simple, try to put a value one whenever
    a company falls with the revenue bin; otherwise, the value is zero. There are
    now four new data fields created from the annual revenue field, right?
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Think of an innovative way to combine existing data fields into new ones.
  prefs: []
  type: TYPE_NORMAL
- en: To be more specific, let's give you some examples. In the very first tip, we
    argue how to create new inputs by rolling up existing fields into broader fields.
    Now, suppose if you want to create a Boolean flag that identifies whether someone
    falls in a VP or higher category with more than 10 years of experience. Therefore,
    in this case, you are actually creating new fields by multiplying, dividing, adding,
    or subtracting one data field by another.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Think about the problem at hand and be creative simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: In previous tips, suppose you have created enough bins and fields or inputs.
    Now, don't worry much about creating too many variables in the first place. It
    would be wise to just let the brainstorming flow a normal flow for the feature
    selection step.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Don't be a fool.
  prefs: []
  type: TYPE_NORMAL
- en: Be cautious about creating unnecessary fields; since creating too many features
    out of a small amount of data may overfit your model, which can lead to spurious
    results. When you face the data correlation, remember that correlation does not
    always imply causation. Our logic to this common point is that modeling observational
    data can only show us that two variables are related, but it cannot tell us the
    reason.
  prefs: []
  type: TYPE_NORMAL
- en: 'Research articles in the book *Freakonomics* (see also at *Steven D. Levitt,
    Stephen J. Dubner, Freakonomics: A Rogue Economist Explores the Hidden Side of
    Everything*, [http://www.barnesandnoble.com/w/freakonomics-steven-d-levitt/1100550563http://www.barnesandnoble.com/w/freakonomics-steven-d-levitt/1100550563](http://www.barnesandnoble.com/w/freakonomics-steven-d-levitt/1100550563http://www.barnesandnoble.com/w/freakonomics-steven-d-levitt/1100550563))
    has found that data from public school''s test scores indicates that children
    living at home with a higher number of books have a tendency of having higher
    standardized test scores compared to those with a lower number of books at home.'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, be cautious before creating and constructing unnecessary features,
    which implies that don't be a fool.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Don't over engineer.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is trivial to judge the difference whether an iteration takes a few minutes
    or half a day during the feature engineering phase. Since the most productive
    time during the feature engineering phase is usually spent on the whiteboard.
    Therefore, the most productive way to make sure it is done right is to ask the
    right questions to your data. It''s true that nowadays the term big data is taking
    over the term feature engineering. There is no room for hacking, so for the over
    engineering:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Innovative way of feature extraction](img/00127.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Real interpretation of false positive and false negative.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Beware of false positives and false negatives.
  prefs: []
  type: TYPE_NORMAL
- en: Another important aspect is comparing the false negatives and false positives.
    Depending on the problem, getting a higher accuracy on one or the other is important.
    For instance, if you are doing research in the healthcare section and trying to
    develop a machine learning model that will work towards the disease prediction,
    getting false positives might be better than getting the false negative results.
    Therefore, our suggestion in this regard would be to look at the confusion matrix
    that will help you to see the predictions made by a classifier in a visual way.
  prefs: []
  type: TYPE_NORMAL
- en: 'The rows indicate the true class of each observation while the columns correspond
    to the class predicted by the model itself, as shown in *Figure 4*. However, *Figure
    5* would provide more insight. Note that the diagonal elements, also called correct
    decision, are marked in bold. The last column, **Acc**, signifies the accuracy
    for each key as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Innovative way of feature extraction](img/00060.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: A simple confusion matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Think about precision and recall before selecting features.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, two more important quantities to consider are the precision and recall.
    More technically, how often your classifier predicts a +ve outcome correctly is
    called recall. On the contrary, when your classifier predicts a +ve output and
    how often it is actually true is the precision.
  prefs: []
  type: TYPE_NORMAL
- en: It's true that it's really difficult to predict these two values. However, a
    careful feature selection would help you to get both the values better in the
    last place.
  prefs: []
  type: TYPE_NORMAL
- en: You will find more interesting and some excellent descriptions about the feature
    selection in a research paper written by *Matthew Shardlow* (see also at Matthew
    Shardlow, *An Analysis of Feature Selection Techniques*, [https://studentnet.cs.manchester.ac.uk/pgt/COMP61011/goodProjects/Shardlow.pdf](https://studentnet.cs.manchester.ac.uk/pgt/COMP61011/goodProjects/Shardlow.pdf)).
    Now let's have a journey to the realm of Spark's feature engineering features
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering with Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning based on big data is a deep and broad area and it needs a new
    recipe and the ingredients would be feature engineering and stable optimization
    of the model out of the data. The optimized model can be called Big Models (see
    also at *S. Martinez*, *A. Chen*, *G. I. Webb*, and *N. A. Zaidi*, *Scalable learning
    of Bayesian network classifiers*, accepted to be published in *Journal of Machine
    Learning Research*) that can learn from big data and holds the key to a breakthrough
    other than big data.
  prefs: []
  type: TYPE_NORMAL
- en: Big model also signifies that your results out of diverse and complex big data
    would be with low bias (see at *D. Brain and G. I. Webb*, *The need for low bias
    algorithms in classification learning from small data sets*, *in PKDD*, *pp. 62,
    73, 2002*) and out-of-core (see out-of-core learning defined at, [https://en.wikipedia.org/wiki/Out-of-core_algorithm](https://en.wikipedia.org/wiki/Out-of-core_algorithm)
    and [https://en.wikipedia.org/wiki/Out-of-core_algorithm](https://en.wikipedia.org/wiki/Out-of-core_algorithm))
    using multi-class machine learning algorithms with minimal tuning parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Spark introduces this big model for us to deploy our machine learning application
    at scale. In this section, we will describe how Spark developed machine learning
    libraries and Spark core to handle the advanced features of feature engineering
    for large-scale datasets and different data structures efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: As we already stated, Spark's machine learning module contains two APIs including
    `spark.mllib` and `spark.ml`. The MLlib package is built on top of RDD, whereas
    the ML package is built on top of DataFrame and Dataset that provides a higher
    label API for constructing an ML pipeline. The next few sections will show you
    the details of the ML (MLlib will be discussed in [Chapter 5](part0043_split_000.html#190862-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 5.  Supervised and Unsupervised Learning by Examples") , *Supervised
    and Unsupervised Learning by Examples*) package with examples concluding with
    a practical machine learning problem.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning pipeline – an overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spark's ML package provides a uniform set of higher-level APIs that helps to
    create a practical machine learning pipeline. The main concept of this pipeline
    is to combine multiple algorithms of machine learning together to make a complete
    workflow. In the machine learning arena, it is often common practice to run a
    sequence of algorithms to process and learn from the available data.
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose you want to develop a text analytics machine learning application.
    The total process could be split into several stages for a collection of some
    simple text document. Naturally, the processing workflow might include several
    stages. In the very first step, you need to split the text into words from each
    document. Once you have the split words, you should convert those words into numerical
    feature vectors for the words from each document.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, you might want to learn a prediction model using the features vector
    you got in stage 2 and also want to label each vector to use supervised machine
    learning algorithms. In brief, these four stages can be summarised as follows.
    For each document, do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Split the texts=> words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convert words => numerical feature vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Numerical feature vectors => labeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build an ML model as a prediction model using vectors and labels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These four stages could be considered as a workflow. The Spark ML represents
    these kinds of workflows as pipelines that consists of a sequence of PipelineStages;
    where a Transformer and an Estimator contribute in each stage of the pipeline
    to be run in a certain order. The Transformer is actually an algorithm to transform
    one Dataset to another Dataset.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, an Estimator is also an algorithm, which is liable for fitting
    on a Dataset to produce a Transformer. Technically, an Estimator implements a
    method called `fit()`, which accepts a Dataset and produces a model, which is
    a Transformer.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Interested readers should refer to this URL [http://spark.apache.org/docs/latest/ml-pipeline.html](http://spark.apache.org/docs/latest/ml-pipeline.html)
    for more details on the Transformer, an Estimator in pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: '![Machine learning pipeline – an overview](img/00005.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Pipeline is an Estimator.'
  prefs: []
  type: TYPE_NORMAL
- en: To be more specific, let's draw an example, suppose a machine learning algorithm
    such as Logistic Regression (or the Linear Regression) is used as an Estimator.
    Now by calling the `fit()` method , which trains a **Logistic Regression Model**
    (which itself is a model, and hence a Transformer). Technically, a Transformer
    implements a method, namely `transform()`, which converts one Dataset into another.
  prefs: []
  type: TYPE_NORMAL
- en: During the conversion, one more column is depending upon the selection and column
    position. It is to be noted that the pipeline concept that Spark has developed
    is mostly inspired by the Scikit-learn project, which is a simple and efficient
    tool for data mining and data analysis (see also at Scikit-learn project, [http://scikit-learn.org/stable/](http://scikit-learn.org/stable/)).
  prefs: []
  type: TYPE_NORMAL
- en: As discussed in [Chapter 1](part0014_split_000.html#DB7S2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 1. Introduction to Data Analytics with Spark"), *Introduction to Data
    Analytics with Spark*, Spark has implemented RDD operation as **Directed Acyclic
    Graph** (**DAG**) style. The same fashion is also applicable on pipelining as
    well; wherein each DAG Pipeline, stages are specified as an ordered array. The
    text-processing pipeline we previously described as an example with four stages
    is actually a linear Pipeline; in which each stage consumes the data produced
    by the previous stage. It is also possible to create the non-linear pipelines
    as long as the data flow of the feature engineering graph forms and aligns in
    a DAG style.
  prefs: []
  type: TYPE_NORMAL
- en: It is to be noted that, if a Pipeline forms a DAG, then the stages need to be
    specified in topological order essentially. The pipeline we are talking about
    can be operated on top of Dataset including various file types, therefore run-time
    and compile-time checking from the pipelining consistencies is required. Unfortunately,
    the current implementation of Spark Pipeline does not provide the use compile-time
    type checking. However, Spark provides the run-time checking that is used by the
    Pipelines and PipelineModels, which is done using the Dataset schema.
  prefs: []
  type: TYPE_NORMAL
- en: Since the concept of RDD is immutable, that means once an RDD is created, it's
    not possible to change the contents of the RDD, similarly, uniqueness in Pipeline
    stages should be persistent (please refer to *Figure 6* and *Figure 7* for the
    clear view) with unique IDs. For simplicity, the preceding text processing workflow
    can be visualized as like Figure 5; where we have shown the text processing pipeline
    with three stages. The **Tokenizer** and **HashingTF** are two unique Transformers.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, LogisticRegression is an Estimator. In the bottom row, a
    cylinder indicates a Dataset. The `fit()` method from pipeline is called on the
    original Dataset containing the documents of text with labels. Now the `Tokenizer.transform()`
    method splits the raw text documents into words and the `HashingTF.transform()`
    method on the other hand converts the words column into feature vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please note in each case, a column on the Dataset is added. Now the `LogisticRegression.fit()`
    method is called to produce a `LogisticRegressionModel`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Machine learning pipeline – an overview](img/00162.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Pipeline is an Estimator.'
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 7*, the PipelineModel has the same number of stages as the original
    Pipeline. However, in this case, all the Estimators from the original Pipeline
    need to be converted into Transformers.
  prefs: []
  type: TYPE_NORMAL
- en: When the `transform()` method from the **PipelineModel** is called on a test
    Dataset (that is, numeric feature vectors), the data is passed through the fitted
    pipeline in a certain order.
  prefs: []
  type: TYPE_NORMAL
- en: In a nutshell, Pipelines and PipelineModel help to ensure that training and
    test data go through identical feature processing steps. The following section
    shows a practical example of the preceding pipelining process we have described.
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline – an example with Spark ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section will show a practical machine-learning problem called **Spam Filtering**,
    which was introduced in [Chapter 3](part0031_split_000.html#TI1E2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 3. Understanding the Problem by Understanding the Data"), *Understanding
    the Problem by Understanding the Data* with Spark''s pipeline. We will use the
    `SMSSpamCollection` dataset downloaded from [https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection)
    to show the feature engineering with Spark. The following code reads a sample
    dataset as a Dataset using a **Plain Old Java Object** (**POJO**) class (see more
    at [https://en.wikipedia.org/wiki/Plain_Old_Java_Object](https://en.wikipedia.org/wiki/Plain_Old_Java_Object)).
    Note that the `SMSSpamHamLabelDocument` class contains the label (`label: double`)
    and SMS lines (`text: String`).'
  prefs: []
  type: TYPE_NORMAL
- en: To run the code, just create a Maven project in your Eclipse IDE by specifying
    the master URL and dependencies on the provided `pom.xml` file under the Maven
    project and package the application as a jar file. Alternatively, run the example
    on Eclipse as a standalone Java application.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for Spark session creation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Here the Spark SQL warehouse is set to the `E:/Exp/` directory for Windows.
    Set your path accordingly based on the OS type.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for the `smsspamdataset` sample is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s see the structure of the Dataset by calling the `show()` method
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Pipeline – an example with Spark ML](img/00099.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The code for the POJO Class is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's split the dataset into `trainingData` (60%) and `testData` (40%)
    for the model training purpose.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for splits is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The objective of the dataset is to build a predictive model using a classification
    algorithm, as we know from the dataset; there are two types of messages. One is
    spam, which is represented as 1.0, and another one is ham, represented as 0.0
    labels. We can consider here the LogisticRegression or linear regression algorithm
    for training a model for the simplicity of training and using.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, more complex classifiers using regression such as generalized regression
    will be discussed in [Chapter 8](part0067_split_000.html#1VSLM1-5afe140a04e845e0842b44be7971e11a
    "Chapter 8.  Adapting Your Machine Learning Models"), *Adapting Your Machine Learning
    Models*. Consequently, our workflow or pipeline will be like the following, according
    to our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: Tokenize the text lines into words from the training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting the features using the hashing technique
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying a LogisticRegression Estimator for building a model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The preceding three steps can be done easily by Spark's pipeline component.
    You can define all the stages into a single Pipeline class that will build a model
    in an efficient way. The following code shows the whole pipeline for building
    the predictive model. The Tokenizer class defines the input and output column
    (for example, `wordText` to words), the `HashTF` class defines how to extract
    the features from the words of the Tokenizer class.
  prefs: []
  type: TYPE_NORMAL
- en: The `LogisticRegression` class configures its parameter. Finally, you can see
    the Pipeline class that takes the preceding methods into a PipelineStage array
    and returns an Estimator. After applying the `fit()` method to the training set
    it will return the final model, which is ready for prediction. You can see the
    output of the test data after applying a model for predicting.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for Pipeline is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Feature transformation, extraction, and selection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The preceding section showed you the overall process of the pipeline. This pipeline
    or workflow is basically the collection of some operation such as transformation
    to one dataset of another data set, extracting the features, and selecting the
    features. These are the basic operators for feature engineering that we already
    described in previous sections. This section will show you the details about those
    operations using Spark machine learning packages. Spark provides some efficient
    APIs for feature engineering including MLlib and ML.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will start with the ML package by continuing the Spam Filter
    examples. Let's read a large dataset from the text file as Dataset, which contains
    the lines starting with ham or spam words. The sample output of this Dataset is
    given here. Now we will use this dataset for feature extracting the features and
    building a model with Spark's APIs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for `Input DF` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature transformation, extraction, and selection](img/00059.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Transformation – RegexTokenizer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'From the preceding output, you can see that we have to transform it into two
    columns for identifying the spam and ham messages. For doing this, we can use
    the `RegexTokenizer` Transformer that can take input from a regular expression
    (`regex`) and transform it to a new dataset. This code produces `labelFeatured`.
    For example, refer the Dataset shown in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output of `labelFeature` Dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s create a new Dataset from the `labelFeatured` Dataset that we just
    created by selecting the label text as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s further explore the contents in the new Dataset by calling the `show()`
    method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Transformation – RegexTokenizer](img/00116.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Transformation – StringIndexer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The preceding output has the classification of ham and spam messages, but we
    have to make the ham and spam text as double values. The `StringIndexer` Transformer
    can do it easily. It can encode a string column of labels into indices in another
    column. The indices are ordered by label frequencies. `StringIndexer` produces
    two indices, 0.0 and 1.0 for our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output for the `indexed.show()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Transformation – StringIndexer](img/00011.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Transformation – StopWordsRemover
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The preceding output contains words or tokens, but some words are not as important
    as features. Therefore, we need to remove those words. For making this task easier,
    Spark provides the list of stop words through the `StopWordsRemover` class that
    will be discussed more in [Chapter 6](part0049_split_000.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 6.  Building Scalable Machine Learning Pipelines"), *Building Scalable
    Machine Learning Pipelines*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use those words to filter unwanted words. Additionally, we will remove
    the ham and spam words from the text column. The `StopWordsRemover` class will
    transform the preceding Dataset into a filtered Dataset by removing the stop works
    from the features. The following output will show us the words without spam and
    ham word tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Transformation – StopWordsRemover](img/00114.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Extraction – TF
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now we have the Dataset containing a label with a double value and filtered
    words or tokens. The next task is to vectorize (make numeric values) the features
    or extract the features from the words or tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '**TF-IDF** (`HashingTF` and `IDF`; also known as **Term Frequency-Inverse Document
    Frequency**) is a feature vectorization method widely used for extracting the
    features, which basically calculates the importance of a term to a document in
    the corpus.'
  prefs: []
  type: TYPE_NORMAL
- en: '`TF` counts the frequency of the terms in a document or line and `IDF` counts
    the document or line frequency, that is, number of document or lines containing
    a particular term. The following code explains the term frequency of the preceding
    dataset using the efficient `HashingTF` class of Spark. `HashingTF` is a Transformer
    that takes sets of terms; and converts those sets into fixed-length feature vectors.
    The output of the featured data is also shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Extraction – IDF
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Similarly, we can apply `IDF` on the featured data to count the document frequency.
    `IDF` is an Estimator that fits on the preceding dataset and produces an `IDFModel`
    that transforms to a rescaled dataset containing features and labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The preceding output extracts features from the raw texts. The very first entry
    is the label and the rest are the feature vector extracted.
  prefs: []
  type: TYPE_NORMAL
- en: Selection – ChiSqSelector
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The preceding output is ready for training using a classification algorithm
    such as `LogisticRegression`. But we can use the more important feature from the
    categorical features. For doing this, Spark provides some feature selector APIs
    such as `ChiSqSelector`. The `ChiSqSelector` is called **Chi-Squared feature selection**.
  prefs: []
  type: TYPE_NORMAL
- en: 'It operates on labeled data with categorical features. It orders features based
    on a Chi-Squared test, which is independent from the class, and then filters the
    top features which the class label depends on the most. This selector is useful
    for improving the predictive power of a model. The following code will select
    the top three features from the feature vectors, along with the output given:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will discuss more on the `ChiSqSelector`, `IDFModel`, `IDF`, `StopWordsRemover`,
    and `RegexTokenizer` classes in [Chapter 6](part0049_split_000.html#1ENBI2-0b803698e2de424b8aa3c56ad52b005d
    "Chapter 6.  Building Scalable Machine Learning Pipelines"), *Building Scalable
    Machine Learning Pipelines*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Now, you can apply `LogisticRegression` when building a model with the feature
    vectors. Spark provides lots of different APIs for feature engineering. However,
    we have not used the other machine learning of Spark (that is, Spark MLlib) for
    the brevity and page limitation. We will discuss the feature engineering using
    `spark.mllib` gradually with examples in future chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced feature engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss some advanced features that are also involved
    in the feature engineering process such as manual feature construction, feature
    learning, iterative process of feature engineering, and deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Feature construction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The best results come down to you through the manual feature engineering or
    feature construction. Therefore, manual construction is the process of creating
    new features from the raw data. Feature selection based on the feature's importance
    can inform you about the objective utility of features; however, those features
    have to come from somewhere else. In fact, sometimes, you need to manually create
    them.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to the feature selection, the feature construction technique requires
    spending a lot of effort and time with not the aggregation or picking the feature,
    but on the actual raw data so that new features can be constructive towards increasing
    the predictive accuracies of the model. Therefore, it also involves thinking of
    the underlying structure of the data along with the ML problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this regard, to construct new features from the complex and high dimensional
    dataset, you need to know the overall structure of the data. In addition to this,
    how to use and apply them in predictive modeling algorithms. There will be three
    aspects in terms of tabular, textual, and multimedia datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: Handling and manual creation from the tabular data often means a mixture of
    combining features to create new features. You might also need the decomposing
    or splitting of some original features to create new features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With textual data, it often means devising document or context-specific indicators
    relevant to the problem. For example, when you are applying text analytics on
    large raw data such as data from Twitter hashtags.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With multimedia data such as image data, it can often mean enormous amounts
    of time are passed to pick out relevant structures in a manual way.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unfortunately, the feature construction technique is not only manual, but the
    whole process is slower, requiring lots of research involvement from humans like
    you and us. However, it can make a big difference in the long run. In fact, feature
    engineering and feature selection are not mutually exclusive; however, both of
    them are important in the realm of machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Feature learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Is it possible to avoid the manual process of prescribing how to construct or
    extract features from raw data? Feature learning helps you to get rid of this.
    Therefore, feature learning is an advanced process; alternatively, an automatic
    identification and use of features from raw data. This is also referred to as
    representation learning that helps your machine learning algorithm to identify
    useful features.
  prefs: []
  type: TYPE_NORMAL
- en: The feature learning technique is commonly used in deep learning algorithms.
    As a result, recent deep learning techniques are achieving some success in this
    area. The auto-encoders and restricted Boltzmann machines are such an example
    where the concept of feature learning was used. The key idea behind feature learning
    is the automatic and abstract representations of the features in a compressed
    form using unsupervised or semi-supervised learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Speech recognition, image classification, and object recognition are some successful
    examples; where researchers have found supported state-of-the-art results. Further
    details could not have been represented in this book due to the brevity.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, Spark has not implemented any APIs for the automatic feature
    extraction or construction.
  prefs: []
  type: TYPE_NORMAL
- en: Iterative process of feature engineering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The whole process of feature engineering is not a standalone, but more or less
    iterative. Since you are actually interplaying with the form the data selection
    to model evaluation again and again until you are completely satisfied or you
    are running out of time. The iteration could be imagined as a four-step workflow
    that iteratively runs over time. When you are aggregating or collecting the raw
    data, you might not be doing enough brainstorming. However, when you start exploring
    the data, you are really getting into the problem into deeper.
  prefs: []
  type: TYPE_NORMAL
- en: 'After that you will be looking at a lot of data, studying the best technique
    of feature engineering and the related problems presented in the state of the
    arts and you will see how much you are able to steal. When you have done enough
    brainstorming, you will start devising the required features or extracting the
    features depending on your problem type or class. You might use the automatic
    feature extraction or manual feature construction (or both sometimes). If you
    are not satisfied with the performance you might redo the feature extraction process
    for improvement. Please refer to *Figure 7* for a clear view of the iterative
    process of feature engineering:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Iterative process of feature engineering](img/00163.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: The iterative processing in feature engineering.'
  prefs: []
  type: TYPE_NORMAL
- en: When you have devised or extracted the feature, you need to select the features.
    You might apply a different scoring or ranking mechanism based on feature importance.
    Similarly, you might iterate the same process such as devising the feature to
    improve the model. And finally, you will evaluate your model to estimate the model's
    accuracy on new data to make your model adaptive.
  prefs: []
  type: TYPE_NORMAL
- en: You also need a well-defined problem that will help you to stop the whole iteration.
    When finished, you can move on to try other models. There will be gain waiting
    for you in the future once you plateau on ideas or the accuracy delta out of your
    ML pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most interesting and promising moves in data representation we would
    say is deep learning. It is very popular on the tensor computing application and
    the **Artificial Intelligent Neural Network** (**AINN**) system. Using the deep
    learning technique, the network learns how to represent data at different levels.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, you will have an exponential ability to represent the linear data
    you have. Spark can take this advantage and it can be used to improve deep learning.
    For more general discussion, please refer to the following URL at [https://en.wikipedia.org/wiki/Deep_learning](https://en.wikipedia.org/wiki/Deep_learning)
    and to learn how to deploy pipelines on a cluster with TensorFlow, see [https://www.tensorflow.org/](https://www.tensorflow.org/).
  prefs: []
  type: TYPE_NORMAL
- en: A recent research and development at Databricks (also see [https://databricks.com/](https://databricks.com/))
    has shown that Spark can also be used to find the best set of hyperparameters
    for AINN training. The advantage is that Spark will do the computation 10X faster
    than a normal deep learning or neural network algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Consequently, your model training time will drastically reduce up to 10 times
    and the error rate will be 34% lower. Moreover, Spark can be applied to a trained
    AINN model on a large amount of data so you can deploy your ML model at scale.
    We will discuss more on deep learning in later chapters as advanced machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feature engineering, feature selection, and feature construction are the three
    most commonly used steps while preparing the training and test set towards building
    a machine learning model. Usually, the feature engineering is applied first to
    generate additional features from the available dataset. After that, the feature
    selection technique is applied to eliminate irrelevant, missing or null, redundant,
    or even highly correlated features so that high predictive accuracy can be availed.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, feature construction is an advanced technique applied to construct
    new features that are either absent or trivial in the raw dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Note that it is not always necessary to perform feature engineering or feature
    selection. Whether to perform feature selection and construction depends on the
    data you have or collected, what kind of ML algorithm you have picked, and the
    objective of the experiment itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we have described all of the three steps in detail with practical
    Spark examples. In the next chapter, we will describe in detail some practical
    examples of supervised and unsupervised learning using two machine learning APIs:
    Spark MLlib and Spark ML.'
  prefs: []
  type: TYPE_NORMAL
