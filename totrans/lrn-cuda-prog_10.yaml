- en: Deep Learning Acceleration with CUDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning is a machine learning method that can interpret data based on
    artificial neural networks. Specifically, we provide data that a machine can understand
    and build neural network models that learn representations from data. We can use
    this technique to build models that recognize speech, classify objects from images,
    understand text, translate languages, transform data domains, and so on. Basic
    neural networks include the **fully connected layer** (**FCL**), the **convolutional
    neural network** (**CNN**), and the **recurrent neural network** (**RNN**). These
    architectures show strong accuracy in data classification, regional understandings,
    and sequential relationships.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning requires large computations so that it can be widely used. However,
    this issue was resolved because we can reduce the training time significantly
    by using GPU computing power. This is because the basic architecture of neural
    networks is based on matrix operations and GPU is a hardware platform that's been
    optimized for this. Specifically, the innovations of deep learning were tackled
    with NVIDIA CUDA accelerations as many algorithms in deep learning can be accelerated.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will review the neural network operations briefly and discuss
    how these can be accelerated on GPUs. As practice, we will implement a convolutional
    network using the cuDNN and cuBLAS CUDA libraries. The cuDNN library is NVIDIA's
    CUDA library that optimizes deep learning operations specifically. We will cover
    its implementation across three sections. We will also cover how GPUs can optimize
    the required operations. Then, we will cover how using the cuDNN library is effective
    by comparing the performance of the **long short-term memory** (**LSTM**) network.
    Then, we will cover profiling methods in deep learning using the **NVIDIA Tools
    Extension** (**NVTX**). This measures network operations on the GPUs so that we
    can analyze the operations in the timeline and understand their performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Fully connected layer acceleration with CUBLAS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Element-wise layers with cuDNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Softmax and loss functions in cuDNN/CUDA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolutional neural networks with cuDNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recurrent neural networks with CUDA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Profiling deep learning frameworks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter requires the cuDNN library and CUDA Toolkit to be installed. We
    also need CUDA-enabled GPUs. This chapter will cover the fundamentals of deep
    learning and its performance, and so will not require new GPU features. In other
    words, if you covered most of the content in the previous chapters, you will have
    a proper GPU to work with.
  prefs: []
  type: TYPE_NORMAL
- en: To install the cuDNN library, you need to download the package from [https://developer.nvidia.com/cudnn](https://developer.nvidia.com/cudnn).
    You need to log in to the NVIDIA developer site to access the download page. You
    will need to register for an NVIDIA developer account if you don't have an account
    already. Make sure that cuDNN is compiled with the CUDA version you have installed.
  prefs: []
  type: TYPE_NORMAL
- en: Fully connected layer acceleration with cuBLAS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The fully connected layer is the basic architecture of deep learning. Let's
    review its operations and see how CUDA accelerates neural networks in terms of
    the forward and back-propagation procedures. Then, we will apply them to the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Neural network operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A neural network''s basic operation is to perform dot operation between the
    input data and parameters. We call this perception. In deep learning, the neural
    network connects multiple perceptions in a layered manner. We call these feed-forward
    neural networks. The following diagram shows a perceptron and the basic neural
    network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/451a22fa-7568-4602-a522-e2dd3826e53e.png)'
  prefs: []
  type: TYPE_IMG
- en: The perceptron's basic operation is to create a dot product with the input data
    and appropriate weights. Then, it performs a non-linear operation with an activation
    function such as a sigmoid or **rectifier linear unit** (**ReLU**). In feed-forward
    neural networks, the operation is just an affine transformation followed by the
    application of an activation function. A vector will be fed to the neural network
    as input and multiplies it with weight parameters between each node in the two
    layers.
  prefs: []
  type: TYPE_NORMAL
- en: To train the neural networks, we perform forward propagation, loss calculation,
    and gradient back-propagation, and then use the update parameter. Let's cover
    them briefly. Then, we will match each step using cuBLAS and other CUDA operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The forward operation can be denoted by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3bbf27ea-c1c4-4091-bfca-a7ef3640f55d.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/f58e3de0-2d4a-43ab-888b-3ffe2c39d537.png) is a prediction result
    from the given input vector, ![](img/3b5c5481-dfe4-49e6-9bfd-16777e251f7f.png), ![](img/bf47e85d-b819-4362-ba4b-2c84785e42ce.png)
    is the weight parameter matrix, and ![](img/7fc375a7-123f-463e-8357-d6907f6095b5.png)
    is the activation function. As we can see, the basic operations in the fully connected
    layer are matrix operations. Therefore, we need to implement the matrix multiplication
    operation to the inputs and activation function. Because we take the classification
    task, we use a softmax function to normalize the output and obtain a probabilistic
    distributed result in the next layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'To obtain the loss between the true value, we apply one-hot encodings on the
    label and get cross-entropy loss by obtaining the entropies from each element,
    like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3ff26108-c02a-4604-9eaf-e1a5d04df24f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can obtain the total loss value by means of the sum of each cross-entropy
    loss. Then, we can obtain the gradient from the preceding equation. This looks
    like a complicated operation, but it can be simplified, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fd0cf659-df61-472c-8fae-26facd67cc53.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we will propagate the gradients to the previous layer, which is called
    back-propagation. In this task, we use the chain rule to obtain the gradients
    for each weight and bias parameter. Then, we can update the weight parameter''s
    set and bias. For example, we can obtain the gradients of the weights and biases
    with the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/78c891c9-a052-46b4-ba22-11229094a2dc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can obtain the gradients to propagate to the previous layer with the following
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/29f600b2-c4fd-4055-8b00-063de51c91a9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![](img/d6c48113-6009-476e-b53a-61484c12014e.png) is the gradient of
    the activation function. Therefore, we need to obtain ![](img/9daa0c33-638e-4a0f-a64c-f6ad7d3b4b60.png) from
    the second layer for the first layer. Then, the first layer''s gradients of weight
    and biases can be obtained with the following equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/10ef2652-3473-4250-ab5a-08b258b45757.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we can update the weights and biases based on the gradient descendant
    rule, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c829793e-26f4-4c44-8a50-d03e1c87a45b.png), ![](img/db73f6e5-a09f-4f6b-b850-9f78b7719aaa.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/1da8983e-1bb9-4ea6-9d8f-39426d0661f3.png) is the iteration step.
  prefs: []
  type: TYPE_NORMAL
- en: 'The gradient of the activation function ![](img/2eb7280e-6e41-45df-889f-049a2ed74da8.png)
    can be different, as well as its type. The implementation of this activation layer
    will be covered in the next section. The derivation of the activation functions
    can be denoted by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3d8453c6-78df-4735-85c0-6b13d98729a1.png),     ![](img/da9de664-20a0-4af2-9b7a-95841c4fd8e6.png)'
  prefs: []
  type: TYPE_IMG
- en: As a result, the neural network operations are a set of linear algebra operations
    and can be covered with the cuBLAS library. The implemented code can be found
    in `01_ann`. We will cover these implementation details in *Implementing a fully
    connected layer,* *Implementing layer operation*, and *Implementing the softmax
    layer* sections.
  prefs: []
  type: TYPE_NORMAL
- en: Design of a neural network layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we write our code, let''s cover how we can package the operations into
    a layer configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we perform forward operation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we perform backward operation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then we get a weight update from the gradient.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, the output layer will obtain the loss.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In this manner, the layer can be configured as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b6bf914f-00bd-414e-b88f-bc36d05bb945.png)'
  prefs: []
  type: TYPE_IMG
- en: It has standardized inputs and outputs and two types of input, depending on
    the workflow. The left-hand data path will be named with the input while the right-hand
    side will be named with the output. The data is fed in two phases (forward and
    backward). We will use blobs to manage the parameters and input/output data. The
    blob is a wrapper of data that's processed across layers and helps manage memory
    space. We will use this design every layer to simplify the network's configuration.
    Every layer will have each blob's descriptors and forward/backward processing
    operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s create a layer class that will be the base class of all the layers.
    The following code shows how the `class` public function stacks. And, you can
    find its implementation in `layer.h` and `layer.cu` in `01_ann/src/ directory`.
    This has not only forward and backward operations but also weight update controls
    and loss calculations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To support these operations, the layer class maintains several cuDNN descriptors,
    blob pointers, and weight update controllers. The detail implementations will
    be covered when we cover the network implementations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This layer class will be used across deep learning network implementation in
    other sections. For this reason, it has `cudnnTensorDescriptor_t` variables for
    cuDNN operations, as well as the `get_loss()` and `get_accuracy()` functions.
  prefs: []
  type: TYPE_NORMAL
- en: Tensor and parameter containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our implementation, we will use a data container named `Blob`. Its name was
    borrowed from Caffe. This allows us to store tensors or network parameters with its
    dimensional size information and memory points. We will connect each layer using
    this. This helps each layer initialize its weights based on the input tensor's
    size information. Also, each layer can validate its result based on the information
    of the `Blob`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This blob will require the dimensional size information in the neural network,
    as shown in the following line of code. Then, its constructor will create a host-side
    buffer following the size information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '`Blob` can also handle memories in the host and device and can help us access
    those memories. `Blob` has the following memory access helper functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'As we discussed earlier, `Blob` can store tensors, we also need to provide
    tensor shape information as a descriptors required by cuDNN APIs. Therefore, `Blob`
    can create and set the tensor descriptor using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's implement a fully connected layer using `Blob`.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a fully connected layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will write a fully connected network using cuBLAS. For
    this layer, we will create a `Dense` class derived from the `Layer` class. The
    class constructor will receive the default layer configuration information, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: But this is not enough to configure the whole layer. The missing information
    will be provided from the input because the input size will be determined by the
    previous layer. Now, let's cover forward propagation.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing forward propagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In forward propagation, we can break the forward process into two steps, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/95d7cd68-8fed-4f8a-bd7c-0f1d9e9a2afb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since the weight size does not have to be affected by the batch size, we only consider
    the number of input weights and output weights. On the other hand, data feeding
    blobs, such as input and output, are affected by the batch size. So, our GEMM
    operation with the filter and input data can be designed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7a96d4c2-0e5a-44ce-8251-e09d28e5e49a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The hidden output will be added with the bias values. The input data is not
    limited to the data from the data loader. As we stack the layers, the output of
    the previous layer will be the current layer''s input data. The forward operation
    can be implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'At the first iteration, each layer needs to initialize its weight and bias.
    For example, this `Dense` layer can initialize its weights, biases, and output
    tensor elements. We can separate this initialization task into two phases. The
    first is for the weights and biases, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The next phases is about updating the input information and initializing the
    output blob. When it''s new or needs to be reconfigured, we need to do the following.
    In this task, we also need to create a vector filled with our batch size. This
    will be used in biases addition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This initialization task triggered not only the first iteration but also batch
    size changes. Checking the batch size is not required in the training phase, but
    it will be useful in the testing phase. This is because the batch sizes in training
    and inference are different. In this case, we need to create an output blob following
    the new batch size. The output tensor''s size is determined as the channel size.
    The output blob''s creation code, as follows, creates a blob of size (`batch_size_`,
    `output_size_`, `1`, `1`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This creates flattened tensors. Then, we feed these tensors, which requires
    them to be aligned in channels. This alignment is specifically required in the
    softmax layer. We will cover this in the softmax layer's implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another important task in this phase is to initialize weights and biases. In
    our implementation, we will use the ReLU as an activator. We will use the normal
    initializer ([https://arxiv.org/abs/1502.01852](https://arxiv.org/abs/1502.01852))
    technique to make the network trainable. Following the guidelines in the preceding
    paper, the required weight values can be generated with the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/85eca19f-7734-471a-b5a7-8191808d66f5.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/0ed79072-61db-46e8-a6bc-66049715f0ff.png) is the number of inputs from
    the previous layer. For this reason, we can initialize the parameters after we
    update the input tensor information. Also, the bias values will be initialized
    as `0`. The following code shows the implementation of this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's cover backward propagation.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing backward propagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we discussed earlier, the gradient from the next layer is propagated to
    this layer. Based on propagated gradients, we need to obtain three gradients for
    the weights, biases, and data (gradient of input). We need to create the blobs
    that can store them. Their size does not depend on the batch size, so we just
    need to make sure that we create them. The following code shows how we can create
    blobs for this purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, `grad_output_` means the gradients of the output data
    that is propagated from the next layer, and `grad_input_` means the gradients
    of the input data that will be propagated to the previous layer. Therefore, we
    don't need to create a `grad_output_` blob. If you find these naming conventions
    confusing, it may be easier if you regard `grad_input_` as ![](img/84d797c7-3cb7-40e4-8b12-bf9e5be2921e.png) and
    `grad_input_` as ![](img/5dbe0798-7e12-4625-adcb-b2a5044495d2.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows how we can implement this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We can also skip computing the gradients of the input data if this layer is
    the first layer in the model since we don't have to do anything with it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The weight and bias updates will be done when we want to update the weights.
    In this section, we will use **Stochastic Gradient Descent** (**SGD**) for this.
    This operation can be used in other layers as well. Here, we will place this function
    in the `Layer` class. The weight updates can also be done with `cublas` functions,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we can update the weights and bias with the learning rate. Of
    course, you can change the `eps` operation to apply other optimization algorithms
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: Layer termination
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In C/C++ programming, the programmers should cover how to return the used resource
    when it terminates class instances. Following our design, the layer will create
    six blobs at most if they have weights parameters and can update them from the
    gradients. The following code shows the layer termination code, which terminates
    blobs that are created internally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The input blob or tensor descriptors will be handled by other layers or blob
    terminations. The layer class is a base class to the other layers. Therefore,
    we can focus on terminating custom-created resources, because this termination
    code will be called together when we terminate any derived layers.
  prefs: []
  type: TYPE_NORMAL
- en: Even though we have architected the network and the layers, we should develop
    some additional layers to complete the network. For example, we didn't implement
    the activation, softmax, and loss calculation layers. We will cover these layers
    in the upcoming sections.
  prefs: []
  type: TYPE_NORMAL
- en: Activation layer with cuDNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are many element-wise operations in neural network layers. The activation
    function is one of these operations. The cuDNN library provides six activation
    functions: sigmoid, ReLU, tanh, clipped ReLU, ELU, and identity. In the cuDNN
    library, `cudnnActivationForward()` does forward operation and `cudnnActivationBackward()`
    does backward operation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the `cuddnnActivationForward()` function''s interface, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Using `cudnnActivationDescriptor_t`, we can determine the types of the activation
    function. Alpha and beta are scalar values that determine the rate of input to
    be added. `xDesc` and `yDesc` hold the tensor's shape information. They can be
    created using `cudnnCreateTensorDescriptor()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you look at the `cudnnActivationBackward()` function, `dy` is the gradient
    input from the next layer and `dx` is the gradient output to the previous layer.
    In this case, `y` becomes the input. In this manner, `dyDesc` provides the gradient
    input shape information while `dxDesc` provides the gradient output shape information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In general, we can expect the tensor shape between layers to not change. Due
    to this, we can use the same tensor descriptor for `x` and `dx`. It is the same
    as using `y` and `dy`.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's implement the cuDNN-enabled activation function using the cuDNN API.
    To use the cuDNN API, we need to provide a tensor descriptor to specify the input
    and an output tensor dimension to the cuDNN functions. We also need to specify
    the activation operation.
  prefs: []
  type: TYPE_NORMAL
- en: Layer configuration and initialization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While our example implementation does not use the layer interface, we need
    to integrate our example into the layer interface. In our layer design, the activation
    layer can be implemented like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'At the initialization step, we need to create several tensor descriptors and
    an activation descriptor. The cuDNN library requires the developers to provide
    a tensor size or any other operational handles corresponding to the API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'In cuDNN, we specify the activation function operation using an activation
    descriptor. We do this with the `cudnnSetActivationDescriptor()` function. Then,
    it can determine the `cudnnActivationForward/Backward()` function''s operation.
    We will cover this in the next section. Before we do that, however, we need to
    implement the class destructor so that it destroys the activation descriptor,
    like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's cover the activation layer's forward and backward operations.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing layer operation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is also known as the caution operation. This layer does not require that
    we handle weights and biases, and so it is simpler to implement than the dense
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing forward propagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At the first iteration, we need to initialize the input descriptors, output
    descriptors, and output blob. We will update the output blob when the batch size
    is changed. However, we don''t have to initialize the weights and bias because
    it doesn''t have those. The following code shows its implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'After initialization, we use the `cudnnActivationForward()` function in cuDNN
    for the activation process, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This activation function's operation is determined when we initialize this layer,
    as we discussed earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing backward propagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The next step is to implement backward propagation. We will reuse the input/output
    tensor descriptors we already have. Now, we have to initialize the gradients we
    wish to back-propagate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'After the initialization, we can call the `cudnnActivationBackward()` function,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Note that we reuse the input tensor descriptor and output tensor descriptor
    that we created in the forward pass. We can do this because the activation operation
    does not change the tensor's size. We could simplify our implementation by using
    the cuDNN API in activating backward propagation.
  prefs: []
  type: TYPE_NORMAL
- en: The output of the `cudnnActivationBackward()` function is `d_grad_input`. As
    we described in the previous section, this gradient will be passed to the lower
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will implement the softmax layer and integrate our layer implementations
    as a network. Then, we will discuss the fully connected layer's accuracy in the
    image classification task.
  prefs: []
  type: TYPE_NORMAL
- en: Softmax and loss functions in cuDNN/CUDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the MNIST dataset classification, we will use the softmax classifier. The
    softmax function normalizes the inputs and generates the probability distribution
    of ![](img/6a23c4d0-c4cb-4caa-b483-128315b59c21.png) probabilities. The softmax
    operation can be denoted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f6b6d132-a44b-4e74-ade8-f2dd5d043ea5.png)'
  prefs: []
  type: TYPE_IMG
- en: cuDNN's softmax forward function supports this operation, along with the channels
    and all the instances. Previously, we aligned the dense layer's output with the
    channels. Therefore, we will apply the softmax operation along with the channels.
  prefs: []
  type: TYPE_NORMAL
- en: 'To confirm that our training is done effectively, we need to calculate the
    loss function. The softmax loss function is called cross-entropy loss since its
    loss function is used to obtain loss across ![](img/0adcd213-509d-4129-8488-317b8bc434e8.png)
    probabilities. The loss function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/28d7a607-4b3b-400a-9382-74a6107c6d57.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We need to obtain the gradient of this softmax loss to update the neural networks.
    Fortunately, the gradient of softmax loss is simple after the derivation, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/07eee5af-eac3-444a-b542-9acdf05f231e.png)'
  prefs: []
  type: TYPE_IMG
- en: For the forward operation, we will use the cuDNN function to get the softmax's
    output. To obtain gradients, having a custom operation is more intuitive and simple.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the softmax layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let's see how the softmax layer can be implemented using cuDNN and CUDA
    code.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing forward propagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can obtain the softmax cost function''s outputs using `cudnnSoftmaxForward()`
    from the cuDNN library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: One of the most important parameter settings to use in this situation is `CUDNN_SOFTMAX_MODE_CHANNEL`.
    This option enables channel-level softmax operations following the input tensor
    descriptor information. By doing this, we can provide tensors that have been aligned
    by channels from mini-batch inputs from the dense layer.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing backward propagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The backward pass in the softmax layer is different from other layer implementation.
    This operation takes the labels of the input data as input and obtains the appropriate
    gradients. As we discussed earlier, the gradients of the softmax loss can be obtained
    using the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4e9ecd44-3c23-4f2a-a26b-2e69c5e10894.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can implement this operation using `cublasSaxpy()`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, the target blob contains one-hot-encoded target vectors,
    so adding negative target vectors to the predicted values produces the appropriate
    gradients. After that, we need to normalize batch gradients ahead of propagation
    to the previous layer, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Since this introduces the mean of the weighted sum, we can expect that the gradients
    of each batch are normalized.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the loss function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Calculating the loss value of softmax is optional. This means its value is not
    accounted for in training and inference. However, we can use this as an indicator
    of the training.
  prefs: []
  type: TYPE_NORMAL
- en: 'The softmax loss function should implement the following equation, as we discussed
    earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9e2f16bd-b541-4709-9d49-02e21c9a5aed.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can obtain the loss from each sample''s output and cumulate them using a
    kernel function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This operation uses parallel reduction, which we covered in [Chapter 3](d2527255-a553-410f-bc03-a0f0c0b50f12.xhtml),
    *CUDA Thread Programming*, to obtain the cumulated loss value in a batch. Since
    we will just use this reduced loss value to confirm the training, we will simply
    monitor its output rather than taking its average.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's integrate all the layers we have implemented with an MNIST dataset
    loader.
  prefs: []
  type: TYPE_NORMAL
- en: MNIST dataloader
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the important parts of this entire process is having a dataloader for
    a specific dataset. In this lab, we will use the MNIST dataset, which contains
    60,000 samples. When it comes to initialization, we tell the data loader whether
    it should load either the train or test set. After that, the data loader will
    load some magic numbers in the dataset, along with all the samples and their labels.
    The loaded data will be stored in vectors and shuffled with the same random seed.
    Since the data loader builds and shuffles the sample vector, the training loop
    or test loop may get randomized input data for each iteration. The fully implemented
    code can be found in the `src/mnist.cpp` file in this book's GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: Managing and creating a model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we have multiple layers, we need an object that can manage those layers
    with neural network operations, that is, forward/backward propagation and weight
    updates. In this lab, we will have an array of layers and iterate the array for
    forward processing. For example, the forward operation can be performed with the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Backward propagation can also be done by iterating over the array in reverse
    order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, we manage the layers in the vector and have each layer''s operations.
    Adding a new layer into the network is even simpler, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'By using the `Network` class, we can use various model management functions,
    such as parameter updates, layer registration, layers initialization, and so on.
    Also, we can build a neural network like a modern deep learning framework. For
    example, we can create a model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also have the following training loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'For the testing phase, we create another dataset loader for the test dataset
    and only iterate with the forward pass. The following code shows its implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: In the testing phase, we will obtain the accuracy after we finish testing all
    the samples in the testing dataset. Now, we need to obtain the accuracy after
    the testing loop.
  prefs: []
  type: TYPE_NORMAL
- en: Network training with the MNIST dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s run our implemented code and see its result. For the training phase,
    we will iterate 2,400 steps with a batch size of 256\. The MNIST dataset has 60,000
    samples in the training set. 2,400 steps means that we will take the iteration
    of about 10 epochs. The sample code can be compiled with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the training and test output of our implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1308649b-62e6-4f67-a98f-876d835049b2.png)'
  prefs: []
  type: TYPE_IMG
- en: In the training iteration, the network achieved 92 percent accuracy from the
    training dataset. However, the testing accuracy is only 77 percent, which is a
    relatively low score against the training result. There can be many reasons why
    inferencing shows a large gap in accuracy between training and inference. One
    possible reason is that the fully connected layer does not consider the regional
    information that's shown in the preceding screenshot. In deep learning, we use
    a convolutional layer to make the network learn about the spacial information.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's implement the convolutional layer with cuDNN, add this to the network,
    and compare the model's performance.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional neural networks with cuDNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The cuDNN library provides optimized performance for convolutional operations.
    By creating a convolutional layer, we will cover the API's configuration for the
    forward and backward operations.
  prefs: []
  type: TYPE_NORMAL
- en: The convolutional network layer performs convolution to the input data with
    its weights. This network architecture is useful when you want to build a neural
    network that's aware of regional information. Recall from the convolution implementation
    in [Chapter 7](71d77c43-0064-491e-9b43-307a05bd6555.xhtml), *Parallel Programming
    Patterns in CUDA*, that it needs considerable memory bandwidth and requires further
    optimization to get optimal performance. However, using the cuDNN library, we
    can obtain the best performance as well since we don't have to reinvent the wheel.
  prefs: []
  type: TYPE_NORMAL
- en: 'The implementation of a convolutional layer is similar to the fully connected
    layer implementation. There are two differences, however, thanks to the cuDNN
    library: we don''t have to fully implement as much detail as we did previously
    and we need to allocate a workspace size for the operation. For each convolution
    operation – forward, backward for the filter, and backward for the input – extra
    memory space is needed, depending on their algorithm. The algorithm can vary following
    the given input/output/filter tensor dimensions. The detailed API call will be
    handled later.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Like other layers, it has three work phases. For the inference phases, we will
    call `cudnnConvolutionForward()` and `cudnnAddTensor()`. For the backward phase,
    we will call `cudnnConvolutionBackwardData()`, `cudnnConvolutionBackwardFilter()`,
    and `cudnnConvolutionBackwardBias()`. Finally, for the update phase we can reuse
    the code from the fully connected layers. An overview of the layer''s configuration
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dfbcecb1-0c13-4d9e-a0a3-ac60623461a2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In deep learning neural networks, it is common to use a pooling layer along
    with the convolutional network. Pooling layers simply select input data to output
    following a simple rule. The following diagram shows examples of max-pooling:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e821b3b9-c7ff-4bdc-817a-1b7c5fb97325.png)'
  prefs: []
  type: TYPE_IMG
- en: Using the cuDNN library, we will implement these two convolution operations.
  prefs: []
  type: TYPE_NORMAL
- en: The convolution layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Like a fully connected layer, this convolution layer has weights and biases
    parameters. In the fully connected layer, we used cuBLAS and it does not require
    cuDNN-related descriptors. However, we will be using cuDNN convolution functions,
    and so we need to use a filter descriptor and convolution operation descriptor.
    The following code shows what resources we should initialize while the layer is
    being constructed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Since we provide convolution operation information when the model is constructed,
    we can specify the convolution descriptor. However, the filter's operation can
    be specified at inference time since we can learn the input tensor's size at that
    time. Now, let's implement the forward pass in the convolution layer.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing forward propagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we discussed earlier, we can initialize the convolution layer with the input
    tensor size. This input tensor size makes an impact on the output tensor''s size.
    The following code shows the parameter initialization step in the forward pass:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we need to update the input resources, initialize the output blob, create
    a cuDNN workspace, and initialize the weights parameters, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: To obtain the output tensor size, we use the `cudnnGetConvolution2dForwardOutputDim()`
    function. This function outputs dimensional size information based on the input
    tensor size, the convolution operation, and the filter size. Then, we reuse the
    same parameter initialization code that we used in the fully connected layer.
  prefs: []
  type: TYPE_NORMAL
- en: To call cuDNN's convolution APIs, we need to provide its working algorithm and
    workspace memory. We do this because cuDNN selects the optimal convolution algorithm
    based on the convolution size, and its measurement needs to be done immediately.
    When the algorithm is determined, cuDNN can determine the workspace size. The
    convolutional layer needs to have the convolution operation for the forward pass,
    gradients of input data, and gradients of weights. We need to handle each algorithm
    individually, but we can allocate just one workspace because the workspace is
    used for each convolution operation exclusively.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we create the workspace with the maximum size among each convolution algorithm
    workspace size that''s required. The following code shows how we can use them
    and manage the workspace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Each convolution algorithm is specified with individual types, that is, `cudnnConvolutionFwdAlgo_t`, `cudnnConvolutionBwdDataAlgo_t`,
    and `cudnnConvolutionBwdFilterAlgo_t`. We can use them by declaring them as class
    member variables, that is,  `conv_fwd_algo_`, `conv_bwd_data_algo_`, and `conv_bwd_filter_algo_`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we write the forward processing code after initialization. We do convolution
    with the filter and add a bias. The following code shows the cuDNN convolution
    forward implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The result of convolution will be passed to the next layer using the output
    blob.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing backward propagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In back-propagation, we should compute the gradients of the bias, the gradients
    of weights, and the gradients of the input data. To do this, we need to create
    blobs at the first iteration so that we can store them. Their size does not depend
    on the batch size, so we just need to make sure they are created. The initialization
    step can be implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we call the cuDNN backward convolution APIs, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Then, we pass the gradients of the input data to the previous layer to propagate
    the gradients. We will update the gradients of the weights and biases at the update
    step by using the base class' gradients update code. We covered this when we implemented
    backward propagation in the fully connected layer. We also can skip computing
    the gradients of the input data if this is the first layer.
  prefs: []
  type: TYPE_NORMAL
- en: Pooling layer with cuDNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The pooling layer has two features. First, its output size is different compared
    to the convolution layer and cuDNN provides the corresponding API for this. Second,
    it does not have any internal weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'To specify the pooling operation, we can use cuDNN''s `cudnnPoolingDescriptor_t` function
    and create and specify the cuDNN''s pooling descriptor in the class constructor,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's implement the forward and backward operation of the pooling layer.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing forward propagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The pooling layer contributes to reducing the tensor''s size. Due to this,
    we need to compute the output size. We can compute the size using the `cudnnGetPooling2dForwardOutputDim()`
    function, like we did in the convolution layer implementation. Also, the tensor
    size depends on the batch size. This means we need to update the tensor size if
    the batch size is changed. The following code shows how we can initialize the
    input and output blobs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'For the forward pass, we call the `cudnnPoolingForward()` function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Implementing backward propagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the back-propagation step, we call the `cudnnPoolingBackward()` function,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The pooling layer's tensor shape of inputs and gradients of inputs are same
    and the shape of outputs and gradients of outputs are same. Therefore, we can
    reuse the tensor descriptors respectively of inputs and outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's integrate these into a single convolutional layer implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Network configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we will update our previous network, LeNet. The network code can be written
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can start the training and inference stages since we have configured
    our layers so that they''re connected to each other. Let''s compile the code with
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can see the training and test result as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7bddab01-eac8-4ff6-9222-861e7e99c72a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, the network achieved higher training accuracy and inferencing
    than when it used the fully connected network only. We also can confirm its operation
    by looking at the NVIDIA profile, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/60c3e92b-b87f-49e5-8a22-012f0da4be45.png)'
  prefs: []
  type: TYPE_IMG
- en: Mixed precision operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The latest NVIDIA GPUs support mixed precision operation deep learning. We will
    not cover this in this book as it is outside of our scope. However, you can access
    the example that's provided by NVIDIA at `/usr/src/cudnn_samples_v7/conv_sample` if
    you wish to learn more. To access this example, you need to download the sample
    from the cuDNN web page. This example code shows how to use mixed precision operations
    using the cuDNN library.
  prefs: []
  type: TYPE_NORMAL
- en: 'To have the cuDNN APIs work with the tensor cores, we need to set the math
    type, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Then, we need to initialize the tensor descriptors of the input and output tensors
    using `cudnnSetTensorNdDescriptor()`. This provides padding for the tensors so
    that we get optimized tensor core performance.
  prefs: []
  type: TYPE_NORMAL
- en: One good cuDNN-based implementation is `cudnn-training`: [https://github.com/tbennun/cudnn-training](https://github.com/tbennun/cudnn-training).
    It implements LeNet as a sequence of cuDNN functions. You can follow each line
    to see how the CUDNN functions work.
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in deploying your network using cuDNN, please check out
    the following video about GTC-CNN inference with cuDNN ([https://developer.nvidia.com/gtc/2019/video/S9644/video](https://developer.nvidia.com/gtc/2019/video/S9644/video)). This
    talk introduces useful performance optimization tricks on CNN inference using
    cuDNN.
  prefs: []
  type: TYPE_NORMAL
- en: Using half-precision in deep learning training requires more than FP16 operations
    utilization. We need to compute tensors in FP16 while we maintain the weights
    in FP32\. Also, some operations require FP32\. We call this the mixed precision.
    The cuDNN library provides a mixed precision inference example named mnistCUDNN.
    This example shows the conversion of input and layer data types. If you want to
    learn more about the mixed precision operation in deep learning and training,
    please read the following article: [https://devblogs.nvidia.com/video-mixed-precision-techniques-tensor-cores-deep-learning/](https://devblogs.nvidia.com/video-mixed-precision-techniques-tensor-cores-deep-learning/).
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will cover other GPU use considerations in deep learning in terms of
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent neural network optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RRNs allow you to analyze sequential data in deep learning. Although this network
    has sequential dependencies, there's plenty of room for optimization. In this
    section, we will cover its algorithm and how cuDNN provides optimized performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many kinds of RNNs, but cuDNN only supports four, that is, RNN with
    ReLU, RNN with tanh, LSTM, and GRU. They have two inputs: the hidden parameters
    from the previous network and the input from the source. Depending on their types,
    they have different operations. In this lab, we will cover the LSTM operation. The
    following diagram shows the forward operation of the LSTM:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/674b1b76-8aa5-44b7-a7cd-dada656f74b0.png)'
  prefs: []
  type: TYPE_IMG
- en: From a computing perspective, there are eight matrix-matrix multiplications
    and many element-wise operations. From this estimation, we can expect that LSTM
    could be memory-bounded since each operation is memory-bounded. On the other hand,
    CUDNN provides the `cudnnRNNForwardInference()` and `cudnnRNNFowardTraining()` RNN
    functions. We will cover the benefits of using this function by measuring the
    performance of this function and simulated LSTM. To do this, we will implement
    a virtual LSTM layer and compare its performance to the cuDNN LSTM function.
  prefs: []
  type: TYPE_NORMAL
- en: 'For test purposes, we will set the hyperparameter like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: The sequence length or hidden size can vary, depending on the problem. In this
    test, we will use `512` as the length, which is used a lot in sequence research.
    The CUDNN API requires more options to work, such as dropout rate, bidirectional
    or unidirectional, and persistent RNNs. We will only test the vanilla LSTM in
    this section.
  prefs: []
  type: TYPE_NORMAL
- en: Using the CUDNN LSTM operation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s write some code that executes the `cudnnRNNForwardTraining()` function
    as an LSTM layer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to initialize the input and output memory space. To execute cuDNN''s
    RNN API, we need to use the following variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'These variables are the inputs and outputs of LSTM. To provide the inputs and
    get the outputs, we need to allocate the appropriate memory space. Following the
    LSTM definition, we need to consider the length of the input, output, and hidden
    layers. These sizes can be determined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Then, we can allocate memory for each item.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need to set up the tensor descriptors for the cuDNN RNN API. The following
    code shows the required tensor descriptors that we should set up:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: For the input and output descriptors, we need to initialize each element, that
    is, the batch size and its input size. The other hidden tensor descriptors are
    initialized with the number of layers, the batch size, and the hidden size. This
    section will not cover how to write the initialization code. However, you can
    check out the code in the `10_deep_learning/03_rnn` file if you want to find out
    more.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also have to provide a workspace for the RNN operation, just like we did
    for the convolution operation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can set the filter descriptor based on the workspace''s size, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'We can measure their performance using `cudaEvnetRecoard()` and flops computation.
    For example, the forward operation can be configured with the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b385c8ec-d143-4f0e-9cc6-d31523159f38.png)'
  prefs: []
  type: TYPE_IMG
- en: Then, we will test our implementation by changing the batch size from 32 to
    256 by increasing the size with 32\. The applicable test range can be different,
    as well as the GPU's memory size.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we implemented the LSTM-based simulation and `cudnnRNNForwardTraining()` call.
    Our partially simulated version only has GEMM operations, which are the most compute-intensive.
    Now, let's compare the performance of these implementations.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a virtual LSTM operation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our implementation, we will focus on simulating LSTM's major operations rather
    than fully implementing it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s determine the hyperparameters of the LSTM network. In general, the input
    sequence length ranges from 512 to 2,048\. The number of layers varies. However,
    it cannot be large due to *tanh* operations. For the input size, we will use 512\.
    Usually, the batch size is between 32 and 256 in terms of RNN usage. CUDNN requires
    more inputs about the dropout rate, bidirectional or unidirectional, and whether
    we''re using a persistent RNN or not. We''re just not using them right now. Our
    LSTM configuration information is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/652a450a-efec-4f75-9b83-4048e9751bc0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we will have a partially implemented operation of LSTM to measure the
    compute intensity. As we discussed earlier, LSTM has two matrix-matrix multiplications
    that we need to compute. The LSTM operation will compute that for each element
    of the input sequence, as well as for each layer. Then, the operation can be configured
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: We can use more element-wise operations, but it will just approximate the compute
    intensity, so we will omit them for now.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing the performance between CUDNN and SGEMM LSTM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s compare their there performance along with the different batch sizes
    as following codes implemented in `main()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'And, we can compile and execute the example source code with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'The following graph shows the measured performance of cuBLAS and cuDNN from
    a Tesla V100 card:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cecc6413-86bd-45aa-ab52-03bb687493b1.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding graph, the two implementations show a huge difference in performance.
    The LSTM's performance of cuDNN is much better than the simulated LSTM using cuBLAS.
    Also, the LSTM operation's performance follows the roofline of the Tesla V100
    GPU. On the other hand, the two SGEMM operations don't show this performance since
    the matrix size isn't large enough to get full performance. To obtain 10 TFlops
    from the Tesla V100, the matrix size should be similar to or larger than the square
    of 1,024\. However, as we can see, our matrix size is around the square of 512.
  prefs: []
  type: TYPE_NORMAL
- en: LSTM optimization is explained in the following NVIDIA article: [https://devblogs.nvidia.com/optimizing-recurrent-neural-networks-cudnn-5](https://devblogs.nvidia.com/optimizing-recurrent-neural-networks-cudnn-5).
    It combines matrix-matrix multiplications, fusing element-wise operations, multiple
    streams, and multi-layer parallelization.
  prefs: []
  type: TYPE_NORMAL
- en: One of the optimization versions of the RNN is the persistent RNN ([https://svail.github.io/persistent_rnns](https://svail.github.io/persistent_rnns)),
    which was introduced by Greg Diamos. Although his implementation does not include
    LSTM and GRU, you can learn how the RNN can be optimized.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling deep learning frameworks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In general, we develop and research neural networks using deep learning frameworks
    such as TensorFlow, PyTorch, and MxNet. Thanks to these frameworks, we can develop
    sophisticated models effectively. However, when it comes to performance engineering,
    understanding the GPU operation underneath the framework is a steep learning curve
    because of the profiling tool's capabilities. For example, profiling with chrome
    tracing is useful when the model is simple, but isn't when the model is complicated.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 5](ea24897f-252a-4e76-81e3-b5d5ff645bb6.xhtml), *CUDA Application
    Profiling and Debugging*, we covered the **NVIDIA Tools Extension** (**NVTX**),
    which allows us to have custom annotations in the GPU applications and review
    the timeline using NVIDIA Nsight Systems. For complicated applications, it is
    useful for programmers to analyze their performance and find bottlenecks.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will cover how to use NVTX in PyTorch and TensorFlow by
    modifying the ResNet-50 sample code. The example code can be found in the `10_deep_learining/05_framework_profile` folder
    in this book's GitHub repository. You can obtain the original source code from
    [https://github.com/nvidia/DeepLearningExamples](https://github.com/nvidia/DeepLearningExamples).
  prefs: []
  type: TYPE_NORMAL
- en: To make an easy working environment configuration, we will use **NVIDIA GPU
    Cloud** (**NGC**) deep learning containers for PyTorch and TensorFlow. If you
    need to learn about NGC or basic usage of the container, please visit the appendix
    for NGC in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's begin with PyTorch first.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling the PyTorch model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In PyTorch, we can place a custom tag using `torch.cuda.nvtx.range_push("foo")`
    and `torch.cuda.nvtx.range_pop()`. This maintains the original CUDA NVTX APIs,
    that is, `nvtxRangePush()` and `nvtxRangePop()`. Let''s see how NVTX annotations
    can help us understand deep learning operations in the timeline. In the following
    steps, we will use the ResNet-50 example code in the `05_framework_profile/pytorch/RN50v1.5` file:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will place NVTX annotations in the training loop in the `train()` function
    to annotate the `step` value. This function can be found in the `image_classificaiton/training.py` file.
    The following screenshot shows the training loop and the NVTX annotations at line
    234 and line 260, respectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a9bbdf88-5cc0-421f-b7ad-00e02655c02c.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding code, the training operations are implemented in the `step`
    function, which is defined by the `get_train_step()` function. Therefore, we need
    to place NVTX annotations in that function to learn more about it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s add some NVTX annotations to the `get_train_step()` function at line
    164\. This function returns the `_step()` function, which includes the training
    operations. Therefore, we will place NVTX annotations in this function. The training
    procedures are forward and backward propagation, all-reduce, and optimization
    (update weights). The following screenshot shows the annotations of forward propagation
    at lines 166 and 171:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/0c8cae13-132c-4220-afe8-224397161611.png)'
  prefs: []
  type: TYPE_IMG
- en: This way, we can place other annotations on the remaining operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also have NVTX annotations for the model layers. In this example, the
    ResNet-50 model is implemented in the `image_classification/resnet.py` file. The
    following screenshot shows the example annotations of the network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c3ac1744-4518-41a9-bfea-8c7cbaf8be42.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, we can place NVTX annotations following the ResNet architecture.
    We can get more information if we place annotations in each building block.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s profile the model. As we discussed earlier, we will use the NGC
    deep learning container known as PyTorch. The `imagenet` dataset is located in
    the `/raid/datasets/imagenet/raw-data` folder. To limit the profiling time range,
    we will use the delay option (`-y`) and duration option (`-d`). The following
    code shows a bash shell script that executes the container and profiles the network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: After execution, the preceding code generates the profiled result in the RN50v1.5
    directory, that is, `resnet50_pyt.qdrep`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, open the profiled output, `resnet50_pyt.qdrep`, using NVIDIA Nsight
    Systems and review the operations. The following screenshot shows a measured step
    with NVTX annotations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/2e2912c2-781b-46c0-9e0f-b8c5f97a3f29.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we can see that the backward operations take twice as long as the forwarding
    operations. Also, PyTorch separates the host threads for the training loop and
    backward propagation. Looking at the kernel profiling, the most time-consuming
    points are element-wise kernel executions. Let''s enlarge the forwarding pass
    to review the layers'' execution time, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/013b8dd6-8fa4-4942-91b5-0519a77b7f4f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we can see that the second convolution block takes the longest time to
    complete. If there are inefficient points in this layer, we can dig further. We
    can also analyze a specific kernel function using NVIDIA Nsight Compute if that
    operation is determined as a bottleneck and needs to be optimized. Comparing the
    host API tracing and the GPUs, we can see that the time durations are different.
    This is because the host and the GPU operations are asynchronous. So, we need
    to be cautious when we measure the GPU execution time from the host. Now, let''s
    take a look at the optimization step, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79d227f1-a5aa-4222-88a3-d62710299655.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that the huge difference was in the measured execution time from
    the host and the GPU. The host's measured execution time was 25.367 ms, whereas
    the GPU's time was 4.048 ms. Its operations are mainly element-wise operations
    and its execution is delayed until backward propagation has finished. We can find
    the asynchronous execution too. After that, we can see the `cudaDeviceSynchronize()`
    function, which prevents the current step from being updated by the next step.
  prefs: []
  type: TYPE_NORMAL
- en: We also can disable these asynchronous operations by setting an environment,
    that is, `CUDA_LAUNCH_BLOCKING=1`. We can pass this to the Nsight System's profile
    option using the environment option (`-e`). Then, we can analyze the application's
    `align` operation with the host and kernel functions.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch has several NVTX featured APIs in their CUDA objects. PyTorch documentation
    can be found at [https://pytorch.org/docs/stable/_modules/torch/cuda/nvtx.html](https://pytorch.org/docs/stable/_modules/torch/cuda/nvtx.html).
    By calling the NVTX APIs in PyTorch directly, the CUDA NVTX APIs are called. This
    means we could obtain custom-tagged NVTX marks in the profiled timeline.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling a TensorFlow model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Profiling TensorFlow graphs requires that we have an NVTX plugin that enables
    NVTX annotations. To use NVTX annotations in TensorFlow, we need to install the `nvtx-plugins-tf` Python
    plugin using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: However, we don't have to do this if we use an NGC TensorFlow container later
    than version 19.o8.
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow graph APIs are symbolic APIs, so they require specific programming
    methods. The NVTX plugin provides two options for this: a decorator and a Python
    function.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of an NVTX decorator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is an example of an NVTX Python function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'The NVTX plugin provides NVTXHook, which allows us to profile the TF estimator
    and session. For example, we can use the hook as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can apply this to either option using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, we can use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s apply this to the sample ResNet-50 code and review the operation.
    The example code can be found in the `05_framework_profile/tensorflow/RN50v1.5`
    folder:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s begin by applying `NVTXHook` to the estimator. The training graph''s
    definition can be found in the `runtime/runner.py` file on line 312\. Ahead of
    building the graph, we will append `NVTXHook` to the list of hooks, as shown in
    the following block of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/e773aace-88c2-445b-9701-5506ba0bf8d1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, we will apply the NVTX annotation to the model-building function. The `model_build()`
    function can be found in the `ResnetModel` class in the `model/resnet_v1_5.py` file.
    The following code shows an example of placing an NVTX annotation by using a Python
    function on the `conv1` layer in the `model_build()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/52fc56c0-b159-4e56-8899-67ab37fb9f4c.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding code, we need to be cautious when to use proper inputs and
    outputs when using the `nvtx_tf.ops.start()` and `nvtx_tf.ops.end()` functions.
    Only place NVTX annotations in the other layers. Be sure that the final fully
    connected layer's output is the output of the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also have to disable the code to check the number of trainable variables
    it has. If NVTX''s `trainable` parameter''s value is `True`, the size changes.
    At line 174 in the `resnet_v1_5.py` file, there''s a block of assertion code that
    checks the number of that variable. Simply comment it out, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e4654a73-e106-4d11-a41b-c66ccd5d104f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We also use NVTX decorators for the ResNet building blocks. In the `model/blocks`
    directory, we can find the `conv2d` and ResNet bottleneck block implementations
    in `conv2d_blocks.py` and `resnet_bottleneck_block.py`. In the `conv2d_blocks.py`
    file, we can decorate `conv2d_block()` function to annotate NVTX profiling, as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/abed69dc-571e-4970-af59-ba04f3dd2ac6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the same way, we can do the same to the `resnet_bottleneck_block.py` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1edce41d-4992-46c0-b7d6-532b4bfcb157.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s profile the model. Like we did with the PyTorch container, we will
    use TensorFlow''s NGC container. We will assume that the `imagenet` dataset''s
    `tfrecord` files are located in the `/raid/datasets/imagenet/tfrecord` directory. The
    following code shows a bash shell script that executes the container and profiles
    the network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: When we execute this function, we will get the `resnet50_tf.qdrep` file in the
    `RN50v1.5` directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s review the profiled output using the NVIDIA Nsight System:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/f5e3118b-c0bc-4cee-83c2-69947821d1f9.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we can confirm that backward propagation takes twice as long as the forward
    pass. This example code isn't synchronized with the CPU and the GPU. Due to this,
    we can see a larger time difference between the host and the GPU. As we place
    additional annotations in the building blocks, we will be able to see the sub-block
    annotations in the layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Profiling with NVIDIA Nsight Systems provides additional benefits when it comes
    to monitoring all-reduce''s execution time in multi-GPU training. The following
    screenshot shows the profiling result of a GPU that was training with two GPUs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e05ad54d-f8a5-482a-935c-f25ed3479b98.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the highlighted row, we can see the `ncclAllRecude()` function, which calls
    the backward propagation simultaneously. By doing this, we don''t get the delay
    of all-reduce operation. This example code uses Horovod to train multiple GPUs.
    If you want to learn more about this, visit Horovod''s GitHub page: [https://github.com/horovod/horovod](https://github.com/horovod/horovod).
    You can get the document and example code from here.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned how to use CUDA libraries for deep learning
    and performance benefits. While we reviewed their uses, we matched them with the
    deep learning mechanisms for each step. Thanks to the deep learning libraries
    that are available to us, we can implement a simple CNN without implementing the
    algorithms too. Then, we profiled the ResNet-50 model in PyTorch and TensorFlow
    using NVTX annotations.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the base algorithms may be impractical for some deep learning engineers
    and researchers. However, understanding the performance factors and the base operations
    can help you build efficient and effective deep learning-based products. Nowadays,
    we see many productized deep learning-based services. Engineers spend a lot of
    resources productizing their trained model, as well as training their model so
    that they get the lowest error rate possible. Hopefully, you managed to gain some
    insight into how you can use NVTX profiling on your deep learning applications.
    Using this knowledge, you can get more from your GPUs. Good luck!
  prefs: []
  type: TYPE_NORMAL
