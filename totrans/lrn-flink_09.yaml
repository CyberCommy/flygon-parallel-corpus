- en: Chapter 9. Deploying Flink on Cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the recent times, more and more companies have been investing in Cloud-based
    solutions and which is justified looking at the cost and efficiency we achieve
    through the Cloud. **Amazon Web Services** (**AWS**), **Google Cloud Platform**
    (**GCP**) and Microsoft Azure are the clear leaders so far in this business. Almost
    all of them provide big data solutions which are quite handy to use. The cloud
    provides efficient solutions in a timely manner where people don't need to worry
    about hardware purchases, networking, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are going to see how we can deploy Flink on cloud. We will
    see a detailed approach to installing and deploying applications on AWS and Google
    Cloud. So let's get started.
  prefs: []
  type: TYPE_NORMAL
- en: Flink on Google Cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Flink can be deployed on Google Cloud using one utility called BDUtil. It is
    an open source utility available for everyone to use [https://cloud.google.com/hadoop/bdutil](https://cloud.google.com/hadoop/bdutil).
    The very first step we need to do is to install **Google Cloud SDK**.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Google Cloud SDK
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Google Cloud SDK is an executable utility that can be installed on the Windows,
    Mac, or UNIX operating systems. You can choose the mode of installation based
    on your operating system. Here is a link that directs users about detailed installations
    at [https://cloud.google.com/sdk/downloads](https://cloud.google.com/sdk/downloads).
  prefs: []
  type: TYPE_NORMAL
- en: Here I assume that you are already familiar with Google Cloud concepts and the
    terminologies; if not I would recommend reading [https://cloud.google.com/docs/](https://cloud.google.com/docs/).
  prefs: []
  type: TYPE_NORMAL
- en: In my case, I will be using UNIX machine to launch a Flink-Hadoop cluster. So
    let's get started with the installation.
  prefs: []
  type: TYPE_NORMAL
- en: First, we need to download the installer for the Cloud SDK.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we un-tar the files by the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Once done, we need to initialize the SDK:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This will start an interactive installation process and will require you to
    provide input as and when needed. The following screenshot shows the process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Installing Google Cloud SDK](img/image_09_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'It is also recommended to get authenticated by executing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This will give you a URL to be opened in your machine's browser. On hitting
    that URL, you will a get code which will be used for authentication.
  prefs: []
  type: TYPE_NORMAL
- en: Once the authentication is done, we are all set for the BDUtil installation.
  prefs: []
  type: TYPE_NORMAL
- en: Installing BDUtil
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we said earlier, BDUtil is a utility developed by Google to facilitate hiccup-free
    big data installations on Google Cloud. You can install the following services:'
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop - HDP and CDH
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flink
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hama
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hbase
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tajo
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following steps are required to install BDUtil. First of all, we need to
    download the source code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Unzip the code by the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is recommended to use a **non-root account** for BDUtil operations if you
    are using it on one the Google Compute machine. Generally root logins are by default
    disabled on all compute engine machines.
  prefs: []
  type: TYPE_NORMAL
- en: Now we are all set with the BDUtil installation and ready for deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Launching a Flink cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'BDUtil needs at least one project in which we will do our installations and
    a bucket where temporary files can be kept. To create a bucket, you can go to
    the **Cloud Storage** sections and choose to create a bucket as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Launching a Flink cluster](img/image_09_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We have named this bucket at **bdutil-flink-bucket**. Next we need to edit
    the `bdutil_env.sh` file to configure information about the project name, bucket
    name and Google Cloud zone to be used. We can also set other things such as the
    machine type and Operating System. `bdutil_env.sh` looks as shown in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: By default, the configuration launches three node, Hadoop/Flink cluster with
    one master and two worker nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you are using the trial version of GCP, then it is recommended to use machine
    type as **n1-standard-2**. This will restrict the CPU and storage of the node
    type.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we are all set to launch the cluster, with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This will start creating machines and will deploy required software on it. It
    generally takes 10-20 minutes of time to get the cluster up and running if everything
    works well. Before starting the executing, you should review what the screen shot
    tell us.
  prefs: []
  type: TYPE_NORMAL
- en: '![Launching a Flink cluster](img/image_09_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Once complete, you will see some messages as shown in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In case of any failures in between, please check what logs say. You can visit
    the Google Cloud Compute Engine Console to get the exact IPs of the master and
    slave machines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now if you check the Job Manager UI, you should have two task managers and
    four task slots available for the use. You can hit URL `http://<master-node-ip>:8081`
    . The following is sample screenshot for the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Launching a Flink cluster](img/image_09_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Executing a sample job
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can check if everything is working fine by launching a sample word count
    program. In order to do so, we first need to log in to Flink Master node. The
    following command starts a sample word count program provided by Flink installation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the execution map of the job:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Executing a sample job](img/image_09_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here is another screenshot of the timeline with which all tasks got executed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Executing a sample job](img/image_09_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Shutting down the cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once we are done with all our executions and if we no longer wish to do any
    further use of the cluster then it is better to shut it down.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a command, we need to execute to shut down the cluster we
    started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Please make sure to confirm the configurations before deleting the cluster.
    The following is a screenshot which shows what it is going to delete and the complete
    process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Shutting down the cluster](img/image_09_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Flink on AWS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now let's look at how we can use Flink on **Amazon Web Services** (**AWS**).
    Amazon provides a hosted Hadoop service called **Elastic Map Reduce** (**EMR**).
    We can use and Flink in combination. We can do reading on EMR at [https://aws.amazon.com/documentation/elastic-mapreduce/](https://aws.amazon.com/documentation/elastic-mapreduce/).
  prefs: []
  type: TYPE_NORMAL
- en: Here I assume that you already have AWS account and knows basics of AWS.
  prefs: []
  type: TYPE_NORMAL
- en: Launching an EMR cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The very first thing we need to do is launch EMR cluster. We first need to
    log in to AWS account and choose **EMR** service from the console as shown in
    the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Launching an EMR cluster](img/image_09_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Next we go to EMR console and launch a three-node cluster with one master and
    two slave nodes. Here we choose minimum cluster size to avoid surprise billing.
    The following screenshot shows the EMR cluster creation screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Launching an EMR cluster](img/image_09_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Generally it takes 10-15 minutes for cluster to be up and running. Once the
    cluster is ready, we can do SSH to the cluster. For that we first need to click
    on **Create Security Group** section and add rule to add SSH port 22 rule. The
    following screen shows the security group section in which we need to edit **In
    Bound** traffic rule for SSH:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Launching an EMR cluster](img/image_09_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we are all set to login to the master node using SSH with the private key.
    You will see the following screen once you login with user name Hadoop:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Launching an EMR cluster](img/image_09_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Installing Flink on EMR
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Installing Flink is very easy once we have our EMR cluster ready. We need to
    do the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the Flink compatible to right Hadoop Version from link - [http://flink.apache.org/downloads.html](http://flink.apache.org/downloads.html).
    I am downloading Flink compatible with Hadoop 2.7 version:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to un-tar the installer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'And that is it, just go the un-tarred folder and set following environment
    variables and we are all set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Executing Flink on EMR-YARN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Executing Flink on YARN is very easy. We have already learnt the details on
    Flink on YARN in the previous chapter. The following steps shows a sample job
    execution. This would submit a single Flink job to YARN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see immediately Flink executing will start and on completion, you
    will see the word count results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also look at YARN cluster UI as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Executing Flink on EMR-YARN](img/image_09_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Starting a Flink YARN session
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Alternatively we can also start a YARN session by blocking the resources which
    we have already seen in the previous chapter. A Flink YARN session will create
    a continuously running YARN session which can be used to execute multiple Flink
    jobs. This session keeps on running until we stop it.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start the Flink YARN session, we need to execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we start two Task Managers with 768 MB memory each and 4 slots. You will
    see the YARN session ready as shown in the console logs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a screenshot of the Flink Job Manager UI, where we can see two Task
    Managers and eight task slots:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Starting a Flink YARN session](img/image_09_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Executing Flink job on YARN session
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now we can use this YARN session to submit Flink Jobs by executing the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see word count job getting executed as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a screenshot of the job execution details and task breakup:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Executing Flink job on YARN session](img/image_09_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also see the timeline details on which all task were executed in parallel
    and which are in sequential manner. Here is screenshot of the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Executing Flink job on YARN session](img/image_09_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Shutting down the cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once all our work is done, it is important to shut down the cluster. To do this,
    we again need to go to AWS console and click on the **Terminate** button.
  prefs: []
  type: TYPE_NORMAL
- en: Flink on EMR 5.3+
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AWS has now started supporting Flink by default in its EMR cluster. In order
    to get that we have to follow these instructions.
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, we have to go to AWS EMR create cluster screen and then click
    on **Go to advanced options link** as heighted in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Flink on EMR 5.3+](img/image_09_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Next you will have a screen which will all you to choose additional services
    you wish to have. There you need to check Flink 1.1.4:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Flink on EMR 5.3+](img/image_09_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: And then click on the **Next** button to continue the rest of the setup. The
    remaining steps would be same as we saw in the previous sections. Once the cluster
    is up and running, you are all set to use Flink directly.
  prefs: []
  type: TYPE_NORMAL
- en: Using S3 in Flink applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Amazon Simple Storage Service** (**S3**) is a Software-as-a-Service provided
    by AWS to store in the AWS Cloud. Many companies use S3 for cheap data storage.
    It is a hosted filesystem as a service. S3 can be used as alternative to the HDFS.
    One can think of using S3 over HDFS if he/she does not want to invest in complete
    Hadoop cluster. Flink provides you API to allow reading data stored on S3.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use S3 objects like simple files. The following code snippet shows how
    to use S3 object in Flink:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: S3 is treated like any other filesystem by Flink. It uses S3 client for Hadoop.
  prefs: []
  type: TYPE_NORMAL
- en: To access S3 objects, Flink needs authentication. This can be provided by using
    AWS IAM service. This method helps maintaining the security as we don't need to
    distribute the access and secret keys.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learnt how we can deploy Flink on AWS and GCP. This is very
    handy for faster deployments and installations. We can spawn and delete Flink
    cluster with minimum efforts.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to learn about the best practice one should
    follow in order to efficiently use Flink.
  prefs: []
  type: TYPE_NORMAL
