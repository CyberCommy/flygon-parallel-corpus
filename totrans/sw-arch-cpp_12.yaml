- en: Continuous Integration and Continuous Deployment
  prefs: []
  type: TYPE_NORMAL
- en: In one of the previous chapters on building and packaging, we learned about
    different build systems and different packaging systems that our application can
    use. **Continuous Integration** (**CI**) and **Continuous Deployment** (**CD**)
    allow us to use knowledge of building and packaging to improve service quality
    and the robustness of the application we are developing.
  prefs: []
  type: TYPE_NORMAL
- en: Both CI and CD rely on good test coverage. CI uses mostly unit tests and integration
    tests, whereas CD depends more on smoke tests and end-to-end tests. You learned
    more about the different aspects of testing in [Chapter 8](160259bc-b601-4854-9aa9-cabe2c4fd691.xhtml),
    *Writing Testable Code*. With this knowledge, you are ready to build a CI/CD pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding CI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reviewing code changes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring test-driven automation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing deployment as code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building deployment code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a CD pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using immutable infrastructure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The sample code of this chapter can be found at [https://github.com/PacktPublishing/Software-Architecture-with-Cpp/tree/master/Chapter09](https://github.com/PacktPublishing/Software-Architecture-with-Cpp/tree/master/Chapter09).
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand the concepts explained in this chapter, you''ll require the following
    installations:'
  prefs: []
  type: TYPE_NORMAL
- en: A free GitLab account
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ansible version 2.8+
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Terraform version 0.12+
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Packer version 1.4+
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding CI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CI is the process of shortening the integration cycles. Whereas in traditional
    software, many different features could have been developed separately and only
    integrated prior to release, in projects developed with CI, integration can occur
    several times a day. Usually, each change a developer makes is tested and integrated
    at the same time as it is committed to the central repository.
  prefs: []
  type: TYPE_NORMAL
- en: Since testing occurs just after development, the feedback loop is much quicker.
    This lets developers fix bugs more easily (as they usually still remember what
    was changed). In contrast to the traditional approach of testing just prior to
    release, CI saves a lot of work and improves the quality of software.
  prefs: []
  type: TYPE_NORMAL
- en: Release early, release often
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Have you ever heard the saying "release early, release often"? This is a software
    development philosophy that emphasizes the importance of short release cycles.
    Short release cycles, in turn, provide a much shorter feedback loop between planning,
    development, and validation. When something breaks, it should break as early as
    possible so that the costs of fixing the problem are relatively small.
  prefs: []
  type: TYPE_NORMAL
- en: This philosophy was popularized by Eric S. Raymond (also known as ESR) in his
    1997 essay entitled *The* *Cathedral and the Bazaar*. There's also a book with
    the same title that contains this and other essays by the author. Considering
    ESR's activity within open source movements, the "release early, release often"
    mantra became synonymous with how open source projects operated.
  prefs: []
  type: TYPE_NORMAL
- en: Some years later, the same principle moved beyond just open source projects.
    With the rising interest in Agile methodologies, such as Scrum, the "release early,
    release often" mantra became synonymous with development sprints that end with
    a product increment. This increment is, of course, a software release, but usually,
    there are many other releases that happened during the sprint.
  prefs: []
  type: TYPE_NORMAL
- en: How can you achieve such short release cycles? One answer is to rely on automation
    as much as possible. Ideally, every commit to the code repository should end as
    a release. Whether this release ends up facing the customers is another matter.
    What's important is that every code change can result in a usable product.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, building and releasing every single commit to the public would be
    a tedious job for any developer. Even when everything is scripted, this can add
    unnecessary overhead to the usual chores. This is why you would want to set up
    a CI system to automate the releases for you and your development team.
  prefs: []
  type: TYPE_NORMAL
- en: Merits of CI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'CI is the concept of integrating the work of several developers, at least daily.
    As already discussed, sometimes it can mean several times a day. Every commit
    that enters the repository is integrated and validated separately. The build system
    checks whether the code can be built without errors. The packaging system may
    create a package that is ready to be saved as an artifact or even deployed later
    on when CD is used. Finally, the automated tests check that no known regression
    occurred in relation to the change. Let''s now see its merits in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: CI allows for the rapid solving of problems. If one of the developers forgot
    a semicolon at the end of the line, the compiler on the CI system will catch that
    error right away before this incorrect code reaches other developers, thereby
    impeding their work. Of course, developers should always build the changes and
    test them before committing the code, but minor typos can go unnoticed on the
    developer's machine and enter the shared repository anyway.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another benefit of using CI is that it prevents the common "works on my machine"
    excuse. If a developer forgets to commit a necessary file, the CI system will
    fail to build the changes, yet again preventing them from spreading further and
    causing mischief to the whole team. The special configuration of one developer's
    environment also stops being an issue. If a change builds on two machines, the
    developer's computer and the CI system, we are safe to assume that it should build
    on other machines as well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gating mechanism
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If we want CI to bring value beyond simply building packages for us, we need
    a gating mechanism. This gating mechanism will allow us to discern good code changes
    from bad ones, thus keeping our application safe from modifications that would
    render it useless. For this to happen, we need a comprehensive suite of tests.
    Such a suite allows us to automatically recognize when a change is problematic,
    and we're able to do it quickly.
  prefs: []
  type: TYPE_NORMAL
- en: For individual components, unit tests play the role of a gating mechanism. A
    CI system can discard any changes that do not pass unit tests or any changes that
    do not reach a certain code coverage threshold. At the time of building individual
    components, a CI system may also use integration tests to further ensure that
    the changes are stable, not only by themselves but also are acting properly together.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the pipeline with GitLab
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Throughout this chapter, we will use popular open source tools to build a full
    CI/CD pipeline consisting of gating mechanisms, automated deployment, and also
    showing the concepts of infrastructure automation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first such tool is GitLab. You may have heard about it as a Git hosting
    solution, but in reality, it''s much more than that. GitLab comes in several distributions,
    namely, the following:'
  prefs: []
  type: TYPE_NORMAL
- en: An open source solution that you can host on your own premises
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Self-hosted paid versions that offer additional features over the open source
    community edition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And finally, a **Software-as-as-Service** (**SaaS**) managed offer hosted under
    [https://gitlab.com](https://gitlab.com)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the requirements of this book, each of the distributions has all the necessary
    features. We will, therefore, focus on the SaaS version, as this requires the
    least amount of preparation.
  prefs: []
  type: TYPE_NORMAL
- en: Although [https://gitlab.com](https://gitlab.com) is mainly targeted at open
    source projects, you can also create private projects and repositories if you
    don't feel like sharing your work with the entire world. This allows us to create
    a new private project in GitLab and populate it with the code we have already
    demonstrated in [Chapter 7](7f997c01-2634-4584-be95-0b068f448312.xhtml), *Building
    and Packaging*.
  prefs: []
  type: TYPE_NORMAL
- en: A lot of modern CI/CD tools could work instead of GitLab CI/CD. Examples include
    GitHub Actions, Travis CI, CircleCI, and Jenkins. We've chosen GitLab as it can
    be used both in SaaS form and on-premises, so should accommodate a lot of different
    use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will then use our previous build system to create a simple CI pipeline in
    GitLab. These pipelines are described in the YAML file as a series of steps and
    metadata. An example pipeline building all the requirements, as well as the sample
    project from [Chapter 7](7f997c01-2634-4584-be95-0b068f448312.xhtml), *Building
    and Packaging*, would look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Saving the preceding file as `.gitlab-ci.yml` in the root directory of your
    Git repository will automatically enable CI in GitLab and run the pipeline with
    each subsequent commit.
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing code changes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Code reviews can be used both with CI systems and without them. Their main purpose
    is to double-check each change introduced to the code to make sure that it is
    correct, that it fits the application's architecture, and that it follows the
    project's guidelines and best practices.
  prefs: []
  type: TYPE_NORMAL
- en: When used without CI systems, it is often the reviewer's task to test the change
    manually and verify it is working as expected. CI reduces this burden, letting
    software developers focus on the logical structure of the code.
  prefs: []
  type: TYPE_NORMAL
- en: Automated gating mechanisms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Automated tests are only one example of a gating mechanism. When their quality
    is high enough, they can guarantee the code works according to design. But there's
    still a difference between code that works correctly and good code. As you've
    learned from this book so far, code can be considered good if it fulfills several
    values. Being functionally correct is just one of them.
  prefs: []
  type: TYPE_NORMAL
- en: There are other tools that can help achieve the desired standard of your code
    base. Some of them have been covered in previous chapters, so we won't go into
    the details. Keep in mind that using linters, code formatters, and static analysis
    in your CI/CD pipeline is a great practice. While static analysis can act as a
    gating mechanism, you can apply linting and formatting to each commit that enters
    the central repository to make it consistent with the rest of the code base. You
    will find more on linters and formatters in the appendix.
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, this mechanism will only have to check whether the code has already
    been formatted, as the formatting step should be done by developers before pushing
    the code to the repository. When using Git as a version control system, the mechanism
    of Git Hooks can prevent committing code without running the necessary tools on
    it.
  prefs: []
  type: TYPE_NORMAL
- en: But automated analysis can only get you so far. You can check that the code
    is functionally complete, that it is free of known bugs and vulnerabilities, and
    that it fits within the coding standard. This is where manual inspection comes
    in.
  prefs: []
  type: TYPE_NORMAL
- en: Code review – the manual gating mechanism
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Manual inspection of a code change is often known as a code review. The aim
    of the code review is to identify problems, both with the implementation of specific
    subsystems and adherence to the overall architecture of the application. Automated
    performance tests may or may not discover potential problems with a given function.
    Human eyes, on the other hand, can usually spot a sub-optimal solution to the
    problem. Whether it is the wrong data structure or an algorithm with unnecessarily
    high computational complexity, a good architect should be able to pinpoint the
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: But it isn't just the architect's role to perform code reviews. Peer reviews,
    that is, code reviews performed by peers of the author, also have their place
    in the development process. Such reviews are valuable not just because they allow
    colleagues to find bugs in each other's code. The more important aspect is the
    fact that many teammates are suddenly aware of what everybody else is doing. This
    way, when there is an absence in the team (whether because of a long meeting,
    vacation, or job rotation), another team member can substitute for the missing
    one. Even if they're not an expert on the topic, every other member at least knows
    where the interesting code is located and everyone should be able to remember
    the last changes to the code. This means both the time when they happened and
    the scope and content of those changes.
  prefs: []
  type: TYPE_NORMAL
- en: With more people aware of how the insides of your application appear, it is
    also more probable that they can figure out a correlation between recent changes
    in one component and a freshly discovered bug. Even though every person on your
    team probably has different experience, they can pool their resources when everyone
    knows the code quite thoroughly.
  prefs: []
  type: TYPE_NORMAL
- en: So code reviews can check whether the change fits within the desired architecture
    and whether its implementation is correct. We call such a code review an architectural
    review, or an expert's review.
  prefs: []
  type: TYPE_NORMAL
- en: Another type of code review, the peer review, not only helps uncover bugs, but
    also raises awareness within the team about what other members are working on.
    If necessary, you can also perform a different kind of expert review when dealing
    with changes that integrate with external services.
  prefs: []
  type: TYPE_NORMAL
- en: As each interface is a source of potential problems, changes close to the interface
    level should be treated as especially dangerous. We advise you to supplement the
    usual peer review with an expert coming from the other side of the interface.
    For example, if you are writing a producer's code, ask a consumer for a review.
    This way, you ensure you won't miss some vital use case that you may consider
    very improbable, but that the other side uses constantly.
  prefs: []
  type: TYPE_NORMAL
- en: Different approaches to a code review
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You will most often conduct code reviews asynchronously. This means that the
    communication between the author of the change under review and the reviewers
    does not happen in real time. Instead, each of the actors posts their comments
    and suggestions at any given time. Once there are no more comments, the author
    reworks the original change and once again puts it under review. This can take
    as many rounds as necessary until everyone agrees that no further corrections
    are necessary.
  prefs: []
  type: TYPE_NORMAL
- en: When a change is particularly controversial and an asynchronous code review
    takes too much time, it is beneficial to conduct a code review synchronously.
    This means a meeting (in-person or remotely) to resolve any opposing views on
    the way forward. This will happen in particular when a change contradicts one
    of the initial decisions due to the new knowledge acquired while implementing
    the change.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some dedicated tools aimed solely at code reviews. More often, you
    will want to use a tool that is built into your repository server, which includes
    services such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: GitHub
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bitbucket
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GitLab
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gerrit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of the preceding offer both Git hosting and code review. Some of them go
    even further, providing a whole CI/CD pipeline, issue management, wiki, and much
    more.
  prefs: []
  type: TYPE_NORMAL
- en: When you use the combined package of code hosting and code review, the default
    workflow is to push the changes as a separate branch and then ask the project's
    owner to merge the changes in a process known as a pull request (or a merge request).
    Despite the fancy name, the pull request or merge request informs the project
    owner that you have code that you wish to merge with the main branch. This means
    that the reviewers should review your changes to make sure everything is in order.
  prefs: []
  type: TYPE_NORMAL
- en: Using pull requests (merge requests) for a code review
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Creating pull requests or merge requests with systems such as GitLab is very
    easy. First of all, when we push a new branch to the central repository from the
    command line, we can observe the following message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: If you previously had CI enabled (by adding the `.gitlab-ci.yml` file), you'll
    also see that the newly pushed branch has been subjected to the CI process. This
    occurs even before you open a merge request, and it means you can postpone tagging
    your colleagues until you get information from CI that every automated check has
    passed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The two main ways to open a merge request are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: By following the link mentioned in the push message
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By navigating to merge requests in the GitLab UI and selecting the Create merge
    request button or the New merge request button
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you submit the merge request, having completed all the relevant fields,
    you will see that the status of the CI pipeline is also visible. If the pipeline
    fails, merging the change wouldn't be possible.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring test-driven automation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CI mainly focuses on the integration part. It means building the code of different
    subsystems and making sure it works together. While tests are not strictly required
    to achieve this purpose, running CI without them seems like a waste. CI without
    automated tests makes it easier to introduce subtle bugs to code while giving
    a false sense of security.
  prefs: []
  type: TYPE_NORMAL
- en: That's one of the reasons why CI often goes hand in hand with continuous testing,
    which we'll cover in this next section.
  prefs: []
  type: TYPE_NORMAL
- en: Behavior-driven development
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we have managed to set up a pipeline that we can call continuous building.
    Each change we make to the code ends up being compiled, but we don't test it any
    further. Now it's time to introduce the practice of continuous testing. Testing
    on a low level will also act as a gating mechanism to automatically reject all
    the changes that do not satisfy requirements.
  prefs: []
  type: TYPE_NORMAL
- en: How can you check whether a given change satisfies requirements? This is best
    achieved by writing tests based on these requirements. One of the ways to do this
    is by following **Behavior-Driven Development** (**BDD**). The concept of BDD
    is to encourage deeper collaboration between the different actors in an Agile
    project.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike the traditional approach, where tests are written either by developers
    or the QA team, with BDD, the tests are created collaboratively by the following
    individuals:'
  prefs: []
  type: TYPE_NORMAL
- en: Developers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: QA engineers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Business representatives.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The most common way to specify tests for BDD is to use the Cucumber framework,
    which uses plain English phrases to describe the desired behavior of any part
    of the system. These sentences follow a specific pattern that can then be turned
    into working code, integrating with the testing framework of choice.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is official support for C++ in the Cucumber framework and it''s based
    on CMake, Boost, GTest, and GMock. After specifying the desired behavior in the
    cucumber format (which uses a domain-specific language known as Gherkin), we also
    need to provide the so-called step definitions. Step definitions are the actual
    code corresponding to the actions described in the cucumber specification. For
    example, consider the following behavior expressed in Gherkin:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can save it as a `sum.feature` file. In order to generate a valid C++ code
    with tests, we would use the appropriate step definitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: When building an application from scratch, it's a good idea to follow the BDD
    pattern. This book aims to show the best practices you can use in such a greenfield
    project. But it doesn't mean you can't try our examples in an existing project.
    CI and CD can be added at any given time during the life cycle of the project.
    Since it's always a good idea to run your tests as often as possible, using a
    CI system just for the purpose of continuous testing is almost always a good idea.
  prefs: []
  type: TYPE_NORMAL
- en: If you don't have behavior tests, you shouldn't need to worry. You can add them
    later and, for the moment, just focus on those tests you already have. Whether
    they are unit tests or end-to-end tests, anything that helps you assess the state
    of your application is a good candidate for the gating mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: Writing tests for CI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For CI, it's best to focus on unit tests and integration tests. They work on
    the lowest possible level, which means they're usually quick to execute and have
    the smallest requirements. Ideally, all unit tests should be self-contained (no
    external dependencies like a working database) and able to run in parallel. This
    way, when the problem appears on the level where unit tests are able to catch
    it, the offending code would be flagged in a matter of seconds.
  prefs: []
  type: TYPE_NORMAL
- en: There are some people who say that unit tests only make sense in interpreted
    languages or languages with dynamic typing. The argument goes that C++ already
    has testing built-in by means of the type system and the compiler checking for
    erroneous code. While it's true that type checking can catch some bugs that would
    require separate tests in dynamically typed languages, this shouldn't be used
    as an excuse not to write unit tests. After all, the purpose of unit tests isn't
    to verify that the code can execute without any problems. We write unit tests
    to make sure our code not only executes, but also fulfills all the business requirements
    we have.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an extreme example, take a look at the following two functions. Both of
    them are syntactically correct and they use proper typing. However, just by looking
    at them, you can probably guess which one is correct and which isn''t. Unit tests
    help to catch this kind of misbehavior:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding function returns a sum of the two arguments provided. The following
    one returns just the value of the first argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Even though the types match and the compiler won't complain, this code wouldn't
    perform its task. To distinguish useful code from erroneous code, we use tests
    and assertions.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Having already established a simple CI pipeline, it is very easy to extend
    it with testing. Since we are already using CMake and CTest for the building and
    testing process, all we need to do is add another step to our pipeline that will
    execute the tests. This step may look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'An entire pipeline will therefore appear as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This way, each commit will not only be subjected to the build process, but also
    to testing. If one of the steps fails, we will be notified which one was the source
    of the failure and we could see in the dashboard which steps were successful.
  prefs: []
  type: TYPE_NORMAL
- en: Managing deployment as code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With changes tested and approved, now it's time to deploy them to one of the
    operating environments.
  prefs: []
  type: TYPE_NORMAL
- en: There are many tools to help with deployment. We decided to provide examples
    with Ansible as this doesn't require any setup on the target machines besides
    a functional Python installation (which the majority of UNIX systems already have
    anyway). Why Ansible? It is very popular in the configuration management space
    and it's backed up by a trustworthy open source company (Red Hat).
  prefs: []
  type: TYPE_NORMAL
- en: Using Ansible
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Why not use something that's already available, such as Bourne shell script
    or PowerShell? For simple deployments, shell scripts may be a better approach.
    But as our deployment process becomes more complex, it is much harder to handle
    every possible initial state using the shell's conditional statements.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with differences between initial states is actually something Ansible
    is especially good at. Unlike traditional shell scripts, which use the imperative
    form (move this file, edit that file, run a particular command), Ansible playbooks,
    as they are called, use the declarative form (make sure the file is available
    in this path, make sure the file contains specified lines, make sure the program
    is running, make sure the program completes successfully).
  prefs: []
  type: TYPE_NORMAL
- en: This declarative approach also helps to achieve idempotence. Idempotence is
    a feature of a function that means applying the function several times over will
    have exactly the same results as a single application. If the first run of an
    Ansible playbook introduces some changes to the configuration, each subsequent
    run will already start in the desired state. This prevents Ansible from performing
    any additional changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words, when you invoke Ansible, it will first assess the current state
    of all the machines you wish to configure:'
  prefs: []
  type: TYPE_NORMAL
- en: If any of them requires any changes, Ansible will only run the tasks required
    to achieve the desired state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If there's no need to modify a particular thing, Ansible won't touch it. Only
    when the desired and actual states differ will you see Ansible taking action to
    converge the actual state toward the desired one described by the contents of
    the playbook.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How Ansible fits with the CI/CD pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ansible's idempotence makes it a great target to use in CI/CD pipelines. After
    all, there's no risk in running the same Ansible playbook multiple times even
    if nothing changes between the two runs. If you use Ansible for your deployment
    code, creating a CD is just a matter of preparing appropriate acceptance tests
    (such as smoke tests or end-to-end tests).
  prefs: []
  type: TYPE_NORMAL
- en: The declarative approach may require changing the way you think about deployments,
    but the gains are well worth it. Besides running playbooks, you can also use Ansible
    to perform one-off commands on remote machines, but we won't cover this use case
    as it doesn't really help with deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Everything you can do with a shell you can do with Ansible's `shell` module.
    That's because, in the playbooks, you write tasks specifying which modules they
    use and their respective parameters. One such module is the aforementioned `shell`
    module, which simply executes the provided parameters in a shell on a remote machine.
    But what makes Ansible not only convenient but also cross-platform (at least when
    different UNIX distributions are concerned) is the availability of modules to
    manipulate common concepts such as user administration, package management, and
    similar instances.
  prefs: []
  type: TYPE_NORMAL
- en: Using components to create deployment code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to the regular modules provided in the standard library, there are
    also third-party components to allow for code reuse. You can test such components
    individually, which also makes your deployment code more robust. Such components
    are called roles. They contain a set of tasks to make a machine fit to take on
    a specific role, such as `webserver`, `db`, or `docker`. While some roles prepare
    the machine to provide particular services, other roles may be more abstract,
    such as the popular `ansible-hardening` role. This has been created by the OpenStack
    team and it makes it much harder to break into a machine secured by using this
    role.
  prefs: []
  type: TYPE_NORMAL
- en: When you start to understand the language Ansible uses, all the playbooks cease
    to be just the scripts. In turn, they will become the documentation of the deployment
    process. You can either use them verbatim by running Ansible, or you can read
    the described tasks and perform all the operations manually, for example, on an
    offline machine.
  prefs: []
  type: TYPE_NORMAL
- en: There is one risk related to using Ansible for deployment in your team. Once
    you start using it, you have to make sure that everyone on the team is able to
    use it and modify the relevant tasks. DevOps is a practice the whole team has
    to follow; it cannot be implemented only partially. When the application's code
    changes considerably, requiring appropriate changes on the deployment side, the
    person responsible for changes in the application should also supply the changes
    in the deployment code. Of course, this is something that your tests can verify,
    so the gating mechanism can reject the changes that are incomplete.
  prefs: []
  type: TYPE_NORMAL
- en: 'One noteworthy aspect of Ansible is that it can run both in a push and pull
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: The push model is when you run Ansible on your own machine or in the CI system.
    Ansible then connects to the target machine, for example, over an SSH connection,
    and performs the necessary steps on the target machine.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the pull model, the whole process is initiated by the target machine. Ansible's
    component, `ansible-pull`, runs directly on the target machine and checks the
    code repository to establish whether there's been any update to the particular
    branch. After refreshing the local playbook, Ansible performs all the steps as
    usual. This time, both the controlling component and the actual execution happen
    on the same machine. Most of the time, you will want to run `ansible-pull` periodically,
    for example, from within a cron job.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building deployment code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In its simplest form, deployment with Ansible may consist of copying a single
    binary to the target machine and then running that binary. We can achieve this
    with the following Ansible code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Every single task starts with a hyphen. For each of the tasks, you need to specify
    the module it uses (such as the `copy` module or the `shell` module), along with
    its parameters (if applicable). A task may also have a `name` parameter, which
    makes it easier to reference the task individually.
  prefs: []
  type: TYPE_NORMAL
- en: Building a CD pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have reached the point when we can safely build a CD pipeline using the tools
    we learned about in this chapter. We already know how CI operates and how it helps
    to reject changes that are unsuitable for release. The section on test automation
    presented different ways of making the rejection process more robust. Having smoke
    tests or end-to-end tests allows us to go beyond CI and to check whether the whole
    deployed service satisfies requirements. And with deployment code, we can not
    only automate the process of deployment, but also prepare a rollback when our
    tests begin to fail.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous deployment and continuous delivery
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By a funny coincidence, the abbreviation CD can mean two different things. The
    concepts of continuous delivery and Continuous deployment are pretty similar,
    but they have some subtle differences. Throughout the book, we are focusing on
    the concept of continuous deployment. This is the automated process that originates
    when a person pushes a change into the central repository and finishes with the
    change successfully deployed to the production environment with all the tests
    passing. We can therefore say that this is an end-to-end process as the developer's
    work travels all the way to the customer without manual intervention (following
    the code review, of course). You may have heard the term GitOps to relate to such
    an approach. As all operations are automated, pushing to a specified branch in
    Git triggers the deployment scripts.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous delivery doesn't go that far. Like CD, it features a pipeline able
    to release the final product and test it, but the final product is never automatically
    delivered to the customers. It can be delivered to the QA first or to the business
    for internal use. Ideally, the delivered artifact is ready to be deployed in the
    production environment as soon as the internal clients accept it.
  prefs: []
  type: TYPE_NORMAL
- en: Building an example CD pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's put all of these skills together once again using the GitLab CI as an
    example to build our pipeline. Following the testing step, we will add two more
    steps, one that creates the package and another one that uses Ansible to deploy
    this package.
  prefs: []
  type: TYPE_NORMAL
- en: 'All we need for the packaging step is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: When we add the package step containing artifacts definitions, we'll be able
    to download them from the dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this, we can invoke Ansible as part of the deployment step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The final pipeline would then look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: To see the whole example, go to the repository from the *Technical requirements*
    section for the original sources.
  prefs: []
  type: TYPE_NORMAL
- en: Using immutable infrastructure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are sufficiently confident with your CI/CD pipeline, you may go one step
    further. Instead of deploying artifacts of the application, you can deploy artifacts
    of the *system*. What's the difference? We will come to know about this in the
    following sections.
  prefs: []
  type: TYPE_NORMAL
- en: What is immutable infrastructure?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Previously, we focused on how to make your application's code deployable on
    the target infrastructure. The CI system created software packages (such as containers)
    and those packages were then deployed by the CD process. Each time the pipeline
    ran, the infrastructure stayed the same, but the software differed.
  prefs: []
  type: TYPE_NORMAL
- en: The point is, if you are using cloud computing, you can treat infrastructure
    just like any other artifact. Instead of deploying a container, you can deploy
    an entire **Virtual Machine** (**VM**), for example, as an AWS EC2 instance. You
    can build such a VM image upfront as yet another element of your CI process. This
    way, versioned VM images, as well as the code required to deploy them, become
    your artifacts, and not the containers themselves.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two tools, both authored by HashiCorp, that deal with precisely this
    scenario. Packer helps to create VM images in a repeatable way, storing all the
    instructions as code, usually in the form of a JSON file. Terraform is an Infrastructure
    as Code tool, which means it''s used to provision all the necessary infrastructure
    resources. We will use the output from Packer as input for Terraform. This way,
    Terraform will create an entire system consisting of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Instance groups
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load balancers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VPCs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other cloud elements while using the VMs containing our own code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The title of this section may confuse you. Why is it called **immutable infrastructure**
    while we are clearly advocating to change the entire infrastructure after every
    commit? The concept of immutability may be clearer to you if you've studied functional
    languages.
  prefs: []
  type: TYPE_NORMAL
- en: 'A mutable object is one whose state we can alter. In infrastructure, this is
    pretty easy to understand: you can log in to the VM and download a more recent
    version of the code. The state is no longer the same as it was prior to your intervention.'
  prefs: []
  type: TYPE_NORMAL
- en: An immutable object is one whose state we cannot alter. It means we have no
    means of logging in to the machines and changing things. Once we deploy a VM from
    an image, it stays like that until we destroy it. This may sound terribly cumbersome,
    but in fact, it solves a few problems of software maintenance.
  prefs: []
  type: TYPE_NORMAL
- en: The benefits of immutable infrastructure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First of all, immutable infrastructure makes the concept of configuration drift
    obsolete. There is no configuration management so there can also be no drift.
    The upgrade is much safer as well because we cannot end up in a half-baked state.
    That is the state that''s neither the previous version nor the next version, but
    something in between. The deployment process provides binary information: either
    the machine is created and operational or it isn''t. There''s no other way.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For immutable infrastructure to work without affecting uptime, you also need
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some degree of redundancy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After all, the upgrade process consists of taking down an entire instance. You
    cannot rely on this machine's address or anything that's particular to that one
    machine. Instead, you need to have at least a second one that will handle the
    workload while you replace the other one with the more recent version. When you
    finish upgrading the one machine, you can repeat the same process with another
    one. This way, you will have two upgraded instances without losing the service.
    Such a strategy is known as the rolling upgrade.
  prefs: []
  type: TYPE_NORMAL
- en: As you can realize from the process, immutable infrastructure works best when
    dealing with stateless services. When your service has some form of persistence,
    things become tougher to implement properly. In that case, you usually have to
    split the persistence level into a separate object, for example, an NFS volume
    containing all of the application data. Such volumes can be shared across all
    the machines in an instance group and each new machine that comes up can access
    the common state left by the previous running applications.
  prefs: []
  type: TYPE_NORMAL
- en: Building instance images with Packer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Considering our example application is already stateless, we can proceed with
    building an immutable infrastructure on top of it. Since the artifacts Packer
    generates are VM images, we have to decide on the format and the builder we would
    like to use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s focus our example on Amazon Web Services, while keeping in mind that
    a similar approach will also work with other supported providers. A simple Packer
    template may look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code will build an image for Amazon Web Services using the EBS
    builder. The image will reside in `eu-central-1` region and will be based on `ami-5900cc36`,
    which is a Debian Jessie image. We want the builder to be a `t2.micro` instance
    (that's a VM size in AWS). To prepare our image, we run the two `apt-get` commands.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also reuse the previously defined Ansible code and, instead of using
    Packer to provision our application, we can substitute Ansible as the provisioner.
    Our code will appear as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The changes are in the `provisioners` block and also a new block, `post-processors`,
    is added. This time, instead of shell commands, we are using a different provisioner
    that runs Ansible for us. The post-processor is here to produce the results of
    the build in a machine-readable format. Once Packer finishes building the desired
    artifact, it returns its ID and also saves it in `manifest.json`. For AWS, this
    would mean an AMI ID that we can then feed to Terraform.
  prefs: []
  type: TYPE_NORMAL
- en: Orchestrating the infrastructure with Terraform
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Creating an image with Packer is the first step. After that, we would like to
    deploy the image to use it. We can build an AWS EC2 instance based on the image
    from our Packer template using Terraform.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example Terraform code would look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This creates a key pair and an EC2 instance using this key pair. The EC2 instance
    is based on AMI provided as a variable. When calling Terraform, we will set this
    variable to point to the image generated by Packer.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By now, you should have learned how implementing CI at the beginning of the
    project can help you save time in the long run. It can also reduce work in progress,
    especially when paired with CD. In this chapter, we've presented useful tools
    that can help you implement both of the processes.
  prefs: []
  type: TYPE_NORMAL
- en: We've shown how GitLab CI allows us to write pipelines in YAML files. We've
    discussed the importance of code review and explained the differences between
    the various forms of code review. We've introduced Ansible, which assists in configuration
    management and the creation of deployment code. Finally, we tried Packer and Terraform
    to move our focus from creating applications to creating systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'The knowledge in this chapter is not unique to the C++ language. You can use
    it in projects written in any language using any technology. The important thing
    that you should keep in mind is this: all applications require testing. A compiler
    or a static analyzer is not enough to validate your software. As an architect,
    you would also have to take into account not only your project (the application
    itself), but also the product (the system your application will work in). Delivering
    working code is no longer sufficient. Understanding the infrastructure and the
    process of deployment is crucial as they are the new building blocks of modern
    systems.'
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter is focused on the security of the software. We will cover the
    source code itself, the operating system level, and the possible interactions
    with external services as well as with end users.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In what ways does CI save time during development?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do you require separate tools to implement CI and CD?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When does it make sense to perform a code review in a meeting?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What tools can you use to assess the quality of your code during CI?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Who participates in specifying the BDD scenarios?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When would you consider using immutable infrastructure? When would you rule
    it out?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How would you characterize the differences between Ansible, Packer, and Terraform?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Continuous integration/continuous deployment/continuous delivery:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.packtpub.com/virtualization-and-cloud/hands-continuous-integration-and-delivery](https://www.packtpub.com/virtualization-and-cloud/hands-continuous-integration-and-delivery)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.packtpub.com/virtualization-and-cloud/cloud-native-continuous-integration-and-delivery](https://www.packtpub.com/virtualization-and-cloud/cloud-native-continuous-integration-and-delivery)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ansible:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.packtpub.com/virtualization-and-cloud/mastering-ansible-third-edition](https://www.packtpub.com/virtualization-and-cloud/mastering-ansible-third-edition)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.packtpub.com/application-development/hands-infrastructure-automation-ansible-video](https://www.packtpub.com/application-development/hands-infrastructure-automation-ansible-video)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Terraform:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.packtpub.com/networking-and-servers/getting-started-terraform-second-edition](https://www.packtpub.com/networking-and-servers/getting-started-terraform-second-edition)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.packtpub.com/big-data-and-business-intelligence/hands-infrastructure-automation-terraform-aws-video](https://www.packtpub.com/big-data-and-business-intelligence/hands-infrastructure-automation-terraform-aws-video)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cucumber:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.packtpub.com/web-development/cucumber-cookbook](https://www.packtpub.com/web-development/cucumber-cookbook)'
  prefs: []
  type: TYPE_NORMAL
- en: 'GitLab:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.packtpub.com/virtualization-and-cloud/gitlab-quick-start-guide](https://www.packtpub.com/virtualization-and-cloud/gitlab-quick-start-guide)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.packtpub.com/application-development/hands-auto-devops-gitlab-ci-video](https://www.packtpub.com/application-development/hands-auto-devops-gitlab-ci-video)'
  prefs: []
  type: TYPE_NORMAL
