- en: Developing Dockerized .NET Framework and .NET Core Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Docker is a platform for packaging, distributing, running, and managing applications.
    When you package your applications as Docker images, they all have the same shape.
    You can deploy, manage, secure, and upgrade them all in the same way. All Dockerized
    applications have the same requirements to run them: a Docker Engine running on
    a compatible operating system. Applications run in isolated environments, so you
    can host different application platforms and different platform versions on the
    same machine with no interference.'
  prefs: []
  type: TYPE_NORMAL
- en: In the .NET world, this means you can run multiple workloads on a single Windows
    machine. They could be ASP.NET websites, or **Windows Communication Foundation**
    (**WCF**) apps running as .NET console applications or .NET Windows Services.
    In the previous chapter we looked at Dockerizing legacy .NET applications without
    any code changes, but Docker has some simple expectations about how applications
    running inside containers should behave so that they can get the full benefit
    of the platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter we''ll look at how to build applications so that they can take
    complete advantage of the Docker platform, including:'
  prefs: []
  type: TYPE_NORMAL
- en: The integration points between Docker and your application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring your application with config files and environment variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring applications with health checks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running distributed solutions with components in different containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This will help you develop .NET and .NET Core applications that behave in a
    way Docker expects so that you can manage them fully with Docker.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Building good citizens for Docker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Separating dependencies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Breaking up monolithic applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building good citizens for Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Docker platform makes very few demands on applications which use it. You're
    not restricted to certain languages or frameworks, you don't need to use special
    libraries to communicate between the app and container, and you don't need to
    structure your application in a certain way.
  prefs: []
  type: TYPE_NORMAL
- en: To support the widest possible range of applications, Docker uses the console
    to communicate between the application and the container runtime. Application
    logs and error messages are expected on the console output and error streams.
    Storage managed by Docker is presented as a normal disk to the operating system,
    and Docker's networking stack is transparent. Applications will appear to be running
    on their own machine, connected to other machines by a normal TCP/IP network.
  prefs: []
  type: TYPE_NORMAL
- en: 'A good citizen for Docker is an app which makes very few assumptions about
    the system it''s running on, and uses basic mechanisms which all operating systems
    support: the filesystem, environment variables, networking, and the console. Most
    importantly, the application should only do one thing. As you''ve seen, when Docker
    runs a container, it starts the process specified in the Dockerfile or the command
    line, and it watches that process. When the process ends, the container exits.
    So ideally you should build your app to have a single process, which ensures Docker
    is watching the process that matters.'
  prefs: []
  type: TYPE_NORMAL
- en: These are recommendations though, not requirements. You can start multiple processes
    in a bootstrap script when a container starts and Docker will run it happily,
    but it will only monitor the last process that started. Your apps can write log
    entries to local files instead of the console and Docker will still run them,
    but you won't see any output if you use Docker to check the container logs.
  prefs: []
  type: TYPE_NORMAL
- en: In .NET you can easily meet the recommendations by running a console application,
    which provides simplified integration between the application and the host, and
    it's one reason why all .NET Core apps – including websites and Web APIs – run
    as console applications. With legacy .NET apps, you won't be able to make them
    into perfect citizens, but you can take care to package them so that they make
    good use of the Docker platform.
  prefs: []
  type: TYPE_NORMAL
- en: Hosting Internet Information Services (IIS) applications in Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Complete .NET Framework apps can be easily packaged into Docker images, but
    there are some limitations you need to be aware of. Microsoft provides Nano Server
    and Windows Server Core base images for Docker. The complete .NET Framework doesn't
    run on Nano Server, so to host your existing .NET apps in Docker, you need to
    use the Windows Server Core base image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Running from Windows Server Core means your application images will be around
    4 GB in size, the bulk of which is in the base image. You have a complete Windows
    Server operating system, with all the packages available to enable Windows Server
    features, like **Domain Name System** (**DNS**) and **Dynamic Host Configuration
    Protocol** (**DHCP**), even though you only want to use it for a single application
    role. It''s perfectly reasonable to run containers from Windows Server Core, but
    you need to be aware of the implications:'
  prefs: []
  type: TYPE_NORMAL
- en: The base image has a large surface area with a lot of software installed, which
    means it's likely to have more frequent security and functional patches.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The OS runs a lot of its own processes in addition to your application process,
    as several core parts of Windows run as background Windows services.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Windows has its own application platforms, with high-value feature sets for
    hosting and management, which do not natively integrate with the Docker approach.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can take an ASP.NET web application and Dockerize it in a few hours. It
    will build into a large Docker image that takes a little longer to distribute
    and start up than an application built on a lightweight, modern application stack.
    But you'll still have a single package with your whole application deployed, configured,
    and ready to run. This is a big step in improving quality and reducing deployment
    time, and it can be the first part of a program to modernize a legacy application.
  prefs: []
  type: TYPE_NORMAL
- en: To integrate an ASP.NET app more closely with Docker, you can modify how IIS
    logs are written, specify how Docker checks whether the container is healthy,
    and inject configuration into containers without any changes to the application
    code. If changing code is part of your modernization program, then with minimal
    changes, you can use the container's environment variables and filesystem for
    application configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring IIS for Docker-friendly logging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: IIS writes log entries to text files, recording HTTP requests and responses.
    You can configure exactly what fields are written, but the default installation
    records useful things like the route of the HTTP request, the response status
    code, and the time taken for IIS to respond. It would be good to surface these
    logs entries to Docker, but IIS manages its own log files, buffering entries before
    writing them to the disk, and rotating log files to manage the disk space.
  prefs: []
  type: TYPE_NORMAL
- en: Log management is a fundamental part of application platforms, which is why
    IIS takes care of it for web apps, but Docker has its own logging system. Docker
    logging is far more powerful and pluggable than the text filesystem that IIS uses,
    but it only reads log entries from the container's console output stream. You
    can't have IIS writing logs to the console because it runs in a background Windows
    Service with no console attached, so you need a different approach.
  prefs: []
  type: TYPE_NORMAL
- en: There are two options for this. The first is to build an HTTP module which plugs
    into the IIS platform with an event handler that receives logs from IIS. This
    handler can publish all messages to a queue or a Windows pipe, so you don't change
    how IIS logs; you just add another log sink. Then, you'd package your web application
    together with a console app that listens for published log entries and relays
    them on the console. The console app would be the entry point when a container
    starts, so every IIS log entry would get routed to the console for Docker to read.
  prefs: []
  type: TYPE_NORMAL
- en: The HTTP module approach is robust and scalable, but it adds more complexity
    than we need when we're getting started. The second option is simpler - configure
    IIS to write all of the log entries to a single text file, and in the startup
    command for the container, run a PowerShell script to watch that file and echo
    new log entries to the console. When the container is running, all the IIS log
    entries get echoed to the console, which surfaces them to Docker.
  prefs: []
  type: TYPE_NORMAL
- en: 'To set this up in the Docker image, you first need to configure IIS so that
    it writes all of the log entries from any site to a single file, and it lets the
    file grow without rotating it. You can do this with PowerShell, using the `Set-WebConfigurationProperty`
    cmdlet in the Dockerfile, modifying the central logging properties at the application
    host level. I use this cmdlet in the Dockerfile for the `dockeronwindows/ch03-iis-log-watcher`
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This is ugly code, but it shows you can write whatever you need in a Dockerfile
    to set up your application. It configures IIS to log all entries to a file in
    `C:\iislog`, and to set the maximum file size for log rotation, letting the log
    file grow to 4 GB. That's plenty of room to play with - remember, containers are
    not meant to be long-lived, so we shouldn't have gigabytes of log entries in a
    single container. IIS still uses a subdirectory format for the log file, so the
    actual log file path will be `C:\iislog\W3SVC\u_extend1.log`. Now that I have
    a known log file location, I can use PowerShell to echo log entries to the console.
  prefs: []
  type: TYPE_NORMAL
- en: 'I do this in the `CMD` instruction, so the final command that Docker runs and
    monitors is the PowerShell cmdlet to echo log entries. When new entries are written
    to the console, they get picked up by Docker. PowerShell makes it easy to watch
    the file, but there''s a complication because the file needs to exist before PowerShell
    can watch it. In the Dockerfile, I run multiple commands in sequence at startup:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Four things happen when a container starts:'
  prefs: []
  type: TYPE_NORMAL
- en: Start the IIS Windows service (W3SVC).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make an HTTP `GET` request to the localhost, which starts the IIS worker process
    and writes the first log entry.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Flush the HTTP log buffer, so the log file gets written to the disk and exists
    for PowerShell to watch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read the content of the log file in tail mode, so any new lines written to the
    file are shown on the console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'I can run a container from this image in the usual way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'When I send some traffic to the site by browsing to the container''s IP address
    (or using `Invoke-WebRequest` in PowerShell), I can see the IIS log entries that
    are relayed to Docker from the `Get-Content` cmdlet using `docker container logs`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'IIS always buffers log entries in the memory before writing them to the disk,
    so it micro-batches writes to improve performance. Flushing happens every 60 seconds,
    or when the buffer is 64 KB in size. If you want to force the IIS log in a container
    to flush, use the same `netsh` command I used in the Dockerfile: `docker container
    exec log-watcher netsh http flush logbuffer`. You''ll see an `Ok` output, and
    new entries will be in `docker container logs`.'
  prefs: []
  type: TYPE_NORMAL
- en: I've added the configuration to IIS in the image and a new command, which means
    all IIS log entries get echoed to the console. This will work for any application
    hosted in IIS, so I can echo HTTP logs for ASP.NET applications and static websites
    without any changes to the apps or the site content. Console output is where Docker
    looks for log entries, so this simple extension integrates logging from the existing
    application into the new platform.
  prefs: []
  type: TYPE_NORMAL
- en: Managing application configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The goal of packaging applications in Docker images is to use the same image
    in every environment. You don't build separate images for test and production,
    because that would make them separate apps and there could be inconsistencies
    between them. You should deploy your production app from the exact same Docker
    image that was tested by the users, which is the exact same image generated by
    the build process and used for all of your automated integration tests.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, some things need to change between environments – connection strings
    for the database, logging level, and feature switches. This is application configuration,
    and in the Docker world you build your application image with a default set of
    configuration, typically for the development environment.  At runtime you inject
    the correct configuration for the current environment into the container, and
    that overwrites the default configuration.
  prefs: []
  type: TYPE_NORMAL
- en: There are different approaches to injecting this configuration. In this chapter
    I'll show you how to use volume mounts and environment variables. In production,
    you'll be running a cluster of machines running Docker, and you can store configuration
    data in the cluster's secure database, as Docker config objects or Docker secrets.
    I'll cover this in [Chapter 7](bf6a5e90-bbba-435b-b0a0-734611e0e834.xhtml), *Orchestrating
    Distributed Solutions with Docker Swarm*.
  prefs: []
  type: TYPE_NORMAL
- en: Mounting configuration files in Docker volumes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Traditional application platforms use configuration files to change behavior
    between environments. .NET Framework apps have a rich XML-based configuration
    framework, and Java apps commonly use key-value pairs in properties files. You
    can add these configuration files to your application image in a Dockerfile, and
    when you run a container from the image, it will use this default configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Your application setup should use a specific directory for configuration files,
    and this will let you overwrite them at runtime by mounting a Docker volume. I''ve
    done this with a simple ASP.NET WebForms application in `dockeronwindows/ch03-aspnet-config:2e`.
    The Dockerfile only uses commands you''ve already seen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This uses Microsoft's ASP.NET image as the base, and copies in my application
    files – an ASPX page and some config files. In this example I'm using the default
    IIS website, which loads content from `C:\inetpub\wwwroot`, so I just have the `COPY`
    instructions in the Dockerfile, I don't need to run any PowerShell scripts.
  prefs: []
  type: TYPE_NORMAL
- en: 'ASP.NET expects to find the `Web.config` file in the application directory,
    but you can split out sections of config into separate files. I''ve done this
    with the `appSettings ` and `connectionStrings` sections, which are loaded from
    files in a subdirectory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The `config` directory is populated with default configuration files, so I
    can run a container from the image without having to specify any extra settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'When I fetch the container''s port and browse to it, I see the web page displaying
    values from the default config files:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/c2ba482d-3bc5-451f-af92-3f10af0aebb4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'I can run the app for a different environment by loading configuration files
    from a directory on the host, mounting the local directory as a volume targeting
    `C:\inetpub\wwwroot\config` inside the container. When the container runs, the
    contents of that directory will be loaded from the directory on the host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: I'm using PowerShell to run this command, which expands `$pwd` to the full value
    of the current directory, so I'm saying the `prod-config` directory in the current
    path should be mounted as `C:\inetpub\wwwroot\config` in the container. You can
    use fully-qualified paths too.
  prefs: []
  type: TYPE_NORMAL
- en: 'When I browse to this container''s port, I see different config values displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/a42dd810-c2c0-4f38-9f3b-455b9164f98d.png)'
  prefs: []
  type: TYPE_IMG
- en: The important thing here is that I'm using the exact same Docker image, with
    the same setup and the same binaries in every environment. Only the configuration
    files change, and Docker provides an elegant way of doing this.
  prefs: []
  type: TYPE_NORMAL
- en: Promoting environment variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Modern apps increasingly use environment variables for configuration settings
    because they're supported by practically every platform, from physical machines
    to PaaS, to serverless functions. All platforms use environment variables in the
    same way – as a store of key-value pairs – so by using environment variables for
    configuration, you make your app highly portable.
  prefs: []
  type: TYPE_NORMAL
- en: ASP.NET apps already have the rich configuration framework in `Web.config`,
    but with some small code changes, you can take key settings and move them to environment
    variables. This lets you build one Docker image for your app, which you can run
    in different platforms, setting environment variables in containers to change
    configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Docker lets you specify environment variables in the Dockerfile and give them
    initial default values. The `ENV` instruction sets environment variables, and
    you can set either one variable or many variables in each `ENV` instruction. The
    following example is from the Dockerfile for `dockeronwindows/ch03-iis-environment-variables:2e`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Settings added to the Dockerfile with `ENV` become part of the image, so every
    container you run from the image will have these values set. When you run a container,
    you can add new environment variables or replace the values of existing image
    variables using the `--env` or `-e` option. You can see how environment variables
    work with a simple Nano Server container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: With apps hosted in IIS, there's a complication in using environment variables
    from Docker. When IIS starts it reads all the environment variables from the system
    and caches them. When Docker runs a container with environment variables set,
    it writes them at the process level, but that happens after IIS has cached the
    original values, so they don't get updated and IIS applications won't see the
    new value. IIS doesn't cache machine-level environment variables in the same way
    however, so we can promote the values set by Docker to machine-level environment
    variables, and IIS apps will be able to read them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Promoting environment variables can be done by copying them from the process
    level to the machine level. You can use  a PowerShell script in your container
    startup command, which does it by looping through all process-level variables
    and copying them to the machine level, unless the machine-level key that already
    exists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: You don't need to do this if you're using an image based on Microsoft's IIS
    image, because it's done for you with a utility called `ServiceMonitor.exe`, which
    is packaged in the IIS image. ServiceMonitor does three things – it makes process-level
    environment variables available, it starts a background Windows Service, and then
    it watches the service to make sure it keeps running. This means you can use ServiceMonitor
    as the start process for your container, and if the IIS Windows Service fails,
    ServiceMonitor will exit and Docker will see that your application has stopped.
  prefs: []
  type: TYPE_NORMAL
- en: '`ServiceMonitor.exe` is available as a binary file on GitHub, but it''s not
    open source and not all its behavior is documented (it seems to only work for
    the default IIS App Pool). It''s copied into Microsoft''s IIS image and set as
    the `ENTRYPOINT` for containers. The ASP.NET image is based on the IIS image,
    so it also has ServiceMonitor configured.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to use ServiceMonitor along with your own logic to echo out IIS
    logs, you need to start ServiceMonitor in the background and finish your startup
    command in the Dockerfile with the log read. I do this in `dockeronwindows/ch03-iis-environment-variables:2e`,
    running ServiceMonitor with PowerShell''s `Start-Process` cmdlet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The `ENTRYPOINT` and `CMD` instructions both tell Docker how to run your application.
    You can combine them to specify a default entry point and allow users of your
    image to override the command when they start a container.
  prefs: []
  type: TYPE_NORMAL
- en: 'The application in the image is a simple ASP.NET Web Forms page that lists
    environment variables. I can run this in a container in the usual way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'When the container starts, I can get the container''s port and open a browser
    on the ASP.NET Web Forms page a some simple PowerShell script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The website show the default environment variable values from the Docker image
    listed as process-level variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/2e618d29-6ac9-4bbd-9146-05ec35667a31.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You can run the same image with different environment variables, overriding
    one of the image variables and adding a new variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Browse the new container''s port and you''ll see the new values written out
    by the ASP.NET page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/ef181092-bc3c-4348-8c38-689692850087.png)'
  prefs: []
  type: TYPE_IMG
- en: I've added support for Docker's environment variable management into an IIS
    image now, so ASP.NET apps can use the `System.Environment` class to read the
    configuration settings. I've retained the IIS log echo in this new image, so this
    is a good Docker citizen and now you can configure the application and check the
    logs through Docker.
  prefs: []
  type: TYPE_NORMAL
- en: One last improvement I can make is to tell Docker how to monitor the application
    running inside the container, so Docker can determine whether the application
    is healthy and take action if it becomes unhealthy.
  prefs: []
  type: TYPE_NORMAL
- en: Building Docker images that monitor applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When I add these new features to the NerdDinner Dockerfile and run a container
    from the image, I'll be able to see the web request and response logs with the
    `docker container logs` command, which relays all of the IIS log entries captured
    by Docker, and I can use environment variables and configuration files to specify
    API keys and database user credentials. This makes running and administering the
    legacy ASP.NET application consistent with how I use any other containerized application
    running on Docker. I can also configure Docker to monitor the container for me,
    so I can manage any unexpected failures.
  prefs: []
  type: TYPE_NORMAL
- en: Docker provides the ability to monitor an application's health, rather than
    just checking whether the application process is still running, with the `HEALTHCHECK`
    instruction in the Dockerfile. With `HEALTHCHECK` you tell Docker how to test
    whether the application is still healthy. The syntax is similar to the `RUN` and
    `CMD` instructions. You pass in a shell command to execute, which should have
    a return code of `0` if the application is healthy, and `1` if it's not. Docker
    runs a health check periodically when the container is running and emits status
    events if the health of a container changes.
  prefs: []
  type: TYPE_NORMAL
- en: The simple definition of *healthy* for a web application is the ability to respond
    normally to HTTP requests. Which request you make depends on how thorough you
    want the check to be. Ideally the request should execute key parts of your application,
    so you're confident it is all working correctly. But equally, the request should
    complete quickly and have a minimal compute impact, so processing lots of health
    checks doesn't affect consumer requests.
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple health check for any web application is to just use the `Invoke-WebRequest`
    PowerShell cmdlet to fetch the home page and check whether the HTTP response code
    is `200`, which means the response was successfully received:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: For a more complex web application, it can be useful to add a new endpoint specifically
    for health checks. You can add a diagnostic endpoint to APIs and websites that
    exercises some of the core logic of your app and returns a Boolean result to indicate
    whether the app is healthy. You can call this endpoint in the Docker health check
    and check the response content, as well as the status code, in order to give you
    more confidence that the app is working correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `HEALTHCHECK` instruction in the Dockerfile is very simple. You can configure
    the interval between checks and the number of checks that can fail before the
    container is considered unhealthy, but to use the default values just specify
    the test script in `HEALTHCHECK CMD`. The following example from the Dockerfile
    for the `dockeronwindows/ch03-iis-healthcheck:2e` image uses PowerShell to make
    a `GET` request to the diagnostics URL and check the response status code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: I've specified an interval for the health check, so Docker will execute this
    command inside the container every 5 seconds (the default interval is 30 seconds
    if you don't specify one). The health check is very cheap to run as it's local
    to the container, so you can have a short interval like this and catch any problems
    quickly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The application in this Docker image is an ASP.NET Web API app, which has a
    diagnostics endpoint, and a controller you can use to toggle the health of the
    application. The Dockerfile contains a health check, and you can see how Docker
    uses it when you run a container from that image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run `docker container ls` after starting that container, you''ll see
    a slightly different output in the status field, similar to `Up 3 seconds (health:
    starting)`. Docker runs the health check every 5 seconds for this container, so
    at this point, the check hasn''t been run. Wait a little longer and then the status
    will be something like `Up 46 seconds (healthy)`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can check the current health of the API by querying the `diagnostics` endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'In the returned content, you''ll see `"Status":"GREEN"` meaning the API is
    healthy. This container will stay healthy until I make a call to the controller
    to toggle the health. I can do that with a `POST` request that sets the API to
    return HTTP status `500` for all subsequent requests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Now the application will respond with a 500 response to all the `GET` requests
    that the Docker platform makes, which will fail the healthcheck. Docker keeps
    trying the healthcheck, and if there are three failures in a row then it considers
    the container to be unhealthy. At this point, the status field in the container
    list shows `Up 3 minutes (unhealthy)`. Docker doesn't take automatic action on
    single containers that are unhealthy, so this one is left running and you can
    still access the API.
  prefs: []
  type: TYPE_NORMAL
- en: Healthchecks are important when you start running containers in a clustered
    Docker environment (which I cover in [Chapter 7](bf6a5e90-bbba-435b-b0a0-734611e0e834.xhtml),
    *Orchestrating Distributed Solutions with Docker Swarm*), and it's good practice
    to include them in all Dockerfiles. Being able to package an application which
    the platform can test for health is a very useful feature - it means that wherever
    you run the app, Docker can keep a check on it.
  prefs: []
  type: TYPE_NORMAL
- en: Now you have all the tools to containerize an ASP.NET application and make it
    a good Docker citizen, integrating with the platform so it can be monitored and
    administered in the same way as other containers. A full .NET Framework application
    running on Windows Server Core can't meet the expectation of running a single
    process, because of all the necessary background Windows services, but you should
    still build container images so they run only one logical function and separate
    any dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: Separating dependencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the last chapter I Dockerized the legacy NerdDinner app and got it running,
    but without a database. The original application expected to use SQL Server LocalDB
    on the same host where the app is running. LocalDB is an MSI-based installation,
    and I can add it to the Docker image by downloading the MSI and installing it
    with `RUN` commands in the Dockerfile. But this means that when I start a container
    from the image, it has two functions: hosting a web application and running a
    database.'
  prefs: []
  type: TYPE_NORMAL
- en: Having two functions in one container is not a good idea. What would happen
    if you wanted to upgrade your website without changing the database? Or what if
    you needed to do some maintenance on the database, which didn't impact the website?
    What if you need to scale out the website? By coupling the two functions together,
    you've added deployment risk, test effort, and administration complexity, and
    reduced your operational flexibility.
  prefs: []
  type: TYPE_NORMAL
- en: Instead I'm going to package the database in a new Docker image, run it in a
    separate container and use Docker's network layer to access the database container
    from the website container. SQL Server is a licensed product, but the free variant
    is SQL Server Express, which is available from Microsoft as an image on Docker
    Hub and comes with a production license. I can use this as the base for my image,
    building on it to prepare a preconfigured database instance, with the schema deployed
    and ready to connect to the web application.
  prefs: []
  type: TYPE_NORMAL
- en: Creating Docker images for SQL Server databases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Setting up a database image is just like any other Docker image. I''ll be encapsulating
    the setup tasks in a Dockerfile. Broadly, for a new database, the steps will be:'
  prefs: []
  type: TYPE_NORMAL
- en: Install SQL Server
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configure SQL server
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run DDL scripts to create the database schema
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run DML scripts to populate static data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This fits in very well with a typical build process using Visual Studio's SQL
    database project type and the Dacpac deployment model. The output from publishing
    the project is a `.dacpac` file which contains the database schema and any custom
    SQL scripts to run. Using the `SqlPackage` tool you can deploy the Dacpac file
    to a SQL Server instance, and it will either create a new database if one doesn't
    exist, or it will upgrade an existing database so the schema matches the Dacpac.
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach is perfect for a custom SQL Server Docker image. I can use multi-stage
    builds again for the Dockerfile, so other users don''t need Visual Studio installed
    to package the database from the source code. This is the first stage of the Dockerfile
    for the `dockeronwindows/ch03-nerd-dinner-db:2e` image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: There's quite a bit in here, but it's all straightforward. The `builder` stage
    starts from Microsoft's .NET Framework SDK image. That gives me `NuGet` and `MSBuild`,
    but not the dependencies I need to build the SQL Server Dacpac. The first two
    `RUN` instructions install the SQL Server Data Tools and the `SqlPackage` tool.
    I could package this as a separate SQL Server SDK image if I had many database
    projects to containerize.
  prefs: []
  type: TYPE_NORMAL
- en: The rest of the stage just copies in the SQL project source and runs `MSBuild`
    to produce the Dacpac.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the second stage of the Dockerfile, which packages the NerdDinner Dacpac
    to run in SQL Server Express:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: I'm using my own Docker image with SQL Server Express 2017 installed here. Microsoft
    do have SQL Server images published on Docker Hub for Windows and Linux, but the
    Windows versions haven't been regularly maintained. SQL Server Express is freely
    distributed, so you can package it into your own Docker image (the Dockerfile
    for `dockeronwindows/ch03-sql-server` is on GitHub in the `sixeyed/docker-on-windows`
    repository).
  prefs: []
  type: TYPE_NORMAL
- en: There are no new instructions here, beyond what you've seen so far. There's
    a volume set up for the SQL Server data files, and an environment variable to
    set the default data file path to `C:\data`. You'll see that there are no `RUN`
    commands, so I'm not actually setting up the database schema when I build the
    image; I'm just packaging the Dacpac file into the image so I have everything
    I need to create or upgrade the database when the container starts.
  prefs: []
  type: TYPE_NORMAL
- en: In the `CMD` instruction, I run a PowerShell script which sets up the database.
    It's sometimes not a good idea to hide all the startup details in a separate script,
    because that means you can't see from the Dockerfile alone what's going to happen
    when the container runs. But in this case, the startup procedure has quite a few
    functions, and they would make for a huge Dockerfile if we put them all in there.
  prefs: []
  type: TYPE_NORMAL
- en: The base SQL Server Express image defines an environment variable called `sa_password`
    to set the administrator password. I extend this image and set a default value
    for the variable. I'll use the variable in the same way in order to allow users
    to specify an administrator password when they run the container. The rest of
    the startup script deals with the problem of storing database state in a Docker
    volume.
  prefs: []
  type: TYPE_NORMAL
- en: Managing database files for SQL Server containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A database container is like any other Docker container, but with a focus on
    statefulness. You'll want to ensure your database files are stored outside of
    the container, so you can replace the database container without losing any data.
    You can easily do this with volumes as we saw in the last chapter, but there is
    a catch.
  prefs: []
  type: TYPE_NORMAL
- en: If you build a custom SQL Server image with a deployed database schema, your
    database files will be inside the image in a known location. You can run a container
    from that image without mounting a volume and it will just work, but the data
    will be stored in the container's writeable layer. If you replace the container
    when you have a database upgrade to perform, then you'll lose all your data.
  prefs: []
  type: TYPE_NORMAL
- en: Instead you can run the container with a volume mounted from the host, mapping
    the expected SQL Server data directory from a host directory, so your files live
    outside of the container in a known location on the host. This way, you can ensure
    your data files are stored somewhere reliable, like in a RAID array on your server.
    But that means you can't deploy the database in the Dockerfile, because the data
    directory will have data files stored in the image, and if you mount a volume
    over the directory these files will be hidden.
  prefs: []
  type: TYPE_NORMAL
- en: The SQL Server images from Microsoft deal with this by letting you attach a
    database and log files when it runs, so it works on the basis that you already
    have your database files on the host. In this case, you can use the image directly,
    mount your data folder, and run a SQL Server container with arguments telling
    it which database(s) to attach. This is a very limited approach – it means you
    need to create the database on a different SQL Server instance first, and then
    attach it when you run the container. That doesn't fit with an automated release
    process.
  prefs: []
  type: TYPE_NORMAL
- en: For my custom image I want to do something different. The image contains the
    Dacpac, so it has everything it needs to deploy the database. When the container
    starts, I want it to check the data directory, and if it's empty, then I create
    a new database by deploying the Dacpac model. If the database files already exist
    when the container starts, then attach the database files first and upgrade the
    database using the Dacpac model.
  prefs: []
  type: TYPE_NORMAL
- en: This approach means you can use the same image to run a fresh database container
    for a new environment, or upgrade an existing database container without losing
    any of its data. This works just as well whether you mount the database directory
    from the host or not, so you can let the user choose how to manage the container
    storage, so the image supports many different scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'The logic to do this is all in the `Initialize-Database.ps1` PowerShell script,
    which the Dockerfile sets as the entry point for containers. In the Dockerfile,
    I pass the data directory to the PowerShell script in the `data_path` variable,
    and the script checks whether the NerdDinner data (`mdf`) and log (`ldf`) files
    are in that directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This script looks complex, but actually, it's just building a `CREATE DATABASE...FOR
    ATTACH` statement, filling in the paths of the MDF data file and LDF log files
    if they exist. Then it invokes the SQL statement, which attaches the database
    files from the external volume as a new database in the SQL Server container.
  prefs: []
  type: TYPE_NORMAL
- en: 'This covers the scenario where a user runs a container with a volume mount,
    and the host directory already contains data files from a previous container.
    These files are attached, and the database is available in the new container.
    Next, the script uses the `SqlPackage` tool to generate a deployment script from
    the Dacpac. I know the `SqlPackage` tool exists and I know the path to it because
    it''s packaged into my image from the builder stage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: If the database directory was empty when the container started, there's no `NerdDinner`
    database on the container, and `SqlPackage` will generate a script with a set
    of `CREATE` statements to deploy the new database. If the database directory did
    contain files, then the existing database would be attached. In that case `SqlPackage`
    would generate a script with a set of `ALTER` and `CREATE` statements to bring
    the database in line with the Dacpac.
  prefs: []
  type: TYPE_NORMAL
- en: The `deploy.sql` script generated in this step will create the new schema, or
    apply changes to the old schema to upgrade it. The final database schema will
    be the same in both cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, the PowerShell script executes the SQL script, passing in variables
    for the database name, file prefixes, and data paths:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: After the SQL script runs, the database exists in the container with the schema
    modeled in the Dacpac, which was built from the SQL project in the builder stage
    of the Dockerfile. The database files are in the expected location with the expected
    names, so if this container is replaced with another one from the same image,
    the new container will find the existing database and attach it.
  prefs: []
  type: TYPE_NORMAL
- en: Running databases in containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now I have a database image that can work for new deployments and for upgrades.
    The image can be used by developers who might run it without mounting a volume
    while they're working on a feature, so they can start with a fresh database every
    time they run a container. And the same image can be used in environments where
    the existing database needs to be preserved by running the container with a volume
    that contains the database files.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is how you run the NerdDinner database in Docker, using the default administrator
    password, with a host directory for the database files, and naming the container
    so that I can access it from other containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The first time you run this container, the Dacpac will run to create the database,
    saving the data and log files in the mounted directory on the host. You can check
    whether the files exist on your host with `ls`, and the output from `docker container
    logs` shows the generated SQL script running, and creating resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The `docker container run` command I used also publishes the standard SQL Server
    port `1433`, so you can connect to the database running inside the container remotely
    through a .NET connection, or with **SQL Server Management Studio** (**SSMS**).
    If you already have a SQL Server instance running on your host, you can map the
    container's port `1433` to a different port on the host.
  prefs: []
  type: TYPE_NORMAL
- en: 'To connect to the SQL Server instance running in the container with SSMS, Visual
    Studio, or Visual Studio Code, use `localhost` as the the server name, select
    SQL Server Authentication, and use the `sa` credentials. I use **SqlElectron**,
    which is a very lightweight SQL database client:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/6fb1e102-ad53-4aca-a138-a1de00d35260.png)'
  prefs: []
  type: TYPE_IMG
- en: Then, you can work with the Dockerized database just like any other SQL Server
    database, querying tables and inserting data. From the Docker host machine, you
    use `localhost` as the database server name. By publishing the port, you can access
    the containerized database outside of the host, using the host machine name as
    the server name. Docker will route any traffic on port `1433` into SQL Server
    running on the container.
  prefs: []
  type: TYPE_NORMAL
- en: Connecting to database containers from application containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Docker has a DNS server built into the platform, which is used by containers
    for service discovery. I started the NerdDinner database container with an explicit
    name, and any other containers running in the same Docker network can access this
    container by its name, in exactly the same way as a web server would access a
    remote database server by its DNS hostname:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/57115c85-6752-43a3-9e3e-9f7c07780995.png)'
  prefs: []
  type: TYPE_IMG
- en: This makes application configuration much simpler than a traditional distributed
    solution. Every environment will look the same. In development, integration testing,
    QA, and production, the web container will always connect to a database using
    the `nerd-dinner-db` hostname, which is actually running inside a container. The
    container could be on the same Docker host or a separate machine in a Docker Swarm
    cluster, and that's transparent to the application.
  prefs: []
  type: TYPE_NORMAL
- en: Service discovery in Docker isn't for containers only. A container can access
    another server on the network using its hostname. You could run your web application
    in a container, but still have it connected to SQL Server running on a physical
    machine rather than using a database container.
  prefs: []
  type: TYPE_NORMAL
- en: There's one piece of configuration that could be different for each environment,
    and that's the SQL Server login credentials. In the NerdDinner database image,
    I use the same configuration approach I used earlier in this chapter with `dockeronwindows/ch03-aspnet-config`.
    I've split the `appSettings` and `connectionStrings` sections from `Web.config` into
    separate files, and the Docker image bundles those configuration files with default
    values.
  prefs: []
  type: TYPE_NORMAL
- en: Developers can just run a container from the image and it will use the default
    database credentials, which match the default credentials built into the NerdDinner
    database Docker image. In other environments, containers can be run with a volume
    mount, using configuration files on the host server, which specify different application
    settings and database connection strings.
  prefs: []
  type: TYPE_NORMAL
- en: This is a simplified approach to security credentials, which I'm using to show
    how we can make our application more Docker-friendly without changing the code.
    Keeping credentials in a plain-text file on the server isn't a great way to manage
    secrets, and I'll look at this again in Chapter 9, *Understanding the Security
    Risks and Benefits of Docker,* when I cover security in Docker.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some updates to the Dockerfile for NerdDinner in this chapter. I''ve
    added a health check and the setup to echo logs out from IIS. I still haven''t
    made any functional changes to the NerdDinner code base, only splitting up the
    `Web.config` file and setting the default database connection string to use the
    SQL Server database container. When I run the web application container now, it
    will be able to connect to the database container by name and use the SQL Server
    Express database running in Docker:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: You can explicitly specify the Docker network a container should join when it's
    created, but on Windows, all containers default to joining the system-created
    Docker network called `nat`. The database container and web container are both
    connected to the `nat` network, so they can reach each other by the container
    name.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the container starts up, I can now open the website using the container''s
    port, click on the Register link, and create an account:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/3a32ee44-2cc1-4d04-9244-6d2dd9139dad.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Register page queries the ASP.NET membership database, running in the SQL
    Server container. If the registration page is functioning, then the web application
    has a working connection to the database. I can verify this in Sqlectron, querying
    the `UserProfile` table and seeing the new user row:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/d293f55f-0ad4-4806-b9a6-f5e771799b2e.png)'
  prefs: []
  type: TYPE_IMG
- en: I've now separated the SQL Server database from the web application, and each
    component is running in a lightweight Docker container. On my development laptop,
    each container uses less than 1% of the host CPU at idle, with the database using
    250 MB of memory, and the web server 70 MB.
  prefs: []
  type: TYPE_NORMAL
- en: '`docker container top` shows you information on the processes running inside
    a container, including memory and CPU.'
  prefs: []
  type: TYPE_NORMAL
- en: Containers are light on resources, so there's no penalty in splitting functional
    units into different containers, then you can scale, deploy, and upgrade these
    components individually.
  prefs: []
  type: TYPE_NORMAL
- en: Breaking up monolithic applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Traditional .NET web applications which rely on a SQL Server database can be
    migrated to Docker with minimal effort and without having to rewrite any application
    code. At this stage in my NerdDinner migration, I have an application Docker image
    and a database Docker image which I can reliably and repeatedly deploy and maintain.
    I also have some beneficial side effects.
  prefs: []
  type: TYPE_NORMAL
- en: Encapsulating the database definition in a Visual Studio project may be a new
    approach, but it adds quality assurance to database scripts and brings the schema
    into the code base, so it can be source-controlled and managed alongside the rest
    of the system. Dacpacs, PowerShell scripts, and Dockerfiles provide a new common
    ground for different IT functions. Development, operations, and database administration
    teams can work together on the same artifacts, using the same language.
  prefs: []
  type: TYPE_NORMAL
- en: Docker is an enabler for DevOps transitions, but whether or not DevOps is on
    your road map, Docker provides the foundation for fast, reliable releases. To
    make the best use of this, you need to look at breaking down monolithic apps into
    smaller pieces, so you can release high-value components frequently without having
    to do a regression test on the whole of a large application.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting core components from an existing application lets you bring modern,
    lightweight technologies into your system without having to do a large, complex
    rewrite. You can apply microservices architecture principles to an existing solution,
    where you already understand the areas which are worth extracting into their own
    services.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting high-value components from monoliths
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Docker platform offers a huge opportunity to modernize legacy applications,
    allowing you to take features out of monoliths and run them in separate containers.
    If you can isolate the logic in a feature, this is also an opportunity to migrate
    it to .NET Core, which lets you package it into a much smaller Docker image.
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft's road map for .NET Core has seen it adopt more and more functionality
    of the full .NET Framework, but porting parts of a legacy .NET application to
    .NET Core could still be a large undertaking. It's an option worth evaluating,
    but it doesn't have to be part of your modernization approach. The value in breaking
    down the monolith is having features which can be developed, deployed, and maintained
    independently. If those components are using the full .NET Framework, you still
    get those benefits.
  prefs: []
  type: TYPE_NORMAL
- en: The advantage you have when you're modernizing a legacy app is that you already
    understand the feature set. You can identify the high-value functionality in your
    system and start by extracting those features into their own components. Good
    candidates would be features that offer value to the business if they change frequently,
    so new feature requests can be rapidly built and deployed without modifying and
    testing the whole application.
  prefs: []
  type: TYPE_NORMAL
- en: Equally good candidates are features that offer value to IT if they stay the
    same – complex components with a lot of dependencies which the business doesn't
    change often. Extracting such a feature into a separate component means you can
    deploy upgrades to the main application without having to test the complex component
    because it remains unchanged. Breaking up a monolith like this gives you a set
    of components that each have their own delivery cadence.
  prefs: []
  type: TYPE_NORMAL
- en: 'In NerdDinner, there are some good candidates to break out into their own services.
    In the rest of this chapter, I''ll focus on one of them: the home page. The home
    page is the feature that renders the HTML for the first page of the application.
    A process to deploy changes to the home page quickly and safely in production
    will let the business experiment with a new look and feel, evaluate the impact
    of the new version, and decide whether to continue with it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The current application is distributed between two containers. For this part
    of this chapter, I''ll break the home page out into its own component, so the
    whole NerdDinner app will run across three containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/d7df03c2-ff8d-42c1-ae41-cda4c2ad0df0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'I won''t change the routing for the application. Users will still come to the
    NerdDinner application first, and the application container will call the new
    home page service container to get the content to appear. This way I don''t need
    to expose the new container publicly. There is only one technical requirement
    for the change: the main application needs to be able to communicate with the
    new homepage service component.'
  prefs: []
  type: TYPE_NORMAL
- en: You're free to choose how applications in containers communicate. Docker networking
    gives you full protocol support for TCP/IP and UDP. You could make the whole process
    asynchronous, running a message queue in another container, with message handlers
    listening in other containers. But I'll start with something simpler in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Hosting a UI component in an ASP.NET Core application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ASP.NET Core is a modern application stack which delivers the best of ASP.NET
    MVC and Web API in a fast and lightweight runtime. ASP.NET Core websites run as
    console applications, they write logs to the console output stream, and they can
    use environment variables and files for configuration. The architecture makes
    them good Docker citizens out of the box.
  prefs: []
  type: TYPE_NORMAL
- en: 'The easiest way to extract the NerdDinner home page into a new service is to
    write it as an ASP.NET Core website with a single page and relay the new application''s
    output from the existing application. The following screenshot shows my stylish,
    modern redesign of the home page running in ASP.NET Core Razor Pages in Docker:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/76fda9ae-bd4e-4b22-894c-26fef2521d7a.png)'
  prefs: []
  type: TYPE_IMG
- en: To package the home page application as a Docker image I'm using the same multi-stage
    build approach I used for the main application and the database images. In [Chapter
    10](e0946741-5df7-4a13-b220-ffc963f1e3d3.xhtml), *Powering a Continuous Deployment
    Pipeline with Docker,* you'll see how to use Docker to power a CI/CD build pipeline
    and tie the whole automated deployment process together.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Dockerfile for the `dockeronwindows/ch03-nerd-dinner-homepage:2e` image
    uses the same pattern I have for the full ASP.NET application. The builder stages
    uses the SDK image and separates the package restore and the compilation steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The final stage of the Dockerfile provides a default value for the `NERD_DINNER_URL`
    environment variable. The application uses it as the target for the link on the
    home page. The rest of the Dockerfile instructions just copy in the published
    application and set up the entry point:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: I can run the home page component in a separate container, but it's not connected
    to the main NerdDinner app yet. With the approach I've taken in this chapter,
    I need to make a code change to the original app in order to integrate the new
    home page service.
  prefs: []
  type: TYPE_NORMAL
- en: Connecting to application containers from other application containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Calling the new home page service from the main application container is fundamentally
    the same as connecting to the database: I will run the home page container with
    a known name, and I can access the service in other containers using its name
    and Docker''s built-in service discovery.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple change to the `HomeController` class in the main NerdDinner application
    will relay the response from the new home page service instead of rendering the
    page from the main application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: In the new code I get the URL for the home page service from an environment
    variable. Just as with the database connection, I can set a default value for
    that in the Dockerfile. This would be bad practice in a distributed application
    where we can't guarantee where the components are running, but, in a Dockerized
    application I can do it safely because I will control the names of the containers,
    so I can be sure the service names are correct when I deploy them.
  prefs: []
  type: TYPE_NORMAL
- en: 'I''ve tagged this updated image as `dockeronwindows/ch03-nerd-dinner-web:2e-v2`.
    To start the whole solution now, I need to run three containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'When the containers are running, I browse to the NerdDinner container''s published
    port, and I see the home page from the new component:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/f9efcc3f-8948-422d-84bb-56c6411792bb.png)'
  prefs: []
  type: TYPE_IMG
- en: The Find Dinner link takes me back to the original web app, and now I can iterate
    over the home page and release a new UI just by replacing that container – without
    releasing or testing the rest of the app.
  prefs: []
  type: TYPE_NORMAL
- en: What happened to the new UI? In this simple example, the integrated home page
    doesn't have the styling of the new ASP.NET Core version because the main application
    only reads the HTML for the page, not the CSS files or other assets. A better
    approach would be to run a reverse proxy in a container and use that as the entry
    point to other containers, so each container serves all its assets. I'll do that
    later in the book.
  prefs: []
  type: TYPE_NORMAL
- en: Now that I have my solution split across three containers, I've dramatically
    improved flexibility. At build time I can focus on features that give the highest
    value without expending effort to test components that haven't changed. At deployment
    time, I can release quickly and confidently, knowing that the new image we push
    to production will be exactly what was tested. Then at runtime, I can scale components
    independently according to their requirements.
  prefs: []
  type: TYPE_NORMAL
- en: I do have a new non-functional requirement, which is to ensure that all the
    containers have the expected names, are started in the correct order, and are
    in the same Docker network, so the solution as a whole works correctly. Docker
    has support for this, which is focused on organizing distributed systems with
    Docker Compose. I'll show you this in [Chapter 6](83637474-1791-48b6-8ce1-5aa07f00b46c.xhtml),
    *Organizing Distributed Solutions with Docker Compose.*
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered three main topics. First, we covered containerizing
    legacy .NET Framework applications so that they are good Docker citizens and integrate
    with the platform for configuration, logging, and monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: Then we covered containerizing database workloads with SQL Server Express and
    the Dacpac deployment model, building a versioned Docker image that can run a
    container as a new database or upgrade an existing database.
  prefs: []
  type: TYPE_NORMAL
- en: Finally we showed how to extract functionality from monolithic apps into separate
    containers, using ASP.NET Core and Windows Nano Server to package a fast, lightweight
    service that the main application consumes.
  prefs: []
  type: TYPE_NORMAL
- en: You've learned how to use more images from Microsoft on Docker Hub and how to
    use Windows Server Core for full .NET applications, SQL Server Express for databases,
    and the Nano Server flavors of the .NET Core image.
  prefs: []
  type: TYPE_NORMAL
- en: In later chapters I'll return to NerdDinner and continue to modernize it by
    extracting features into dedicated services. Before that, in the next chapter,
    I'll look more closely at using Docker Hub and other registries to store images.
  prefs: []
  type: TYPE_NORMAL
