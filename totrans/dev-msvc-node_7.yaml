- en: Chapter 7. Monitoring Microservices
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Monitoring servers is always a controversial subject. It usually falls under
    system administration, and software engineers don''t even go near it, but we are
    losing one of the huge benefits of monitoring: *the ability to react quickly to
    failures*. By monitoring our system very closely, we can be aware of problems
    almost immediately so that the actions to correct the problem may even save us
    from impacting the customers. Along with monitoring, there is the concept of performance.
    By knowing how our system behaves during load periods, we will be able to anticipate
    the necessity of scaling the system. In this chapter, we will discuss how to monitor
    servers, and specifically microservices, in order to maintain the stability of
    our system.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring services
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring using PM2 and Keymetrics
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring metrics
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simian Army – the active monitoring from Spotify
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Throughput and performance degradation
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring services
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When monitoring a microservice, we are interested in a few different types
    of metrics. The first big group of metrics is the hardware resources, which are
    described as follows:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '**Memory metrics**: This indicates how much memory is left in our system or
    consumed by our application'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CPU utilization**: As the name suggests, this indicates how much CPU are
    we using at a given time'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Disk utilization**: This indicates the I/O pressure in the physical hard
    drives'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The second big group is the application metrics, as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Number of errors per time unit
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of calls per time unit
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Response time
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even though both groups are connected and a problem in the hardware will impact
    the application performance (and the other way around), knowing all of them is
    a must.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Hardware metrics are easy to query if our server is a Linux machine. On Linux,
    all the magic of hardware resources happens in the `/proc` folder. This folder
    is maintained by the kernel of the system and contains files about how the system
    behaves regarding a number of aspects in the system.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Software metrics are harder to collect and we are going to use **Keymetrics**
    from the creators of PM2 to monitor our Node.js applications.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring using PM2 and Keymetrics
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PM2, as we've seen before, is a very powerful instrument to run Node applications,
    but it is also very good at monitoring standalone applications in production servers.
    However, depending on your business case, it is not always easy to get access
    to the production.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 'The creators of PM2 have solved this problem by creating Keymetrics. Keymetrics
    is a **Software as a service** (**SaaS**) component that allows PM2 to send monitoring
    data across the network to its website, as shown in the following image (as found
    at [https://keymetrics.io/](https://keymetrics.io/)):'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '![Monitoring using PM2 and Keymetrics](img/B04889_07_01.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
- en: Even though Keymetrics is not free, it provides a free tier to demonstrate how
    it works. We are going to use it in this chapter.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 'The very first thing that we need to do is register a user. Once we get access
    to our account, we should see something similar to the following screen:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '![Monitoring using PM2 and Keymetrics](img/B04889_07_02.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
- en: This screen is asking us to create a bucket. Keymetrics uses the bucket concept
    to define a context. For example, if our organization has different areas (payments,
    customer service, and so on) with different servers on each area, we could monitor
    all the servers in one bucket. There are no restrictions on how many servers you
    can have in one bucket. It is even possible to have all the organization in the
    same bucket so that everything is easy to access.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a bucket called `Monitoring Test`, as shown in the following
    image:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '![Monitoring using PM2 and Keymetrics](img/B04889_07_03.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
- en: 'Easy, once we tap on **Create Bucket**, Keymetrics will show us a screen with
    the information needed to start monitoring our app, as shown in the following
    image:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '![Monitoring using PM2 and Keymetrics](img/B04889_07_04.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
- en: As you can see, the screen displays information about the private key used by
    Keymetrics. Usually, it is a very bad idea to share this key with anyone.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown on the screen, the next step is to configure PM2 to push data into
    Keymetrics. There is also useful information about the networking needed to make
    Keymetrics work:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: PM2 will be pushing data to the port **80** on Keymetrics
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keymetrics will be pushing data back to us on the port **43554**
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Usually, in large organizations, there are restrictions about the networking,
    but if you are testing this from home, everything should work straightaway.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to make PM2 able to send metrics to Keymetrics, we need to install
    one PM2 module called `pm2-server-monit`. This is a fairly easy task:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This will result in an output similar to the following:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '![Monitoring using PM2 and Keymetrics](img/B04889_07_05.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
- en: 'Let''s run the advised command:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In this case, I have replaced `[server name]` with `my-server`. There are no
    restrictions on the server name; however, when rolling out Keymetrics into a real
    system, it is recommended to choose a descriptive name in order to easily identify
    the server in the dashboard.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding command will produce an output similar to the following image:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '![Monitoring using PM2 and Keymetrics](img/B04889_07_16.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
- en: 'This is an indication that everything went well and our application is ready
    to be monitored from Keymetrics that can be checked on [https://app.keymetrics.io/](https://app.keymetrics.io/),
    as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '![Monitoring using PM2 and Keymetrics](img/B04889_07_06.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
- en: Now, our server is showing up in the interface. As we previously stated, this
    bucket could monitor different servers. A simple virtual machine is created, and
    as you can see at the bottom of the screen, Keymetrics provides us with the command
    to be executed in order to add another server. In this case, as we are using the
    free access to Keymetrics, so we can only monitor one server.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Let's see what Keymetrics can offer us. At first sight, we can see interesting
    metrics such as CPU usage, memory available, disk available, and so on.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: All these are hardware metrics that indicate how our system is behaving. Under
    pressure, they are the perfect indicator to point out the need for more hardware
    resources.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Usually, the hardware resources are the main indicator of failure in an application.
    Now, we are going to see how to use Keymetrics to diagnose the problem.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Diagnosing problems
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A memory leak is usually a difficult problem to solve due to the nature of the
    flaw. Take a look at the following code.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s run the program using a simple `seneca.act()` action:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This program has a very obvious memory leak, and by obvious, I mean that it
    is written to be obvious. The `names` array will keep growing indefinitely. In
    the preceding example, it is not a big deal due to the fact that our application
    is a script that will start and finish without keeping the state in memory.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Remember that JavaScript allocates variables in the global scope if the `var`
    keyword is not used.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: The problem comes when someone else reutilizes our code in a different part
    of the application.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s assume that our system grows to a point that we need a microservice
    to greet new customers (or deliver the initial payload of personal information
    such as name, preferences, configuration, and so on). The following code could
    be a good example on how to build it:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In this example, Seneca will be listening over HTTP for requests from Seneca
    clients or other types of systems such as **curl**. When we run the application:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then from another terminal, we use curl to act as a client of our microservice,
    everything will work smoothly and our memory leak will go unnoticed:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'However, we are going to use Keymetrics to find the problem. The first thing
    we need to do is run our program using PM2\. In order to do it so, we run the
    following command:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This command will produce the following output:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagnosing problems](img/B04889_07_07.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
- en: 'Let''s explain the output in the following:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: The first line gives us information about the integration with Keymetrics. Data
    such as public key, server name, and the URL to access Keymetrics.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the first table, we can see the name of the application running, as well
    as few statistics on memory, uptime, CPU, and so on.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the second table, we can see the information relevant to the `pm2-server-monit`
    PM2 module.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let''s go to Keymetrics and see what has happened:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagnosing problems](img/B04889_07_08.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
- en: The application is now registered in Keymetrics as it can be seen in the control
    panel
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, now our application is showing up in Keymetrics.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Straightaway, we can see the very useful things about our app. One of these
    is the memory used. This is the metric that will indicate a memory leak, as it
    will keep growing.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we are going to force the memory leak to cause a problem in our application.
    In this case, the only thing that we need to do is start our server (the small
    application that we wrote before) and issue a significant number of requests:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As simple as the small bash script, this is all it takes to open Pandora''s
    Box in our application:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagnosing problems](img/B04889_07_09.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
- en: The application is now showing a high load (36% of CPU and an increased use
    of memory up to 167 MB)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding image shows the impact of running the loop of requests in our
    system. Let''s explain it in the following:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: The CPU in our application has gone to **11%** with an average loop delay of
    **1.82** milliseconds. Regarding our system, the overall CPU utilization has gone
    up to **36.11%** due to the fact that both the application and bash script use
    a significant amount of resources.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The memory consumption has soared from **81.9 MB** to **167.6 MB**. As you can
    see, the line on the graph of memory allocation is not going straight up, and
    that is due to garbage collections. A garbage collection is an activity within
    the Node.js framework where unreferenced objects are freed from the memory, allowing
    our system to reuse the hardware resources.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regarding the errors, our application has been stable with **0** errors (we'll
    come back to this section later).
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, once our bash script is finished (I stopped it manually, as it can take
    a significant amount of resources and time to finish), we can again see what happened
    to our system in the following screenshot:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagnosing problems](img/B04889_07_10.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
- en: We can see that the CPU has gone back to normal, but what about the memory?
    The memory consumed by our application hasn't been freed due to the fact that
    our program has a memory leak, and as long as our variable is referencing the
    memory consumed (remember the `names` array is accumulating more and more names),
    it won't be freed.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we have a very simple example that clearly shows where the memory
    leak is, but in complex applications, it is not always obvious. This error, in
    particular, could never show up as a problem due to the fact that we probably
    deploy new versions of our app often enough to not realize it.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring application exceptions
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Application errors are events that occur when our application can't handle an
    unexpected situation. Things such as dividing a number by zero or trying to access
    an undefined property of our application usually leads to these type of problems.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: When working on a multithreaded framework (language) such as Tomcat, the fact
    that one of our threads dies due to an exception usually only affects to one customer
    (the one holding the thread). However, in Node.js, a bubbled exception could be
    a significant problem as our application dies.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: PM2 and Seneca do a very good job at keeping it alive as PM2 will restart our
    app if something makes it stop, and Seneca won't let the application die if an
    exception occurs in one of the actions.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 'Keymetrics has developed a module called **pmx** that we can use to programmatically
    get alerts on errors:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'It is easy and self-descriptive: an action that sends an exception to Keymetrics
    if the number sent as a parameter is zero. If we run it, we will get the following
    output:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '![Monitoring application exceptions](img/B04889_07_11.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
- en: 'Now we need to hit the server in order to cause the error. As we did earlier,
    we will do this using curl:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, when we go to Keymetrics, we can see that there is an error logged, as
    shown in the following image:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '![Monitoring application exceptions](img/B04889_07_12.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
- en: Another interesting point of Keymetrics is the configuration of alerts. As PM2
    sends data about pretty much every metric in our system, we have the ability to
    configure Keymetrics on the thresholds that we consider healthy for our application.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: This is very handy as we could get the notifications integrated in our corporate
    chat (something similar to **Slack**) and be alerted real time when something
    is not going correctly in our application.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Custom metrics
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Keymetrics also allows us to use **probes**. A probe is a custom metric that
    is sent programmatically by the application to Keymetrics.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: There are different types of values that the native library from Keymetrics
    allows us to push directly. We are going to see the most useful ones.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Simple metric
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A simple metric is, as its name indicates, a very basic metric where the developer
    can set the value to the data sent to Keymetrics:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In this case, the metric will send the time when the action was called for
    the last time to the Keymetrics:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '![Simple metric](img/B04889_07_13.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
- en: 'The configuration for this metric is non-existent:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: There is no complexity in this metric.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Counter
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This metric is very useful to count how many times an event occurred:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In the preceding code, we can see how the counter is incremented for every single
    call to the action counter.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'This metric will also allow us to decrement the value calling the `dec()` method
    on the counter:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Average calculated values
  id: totrans-123
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This metric allows us to record when an event occurs, and it will automatically
    calculate the number of events per time unit. It is quite useful to calculate
    averages and is a good tool to measure the load in the system. Let''s see a simple
    example, as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The preceding code creates a probe and sends a new metric called `Calls per
    minute` to Keymetrics.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, if we run the application and the following command a few times, the metric
    is shown in the following Keymetrics interface:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![Average calculated values](img/B04889_07_14.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, there is a new metric called `Calls per minute` in the UI.
    The key to configure this metric is in the following initialization:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'As you can see, the name of the metric is in the configuration dictionary as
    well as in two parameters: `samples` and `timeframe`.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: The `samples` parameter correspond to the interval where we want to rate the
    metric; in this case, it is the number of calls per minute so that rate is `60`
    seconds.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: The `timeframe` parameter, on the other hand, is for how long we want Keymetrics
    to hold the data for, or to express in simpler words, it is the timeframe over
    which the metric will be analyzed.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Simian Army – the active monitoring from Spotify
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Spotify** is one of the companies of reference when building microservices-oriented
    applications. They are extremely creative and talented when it boils down to coming
    up with new ideas.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: One of my favourite ideas among them is what they call the **Simian Army**.
    I like to call it **active monitoring**.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: In this book, I have talked a lot times about how humans fail at performing
    different tasks. No matter how much effort you put in to creating your software,
    there are going to be bugs that will compromise the stability of the system.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: This is a big problem, but it becomes a huge deal when, with the modern cloud
    providers, your infrastructure is automated with a script.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Think about it, what happens if in a pool of thousand servers, three of them
    have the *time zone out of sync* with the rest of the servers? Well, depending
    on the nature of your system, it could be fine or it could be a big deal. Can
    you imagine your bank giving you a statement with disordered transactions?
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Spotify has solved (or mitigated) the preceding problem by creating a number
    of software agents (a program that moves within the different machines of our
    system), naming them after different species of monkeys with different purposes
    to ensure the robustness of their infrastructure. Let's explain it a bit further.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'As you are probably aware, if you have worked before with Amazon Web Services,
    the machines and computational elements are divided in to regions (EU, Australia,
    USA, and so on) and inside every region, there are availability zones, as shown
    in the following diagram:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '![Simian Army – the active monitoring from Spotify](img/B04889_07_15.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
- en: This enables us, the engineers, to create software and infrastructure without
    hitting what we call a single point of failure.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A **single point of failure** is a condition in a system where the failure of
    a single element will cause the system to misbehave.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'This configuration raised a number of questions to the engineers in Spotify,
    as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: What happens if we blindly trust our design without testing whether we actually
    have any point of failures or not?
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What happens if a full availability zone or region goes down?
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How is our application going to behave if there is an abnormal latency for some
    reason?
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To answer all these questions, Netflix has created various agents. An agent
    is a software that runs on a system (in this case, our microservices system) and
    carries on different operations such as checking the hardware, measuring the network
    latency, and so on. The idea of agent is not new, but until now, its application
    was nearly a futuristic subject*.* Let''s take a look at the following agents
    created by Netflix:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '**Chaos Monkey**: This agent disconnects healthy machines from the network
    in a given availability zone. This ensures that there are *no single points of
    failures within an availability zone*. So that if our application is balanced
    across four nodes, when the Chaos Monkey kicks in, it will disconnect one of these
    four machines.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Chaos Gorilla**: This is similar to Chaos Monkey, Chaos Gorilla will disconnect
    a full availability zone in order to verify that Netflix services rebalance to
    the other available zones. In other words, Chaos Gorilla is the big brother of
    Chaos Monkey; instead of operating at the server level, it operates at the partition
    level.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Latency Monkey**: This agent is responsible for introducing artificial latencies
    in the connections. Latency is usually something that is hardly considered when
    developing a system, but it is a very delicate subject when building a microservices
    architecture: latency in one node could compromise the quality of the full system.
    When a service is running out of resources, usually the first indication is the
    latency in the responses; therefore, Latency Monkey *is a good way to find out
    how our system will behave under pressure*.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Doctor Monkey**: A health check is an endpoint in our application that returns
    an HTTP 200 if everything is correct and 500 error code if there is a problem
    within the application. Doctor Monkey is an agent that will randomly execute the
    health check of nodes in our application and report the faulty ones in order to
    replace them.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**10-18 Monkey**: Organizations such as Netflix are global, so they need to
    be language-aware (certainly, you don''t want to get a website in German when
    your mother tongue is Spanish). The 10-18 Monkey reports on instances that are
    misconfigured.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are a few other agents, but I just want to explain active monitoring to
    you. Of course, this type of monitoring is out of reach of small organizations,
    but it is good to know about their existence so that we can get inspired to set
    up our monitoring procedures.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The code is available under Apache License in the following repository:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/Netflix/SimianArmy](https://github.com/Netflix/SimianArmy).'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: In general, this active monitoring follows the philosophy of *fail early*, of
    which, I am a big adept. No matter how big the flaw in your system is or how critical
    it is, you want to find it sooner than later, and ideally, without impacting any
    customer.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Throughput and performance degradation
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Throughput is to our application what the monthly production is to a factory.
    It is a unit of measurement that gives us an indication about how our application
    is performing and answers the *how many* question of a system.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 'Very close to throughput, there is another unit that we can measure: **latency**.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Latency is the performance unit that answers the question of *how long*.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider the following example:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'Our application is a microservices-based architecture that is responsible for
    calculating credit ratings of applicants to withdraw a mortgage. As we have a
    large volume of customers (a nice problem to have), we have decided to process
    the applications in batches. Let''s draw a small algorithm around it:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This is a small and simple Seneca application (this is only theoretical, don''t
    try to run it as there is a lot of code missing!) that acts as a client for two
    other microservices, as follows:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: The first one gets the list of pending applications for mortgages
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second one gets the list of credit rating for the customers that we have
    requested
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This could be a real situation for processing mortgage applications. In all
    fairness, I worked on a very similar system in the past, and even though it was
    a lot more complex, the workflow was very similar.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s talk about throughput and latency. Imagine that we have a fairly big
    batch of mortgages to process and the system is misbehaving: the network is not
    being as fast as it should and is experiencing some dropouts.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Some of these applications will be lost and will need to be retried. In ideal
    conditions, our system is producing a throughput of 500 applications per hour
    and takes an average of 7.2 seconds on latency to process every single application.
    However, today, as we stated before, the system is not at its best; we are processing
    only 270 applications per hour and takes on average 13.3 seconds to process a
    single mortgage application.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, with latency and throughput, we can measure how our business
    transactions are behaving with respect to the previous experiences; we are operating
    at 54% of our normal capacity.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: This could be a serious issue. In all fairness, a drop off like this should
    ring all the alarms in our systems as something really serious is going on in
    our infrastructure; however, if we have been smart enough while building our system,
    the performance will be degraded, but our system won't stop working. This can
    be easily achieved by the usage of circuit breakers and queueing technologies
    such as **RabbitMQ**.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Queueing is one of the best examples to show how to apply human behavior to
    an IT system. Seriously, the fact that we can easily decouple our software components
    having a simple message as a joint point, which our services either produce or
    consume, gives us a big advantage when writing complex software.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Other advantage of queuing over HTTP is that an HTTP message is lost if there
    is a network drop out.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to build our application around the fact that it is either full success
    or error. With queueing technologies such as RabbitMQ, our messaging delivery
    is asynchronous so that we don''t need to worry about intermittent failures: *as
    soon as we can deliver the message to the appropriate queue, it will get persisted
    until the client is able to consume it (or the message timeout occurs)*.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要围绕这样一个事实构建我们的应用程序，即它要么完全成功，要么出现错误。使用RabbitMQ等排队技术，我们的消息传递是异步的，因此我们不需要担心间歇性故障：*一旦我们能够将消息传递到适当的队列，它将被持久化，直到客户端能够消费它（或者消息超时发生）*。
- en: This enables us to account for intermittent errors in the infrastructure and
    build even more robust applications based on the communication around queues.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们能够考虑基础设施中的间歇性错误，并基于队列周围的通信构建更加健壮的应用程序。
- en: 'Again, Seneca makes our life very easy: the plugin system on which the Seneca
    framework is built makes writing a transport plugin a fairly simple task.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，Seneca让我们的生活变得非常简单：Seneca框架构建的插件系统使编写传输插件成为一项相当简单的任务。
- en: Note
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'RabbitMQ transport plugin can be found in the following GitHub repository:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: RabbitMQ传输插件可以在以下GitHub存储库中找到：
- en: '[https://github.com/senecajs/seneca-rabbitmq-transport](https://github.com/senecajs/seneca-rabbitmq-transport)'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/senecajs/seneca-rabbitmq-transport](https://github.com/senecajs/seneca-rabbitmq-transport)'
- en: There are quite few transport plugins and we can also create our own ones (or
    modify the existing ones!) to satisfy our needs.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 有相当多的传输插件，我们也可以创建我们自己的插件（或修改现有的插件！）来满足我们的需求。
- en: 'If you take a quick look at the RabbitMQ plugin (just as an example), the only
    thing that we need to do to write a transport plugin is overriding the following
    two Seneca actions:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你快速浏览一下RabbitMQ插件（只是举个例子），我们需要做的唯一一件事就是覆盖以下两个Seneca操作：
- en: '`seneca.add({role: ''transport'', hook: ''listen'', type: ''rabbitmq''}, ...)`'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`seneca.add({role: ''transport'', hook: ''listen'', type: ''rabbitmq''}, ...)`'
- en: '`seneca.add({role: ''transport'', hook: ''client'', type: ''rabbitmq''}, ...)`'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`seneca.add({role: ''transport'', hook: ''client'', type: ''rabbitmq''}, ...)`'
- en: Using queueing technologies, our system will be *more resilient against intermittent
    failures* and we would be able to degrade the performance instead of heading into
    a catastrophic failure due to missing messages.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 使用排队技术，我们的系统将*更加抗干扰*，我们将能够降低性能而不是因为丢失消息而陷入灾难性的故障。
- en: Summary
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we deep dived into PM2 monitoring through Keymetrics. We learned
    how to put tight monitoring in place so that we are quickly informed about the
    failures in our application.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入了解了通过Keymetrics进行PM2监控。我们学会了如何进行严格的监控，以便我们能够迅速了解应用程序中的故障。
- en: 'In the software development life cycle, the **QA** phase is, in my opinion,
    one of the most important one: no matter how good your code looks, if it does
    not work, it is useless. However, if I have to choose another phase where engineers
    should put more emphasis, it would be the deployment, and more specifically, the
    monitoring that is carried out after every deployment. If you receive error reports
    immediately, chances are that the reaction can be quick enough to avoid bigger
    problems such as corrupted data or customers complaining.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件开发生命周期中，**QA**阶段，在我看来，是最重要的阶段之一：无论你的代码看起来多么好，如果它不起作用，那就是无用的。然而，如果我必须选择另一个阶段，工程师应该更加重视的是部署，更具体地说，是在每次部署后进行的监控。如果你立即收到错误报告，那么很有可能能够迅速做出反应，避免更大的问题，比如数据损坏或客户抱怨。
- en: We also saw an example of active monitoring carried out by Netflix on their
    systems, which even though might be out of the reach of your company, it can spark
    good ideas and practices in order to monitor your software.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还看到了Netflix在其系统上进行的主动监控的例子，尽管这可能超出了你公司的范围，但它可以激发出对监控软件的好想法和实践。
- en: Keymetrics is just an example that fits the bill for Node.js as it is extremely
    well integrated with PM2, but there are also other good monitoring systems such
    as **AppDynamics**, or if you want to go for an in-house software, you could use
    Nagios. The key is being clear about what you need to monitor in the application,
    and then, find the best provider.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: Keymetrics只是一个符合Node.js要求的例子，因为它与PM2集成非常好，但也有其他好的监控系统，比如**AppDynamics**，或者如果你想使用内部软件，你可以使用Nagios。关键是要清楚地知道需要监控应用程序中的内容，然后找到最好的提供者。
- en: Another two good options for monitoring Node.js apps are StrongLoop and New
    Relic. They are both on the same line with Keymetrics, but they work better for
    large-scale systems, especially StrongLoop, which is oriented to applications
    written in Node.js and oriented to microservices.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 监控Node.js应用程序的另外两个好选择是StrongLoop和New Relic。它们与Keymetrics处于同一水平线，但对于大规模系统来说效果更好，特别是StrongLoop，它专门针对使用Node.js编写的应用程序和微服务。
