- en: Chapter 7. Monitoring Microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Monitoring servers is always a controversial subject. It usually falls under
    system administration, and software engineers don''t even go near it, but we are
    losing one of the huge benefits of monitoring: *the ability to react quickly to
    failures*. By monitoring our system very closely, we can be aware of problems
    almost immediately so that the actions to correct the problem may even save us
    from impacting the customers. Along with monitoring, there is the concept of performance.
    By knowing how our system behaves during load periods, we will be able to anticipate
    the necessity of scaling the system. In this chapter, we will discuss how to monitor
    servers, and specifically microservices, in order to maintain the stability of
    our system.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring using PM2 and Keymetrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simian Army – the active monitoring from Spotify
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Throughput and performance degradation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When monitoring a microservice, we are interested in a few different types
    of metrics. The first big group of metrics is the hardware resources, which are
    described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Memory metrics**: This indicates how much memory is left in our system or
    consumed by our application'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CPU utilization**: As the name suggests, this indicates how much CPU are
    we using at a given time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Disk utilization**: This indicates the I/O pressure in the physical hard
    drives'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The second big group is the application metrics, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Number of errors per time unit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of calls per time unit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Response time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even though both groups are connected and a problem in the hardware will impact
    the application performance (and the other way around), knowing all of them is
    a must.
  prefs: []
  type: TYPE_NORMAL
- en: Hardware metrics are easy to query if our server is a Linux machine. On Linux,
    all the magic of hardware resources happens in the `/proc` folder. This folder
    is maintained by the kernel of the system and contains files about how the system
    behaves regarding a number of aspects in the system.
  prefs: []
  type: TYPE_NORMAL
- en: Software metrics are harder to collect and we are going to use **Keymetrics**
    from the creators of PM2 to monitor our Node.js applications.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring using PM2 and Keymetrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PM2, as we've seen before, is a very powerful instrument to run Node applications,
    but it is also very good at monitoring standalone applications in production servers.
    However, depending on your business case, it is not always easy to get access
    to the production.
  prefs: []
  type: TYPE_NORMAL
- en: 'The creators of PM2 have solved this problem by creating Keymetrics. Keymetrics
    is a **Software as a service** (**SaaS**) component that allows PM2 to send monitoring
    data across the network to its website, as shown in the following image (as found
    at [https://keymetrics.io/](https://keymetrics.io/)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Monitoring using PM2 and Keymetrics](img/B04889_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Even though Keymetrics is not free, it provides a free tier to demonstrate how
    it works. We are going to use it in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The very first thing that we need to do is register a user. Once we get access
    to our account, we should see something similar to the following screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Monitoring using PM2 and Keymetrics](img/B04889_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This screen is asking us to create a bucket. Keymetrics uses the bucket concept
    to define a context. For example, if our organization has different areas (payments,
    customer service, and so on) with different servers on each area, we could monitor
    all the servers in one bucket. There are no restrictions on how many servers you
    can have in one bucket. It is even possible to have all the organization in the
    same bucket so that everything is easy to access.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a bucket called `Monitoring Test`, as shown in the following
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Monitoring using PM2 and Keymetrics](img/B04889_07_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Easy, once we tap on **Create Bucket**, Keymetrics will show us a screen with
    the information needed to start monitoring our app, as shown in the following
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Monitoring using PM2 and Keymetrics](img/B04889_07_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the screen displays information about the private key used by
    Keymetrics. Usually, it is a very bad idea to share this key with anyone.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown on the screen, the next step is to configure PM2 to push data into
    Keymetrics. There is also useful information about the networking needed to make
    Keymetrics work:'
  prefs: []
  type: TYPE_NORMAL
- en: PM2 will be pushing data to the port **80** on Keymetrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keymetrics will be pushing data back to us on the port **43554**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Usually, in large organizations, there are restrictions about the networking,
    but if you are testing this from home, everything should work straightaway.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to make PM2 able to send metrics to Keymetrics, we need to install
    one PM2 module called `pm2-server-monit`. This is a fairly easy task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in an output similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Monitoring using PM2 and Keymetrics](img/B04889_07_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s run the advised command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In this case, I have replaced `[server name]` with `my-server`. There are no
    restrictions on the server name; however, when rolling out Keymetrics into a real
    system, it is recommended to choose a descriptive name in order to easily identify
    the server in the dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding command will produce an output similar to the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Monitoring using PM2 and Keymetrics](img/B04889_07_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This is an indication that everything went well and our application is ready
    to be monitored from Keymetrics that can be checked on [https://app.keymetrics.io/](https://app.keymetrics.io/),
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Monitoring using PM2 and Keymetrics](img/B04889_07_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now, our server is showing up in the interface. As we previously stated, this
    bucket could monitor different servers. A simple virtual machine is created, and
    as you can see at the bottom of the screen, Keymetrics provides us with the command
    to be executed in order to add another server. In this case, as we are using the
    free access to Keymetrics, so we can only monitor one server.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see what Keymetrics can offer us. At first sight, we can see interesting
    metrics such as CPU usage, memory available, disk available, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: All these are hardware metrics that indicate how our system is behaving. Under
    pressure, they are the perfect indicator to point out the need for more hardware
    resources.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, the hardware resources are the main indicator of failure in an application.
    Now, we are going to see how to use Keymetrics to diagnose the problem.
  prefs: []
  type: TYPE_NORMAL
- en: Diagnosing problems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A memory leak is usually a difficult problem to solve due to the nature of the
    flaw. Take a look at the following code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s run the program using a simple `seneca.act()` action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This program has a very obvious memory leak, and by obvious, I mean that it
    is written to be obvious. The `names` array will keep growing indefinitely. In
    the preceding example, it is not a big deal due to the fact that our application
    is a script that will start and finish without keeping the state in memory.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Remember that JavaScript allocates variables in the global scope if the `var`
    keyword is not used.
  prefs: []
  type: TYPE_NORMAL
- en: The problem comes when someone else reutilizes our code in a different part
    of the application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s assume that our system grows to a point that we need a microservice
    to greet new customers (or deliver the initial payload of personal information
    such as name, preferences, configuration, and so on). The following code could
    be a good example on how to build it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, Seneca will be listening over HTTP for requests from Seneca
    clients or other types of systems such as **curl**. When we run the application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Then from another terminal, we use curl to act as a client of our microservice,
    everything will work smoothly and our memory leak will go unnoticed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'However, we are going to use Keymetrics to find the problem. The first thing
    we need to do is run our program using PM2\. In order to do it so, we run the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This command will produce the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagnosing problems](img/B04889_07_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s explain the output in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The first line gives us information about the integration with Keymetrics. Data
    such as public key, server name, and the URL to access Keymetrics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the first table, we can see the name of the application running, as well
    as few statistics on memory, uptime, CPU, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the second table, we can see the information relevant to the `pm2-server-monit`
    PM2 module.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let''s go to Keymetrics and see what has happened:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagnosing problems](img/B04889_07_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The application is now registered in Keymetrics as it can be seen in the control
    panel
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, now our application is showing up in Keymetrics.
  prefs: []
  type: TYPE_NORMAL
- en: Straightaway, we can see the very useful things about our app. One of these
    is the memory used. This is the metric that will indicate a memory leak, as it
    will keep growing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we are going to force the memory leak to cause a problem in our application.
    In this case, the only thing that we need to do is start our server (the small
    application that we wrote before) and issue a significant number of requests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'As simple as the small bash script, this is all it takes to open Pandora''s
    Box in our application:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagnosing problems](img/B04889_07_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The application is now showing a high load (36% of CPU and an increased use
    of memory up to 167 MB)
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding image shows the impact of running the loop of requests in our
    system. Let''s explain it in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The CPU in our application has gone to **11%** with an average loop delay of
    **1.82** milliseconds. Regarding our system, the overall CPU utilization has gone
    up to **36.11%** due to the fact that both the application and bash script use
    a significant amount of resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The memory consumption has soared from **81.9 MB** to **167.6 MB**. As you can
    see, the line on the graph of memory allocation is not going straight up, and
    that is due to garbage collections. A garbage collection is an activity within
    the Node.js framework where unreferenced objects are freed from the memory, allowing
    our system to reuse the hardware resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regarding the errors, our application has been stable with **0** errors (we'll
    come back to this section later).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, once our bash script is finished (I stopped it manually, as it can take
    a significant amount of resources and time to finish), we can again see what happened
    to our system in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagnosing problems](img/B04889_07_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can see that the CPU has gone back to normal, but what about the memory?
    The memory consumed by our application hasn't been freed due to the fact that
    our program has a memory leak, and as long as our variable is referencing the
    memory consumed (remember the `names` array is accumulating more and more names),
    it won't be freed.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we have a very simple example that clearly shows where the memory
    leak is, but in complex applications, it is not always obvious. This error, in
    particular, could never show up as a problem due to the fact that we probably
    deploy new versions of our app often enough to not realize it.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring application exceptions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Application errors are events that occur when our application can't handle an
    unexpected situation. Things such as dividing a number by zero or trying to access
    an undefined property of our application usually leads to these type of problems.
  prefs: []
  type: TYPE_NORMAL
- en: When working on a multithreaded framework (language) such as Tomcat, the fact
    that one of our threads dies due to an exception usually only affects to one customer
    (the one holding the thread). However, in Node.js, a bubbled exception could be
    a significant problem as our application dies.
  prefs: []
  type: TYPE_NORMAL
- en: PM2 and Seneca do a very good job at keeping it alive as PM2 will restart our
    app if something makes it stop, and Seneca won't let the application die if an
    exception occurs in one of the actions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keymetrics has developed a module called **pmx** that we can use to programmatically
    get alerts on errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'It is easy and self-descriptive: an action that sends an exception to Keymetrics
    if the number sent as a parameter is zero. If we run it, we will get the following
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Monitoring application exceptions](img/B04889_07_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we need to hit the server in order to cause the error. As we did earlier,
    we will do this using curl:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, when we go to Keymetrics, we can see that there is an error logged, as
    shown in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Monitoring application exceptions](img/B04889_07_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Another interesting point of Keymetrics is the configuration of alerts. As PM2
    sends data about pretty much every metric in our system, we have the ability to
    configure Keymetrics on the thresholds that we consider healthy for our application.
  prefs: []
  type: TYPE_NORMAL
- en: This is very handy as we could get the notifications integrated in our corporate
    chat (something similar to **Slack**) and be alerted real time when something
    is not going correctly in our application.
  prefs: []
  type: TYPE_NORMAL
- en: Custom metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Keymetrics also allows us to use **probes**. A probe is a custom metric that
    is sent programmatically by the application to Keymetrics.
  prefs: []
  type: TYPE_NORMAL
- en: There are different types of values that the native library from Keymetrics
    allows us to push directly. We are going to see the most useful ones.
  prefs: []
  type: TYPE_NORMAL
- en: Simple metric
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A simple metric is, as its name indicates, a very basic metric where the developer
    can set the value to the data sent to Keymetrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, the metric will send the time when the action was called for
    the last time to the Keymetrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Simple metric](img/B04889_07_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The configuration for this metric is non-existent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: There is no complexity in this metric.
  prefs: []
  type: TYPE_NORMAL
- en: Counter
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This metric is very useful to count how many times an event occurred:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we can see how the counter is incremented for every single
    call to the action counter.
  prefs: []
  type: TYPE_NORMAL
- en: 'This metric will also allow us to decrement the value calling the `dec()` method
    on the counter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Average calculated values
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This metric allows us to record when an event occurs, and it will automatically
    calculate the number of events per time unit. It is quite useful to calculate
    averages and is a good tool to measure the load in the system. Let''s see a simple
    example, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code creates a probe and sends a new metric called `Calls per
    minute` to Keymetrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, if we run the application and the following command a few times, the metric
    is shown in the following Keymetrics interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![Average calculated values](img/B04889_07_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, there is a new metric called `Calls per minute` in the UI.
    The key to configure this metric is in the following initialization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the name of the metric is in the configuration dictionary as
    well as in two parameters: `samples` and `timeframe`.'
  prefs: []
  type: TYPE_NORMAL
- en: The `samples` parameter correspond to the interval where we want to rate the
    metric; in this case, it is the number of calls per minute so that rate is `60`
    seconds.
  prefs: []
  type: TYPE_NORMAL
- en: The `timeframe` parameter, on the other hand, is for how long we want Keymetrics
    to hold the data for, or to express in simpler words, it is the timeframe over
    which the metric will be analyzed.
  prefs: []
  type: TYPE_NORMAL
- en: Simian Army – the active monitoring from Spotify
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Spotify** is one of the companies of reference when building microservices-oriented
    applications. They are extremely creative and talented when it boils down to coming
    up with new ideas.'
  prefs: []
  type: TYPE_NORMAL
- en: One of my favourite ideas among them is what they call the **Simian Army**.
    I like to call it **active monitoring**.
  prefs: []
  type: TYPE_NORMAL
- en: In this book, I have talked a lot times about how humans fail at performing
    different tasks. No matter how much effort you put in to creating your software,
    there are going to be bugs that will compromise the stability of the system.
  prefs: []
  type: TYPE_NORMAL
- en: This is a big problem, but it becomes a huge deal when, with the modern cloud
    providers, your infrastructure is automated with a script.
  prefs: []
  type: TYPE_NORMAL
- en: Think about it, what happens if in a pool of thousand servers, three of them
    have the *time zone out of sync* with the rest of the servers? Well, depending
    on the nature of your system, it could be fine or it could be a big deal. Can
    you imagine your bank giving you a statement with disordered transactions?
  prefs: []
  type: TYPE_NORMAL
- en: Spotify has solved (or mitigated) the preceding problem by creating a number
    of software agents (a program that moves within the different machines of our
    system), naming them after different species of monkeys with different purposes
    to ensure the robustness of their infrastructure. Let's explain it a bit further.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you are probably aware, if you have worked before with Amazon Web Services,
    the machines and computational elements are divided in to regions (EU, Australia,
    USA, and so on) and inside every region, there are availability zones, as shown
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Simian Army – the active monitoring from Spotify](img/B04889_07_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This enables us, the engineers, to create software and infrastructure without
    hitting what we call a single point of failure.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A **single point of failure** is a condition in a system where the failure of
    a single element will cause the system to misbehave.
  prefs: []
  type: TYPE_NORMAL
- en: 'This configuration raised a number of questions to the engineers in Spotify,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: What happens if we blindly trust our design without testing whether we actually
    have any point of failures or not?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What happens if a full availability zone or region goes down?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How is our application going to behave if there is an abnormal latency for some
    reason?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To answer all these questions, Netflix has created various agents. An agent
    is a software that runs on a system (in this case, our microservices system) and
    carries on different operations such as checking the hardware, measuring the network
    latency, and so on. The idea of agent is not new, but until now, its application
    was nearly a futuristic subject*.* Let''s take a look at the following agents
    created by Netflix:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Chaos Monkey**: This agent disconnects healthy machines from the network
    in a given availability zone. This ensures that there are *no single points of
    failures within an availability zone*. So that if our application is balanced
    across four nodes, when the Chaos Monkey kicks in, it will disconnect one of these
    four machines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Chaos Gorilla**: This is similar to Chaos Monkey, Chaos Gorilla will disconnect
    a full availability zone in order to verify that Netflix services rebalance to
    the other available zones. In other words, Chaos Gorilla is the big brother of
    Chaos Monkey; instead of operating at the server level, it operates at the partition
    level.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Latency Monkey**: This agent is responsible for introducing artificial latencies
    in the connections. Latency is usually something that is hardly considered when
    developing a system, but it is a very delicate subject when building a microservices
    architecture: latency in one node could compromise the quality of the full system.
    When a service is running out of resources, usually the first indication is the
    latency in the responses; therefore, Latency Monkey *is a good way to find out
    how our system will behave under pressure*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Doctor Monkey**: A health check is an endpoint in our application that returns
    an HTTP 200 if everything is correct and 500 error code if there is a problem
    within the application. Doctor Monkey is an agent that will randomly execute the
    health check of nodes in our application and report the faulty ones in order to
    replace them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**10-18 Monkey**: Organizations such as Netflix are global, so they need to
    be language-aware (certainly, you don''t want to get a website in German when
    your mother tongue is Spanish). The 10-18 Monkey reports on instances that are
    misconfigured.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are a few other agents, but I just want to explain active monitoring to
    you. Of course, this type of monitoring is out of reach of small organizations,
    but it is good to know about their existence so that we can get inspired to set
    up our monitoring procedures.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The code is available under Apache License in the following repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/Netflix/SimianArmy](https://github.com/Netflix/SimianArmy).'
  prefs: []
  type: TYPE_NORMAL
- en: In general, this active monitoring follows the philosophy of *fail early*, of
    which, I am a big adept. No matter how big the flaw in your system is or how critical
    it is, you want to find it sooner than later, and ideally, without impacting any
    customer.
  prefs: []
  type: TYPE_NORMAL
- en: Throughput and performance degradation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Throughput is to our application what the monthly production is to a factory.
    It is a unit of measurement that gives us an indication about how our application
    is performing and answers the *how many* question of a system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Very close to throughput, there is another unit that we can measure: **latency**.'
  prefs: []
  type: TYPE_NORMAL
- en: Latency is the performance unit that answers the question of *how long*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our application is a microservices-based architecture that is responsible for
    calculating credit ratings of applicants to withdraw a mortgage. As we have a
    large volume of customers (a nice problem to have), we have decided to process
    the applications in batches. Let''s draw a small algorithm around it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a small and simple Seneca application (this is only theoretical, don''t
    try to run it as there is a lot of code missing!) that acts as a client for two
    other microservices, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The first one gets the list of pending applications for mortgages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second one gets the list of credit rating for the customers that we have
    requested
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This could be a real situation for processing mortgage applications. In all
    fairness, I worked on a very similar system in the past, and even though it was
    a lot more complex, the workflow was very similar.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s talk about throughput and latency. Imagine that we have a fairly big
    batch of mortgages to process and the system is misbehaving: the network is not
    being as fast as it should and is experiencing some dropouts.'
  prefs: []
  type: TYPE_NORMAL
- en: Some of these applications will be lost and will need to be retried. In ideal
    conditions, our system is producing a throughput of 500 applications per hour
    and takes an average of 7.2 seconds on latency to process every single application.
    However, today, as we stated before, the system is not at its best; we are processing
    only 270 applications per hour and takes on average 13.3 seconds to process a
    single mortgage application.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, with latency and throughput, we can measure how our business
    transactions are behaving with respect to the previous experiences; we are operating
    at 54% of our normal capacity.
  prefs: []
  type: TYPE_NORMAL
- en: This could be a serious issue. In all fairness, a drop off like this should
    ring all the alarms in our systems as something really serious is going on in
    our infrastructure; however, if we have been smart enough while building our system,
    the performance will be degraded, but our system won't stop working. This can
    be easily achieved by the usage of circuit breakers and queueing technologies
    such as **RabbitMQ**.
  prefs: []
  type: TYPE_NORMAL
- en: Queueing is one of the best examples to show how to apply human behavior to
    an IT system. Seriously, the fact that we can easily decouple our software components
    having a simple message as a joint point, which our services either produce or
    consume, gives us a big advantage when writing complex software.
  prefs: []
  type: TYPE_NORMAL
- en: Other advantage of queuing over HTTP is that an HTTP message is lost if there
    is a network drop out.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to build our application around the fact that it is either full success
    or error. With queueing technologies such as RabbitMQ, our messaging delivery
    is asynchronous so that we don''t need to worry about intermittent failures: *as
    soon as we can deliver the message to the appropriate queue, it will get persisted
    until the client is able to consume it (or the message timeout occurs)*.'
  prefs: []
  type: TYPE_NORMAL
- en: This enables us to account for intermittent errors in the infrastructure and
    build even more robust applications based on the communication around queues.
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, Seneca makes our life very easy: the plugin system on which the Seneca
    framework is built makes writing a transport plugin a fairly simple task.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'RabbitMQ transport plugin can be found in the following GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/senecajs/seneca-rabbitmq-transport](https://github.com/senecajs/seneca-rabbitmq-transport)'
  prefs: []
  type: TYPE_NORMAL
- en: There are quite few transport plugins and we can also create our own ones (or
    modify the existing ones!) to satisfy our needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you take a quick look at the RabbitMQ plugin (just as an example), the only
    thing that we need to do to write a transport plugin is overriding the following
    two Seneca actions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`seneca.add({role: ''transport'', hook: ''listen'', type: ''rabbitmq''}, ...)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`seneca.add({role: ''transport'', hook: ''client'', type: ''rabbitmq''}, ...)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using queueing technologies, our system will be *more resilient against intermittent
    failures* and we would be able to degrade the performance instead of heading into
    a catastrophic failure due to missing messages.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we deep dived into PM2 monitoring through Keymetrics. We learned
    how to put tight monitoring in place so that we are quickly informed about the
    failures in our application.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the software development life cycle, the **QA** phase is, in my opinion,
    one of the most important one: no matter how good your code looks, if it does
    not work, it is useless. However, if I have to choose another phase where engineers
    should put more emphasis, it would be the deployment, and more specifically, the
    monitoring that is carried out after every deployment. If you receive error reports
    immediately, chances are that the reaction can be quick enough to avoid bigger
    problems such as corrupted data or customers complaining.'
  prefs: []
  type: TYPE_NORMAL
- en: We also saw an example of active monitoring carried out by Netflix on their
    systems, which even though might be out of the reach of your company, it can spark
    good ideas and practices in order to monitor your software.
  prefs: []
  type: TYPE_NORMAL
- en: Keymetrics is just an example that fits the bill for Node.js as it is extremely
    well integrated with PM2, but there are also other good monitoring systems such
    as **AppDynamics**, or if you want to go for an in-house software, you could use
    Nagios. The key is being clear about what you need to monitor in the application,
    and then, find the best provider.
  prefs: []
  type: TYPE_NORMAL
- en: Another two good options for monitoring Node.js apps are StrongLoop and New
    Relic. They are both on the same line with Keymetrics, but they work better for
    large-scale systems, especially StrongLoop, which is oriented to applications
    written in Node.js and oriented to microservices.
  prefs: []
  type: TYPE_NORMAL
