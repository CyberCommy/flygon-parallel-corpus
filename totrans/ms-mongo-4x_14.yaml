- en: Harnessing Big Data with MongoDB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MongoDB is often used in conjunction with big data pipelines because of its
    performance, flexibility, and lack of rigorous data schemas. This chapter will
    explore the big data landscape, and how MongoDB fits alongside message queuing,
    data warehousing, and extract, transform, load pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics that we will discuss in this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: What is big data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Message queuing systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data warehousing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A big data use case using Kafka, Spark on top of HDFS, and MongoDB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is big data?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last five years, the number of people accessing and using the internet
    has almost doubled from a little under 2 billion to around 3.7 billion. Half of
    the global population are now online.
  prefs: []
  type: TYPE_NORMAL
- en: With the number of internet users increasing, and with networks evolving, more
    data is being added to existing datasets each year. In 2016, global internet traffic
    was 1.2 zettabytes (which is 1.2 billion terabytes) and it is expected to grow
    to 3.3 zettabytes by 2021.
  prefs: []
  type: TYPE_NORMAL
- en: This enormous amount of data that is generated every year means that it is imperative
    that databases and data stores in general can scale and process our data efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: The term **big data** was first coined in the 1980's by John Mashey ([http://static.usenix.org/event/usenix99/invited_talks/mashey.pdf](http://static.usenix.org/event/usenix99/invited_talks/mashey.pdf)),
    and mostly came into play in the past decade with the explosive growth of the
    internet. Big data typically refers to datasets that are too large and complex
    to be processed by traditional data processing systems, and so need some kind
    of specialized system architecture to be processed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Big data''s defining characteristics are as follows, in general:'
  prefs: []
  type: TYPE_NORMAL
- en: Volume
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variety
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Velocity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Veracity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variety and variability refer to the fact that our data comes in different forms
    and our datasets have internal inconsistencies. These need to be smoothed out
    by a data cleansing and normalization system before we can actually process our
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Veracity refers to the uncertainty of the quality of data. Data quality may
    vary, with perfect data for some dates and missing datasets for others. This affects
    our data pipeline and how much we can invest in our data platforms, since, even
    today, one out of three business leaders don't completely trust the information
    they use to make business decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, velocity is probably the most important defining characteristic of
    big data (other than the obvious volume attribute) and it refers to the fact that
    big datasets not only have large volumes of data, but also grow at an accelerated
    pace. This makes traditional storage using, for example, indexing a difficult
    task.
  prefs: []
  type: TYPE_NORMAL
- en: The big data landscape
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Big data has evolved into a complex ecosystem affecting every sector of the
    economy. Going from hype to unrealistic expectations and back to reality, we now
    have big data systems implemented and deployed in most Fortune 1000 companies
    that deliver real value.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we segment the companies that participate in the big data landscape by industry,
    we would probably come up with the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analytics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications-enterprise
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications-industry
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cross-infrastructure analytics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data sources and APIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open source
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From an engineering point of view, we are probably more concerned about the
    underlying technologies than their applications in different industry sectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on our business domain, we may have data coming in from different
    sources, such as transactional databases, IoT sensors, application server logs,
    other websites via a web service API, or just plain web page content extraction:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/810c5ec0-b3c4-469b-a7fd-c26d1bc1dadf.png)'
  prefs: []
  type: TYPE_IMG
- en: Message queuing systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In most of the flows previously described, we have data being **extracted, transformed,
    loaded** (**ETL**) into an **enterprise data warehouse** (**EDW**). To extract
    and transform this data, we need a message queuing system to deal with spikes
    in traffic, endpoints being temporarily unavailable, and other issues that may
    affect the availability and scalability of this part of the system.
  prefs: []
  type: TYPE_NORMAL
- en: Message queues also provide decoupling between producers and consumers of messages.
    This allows for better scalability by partitioning our messages into different
    topics/queues.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, using message queues, we can have location-agnostic services that don't
    care where the message producers sit, which provide interoperability between different
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: In the message queuing world, the most popular systems in production at the
    time of writing this book are RabbitMQ, ActiveMQ, and Kafka. We will provide a
    small overview of them before we dive into our use case to bring all of them together.
  prefs: []
  type: TYPE_NORMAL
- en: Apache ActiveMQ
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache ActiveMQ is an open source message broker, written in Java, together
    with a full **Java Message Service** (**JMS**) client.
  prefs: []
  type: TYPE_NORMAL
- en: It is the most mature implementation out of the three that we examine here,
    and has a long history of successful production deployments. Commercial support
    is offered by many companies, including Red Hat.
  prefs: []
  type: TYPE_NORMAL
- en: It is a fairly simple queuing system to set up and manage. It is based on the
    JMS client protocol and is the tool of choice for Java EE systems.
  prefs: []
  type: TYPE_NORMAL
- en: RabbitMQ
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RabbitMQ, on the other hand, is written in Erlang and is based on the **Advanced
    Message Queuing Protocol** (**AMQP**) protocol. AMQP is significantly more powerful
    and complicated than JMS, as it allows peer-to-peer messaging, request/reply,
    and publish/subscribe models for one-to-one or one-to-many message consumption.
  prefs: []
  type: TYPE_NORMAL
- en: RabbitMQ has gained popularity in the past 5 years, and is now the most searched-for
    queuing system.
  prefs: []
  type: TYPE_NORMAL
- en: 'RabbitMQ''s architecture is outlined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b4473fae-e41f-43d5-b002-2a30e6ff17ba.png)'
  prefs: []
  type: TYPE_IMG
- en: Scaling in RabbitMQ systems is performed by creating a cluster of RabbitMQ servers.
    Clusters share data and state, which are replicated, but message queues are distinct
    per node. To achieve high availability we can also replicate queues in different
    nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Kafka
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kafka, on the other hand, is a queuing system that was first developed by LinkedIn
    for its own internal purposes. It is written in Scala and is designed from the
    ground up for horizontal scalability and the best performance possible.
  prefs: []
  type: TYPE_NORMAL
- en: Focusing on performance is a key differentiator for Apache Kafka, but it means
    that in order to achieve performance, we need to sacrifice something. Messages
    in Kafka don't hold unique IDs, but are addressed by their offset in the log.
    Apache Kafka consumers are not tracked by the system; it is the responsibility
    of the application design to do so. Message ordering is implemented at the partition
    level and it is the responsibility of the consumer to identify if a message has
    been delivered already.
  prefs: []
  type: TYPE_NORMAL
- en: 'Semantics were introduced in version 0.11 and are part of the latest 1.0 release
    so that messages can now be both strictly ordered within a partition and always
    arrive exactly once for each consumer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c856121-fad0-42bb-a2fb-6c572334bbc6.png)'
  prefs: []
  type: TYPE_IMG
- en: Data warehousing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using a message queuing system is just the first step in our data pipeline
    design. At the other end of message queuing, we would typically have a data warehouse
    to process the vast amount of data that arrives. There are numerous options there,
    and it is not the main focus of this book to go over these or compare them. However,
    we will skim through two of the most widely-used options from the Apache Software
    Foundation: Apache Hadoop and Apache Spark.'
  prefs: []
  type: TYPE_NORMAL
- en: Apache Hadoop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first, and probably still most widely used, framework for big data processing
    is Apache Hadoop. Its foundation is the **Hadoop Distributed File System** (**HDFS**).
    Developed at Yahoo! in the 2000s, it originally served as an open source alternative
    to **Google File System** (**GFS**), a distributed filesystem that was serving
    Google's needs for distributed storage of its search index.
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop also implemented a MapReduce alternative to Google's proprietary system,
    Hadoop MapReduce. Together with HDFS, they constitute a framework for distributed
    storage and computations. Written in Java, with bindings for most programming
    languages and many projects that provide abstracted and simple functionality,
    and sometimes based on SQL querying, it is a system that can reliably be used
    to store and process terabytes, or even petabytes, of data.
  prefs: []
  type: TYPE_NORMAL
- en: In later versions, Hadoop became more modularized by introducing **Yet Another
    Resource Negotiator** (**YARN**), which provides the abstraction for applications
    to be developed on top of Hadoop. This has enabled several applications to be
    deployed on top of Hadoop, such as **Storm**, **Tez**, **OpenMPI**, **Giraph**,
    and, of course, **Apache Spark**, as we will see in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop MapReduce is a batch-oriented system, meaning that it relies on processing
    data in batches, and is not designed for real-time use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Spark is a cluster-computing framework from the University of California,
    Berkeley's AMPLab. Spark is not a substitute for the complete Hadoop ecosystem,
    but mostly for the MapReduce aspect of a Hadoop cluster. Whereas Hadoop MapReduce
    uses on-disk batch operations to process data, Spark uses both in-memory and on-disk
    operations. As expected, it is faster with datasets that fit in memory. This is
    why it is more useful for real-time streaming applications, but it can also be
    used with ease for datasets that don't fit in memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apache Spark can run on top of HDFS using YARN or in standalone mode, as shown
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/72713523-c529-41c1-850c-451605c31865.png)'
  prefs: []
  type: TYPE_IMG
- en: This means that in some cases (such as the one that we will use in our following
    use case) we can completely ditch Hadoop for Spark if our problem is really well
    defined and constrained within Spark's capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Spark can be up to 100 times faster than Hadoop MapReduce for in-memory operations.
    Spark offers user-friendly APIs for Scala (its native language), Java, Python,
    and Spark SQL (a variation of the SQL92 specification). Both Spark and MapReduce
    are resilient to failure. Spark uses RDDs that are distributed across the whole
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: As we can see from the overall Spark architecture, as follows, we can have several
    different modules of Spark working together for different needs, from SQL querying
    to streaming and machine learning libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing  Spark with Hadoop MapReduce
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Hadoop MapReduce framework is more commonly compared to Apache Spark, a
    newer technology that aims to solve problems in a similar problem space. Some
    of their most important attributes are summarized in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Hadoop MapReduce** | **Apache Spark** |'
  prefs: []
  type: TYPE_TB
- en: '| Written in | Java | Scala |'
  prefs: []
  type: TYPE_TB
- en: '| Programming model | MapReduce | RDD |'
  prefs: []
  type: TYPE_TB
- en: '| Client bindings | Most high-level languages | Java, Scala, Python |'
  prefs: []
  type: TYPE_TB
- en: '| Ease of use | Moderate, with high-level abstractions (Pig, Hive, and so on)
    | Good |'
  prefs: []
  type: TYPE_TB
- en: '| Performance | High throughput in batch | High throughput in streaming and
    batch mode |'
  prefs: []
  type: TYPE_TB
- en: '| Uses | Disk (I/O bound) | Memory, degrading performance if disk is needed
    |'
  prefs: []
  type: TYPE_TB
- en: '| Typical node | Medium | Medium-large |'
  prefs: []
  type: TYPE_TB
- en: As we can see from the preceding comparison, there are pros and cons for both
    technologies. Spark arguably has better performance, especially in problems that
    use fewer nodes. On the other hand, Hadoop is a mature framework with excellent
    tooling on top of it to cover almost every use case.
  prefs: []
  type: TYPE_NORMAL
- en: MongoDB as a data warehouse
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Hadoop is often described as the 800 lb gorilla in the room of big data
    frameworks. Apache Spark, on the other hand, is more like a 200 lb cheetah for
    its speed, agility, and performance characteristics, which allow it to work well
    in a subset of the problems that Hadoop aims to solve.
  prefs: []
  type: TYPE_NORMAL
- en: MongoDB, on the other hand, can be described as the MySQL equivalent in the
    NoSQL world, because of its adoption and ease of use. MongoDB also offers an aggregation
    framework, MapReduce capabilities, and horizontal scaling using sharding, which
    is essentially data partitioning at the database level. So naturally, some people
    wonder why we don't use MongoDB as our data warehouse to simplify our architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a pretty compelling argument, and it may or may not be the case that
    it makes sense to use MongoDB as a data warehouse. The advantages of such a decision
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Simpler architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Less need for message queues, reducing latency in our system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The disadvantages are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: MongoDB's MapReduce framework is not a replacement for Hadoop's MapReduce. Even
    though they both follow the same philosophy, Hadoop can scale to accommodate larger
    workloads.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling MongoDB's document storage using sharding will hit a wall at some point.
    Whereas Yahoo! has reported using 42,000 servers in its largest Hadoop cluster,
    the largest MongoDB commercial deployments stand at 5 billion (Craigslist), compared
    to 600 nodes and petabytes of data for Baidu, the internet giant dominating, among
    others, the Chinese internet search market.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is more than an order of magnitude of difference in terms of scaling.
  prefs: []
  type: TYPE_NORMAL
- en: MongoDB is mainly designed around being a real-time querying database based
    on stored data on disk, whereas MapReduce is designed around using batches, and
    Spark is designed around using streams of data.
  prefs: []
  type: TYPE_NORMAL
- en: A big data use case
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Putting all of this into action, we will develop a fully working system using
    a data source, a Kafka message broker, an Apache Spark cluster on top of HDFS
    feeding a Hive table, and a MongoDB database. Our Kafka message broker will ingest
    data from an API, streaming market data for an XMR/BTC currency pair. This data
    will be passed on to an Apache Spark algorithm on HDFS to calculate the price
    for the next ticker timestamp, based on the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The corpus of historical prices already stored on HDFS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The streaming market data arriving from the API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This predicted price will then be stored in MongoDB using the MongoDB Connector
    for Hadoop. MongoDB will also receive data straight from the Kafka message broker,
    storing it in a special collection with the document expiration date set to one
    minute. This collection will hold the latest orders, with the goal of being used
    by our system to buy or sell, using the signal coming from the Spark ML system.
  prefs: []
  type: TYPE_NORMAL
- en: So, for example, if the price is currently 10 and we have a bid for 9.5, but
    we expect the price to go down at the next market tick, then the system would
    wait. If we expect the price to go up in the next market tick, then the system
    would increase the bid price to 10.01 to match the price in the next ticker.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, if the price is 10 and we bid for 10.5, but expect the price to go
    down, we would adjust our bid to 9.99 to make sure we don't overpay for it. But,
    if the price is expected to go up, we would immediately buy to make a profit at
    the next market tick.
  prefs: []
  type: TYPE_NORMAL
- en: 'Schematically, our architecture looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b14f1dcd-50d9-44b7-9902-c3234bb6efa8.png)'
  prefs: []
  type: TYPE_IMG
- en: The API is simulated by posting JSON messages to a Kafka topic named `xmr_btc`.
    On the other end, we have a Kafka consumer importing real-time data to MongoDB.
  prefs: []
  type: TYPE_NORMAL
- en: We also have another Kafka consumer importing data to Hadoop to be picked up
    by our algorithms, which send recommendation data (signals) to a Hive table. Finally,
    we export data from the Hive table into MongoDB.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Kafka
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step in setting up the environment for our big data use case is to
    establish a Kafka node. Kafka is essentially a FIFO queue, so we will use the
    simplest single node (broker) setup. Kafka organizes data using topics, producers,
    consumers, and brokers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The important Kafka terminologies are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A **broker** is essentially a node.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **producer** is a process that writes data to the message queue.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **consumer** is a process that reads data from the message queue.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **topic** is the specific queue that we write to and read data from.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Kafka topic is further subdivided into a number of partitions. We can split
    data from a particular topic into multiple brokers (nodes), both when we write
    to the topic and also when we read our data at the other end of the queue.
  prefs: []
  type: TYPE_NORMAL
- en: 'After installing Kafka on our local machine, or any cloud provider of our choice
    (there are excellent tutorials for EC2 to be found just a search away), we can
    create a topic using this single command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This will create a new topic called `xmr-btc`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deleting the topic is similar to creating one, by using this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then get a list of all topics by issuing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then create a command-line producer for our topic, just to test that
    we can send messages to the queue, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Data on every line will be sent as a string encoded message to our topic, and
    we can end the process by sending a `SIGINT` signal (typically *Ctrl* + *C*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Afterwards we can view the messages that are waiting in our queue by spinning
    up a consumer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This consumer will read all messages in our `xmr-btc` topic, starting from the
    beginning of history. This is useful for our test purposes, but we will change
    this configuration in real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: You will keep seeing `zookeeper`, in addition to `kafka`, mentioned in the commands.
    Apache Zookeeper comes together with Apache Kafka, and is a centralized service
    that is used internally by Kafka for maintaining configuration information, naming,
    providing distributed synchronization, and providing group services.
  prefs: []
  type: TYPE_NORMAL
- en: Now we have set up our broker, we can use the code at [https://github.com/agiamas/mastering-mongodb/tree/master/chapter_9 ](https://github.com/agiamas/mastering-mongodb/tree/master/chapter_9)to
    start reading (consuming) and writing (producing) messages to the queue. For our
    purposes, we are using the `ruby-kafka` gem, developed by Zendesk.
  prefs: []
  type: TYPE_NORMAL
- en: For simplicity, we are using a single class to read from a file stored on disk
    and to write to our Kafka queue.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our `produce` method will be used to write messages to Kafka as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Our `consume` method will read messages from Kafka as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Notice that we are using the consumer group API feature (added in Kafka 0.9)
    to get multiple consumers to access a single topic by assigning each partition
    to a single consumer. In the event of a consumer failure, its partitions will
    be reallocated to the remaining members of the group.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to write these messages to MongoDB, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create our collection so that our documents expire after one minute.
    Enter the following in the `mongo` shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This way, we create a new database called `exchange_data` with a new collection
    called `xmr_btc` that has auto-expiration after one minute. For MongoDB to auto-expire
    documents, we need to provide a field with a `datetime` value to compare its value
    against the current server time. In our case, this is the `createdAt` field.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our use case, we will use the low-level MongoDB Ruby driver. The code for
    `MongoExchangeClient` is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This client connects to our local database, sets the `createdAt` field for the
    TTL document expiration, and saves the message to our collection.
  prefs: []
  type: TYPE_NORMAL
- en: With this setup, we can write messages to Kafka, read them at the other end
    of the queue, and write them into our MongoDB collection.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Hadoop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can install Hadoop and use a single node for the use case in this chapter
    using the instructions from Apache Hadoop's website at [https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html).
  prefs: []
  type: TYPE_NORMAL
- en: After following these steps, we can browse the HDFS files in our local machine
    at `http://localhost:50070/explorer.html#/`. Assuming that our signals data is
    written in HDFS under the `/user/<username>/signals` directory, we will use the
    MongoDB Connector for Hadoop to export and import it into MongoDB.
  prefs: []
  type: TYPE_NORMAL
- en: MongoDB Connector for Hadoop is the officially supported library, allowing MongoDB
    data files or MongoDB backup files in BSON to be used as the source or destination
    for Hadoop MapReduce tasks.
  prefs: []
  type: TYPE_NORMAL
- en: This means that we can also easily export to, and import data from, MongoDB
    when we are using higher-level Hadoop ecosystem tools such as Pig (a procedural
    high-level language), Hive (a SQL-like high-level language), and Spark (a cluster-computing
    framework).
  prefs: []
  type: TYPE_NORMAL
- en: Steps for Hadoop setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The different steps to set up Hadoop are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the JAR from the Maven repository at [http://repo1.maven.org/maven2/org/mongodb/mongo-hadoop/mongo-hadoop-core/2.0.2/](http://repo1.maven.org/maven2/org/mongodb/mongo-hadoop/mongo-hadoop-core/2.0.2/).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download `mongo-java-driver` from [https://oss.sonatype.org/content/repositories/releases/org/mongodb/mongodb-driver/3.5.0/](https://oss.sonatype.org/content/repositories/releases/org/mongodb/mongodb-driver/3.5.0/).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a directory (in our case, named `mongo_lib`) and copy these two JARs
    in there with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Alternatively, we can copy these JARs under the `share/hadoop/common/` directory.
    As these JARs will need to be available in every node, for clustered deployment,
    it's easier to use Hadoop's `DistributedCache` to distribute the JARs to all nodes.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to install Hive from [https://hive.apache.org/downloads.html](https://hive.apache.org/downloads.html).
    For this example, we used a MySQL server for Hive's metastore data. This can be
    a local MySQL server for development, but it is recommended that you use a remote
    server for production environments.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once we have Hive set up, we just run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we add the three JARs (`mongo-hadoop-core`, `mongo-hadoop-driver`, and `mongo-hadoop-hive`)
    that we downloaded earlier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'And then, assuming our data is in the table exchanges:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **customerid                                             ** | **int** |'
  prefs: []
  type: TYPE_TB
- en: '| `pair` | `String` |'
  prefs: []
  type: TYPE_TB
- en: '| `time` | `TIMESTAMP` |'
  prefs: []
  type: TYPE_TB
- en: '| `recommendation` | `int` |'
  prefs: []
  type: TYPE_TB
- en: We can also use Gradle or Maven to download the JARs in our local project. If
    we only need MapReduce, then we just download the `mongo-hadoop-core` JAR. For
    Pig, Hive, Streaming, and so on, we must download the appropriate JARs from
  prefs: []
  type: TYPE_NORMAL
- en: '[http://repo1.maven.org/maven2/org/mongodb/mongo-hadoop/](http://repo1.maven.org/maven2/org/mongodb/mongo-hadoop/).'
  prefs: []
  type: TYPE_NORMAL
- en: Some useful Hive commands include the following: `show databases;` and
  prefs: []
  type: TYPE_NORMAL
- en: '`create table exchanges(customerid int, pair String, time TIMESTAMP, recommendation
    int);`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we are all set, we can create a MongoDB collection backed by our local
    Hive data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can copy all data from the `exchanges` Hive table into MongoDB
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This way, we have established a pipeline between Hadoop and MongoDB using Hive,
    without any external server.
  prefs: []
  type: TYPE_NORMAL
- en: Using a Hadoop to MongoDB pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An alternative to using the MongoDB Connector for Hadoop is to use the programming
    language of our choice to export data from Hadoop, and then write into MongoDB
    using the low-level driver or an ODM, as described in previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in Ruby, there are a few options:'
  prefs: []
  type: TYPE_NORMAL
- en: '**WebHDFS** on GitHub, which uses the WebHDFS or the **HttpFS** Hadoop API
    to fetch data from HDFS'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: System calls, using the Hadoop command-line tool and Ruby's `system()` call
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Whereas in Python, we can use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**HdfsCLI**, which uses the WebHDFS or the HttpFS Hadoop API'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**libhdfs**, which uses a JNI-based native C wrapped around the HDFS Java client'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of these options require an intermediate server between our Hadoop infrastructure
    and our MongoDB server, but, on the other hand, allow for more flexibility in
    the ETL process of exporting/importing data.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Spark to MongoDB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MongoDB also offers a tool to directly query Spark clusters and export data
    to MongoDB. Spark is a cluster computing framework that typically runs as a YARN
    module in Hadoop, but can also run independently on top of other filesystems.
  prefs: []
  type: TYPE_NORMAL
- en: MongoDB Spark Connector can read and write to MongoDB collections from Spark
    using Java, Scala, Python, and R. It can also use aggregation and run SQL queries
    on MongoDB data after creating a temporary view for the dataset backed by Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Using Scala, we can also use Spark Streaming, the Spark framework for data-streaming
    applications built on top of Apache Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can refer to the following references for further information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.cisco.com/c/en/us/solutions/collateral/service-provider/visual-networking-index-vni/vni-hyperconnectivity-wp.html](https://www.cisco.com/c/en/us/solutions/collateral/service-provider/visual-networking-index-vni/vni-hyperconnectivity-wp.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://www.ibmbigdatahub.com/infographic/four-vs-big-data](http://www.ibmbigdatahub.com/infographic/four-vs-big-data)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://spreadstreet.io/database/](https://spreadstreet.io/database/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://mattturck.com/wp-content/uploads/2017/05/Matt-Turck-FirstMark-2017-Big-Data-Landscape.png](http://mattturck.com/wp-content/uploads/2017/05/Matt-Turck-FirstMark-2017-Big-Data-Landscape.png)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://mattturck.com/bigdata2017/](http://mattturck.com/bigdata2017/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://dzone.com/articles/hadoop-t-etl](https://dzone.com/articles/hadoop-t-etl)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.cloudamqp.com/blog/2014-12-03-what-is-message-queuing.html](https://www.cloudamqp.com/blog/2014-12-03-what-is-message-queuing.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.linkedin.com/pulse/jms-vs-amqp-eran-shaham](https://www.linkedin.com/pulse/jms-vs-amqp-eran-shaham)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.cloudamqp.com/blog/2017-01-09-apachekafka-vs-rabbitmq.html](https://www.cloudamqp.com/blog/2017-01-09-apachekafka-vs-rabbitmq.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://trends.google.com/trends/explore?date=all&q=ActiveMQ,RabbitMQ,ZeroMQ](https://trends.google.com/trends/explore?date=all&q=ActiveMQ,RabbitMQ,ZeroMQ)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://thenextweb.com/insider/2017/03/06/the-incredible-growth-of-the-internet-over-the-past-five-years-explained-in-detail/#.tnw_ALaObAUG](https://thenextweb.com/insider/2017/03/06/the-incredible-growth-of-the-internet-over-the-past-five-years-explained-in-detail/#.tnw_ALaObAUG)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf](https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/Apache_Hadoop#Architecture](https://en.wikipedia.org/wiki/Apache_Hadoop#Architecture)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://wiki.apache.org/hadoop/PoweredByYarn](https://wiki.apache.org/hadoop/PoweredByYarn)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.slideshare.net/cloudera/introduction-to-yarn-and-mapreduce-2?next_slideshow=1](https://www.slideshare.net/cloudera/introduction-to-yarn-and-mapreduce-2?next_slideshow=1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.mongodb.com/blog/post/mongodb-live-at-craigslist](https://www.mongodb.com/blog/post/mongodb-live-at-craigslist)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.mongodb.com/blog/post/mongodb-at-baidu-powering-100-apps-across-600-nodes-at-pb-scale](https://www.mongodb.com/blog/post/mongodb-at-baidu-powering-100-apps-across-600-nodes-at-pb-scale)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://www.datamation.com/data-center/hadoop-vs.-spark-the-new-age-of-big-data.html](http://www.datamation.com/data-center/hadoop-vs.-spark-the-new-age-of-big-data.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.mongodb.com/mongodb-data-warehouse-time-series-and-device-history-data-medtronic-transcript](https://www.mongodb.com/mongodb-data-warehouse-time-series-and-device-history-data-medtronic-transcript)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.mongodb.com/blog/post/mongodb-debuts-in-gartner-s-magic-quadrant-for-data-warehouse-and-data-management-solutions-for-analytics](https://www.mongodb.com/blog/post/mongodb-debuts-in-gartner-s-magic-quadrant-for-data-warehouse-and-data-management-solutions-for-analytics)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.infoworld.com/article/3014440/big-data/five-things-you-need-to-know-about-hadoop-v-apache-spark.html](https://www.infoworld.com/article/3014440/big-data/five-things-you-need-to-know-about-hadoop-v-apache-spark.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.quora.com/What-is-the-difference-between-Hadoop-and-Spark](https://www.quora.com/What-is-the-difference-between-Hadoop-and-Spark)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://iamsoftwareengineer.wordpress.com/2015/12/15/hadoop-vs-spark/?iframe=true&theme_preview=true](https://iamsoftwareengineer.wordpress.com/2015/12/15/hadoop-vs-spark/?iframe=true&theme_preview=true)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.infoq.com/articles/apache-kafka](https://www.infoq.com/articles/apache-kafka)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://stackoverflow.com/questions/42151544/is-there-any-reason-to-use-rabbitmq-over-kafka](https://stackoverflow.com/questions/42151544/is-there-any-reason-to-use-rabbitmq-over-kafka)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://medium.com/@jaykreps/exactly-once-support-in-apache-kafka-55e1fdd0a35f](https://medium.com/@jaykreps/exactly-once-support-in-apache-kafka-55e1fdd0a35f)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.slideshare.net/sbaltagi/apache-kafka-vs-rabbitmq-fit-for-purpose-decision-tree](https://www.slideshare.net/sbaltagi/apache-kafka-vs-rabbitmq-fit-for-purpose-decision-tree)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://techbeacon.com/what-apache-kafka-why-it-so-popular-should-you-use-it](https://techbeacon.com/what-apache-kafka-why-it-so-popular-should-you-use-it)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/zendesk/ruby-kafka](https://github.com/zendesk/ruby-kafka#producing-messages-to-kafka)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://zhongyaonan.com/hadoop-tutorial/setting-up-hadoop-2-6-on-mac-osx-yosemite.html](http://zhongyaonan.com/hadoop-tutorial/setting-up-hadoop-2-6-on-mac-osx-yosemite.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/mtth/hdfs](https://github.com/mtth/hdfs)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://wesmckinney.com/blog/outlook-for-2017/](http://wesmckinney.com/blog/outlook-for-2017/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://wesmckinney.com/blog/python-hdfs-interfaces/](http://wesmckinney.com/blog/python-hdfs-interfaces/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://acadgild.com/blog/how-to-export-data-from-hive-to-mongodb/](https://acadgild.com/blog/how-to-export-data-from-hive-to-mongodb/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://sookocheff.com/post/kafka/kafka-in-a-nutshell/](https://sookocheff.com/post/kafka/kafka-in-a-nutshell/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.codementor.io/jadianes/spark-mllib-logistic-regression-du107neto](https://www.codementor.io/jadianes/spark-mllib-logistic-regression-du107neto)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://ondra-m.github.io/ruby-spark/](http://ondra-m.github.io/ruby-spark/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://amodernstory.com/2015/03/29/installing-hive-on-mac/](https://amodernstory.com/2015/03/29/installing-hive-on-mac/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.infoq.com/articles/apache-spark-introduction](https://www.infoq.com/articles/apache-spark-introduction)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://cs.stanford.edu/~matei/papers/2010/hotcloud_spark.pdf](https://cs.stanford.edu/~matei/papers/2010/hotcloud_spark.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about the big data landscape and how MongoDB compares
    with, and fares against, message-queuing systems and data warehousing technologies.
    Using a big data use case, we learned how to integrate MongoDB with Kafka and
    Hadoop from a practical perspective.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will turn to replication and cluster operations, and
    discuss replica sets, the internals of elections, and the setup and administration
    of our MongoDB cluster.
  prefs: []
  type: TYPE_NORMAL
