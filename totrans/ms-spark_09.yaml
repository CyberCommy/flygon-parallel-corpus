- en: Chapter 9. Databricks Visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter builds on the work done in [Chapter 8](ch08.html "Chapter 8. Spark
    Databricks"), *Spark Databricks*, and continues to investigate the functionality
    of the Apache Spark-based service at [https://databricks.com/](https://databricks.com/).
    Although I will use Scala-based code examples in this chapter, I wish to concentrate
    on the Databricks functionality rather than the traditional Spark processing modules:
    MLlib, GraphX, Streaming, and SQL. This chapter will explain the following Databricks
    areas:'
  prefs: []
  type: TYPE_NORMAL
- en: Data visualization using Dashboards
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An RDD-based report
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Data stream-based report
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Databricks Rest interface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moving data with Databricks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, this chapter will examine the functionality in Databricks to analytically
    visualize data via reports, and dashboards. It will also examine the REST interface,
    as I believe it to be a useful tool for both, remote access, and integration purposes.
    Finally, it will examine the options for moving data, and libraries, into a Databricks
    cloud instance.
  prefs: []
  type: TYPE_NORMAL
- en: Data visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Databricks provides tools to access S3, and the local file system-based files.
    It offers the ability to import data into tables, as already shown. In the last
    chapter, raw data was imported into the shuttle table to provide the table-based
    data that SQL could be run against, to filter against rows and columns, allow
    data to be sorted, and then aggregated. This is very useful, but we are still
    left looking at raw data output when images, and reports, present information
    that can be more readily, and visually, interpreted.
  prefs: []
  type: TYPE_NORMAL
- en: Databricks provides a visualization interface, based on the tabular result data
    that your SQL session produces. The following screenshot shows some SQL that has
    been run. The resulting data, and the visualization drop-down menu under the data,
    show the possible options.
  prefs: []
  type: TYPE_NORMAL
- en: '![Data visualization](img/B01989_09_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'There is a range of visualization options here, starting with the more familiar
    **Bar** graphs, and **Pie** charts through to **Quantiles**, and **Box** plots.
    I''m going to change my SQL so that I get more options to plot a graph, which
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Data visualization](img/B01989_09_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Then, having selected the visualization option; **Bar** graph, I will select
    the **Plot** options which will allow me to choose the data for the graph vertices.
    It will also allow me to select a data column to pivot on. The following screenshot
    shows the values that I have chosen.
  prefs: []
  type: TYPE_NORMAL
- en: '![Data visualization](img/B01989_09_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The All fields section, from the **Plot** options display, shows all of the
    fields available for the graph display from the SQL statement result data. The
    **Keys** and **Values** sections define the data fields that will form the graph
    axes. The **Series grouping** field allows me to define a value, education, to
    pivot on. By selecting **Apply**, I can now create a graph of total balance against
    a job type, grouped by the education type, as the following screenshot shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Data visualization](img/B01989_09_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If I were an accountant trying to determine the factors affecting wage costs,
    and groups of employees within the company that cost the most, I would then see
    the green spike in the previous graph. It seems to indicate that the **management**
    employees with a tertiary education are the most costly group within the data.
    This can be confirmed by changing the SQL to filter on a **tertiary education**,
    ordering the result by balance descending, and creating a new bar graph.
  prefs: []
  type: TYPE_NORMAL
- en: '![Data visualization](img/B01989_09_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Clearly, the **management** grouping is approximately **14 million**. Changing
    the display option to **Pie** represents the data as a pie graph, with clearly
    sized segments and colors, which visually and clearly present the data, and the
    most important items.
  prefs: []
  type: TYPE_NORMAL
- en: '![Data visualization](img/B01989_09_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: I cannot examine all of the display options in this small chapter, but what
    I did want to show is the world map graph that can be created using geographic
    information. I have downloaded the `Countries.zip` file from [http://download.geonames.org/export/dump/](http://download.geonames.org/export/dump/).
  prefs: []
  type: TYPE_NORMAL
- en: 'This will offer a sizeable data set of around 281 MB compressed, which can
    be used to create a new table. It is displayed as a world map graph. I have also
    sourced an ISO2 to ISO3 set of mapping data, and stored it in a Databricks table
    called `cmap`. This allows me to convert ISO2 country codes in the data above
    i.e “AU” to ISO3 country codes i.e “AUS” (needed by the map graph I am about to
    use). The first column in the data that we will use for the map graph, must contain
    the geo location data. In this instance, the country codes in the ISO 3 format.
    So from the countries data, I will create a count of records for each country
    by ISO3 code. It is also important to ensure that the plot options are set up
    correctly in terms of keys, and values. I have stored the downloaded country-based
    data in a table called `geo1`. The SQL used is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Data visualization](img/B01989_09_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As shown previously, this gives two columns of data an ISO3-based value called
    `country`, and a numeric count called `value`. Setting the display option to `Map`
    creates a color-coded world map, shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Data visualization](img/B01989_09_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: These graphs show how data can be visually represented in various forms, but
    what can be done if a report is needed for external clients or a dashboard is
    required? All this will be covered in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Dashboards
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, I will use the data in the table called `geo1`, which was created
    in the last section for a map display. It was made to create a simple dashboard,
    and publish the dashboard to an external client. From the **Workspace** menu,
    I have created a new dashboard called `dash1`. If I edit the controls tab of this
    dashboard, I can start to enter SQL, and create graphs, as shown in the following
    screenshot. Each graph is represented as a view and can be defined via SQL. It
    can be resized, and configured using the plot options as per the individual graphs.
    Use the **Add** drop-down menu to add a view. The following screenshot shows that
    `view1` is already created, and added to `dash1`. `view2` is being defined.
  prefs: []
  type: TYPE_NORMAL
- en: '![Dashboards](img/B01989_09_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Once all the views have been added, positioned, and resized, the edit tab can
    be selected to present the finalized dashboard. The following screenshot now shows
    the finalized dashboard called `dash1` with three different graphs in different
    forms, and segments of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dashboards](img/B01989_09_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This is very useful for giving a view of the data, but this dashboard is within
    the Databricks cloud environment. What if I want a customer to see this? There
    is a **publish** menu option in the top-right part of the dashboard screen, which
    allows you to publish the dashboard. This displays the dashboard under a new publicly
    published URL, as shown in the following screenshot. Note the new URL at the top
    of the following screenshot. You can now share this URL with your customers to
    present results. There are also options to periodically update the display to
    represent updates in the underlying data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Dashboards](img/B01989_09_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This gives you an idea of the available display options. All of the reports,
    and dashboards created so far have been based upon SQL, and the data returned.
    In the next section, I will show that reports can be created programmatically
    using a Scala-based Spark RDD, and streamed data.
  prefs: []
  type: TYPE_NORMAL
- en: An RDD-based report
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following Scala-based example uses a user-defined class type called `birdType`,
    based on the bird name, and the volume encountered. An RDD is created of the bird
    type records, and then converted into a data frame. The data frame is then displayed.
    Databricks allows the displayed data to be presented as a table or using plot
    options as a graph. The following image shows the Scala that is used:'
  prefs: []
  type: TYPE_NORMAL
- en: '![An RDD-based report](img/B01989_09_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The bar graph, which this Scala example allows to be created, is shown in the
    following screenshot. The previous Scala code and the following screenshot are
    less important than the fact that this graph has been created programmatically
    using a data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![An RDD-based report](img/B01989_09_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This opens up the possibility of programmatically creating data frames, and
    temporary tables from calculation-based data sources. It also allows for streamed
    data to be processed, and the refresh functionality of dashboards to be used,
    to constantly present a window of streamed data. The next section will examine
    a stream-based example of report generation.
  prefs: []
  type: TYPE_NORMAL
- en: A stream-based report
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, I will use Databricks capability to upload a JAR-based library,
    so that we can run a Twitter-based streaming Apache Spark example. In order to
    do this, I must first create a Twitter account, and a sample application at: [https://apps.twitter.com/](https://apps.twitter.com/).'
  prefs: []
  type: TYPE_NORMAL
- en: The following screenshot shows that I have created an application called `My
    example app`. This is necessary, because I need to create the necessary access
    keys, and tokens to create a Scala-based twitter feed.
  prefs: []
  type: TYPE_NORMAL
- en: '![A stream-based report](img/B01989_09_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If I now select the application name, I can see the application details. This
    provides a menu option, which provides access to the application details, settings,
    access tokens, and permissions. There is also a button which says **Test OAuth**,
    this enables the access and token keys that will be created to be tested. The
    following screenshot shows the application menu options:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A stream-based report](img/B01989_09_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: By selecting the **Keys and Access Tokens** menu option, the access keys, and
    the access tokens can be generated for the application. Each of the application
    settings and tokens, in this section, have an API key, and a secret key. The top
    of the form, in the following screenshot, shows the consumer key, and consumer
    secret (of course, the key and account details have been removed from these images
    for security reasons).
  prefs: []
  type: TYPE_NORMAL
- en: '![A stream-based report](img/B01989_09_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'There are also options in the previous screenshot to regenerate the keys, and
    set permissions. The next screenshot shows the application access token details.
    There is an access token, and an access token secret. It also has the options
    to regenerate the values, and revoke access:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A stream-based report](img/B01989_09_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Using these four alpha numeric value strings, it is possible to write a Scala
    example to access a Twitter stream. The values that will be needed are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Consumer Key
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consumer Secret
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access Token
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access Token Secret
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following code sample, I will remove my own key values for security
    reasons. You just need to add your own values to get the code to work. I have
    developed my library, and run the code locally to check whether it will work.
    I did this before loading it to Databricks, in order to reduce time, and costs
    due to debugging. My Scala code sample looks like the following code. First, I
    define a package, import Spark streaming, and twitter resources. Then, I define
    an object class called `twitter1`, and create a main function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, I create a Spark configuration object using an application name. I have
    not used a Spark master URL, as I will let both, `spark-submit`, and Databricks
    assign the default URL. From this, I will create a Spark context, and define the
    Twitter consumer, and access values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'I set the Twitter access properties using the `System.setProperty` call, and
    use it to set the four `twitter4j` `oauth` access properties using the access
    keys, which were generated previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'A streaming context is created from the Spark context, which is used to create
    a Twitter-based Spark DStream. The stream is split by spaces to create words,
    and it gets filtered by the words starting with `#`, to select hash tags:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The function used below to get a singleton SQL Context is defined at the end
    of this example. So, for each RDD in the stream of hash tags, a single SQL context
    is created. This is used to import implicits which allows an RDD to be implicitly
    converted to a data frame using `toDF`. A data frame is created from each `rdd`
    called `dfHashTags`, and this is then used to register a temporary table. I have
    then run some SQL against the table to get a count of rows. The count of rows
    is then printed. The horizontal banners in the code are just used to enable easier
    viewing of the output of results when using `spark-submit`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'I have also output a list of the top five tweets by volume in my current tweet
    stream data window. You might recognize the following code sample. It is from
    the Spark examples on GitHub. Again, I have used the banner to help with the results
    that will be seen in the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, I have used `start` and `awaitTermination`, via the Spark stream context
    `ssc`, to start the application, and keep it running until stopped:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, I have defined the singleton SQL context function, and the `dataframe`
    `case` `class` for each row in the hash tag data stream `rdd`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'I compiled this Scala application code using SBT into a JAR file called `data-bricks_2.10-1.0.jar`.
    My `SBT` file looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'I downloaded the correct version of Apache Spark onto my cluster to match the
    current version used by Databricks at this time (1.3.1). I then installed it under
    `/usr/local/` on each node in my cluster, and ran it in local mode with spark
    as the cluster manager. My `spark-submit` script looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'I won''t go through the details, as it has been covered quite a few times,
    except to note that the class value is now `nz.co.semtechsolutions.twitter1`.
    This is the package class name, plus the application object class name. So, when
    I run it locally, I get an output as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This tells me that the application library works. It connects to Twitter, creates
    a data stream, is able to filter the data into hash tags, and creates a temporary
    table using the data. So, having created a JAR library for Twitter data streaming,
    and proving that it works, I'm now able to load it onto the Databricks cloud.
    The following screenshot shows that a job has been created from the Databricks
    cloud jobs menu called `joblib1`. The **Set Jar** option has been used to upload
    the JAR library that was just created. The full package-based name to the `twitter1`
    application object class has been specified.
  prefs: []
  type: TYPE_NORMAL
- en: '![A stream-based report](img/B01989_09_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The following screenshot shows the `joblib1` job, which is ready to run. A Spark-based
    cluster will be created on demand, as soon as the job is executed using the **Run
    Now** option, under the **Active runs** section. No scheduling options have been
    specified, although the job can be defined to run at a given date and time.
  prefs: []
  type: TYPE_NORMAL
- en: '![A stream-based report](img/B01989_09_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: I selected the **Run Now** option to start the job run, as shown in the following
    screenshot. This shows that there is now an active run called `Run 1` for this
    job. It has been running for six seconds. It was launched manually, and is pending
    while a on-demand cluster is created. By selecting the run name `Run 1`, I can
    see details about the job, especially the logged output.
  prefs: []
  type: TYPE_NORMAL
- en: '![A stream-based report](img/B01989_09_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The following screenshot shows an example of the output for `Run 1` of `joblib1`.
    It shows the time started and duration, it also shows the running status and job
    details in terms of class and JAR file. It would have shown class parameters,
    but there were none in this case. It also shows the details of the 54 GB on-demand
    cluster. More importantly, it shows the list of the top five tweet hash tag values.
  prefs: []
  type: TYPE_NORMAL
- en: '![A stream-based report](img/B01989_09_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The following screenshot shows the same job run output window in the Databricks
    cloud instance. But this shows the output from the SQL `count(*)`, showing the
    number of tweet hash tags in the current data stream tweet window from the temporary
    table.
  prefs: []
  type: TYPE_NORMAL
- en: '![A stream-based report](img/B01989_09_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: So, this proves that I can create an application library locally, using Twitter-based
    Apache Spark streaming, and convert the data stream into data frames, and a temporary
    table. It shows that I can reduce costs by developing locally, and then port my
    library to my Databricks cloud. I am aware that I have neither visualized the
    temporary table, nor the DataFrame in this example, into a Databricks graph, but
    time scales did not allow me to do this. Also, another thing that I would have
    done, if I had time, would be to checkpoint, or periodically save the stream to
    file, in case of application failure. However, this topic is covered in [Chapter
    3](ch03.html "Chapter 3. Apache Spark Streaming"), *Apache Spark Streaming* with
    an example, so you can take a look there if you are interested. In the next section,
    I will examine the Databricks REST API, which will allow better integration between
    your external applications, and your Databricks cloud instance.
  prefs: []
  type: TYPE_NORMAL
- en: REST interface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Databricks provides a REST interface for Spark cluster-based manipulation.
    It allows for cluster management, library management, command execution, and the
    execution of contexts. To be able to access the REST API, the port `34563` must
    be accessible for your instance in the AWS EC2-based Databricks cloud. The following
    Telnet command shows an attempt to access the port `34563` of my Databricks cloud
    instance. Note that the Telnet attempt has been successful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: If you do not receive a Telnet session, then contact Databricks via `<[help@databricks.com](mailto:help@databricks.com)>`.
    The next sections provide examples of REST interface access to your instance on
    the Databricks cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to use the interface, I needed to whitelist the IP address that I use
    to access my Databricks cluster instance. This is the IP address of the machine
    from which I will be running the REST API commands. By whitelisting the IP addresses,
    Databricks can ensure that a secure list of users access each Databricks cloud
    instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'I contacted Databricks support via the previous help email address, but there
    is also a Whitelist IP Guide, found in the **Workspace** menu in your cloud instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Workspace** | **databricks_guide** | **DevOps Utilitie**s | **Whitelist IP**.'
  prefs: []
  type: TYPE_NORMAL
- en: REST API calls can now be submitted to my Databricks cloud instance, from the
    Linux command line, using the Linux `curl` command. The example general form of
    the `curl` command is shown next using my Databricks cloud instance username,
    password, cloud instance URL, REST API path, and parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Databricks forum, and the previous help email address can be used to gain
    further information. The following sections will provide some REST API worked
    examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Cluster management
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You will still need to create Databricks Spark clusters from your cloud instance
    user interface. The list REST API command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'It needs no parameters. This command will provide a list of your clusters,
    their status, IP addresses, names, and the port numbers that they run on. The
    following output shows that the cluster `semclust1` is in a pending state in the
    process of being created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The same REST API command run when the cluster is available, shows that the
    cluster called `semcust1` is running, and has one worker:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Terminating this cluster, and creating a new one called `semclust` changes
    the results of the REST API call as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The execution context
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With these API calls, you can create, show the status of, or delete an execution
    context. The REST API calls are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`/api/1.0/contexts/create`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`/api/1.0/contexts/status`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`/api/1.0/contexts/destroy`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following REST API call example, submitted via `curl`, a Scala context
    has been created for the cluster `semclust` identified by it's cluster ID.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The result returned is either an error, or a context ID. The following three
    example return values show an error caused by an invalid URL, and two successful
    calls returning context IDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Command execution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'These commands allow you to run a command, list a command status, cancel a
    command, or show the results of a command. The REST API calls are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: /api/1.0/commands/execute
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: /api/1.0/commands/cancel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: /api/1.0/commands/status
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following example shows an SQL statement being run against an existing
    table called `cmap`. The context must exist, and must be of the SQL type. The
    parameters have been passed on to the HTTP GET call via a `–d` option. The parameters
    are language, the cluster ID, the context ID, and the SQL command. The command
    ID is returned as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The REST API also allows for libraries to be uploaded to a cluster and their
    statuses checked. The REST API call paths are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`/api/1.0/libraries/upload`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`/api/1.0/libraries/list`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An example is given next of a library upload to the cluster instance called
    `semclust`. The parameters passed on to the HTTP GET API call via a `–d` option
    are the language, cluster ID, the library name and URI. A successful call results
    in the name and URI of the library, which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Note that this REST API can change by content and version overtime, so check
    in the Databricks forum, and use the previous help email address to check the
    API details with Databricks support. I do think though that, with these simple
    example calls, it is clear that this REST API can be used to integrate Databricks
    with the external systems, and ETL chains. In the next section, I will provide
    an overview of data movement within the Databricks cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Moving data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some of the methods of moving data in and out of Databricks have already been
    explained in [Chapter 8](ch08.html "Chapter 8. Spark Databricks"), *Spark Databricks*
    and [Chapter 9](ch09.html "Chapter 9. Databricks Visualization"), *Databricks
    Visualization*. What I would like to do in this section is provide an overview
    of all of the methods available for moving data. I will examine the options for
    tables, workspaces, jobs, and Spark code.
  prefs: []
  type: TYPE_NORMAL
- en: The table data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The table import functionality for Databricks cloud allows data to be imported
    from an AWS **S3** bucket, from the **Databricks file system** (**DBFS**), via
    JDBC and finally from a local file. This section gives an overview of each type
    of import, starting with **S3**. Importing the table data from AWS **S3** requires
    the AWS Key, the AWS secret key, and the **S3** bucket name. The following screenshot
    shows an example. I have already provided an example of **S3** bucket creation,
    including adding an access policy, so I will not cover it again.
  prefs: []
  type: TYPE_NORMAL
- en: '![The table data](img/B01989_09_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Once the form details are added, you will be able to browse your **S3** bucket
    for a data source. Selecting `DBFS` as a table data source enables your `DBFS`
    folders, and files to be browsed. Once a data source is selected, it can display
    a preview as the following screenshot shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The table data](img/B01989_09_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Selecting `JDBC` as a table data source allows you to specify a remote SQL
    database as a data source. Just add an access **URL**, **Username**, and **Password**.
    Also, add some SQL to define the table, and columns to source. There is also an
    option of adding extra properties to the call via the **Add Property** button,
    as the following screenshot shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The table data](img/B01989_09_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Selecting the **File** option to populate a Databricks cloud instance table,
    from a file, creates a drop down or browse. This upload method was used previously
    to upload CSV-based data into a table. Once the data source is specified, it is
    possible to specify a data separator string or header row, define column names
    or column types and preview the data before creating the table.
  prefs: []
  type: TYPE_NORMAL
- en: '![The table data](img/B01989_09_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Folder import
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'From either a workspace, or a folder drop-down menu, it is possible to import
    an item. The following screenshot shows a compound image from the **Import Item**
    menu option:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Folder import](img/B01989_09_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This creates a file drop or browse window, which when clicked, allows you to
    browse the local server for the items to import. Selecting the `All Supported
    Types` option shows that the items to import can be JAR files, dbc archives, Scala,
    Python, or SQL files.
  prefs: []
  type: TYPE_NORMAL
- en: Library import
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following screenshot shows the **New Library** functionality, from the
    Workspace and folder menu options. This allows an externally created and tested
    library to be loaded to your Databricks cloud instance. The library can be in
    the form of a Java or Scala JAR file, a Python Egg or a Maven coordinate for repository
    access. In the following screenshot, a JAR file is being selected from the local
    server via a browse window. This functionality has been used in this chapter to
    test stream-based Scala programming:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Library import](img/B01989_09_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before summing up this chapter, and the last for cloud-based Apache Spark usage
    in Databricks, I wanted to mention some resources for gaining extra information
    on both, Apache Spark, and Databricks. First, there is the Databricks forum available
    at: [forums.databricks.com/](http://forums.databricks.com/) for questions, and
    answers related to the use of [https://databricks.com/](https://databricks.com/).
    Also, within your Databricks instance, under the Workspace menu option, there
    will be a Databricks guide that contains a lot of useful information. The Apache
    Spark website at [http://spark.apache.org/](http://spark.apache.org/) also contains
    a lot of useful information, as well as module-based API documentation. Finally,
    there is the Spark mailing list, `<[user@spark.apache.org](mailto:user@spark.apache.org)>`,
    which provides a great deal of Spark usage information, and problem solving.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Chapter 8](ch08.html "Chapter 8. Spark Databricks"), *Spark Databricks* and
    [Chapter 9](ch09.html "Chapter 9. Databricks Visualization"), *Databricks Visualization*,
    have provided an introduction to Databricks in terms of cloud installation, and
    the use of Notebooks and folders. Account and cluster management have been examined.
    Also, job creation, the idea of remote library creation, and importing have been
    examined. The functionality of the Databricks `dbutils` package, and the Databricks
    file system was explained in [Chapter 8](ch08.html "Chapter 8. Spark Databricks"),
    *Spark Databricks*. Tables, and an example of data import was also shown so that
    SQL can be run against a dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: The idea of data visualization has been examined, and a variety of graphs have
    also been created. Dashboards have been made to show how easy it is to both, create,
    and share this kind of data presentation. The Databricks REST interface has been
    shown via worked examples, as an aid to using a Databricks cloud instance remotely,
    and integrating it with external systems. Finally, the data and library movement
    options have been examined in terms of workspace, folders, and tables.
  prefs: []
  type: TYPE_NORMAL
- en: You might ask why I have committed two chapters to a cloud-based service such
    as Databricks. The reason is that Databricks seems to be a logical, cloud-based
    progression, from Apache Spark. It is supported by the people who originally developed
    Apache Spark and although in it's infancy as a service and subject to change still
    capable of providing a Spark cloud based production service. This means that a
    company wishing to use a Spark could use Databricks and grow their cloud as demand
    grows and have access to dynamic Spark-based machine learning, graph processing,
    SQL, streaming and visualization functionality.
  prefs: []
  type: TYPE_NORMAL
- en: As ever, these Databricks chapters have just scratched the surface of the functionality
    available. The next step will be to create an AWS and Databricks account yourself,
    and use the information provided here to gain practical experience.
  prefs: []
  type: TYPE_NORMAL
- en: 'As this is the last chapter, I will provide my contact details again. I would
    be interested in the ways that people are using Apache Spark. I would be interested
    in the size of clusters you are creating, and the data that you are processing.
    Are you using Spark as a processing engine? Or are you building systems on top
    of it? You can connect with me at LinkedIn at: [linkedin.com/profile/view?id=73219349](http://linkedin.com/profile/view?id=73219349).'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can contact me via my website at `semtech-solutions.co.nz` or finally,
    by email at: `<[info@semtech-solutions.co.nz](mailto:info@semtech-solutions.co.nz)>`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, I maintain a list of open-source-software-related presentations when
    I have the time. Anyone is free to use, and download them. They are available
    on SlideShare at: [http://www.slideshare.net/mikejf12/presentations](http://www.slideshare.net/mikejf12/presentations).'
  prefs: []
  type: TYPE_NORMAL
- en: If you have any challenging opportunities or problems, please feel free to contact
    me using the previous details.
  prefs: []
  type: TYPE_NORMAL
