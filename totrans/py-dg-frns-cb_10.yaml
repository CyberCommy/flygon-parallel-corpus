- en: Exploring Windows Forensic Artifacts Recipes - Part II
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, the following recipes will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Parsing prefetch files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A series of fortunate events
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indexing internet history
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shadow of a former self
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dissecting the SRUM database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Microsoft Windows is one of the most common operating systems found on machines
    during forensic analysis. This has led to a large effort in the community over
    the past two decades to develop, share, and document artifacts deposited by this
    operating system for use in forensic casework.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we continue to look at various Windows artifacts and how to
    process them using Python. We will leverage the framework we developed in [Chapter
    8](part0241.html#75QNI0-260f9401d2714cb9ab693c4692308abe), *Working with Forensic
    Evidence Container Recipes* to process these artifacts directly from forensic
    acquisitions. We''ll use various `libyal` libraries to handle the underlying processing
    of various files, including `pyevt`, `pyevtx`, `pymsiecf`, `pyvshadow`, and `pyesedb`.
    We''ll also explore how to process prefetch files using `struct` and a file format
    table of offsets and data types of interest. Here''s what we''ll learn to do in
    this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Parsing prefetch files for application execution information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Searching for event logs and extract events to a spreadsheet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting internet history from `index.dat` files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enumerating and creating file listings of volume shadow copies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dissecting the Windows 10 SRUM database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a full listing of `libyal` repositories, visit [https://github.com/libyal](https://github.com/libyal).
    Visit [www.packtpub.com/books/content/support](http://www.packtpub.com/books/content/support)
    to download the code bundle for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Parsing prefetch files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recipe difficulty: Medium'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python version: 2.7'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operating system: Linux'
  prefs: []
  type: TYPE_NORMAL
- en: Prefetch files are a common artifact to rely on for information about application
    execution. While they may not always be present, they are undoubtedly worth reviewing
    in scenarios where they exist. Recall that prefetching can be enabled to various
    degrees or disabled based upon the value of the `PrefetchParameters` subkey in
    the `SYSTEM` hive. This recipe searches for files with the prefetch extension
    (`.pf`) and processes them for valuable application information. We will only
    demonstrate this process for Windows XP prefetch files; however, be aware that
    the underlying process we use is similar to other iterations of Windows.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Because we have decided to build out the Sleuth Kit and its dependencies on
    an Ubuntu environment, we continue development on that operating system for ease
    of use. This script will require the installation, if they are not already present,
    of three additional libraries: `pytsk3`, `pyewf`, and `unicodecsv`. All other
    libraries used in this script are present in Python''s standard library.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to *[Chapter 8](part0241.html#75QNI0-260f9401d2714cb9ab693c4692308abe),
    Working with Forensic Evidence Container Recipes* for a detailed explanation of
    installing the `pytsk3` and `pyewf` modules*.* Because we are developing these
    recipes in Python 2.x, we are likely to encounter Unicode encode and decode errors.
    To account for that, we use the `unicodecsv` library to write all CSV output in
    this chapter. This third-party module takes care of Unicode support, unlike Python
    2.x''s standard `csv` module, and will be put to great use here. As usual, we
    can use `pip` to install `unicodecsv`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In addition to these, we'll continue to use the `pytskutil` module developed
    from [Chapter 8](https://cdp.packtpub.com/python_digital_forensics_cookbook/wp-admin/post.php?post=260&action=edit#post_218),
    *Working with Forensic Evidence Container Recipes**,* to allow interaction with
    forensic acquisitions. This module is largely similar to what we previously wrote,
    with some minor changes to better suit our purposes. You can review the code by
    navigating to the utility directory within the code package.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We process prefetch files following these basic principles:'
  prefs: []
  type: TYPE_NORMAL
- en: Scan for files ending with the `.pf` extension.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Eliminate false positives through signature verification.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Parse the Windows XP prefetch file format.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a spreadsheet of parsed results to the current working directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We import a number of libraries to assist with argument parsing, parsing dates,
    interpreting binary data, writing CSVs, and the custom `pytskutil` module.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This recipe''s command-line handler takes two positional arguments, `EVIDENCE_FILE`
    and `TYPE`, which represent the path to the evidence file and the type of evidence
    file (that is, `raw` or `ewf`). Most of the recipes featured in this chapter will
    only feature the two positional inputs. The output from these recipes will be
    spreadsheets created in the current working directory. This recipe has an optional
    argument, `d`, which specifies the path to scan for prefetch files. By default,
    this is set to the `/Windows/Prefetch` directory, although users can elect to
    scan the entire image or a separate directory if desired. After performing some
    input validation on the evidence file, we supply the `main()` function with the
    three inputs and begin executing the script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In the `main()` function, we first create the `TSKUtil` object, `tsk_util`,
    which represents the `pytsk3` image object. With the `TSKUtil` object, we can
    call a number of helper functions to directly interact with the evidence file.
    We use the `TSKUtil.query_directory()` function to confirm that the specified
    directory exists. If it does, we use the `TSKUtil.recurse_files()` method to recurse
    through the specified directory and identify any file that ends with the `.pf`
    extension. This method returns a list of tuples, where each tuple contains a number
    of potentially useful objects, including the `filename`, path, and object itself.
    If no such files are found, `None` is returned instead.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: If we do find files matching the search criteria, we print a status message
    to the console with the number of files found. Next, we set up the `prefetch_data`
    list, which will be used to store the parsed prefetch data from each valid file.
    As we iterate through each hit in the search, we extract the file object, the
    second index of the tuple, for further processing.
  prefs: []
  type: TYPE_NORMAL
- en: Before we do anything with the file object, we validate the file signature of
    the potential prefetch file with the `check_signature()` method. If the file does
    not match the known prefetch file signature, `None` is returned as the `pf_version`
    variable, preventing further processing from occurring for this particular file.
    Before we delve any further into the actual processing of the file, let's look
    at how this `check_signature()` method functions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `check_signature()` method takes the file object as its input and returns
    either the prefetch version or, if the file is not a valid prefetch file, returns
    `None`. We use `struct` to extract two little-endian `32-bit` integers from the
    first `8` bytes of the potential prefetch file. The first integer represents the
    file version, while the second integer is the file's signature. The file signature
    should be `0x53434341`, whose decimal representation is `1,094,927,187`. We compare
    the value we extracted from the file to that number to determine whether the file
    signatures match. If they do match, we return the prefetch version to the `main()`
    function. The prefetch version tells us what type of prefetch file we are working
    with (Windows XP, 7, 10, and so on). We return this value back to dictate how
    to process the file as prefetch files have changed slightly in different versions
    of Windows. Now, back to the `main()` function!
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about prefetch versions and file formats, visit [http://www.forensicswiki.org/wiki/Windows_Prefetch_File_Format](http://www.forensicswiki.org/wiki/Windows_Prefetch_File_Format).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Back in the `main()` function, we check that the `pf_version` variable is not
    `None`, indicating it was successfully validated. Following that, we extract the
    name of the file to the `pf_name` variable, which is stored at the zero index
    of the tuple. Next, we check which version of prefetch file we are working with.
    A breakdown of prefetch versions and their related operating systems can be viewed
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Prefetch version** | **Windows desktop operating system** |'
  prefs: []
  type: TYPE_TB
- en: '| 17 | Windows XP |'
  prefs: []
  type: TYPE_TB
- en: '| 23 | Windows Vista, Windows 7 |'
  prefs: []
  type: TYPE_TB
- en: '| 26 | Windows 8.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 30 | Windows 10 |'
  prefs: []
  type: TYPE_TB
- en: 'This recipe has only been developed to process Windows XP prefetch files using
    the file format as recorded on the previously referenced forensics wiki page.
    However, there are placeholders to add in the logic to support the other prefetch
    formats. They are largely similar, with the exception of Windows 10, and can be
    parsed by following the same basic methodology used for Windows XP. Windows 10
    prefetch files are MAM compressed and must be decompressed first before they can
    be processed--other than that, they can be handled in a similar manner. For version
    17 (Windows XP format), we call the parsing function, providing the TSK file object
    and name of the prefetch file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We begin processing the Windows XP prefetch file by storing the `create` and
    `modify` timestamps of the file itself into local variables. These `Unix` timestamps
    are converted using the `convertUnix()` method, which we have worked with before.
    Besides `Unix` timestamps, we also encounter `FILETIME` timestamps embedded within
    the prefetch file themselves. Let''s look at these functions briefly to get them
    out of the way before continuing our discussion of the `main()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Both functions rely on the `datetime` module to appropriately convert the timestamps
    into a human-readable format. Both functions check whether the supplied timestamp
    string is equal to `"0"` and return an empty string if that is the case. Otherwise,
    for the `convert_unix()` method, we use the `utcfromtimestamp()` method to convert
    the `Unix` timestamp to a `datetime` object and return that. For the `FILETIME`
    timestamp, we add the number of 100 nanoseconds elapsed since January 1, 1601,
    and return the resulting `datetime` object. With our brief dalliance with time
    complete, let's get back to the `main()` function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have extracted the file metadata, we start using `struct` to extract
    the data embedded within the prefetch file itself. We read in `136` bytes from
    the file using the `pytsk3.read_random()` method and `struct` and unpack that
    data into Python variables. Specifically, in those `136` bytes, we extract five
    `32-bit` integers (`i`), one `64-bit` integer (`q`), and a 60-character string
    (`s`). In parentheses in the preceding sentence are the `struct` format characters
    related to those data types. This can also be seen in the `struct` format string
    `"<i60s32x3iq16xi"`, where the number preceding the `struct` format character
    instructs `struct` how many there are (for example, `60s` tells `struct` to interpret
    the next `60` bytes as a string). Likewise, the `"x"` `struct` format character
    is a null value. If `struct` receives `136` bytes to read, it must also receive
    format characters accounting for each of those `136` bytes. Therefore, we must
    supply these null values to ensure we appropriately account for the data we are
    reading in and ensure we are interpreting the values at the appropriate offsets.
    The `"**<**"` character at the beginning of the string ensures all values are
    interpreted as little-endian.
  prefs: []
  type: TYPE_NORMAL
- en: 'Right, that was maybe a bit much, but we probably all have a better understanding
    of `struct` now. After `struct` interprets the data, it returns a tuple of unpacked
    data types in the order in which they were unpacked. We assign these to a series
    of local variables including the prefetch file size, application name, last executed
    `FILETIME,` and the execution count. The application''s `name` variable, the 60-character
    string we extracted, needs to be UTF-16 decoded, and we need to remove all `x00`
    values padding the string. Notice that one of the values we extracted, `vol_info`,
    is the pointer to where volume information is stored within the prefetch file.
    We extract this information next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Let's look at a simpler example with `struct`. We read `20` bytes, starting
    from the `vol_info` pointer, and extract three `32-bit` integers and one `64-bit`
    integer. These are the volume name offset and length, the volume serial number,
    and the volume creation date. Most forensic programs display the volume serial
    number as two four-character hex values separated by a dash. We do the same by
    converting the integer to hex and removing the prepended `"0x"` value to isolate
    just the eight-character hex value. Next, we append a dash halfway between the
    volume serial number using string slicing and concatenation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we use the volume name offset and length we extracted to pull out
    the volume name. We use string formatting to insert the volume name length in
    the `struct` format string. We must multiply the length by two to extract the
    full string. Similar to the application name, we must decode the string as UTF-16
    and remove any `"/x00"` values present. We append the extracted elements from
    the prefetch file to the list. Notice how we perform a few last-minute operations
    while doing so, including converting two `FILETIME` timestamps and joining the
    prefetch path with the name of the file. Note that if we do not remove the prepended
    `"**/**"` character from the `filename`, the `os.path.join()` method will not
    combine these two strings correctly. Therefore, we use `lstrip()` to remove it
    from the beginning of the string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'As we discussed at the beginning of this recipe, we currently only support
    Windows XP-format prefetch files. We have left placeholders to support the other
    format types. Currently, however, if these formats are encountered, an unsupported
    message is printed to the console and we continue onto the next prefetch file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Recall back to the beginning of this recipe how we checked if the `pf_version`
    variable was `None`. If that is the case, the prefetch file does not pass signature
    verification, and so we print a message to that effect and continue onto the next
    file. Once we have finished processing all prefetch files, we send the list containing
    the parsed data to the `write_output()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The `write_output()` method takes the data list we created and writes that data
    out to a CSV file. We use the `os.getcwd()` method to identify the current working
    directory, where we write the CSV file. After printing a status message to the
    console, we create our CSV file, write the names of our columns, and then use
    the `writerows()` method to write all of the lists of parsed prefetch data within
    the data list.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run this script, we generate a CSV document with the following columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00107.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Scrolling left, we can see the following columns for the same entries (the file
    path column is not shown due to its size).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00108.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This script can be further improved. We have provided one or more recommendations
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: Add support for other Windows prefetch file formats. Starting with Windows 10,
    prefetch files now have MAM compression and must first be decompressed prior to
    parsing the data with `struct`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check out the `libscca` ([https://github.com/libyal/libscca](https://github.com/libyal/libscca))
    library and its Python bindings, `pyscca`, which was developed to process prefetch
    files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A series of fortunate events
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recipe Difficulty: Hard'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python Version: 2.7'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operating System: Linux'
  prefs: []
  type: TYPE_NORMAL
- en: Event logs, if configured appropriately, contain a wealth of information useful
    in any cyber investigation. These logs retain historical user activity information,
    such as logons, RDP access, Microsoft Office file access, system changes, and
    application-specific events. In this recipe, we use the `pyevt` and `pyevtx` libraries
    to process both legacy and current Windows event log formats.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This recipe requires the installation of five third-party modules to function:
    `pytsk3`, `pyewf`, `pyevt`, `pyevtx`, and `unicodecsv`. Refer to [Chapter 8](part0241.html#75QNI0-260f9401d2714cb9ab693c4692308abe),
    *Working with Forensic Evidence Container* *Recipes* for a detailed explanation
    of installing the `pytsk3` and `pyewf` modules. Likewise, refer to the *Getting
    started* section in the *Parsing prefetch files* recipe, for details on installing
    `unicodecsv`. All other libraries used in this script are present in Python''s
    standard library. When it comes to installing the Python bindings of most `libyal`
    libraries, they follow a very similar path.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to the GitHub repository and download the desired release for each
    library. This recipe was developed using the `libevt-alpha-20170120` and `libevtx-alpha-20170122`
    releases of the `pyevt` and `pyevtx` libraries, respectively. Next, once the contents
    of the release are extracted, open a terminal and navigate to the extracted directory
    and execute the following commands for each release:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: To learn more about the `pyevt` library, visit [https://github.com/libyal/libevt](https://github.com/libyal/libevt).
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about the `pyevtx` library, visit [https://github.com/libyal/libevtx](https://github.com/libyal/libevtx).
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we can check the libraries installation by opening a Python interpreter,
    importing `pyevt` and `pyevtx`, and running their respective `get_version()` methods
    to ensure we have the correct release versions.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We extract event logs with these basic steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Search for all event logs matching the input argument.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Eliminate false positives with file signature verification.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Process each event log found with the appropriate library.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Output a spreadsheet of all discovered events to the current working directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We import a number of libraries to assist with argument parsing, writing CSVs,
    processing event logs, and the custom `pytskutil` module.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This recipe''s command-line handler takes three positional arguments, `EVIDENCE_FILE`,
    `TYPE`, and `LOG_NAME`, which represents the path to the evidence file, the type
    of evidence file, and the name of the event log to process. Additionally, the
    user may specify the directory within the image to scan with the `"d"` switch
    and enable fuzzy searching with the `"f"` switch. If the user does not supply
    a directory to scan, the script defaults to the `"/Windows/System32/winevt"` directory.
    The fuzzy search, when comparing file names, will check whether the suppled `LOG_NAME`
    is a substring of the `filename` rather than equal to the filename. This capability
    allows a user to search for a very specific event log or any file with an `.evt`
    or `.evtx` extension, and anything in between. After performing input validation
    checks, we pass the five arguments to the `main()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In the `main()` function, we create our `TSKUtil` object, which we will be interacting
    with to query the existence of the user-supplied path. If the path exists and
    is not `None`, we then check whether fuzzy searching has been enabled. Regardless,
    we call the same function, `recurse_files()`, and pass it the log to search for
    and the directory to scan. If fuzzy searching was enabled, we supply the `recurse_files()`
    method an additional optional argument by setting logic to `"equal"`. Without
    specifying this optional argument, the function will check whether the log is
    a substring of a given file rather than an exact match. We store any resulting
    hits in the `event_log` variable.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: If we do have hits for the log, we set up the `event_data` list, which will
    hold the parsed event log data. Next, we begin iterating through each discovered
    event log. For each hit, we extract its file object, which is the second index
    of the tuple returned by the `recurse_files()` method, and send that to be temporarily
    written to the host filesystem with the `write_file()` method. This will be a
    common practice in further recipes so that these third-party libraries can more
    easily interact with the file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The `write_file()` method is rather simplistic. All it does is open a Python
    `File` object in `"w"` mode with the same name and write the entire contents of
    the input file to the current working directory. We return the name of this output
    file back to the `main()` method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Back in the `main()` method, we use the `pyevt.check_file_signature()` method
    to check whether the file we just cached is a valid `evt` file. If it is, we use
    the `pyevt.open()` method to create our `evt` object. After printing a status
    message to the console, we iterate through all of the records within the event
    log. The record can have a number of strings, and so we iterate through those
    and ensure they are added to the `strings` variable. We then append a number of
    event log attributes to the `event_data` list, including the computer name, the
    SID, the creation and written time, the category, source name, event ID, event
    type, the strings, and the file path.
  prefs: []
  type: TYPE_NORMAL
- en: You may notice the empty string added as the second-to-last item in the list.
    This empty string is there due to a lack of an equivalent counterpart found in
    `.evtx` files and is necessary to maintain proper spacing as the output spreadsheet
    is designed to accommodate both `.evt` and `.evtx` results. That's all we need
    to do to process the legacy event log format. Let's now move on to the scenario
    where the log file is an `.evtx` file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Thankfully, both `pyevt` and `pyevtx` libraries handle similarly. We start
    by validating the file signature of the log search hit using the `pyevtx.check_file_signature()`
    method. As with its `pyevt` counterpart, this method returns a Boolean `True`
    or `False` depending on the results of the file signature check. If the file''s
    signature checks out, we use the `pyevtx.open()` method to create an `evtx` object,
    write a status message to the console, and begin iterating through the records
    present in the event log:'
  prefs: []
  type: TYPE_NORMAL
- en: After storing all strings into the `strings` variable, we append a number of
    event log record attributes to the event log list. These include the computer
    name, SID, written time, event level, source, event ID, strings, any XML strings,
    and the event log path. Note there are a number of empty strings, which are present
    to maintain spacing and fill gaps where an `.evt` equivalent is not fount. For
    example, there is no `creation_time` timestamp as seen in the legacy `.evt` logs,
    and therefore, an empty string replaced it instead.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'If the given log hit from the search cannot be validated as either a `.evt`
    or `.evtx` log, we print a status message to the console, remove the cached file
    with the `os.remove()` method, and continue onto the next hit. Note that we only
    remove cached event logs if they could not be validated. Otherwise, we leave them
    in the current working directory so as to allow the user the opportunity to process
    them further with other tools if desired. After we have finished processing all
    of the event logs, we write the parsed list of lists to a CSV with the `write_output()`
    method. The two remaining `else` statements handle situations where there are
    either no event log hits from our search or the directory we scanned for does
    not exist in the evidence file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The `write_output()` method behaves similarly to that discussed in the previous
    recipe. We create a CSV in the current working directory and write all of the
    parsed results to it using the `writerows()` method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows basic information about events in the specified
    log files:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00109.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The second screenshot shows additional columns for these rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00110.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This script can be further improved. We have provided one or more recommendations
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: Enable loose file support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add an event ID argument to selectively extract events only matching the given
    event ID
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indexing internet history
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recipe Difficulty: Medium'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python Version: 2.7'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operating System: Linux'
  prefs: []
  type: TYPE_NORMAL
- en: Internet history can be invaluable during an investigation. These records can
    give insight into a user's thought process and provide context around other user
    activity occurring on the system. Microsoft has been persistent in getting users
    to use Internet Explorer as their browser of choice. As a result, it is not uncommon
    to see internet history information present in `index.dat` files used by Internet
    Explorer. In this recipe, we scour the evidence file for these `index.dat` files
    and attempt to process them using `pymsiecf`.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This recipe requires the installation of four third-party modules to function:
    `pytsk3`, `pyewf`, `pymsiecf`, and `unicodecsv`. Refer to [Chapter 8](part0241.html#75QNI0-260f9401d2714cb9ab693c4692308abe),
    *Working with Forensic Evidence Container* *Recipes,* for a detailed explanation
    on installing the `pytsk3` and `pyewf` modules. Likewise, refer to the *Getting
    started* section in the *Parsing prefetch files* recipe for details on installing
    `unicodecsv`. All other libraries used in this script are present in Python''s
    standard library'
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to the GitHub repository and download the desired release of the `pymsiecf`
    library. This recipe was developed using the `libmsiecf-alpha-20170116` release.
    Once the contents of the release are extracted, open a terminal and navigate to
    the extracted directory and execute the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: To learn more about the `pymsiecf` library, visit [https://github.com/libyal/libmsiecf](https://github.com/libyal/libmsiecf).
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we can check our library's installation by opening a Python interpreter,
    importing `pymsiecf`, and running the `gpymsiecf.get_version()` method to ensure
    we have the correct release version.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We follow these steps to extract Internet Explorer history:'
  prefs: []
  type: TYPE_NORMAL
- en: Find and verify all `index.dat` files within the image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Process the files for internet history.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Output a spreadsheet of the results to the current working directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We import a number of libraries to assist with argument parsing, writing CSVs,
    processing `index.dat` files, and the custom `pytskutil` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This recipe's command-line handler takes two positional arguments, `EVIDENCE_FILE`
    and `TYPE`, which represent the path to the evidence file and the type of evidence
    file, respectively. Similar to the previous recipe, the optional `d` switch can
    be supplied to specify a directory to scan. Otherwise, the recipe starts scanning
    at the `"/Users"` directory. After performing input validation checks, we pass
    the three arguments to the `main()` function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The `main()` function starts by creating the now-familiar `TSKUtil` object and
    scans the specified directory to confirm it exists within the evidence file. If
    it does exist, we recursively scan from the specified directory for any file that
    is equal to the string `"index.dat"`. These files are returned from the `recurse_files()`
    method as a list of tuples, where each tuple represents a particular file matching
    the search criteria.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'If we do find potential `index.dat` files to process, we print a status message
    to the console and set up a list to retain the parsed results of the said files.
    We begin to iterate through hits; extract the `index.dat` file object, which is
    the second index of the tuple; and write it out to the host filesystem using the
    `write_file()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The `write_file()` method was discussed in more detail in the previous recipe.
    It is identical to what we previously discussed. In essence, this function copies
    out the `index.dat` file in the evidence container to the current working directory
    to allow processing by the third-party module. Once that output is created, we
    return the name of the output file, which in this case is going to always be `index.dat`,
    back to the `main()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Similar to the other `libyal` libraries before, the `pymsiecf` module has a
    built-in method, `check_file_signature()`, which we use to determine if the search
    hit is a valid `index.dat` file. If it is, we use the `pymsiecf.open()` method
    to create an object we can manipulate with the library. We print a status message
    to the console and begin iterating through the items present in the `.dat` file.
    The very first thing we attempt is to access the `data` attribute. This contains
    the bulk of information we will be interested in but is not necessarily always
    available. If the attribute is present, however, and is not `None`, we remove
    an appended `"\x00"` value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'As alluded to before, there are scenarios where there will be no `data` attribute.
    Two examples are the `pymsiecf.redirected` and `pymsiecf.leak` objects. These
    objects, however, still have data that may potentially be relevant. Therefore,
    in the exception, we check if the record is an instance of one of those two objects
    and append what data is available to our list of parsed `index.dat` data. We continue
    on to the next `record` after we have appended this data to our list or if the
    record is not an instance of either of those types, except `AttributeError`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'In most scenarios, the `data` attribute is present and we can extract a number
    of potentially relevant information points from the record. This includes the
    filename, the type, a number of timestamps, the location, number of hits, and
    the data itself. To be clear, the `data` attribute is often a URL of some sort
    recorded as a result of browsing activity on the system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'If the `index.dat` file cannot be validated, we remove the offending cached
    file and continue iterating through all other search results. Likewise, this time
    we elect to remove the `index.dat` cached file regardless of whether it was valid
    or not after we finish processing the final one. Because all of these files will
    have the same name, they will overwrite each other as they are being processed.
    Therefore, it did mot make sense to keep only one in the current working directory
    for further processing. If desired, however, one could do something a bit more
    elaborate and cache each file to the host filesystem while preserving its path.
    The remaining two `else` statements are reserved for situations where no `index.dat`
    files are found and the directory to scan for does not exist in the evidence file,
    respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The `write_output()` method behaves like the other methods of the same name
    in the previous recipes. We create a mildly descriptive output name, create the
    output CSV in the current working directory, and then write the headers and data
    to the file. With that, we have completed this recipe and can now add processed
    `index.dat` files to our toolbox:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'When we execute the script, we can review a spreadsheet containing data such
    as the one shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00111.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'While this report has many columns, the following screenshot shows a snippet
    of a few additional columns for the same rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00112.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This script can be further improved. We have provided one or more recommendations
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: Create summary metrics of available data (most and least popular domain visited,
    average time-frame of internet usage, and so on)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shadow of a former self
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recipe Difficulty: Hard'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python Version: 2.7'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operating System: Linux'
  prefs: []
  type: TYPE_NORMAL
- en: Volume shadow copies can contain data from files that are no longer present
    on the active system. This can give an examiner some historical information about
    how the system changed over time and what files used to exist on the computer.
    In this recipe, we will use the `pvyshadow` library to enumerate and access any
    volume shadow copies present in the forensic image.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This recipe requires the installation of five third-party modules to function:
    `pytsk3`, `pyewf`, `pyvshadow`, `unicodecsv`, and `vss`. Refer to [Chapter 8](part0241.html#75QNI0-260f9401d2714cb9ab693c4692308abe),
    *Working with Forensic Evidence Container* *Recipes* for a detailed explanation
    on installing the `pytsk3` and `pyewf` modules. Likewise, refer to the *Getting
    started* section in the *Parsing prefetch files* recipe for details on installing
    `unicodecsv`. All other libraries used in this script are present in Python''s
    standard library.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to the GitHub repository and download the desired release for the
    `pyvshadow` library. This recipe was developed using the `libvshadow-alpha-20170715`
    release. Once the contents of the release are extracted, open a terminal, navigate
    to the extracted directory, and execute the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Learn more about the `pyvshadow` library at [https://github.com/libyal/libvshadow](https://github.com/libyal/libvshadow).
  prefs: []
  type: TYPE_NORMAL
- en: The `pyvshadow` module is designed to work only with raw images and does not
    support other forensic image types out of the box. As noted in a blog post by
    *David Cowen* at [http://www.hecfblog.com/2015/05/automating-dfir-how-to-series-on_25.html](http://www.hecfblog.com/2015/05/automating-dfir-how-to-series-on_25.html),
    the plaso project has created a helper library, `vss`, that can be integrated
    with `pyvshadow`, which we will use here. The `vss` code can be found in the same
    blog post.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we can check our library's installation by opening a Python interpreter,
    importing `pyvshadow`, and running the `pyvshadow.get_version()` method to ensure
    we have the correct release version.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We access volume shadow copies using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Access the volume of the raw image and identify all NTFS partitions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enumerate over each volume shadow copy found on valid NTFS partitions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a file listing of data within the snapshots.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We import a number of libraries to assist with argument parsing, date parsing,
    writing CSVs, processing volume shadow copies, and the custom `pytskutil` module.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'This recipe''s command-line handler takes two positional arguments: `EVIDENCE_FILE`
    and `OUTPUT_CSV`. These represent the path to the evidence file and the file path
    for the output spreadsheet, respectively. Notice the conspicuous absence of the
    evidence type argument. This script only supports raw image files and does not
    work with `E01s`. To prepare an EWF image for use with the script you may either
    convert it to a raw image or mount it with `ewfmount`, a tool associated with
    `libewf`, and provide the mount point as the input.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: After parsing the input arguments, we separate the directory from the `OUTPUT_CSV`
    input and confirm that it exists or create it if it is not present. We also validate
    the input file path's existence before passing the two positional arguments to
    the `main()` function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The `main()` function calls a few new functions within the `TSKUtil` object
    that we have not explored yet. After we create our `TSKUtil` object, we extract
    its volume using the `return_vol()` method. Interacting with an evidence file's
    volume, as we have seen in previous recipes, is one of the requisite steps before
    we can interact with the filesystem. However, this process has been previously
    performed in the background when necessary. This time, however, we need access
    to the `pytsk3` volume object to iterate through each partition to identify NTFS
    filesystems. The `detect_ntfs()` method returns a Boolean value if the specific
    partition has an NTFS filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: For each NTFS filesystem we encounter, we pass the evidence file, the offset
    of the discovered NTFS partition, and the output CSV file to the `explore_vss()`
    function. If the volume object is `None`, we print a status message to the console
    to remind users that the evidence file must be a physical device image as opposed
    to only a logical image of a specific partition.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The `explore_vss()` method starts by creating a `pyvshadow.volume()` object.
    We use this volume to open the `vss_handle` object created from the `vss.VShadowVolume()`
    method. The `vss.VShadowVolume()` method takes the evidence file and the partition
    offset value and exposes a volume-like object that is compatible with the `pyvshadow`
    library, which does not natively support physical disk images. The `GetVssStoreCount()`
    function returns the number of volume shadow copies found in the evidence.
  prefs: []
  type: TYPE_NORMAL
- en: If there are volume shadows, we open our `vss_handle` object with the `pyvshadow
    vss_volume` and instantiate a list to hold our data. We create a `for` loop to
    iterate through each volume shadow copy present and perform the same series of
    steps. First, we use the `pyvshadow get_store()` method to access the particular
    volume shadow copy of interest. Then, we use the `vss` helper library `VShadowImgInfo`
    to create a `pytsk3` image handle. Lastly, we pass the image handle to the `openVSSFS()`
    method and append the returned data to our list. The `openVSSFS()` method uses
    similar methods as discussed before to create a `pytsk3` filesystem object and
    then recurse through the directories present to return an active file listing.
    After we have performed these steps on all of the volume shadow copies, we pass
    the data and the output CSV file path to our `csvWriter()` method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The `write_csv()` method functions as you would expect it to. It first checks
    if there is any data to write. If there isn't, it prints a status message to the
    console before exiting the script. Alternatively, it creates a CSV file using
    the user-provided input, writes the spreadsheet headers, and iterates through
    each list, calling `writerows()` for each volume shadow copy. To prevent the headers
    from ending up several times in the CSV output, we will check to see if the CSV
    already exists and add new data in for review. This allows us to dump information
    after each volume is processed for volume shadow copies.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'After running this script, we can review the files found within each volume
    shadow copy and learn about the metadata of each item:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00113.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This script can be further improved. We have provided one or more recommendations
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: Add support for logical acquisitions and additional forensic acquisition types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add support to process artifacts found within snapshots using previously written
    recipes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dissecting the SRUM database
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recipe Difficulty: Hard'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python Version: 2.7'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operating System: Linux'
  prefs: []
  type: TYPE_NORMAL
- en: With the major release of popular operating systems, everyone in the cyber community
    gets excited (or worried) about the potential new artifacts and changes to existing
    artifacts. With the advent of Windows 10, we saw a few changes (such as the MAM
    compression of prefetch files) and new artifacts as well. One of these artifacts
    is the **System Resource Usage Monitor** (**SRUM**), which can retain execution
    and network activity for applications. This includes information such as when
    a connection was established by a given application and how many bytes were sent
    and received by this application. Obviously, this can be very useful in a number
    of different scenarios. Imagine having this information on hand with a disgruntled
    employee who uploads many gigabytes of data on their last day using the Dropbox
    desktop application.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we leverage the `pyesedb` library to extract data from the database.
    We will also implement logic to interpret this data as the appropriate type. With
    this accomplished, we will be able to view historical application information
    stored within the `SRUM.dat` file found on Windows 10 machines.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about the SRUM database, visit [https://www.sans.org/summit-archives/file/summit-archive-1492184583.pdf](https://www.sans.org/summit-archives/file/summit-archive-1492184583.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This recipe requires the installation of four third-party modules to function:
    `pytsk3`, `pyewf`, `pyesedb`, and `unicodecsv`. Refer to [Chapter 8](part0241.html#75QNI0-260f9401d2714cb9ab693c4692308abe),
    *Working with Forensic Evidence Container* *Recipes,* for a detailed explanation
    on installing the `pytsk3` and `pyewf` modules. Likewise, refer to the *Getting
    started* section in the *Parsing prefetch files* recipe for details on installing
    `unicodecsv`. All other libraries used in this script are present in Python''s
    standard library.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to the GitHub repository and download the desired release for each
    library. This recipe was developed using the `libesedb-experimental-20170121`
    release. Once the contents of the release are extracted, open a terminal, navigate
    to the extracted directory, and execute the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: To learn more about the `pyesedb` library, visit [**https://github.com/libyal/libesedb**](https://github.com/libyal/libesedb)**.**Lastly,
    we can check our library's installation by opening a Python interpreter, importing
    `pyesedb`, and running the `gpyesedb.get_version()` method to ensure we have the
    correct release version.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We use the following methodology to accomplish our objective:'
  prefs: []
  type: TYPE_NORMAL
- en: Determine if the `SRUDB.dat` file exists and perform a file signature verification.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract tables and table data using `pyesedb.`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Interpret extracted table data as appropriate data types.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create multiple spreadsheets for each table present within the database.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We import a number of libraries to assist with argument parsing, date parsing,
    writing CSVs, processing the ESE database, and the custom `pytskutil` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: This script uses two global variables during its execution. The `TABLE_LOOKUP`
    variable is a lookup table matching various SRUM table names to a more human-friendly
    description. These descriptions were pulled from *Yogesh Khatri's* presentation,
    referenced at the beginning of the recipe. The `APP_ID_LOOKUP` dictionary will
    store data from the SRUM `SruDbIdMapTable` table, which assigns applications to
    an integer value referenced in other tables.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: This recipe's command-line handler takes two positional arguments, `EVIDENCE_FILE`
    and `TYPE`, which represent the evidence file and the type of evidence file, respectively.
    After validating the provided arguments, we pass these two inputs to the `main()`
    method, where the action kicks off.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The `main()` method starts by creating a `TSKUtil` object and creating a variable
    to reference the folder which contains the SRUM database on Windows 10 systems.
    Then, we use the `query_directory()` method to determine if the directory exist.
    If it does, we use the `recurse_files()` method to return the SRUM database from
    the evidence (if present):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'If we do find the SRUM database, we print a status message to the console and
    iterate through each hit. For each hit, we extract the file object stored in the
    second index of the tuple returned by the `recurse_files()` method and use the
    `write_file()` method to cache the file to the host filesystem for further processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: The `write_file()` method, as seen before, simply creates a file of the same
    name on the host filesystem. This method reads the entire contents of the file
    in the evidence container and writes it to the temporary file. After this has
    completed, it returns the name of the file to the parent function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Back in the `main()` method, we use the `pyesedb.check_file_signature()` method
    to validate the file hit before proceeding with any further processing. After
    the file is validated, we use the `pyesedb.open()` method to create the `pyesedb`
    object and print a status message to the console with the number of tables contained
    within the file. Next, we create a `for` loop to iterate through all of the tables
    within the database. Specifically, we look for the `SruDbIdMapTable` as we first
    need to populate the `APP_ID_LOOKUP` dictionary with the integer-to-application
    name pairings before processing any other table.
  prefs: []
  type: TYPE_NORMAL
- en: Once that table is found, we read each record within the table. The integer
    value of interest is stored in the first index while the application name is stored
    in the second index. We use the `get_value_data_as_integer()` method to extract
    and interpret the integer appropriately. Using the `get_value_data()` method instead,
    we can extract the application name from the record and attempt to replace any
    padding bytes from the string. Finally, we store both of these values in the global
    `APP_ID_LOOKUP` dictionary, using the integer as a key and the application name
    as the value.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: After creating the `app lookup` dictionary, we are ready to iterate over each
    table (again) and this time actually extract the data. For each table, we assign
    its name to a local variable and print a status message to the console regarding
    execution progress. Then, within the dictionary that will hold our processed data,
    we create a key using the table's name and a dictionary containing column and
    data lists. The column list represents the actual column names from the table
    itself. These are extracted using list comprehension and then assigned to the
    column's key within our dictionary structure.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: With the columns handle, we turn our attention to the data itself. As we iterate
    through each row in the table, we use the `number_of_values()` method to create
    a loop to iterate through each value in the row. As we do this, we append the
    interpreted value to a list, which itself is later assigned to the data key within
    the dictionary. The SRUM database stores a number of different types of data (`32-bit`
    integers, `64-bit` integers, strings, and so on). The `pyesedb` library does not
    necessarily support each data type present using the various `get_value_as` methods.
    We must interpret the data for ourselves and have created a new function, `convert_data()`,
    to do just that. This function needs the value's raw data, the column name, and
    the column's type (which correlates to the type of data present). Let's focus
    on this method now.
  prefs: []
  type: TYPE_NORMAL
- en: If the search hit fails the file signature verification, we print a status message
    to the console, delete the temporary file, and continue onto the next hit. The
    remaining `else` statements handle scenarios where there are no SRUM databases
    found and where the SRUM database directory does not exist, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: The `convert_data()` method relies on the column type to dictate how the data
    should be interpreted. For the most part, we use `struct` to unpack the data as
    the appropriate data types. This function is one large `if-elif-else` statement.
    In the first scenario, we check if the data is `None`, and if it is, return an
    empty string. In the first `elif` statement, we check if the column name is `"AppId"`;
    if it is, we unpack the `32-bit` integer representing the value from the `SruDbIdMapTable`,
    which correlates to an application name. We return the proper application name
    using the global `APP_ID_LOOKUP` dictionary created previously. Next, we create
    cases for various column values to return the appropriate data types, such as
    `8-bit` unsigned integers, `16-` and `32-bit` signed integers, `32-bit` floats,
    and `64-bit` doubles.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Continuing where the other paragraph left off, we have an `OLE` timestamp when
    the column type is equal to `8`. We must unpack the value as a `64-bit` integer
    and then use the `convert_ole()` method to convert this to a `datetime` object.
    Column types `5`, `9`, `10`, `12`, `13`, and `16` are returned as is without any
    additional processing. Most of the other `elif` statements use different `struct`
    format characters to interpret the data appropriately. Column type `15` can also
    be a timestamp or a `64-bit` integer. Therefore, specific to the SRUM database,
    we check if the column name is either `"EventTimestamp"` or `"ConnectStartTime"`,
    in which case the value is a `FILETIME` timestamp and must be converted. Regardless
    of the column type, suffice to say that it is handled here and returned to the
    `main()` method as the appropriate type.
  prefs: []
  type: TYPE_NORMAL
- en: 'Enough of this; let''s go look at these timestamp conversion methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: To learn more about the ESE database column types, visit [https://github.com/libyal/libesedb/blob/b5abe2d05d5342ae02929c26475774dbb3c3aa5d/include/libesedb/definitions.h.in](https://github.com/libyal/libesedb/blob/b5abe2d05d5342ae02929c26475774dbb3c3aa5d/include/libesedb/definitions.h.in).
  prefs: []
  type: TYPE_NORMAL
- en: The `convert_filetime()` method takes an integer and attempts to convert it
    using the tried-and-true method shown before. We have observed scenarios where
    the input integer can be too large for the `datetime` method and have added some
    error handling for that scenario. Otherwise, this method is similar to what has
    been previously discussed.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: New to any of our recipes is the `convert_ole()` method. The `OLE` timestamp
    format is a floating point number representing the number of days since midnight
    of December 30, 1899\. We take the `64-bit` integer supplied to the function and
    pack and unpack it into the appropriate format required for the date conversion.
    Then, we use the familiar process, with `datetime` specifying our epoch and `timedelta`
    to provide the appropriate offset. If we find this value to be too large, we catch
    the `OverflowError` and return the `64-bit` integer as is.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: To learn more about common timestamp formats (including `ole`), visit [https://blogs.msdn.microsoft.com/oldnewthing/20030905-02/?p=42653](https://blogs.msdn.microsoft.com/oldnewthing/20030905-02/?p=42653).
  prefs: []
  type: TYPE_NORMAL
- en: The `write_output()` method is called for every table in the database. We check
    the dictionary and return the function if there are no results for the given table.
    As long as we do have results, we create an output name to distinguish the SRUM
    table and create it in the current working directory. We then open the spreadsheet,
    create the CSV writer, and then write the columns and data to the spreadsheet
    using the `writerow()` and `writerows()` methods, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the code, we can review the extracted values in the spreadsheets.
    The following two screenshots display the first few values found in our application
    resource usage report:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00114.jpeg)![](../images/00115.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This script can be further improved. We have provided one or more recommendations
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: Further research the file format and extend support for other information of
    interest via this recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check out `srum-dump` by Mark Baggett ([https://github.com/MarkBaggett/srum-dump](https://github.com/MarkBaggett/srum-dump))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whether this was your first time using Python, or you have employed it numerous
    times before, you can see how the correct code can make all the difference in
    your investigative process. Python gives you the ability to effectively sift through
    large datasets and more effectively find that proverbial smoking gun in an investigation.
    As you continue developing, you'll find automation becomes second nature and you're
    many times more productive because of it.
  prefs: []
  type: TYPE_NORMAL
- en: The quote "While we teach, we learn", attributed to Roman philosopher Seneca,
    is fitting here, even if a computer was not originally thought of as the subject
    being taught at the quote's conception. But it is apropos, writing code helps
    refine your knowledge of a given artifact by requiring you to understand its structure
    and content at a deeper level.
  prefs: []
  type: TYPE_NORMAL
- en: 'We hope you have learned a lot and continue to do so. There are a plethora
    of freely available resources worth checking out and open source projects to work
    on to better hone your skills. If there''s one thing you should have learned from
    this book: how to write an amazing CSV writer. But, really, we hope through these
    examples you''ve developed a better feel for when and how to use Python to your
    advantage. Good luck cookin''.'
  prefs: []
  type: TYPE_NORMAL
