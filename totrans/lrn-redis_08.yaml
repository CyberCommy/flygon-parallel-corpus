- en: Chapter 8. Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are reading this, it would mean that you have fair amount of understanding
    of what Redis is and how it can be used in applications for web and business.
    Apart from that, it would be fair to assume that you also have fair amount of
    understanding of the data structures it can hold and how to use them in your application.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will continue ahead and discuss the steps that need to be
    taken for a Redis application to get deployed in a production environment. Well,
    deployment in a production environment is always tricky and calls for a greater
    in-depth understanding of the architecture and the business requirement. Since
    we cannot envisage the business requirements that the applications have to fulfil
    but we can always abstract out the nonfunctional requirements, most of applications
    have and create patterns which can be used by the readers as they see fit.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the most common nonfunctional requirements that come to mind when we
    think or talk about production environments are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Availability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scalability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manageability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the mentioned nonfunctional requirements are always addressed in the way
    we create the blueprint for our deployment architecture. Going forward, we will
    take these nonfunctional requirements and map them to the cluster patterns that
    we will discuss.
  prefs: []
  type: TYPE_NORMAL
- en: Clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A computer cluster consists of a set of loosely or tightly connected computers
    that work together so that, in many respects, they can be viewed as a single system.
    The source of this information is [http://en.wikipedia.org/wiki/Computer_cluster](http://en.wikipedia.org/wiki/Computer_cluster).
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple reasons why we do clustering of systems. Enterprises have
    requirements to grow which have to be matched with cost effectiveness and future
    roadmap of solutions; therefore, it always makes sense to go for clustered solution.
    One big machine to handle all the traffic is always desirable but the problem
    with vertical scalability is the ceiling in compute capability of the chip. Moreover,
    bigger machines always cost more as compared to a group of smaller machines with
    aggregate same compute power. Along with cost effectiveness, the other nonfunctional
    requirements that a cluster can take care of are performance, availability, and
    scalability. However, having a cluster also increases efforts of manageability,
    maintainability, and security.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the traffic that the modern websites are generating, clustering is not
    just a low cost option but the only option left. With that perspective in mind,
    let''s look into various patterns of clustering and see how they fit with Redis.
    The two patterns that can be developed for Redis-based clusters are:'
  prefs: []
  type: TYPE_NORMAL
- en: Master-master
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Master-slave
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cluster pattern – master-master
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This pattern of cluster is created for applications where read and writes are
    very frequent and the state across the nodes needs to be the same at any given
    point in time.
  prefs: []
  type: TYPE_NORMAL
- en: 'From a nonfunctional requirement perspective, following behaviors can be seen
    in a master-master setup:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cluster pattern – master-master](img/1794_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Getters and setters in master–master cluster pattern
  prefs: []
  type: TYPE_NORMAL
- en: Performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The performance for reads and writes are very high in this kind of setup. Since
    the requests are load balanced across the master nodes, the individual load on
    a master reduces, thus resulting in better performance. As Redis inherently does
    not have this capability, this has to be provided outside the box. A write replicator
    and read load balancer kept in front of the master-master cluster will do the
    trick. What we are doing here is that, if there is a write request, the data will
    be written to all the masters, and all the read requests can be split among any
    of the master nodes, since the data in all the master nodes is in a consistent
    state.
  prefs: []
  type: TYPE_NORMAL
- en: Another dimension that we have to keep in mind is that when data quantity is
    very high. In case the data quantity is very high, then we have to create **shards**
    (**nodes**) inside the master node setup. These shards in individual master nodes
    can distribute the data based on the key. Later in the chapter, we will discuss
    the sharding capability in Redis.
  prefs: []
  type: TYPE_NORMAL
- en: '![Performance](img/1794_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Read and write operations in a sharded environment
  prefs: []
  type: TYPE_NORMAL
- en: Availability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The availability of data is high or rather depends upon the number of master
    nodes maintained for replication. The state of the data is same across all the
    master nodes, so even if one of the master nodes goes down, the rest of the master
    nodes can cater to the request. During this condition, the performance of the
    application will dip since the requests have to be shared among the remaining
    master nodes. In case of data being distributed across shards inside the master
    node, if one of the shards were to go down, the request for that shard can be
    catered to by the replica shard in the other master nodes. This will still keep
    the affected master node working (but not to the full extent).
  prefs: []
  type: TYPE_NORMAL
- en: Scalability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The issue of scalability is a bit tricky in this case. While provisioning a
    new master node, following care has to be taken:'
  prefs: []
  type: TYPE_NORMAL
- en: The new master node has to be in the same state as the other master nodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The information for the new master node has to be added in the client API, as
    the client can then take this new master node while governing the data writes
    and data reads.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For these tasks, a period of downtime is required which will impact availability.
    Moreover, data volumes have to be factored in before sizing the master nodes or
    shard nodes in a master node to avoid these scenarios.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manageability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Manageability of this type of cluster pattern requires efforts at both the
    node level and client level. This is because Redis does not provide an in-built
    mechanism for this pattern. Since the responsibility of doing data replication
    and data load is of the client adaptor, so following observations need to be addressed:'
  prefs: []
  type: TYPE_NORMAL
- en: The client adaptor has to factor serviceability in case the node (master or
    shard) goes down.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The client adaptor has to factor in serviceability in case a new master node
    is added.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding a new shard in to an already configured shard ecosystem should be avoided,
    as the sharding technique is based on the unique key generated by the client adaptor.
    This is decided on the basis of shard nodes configured at the beginning of the
    application, and adding a new shard will disturb the already set shards and the
    data inside it. This will render the entire application in an inconsistent state.
    This new shard will have some data replicated from the other shards in the master
    node's ecosystem. So the opted way for doing this would be to introduce consistent
    hashing to generate unique keys for assigning the master nodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Redis, being a very light weight data store, has very little to offer from a
    security perspective. The expectation here is that the Redis nodes will be provisioned
    in a secured environment where the responsibility is outside the box. Nevertheless,
    Redis does provide some form of security in terms of username/password authentication
    to connect to node. This mechanism has its limitations since the password is stored
    in the `Config` file in clear text. Another form of security can be obfuscating
    the commands so that it cannot be called accidently. In the cluster pattern we
    are discussing it has limited use and is more from a program perspective.
  prefs: []
  type: TYPE_NORMAL
- en: Drawbacks of this pattern
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This pattern has a few grey areas which we need to look out for before deciding
    to adopt it. A planned downtime is required for this pattern to work in a production
    environment. This downtime is essential if a node goes down and a new node is
    added to the cluster. This new node has to have the same state as the other replica
    master node. Another thing to look out for is data capacity planning, if underestimated,
    scaling horizontally would be a problem if done in a sharded environment. In the
    next section, we will run an example where we add another node and see a different
    distribution of data which can give us a hint of the problems. Data purging is
    another feature which is not addressed by Redis server as it's meant to hold all
    the data in the memory.
  prefs: []
  type: TYPE_NORMAL
- en: Sharding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sharding is a mechanism of horizontally splitting data and placing it in different
    nodes (machines). Here, each partition of data residing in a different node or
    machine forms a shard. Sharding technique is useful in scaling a data store to
    multiple nodes or machines. Sharding, if done correctly, can improve the performance
    of the system at large.
  prefs: []
  type: TYPE_NORMAL
- en: It can also overcome the need to go for a bigger machine and can get the job
    done with smaller machines. Sharding can provide partial fault tolerance since
    if a node goes down then the request coming to that particular node cannot be
    served unless all the other nodes can cater to the incoming request.
  prefs: []
  type: TYPE_NORMAL
- en: Redis does not provide a direct mechanism to support sharding of data internally,
    so to achieve partitioning of data, techniques have to be applied from the client
    API when splitting the data. Since Redis is a key value data store, a unique ID
    can be generated based on an algorithm which can be mapped to nodes. So if a request
    to read, write, update, or delete comes, the algorithm can generate the same unique
    key or can direct it to the mapped node where the action can take place.
  prefs: []
  type: TYPE_NORMAL
- en: Jedis, the client API we are using in this book, does provide a mechanism to
    shard data based on keys. Let's try out a sample and see the distribution of data
    across the nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start with at least two nodes. The procedure has been discussed in the previous
    chapters. In the current sample, we will be starting one node on port 6379 and
    other on 6380\. The first shard node should look similar to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sharding](img/1794_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot for first shard node
  prefs: []
  type: TYPE_NORMAL
- en: 'The second shard node should look similar to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sharding](img/1794_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot for second shard node
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s open our editor and type in the following program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The response in the console output should be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Observations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following observations can be made about the sample:'
  prefs: []
  type: TYPE_NORMAL
- en: The data distribution is random which basically is dependent upon the hashing
    algorithm used to distribute the program or shards
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiple execution of the same program with result in same result. This signifies
    that hashing algorithm is consistent with the hash it creates for a key.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the key changes then the distribution of the data will be different, since
    a new hash code will be generated for the same given key; hence, a new target
    shard.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Add a new shard without cleaning the other shards:'
  prefs: []
  type: TYPE_NORMAL
- en: Start a new master at 6381:![Observations](img/1794_08_05.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s type a new program wherein the client''s new shard information is added:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The result will be as follows as we can see data from `SHARD_1` and `SHARD_2`
    getting replicated in `SHARD_3`. This *replicated data* is nothing but older data
    in the `SHARD_1` and `SHARD_2` because of the previous executions. This, in production
    environment, can be dangerous as it increases the dead data which cannot be accounted
    for:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Add a new master node for the same set of data and clean all the previous data
    in the `SHARD_1` and `SHARD_2` nodes, and the result will be as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We can see data getting distributed cleanly amongst all the shards with no repetitions
    such as older data is cleaned.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster pattern – master-slave
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This pattern of cluster is created for applications where reads are very frequent
    and writes are less frequent. Another condition necessary for this pattern to
    work is to have a limited size of data, or in other words, data capacity that
    can fit into the hardware provisioned for the master (the same hardware configuration
    is needed for the slaves too). Since the requirement is to cater to frequent reads,
    this pattern also has the capability to scale horizontally. Another point we have
    to keep in mind is that replication in slaves can have a time delay which can
    result in stale data getting served. The business requirement should be okay with
    that scenario.
  prefs: []
  type: TYPE_NORMAL
- en: The solution for this pattern would be to have all the writes done to the master,
    and have the slave cater to all the reads. The reads to slaves need to be load
    balanced so that performance can be met.
  prefs: []
  type: TYPE_NORMAL
- en: Redis provides an in-built capability to have master-slave configuration wherein
    the writes can be done to the master and the reads can be done to the slaves.
  prefs: []
  type: TYPE_NORMAL
- en: '![Cluster pattern – master-slave](img/1794_08_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Getters and setters in master–slave pattern
  prefs: []
  type: TYPE_NORMAL
- en: From a nonfunctional requirement perspective, the behaviors that can be seen
    in a master-slave setup are discussed in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The performance writes are very high in this kind of setup. This is because
    all the writes are happening to a single master node and the frequency of the
    writes is less as mentioned in the assumption. Since the read requests are load
    balanced across the slave nodes, the individual load on a slave reduces, thus
    resulting in better performance. As Redis inherently provides reads from slaves,
    nothing except for load balancing has to be provided outside the box. A read load
    balancer kept in front of the slave nodes will do the trick. What we are doing
    here is that if there is a write request the data will be written to the master
    and all the read requests will be split among all of the slave nodes since the
    data in all the slave nodes is in an eventual consistent state.
  prefs: []
  type: TYPE_NORMAL
- en: The thing to be noted in this case is that due to the time difference between
    master pushing the new updated data and the slave updating it, a scenario can
    be reached where the slave continues to serve stale data.
  prefs: []
  type: TYPE_NORMAL
- en: Availability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Availability in master-slave cluster pattern requires different approaches,
    one for the master nodes and the other for the slave nodes. The easiest part is
    slave availability. When it comes to availability of the slaves, it''s pretty
    easy to handle since there are more slaves as compared to a master, and even if
    one of the slaves goes down, there are other slaves to cater to requests. In case
    of the master, since there is only one master, if that node goes down then we
    have a problem. While the reads will continue to happen unaffected, the writes
    will stop. In order to do away with data losses, there are two things that can
    be done:'
  prefs: []
  type: TYPE_NORMAL
- en: Have a message queue in front of the master node so that even in case the master
    goes down, the write message persists, which can be later written down.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Redis provides a mechanism or an observer called Sentinel which can be used.
    Discussion on Sentinel has been done in some of the upcoming sections.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scalability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The issue of scalability is a bit tricky in this case since we have two types
    of nodes here and both of them solve different kind of purpose. The scalability
    here will not be in terms of distributing the data across but more in terms of
    scaling out for performance. Following are some features:'
  prefs: []
  type: TYPE_NORMAL
- en: Master node has to be sized according to the data capacity that needs to be
    kept in RAM for performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The slave nodes can be attached to the cluster at run time but they eventually
    would come to the same state as master node, and the hardware capability of the
    slave should be on a par with the master
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The new slave node should be registered into the load balancer for the load
    balancer to distribute the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manageability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Manageability of this type of cluster pattern requires little effort at the
    master and slave node level and at the client level. This is because Redis does
    provide an in-built mechanism to support this pattern. Since the responsibility
    of doing data replication and data load is of the slave nodes, so what is left
    is managing the master and the client adaptors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following observations need to be addressed:'
  prefs: []
  type: TYPE_NORMAL
- en: The client adaptor has to factor serviceability in case the slave node goes
    down. The adaptor has to be intelligent enough to avoid the slave node that is
    down.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The client adaptor has to factor in serviceability in case a new slave node
    is added.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The client adaptor has to have a temporary persistence mechanism in case the
    master goes down.![Manageability](img/1794_08_07.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fault tolerance in master node
  prefs: []
  type: TYPE_NORMAL
- en: Security
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Redis, being a very light weight data store, has very little to offer from a
    security perspective. The expectation here is that the Redis nodes will be provisioned
    in a secured environment where the responsibility is outside the box. Nevertheless,
    Redis does provide some form of security in terms of username/password authentication
    to connect to node. This mechanism has its limitations since the password is stored
    in the `Config` file in clear text. Another form of security can be obfuscating
    the commands so that it cannot be called accidently.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the cluster pattern we are discussing it has limited use and is more from
    a program perspective. Another good practice is to have separate APIs so that
    the program doesn''t accidentally write to the slave nodes (though this will result
    in an error). Following are some APIs discussed:'
  prefs: []
  type: TYPE_NORMAL
- en: '**WRITE API**: This component should be with the program interacting with the
    master node since the master can do writes in master-slave'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**READ API**: This component should be with the program interacting with the
    slaves which have to fetch the records'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Drawbacks of this pattern
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This pattern has few grey areas which we need to look out for before deciding
    to adopt it. One of the biggest problems is data size. The capacity sizing should
    be done in accordance to the vertical scaling capability of the master. The same
    hardware capability has to be done for the slaves. Another problem is the latency
    which can happen when the master copies the data to the slaves. This can sometimes
    result in stale data getting served in some conditions. Another area to look for
    is the time taken by the Sentinel to elect a new master if there is a failover
    in the master.
  prefs: []
  type: TYPE_NORMAL
- en: This pattern is best suited for scenarios where Redis is used as caching engine.
    In case it's used as a caching engine, then it's a good practice to evict the
    data after it's reached a certain size. In the section that follows, there are
    the eviction policies which we can use in Redis to manage the data size.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Redis Sentinel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Datastores provide capability in handling faulty scenarios. These capabilities
    are in-built and do not expose themselves in the manner in which they handle fault
    tolerance. Redis, which started with simple key value datastore, has evolved into
    a system which provides an independent node to take care of the fault management.
    This system is called **Sentinel**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea behind Sentinel is that it''s an independent node which keeps a track
    on the master node and the other slave nodes. When a master node goes down, it
    promotes the slave node to become the master. As discussed, in a master-slave
    scenario, master is meant to write and slaves are meant to read, so when a slave
    is promoted to master, it has the capability to read and write. All the other
    slaves will become slave to this new slave turned master. The following figure
    shows how Sentinel works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Configuring Redis Sentinel](img/1794_08_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The working of Sentinel
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s have an example now to demonstrate how Sentinel works as of Redis 2.6
    Version. Sentinel has problems running in Windows machines, so this example is
    best executed out of *NIX machine. The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Start a master node as shown:![Configuring Redis Sentinel](img/1794_08_09.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start a slave as shown. Let's call it slave:![Configuring Redis Sentinel](img/1794_08_10.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's start Sentinel as shown next:![Configuring Redis Sentinel](img/1794_08_11.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s write a program in which we will do the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write to the master
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read from the master
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write to a slave
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stop the master
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read from the master after shutting the master
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read from the slave after shutting the master
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write to the slave
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sentinel configuration
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s type the program:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'You should be able to see the following result for the program you have written:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write to the master:![Configuring Redis Sentinel](img/1794_08_12.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read from the master:![Configuring Redis Sentinel](img/1794_08_13.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write to a slave:![Configuring Redis Sentinel](img/1794_08_14.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stop the master:![Configuring Redis Sentinel](img/1794_08_15.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read from the master after shutting the master:![Configuring Redis Sentinel](img/1794_08_16.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read from the slave after shutting the master:![Configuring Redis Sentinel](img/1794_08_17.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write to the slave:![Configuring Redis Sentinel](img/1794_08_18.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add the following text to the default Sentinel configuration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Let's understand the meaning of the five lines that we have added in the preceding
    code. The default Sentinel will have the information of the master running in
    the default port. If you have started the master in some other host or port, accordingly
    those changes have to be made in the Sentinel file.
  prefs: []
  type: TYPE_NORMAL
- en: '**Sentinel monitor slave2master 127.0.0.1 63682 1**: This gives us the information
    of the host and port of the slave node. Apart from that, `1` indicates the quorum
    agreement between the Sentinels for them to agree upon if the master fails. In
    our case, since we are running only one Sentinel, hence `1` is the value mentioned.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentinel down-after-milliseconds slave2master 10000**: This is the time for
    which the master should not be reachable. The idea is for the Sentinel to keep
    on pinging the master and if the master does not respond or responds with an error,
    than the Sentinel kicks in and starts its activity. If the Sentinel detects the
    master as down then it will mark the node as `SDOWN`. But this alone cannot decide
    if the master is down, there has to be an agreement between all the Sentinels
    to initiate the failover activity. When the agreement has been reached by the
    Sentinels that the master is down, then it''s called `ODOWN` state. Think of this
    as democracy among the Sentinel before a master which is down is ousted and a
    new master is chosen.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentinel failover-timeout slave2master 900000**: This is time specified in
    milliseconds which takes care of the entire time span of the entire failover process.
    When a failover is detected, the Sentinel requests the configuration of the new
    master is written in all the slaves configured.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentinel parallel-syncs slave2master 1**: This configuration indicates the
    number of slaves that are reconfigured simultaneously after a failover event.
    If we serve read queries from the read-only slaves, we will want to keep this
    value low. This is because all the slaves will be unreachable at the time when
    the synchronization is happening.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learnt how to use clustering techniques to maximize performance
    and handle growing datasets. Apart from that, we also had a glimpse of handing
    of data for availability and how well we can do fault handling. Though there are
    techniques which Redis provides but we also saw how we can use other techniques
    if we do not require Sentinel. In the next chapter we will focus on how to maintain
    Redis.
  prefs: []
  type: TYPE_NORMAL
