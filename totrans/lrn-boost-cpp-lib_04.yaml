- en: Chapter 4. Working with Strings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text data is the most important and pervasive form of data that modern applications
    deal with. The ability to process text data efficiently through intuitive abstractions
    is a key marker of effectiveness in dealing with text data. Boost has a number
    of libraries dedicated toward effective text processing that enhance and extend
    the capabilities provided by the C++ Standard Library.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will look at three key Boost libraries for processing text
    data. We will start with the Boost String Algorithms library, a library of general-purpose
    algorithms for text data that provides a host of easy text operations, often missed
    in the Standard Library. We will then look at the Boost Tokenizer library, an
    extensible framework for tokenizing string data based on various criteria. Thereafter,
    we will examine a regular expression library for searching and parsing strings,
    Boost.Regex, which has been included in the C++11 standard as well. The following
    topics appear in the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Text processing with Boost String Algorithms library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Splitting text using the Boost Tokenizer library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regular expressions with Boost.Regex
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter should help you get a good grasp of text processing techniques
    available in the Boost libraries. We do not deal with internationalization issues
    in this book, but most of the concepts discussed in this chapter will apply to
    text in languages with writing systems based on non-Latin character sets.
  prefs: []
  type: TYPE_NORMAL
- en: Text processing with Boost String Algorithms library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Text data is commonly represented as a sequence or *string* of characters laid
    out contiguously in memory and terminated by a special marker (the null terminator).
    While the actual data type used to represent a character can vary case by case,
    the C++ Standard Library abstracts the string concept in the class template `std::basic_string`,
    which takes the character data type as a parameter. The `std::basic_string` template
    takes three type parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: The character type
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some of the intrinsic properties and behaviors of the character type encapsulated
    in a traits class
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An allocator type that is used to allocate the internal data structures for
    `std::basic_string`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The traits and allocator parameters are defaulted, as shown in the following
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The C++03 Standard Library also provides two specializations of `std::basic_string`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::string` for narrow characters (8-bit `char`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`std::wstring` for wide characters (16- or 32-bit `wchar_t`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In C++11, we have two more:'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::u16string` (for `u16char_t`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`std::u32string` (for `u32char_t`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to these classes, plain old C-style strings, which are just arrays
    of `char` or `wchar_t` terminated by a null character, are also quite commonly
    used, especially in legacy C++ code.
  prefs: []
  type: TYPE_NORMAL
- en: There are two major shortcomings in the Standard Library, which makes dealing
    with text data types overly tedious at times. For one, there is only a limited
    set of readily available algorithms that can be applied to `string` and `wstring`.
    Moreover, most of these algorithms are member functions of `std::basic_string`
    and are not applicable to other string representations like character arrays.
    Even the algorithms available as non-member function templates deal in iterators
    rather than containers, making the code tedious and less flexible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider how you would convert a string to its uppercase using the C++ Standard
    library:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 4.1: Changing a string to uppercase using std::transform**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the `std::transform` algorithm to convert a sequence of characters to
    their uppercase forms, using the `toupper` function from the Standard Library
    applied to each character (lines 8-9). The sequence of characters to transform
    is specified by a pair of iterators to the first character of the string `song`
    (`song.begin()`) and one past its last character (`song.end()`)—passed as the
    first two arguments to `std::transform`. The transformed sequence is written back
    in-place starting at `song.begin()`, which is the third argument to `std::transform`.
    You may not see a lot amiss if you have programmed in C++ for a while, but the
    generality of the `transform` function somewhat obscures the expression of intent.
    This is where Boost String Algorithms library helps by providing a slew of useful
    string algorithm function templates that are intuitively named and work effectively,
    sometimes even on different string abstractions. Consider the following alternative
    to the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 4.2: Changing a string to uppercase using boost::to_upper**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'To convert the string `song` to uppercase, you call `boost::to_upper(song)`
    (line 8). We include the header `boost/algorithm/string.hpp` (line 2) to access
    `boost::to_upper`, which is an algorithm function template from Boost String Algorithms
    library. It is named `to_upper`, not `transform`, and takes just one argument
    instead of four and no iterators—what''s not to like? Also, you can run the same
    code on bare arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 4.3: Changing a character array to uppercase using boost::to_upper**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: But iterators let you choose the range you want to transform to uppercase and
    here, we only seem to be able to apply anything to the whole string. Actually,
    that's not a problem either as we shall see.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Boost.Range**'
  prefs: []
  type: TYPE_NORMAL
- en: The algorithms from Boost String Algorithms library actually work on abstractions
    called ranges, not containers or iterators. A **range** is just a sequence of
    elements that can be completely traversed in some order. Loosely speaking, a container
    like `std::string` is a sequence of contiguous single-byte characters and a container
    like `std::list<Foo>` is a sequence of elements of type `Foo`. Thus, they qualify
    as valid ranges.
  prefs: []
  type: TYPE_NORMAL
- en: A simple range can be represented by a pair of iterators—one pointing to the
    first element in the range, and the other pointing to one past the last element
    in the range. A range can represent the entire sequence of elements in a container.
    Generalizing further, a range can be described as a subsequence of a container,
    that is, a subset of the elements in the container with their relative ordering
    preserved. For example, the subsequence of elements of a container with odd-numbered
    indexes is a valid range. A single iterator pair may not be sufficient to represent
    such a range; we need more constructs to represent them.
  prefs: []
  type: TYPE_NORMAL
- en: The Boost.Range library provides the necessary abstractions and functions needed
    to generate and deal with all kinds of ranges. The class template `boost::iterator_range`
    is used to represent different kinds of ranges using a pair of iterators. The
    algorithms in Boost String Algorithms take parameters that are ranges and also
    return them, enabling chaining of calls, something that is not possible with most
    STL algorithms. We will not venture into too many details of Boost.Range in this
    chapter but will develop an intuitive understanding needed to use ranges with
    the String Algorithms library.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to transform the case of only a part of a string, we will need to
    construct a range representing that section. We can use the `boost::iterator_range`
    class template to generate arbitrary ranges. Here is how we do it:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 4.4: Changing a section of a string to uppercase using to_upper**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Specifically, we want to construct the range using two iterators to a string.
    So, the type of the range will be `boost::iterator_range<std::string::iterator>`.
    We create a typedef for this rather long type name (lines 8-9). We wish to change
    the word `"sixties"` in the string `"Green-tinted sixties mind"` to uppercase.
    This word starts at index 13 of the string `song` and is seven characters long.
    So, the iterators that define the range containing `"sixties"` are `song.begin()
    + 13` and `song.begin() + 13 + 7`, that is, `song.begin() + 20`. The actual range
    (`range`) is constructed by passing these two iterators to the function template
    `boost::make_iterator_range` (lines 10-11). We pass this range to the `boost::to_upper`
    algorithm, which changes the case of the substring `"sixties"` (line 12), and
    we assert on the expected change (line 13).
  prefs: []
  type: TYPE_NORMAL
- en: 'This may look like a lot of code but remember that you don''t have to construct
    an explicit range when you apply an algorithm to the whole string or container.
    Also, if you are using C++11, the `auto` keyword can help reduce verbosity; thus
    you can replace the highlighted lines (8-11) like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: You can learn more about the `auto` keyword in [Appendix](apa.html "Appendix A. C++11
    Language Features Emulation"), *C++11 Language Features Emulation*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Constructing iterator ranges from arrays is not all that different either:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 4.5: Changing a section of a char array to uppercase using to_upper**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The range is defined to be of type `boost::iterator_range<char*>`, the type
    of the iterator for the array being `char*` (line 9). Once again, we can use `auto`
    to eliminate all the syntactic pain if we are on C++11\. We create the iterator
    range using the appropriate offsets (8 and 16), bounding the word `"Taliesyn"`
    (lines 10-11) and transform the range using `boost::to_upper` (line 12).
  prefs: []
  type: TYPE_NORMAL
- en: Using Boost String Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we explore the various string algorithms available to us and
    understand the conditions under which they can be applied. Before we look at specific
    algorithms though, we will try to understand the general scheme of things first.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the algorithm `boost::contains`. It checks whether the string passed,
    as its second argument, is a substring of the string passed as its first argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 4.6: Using boost::contains**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The algorithm `boost::contains` should return true because `"linearize"` contains
    the substring `"near"` (line 8). While this call to `boost::contains` returns
    true, had we set `test` to `"Near"` instead of `"near"`, it would return false.
    If we want to check for substrings without caring about the case, we have to use
    `boost::icontains` instead as a drop-in replacement for `boost::contains`. Like
    `boost::contains`, most algorithms from Boost String Algorithms have a case insensitive
    version with an `i-` prefix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike `boost::contains`, some string algorithms generate a modified string
    content based on the string passed to it. For example, `boost::to_lower` converts
    the string content passed to it to lowercase. It does so by changing the string
    in-place thus, modifying its argument. A non-mutating version of the algorithm
    called `boost::to_lower_copy` copies the passed string, transforms the case of
    the copied string, and returns it, without modifying the original string. Such
    non-mutating variants have the `_copy` suffix in their names. Here is a short
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 4.7: Using _copy versions of Boost String Algorithms**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The string `str1` is first copied and converted to lowercase using the non-mutating
    variant `boost::to_lower_copy`, and the result is assigned to `str2` (line 7).
    At this point, `str1` remains unchanged. Next, `str1` is converted to lowercase
    in-place, using `boost::to_lower` (line 9). At this point, both `str1` and `str2`
    have the same content (line 10). In most of what follows, we will work with case-sensitive
    variants and mutating variants where applicable, with the understanding that the
    case-insensitive and non-mutating (copy) versions of the algorithms also exist.
    We now start look at specific algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Find algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are several variants of *find algorithm* available from the Boost String
    Algorithms library, all of which search for a string or pattern in another input
    string. Each algorithm takes the input string and the search string as parameters,
    converts them to ranges, and then performs the search. Each find-variant returns
    the contiguous subsequence in the input, which matches the search string or pattern,
    as a range. An empty range is returned if no match was found.
  prefs: []
  type: TYPE_NORMAL
- en: find_first
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We start by looking at `boost::find_first`, which looks for a string in another
    string:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 4.8: Using boost::find_first**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We have an array of strings we want to search for, called `needles` (line 8).
    We also have a C-style string called `haystack`, in which we want to look for
    the search strings which contains the text we want to search for (line 7). We
    loop through each string in `needles` and call the `boost::find_first` algorithm
    to look for it in `haystack` (line 11). We check whether the search failed to
    find a match (line 13). If a match was found, then we compute the offset in `haystack`
    where the match was found (line 18). The range `ret` defines a range of the input
    string `haystack`; hence, we can always perform offset computations like `ret.begin()
    – haystack`.
  prefs: []
  type: TYPE_NORMAL
- en: The first iteration would be able to find `"little"`, while the second iteration
    would fail to find `"Little"` because `boost::find_first` is case-sensitive. If
    we used `boost::ifind_first` which performs case-insensitive search, then both
    would match.
  prefs: []
  type: TYPE_NORMAL
- en: We use the C++11 `auto` keyword to escape writing an ungainly type for `ret`
    (line 11), but if we had to write, it would be `boost::iterator_range<char*>`.
    Note that we can actually stream the range `ret` returned from the algorithm to
    an output stream (line 22).
  prefs: []
  type: TYPE_NORMAL
- en: 'This example illustrates the technique on C-style character arrays but to apply
    it to `std::string` would require surprisingly little change. If `haystack` was
    a `std::string` instance, then the only change will be in the way we calculate
    offsets (line 18):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Since `haystack` is not a character array but an `std::string`, the iterator
    to its start is obtained via a call to its `begin()` member function.
  prefs: []
  type: TYPE_NORMAL
- en: If we want to find the last instance of the search string in `haystack` instead
    of the first, we can replace `boost::find_first` with `boost::find_last`. If there
    are potentially multiple matching tokens, we may ask for a specific match by index.
    For this, we would need to call `boost::find_nth`, passing it a third argument,
    which would be a zero-based index of the match. We may pass a negative index to
    ask for matches from the end. Thus, passing `-1` would give us the last match,
    `-2` the second-last match, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: find_all
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To find all matching substrings in an input string, we must use `boost::find_all`
    and pass it a sequence container to put all the matched substrings into. Here
    is a short example of how to do it:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 4.9: Using boost::find_all to find all matching substrings**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We first create a typedef `string_range` for the appropriate range type (lines
    8-9). The `boost::find_all` algorithm copies all the matching ranges into the
    vector of ranges, `matches` (line 14). We iterate over the vector `matches` using
    C++11's new **range-based for-loop** syntax (line 15), and print the offsets at
    which each match was found (line 17). The nifty range-based for-loop declares
    a loop variable `match` to iterate over successive elements of the container `matches`.
    Using the `auto` keyword, the type of `match` is automatically deduced based on
    the type of values contained in `matches`. Using a vector of ranges rather than
    a vector of strings, we are able to calculate the exact offsets in `str` at which
    the matches occur.
  prefs: []
  type: TYPE_NORMAL
- en: find_token
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One more interesting find algorithm is the `boost::find_token` algorithm. Using
    this algorithm, we can find substrings whose characters satisfy some predicate
    we specify. We can use a set of predefined predicates or define our own, although
    the latter approach requires a fair bit of work, and we will not attempt it in
    this book. In the next example, we search for hexadecimal numbers with four or
    more digits in a string. This will also illustrate how you can use functions to
    perform repeated searches.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this purpose, we use the `boost::is_xdigit` predicate, which returns true
    if a particular character passed to it is a valid hexadecimal character. Here
    is the sample code:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 4.10: Finding substrings using boost::find_token and predicates**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The string `str` contains an interesting hexadecimal token (`0xbeeffed`). We
    pass `str` to `boost::find_token` along with an instance of the predicate `boost::is_xdigit`,
    which identifies valid hexadecimal digits (line 10). We indicate, using `boost::token_compress_on`,
    that contiguous matching characters should be concatenated (line 11); this option
    is turned off by default. The returned range `token` represents the currently
    matched substring. We loop as long as the returned range `token` is not empty,
    that is, `token.begin() != token.end()` (line 12), and print its contents if it
    is longer than 3 in length (line 13). Note the use of the function `boost::size`
    on `token`. This is one of several functions that can be used to compute properties
    of a range like its beginning and end iterators, size, and so on. Also, note that
    we can directly stream a range object like a token to an `ostream` object, such
    as `std::cout`, to print all the characters in the range (line 14).
  prefs: []
  type: TYPE_NORMAL
- en: In each iteration, we search the remaining string after the match using `find_token`.
    The remaining string is constructed as a range called `remnant` (lines 17-18).
    The beginning of `remnant` is `token.end()`, which is the first position after
    the last matching token. The end of remnant is simply the end of the string `str.end()`.
  prefs: []
  type: TYPE_NORMAL
- en: iter_find
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Iterating through a string and finding all substrings matching some criterion
    is a common enough use case, and Boost provides an easier way to do this. By using
    `boost::iter_find` algorithm, passing it the input string, a finder functor, and
    a sequence container to hold the matched ranges, we can get the matching substrings
    back in the container passed. Here is the above example rewritten using `boost::iter_find`:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 4.11: Using boost::iter_find with boost::token_finder**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The `boost::find_regex` algorithm can search a string for substrings that match
    a regular expression pattern. We will cover this algorithm when we deal with regular
    expressions using Boost.Regex, later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: find
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There is a generic `boost::find` algorithm in terms of which most of the other
    find algorithms are implemented. Using the available finder-functor templates,
    as part of the string algorithms library, or writing our own, we can make the
    generic `boost::find` string algorithm do a variety of search tasks for us. Here
    is an example of using the `boost::last_finder` functor with `boost::find` algorithm
    to find the last matching substring—exactly what `boost::ifind_last` does. The
    `boost::last_finder` functor and others like it take an optional predicate and
    can be used to influence how character comparisons are done. To simulate the case-insensitive
    comparisons that `ifind_last` does, we need to pass a predicate that compares
    two characters in a case-insensitive way. For this, we use the `boost::is_iequal`
    predicate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We call `boost::find` on `haystack` passing it the `boost::last_finder` functor.
    Since we want `last_finder` to perform case insensitive comparisons, we pass it
    an instance of the `boost::is_iequal` predicate. This works like `boost::ifind_last`
    and is essentially the way it is implemented. You can even pass your own predicates
    for character comparisons. Say you received an encoded message, where each character
    is shifted by 4, and it wraps around so that `a` is `e` and `z` is `d`. You can
    use the `equalsShift` functor in the following code to check whether a particular
    real word exists in the encoded text:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 4.12: Using custom predicates with Boost substring finders**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Without decoding the whole string contained in the variable `encoded`, we want
    to find a substring of `encoded` that, when decoded would match the string contained
    in the variable `realWord`. In order to do this, we call `boost::find` with two
    arguments, the encoded input string called `encoded` and a predicate that returns
    `true` only if a matching substring is found (line 17-19).
  prefs: []
  type: TYPE_NORMAL
- en: 'For the predicate, we construct a temporary class of type `boost::first_finder`,
    passing two arguments to its constructor: the word to look for is `realWord` and
    a binary predicate `EqualShift(4)`. The `EqualsShift` functor performs a case-insensitive
    comparison of two characters: one from the encoded input and one from the word
    to look up. It returns true if the first character is an encoding of the second
    character, according to the scheme of shifting by a fixed integer N, as described
    earlier (N=4 in our case).'
  prefs: []
  type: TYPE_NORMAL
- en: find_head and find_tail
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are a few more *find* algorithms like `boost::find_head` and `boost::find_tail`,
    which could well have been named `prefix` and `suffix` for that is exactly what
    they do—carve out a prefix or suffix of a specified length from a string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: You call `find_head` with the input string and an offset. If the offset is a
    positive number `N`, `find_head` returns the first `N` characters in the input
    string or the whole string if `N` is larger than the size of the string. If the
    offset is a negative number `-N`, `find_head` returns the first `size - N` characters,
    where `size` represents the total number of characters in the string `run`.
  prefs: []
  type: TYPE_NORMAL
- en: You call `find_tail` with a string and an integer. When a positive integer `N`
    is passed, `find_tail` returns the last `N` characters of the input string or
    the whole string if `N` is larger than the size of the string. When a negative
    integer `-N` is passed, `find_tail` returns the last `size - N` characters in
    the string, where `size` represents the total number of characters in the string,
    an empty string if `N > size`.
  prefs: []
  type: TYPE_NORMAL
- en: Other algorithms for testing string properties
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There exist several convenience functions, which make certain common operations
    very easy to code. Algorithms like `boost::starts_with` and `boost::ends_with`
    (and their case-insensitive variants), test whether a particular string is a prefix
    or suffix of another. To determine the dictionary order of two strings, you can
    use `boost::lexicographical_compare`. You can check for equality using `boost::equals`,
    and check whether a string is a substring of another using `boost::contains`.
    Corresponding case-insensitive variants exist for each of these functions, and
    the case-sensitive variants take an optional predicate for comparing characters.
    The Boost online documentation provides an adequately detailed listing of these
    functions and their behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Case-conversion and trimming algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Changing the case of a string or some part of it and trimming extra whitespace
    that is preceding or trailing a string are very common tasks, which take a bit
    of effort to be done using only the Standard Library. We have already seen `boost::to_upper`,
    `boost::to_lower`, and their copying versions for performing case changes in action.
    In this section, we will apply these algorithms to more interesting ranges and
    also look at trimming algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Case-conversion algorithms
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'How does one convert alternate characters in a string to uppercase leaving
    the rest untouched? Since the `boost::to_upper` function takes a range, we need
    to somehow generate the range that contains alternate elements from the string.
    The way to do this is to use **range adaptors**. Boost Range library provides
    a number of adaptors that allow the generation of newer patterns of ranges from
    existing ones. The adaptor that we are looking for is the `strided` adaptor that
    allows traversing the range by skipping a fixed number of elements at each step.
    We need to skip just one element per step:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 4.13: Generating non-contiguous ranges with Boost.Range adaptors**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In order to apply the `boost::to_upper` algorithm to the even-indexed characters,
    we first generate the correct range. The pipe operator (`operator |`) is overloaded
    to create an intuitive chaining syntax for adaptors, such as `strided`. Using
    the expression `str | strided(2)`, we are essentially applying the `strided` adaptor
    with an argument of `2` to the string `str` to get a range containing the even-indexed
    elements of `str` (line 11). Note that the `strided` adaptor always starts from
    the first character of the input.
  prefs: []
  type: TYPE_NORMAL
- en: 'The same effect can be achieved by writing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: I prefer the piped notation, as it seems a lot more expressive, especially when
    more adaptors need to be chained. Following the generation of this `range`, we
    apply `to_upper` to it (line 12) and expectedly, the even-index characters of
    `str` are transformed to uppercase (line 13).
  prefs: []
  type: TYPE_NORMAL
- en: If we want to perform the same operation, but on all the odd indexes, there
    is one problem we need to solve. The `strided` adaptor takes the number to skip
    between two elements as an argument but always starts from the first character
    of the input. To start from the element at index 1 instead of 0, we have to take
    a slice of the container starting at the element we intend to start from (index
    1 in this case), and then apply `strided` with an argument of `2`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To take the slice first, we use another adaptor, called `boost::adaptors::sliced`.
    It takes the indexes to the starting location and one past the ending location
    as arguments. In this case, we would like to start from index 1 and slice the
    rest of the container. So, we can write the entire expression like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Chaining adaptors in this way is a powerful way to generate ranges on the fly
    with a very readable syntax. The same techniques apply to C-style character arrays
    also.
  prefs: []
  type: TYPE_NORMAL
- en: Trimming algorithms
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For trimming strings, there are three main algorithms: `boost::trim_left` for
    trimming leading whitespace in a string, `boost::trim_right` for trimming trailing
    whitespace in a string, and `boost::trim` for trimming both. Trimming algorithms
    potentially change the length of the output. Each algorithm has an `_if` variant
    that takes a predicate, which is used to identify what characters to trim. For
    example, if you want to drop only trailing newlines from a string read from the
    console (a frequent chore), you may write an appropriate predicate to identify
    only newlines. Finally, there are copy variants of all these algorithms. If we
    wrote an expanded list of the available algorithms, there would be twelve of them;
    four for `trim_left`: `trim_left`, `trim_left_copy`, `trim_left_if`, and `trim_left_if_copy`;
    and similarly four for `trim_right` and `trim` each. Here is an example of performing
    trims on strings:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 4.14: Using boost::trim and its variants**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'In listing 4.14, we have two strings: `input` with leading and trailing spaces
    (line 12), and `input2` with trailing spaces and a newline at the end (line 13).
    By applying `boost::trim` on the `input`, the leading and trailing spaces are
    trimmed (line 15). If we had applied `boost::trim_right` on `input2`, it would
    have removed all trailing whitespaces, including the spaces and the newline. We
    only wanted to drop the newline, not the spaces; so we wrote a predicate `isNewline`
    to help choose what needs to be trimmed. This technique can be used for non-whitespace
    characters too.'
  prefs: []
  type: TYPE_NORMAL
- en: These functions do not work on C-style arrays and the non-copy versions expect
    a member function called `erase`. They work with the `basic_string` specializations
    in the Standard Library, and other classes that provide an `erase` member function
    with similar interface and semantics.
  prefs: []
  type: TYPE_NORMAL
- en: The replace and erase algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The replace and erase algorithms are handy functions to perform search and replace
    operations on strings. The basic idea is to find one or more matches for a search
    string and replace the matches with a different string. Erase is a special case
    of replace, when we replace the matches with a null string.
  prefs: []
  type: TYPE_NORMAL
- en: 'These operations may change the length of the input when performed in-place
    because the matched content and its replacement may have different lengths. The
    core algorithm in the library is `boost::find_format` in terms of which all other
    algorithms are implemented. The algorithms `boost::replace_first`, `boost::replace_last`,
    `boost::replace_nth`, and `boost::replace_all` respectively replace the first,
    last, nth, or all matching occurrences of a search string in the input with an
    alternative string. The corresponding erase algorithms simply erase the matched
    sections. These algorithms do not work on C-style arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 4.15: Using boost::replace and boost::erase variants**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'In listing 4.15, we first use the `boost::replace_first` algorithm to replace
    the first instance of the string `"Hello"` with `"Hola"` (line 9). Had we used
    `boost::replace_all` instead, both instances of `"Hello"` would be replaced, and
    we would get `"Hola, World! Hola folks!"`. We then call `boost::erase_first` to
    remove the remaining `"Hello"` in the string (line 11). Each of these algorithms
    has a case-insensitive variant, which matches in a case-insensitive way. Predictably,
    they are named with an `i-` prefix: `ireplace_first`, `ierase_first`, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a `_copy` variant of each algorithm returning too, a new string rather
    than changing in place. Here is a short illustration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Note how the `boost::ireplace_last_copy` variant worked here, matching `"hello"`
    in a case-insensitive manner and performing the replacement in a copy of the input.
  prefs: []
  type: TYPE_NORMAL
- en: You can replace or erase a prefix or suffix of a string using `boost::replace_head`
    or `boost::replace_tail` (and their erase variants). The `boost::replace_regex`
    and `boost::replace_regex_all` algorithms take a regular expression for finding
    matches, and replace them with a replacement string. The replacement string may
    contain a special syntax to refer back to parts of the matched string, the details
    of which we will defer till the section on Boost.Regex, later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The split and join algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Boost provides an algorithm called `boost::split`, which is essentially used
    to split an input string into tokens based on some separators. The algorithm is
    passed an input string, a predicate for identifying separators, and a sequence
    container to store the parsed tokens. Here is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 4.16: Splitting a string on simple tokens using boost::split**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The listing 4.16 will list out the four types of dogs that appear in the string
    `dogtypes` separated by commas and spaces (line 9). It uses the `boost::split`
    algorithm to do so. The `dogtypes` string is tokenized using the predicate `boost::is_any_of("
    ,")`, which identifies any space or comma as a separator (line 11).The `boost::token_compress_on`
    option ensures that the `boost::split` algorithm does not return an empty string
    for each adjacent pair of separator characters but clubs them together, treating
    it as a single separator (line 12). If we want to split a string at any punctuation
    mark, we will use `boost::is_punct()` instead of `boost::is_any_of(…)`. However,
    it is a somewhat inflexible scheme of tokenizing with only a limited set of predicates
    available.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you simply want to split a string using another string as a separator, you
    may use `boost::iter_split` instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 4.17: Using boost::iter_split to tokenize strings**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The main difference between `boost::split` and `boost::iter_split` is that in
    the latter, you use a finder to identify a separator, which can thus be a specific
    string. Both `boost::iter_split` and `boost::iter_find` take the same kind of
    arguments and use a finder to search for a matching substring, but `boost::iter_split`
    returns tokens that lie between two matching substrings, while its complement
    `boost::iter_find` returns the matching substring.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the `boost::join` and `boost::join_if` algorithms are pretty useful
    when you are trying to string together a sequence of values with some separator
    between successive values. While `boost::join` concatenates all the values in
    the sequence, `boost::join_if` concatenates only those values from the sequence
    that satisfy a passed predicate. Here is `boost::join` in action taking a vector
    of strings and a separator, and returning the joined string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding example, we see yet another useful C++11 feature in action:
    uniform initialization. We initialize the vector `vec` with a sequence of four
    strings enclosed in braces and separated by a comma. This initialization syntax
    works for all STL containers and can be used with regular classes with specific
    types of constructors. Now, if we wanted to pick and choose which strings were
    concatenated and which were not, we would use `boost::join_if` like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The `fiveOrLessChars` predicate checks whether the string passed to it is of
    length five or less. Thus, the string `"mongrel"` does not feature in the joined
    string as its length is more than five.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting text using the Boost Tokenizer library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `boost::split` algorithm, we saw in the last section, splits a string using
    a predicate and puts the tokens into a sequence container. It requires extra storage
    for storing all the tokens, and the user has limited choices for the tokenizing
    criteria used. Splitting a string into a series of tokens based on various criteria
    is a frequent programming requirement, and the Boost.Tokenizer library provides
    an extensible framework for accomplishing this. Also, this does not require extra
    storage for storing tokens. It provides a generic interface to retrieve successive
    tokens from a string. The criterion to split the string into successive tokens
    is passed as a parameter. The Tokenizer library itself provides a few reusable,
    commonly used tokenizing policies for splitting, but, most importantly, it defines
    an interface using which we can write our own splitting policies. It treats the
    input string like a container of tokens from which successive tokens may be parsed
    out.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizing based on separators
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To begin with, let''s see how we can split a string into its constituent words:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 4.19: Using Boost Tokenizer to tokenize strings into words**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The `boost::tokenizer` class template abstracts the tokenization process. We
    create an instance of the default specialization of `boost::tokenizer`, passing
    it our input string `input` (line 10). Next, using the iterator interface of `boost::tokenizer`,
    we split `input` into successive tokens (lines 12-14). In general, you can customize
    how strings are split by passing appropriate tokenizing policies. As we did not
    pass one explicitly to the `boost::tokenizer` template, the default tokenizing
    policy splits the string using whitespace and punctuation as token delimiters
    or separators. The preceding code will print the following output to the standard
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Thus, it splits not only on spaces but also commas and apostrophes; `"I've"`
    is split into `"I"` and `"ve"` due to the apostrophe.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we wanted to split the input based on spaces and punctuation but not split
    on an apostrophe, we would need to do more. Boost provides a few reusable templates
    for commonly used splitting policies. The `boost::char_delimiter` template splits
    the string using specified characters as delimiters. Here is the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 4.20: Using Boost Tokenizer with boost::char_separator**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: In this case, we first construct the splitting policy `sep` using the `boost::char_separator`
    template (line 10). Since we are splitting text of type `std::string` whose character
    type is `char`, we must pass `char` as argument to `boost::char_separator` to
    specify that the delimiters are of type `char`. We can also write `boost::char_separator<std::string::value_type>`
    instead of `boost::char_separator<char>` to better express the relationship. We
    construct the list of punctuation marks and whitespace characters we would like
    to use as delimiters and pass it as the constructor argument of `sep`. Finally,
    we construct the tokenizer, passing it the input string `input` and the splitting
    policy `sep`. We iterate through the successive tokens using a range-based for-loop,
    which makes for less verbose code than when using a token iterator.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizing records with fields containing metacharacters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `boost::char_delimiter` policy is not the only available splitting policy.
    Consider a comma-separated data format, as shown in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We have one record per line and three fields per record: the name, age, and
    city of residence of a person. We can parse such records with the `boost::char_separator`
    policy, passing it a comma as a separator character. Now, if we want to make the
    format a little richer, we may include full addresses of people instead of their
    current city. But addresses are longer fields, sometimes with embedded commas,
    and such addresses would break the parsing, which is based on using a comma as
    a separator. So, we decide to quote strings that may have embedded commas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Quoting itself may not be enough. Some addresses might have quoted strings,
    and we would like to preserve those. To fix this, we decide on using backslash
    (`\`) as an escape character. Here is a fourth record with quoted strings in the
    address:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The trouble now is that it is no longer possible to parse the preceding records
    using the `boost::char_separator` policy. For such records, we should instead
    use `boost::escaped_list_char`. The `boost::escaped_list_char` policy is tailor-made
    for this kind of use. By default, it uses comma (,) as a field separator, double
    quotes (") as the quoting character, and backslash (\) as the escape character.
    To include commas in fields, quote the fields. To include quotes in the fields,
    escape the embedded quotes. We can now attempt to parse the most complex of the
    four persons'' records, as discussed earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 4.21: Using boost::tokenizer with boost::escaped_list_separator**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: An instance of `boost::tokenizer<boost::escaped_list_separator<char> >` is created
    (line 12) using the typedef (lines 10-11). This is really the only operative change
    to take care of for this new format. The record, hardcoded in the variable `input`,
    needs some extra level of escaping to be made into a valid C++ string literal
    (lines 7-8).
  prefs: []
  type: TYPE_NORMAL
- en: 'If the record had a different set of metacharacters, say hyphen (-) for field
    separator, forward slash (/) for quotes, and tilde (~) for escaping, we would
    need to specify these explicitly, as the default options for `boost::escaped_list_separator<<char>
    >` would no longer work. Consider a person named Alon Ben-Ari, aged 35, who lives
    at 11/5 Zamenhoff St., Tel Aviv. Using the specified quote, field separators,
    and escape characters, this could be represented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The name field has a hyphen in the last name Ben-Ari. As hyphen is also a field
    separator, the name field must be quoted using forward slashes. The address field
    has a forward slash and since a forward slash is the quote character, the address
    field must be escaped with the escape character (~). Now it is our turn to tokenize
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 4.22: Using boost::escaped_list_separator with funky delimiters**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Tokenizing records with fixed-length fields
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One class of data formats that frequently occurs in financial transactions
    and several other domains consists of records at fixed offsets. Consider the following
    record format representing a payment instruction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the record is barely human readable and is meant for consumption only
    by a program. It has fields at fixed offsets whose meanings must be known by the
    parsing program. The individual fields are described here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: In order to parse records like these, we use the `boost::offset_separator` splitting
    policy. This class (note that it isn't a template) takes lengths of successive
    tokens to parse in the form of a pair of iterators, bounding the sequence of lengths.
  prefs: []
  type: TYPE_NORMAL
- en: 'A code example to parse the preceding payment instruction should help illustrate
    the idea:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 4.23: Tokenizing records with fixed-length fields**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: We first define an array containing the lengths of successive fields (line 10),
    and use it to initialize an object `ofs` of type `boost::offset_separator` (line
    12). We could have also used a vector instead of an array and passed its `begin()`
    and `end()` iterators to the `offset_separator` constructor. We then create a
    tokenizer, which tokenizes a string based on offsets specified in `ofs` (lines
    13-14), and print the successive tokens using a range-based for-loop (lines 16-18).
  prefs: []
  type: TYPE_NORMAL
- en: 'This program produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: We see listed on successive lines, we see listed the values of the date, time,
    ID, sender SWIFT bank code (an identifier for the sender bank), receiver SWIFT
    bank code, amount, and currency of the transaction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, what happens if all the fields have been parsed and there is still some
    input left? The default behavior is to start parsing afresh the remaining text,
    applying the length offsets to it from the start. This may make sense for some
    formats and may not make sense for some. If you want to turn this behavior off
    so that the parsing stops once all the length offsets have been used, you should
    pass a third argument to the constructor of `boost::offset_separator`, and its
    value should be `false`, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Here, `lengths` is the array of length offsets and `nfields` is the number of
    fields we expect to parse.
  prefs: []
  type: TYPE_NORMAL
- en: 'Conversely, what happens if the input is shorter than the sum of the lengths?
    The default behavior is to return the last partially parsed field and stop. Suppose
    you have a format in which the payer''s comments are appended to each transaction
    record. A comment is optional and need not be there. If it is there, it may or
    may not have a maximum size limit. The first behavior can be used to parse the
    last comment field by specifying the maximum size, or an arbitrarily large size
    that you don''t expect the comments to reach, and thus leverage the partial parse
    of the last record. Again, if you want to turn this behavior off so that the first
    partial field encountered stops the parsing, you should pass a fourth argument
    of type `bool` to the `boost::offset_separator` constructor and its value should
    be `false`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Writing your own tokenizer functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are many instances when you will need to parse a string according to some
    criteria that are not available in a reusable class or template in Boost. While
    you could use alternative libraries like `boost::split`, you can use the `boost::tokenizer`
    facility by plugging in a custom **token generator**. A token generator class
    encapsulates the tokenizing strategy and is passed as a template argument to `boost::tokenizer`.
  prefs: []
  type: TYPE_NORMAL
- en: 'A token generator can be defined as a functor that conforms to the following
    requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: Is copy-assignable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is copy-constructible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Has an overloaded public function call operator (`operator()`) with the following
    signature:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: This operator is passed two iterators that define a section of a string in which
    it looks for the next token it is passed. If and only if a new token is found,
    it returns true. In such case, it sets its third parameter to the token and its
    first parameter to the first position in the string after the end of the token,
    from where parsing may continue. It returns false if no token is found. We must
    write the logic to identify successive tokens in this function.
  prefs: []
  type: TYPE_NORMAL
- en: Has a public member function `void reset()`. This can be used to clear any member
    variables used to keep parsing state for a string. Then, the same instance of
    the object may be used to parse multiple inputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These functions are called by the `boost::tokenizer` implementation, never directly
    by the programmer.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now write a token generator class to pick from some text, strings that are
    quoted or bracketed. For example, given the string `"I''m taking a train from
    Frankfurt (am Main) to Frankfurt (an der Oder)"`, we want to pick out the tokens
    `"am Main"` and `"an der Oder"`. To simplify our implementation, given strings
    with nested brackets or quotes, only the content of innermost quotes need be retrieved.
    Thus, given the string `"tokenizer<char_separator<char> >"`, it should return
    `"char"`, the innermost bracketed entity. Here is the code for such a class, named
    `qstring_token_generator`:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 4.24a: The qstring_token_generator interface**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The `qstring_token_generator` class has a constructor that takes the necessary
    inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: The start and end marker characters, which are by default both double quotes
    (")
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The escape character, which is by default the backslash (\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Boolean indicating whether to skip empty tokens, which is by default true
    (lines 6-8)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The corresponding private variables for storing these values are defined (lines
    18-21). The class uses two additional state variables to keep track of parsing
    state: the `in_token` variable (line 22) which is true while parsing content inside
    quotes and false otherwise, and the `in_escape` variable (line 23) which is true
    if the current character is part of an escape sequence and false otherwise. Here
    is the implementation of the constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 4.24b: The qstring_token_generator constructor**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Note that `in_token` and `in_escape` are initialized to false. Each time we
    iterate through the successive tokens in the input using the tokenizer interface,
    the tokenizer implementation calls the token generator to parse the input again.
    To start parsing afresh, any internal parsing state must be reset. The `reset`
    function encapsulates these actions and is called by the tokenizer when new token
    iterators are created.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the implementation of the reset function:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 4.24c: The qstring_token_generator reset function**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: The reset function makes sure that the internal variables used to maintain parsing
    state are reset appropriately for the parsing to restart.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the parsing algorithm is implemented in the overloaded function call
    operator member (`operator()`). To parse the string, we look for start and end
    markers to identify the start and end of tokens and count-escaped start and end
    markers as part of the tokens, and handle the case where the start and end markers
    are the same characters. We also handle cases where quoted tokens are nested.
    We will write the algorithms in terms of a few helper private functions in `qstring_token_generator`
    class.
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 4.24d: The parsing algorithm helpers**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: The `start_token` function is meant to be called each time we identify the beginning
    of a new token (line 1). It sets the `in_token` flag to true, increments the iterator
    `next`, and returns its value.
  prefs: []
  type: TYPE_NORMAL
- en: The `end_token` function is meant to be called each time we identify the end
    of a token (line 7). It sets the `in_token` flag to false, increments the iterator
    `next`, and returns the complete token as a string.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now need to write the logic to identify the start and end of tokens and
    call the preceding function appropriately. We do this directly in the overloaded
    `operator()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 4.24e: The parsing algorithm**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: We loop through the successive characters of the input using a while loop (line
    6). For each character, we check whether it is preceded by the escape character
    (line 7), or if it is the start marker (line 10), end marker (line 29), or the
    escape character (line 37).
  prefs: []
  type: TYPE_NORMAL
- en: If an unescaped start marker is found, and we are not already in the middle
    of parsing a token (line 11), then it potentially represents the start of a new
    token. So, we call `start_token`, note the starting position of the token, and
    continue to the next iteration (lines 12-13). But if we are already in the middle
    of parsing a token, and we find the start marker, then there are two possibilities.
    If the start and end markers happen to be the same, then this represents the end
    of the token (line 15). In this case, we call `end_token` to get the complete
    token and return it unless it is empty and `skip_empty_tokens` is set (lines 16-20).
    If start and end markers are not the same, then a second start marker represents
    a nested token. Since we want to only extract the most nested token, we discard
    the previous token and call `start_token` to indicate that we have the start of
    a new token (lines 25-26).
  prefs: []
  type: TYPE_NORMAL
- en: If the end marker is distinct from the start marker, and we find it (line 29),
    then we call `end_token` generating and returning the complete token found, unless
    it is empty and `skip_empty_tokens` is set. Finally, if we find the escape character,
    we set the `in_escape` flag (lines 37-38).
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the `qstring_token_generator` class to tokenize our input string:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 4.25: Extracting bracketed strings using the custom tokenizer**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: The preceding highlighted code shows the key changes in our code. We define
    a `qstring_token_generator` object that takes a left and right quote character
    (in this case, left and right parentheses) and skips empty tokens (line 4). We
    then create a typedef for `boost::tokenizer<qstring_token_generator>` (line 4),
    create a tokenizer of that type to parse input (line 6), and print successive
    tokens (line 10).
  prefs: []
  type: TYPE_NORMAL
- en: Regular expressions using Boost.Regex
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we write a line of code like `boost::find_first("Where have all the flowers
    gone?", "flowers")`, we are asking for the string `"flowers"` (call it the **needle**)
    to be found in the larger string `"Where have all the flowers gone?"` (call it
    the **haystack**). The needle is the pattern; seven specific characters in a particular
    order whose presence must be looked up in the haystack. Sometimes, however, we
    don't know the exact string we are looking for; we only have an abstract idea
    or a pattern in mind. Regular expressions is a powerful language to express this
    abstract pattern.
  prefs: []
  type: TYPE_NORMAL
- en: Regular expression syntax
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Regular expressions are strings that encode a pattern of text using a mix of
    regular characters and some characters with special interpretation, collectively
    called *metacharacters*. The Boost.Regex library provides functions that consume
    regular expression strings and generate the logic to search and verify text conforming
    to particular patterns. For example, to define the pattern, "a followed by zero
    or more b's", we use the regular expression `ab*`. This pattern will match text
    like `a`, `ab`, `abb`, `abbb`, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Atoms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At a very basic level, regular expressions consist of groups of one or more
    characters called **atoms**, each with an associated **quantifier** that trails
    the atom and optionally, **anchors** that define how some text is located relative
    to the surrounding text. The quantifier may be implicit. An atom can be a single
    character (or an escaped metacharacter), a **character class**, a string, or a
    **wildcard**. If it is a string, it must be enclosed in parentheses to indicate
    that it is an atom. A wildcard matches any character (other than a newline) and
    is written using the dot (.) metacharacter.
  prefs: []
  type: TYPE_NORMAL
- en: Quantifiers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A single atom without a trailing quantifier just matches a single occurrence
    of itself. When present, the trailing quantifier determines the minimum and maximum
    allowed occurrences of the preceding atom. The general quantifier looks like `{m,
    M}`, where `m` denotes minimum and `M` denotes maximum occurrence frequency. Omitting
    the maximum as in `{m,}` indicates that the maximum number of times the atom may
    be present is unbounded. One may also use a single number as `{n}` to match a
    fixed number of instances. More often, we use the following shortcut quantifiers:'
  prefs: []
  type: TYPE_NORMAL
- en: '`*`: Equivalent to `{0,}`, called the **Kleene star**. Represents an atom that
    may not occur, or may occur any number of times.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`+`: Equivalent to `{1,}`. Represents an atom that must occur at least once.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`?`: Equivalent to `{0,1}`. Represents an optional atom.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using the above syntax rules, we construct summary examples in the following
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Regular Expression | Atoms | Quantifier | Equivalent quantifier | Matching
    text |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| W | w | None (implicit) | `{1}` | w |'
  prefs: []
  type: TYPE_TB
- en: '| a* | a | * | `{0,}` | (blank), a, aa, aaa, aaaa, … |'
  prefs: []
  type: TYPE_TB
- en: '| (abba)+ | abba | + | `{1,}` | abba, abbaabba, abbaabbaabba, … |'
  prefs: []
  type: TYPE_TB
- en: '| a?b | a, b | ? | `{0,1}` | b, ab |'
  prefs: []
  type: TYPE_TB
- en: '| (ab){2,4} | (ab) | {2,4} | `{2,4}` | abab, ababab, abababab |'
  prefs: []
  type: TYPE_TB
- en: '| .*x | . and x | * and None | `{0,}` and `{1}` | x and any string ending in
    x |'
  prefs: []
  type: TYPE_TB
- en: By default, quantifiers are *greedy* and match as many characters as possible.
    Thus, given the string `"abracadabra"`, the regular expression `"a.*a"` will match
    the entire string instead of the smaller substrings `"abra"`, `"abraca"`, or `"abracada"`,
    all of which also start and end in `'a'`. If we want to match only the smallest
    matching substring, we need to override the greedy semantics. To do this, we put
    the question mark (?) metacharacter after the quantifier `"a.*?a"`.
  prefs: []
  type: TYPE_NORMAL
- en: Character classes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Characters can also be matched against character classes, which are shorthand
    representations of a group of functionally related characters. The following is
    a partial list of predefined character classes in the Boost libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Character class | Short form | Meaning | Complement |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [[:digit:]] | `\d` | Any decimal digit (0-9) | \D |'
  prefs: []
  type: TYPE_TB
- en: '| [[:space:]] | `\s` | Any whitespace character | \S |'
  prefs: []
  type: TYPE_TB
- en: '| [[:word:]] | `\w` | Any word character: letter, number, and underscore |
    \W |'
  prefs: []
  type: TYPE_TB
- en: '| [[:lower:]] | `\l` | Any lowercase character |   |'
  prefs: []
  type: TYPE_TB
- en: '| [[:upper:]] | `\u` | Any uppercase character |   |'
  prefs: []
  type: TYPE_TB
- en: '| [[:punct:]] | None | Any punctuation character |   |'
  prefs: []
  type: TYPE_TB
- en: For example, `\d` is a character class that matches a single decimal digit.
    Its complement \`D` matches any single character, except decimal digits. `\s`
    matches a whitespace character and `\S` matches a non-whitespace character. Ad
    hoc character classes can be created with square brackets; `[aeiouAEIOU]` matches
    any character that is an English vowel, `[1-5]` matches a digit between 1 and
    5 both inclusive. The expression `[^2-4]` matches any character except 2, 3, and
    4, and the leading caret inside the square brackets having the effect of negating
    the characters following it. We can combine multiple character classes something
    like—[[:digit:][:lower:]]—to indicate the set of lowercase letters and decimal
    digits.
  prefs: []
  type: TYPE_NORMAL
- en: Anchors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Certain metacharacters, referred to as **anchors**, do not match characters
    but can be used to match specific locations in text. For example, a caret (`^`)
    in a regular expression (outside a character class) matches text at the start
    of a line (just after a newline). A dollar(`$`) matches text before the end of
    a line (just before a newline). Also, `\b` represents a word boundary, while `\B`
    matches any location other than a word boundary.
  prefs: []
  type: TYPE_NORMAL
- en: Sub-expressions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In general, each character in a string of characters is interpreted as a distinct
    atom. In order to treat a string of characters as a single atom, we must parenthesize
    it. Parenthesized substrings of a regular expression are called **sub-expressions**.
    A quantifier following a sub-expression applies to the entire sub-expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: The preceding expression represents a number (`[1-9][0-9]*`) followed by zero
    or more words (`\w+`) separated from it and from each other by one or more whitespace
    characters (`\s+`). The second Kleene star applies to the entire sub-expression
    `\s+\w+` due to the parentheses.
  prefs: []
  type: TYPE_NORMAL
- en: Regular expression libraries, including Boost.Regex keep track of substrings
    of a string that match the parenthesized sub-expressions. Matched sub-expressions
    can be referred back from within the regular expression using back-references,
    such as `\1`, `\2`, `\3`, and so on. For example, in the previous regular expression,
    the term `\1` matches the leading number, while `\2` matches the last matched
    word with leading spaces. It matches nothing if there are no trailing words. Sub-expressions
    can be nested and are numbered incrementally starting at 1 in the order that their
    left parentheses appear in the string from left to right.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to use sub-expressions to be able to apply quantifiers and anchors
    to groups of characters, but do not need to capture them for later reference,
    you can use **non-capturing sub-expressions** of the form `(?:expr)`, where the
    leading metacharacter sequence `?:` inside the parentheses indicates that it is
    a non-capturing sub-expression, and `expr` is some valid regular expression. This
    will treat expr as an atom, but will not capture it. Sub-expressions without the
    leading `?:` inside parentheses are thus called **capture groups** or **capturing
    sub-expressions**.
  prefs: []
  type: TYPE_NORMAL
- en: Disjunctions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can create a regular expression that is a logical-or of one or more regular
    expressions. To do this, you use the |**disjunction operator**. For example, to
    match a word that contains a mix of lowercase and uppercase characters, you can
    use the expression `(\l|\u)+`.
  prefs: []
  type: TYPE_NORMAL
- en: You can use the disjunction operator to combine regular expressions and form
    more complex expressions. For example, to match either a word containing upper
    or lowercase characters, or a positive integer, we can use the expression `(\l|\u)+|\d+`.
  prefs: []
  type: TYPE_NORMAL
- en: Using Boost.Regex to parse regular expressions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Regular expressions are a rich topic that we have barely scratched the surface
    of in the preceding paragraphs. But this basic familiarity is sufficient for us
    to start using the Boost.Regex library. The Boost.Regex library was one of the
    libraries that was accepted into the C++ 11 Standard and is now part of the C++
    11 Standard Library, minus its ability to handle Unicode characters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Boost Regular Expressions library is *not* header-only and requires linking
    against the Boost.Regex shared or static library. It is available from the header
    file `boost/regex.hpp`. On my Linux desktop with Boost libraries installed via
    the native package manager, I use the following command line to build regex programs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'On Linux systems, where Boost has been installed from source, the header files
    could be under a nonstandard location like `/opt/boost/include` and libraries
    under `/opt/boost/lib`. On such systems, I have to use the following command line
    to build my programs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: The `-Wl`, `-rpath`, `/opt/boost/lib` directive tells the linker to hard-code
    the path from where shared libraries, like `libboost_regex-mt`, are loaded, and
    helps our program to run without additional settings. On Windows using Visual
    Studio, linking is automatic.
  prefs: []
  type: TYPE_NORMAL
- en: It uses the `boost::basic_regex` template to model regular expressions and provides
    its specializations `boost::regex` for type `char` and `boost::wregex` for type
    `wchar_t` as typedefs. Using this library, we can check whether a string conforms
    to a pattern or contains a substring conforming to a pattern, extract all substrings
    of a string conforming to a pattern, replace a substring matching a pattern with
    another formatted string, and split a string based on a matching expression to
    name the few most commonly used operations.
  prefs: []
  type: TYPE_NORMAL
- en: Matching text
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Consider the string `"Alaska area"`. We want to match this against the regular
    expression `a.*a` to see whether the string fits the pattern. To do this, we need
    to call the `boost::regex_match` function, which returns a Boolean true to indicate
    a successful match and false otherwise. Here is the code for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 4.26: Matching a string with a regular expression**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The regular expression `"a.*a"` is encapsulated in an instance of `boost::regex`.
    When we match the string against this expression, the match fails (line 8) because
    the string starts with an uppercase `''A''`, while the regular expression expects
    a lowercase `''a''` at the start. We could have asked for a case insensitive regular
    expression by constructing and passing `boost::regex::icase` as a flag to the
    `boost::regex` constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Note that we called a different overload of `boost::regex_match`, which takes
    two iterators to a `std::string` (line 8) just to illustrate an alternative signature.
    You can also call `boost::regex_match` with a `const char*` or a `std::string`
    like in listing 4.25\. The outcome of the function is not dependent on the variant.
  prefs: []
  type: TYPE_NORMAL
- en: Searching text
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If we want to search for substrings of a string that matches a particular regular
    expression, we should use the `boost::regex_search` function instead of `boost::regex_match`.
    Consider the string `"An array of papers from the academia on Alaska area''s fauna"`.
    We want to find all substrings that are part of the same word in this phrase and
    start and end with `''a''`. The regular expression to use would be `a\w*a`. Let
    us see how we can do this using `boost::regex_search`:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 4.27: Searching for substrings matching a regular expression**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'This prints the following lines, each with a word or part of the word that
    begins and ends in `''a''`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: In the code example, we construct the string (line 6), the regular expression
    (line 8), and an instance of `boost::smatch` (line 9), which is a specialization
    of the template `boost::match_results` to be used when the input is of type `std::string`.
    We search for successive matching substrings in a loop, calling `boost::regex_search`.
    We pass to `boost::regex_search` two iterators to the input string, the `smatch`
    instance called `matches`, and the regular expression `r2` (line 13). You must
    pass `const` iterators to `boost::regex_search` (lines 10, 11), or the compilation
    will fail to resolve the function call with a ton of gratuitous messages.
  prefs: []
  type: TYPE_NORMAL
- en: The object `matches` of type `boost::smatch` stores information about the substring
    that matches a regular expression after a call to `regex_search`. Its `str` member
    returns the substring that was matched by the regular expression. `boost::smatch`
    is a sequence collection of `boost::ssub_match` objects. When a regular expression
    matches a substring, the pair of iterators to the start and one part to the end
    of that substring is stored in an object of type `boost::ssub_match`. This is
    stored at index 0 of `matches` and accessed as `matches[0]`. The members `first`
    and `second` of `ssub_match` are iterators to the start of the match (line 15)
    and one past the end of the match. The member function `length()` returns the
    length of the match (line 16). At the end of each iteration, we set the `start`
    iterator to the first location past the end of the last match (line 17) to begin
    looking for the next match. The `boost::ssub_match` is a specialization of the
    template `boost::sub_match` to be used when the input string is of type `std::string`.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that, for each match, we want to extract what lies between the two a's
    at the two ends. To do this, we can use capturing sub-expressions. The regular
    expression would be modified slightly to `a(\\w*)a`. To access what matches the
    parenthesized sub-expression, we again use the `boost::smatch` object. An additional
    `boost::ssub_match` object is constructed for each such sub-expression in the
    regular expression and added to successive indexes of the `boost::smatch` object
    passed. If the sub-expression matched anything in the string, then the start and
    end of the substring matching that sub-expression are stored in the `ssub_match`
    object.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is how we would use it with the modified regular expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 4.28: Parsing matching substrings and sub-expressions**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: In the inner loop (line 18), we iterate through all sub-expressions and for
    the ones that match any substring (line 19), we print that matching substring
    using the `str` member function of `boost::ssub_match` (line 20), the offset of
    the substring (line 21), and its length (line 22). The `prefix` and `suffix` methods
    of the `matches` object return respectively, the parts preceding and following
    the matched substring as `boost::ssub_match` objects (lines 15, 16).
  prefs: []
  type: TYPE_NORMAL
- en: The `boost::match_results` and `boost::sub_match` templates have different available
    specializations appropriate for different types of inputs, like an array of narrow
    or wide characters, or a specialization of `std::basic_string` (`std::string`
    or `std::wstring`).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table summarizes these specializations:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Input type | std::match_results specialization | std::sub_match specialization
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `std::string` | `std::smatch` | `std::ssub_match` |'
  prefs: []
  type: TYPE_TB
- en: '| `std::wstring` | `std::wmatch` | `std::wsub_match` |'
  prefs: []
  type: TYPE_TB
- en: '| `const char*` | `std::cmatch` | `std::csub_match` |'
  prefs: []
  type: TYPE_TB
- en: '| `const wchar_t*` | `std::wcmatch` | `std::wcsub_match` |'
  prefs: []
  type: TYPE_TB
- en: Tokenizing text using regex
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This is a lot of work to parse an input using a regular expression, and there
    ought to be better abstractions available for the application programmer. Indeed,
    this is the kind of job you can simplify using a `boost::regex_iterator` and `boost::regex_token_iterator`.
    Let us suppose we want to pick all words in the string that start and end in `''a''`.
    Here is a relatively painless way to do it:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 4.29: Parsing strings using boost::regex_iterator**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'This program prints the following text to the output, consisting of the three
    words that begin and end in `''a''`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: The `boost::sregex_iterator` is a specialization of the template `boost::regex_iterator`
    to be used when the input string is of type `std::string`. Its instance `rit`
    is initialized with the string iterators, defining the input string and the regular
    expression used to look for successive tokens (line 10). It is then used to iterate
    through successive tokens like any other iterator (line 12).
  prefs: []
  type: TYPE_NORMAL
- en: In the previous example, we didn't deal with sub-expressions. So, let us look
    at an example with sub-expressions. Consider a string `"animal=Llama lives_in=Llama
    and is related_to=vicuna"`. It consists of some key-value pairs separated by the
    equals sign, among other content. If we want to extract all such key-value pairs,
    we can use a regular expression like `\w+=\w+`. We assume that the keys and values
    are single words without embedded punctuation or spaces. If we also want to pick
    out the key and value separately, we can use capture-groups like `(\w+)=(\w+)`
    for sub-expression matching:.
  prefs: []
  type: TYPE_NORMAL
- en: 'By using the `boost::sregex_token_iterator`, we can actually pick out substrings
    matching individual sub-expressions relatively easily. The `boost::sregex_token_iterator`
    is a specialization of the template `boost::regex_token_iterator` for use with
    input string of type `std::string`. It takes the iterators to the input string,
    regular expression, and optional arguments specifying which sub-expressions to
    iterator over. Here is the code to boot:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 4.30: Parsing input strings with boost::regex_token_iterator**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'This code prints the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: You may have noticed that we print the values followed by the keys. We initialize
    a `boost::sregex_token_iterator` using the iterators defining the input string,
    the regular expression, and the array `subindx` specifying the sub-expressions
    we are interested in (line 11). As `subindx` has value `{2, 1}` (line 10), the
    second field is printed before the first. Besides an array, we could have also
    passed a vector of integers identifying the sub-expression indexes, or a single
    integer identifying the index of the only sub-expression we are interested in.
    If we omit this argument, the behavior of `boost::regex_token_iterator` is identical
    to that of `boost::regex_iterator`. The size of the array does not need to be
    passed and is automatically deduced via template argument deduction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some algorithms in the Boost String Algorithms library provide convenient wrappers
    around the functionality in Boost.Regex. The `boost::find_all_regex` algorithm
    takes a sequence container, an input string, and a regular expression, and puts
    all substrings of the input string that match the regular expression into the
    sequence container with a single function call. The `boost::split_regex` container
    splits a string into tokens separated by text that matches some regular expression
    and puts the tokens into a sequence container. Here are both in action; `find_all_regex`
    splitting a sentence into words, and `split_regex` splitting a record with pipe
    character separators into fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 4.31: Using find_all_regex and split_regex**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'This prints the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Note that the first line prints all possible substrings that match the regular
    expression `\w+` (line 11), not just the largest disjoint matching substrings.
    This is because `find_all_regex` finds every matching substring in the input.
  prefs: []
  type: TYPE_NORMAL
- en: Replacing text
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One frequent use of regular expressions is to search for text and replace matching
    text by other text. For example, we may want to scan a particular paragraph for
    possessive phrases (England's Queen, India's culture, people's choice, and so
    on.) and convert them to an alternative form (Queen of England, culture of India,
    choice of people, and so on). The `boost::regex_replace` function template can
    come in handy for the purpose.
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin with, we define the regular expression `\w+''s\s+\w+`. Since we have
    to reorder the phrase, we must capture parts of the match using sub-expressions.
    We use the regular expression `(\w+)''s\s+(\w+)` to match. We can use numbered
    back-references in the replacement string to refer to the submatches, so the replacement
    string is `"\2 of \1"`. We pass these along with the input string to `boost::regex_replace`,
    which returns a string with the matched sections replaced appropriately. Here
    is the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listing 4.32: Finding/Replacing strings with regular expressions**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, `regex_replace` replaces all matching substrings. If we want to
    replace only the first matching substring instead, then we need to pass `boost::regex_constants::
    format_first_only` as a fourth argument to `regex_replace`.'
  prefs: []
  type: TYPE_NORMAL
- en: Self-test questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For multiple choice questions, choose all options that apply:'
  prefs: []
  type: TYPE_NORMAL
- en: How does Boost Range help Boost Algorithms provide a better interface?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. Any character range expressed as a single argument, not iterator pair
  prefs: []
  type: TYPE_NORMAL
- en: b. It is faster than iterator pairs
  prefs: []
  type: TYPE_NORMAL
- en: c. It supports C-style arrays, and is extensible to other abstractions
  prefs: []
  type: TYPE_NORMAL
- en: d. It provides better exception safety
  prefs: []
  type: TYPE_NORMAL
- en: Which algorithm produces the shortest code for searching all substrings matching
    a search string or pattern?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. `boost::find_all`
  prefs: []
  type: TYPE_NORMAL
- en: b. `boost::find_all_regex`
  prefs: []
  type: TYPE_NORMAL
- en: c. `boost::find_first`
  prefs: []
  type: TYPE_NORMAL
- en: d. `boost::regex_iterator`
  prefs: []
  type: TYPE_NORMAL
- en: Which of these are tokenizer functions provided by the Boost Tokenizer library?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. `boost::char_separator`
  prefs: []
  type: TYPE_NORMAL
- en: b. `boost::split`
  prefs: []
  type: TYPE_NORMAL
- en: c. `boost::escaped_list_separator`
  prefs: []
  type: TYPE_NORMAL
- en: d. `boost::tokenizer`
  prefs: []
  type: TYPE_NORMAL
- en: The regular expression `"\ba.*a"` matches which part of the string `"two giant
    anacondas creeping around"`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. `"ant anacondas creeping a"`
  prefs: []
  type: TYPE_NORMAL
- en: b. `"anacondas creeping a"`
  prefs: []
  type: TYPE_NORMAL
- en: c. `"ant anaconda"`
  prefs: []
  type: TYPE_NORMAL
- en: d. `"anaconda"`
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following is true of `boost::smatch`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'a. It is a specialization of `boost:: match_results`'
  prefs: []
  type: TYPE_NORMAL
- en: b. It stores only matched sub-expressions
  prefs: []
  type: TYPE_NORMAL
- en: c. It stores a `boost::ssub_match` object for each sub-expression
  prefs: []
  type: TYPE_NORMAL
- en: d. Its `str` member returns the matched substring
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned the use of miscellaneous functions from the Boost
    String Algorithms library for performing various operations on string data types.
    We then looked at the generic Boost String Tokenizer framework that provides an
    efficient and extensible way to tokenize strings based on criteria that the user
    can define. We finally looked at regular expressions, and the Boost.Regex library
    that provides the ability to match character data against regular expressions,
    search for patterns, tokenize, and replace patterns using regular expressions.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter should have given you a broad perspective of basic text handling
    facilities available from the Boost libraries. Along the way, we also picked up
    some useful techniques from the Boost Range abstraction. In the next chapter,
    we turn our attention to various data structures available from the Boost libraries.
  prefs: []
  type: TYPE_NORMAL
