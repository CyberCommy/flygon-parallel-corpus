- en: Using Spark SQL in Streaming Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will present typical use cases for using Spark SQL in streaming
    applications. Our focus will be on structured streaming using the Dataset/DataFrame
    APIs introduced in Spark 2.0\. Additionally, we will introduce and work with Apache
    Kafka, as it is an integral part of many web-scale streaming application architectures.
    Streaming applications typically involve real-time, context-aware responses to
    incoming data or messages. We will use several examples to illustrate the key
    concepts and techniques to build such applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn these topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What is a streaming data application?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Typical streaming use cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Spark SQL DataFrame/Dataset APIs to build streaming applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Kafka in Structured Streaming applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a receiver for a custom data source
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing streaming data applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Traditional batch applications typically ran for hours, processing all or most
    of the data stored in relational databases. More recently, Hadoop-based systems
    have been used to support MapReduce-based batch jobs to process very large volumes
    of distributed data. In contrast, stream processing occurs on streaming data that
    is continuously generated. Such processing is used in a wide variety of analytics
    applications that compute correlations between events, aggregate values, sample
    incoming data, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Stream processing typically ingests a sequence of data and incrementally computes
    statistics and other functions on a record-by-record/event-by-event basis, or
    over sliding time windows, on the fly.
  prefs: []
  type: TYPE_NORMAL
- en: Increasingly, streaming data applications are applying machine learning algorithms
    and **Complex Event Processing** (**CEP**) algorithms to provide strategic insights
    and the ability to quickly and intelligently react to rapidly changing business
    conditions. Such applications can scale to handle very high volumes of streaming
    data and respond appropriately on a real-time basis. Additionally, many organizations
    are implementing architectures containing both a real-time layer and a batch layer.
    In such implementations, it is very important to maintain a single code base,
    as far as possible, for these two layers (for examples such architectures, refer
    [Chapter 12](part0216.html#6DVPG0-e9cbc07f866e437b8aa14e841622275c), *Spark SQL
    in Large-Scale Application Architectures*). Spark Structured Streaming APIs helps
    us achieve such objectives in a scalable, reliable, and fault-tolerant manner.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the real-world uses cases for streaming applications include processing
    of sensor data in IoT applications, stock market applications such as risk management
    and algorithmic trading, network monitoring, surveillance applications, in-the-moment
    customer engagement in e-commerce applications, fraud detection, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, many platforms have emerged that provide the infrastructure needed
    to build streaming data applications, including Apache Kafka, Apache Spark Streaming,
    Apache Storm, Amazon Kinesis Streams, and others.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will explore stream processing using Apache Spark and Apache
    Kafka. Over the next few sections, we will explore Spark Structured Streaming
    in detail using Spark SQL DataFrame/Dataset APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Building Spark streaming applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will primarily focus on the newly introduced structured
    streaming feature (in Spark 2.0). Structured streaming APIs are GA with Spark
    2.2 and using them is the preferred method for building streaming Spark applications.
    Several updates to Kafka-based processing components including performance improvements
    have also been released in Spark 2.2\. We introduced structured streaming in [Chapter
    1](part0022.html#KVCC0-e9cbc07f866e437b8aa14e841622275c), *Getting Started with
    Spark SQL*. In this chapter, we will get deeper into the topic and present several
    code examples to showcase its capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: As a quick recap, structured streaming provides a fast, scalable, fault-tolerant,
    end-to-end exactly-once stream processing without the developer having to reason
    about the underlying streaming mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: It is built on the Spark SQL engine, and the streaming computations can be expressed
    in the same way batch computations are expressed on static data. It provides several
    data abstractions including Streaming Query, Streaming Source, and Streaming Sink
    to simplify streaming applications without getting into the underlying complexities
    of data streaming. Programming APIs are available in Scala, Java, and Python,
    and you can use the familiar Dataset / DataFrame API to implement your applications.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 1](part0022.html#KVCC0-e9cbc07f866e437b8aa14e841622275c), *Getting
    Started with Spark SQL*, we used the IPinYou Dataset to create a streaming DataFrame
    and then defined a streaming query on it. We showed the results getting updated
    in each time interval. Here, we recreate our streaming DataFrame, and then execute
    various functions on it to showcase the types of computations possible on the
    streaming input data.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we start the Spark shell and import the necessary classes required for
    the hands-on part of this chapter. We will be using file sources to simulate the
    incoming data in most of our examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define the schema for the bid records in our source files, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In the next step, we will define a streaming data source based on the input
    CSV file. We specify the schema defined in the previous step and other required
    parameters (using options). We also limit the number of files processed to one
    per batch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'You can print the schema of the streaming DataFrame as you would in the case
    of a static one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Implementing sliding window-based functionality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this subsection, we will cover the sliding window operation on streaming
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'As the timestamp data is not in the correct format, we will define a new column
    and convert the input timestamp string to the right format and type for our processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will define a streaming query that writes the output to the standard
    output. We will define aggregations over a sliding window where we group the data
    by window and city ID, and compute the count of each group.
  prefs: []
  type: TYPE_NORMAL
- en: For a more detailed description of structured streaming programming, refer to [http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html.](http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we count the number of bids within 10-minute windows updating every five
    minutes, that is, bids received in 10-minutes windows sliding every five minutes.
    The streaming query using a window is as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is written to the standard output because we used a `Console Sink`
    as specified by the `console` keyword in the format parameter. The output contains
    columns for the window, the city ID, and the computed counts, as follows. We see
    two batches as we have placed two files in our input directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00170.jpeg)![](img/00171.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Joining a streaming Dataset with a static Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this subsection, we will give an example of joining a streaming Dataset
    with a static one. We will join Datasets based on the `cityID` to achieve user-friendly
    output that contains the city name instead of the `cityID`. First, we define a
    schema for our city records and create static DataFrame from the CSV file containing
    the city IDs and their corresponding city names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will join the streaming and the static DataFrames, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We will execute our previous streaming query with the column for city names
    specified instead of city IDs in the joined DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are as follows. Here, we see a single batch of output data, as
    we have removed one of the input files from our source directory. For the rest
    of this chapter, we will limit processing to a single input file to conserve space:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00172.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we create a new  DataFrame with a timestamp column and a few selected
    columns from a previously created DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'As we are not computing aggregations, and simply want the streaming bids to
    be appended to the results, we use the `outputMode` "append" instead of "complete",
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00173.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Using the Dataset API in Structured Streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have used untyped APIs with DataFrames. In order to use typed APIs,
    we can switch from using DataFrames to Datasets. Most streaming operations are
    supported by the DataFrame/Dataset APIs; however, a few operations such as multiple
    streaming aggregations and distinct operations not supported, yet. And others
    such as outer JOINs and sorting are conditionally supported.
  prefs: []
  type: TYPE_NORMAL
- en: For a complete list of unsupported and conditionally supported operations, refer
    to [http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html](http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html).
  prefs: []
  type: TYPE_NORMAL
- en: Here, we present a few examples of using typed APIs.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will define a `case` class called `Bid`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We can define a streaming Dataset from a streaming DataFrame using the `case`
    class defined in the previous step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Using output sinks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can direct the streaming output data to various output sinks including File,
    Foreach, Console, and Memory sinks. Typically, the Console and Memory sinks are
    used for debugging purposes. As we have already used Console sink in earlier sections;
    here we will discuss the usage of other sinks in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Foreach Sink for arbitrary computations on output
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you want to perform arbitrary computations on the output, then you can use
    the `Foreach` Sink. For this purpose, you will need to implement the `ForeachWriter`
    interface, as shown. In our example, we simply print the record, but you can also
    perform other computations, as per your requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In the next step, we will implement an example using the `Foreach` sink defined
    in the previous step. Specify the `ForeachWriter` implemented in the preceding
    step as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, the user-agent information is displayed as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00174.gif)'
  prefs: []
  type: TYPE_IMG
- en: Using the Memory Sink to save output to a table
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you want to save the output data as a table, you can use the Memory Sink;
    this can be useful for interactive querying. We define a streaming DataFrame as
    before. However, we specify the format parameter as `memory` and the table name.
    Finally, we execute a SQL query on our table, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00175.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Using the File Sink to save output to a partitioned table
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can also save the output as partitioned tables. For instance, we can partition
    the output by time and store them as Parquet files on HDFS. Here, we show an example
    of storing our output to Parquet files using a File Sink. It is mandatory to specify
    the checkpoint directory location in the given command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'You can check the HDFS filesystem for the output Parquet files and the checkpoint
    files, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00176.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00177.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the next section, we will explore some useful features for managing and monitoring
    streaming queries.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring streaming queries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At this stage, if you list the active streaming queries in the system, you
    should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also monitor and manage a specific streaming query, for example, the
    `windowedCounts` query (a `StreamingQuery` object), as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00178.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'To stop the streaming query execution, you can execute the `stop()` command,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will shift our focus using Kafka as the source of incoming
    data streams in our structured streaming applications.
  prefs: []
  type: TYPE_NORMAL
- en: Using Kafka with Spark Structured Streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Kafka is a distributed streaming platform. It enables you to publish
    and subscribe to data streams, and process and store them as they get produced.
    Kafka’s widespread adoption by the industry for web-scale applications is because
    of its high throughput, low latency, high scalability, high concurrency, reliability,
    and fault-tolerance features.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Kafka concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kafka is typically used to build real-time streaming data pipelines to move
    data between systems, reliably, and also to transform and react to the streams
    of data. Kafka is run as a cluster on one or more servers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the key concepts of Kafka are described here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Topic**: High-level abstraction for a category or stream name to which messages
    are published. A topic can have `0`, `1`, or many consumers who subscribe to the
    messages published to it. Users define a new topic for each new category of messages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Producers**: Clients that publish messages to a topic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consumers**: Clients that consume messages from a topic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Brokers**: One or more servers where message data is replicated and persisted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, the producers and consumers can simultaneously write to and read
    from multiple topics. Each Kafka topic is partitioned and messages written to
    each partition are sequential. The messages in the partitions have an offset that
    uniquely identifies each message within the partition.
  prefs: []
  type: TYPE_NORMAL
- en: The reference site for Apache Kafka installation, tutorials, and examples is
    [https://kafka.apache.org/](https://kafka.apache.org/).
  prefs: []
  type: TYPE_NORMAL
- en: The partitions of a topic are distributed and each Broker handles requests for
    a share of the partitions. Each partition is replicated across a configurable
    number of Brokers. The Kafka cluster retains all published messages for a configurable
    period of time. Apache Kafka uses Apache ZooKeeper as a coordination service for
    its distributed processes.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing ZooKeeper concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ZooKeeper is a distributed, open-source coordination service for distributed
    applications. It relieves the developer from having to implement coordination
    services from scratch. It uses a shared hierarchical namespace to allow distributed
    processes to coordinate with each other and avoid errors related to race conditions
    and deadlocks.
  prefs: []
  type: TYPE_NORMAL
- en: The reference site for Apache ZooKeeper installation and tutorials is [https://zookeeper.apache.org/](https://zookeeper.apache.org/).
  prefs: []
  type: TYPE_NORMAL
- en: ZooKeeper data is kept in memory and, hence, it has very high throughput and
    low latency. It is replicated over a set of hosts to provide high availability.
    ZooKeeper provides a set of guarantees, including sequential consistency and atomicity.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Kafka-Spark integration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We present a simple example here to familiarize you with Kafka-Spark integration.
    The environment for this section uses: Apache Spark 2.1.0 and Apache Kafka 0.10.1.0
    (Download file: `kafka_2.11-0.10.1.0.tgz)` .'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we start a single-node ZooKeeper using the script provided with Apache
    Kafka distribution, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'After the Zookeeper node is up and running, we start the Kafka server using
    the script available in the Apache Kafka distribution, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create a topic called `test`, to which we will send messages for Spark
    streaming to consume. For our simple example, we specify both the replication
    factor and the number of partitions as `1`. We can use the utility script available
    for this purpose, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see the list of topics (including “test”) using this script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we start a command line-based producer to send messages to Kafka, as
    follows. Here, each line is sent as a separate message. You should see each line
    appear in your Spark streaming query (running in a different window) as you type
    it and hit enter (as illustrated):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'In a separate window, start Spark shell with the appropriate Kafka packages
    specified in the command line, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'After Spark shell starts up, we will create a streaming Dataset with the format
    specified as "kafka". In addition, we will also specify the Kafka server and the
    port it’s running on, and explicitly subscribe to the topic we created earlier,
    as follows. The key and value fields are cast to String type to make the output
    human-readable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will start a streaming query that outputs the streaming Dataset to
    the standard output, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following output as you type sentences in the Kafka producer
    window:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00179.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Introducing Kafka-Spark Structured Streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Then, we will provide another example of Kafka-Spark Structured Streaming,
    where we direct the contents of a iPinYou bids file to a producer, as demonstrated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also create a new topic, called `connect-test`, a new streaming Dataset
    that contains the records from the file, and a new streaming query that lists
    them on the screen, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The truncated output is given here. The records are spread across multiple
    batches as they stream in:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00180.gif)'
  prefs: []
  type: TYPE_IMG
- en: In the next section, we will create a receiver for accessing an arbitrary streaming
    data source.
  prefs: []
  type: TYPE_NORMAL
- en: Writing a receiver for a custom data source
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have worked with data sources that have built-in support available
    in Spark. However, Spark Streaming can receive data from any arbitrary source,
    but we will need to implement a receiver for receiving data from the custom data
    source.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will define a custom data source for public APIs available
    from the **Transport for London** (**TfL**) site. This site makes a unified API
    available for each mode of transportation in London. These APIs provide access
    to real-time data, for instance, rail arrivals. The output is available in the
    XML and JSON formats. We will use the APIs for current arrival predictions of
    London underground on a specific line.
  prefs: []
  type: TYPE_NORMAL
- en: The reference site for TfL is [https://tfl.gov.uk](https://tfl.gov.uk); register
    on this site to generate an application key for accessing the APIs.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by extending the abstract class `Receiver` and implementing the
    `onStart()` and `onStop()` methods. In the `onStart()` method, we start the threads
    responsible for receiving the data and in `onStop()`, we stop these threads. The
    `receive` method receives the stream of data using a HTTP client, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The following object creates the `StreamingContext` and starts the application.
    The `awaitTermination()` method ensures that the application runs continuously.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can terminate the application using *Ctrl *+ *C *:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The `sbt` file used for compiling and packing the application is listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the `spark-submit` command to execute our application, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The output from the streaming program is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00181.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced streaming data applications. We provided a few
    examples of using Spark SQL DataFrame/Dataset APIs to build streaming applications.
    Additionally, we showed the use of Kafka in structured streaming applications.
    Finally, we presented an example of creating a receiver for a custom data source.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will shift our focus to using Spark SQL in machine learning
    applications. Specifically, we will explore key concepts of feature engineering
    and machine learning pipelines.
  prefs: []
  type: TYPE_NORMAL
