- en: Designing an ELK Stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Designing an **Elastic Stack** that performs to the required specifications
    needs special attention. Each of the components, **Elasticsearch, Logstash, and
    Kibana** (**ELK**), have specific requirements. Correct sizing is crucial for
    best performance and functionality.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter goes through the design considerations when deploying an Elastic
    Stack, taking into consideration the needs of each of the components as well as
    specific setup details. Throughout this chapter, we will describe how each component
    is affected by different resources, how we can handle resource constraints, and
    how to plan and size for different scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will go through the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch CPU sizing requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How memory sizing affects Elasticsearch performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How data is stored within Elasticsearch and how to size for performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Requirements for Logstash and Kibana
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although the documentation found at [https://www.elastic.co/guide/en/elasticsearch/guide/current/hardware.html](https://www.elastic.co/guide/en/elasticsearch/guide/current/hardware.html) is
    outdated, the hardware requirements can be used as a starting point for CPU sizing.
    For more useful documentation, visit the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Setup guide for indexing speed: **[https://www.elastic.co/guide/en/elasticsearch/reference/current/tune-for-indexing-speed.html](https://www.elastic.co/guide/en/elasticsearch/reference/current/tune-for-indexing-speed.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Changing heap configuration for Elasticsearch: **[https://www.elastic.co/guide/en/elasticsearch/reference/current/heap-size.html](https://www.elastic.co/guide/en/elasticsearch/reference/current/heap-size.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Average system memory latency:** [http://www.crucial.com/usa/en/memory-performance-speed-latency](http://www.crucial.com/usa/en/memory-performance-speed-latency)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Elasticsearch system paths: **[https://www.elastic.co/guide/en/elasticsearch/reference/master/path-settings.html](https://www.elastic.co/guide/en/elasticsearch/reference/master/path-settings.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Logstash persistent queues:** [https://www.elastic.co/guide/en/logstash/current/persistent-queues.html](https://www.elastic.co/guide/en/logstash/current/persistent-queues.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Logstash directory paths:** [https://www.elastic.co/guide/en/logstash/current/dir-layout.html](https://www.elastic.co/guide/en/logstash/current/dir-layout.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elasticsearch CPU requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As with any software, sizing for the right CPU requirements determines the overall
    application performance and processing time. Having the wrong CPU configuration
    can lead to an unusable application due to the processing taking too long to complete
    and making it frustrating for users, not to mention the fact that slow processing
    times can cause the application to fail altogether.
  prefs: []
  type: TYPE_NORMAL
- en: While Elasticsearch does not rely heavily on the CPU for indexing and searches,
    several things need to be taken into consideration when designing an Elastic Stack
    that performs well and returns results in a timely manner.
  prefs: []
  type: TYPE_NORMAL
- en: Although Elastic does not publish hard requirements for CPU, there are a couple
    of things that can be applied as a rule of thumb.
  prefs: []
  type: TYPE_NORMAL
- en: CPU count
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Typically, having more cores is better, and this might be the case for most
    workloads. Elasticsearch leverages having multiple cores available on the system
    by scheduling tasks across multiple CPUs; however, it doesn't require large amounts
    of CPU processing power as most of the operations are performed on files that
    are already indexed.
  prefs: []
  type: TYPE_NORMAL
- en: Most cloud providers (if you are deploying on the cloud) have increased rates
    for high CPU count virtual machines, to avoid unnecessary cost, size for a VM
    type that  balances more memory than CPU.
  prefs: []
  type: TYPE_NORMAL
- en: When sizing for sufficient CPU resources, you should allow for some growth without
    having to change settings midway. For a small setup, something with at least two
    CPUs should be sufficient. For testing purposes and a small number of indexes/sources,
    even one CPU should suffice, but performance will suffer, especially if all of
    the components—Elasticsearch, Logstash, and Kibana—are deployed on the same system.
  prefs: []
  type: TYPE_NORMAL
- en: CPU speed
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While there is no hard documentation on the minimum CPU speed (clock speed)
    requirements, it is somewhat difficult to find a CPU with less than 2 GHz nowadays.
    This low watermark seems to be about the minimum required for Elasticsearch to
    avoid problems.
  prefs: []
  type: TYPE_NORMAL
- en: Anything above 2 GHz will perform acceptably, even with a single CPU; this is
    adequate for testing purposes. For production environments, look for CPU clock
    speeds above 2 GHz or 2.3 GHz to avoid problems.
  prefs: []
  type: TYPE_NORMAL
- en: CPU performance impact
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If an incorrect sizing has been configured when it comes to the CPU, Elasticsearch
    will mostly suffer in the following three areas:'
  prefs: []
  type: TYPE_NORMAL
- en: Startup time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Index per second
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Search latency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Startup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: During startup, CPU usage might spike as the JVM starts and Elasticsearch reads
    the data from the cluster. Having a slower CPU configuration will cause Elasticsearch
    to take longer to start up.
  prefs: []
  type: TYPE_NORMAL
- en: If Elasticsearch nodes are to be constantly restarted, having the right CPU
    configuration will help to reduce the time it takes to reach an operational state.
  prefs: []
  type: TYPE_NORMAL
- en: Index per second
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The CPU configuration directly affects the indexes per second that Elasticsearch
    is able to handle, as it will run out of cycles once more documents are indexed.
    Ideally, with multiple cores, Elasticsearch leverages indexing on multiple CPUs,
    allowing more clients to send data without any metric or event being lost.
  prefs: []
  type: TYPE_NORMAL
- en: Search latency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Performance will probably suffer the most regarding the amount of time it takes
    for searches to return results. Remember that one of the main features of Elasticsearch
    is how fast it can retrieve data and display it.
  prefs: []
  type: TYPE_NORMAL
- en: Having an undersized CPU configuration will lead to searches taking longer than
    expected, which can result in a frustrating user experience.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following screenshot, we can see that search latency spikes to almost
    80 ms and hovers at around 20 ms:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/95698154-0f2a-4dad-abf0-75341cd8088d.png)'
  prefs: []
  type: TYPE_IMG
- en: Monitoring latency in Kibana
  prefs: []
  type: TYPE_NORMAL
- en: Note that the preceding screenshot was taken from an undersized system with
    just one CPU running at less than 2 GHz. The latency could be worse, but this
    was taken from a system running on a fast NVMe drive, which can have latency as
    low as 100 microseconds.
  prefs: []
  type: TYPE_NORMAL
- en: Recommendations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For optimal results, the correct CPU setup needs to be implemented. The following
    two main scenarios affect CPU sizing:'
  prefs: []
  type: TYPE_NORMAL
- en: Test/development
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Production
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test/dev
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For testing, anything above one CPU and 2 GHz would be sufficient for a small
    test, with a couple of clients sending data to Elasticsearch. The search results
    might be a little slow to return, but it will work without any problems.
  prefs: []
  type: TYPE_NORMAL
- en: Production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For production, make sure that you use a CPU with at least 2.3 GHz or above.
    The CPU count does not greatly impact performance, but having at least two CPUs
    ensures optimal operation. Once more clients are added, the CPU count might need
    to be modified to meet the extra demand; more Elasticsearch nodes can be added
    if the CPU becomes a constraint.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, when choosing between the core count versus clock speeds, Elasticsearch
    leverages having multiple cores. The performance benefits of fewer but faster
    cores is not as impressive as having a larger number of slower cores.
  prefs: []
  type: TYPE_NORMAL
- en: When deploying on Azure, you can use a DS2v3 VM type for a small setup, as it
    offers two CPUs and enough RAM for basic needs.
  prefs: []
  type: TYPE_NORMAL
- en: Once we correctly size the CPU, we can then focus on how the system memory (RAM)
    affects Elasticsearch performance and usability.
  prefs: []
  type: TYPE_NORMAL
- en: Memory sizing for Elasticsearch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Allocating enough RAM to Elasticsearch is probably the most important resource
    factor to consider to avoid problems and an underperforming setup.
  prefs: []
  type: TYPE_NORMAL
- en: Memory is one resource where having a lot of it is never a problem. As an architect,
    you need to bear in mind several things when sizing memory. Similar to the CPU
    resource, there is no hard documentation for minimum memory requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Filesystem cache
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having a lot of RAM is always a good idea, because of the filesystem cache or
    Linux page cache.
  prefs: []
  type: TYPE_NORMAL
- en: The kernel uses free system memory to cache, read, or write requests by allocating
    some portion of RAM to I/O requests, considerably speeding up searches or indexes
    in the case of Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the following screenshot, the kernel has allocated about
    1.2 GB as page cache:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/94cabe7d-acca-4cd1-baec-b009b8ea24e8.png)'
  prefs: []
  type: TYPE_IMG
- en: Leveraging the use of the page cache can help to reduce the response time when
    doing searches or incoming indexes; be sure to size for as much RAM as possible.
    There is a point were cache usage will balance out, and no more RAM will be used
    for the page cache. At this point, it is worth monitoring the process to try and
    identify this threshold to avoid running into unnecessary charges. To put it into
    perspective, if a **Virtual Machine** (**VM**) is sized with 32 GB of RAM, but
    only uses about 10 GB for the cache and never goes above this number, then it
    might be worth resizing to a smaller VM, as the remaining RAM will be left unused.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the Kibana dashboard in the following screenshot, you can
    monitor cache usage for Elasticsearch, which might help to identify whether resources
    are left unused:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/063feca8-d22e-4fc0-8256-d4107c6a8235.png)'
  prefs: []
  type: TYPE_IMG
- en: Monitoring cache usage for Elasticsearch
  prefs: []
  type: TYPE_NORMAL
- en: Disable swap
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Swap is a mechanism that allows the kernel to move memory pages to disk in the
    event of infrequent access or when there's memory pressure (that is, when the
    system is running out of memory). One of the main problems of swapping is that,
    when a memory page is moved to disk, its access time becomes considerably slower
    than in RAM.
  prefs: []
  type: TYPE_NORMAL
- en: 'DDR4 memory has an average transfer rate of about 10 GB/s and, more impressively,
    an average response time (or latency) of just 13 ns (nanoseconds). Compare that
    to even the fastest NVMe SSD drives in the market, which can achieve just 3.5
    GB/s and latencies of around 400 ūs (microseconds). You can quickly start seeing
    how this becomes a problem: not all cloud providers or on-premises setups use
    NVMe drives, and swapping to even slower spinning media can yield pretty bad results.'
  prefs: []
  type: TYPE_NORMAL
- en: Because of this, Elasticsearch recommends disabling all forms of swapping and
    instead relying on the correct sizing for system memory.
  prefs: []
  type: TYPE_NORMAL
- en: Undersizing memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Having the wrong memory configuration will result in different behaviors. It
    can be boiled down to two different situations: not having enough memory but having
    enough to run the system, and not having enough memory to the point that Elasticsearch
    can''t even start.'
  prefs: []
  type: TYPE_NORMAL
- en: For the first scenario, where there is a memory constraint, but there is just
    enough for Elasticsearch to start and run, the main problem would be that there
    is not enough memory for the page cache, which results in slow searches and reduced
    indexes per second. In this scenario, Elasticsearch is able to run, but with a
    reduced overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The other scenario can be split into two different situations: one where there''s
    not enough memory to start Elasticsearch and the other where Elasticsearch is
    able to start, but as soon as some indexes are added, it runs out of memory. To
    avoid a system crash, Linux has a mechanism called **out-of-memory killer** (**OOM
    killer**).'
  prefs: []
  type: TYPE_NORMAL
- en: Unable start
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Elasticsearch uses the JVM and, by default, it is set to use a minimum of 1
    GB of heap memory. This means that Java needs to allocate at least 1 GB of RAM
    to JVM, so for Elasticsearch to start with just the minimum, it requires about
    2.5 GB of RAM.
  prefs: []
  type: TYPE_NORMAL
- en: 'The easiest way to tell when this problem is occurring is by verifying the
    status of the Elasticsearch service using `systemctl status elasticsearch`; it
    will return an error message similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/57c77d4e-6e95-433b-b5ca-56f62fcf3ec8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Upon further inspection of the error log, we can clearly see how JVM failed
    to allocate the necessary memory, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Testing using the default heap of 1 GB is sufficient. For production, make sure
    that you increase the heap to at least 2 GB and adjust as necessary.
  prefs: []
  type: TYPE_NORMAL
- en: 'To increase the heap, edit the `/etc/elasticsearch/jvm.options` file and find
    the following options:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Change these two options to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `-Xms2g` phrase indicates that Java should have a minimum heap of 2 GB and `-Xmx2g`
    indicates the maximum heap of 2 GB.
  prefs: []
  type: TYPE_NORMAL
- en: OOM killer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **out-of-memory killer** (**OOM killer**) mechanism's main purpose is to
    avoid a total system crash by killing processes that are running processes. Each
    process has an `oom_score` value. OOM killer decides which process to kill based
    on this score; the higher the score, the more likely it is that the process will
    be killed in the event of memory starvation. This score is calculated based on
    how much memory the process would free up if it were killed.
  prefs: []
  type: TYPE_NORMAL
- en: If we take the previous scenario as a starting point, were Elasticsearch is
    able to start with a minimum of 2.5 GB, once more indexes/sources are added to
    Elasticsearch it will start requiring more and more system memory, up to the point
    where there is no more memory, and the system is close to a total crash. At that
    moment, OOM killer jumps in and kills the process (or processes) that consumes
    the most memory—in our case, Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: 'When looking at the events under `/var/log/messages`, we can see how OOM killer kicks
    in and kills the Java process, and then the Elasticsearch service fails, as shown
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e60a2a14-9647-4e00-991c-6ff60d1f572e.png)'
  prefs: []
  type: TYPE_IMG
- en: Recommendations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ideally, enough memory should be allocated for Elasticsearch. The very minimum
    requirement for memory is about 2.5 GB, but that would lead to a situation where
    the system might run out of memory quickly.
  prefs: []
  type: TYPE_NORMAL
- en: For testing purposes, 2.5 GB might be enough for a couple of sources/indexes.
    Performance will undoubtedly suffer, but it will remain somewhat usable.
  prefs: []
  type: TYPE_NORMAL
- en: For production, make sure to have at least 4 GB or more for system memory. This
    should allow Elasticsearch to start without problems and normally run with multiple
    sources/indexes configured. Make sure the heap size for the JVM is increased accordingly,
    and consider allowing some RAM for the page cache for faster response times when
    interacting with the filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will look at the storage configuration required for Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: Storage configuration for Elasticsearch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Storage requirements for Elasticsearch are relatively straightforward, and
    can be divided into two main categories:'
  prefs: []
  type: TYPE_NORMAL
- en: Storage capacity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storage performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's go through both of these and see how decisions made here can affect the
    overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: Capacity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Storage capacity directly affects how much data Elasticsearch is able to store.
    As is the case in many other situations, this is a big and complex requirement
    to consider, as it depends upon so many other variables that affect the utilization
    of space.
  prefs: []
  type: TYPE_NORMAL
- en: The primary variable would be the size of the logs/metrics that are sent to
    Elasticsearch. This depends on the number of logs that are generated daily (or
    monthly). For example, if the daily log rate is 100 MB, then that means that,
    to be able to store a month's worth of logs, at least 3 GB of available space
    is needed (100 MB x 30 days = 3 GB).
  prefs: []
  type: TYPE_NORMAL
- en: Note that this is the minimum space required for a single source. Ideally, some
    overhead should be accounted for as data changes regularly and a figure of 100
    MB/day might not be constant for all of the days in the month or other months
    might have a higher rate due to higher load. Additionally, once more sources (or
    clients) are added, data usage will grow accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: By default, Elasticsearch will store its data under the `/var/lib/elasticsearch`
    directory.
  prefs: []
  type: TYPE_NORMAL
- en: Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the main features of Elasticsearch is its ability to retrieve data really
    fast. While this is done using an enhanced mechanism of storing documents as JSON
    files, having the right performance setup definitely helps achieve the almost
    real-time search results.
  prefs: []
  type: TYPE_NORMAL
- en: There is no hard number provided by Elastic for storage requirements, but using
    a **Solid-State Drive** (**SSD**) for the `/var/lib/elasticsearch` directory helps
    in reducing latency when performing searches, as the SSD has a substantially lower
    latency when compared to HDD. An SSD also helps when ingesting data as writes
    get acknowledged faster, thereby allowing for more concurrent incoming indexes.
    This is reflected in the indexes per second that can be seen on the Kibana monitoring
    dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: When sizing for the cloud, this really depends on the provider as some base
    the performance of the disks on their size, but others allow the performance to
    be manually configured (as is the case with IOPS and throughput).
  prefs: []
  type: TYPE_NORMAL
- en: Having a slower setup will result in searches taking longer than expected and
    a slower data ingestion, due to an unreliable, slower disk setup.
  prefs: []
  type: TYPE_NORMAL
- en: Considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For space, consider a sizing that will allow you enough space for unexpected
    data growth. If, for example, the expected data usage for an entire month would
    be 500 GB, consider sizing for at least 700 GB; doing this gives you a buffer
    and avoids situations where not enough space is left for Elasticsearch indexes.
    A good starting point is 500 GB, as it gives enough space for testing/production
    while the actual data usage and data change is calculated (if not previously known).
  prefs: []
  type: TYPE_NORMAL
- en: For performance, consider using faster storage solutions such as SSD to allow
    for low-latency searches and faster indexes/s. For the cloud, most providers have
    some sort of SSD offering that can be used with Elasticsearch. Make sure that
    at least 500 IOPS are provisioned for optimal performance.
  prefs: []
  type: TYPE_NORMAL
- en: For Azure, you can use a P10 disk—which is an SSD that can provide up to 500
    IOPS—or an E10 as a lower cost alternative that delivers the same result.
  prefs: []
  type: TYPE_NORMAL
- en: We will now look at what needs to be considered for Logstash and Kibana.
  prefs: []
  type: TYPE_NORMAL
- en: Logstash and Kibana requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are no specific requirements for Logstash and Kibana, but keeping in mind
    a couple of things when designing an Elastic Stack is always a good approach.
  prefs: []
  type: TYPE_NORMAL
- en: Logstash
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logstash is not heavy on CPU nor RAM, but this depends entirely on how many
    sources are feeding Logstash data since, for each event that Logstash parses,
    there is some overhead required to complete the process. If Logstash is to be
    installed on its own (with no other components on the same system), anything above
    one vCPU and 2 GBs of RAM should suffice for small/testing deployments. Ideally,
    the actual usage should be monitored and the system tuned accordingly. Logstash
    by default has in-memory queues that are used to store events temporarily; this
    behavior can be changed to use persistent queues when processing events. This
    allows for persistent consistency and avoids data loss during an outage. Additionally,
    having persistent queues helps to absorb bursts of events by acting as a buffer
    between the clients and Logstash.
  prefs: []
  type: TYPE_NORMAL
- en: 'When using persistent queues for storage capacity, the `/var/lib/logstash`
    directory needs to be able to store events while being processed by Logstash.
    The amount of space depends on two factors: the egress speed when sending data
    to Elasticsearch and the number of events being sent to Logstash. The minimum
    would be 1 GB and the space needs to be increased accordingly when the number
    of sources is increased.'
  prefs: []
  type: TYPE_NORMAL
- en: Kibana
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The requirements for Kibana depend entirely on the number of users concurrently
    accessing the dashboard. The amount of resources allocated to Kibana needs to
    be based on the intended usage—for example, what is the expected user base? How
    many of those users will access Kibana at the same time?
  prefs: []
  type: TYPE_NORMAL
- en: For small deployments/testing, the minimum requirements are dictated by the
    JVM. One vCPU and 2 GB of RAM is enough for several users, but once more users
    start using the dashboard, RAM will be the first resource to become a bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: In general, an Elastic Stack has pretty loose requirements that are mostly dictated
    by the usage and the number of sources. Regarding software, the primary requirement
    is Java; since all of the components use the JVM, either the open JDK or the official
    JDK can be used.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we went through the requirements that are needed when designing
    an Elastic Stack using Elasticsearch, Logstash, and Kibana. For Elasticsearch,
    we determined that the minimum CPU requirement is two vCPUs for small setups,
    and the CPU speed should be kept above 2 GHz. If these minimum requirements are
    not met, Elasticsearch will take longer to start up and will perform more slowly.
    This manifests as a decrease in the number of indexes per second and an increased
    search latency, both of which are things that need to be avoided in order for
    us to be able to take full advantage of the near-instant searches that Elasticsearch
    provides.
  prefs: []
  type: TYPE_NORMAL
- en: Memory sizing is probably the most important specification when designing an
    Elasticsearch setup. Part of the system memory will be used for the filesystem
    cache (also known as the page cache), which helps with searches and indexes per
    second. Swapping is not recommended, as it is considered extremely slow when compared
    to actual RAM access, and so swapping should be disabled on Elasticsearch nodes.
    If the correct memory requirements are not met, Elasticsearch will fail to start
    altogether since there will not be enough memory for the JVM to start. If, on
    the other hand, enough memory is present to start the JVM, but the load increases
    over time and the system runs out of memory, the OOM or out-of-memory killer will
    be engaged to avoid a system crash that would lead to a failure of the application.
    The very minimum amount of RAM required is 2.5 GB, but resource constraints will
    be seen relatively quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Storage capacity and performance play an important role when setting up Elasticsearch.
    The capacity depends on the amount of data that needs to be kept and the number
    of sources configured. Latency needs to be kept to a minimum in order for our
    searches to be fast. Ideally, SSD should be used.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, for Logstash and Kibana, the minimum requirements are one vCPU and 2
    GB of RAM for each component. For Logstash, there is a space requirement for the
    persistent queues.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will jump into deploying an Elastic Stack using Elasticsearch,
    Logstash, and Kibana using the facts that learned in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How many CPUs are recommended for Elasticsearch?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the recommended minimum CPU speed for Elasticsearch?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does having the wrong CPU configuration impact Elasticsearch performance?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a page cache?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is it recommended that you disable swapping on Elasticsearch nodes?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does undersizing memory affect Elasticsearch?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the minimum memory required for Elasticsearch?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Where does Elasticsearch store data by default?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is using an SSD recommended for Elasticsearch?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the minimum requirements for Logstash?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are persistent queues?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What affects the resource usage in Kibana?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information, you can read the following book:'
  prefs: []
  type: TYPE_NORMAL
- en: '***Linux: Powerful Server Administration*, by Uday R. Sawant, Et al.**: [https://www.packtpub.com/networking-and-servers/linux-powerful-server-administration](https://www.packtpub.com/networking-and-servers/linux-powerful-server-administration)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
