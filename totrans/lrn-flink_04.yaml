- en: Chapter 4.  Data Processing Using the Table API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the earlier chapters, we talked about batch and stream data processing APIs
    provided by Apache Flink. In this chapter, we are going to talk about Table API
    which is a SQL interface for data processing in Flink. Table API operates on a
    table interface which can be created from a dataset and datastream. Once the dataset/datastream
    is registered as a table, we are free to apply relational operations such as aggregations,
    joins, and selections.
  prefs: []
  type: TYPE_NORMAL
- en: Tables can also be queried like regular SQL queries. Once the operations are
    performed, we need to convert the table back to either a dataset or datastream.
    Apache Flink internally uses another open source project called Apache Calcite
    [https://calcite.apache.org/](https://calcite.apache.org/) for optimizing these
    query transformations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Registering tables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessing the registered table
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Operators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let's get started.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to use Table API, the very first thing we need to do is to create
    a Java Maven project and add the following dependency in it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This dependency will download all the required JARs in your class path. Once
    the download is complete, we are all good to use Table API.
  prefs: []
  type: TYPE_NORMAL
- en: Registering tables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to operate on datasets/datastreams, first we need to register a table
    in `TableEnvironment`. Once the table is registered with a unique name, it can
    be easily accessed from `TableEnvironment`.
  prefs: []
  type: TYPE_NORMAL
- en: '`TableEnvironment` maintains an internal table catalogue for table registration.
    The following diagram shows the details:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Registering tables](img/B05653_image1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: It is very important to have unique table names, otherwise you will get an exception.
  prefs: []
  type: TYPE_NORMAL
- en: Registering a dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to perform SQL operations on a dataset, we need to register it as a
    table in `BatchTableEnvironment`. We need to define a Java POJO class while registering
    the table.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, let''s say we need to register a dataset called Word Count. Each
    record in this table will have word and frequency attributes. The Java POJO for
    the same would look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The same class in Scala can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now we can register this table.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Please make a note that the name of the dataset table must not match the `^_DataSetTable_[0-9]+`
    pattern as it is reserved for internal memory use.
  prefs: []
  type: TYPE_NORMAL
- en: Registering a datastream
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to a dataset, we can also register a datastream in `StreamTableEnvironment`.
    We need to define a Java POJO class while registering the table.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, let''s say we need to register a datastream called Word Count.
    Each record in this table will have a word and frequency attributes. The Java
    POJO for the same would look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The same class in Scala can be defined as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Now we can register this table.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Please make a note that the name of the datastream table must not match the
    `^_DataStreamTable_[0-9]+` pattern as it is reserved for internal memory use.
  prefs: []
  type: TYPE_NORMAL
- en: Registering a table
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to a dataset and a datastream, we can also register a table originating
    from Table API.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Registering external table sources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Flink allows us to register an external table from sources using a `TableSource`.
    A table source can allow us to access data stored in databases such as MySQL and
    Hbase, in. filesystems such as CSVs, Parquet, and ORC, or you can also read messaging
    systems such as RabbitMQ and Kafka.
  prefs: []
  type: TYPE_NORMAL
- en: Currently, Flink allows reading data from CSV files using CSV sources and JSON
    data from Kafka topics using Kafka sources.
  prefs: []
  type: TYPE_NORMAL
- en: CSV table source
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now let's look at how to directly read data using a CSV source and then register
    the source in a table environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'A CSV source is by default available in the `flink-table` API JAR so there
    is no need to add any other extra Maven dependency. The following dependency is
    good enough:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The following code snippet shows how to read CSV files and register the table
    source.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Kafka JSON table source
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can also register the Kafka JSON table source in the table environment.
    In order to use this API we need to add the following two dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first one is for Table API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The second dependency would be for the Kafka Flink connector:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are using Kafka 0.8, apply:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'If you are using Kafka 0.9, apply:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we need to write the code as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we define the Kafka source for Kafka 0.8 and then register
    the source in the table environment.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing the registered table
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once the table is registered, we can access it very easily from `TableEnvironment`
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding statement scans the table registered with the name `"tableName"`
    in `BatchTableEnvironment`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding statement ingests the table registered with the name `"tableName"`
    in `StreamTableEnvironment`:'
  prefs: []
  type: TYPE_NORMAL
- en: Operators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Flink's Table API provides various operators as part of its domain-specific
    language. Most of the operators are available in Java and Scala APIs. Let's look
    at those operators one by one.
  prefs: []
  type: TYPE_NORMAL
- en: The select operator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `select` operator is like a SQL select operator which allows you to select
    various attributes/columns in a table.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The where operator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `where` operator is used for filtering out results.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The filter operator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `filter` operator can be used as a replacement for the `where` operator.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The as operator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `as` operator is used for renaming fields:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The groupBy operator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is similar to SQL `groupBy` operations which aggregate the results according
    to a given attribute.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The join operator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `join` operator is used to join tables. It is compulsory that we specify
    at least one equality joining condition.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The leftOuterJoin operator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `leftOuterJoin` operator joins two tables by getting all the values from
    the table specified on the left side and selects only the matching values from
    the right side table. It is compulsory that we specify at least one equality joining
    condition.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The rightOuterJoin operator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `rightOuterJoin` operator joins two tables by getting all values from the
    table specified on the right side and selects only matching values from the left
    side table. It is compulsory that we specify at least one equality joining condition.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The fullOuterJoin operator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `fullOuterJoin` operator joins two tables by getting all the values from
    both tables. It is compulsory that we specify at least one equality joining condition.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The union operator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `union` operator merges two similar tables. It removes duplicate values
    in the resulting table.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The unionAll operator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `unionAll` operator merges two similar tables.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: The intersect operator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `intersect` operator returns matching values from both tables. It makes
    sure that the resultant table does have any duplicates.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The intersectAll operator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `intersectAll` operator returns matching values from both tables. The resultant
    table might have duplicate records.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The minus operator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `minus` operator returns records from the left table which do not exist
    in the right table. It makes sure that the resultant table does not have any duplicates.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: The minusAll operator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `minusAll` operator returns records from the left table which do not exist
    in the right table. The resultant table might have duplicate records.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: The distinct operator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `distinct` operator returns only unique value records from the table.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: The orderBy operator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `orderBy` operator returns records sorted across globally parallel partitions.
    You can choose the order as ascending or descending.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: The limit operator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `limit` operator limits records sorted across globally parallel partitions
    from a given offset.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Data types
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table API supports common SQL data types which can be used easily. Internally,
    it uses `TypeInformation` to identify various data types. It currently does not
    support all Flink data types:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Table API | SQL | Java type |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `Types.STRING` | `VARCHAR` | `java.lang.String` |'
  prefs: []
  type: TYPE_TB
- en: '| `Types.BOOLEAN` | `BOOLEAN` | `java.lang.Boolean` |'
  prefs: []
  type: TYPE_TB
- en: '| `Types.BYTE` | `TINYINT` | `java.lang.Byte` |'
  prefs: []
  type: TYPE_TB
- en: '| `Types.SHORT` | `SMALLINT` | `java.lang.Short` |'
  prefs: []
  type: TYPE_TB
- en: '| `Types.INT` | `INTEGER`, `INT` | `java.lang.Integer` |'
  prefs: []
  type: TYPE_TB
- en: '| `Types.LONG` | `BIGINT` | `java.lang.Long` |'
  prefs: []
  type: TYPE_TB
- en: '| `Types.FLOAT` | `REAL`, `FLOAT` | `java.lang.Float` |'
  prefs: []
  type: TYPE_TB
- en: '| `Types.DOUBLE` | `DOUBLE` | `java.lang.Double` |'
  prefs: []
  type: TYPE_TB
- en: '| `Types.DECIMAL` | `DECIMAL` | `java.math.BigDecimal` |'
  prefs: []
  type: TYPE_TB
- en: '| `Types.DATE` | `DATE` | `java.sql.Date` |'
  prefs: []
  type: TYPE_TB
- en: '| `Types.TIME` | `TIME` | `java.sql.Time` |'
  prefs: []
  type: TYPE_TB
- en: '| `Types.TIMESTAMP` | `TIMESTAMP(3)` | `java.sql.Timestamp` |'
  prefs: []
  type: TYPE_TB
- en: '| `Types.INTERVAL_MONTHS` | INTERVAL YEAR TO MONTH | `java.lang.Integer` |'
  prefs: []
  type: TYPE_TB
- en: '| `Types.INTERVAL_MILLIS` | INTERVAL DAY TO SECOND(3) | `java.lang.Long` |'
  prefs: []
  type: TYPE_TB
- en: With continuous development and support from the community, more data types
    will be supported soon.
  prefs: []
  type: TYPE_NORMAL
- en: SQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Table API also allows us to write free form SQL queries using the `sql()` method.
    The method internally also uses Apache Calcite for SQL syntax verification and
    optimization. It executes the query and returns results in the table format. Later
    the table can be again transformed into either a dataset or datastream or `TableSink`
    for further processing.
  prefs: []
  type: TYPE_NORMAL
- en: One thing to note here is that, for the SQL method to access the tables, they
    must be registered with `TableEnvironment`.
  prefs: []
  type: TYPE_NORMAL
- en: More support is being added to the SQL method continuously so if any syntax
    is not supported, it will error out with `TableException`.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's look at how to use the SQL method on a dataset and datastream.
  prefs: []
  type: TYPE_NORMAL
- en: SQL on datastream
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'SQL queries can be executed on datastreams registered with `TableEnvironment`
    using the `SELECT STREAM` keyword. Most of the SQL syntax is common between datasets
    and datastreams. To know more about stream syntax, the Apache Calcite''s Streams
    documentation would be helpful. It can be found at: [https://calcite.apache.org/docs/stream.html](https://calcite.apache.org/docs/stream.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Let's say we want to analyze the product schema defined as (`id`, `name`, `stock`).
    The following code needs to be written using the `sql()` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Table API uses a lexical policy similar to Java in order to define queries properly.
    This means the case of the identifiers is preserved and they are matched case
    sensitively. If any of your identifiers contain non-alpha numeric characters then
    you can quote those using back ticks.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, if you want to define a column with the name `''my col''` then
    you need to use back ticks as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Supported SQL syntax
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As stated earlier, Flink uses Apache Calcite for validating and optimizing
    SQL queries. With the current version, the following **Backus Naur Form** (**BNF**)
    is supported:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Scalar functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Table API and SQL support various built-in scalar functions. Let's try to understand
    those one by one.
  prefs: []
  type: TYPE_NORMAL
- en: Scalar functions in the table API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following is the list of supported scalar functions in the table API:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Java function** | **Scala function** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `ANY.isNull` | `ANY.isNull` | Returns `true` if the given expression is null.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `ANY.isNotNull` | `ANY.isNotNull` | Returns `true` if the given expression
    is not null. |'
  prefs: []
  type: TYPE_TB
- en: '| `BOOLEAN.isTrue` | `BOOLEAN.isTrue` | Returns `true` if the given Boolean
    expression is `true`. `False` otherwise. |'
  prefs: []
  type: TYPE_TB
- en: '| `BOOLEAN.isFalse` | `BOOLEAN.isFalse` | Returns `true` if given Boolean expression
    is false. `False` otherwise. |'
  prefs: []
  type: TYPE_TB
- en: '| `NUMERIC.log10()` | `NUMERIC.log10()` | Calculates the base 10 logarithm
    of given value. |'
  prefs: []
  type: TYPE_TB
- en: '| `NUMERIC.ln()` | `NUMERIC.ln()` | Calculates the natural logarithm of given
    value. |'
  prefs: []
  type: TYPE_TB
- en: '| `NUMERIC.power(NUMERIC)` | `NUMERIC.power(NUMERIC)` | Calculates the given
    number raised to the power of the other value. |'
  prefs: []
  type: TYPE_TB
- en: '| `NUMERIC.abs()` | `NUMERIC.abs()` | Calculates the absolute value of given
    value. |'
  prefs: []
  type: TYPE_TB
- en: '| `NUMERIC.floor()` | `NUMERIC.floor()` | Calculates the largest integer less
    than or equal to a given number. |'
  prefs: []
  type: TYPE_TB
- en: '| `NUMERIC.ceil()` | `NUMERIC.ceil()` | Calculates the smallest integer greater
    than or equal to a given number. |'
  prefs: []
  type: TYPE_TB
- en: '| `STRING.substring(INT, INT)` | `STRING.substring(INT, INT)` | Creates a substring
    of the given string at the given index for the given length |'
  prefs: []
  type: TYPE_TB
- en: '| `STRING.substring(INT)` | `STRING.substring(INT)` | Creates a substring of
    the given string beginning at the given index to the end. The start index starts
    at 1 and is inclusive. |'
  prefs: []
  type: TYPE_TB
- en: '| `STRING.trim(LEADING, STRING)` `STRING.trim(TRAILING, STRING)` `STRING.trim(BOTH,
    STRING)` `STRING.trim(BOTH)` `STRING.trim()` | `STRING.trim(leading = true, trailing
    = true, character = " ")` | Removes leading and/or trailing characters from the
    given string. By default, whitespaces at both sides are removed. |'
  prefs: []
  type: TYPE_TB
- en: '| `STRING.charLength()` | `STRING.charLength()` | Returns the length of a string.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `STRING.upperCase()` | `STRING.upperCase()` | Returns all of the characters
    in a string in upper case using the rules of the default locale. |'
  prefs: []
  type: TYPE_TB
- en: '| `STRING.lowerCase()` | `STRING.lowerCase()` | Returns all of the characters
    in a string in lower case using the rules of the default locale. |'
  prefs: []
  type: TYPE_TB
- en: '| `STRING.initCap()` | `STRING.initCap()` | Converts the initial letter of
    each word in a string to uppercase. Assumes a string containing only `[A-Za-z0-9]`,
    everything else is treated as whitespace. |'
  prefs: []
  type: TYPE_TB
- en: '| `STRING.like(STRING)` | `STRING.like(STRING)` | Returns true, if a string
    matches the specified LIKE pattern. For example, `"Jo_n%"` matches all strings
    that start with `"Jo(arbitrary letter)n"`. |'
  prefs: []
  type: TYPE_TB
- en: '| `STRING.similar(STRING)` | `STRING.similar(STRING)` | Returns `true`, if
    a string matches the specified SQL regex pattern. For example, `"A+"` matches
    all strings that consist of at least one `"A"`. |'
  prefs: []
  type: TYPE_TB
- en: '| `STRING.toDate()` | `STRING.toDate` | Parses a date string in the form `"yy-mm-dd"`
    to a SQL date. |'
  prefs: []
  type: TYPE_TB
- en: '| `STRING.toTime()` | `STRING.toTime` | Parses a time string in the form `"hh:mm:ss"`
    to a SQL time. |'
  prefs: []
  type: TYPE_TB
- en: '| `STRING.toTimestamp()` | `STRING.toTimestamp` | Parses a timestamp string
    in the form `"yy-mm-dd hh:mm:ss.fff"` to a SQL timestamp. |'
  prefs: []
  type: TYPE_TB
- en: '| `TEMPORAL.extract(TIMEINTERVALUNIT)` | NA | Extracts parts of a time point
    or time interval. Returns the part as a long value. For example, `2006-06-05 .toDate.extract(DAY)`
    leads to `5`. |'
  prefs: []
  type: TYPE_TB
- en: '| `TIMEPOINT.floor(TIMEINTERVALUNIT)` | `TIMEPOINT.floor(TimeIntervalUnit)`
    | Rounds a time point down to the given unit. For example, `"12:44:31".toDate.floor(MINUTE)`
    leads to `12:44:00`. |'
  prefs: []
  type: TYPE_TB
- en: '| `TIMEPOINT.ceil(TIMEINTERVALUNIT)` | `TIMEPOINT.ceil(TimeIntervalUnit)` |
    Rounds a time point up to the given unit. For example, `"12:44:31".toTime.floor(MINUTE)`
    leads to `12:45:00`. |'
  prefs: []
  type: TYPE_TB
- en: '| `currentDate()` | `currentDate()` | Returns the current SQL date in UTC time
    zone. |'
  prefs: []
  type: TYPE_TB
- en: '| `currentTime()` | `currentTime()` | Returns the current SQL time in UTC time
    zone. |'
  prefs: []
  type: TYPE_TB
- en: '| `currentTimestamp()` | `currentTimestamp()` | Returns the current SQL timestamp
    in UTC time zone. |'
  prefs: []
  type: TYPE_TB
- en: '| `localTime()` | `localTime()` | Returns the current SQL time in local time
    zone. |'
  prefs: []
  type: TYPE_TB
- en: '| `localTimestamp()` | `localTimestamp()` | Returns the current SQL timestamp
    in local time zone. |'
  prefs: []
  type: TYPE_TB
- en: Scala functions in SQL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following is the list of supported scalar functions in the `sql()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Function | Description |'
  prefs: []
  type: TYPE_TB
- en: '| `EXP(NUMERIC)` | Calculates the Euler''s number raised to the given power.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `LOG10(NUMERIC)` | Calculates the base 10 logarithm of the given value. |'
  prefs: []
  type: TYPE_TB
- en: '| `LN(NUMERIC)` | Calculates the natural logarithm of the given value. |'
  prefs: []
  type: TYPE_TB
- en: '| `POWER(NUMERIC, NUMERIC)` | Calculates the given number raised to the power
    of the other value. |'
  prefs: []
  type: TYPE_TB
- en: '| `ABS(NUMERIC)` | Calculates the absolute value of the given value. |'
  prefs: []
  type: TYPE_TB
- en: '| `FLOOR(NUMERIC)` | Calculates the largest integer less than or equal to a
    given number. |'
  prefs: []
  type: TYPE_TB
- en: '| `CEIL(NUMERIC)` | Calculates the smallest integer greater than or equal to
    a given number. |'
  prefs: []
  type: TYPE_TB
- en: '| `SUBSTRING(VARCHAR, INT, INT) SUBSTRING(VARCHAR FROM INT FOR INT)` | Creates
    a substring of the given string at the given index for the given length. The index
    starts at 1 and is inclusive, that is, the character at the index is included
    in the substring. The substring has the specified length or less. |'
  prefs: []
  type: TYPE_TB
- en: '| `SUBSTRING(VARCHAR, INT)``SUBSTRING(VARCHAR FROM INT)` | Creates a substring
    of the given string beginning at the given index to the end. The start index starts
    at 1 and is inclusive. |'
  prefs: []
  type: TYPE_TB
- en: '| `TRIM(LEADING VARCHAR FROM VARCHAR) TRIM(TRAILING VARCHAR FROM VARCHAR) TRIM(BOTH
    VARCHAR FROM VARCHAR) TRIM(VARCHAR)` | Removes leading and/or trailing characters
    from the given string. By default, whitespaces at both sides are removed. |'
  prefs: []
  type: TYPE_TB
- en: '| `CHAR_LENGTH(VARCHAR)` | Returns the length of a string. |'
  prefs: []
  type: TYPE_TB
- en: '| `UPPER(VARCHAR)` | Returns all of the characters in a string in upper case
    using the rules of the default locale. |'
  prefs: []
  type: TYPE_TB
- en: '| `LOWER(VARCHAR)` | Returns all of the characters in a string in lower case
    using the rules of the default locale. |'
  prefs: []
  type: TYPE_TB
- en: '| `INITCAP(VARCHAR)` | Converts the initial letter of each word in a string
    to uppercase. Assumes a string containing only `[A-Za-z0-9]`, everything else
    is treated as whitespace. |'
  prefs: []
  type: TYPE_TB
- en: '| `VARCHAR LIKE VARCHAR` | Returns true if a string matches the specified LIKE
    pattern. For example, `"Jo_n%"` matches all strings that start with `"Jo(arbitrary
    letter)n"`. |'
  prefs: []
  type: TYPE_TB
- en: '| `VARCHAR SIMILAR TO VARCHAR` | Returns true if a string matches the specified
    SQL regex pattern. For example, `"A+"` matches all strings that consist of at
    least one `"A"`. |'
  prefs: []
  type: TYPE_TB
- en: '| `DATE VARCHAR` | Parses a date string in the form `"yy-mm-dd"` to a SQL date.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `TIME VARCHAR` | Parses a time string in the form `"hh:mm:ss"` to a SQL time.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `TIMESTAMP VARCHAR` | Parses a timestamp string in the form `"yy-mm-dd hh:mm:ss.fff"`
    to a SQL timestamp. |'
  prefs: []
  type: TYPE_TB
- en: '| `EXTRACT(TIMEINTERVALUNIT FROM TEMPORAL)` | Extracts parts of a time point
    or time interval. Returns the part as a long value. For example, `EXTRACT(DAY
    FROM DATE ''2006-06-05'')` leads to `5`. |'
  prefs: []
  type: TYPE_TB
- en: '| `FLOOR(TIMEPOINT TO TIMEINTERVALUNIT)` | Rounds a time point down to the
    given unit. For example, `FLOOR(TIME ''12:44:31'' TO MINUTE)` leads to `12:44:00`.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `CEIL(TIMEPOINT TO TIMEINTERVALUNIT)` | Rounds a time point up to the given
    unit. For example, `CEIL(TIME ''12:44:31'' TO MINUTE)` leads to `12:45:00`. |'
  prefs: []
  type: TYPE_TB
- en: '| `CURRENT_DATE` | Returns the current SQL date in UTC timezone. |'
  prefs: []
  type: TYPE_TB
- en: '| `CURRENT_TIME` | Returns the current SQL time in UTC timezone. |'
  prefs: []
  type: TYPE_TB
- en: '| `CURRENT_TIMESTAMP` | Returns the current SQL timestamp in UTC timezone.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `LOCALTIME` | Returns the current SQL time in local timezone. |'
  prefs: []
  type: TYPE_TB
- en: '| `LOCALTIMESTAMP` | Returns the current SQL timestamp in local timezone. |'
  prefs: []
  type: TYPE_TB
- en: Use case - Athletes data insights using Flink Table API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have learnt details of Table API, let's try to apply this knowledge
    to a real life use case. Consider we have a dataset with us, which has information
    about the Olympic athletes and their performance in various games.
  prefs: []
  type: TYPE_NORMAL
- en: 'The sample data looks like that shown in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Player** | **Country** | **Year** | **Game** | **Gold** | **Silver** |
    **Bronze** | **Total** |'
  prefs: []
  type: TYPE_TB
- en: '| Yang Yilin | China | 2008 | Gymnastics | 1 | 0 | 2 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| Leisel Jones | Australia | 2000 | Swimming | 0 | 2 | 0 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| Go Gi-Hyeon | South Korea | 2002 | Short-Track Speed Skating | 1 | 1 | 0
    | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| Chen Ruolin | China | 2008 | Diving | 2 | 0 | 0 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| Katie Ledecky | United States | 2012 | Swimming | 1 | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Ruta Meilutyte | Lithuania | 2012 | Swimming | 1 | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: "| DÃ\x83Â¡niel Gyurta | Hungary | 2004 | Swimming | 0 | 1 | 0 | 1 |"
  prefs: []
  type: TYPE_TB
- en: '| Arianna Fontana | Italy | 2006 | Short-Track Speed Skating | 0 | 0 | 1 |
    1 |'
  prefs: []
  type: TYPE_TB
- en: '| Olga Glatskikh | Russia | 2004 | Rhythmic Gymnastics | 1 | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Kharikleia Pantazi | Greece | 2000 | Rhythmic Gymnastics | 0 | 0 | 1 | 1
    |'
  prefs: []
  type: TYPE_TB
- en: '| Kim Martin | Sweden | 2002 | Ice Hockey | 0 | 0 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Kyla Ross | United States | 2012 | Gymnastics | 1 | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Gabriela Dragoi | Romania | 2008 | Gymnastics | 0 | 0 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Tasha Schwikert-Warren | United States | 2000 | Gymnastics | 0 | 0 | 1 |
    1 |'
  prefs: []
  type: TYPE_TB
- en: Now we want to get answers to the questions like, how many medals were won by
    country or by game. As the data we have in structured data, we can use Table API
    to query data in a SQL way. So let's get started.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data available is in the CSV format. So we will be using a CSV reader provided
    by Flink API as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we need to create a Table with this dataset and register it in Table Environment
    for further processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we can write a regular SQL query to get more insights from the data. Or
    else we can use Table API operators to manipulate the data, as shown in the following
    code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: This way we can analyse such data in a much more simpler way using Table API.
    The complete code for this use case is available on GitHub at [https://github.com/deshpandetanmay/mastering-flink/tree/master/chapter04/flink-table](https://github.com/deshpandetanmay/mastering-flink/tree/master/chapter04/flink-table).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about a SQL-based API supported by Flink called
    Table API. We also learned how to transform a dataset/stream into a table, registering
    a table, datasets, and datastreams with `TableEnvironment` and then using the
    registered tables to perform various operations. For people coming from a SQL
    databases background, this API is bliss.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to talk about a very interesting library called
    **Complex Event Processing** and how to use it for solving various business use
    cases.
  prefs: []
  type: TYPE_NORMAL
