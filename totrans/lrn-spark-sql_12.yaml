- en: Spark SQL in Large-Scale Application Architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this book, we started with the basics of Spark SQL and its components, and
    its role in Spark applications. Later, we presented a series of chapters focusing
    on its usage in various types of applications. With DataFrame/Dataset API and
    the Catalyst optimizer at the heart of Spark SQL, it is no surprise that it plays
    a key role in all applications based on the Spark technology stack. These applications
    include large-scale machine learning, large-scale graphs, and deep learning applications.
    Additionally, we presented Spark SQL-based Structured Streaming applications that
    operate in complex environments as continuous applications. In this chapter, we
    will explore application architectures that leverage Spark modules and Spark SQL
    in real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: More specifically, we will cover key architectural components and patterns in
    large-scale applications that architects and designers will find useful as a starting
    point for their specific use-cases. We will describe the deployment of some of
    the main processing models being used for batch processing, streaming applications,
    and machine learning pipelines. The underlying architecture for these processing
    models is required to support ingesting very large volumes of various types of
    data arriving at high velocities at one end, while making the output data available
    for use by analytical tools, and reporting and modeling software at the other
    end. Additionally, we will present supporting code using Spark SQL for monitoring,
    troubleshooting, and gathering/reporting metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Spark-based batch and stream processing architectures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding Lambda and Kappa Architectures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing scalable stream processing with structured streaming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building robust **Extract-Transform-Load** (**ETL**) pipelines using Spark SQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a scalable monitoring solution using Spark SQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying Spark machine learning pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using cluster managers: Mesos and Kubernetes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding Spark-based application architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Apache Spark is an emerging platform that leverages distributed storage and
    processing frameworks to support querying, reporting, analytics, and intelligent
    applications at scale. Spark SQL has the necessary features, and supports the
    key mechanisms required, to access data across a diverse set of data sources and
    formats, and prepare it for downstream applications either with low-latency streaming
    data or high-throughput historical data stores. The following figure shows a high-level
    architecture that incorporates these requirements in typical Spark-based batch
    and streaming applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00307.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Additionally, as organizations start employing big data and NoSQL-based solutions
    across a number of projects, a data layer comprising RDBMSes alone is no longer
    considered the best fit for all the use-cases in a modern enterprise application.
    RDBMS-only based architectures illustrated in the following figure are rapidly
    disappearing across the industry, in order to meet the requirements of typical
    big-data applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00308.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'A more typical scenario comprising of multiple types of data store is shown
    in the next figure. Applications today use several types of data store that represent
    the best fit for a given set of use cases. Using multiple data storage technologies,
    chosen based on the way, data is being used by applications, is called **polyglot
    persistence**. Spark SQL is an excellent enabler of this and other similar persistence
    strategies in the cloud or on-premise deployments:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00309.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Additionally, we observe that only a small fraction of real-world ML systems
    are composed of ML code (the smallest box in the following figure). However, the
    infrastructure surrounding this ML code is vast and complex. Later in this chapter,
    we will use Spark SQL to create some of the key parts in such applications, including
    scalable ETL pipelines and monitoring solutions. Subsequently, we will also discuss
    the production deployment of machine learning pipelines, and the use of cluster
    managers such as Mesos and Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00310.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Reference: "Hidden Technical Debt in Machine Learning Systems," Google NIPS
    2015'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss the key concepts and challenges in Spark-based
    batch and stream processing architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Using Apache Spark for batch processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Typically, batch processing is done on huge volumes of data to create batch
    views in order to support ad hoc querying and MIS reporting functionality, and/or
    to apply scalable machine learning algorithms for classification, clustering,
    collaborative filtering, and analytics applications.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the data volume involved in batch processing, these applications are
    typically long-running jobs and can easily extend over hours, days, or weeks,
    for example, aggregation queries such as count of daily visitors to a page, unique
    visitors to a website, and total sales per week.
  prefs: []
  type: TYPE_NORMAL
- en: Increasingly, Apache Spark is becoming popular as the engine for large-scale
    data processing. It can run programs up to 100x faster than Hadoop MapReduce in
    memory, or 10x faster on disk. An important reason for the rapid adoption of Spark
    is the common/similar coding required to address both batch and stream processing
    requirements.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will introduce the key characteristics and concepts
    of stream processing.
  prefs: []
  type: TYPE_NORMAL
- en: Using Apache Spark for stream processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most modern businesses are trying to deal with high data volumes (and associated
    rapid and unbounded growth of such data), combined with low-latency processing
    requirements. Additionally, higher value is being associated with near real-time
    business insights derived from real-time streaming data than traditional batch
    processed MIS reports. In contrast to streaming systems, the traditional batch
    processing systems were designed to process large amounts of a set of bounded
    data. Such systems are provided with all the data they need at the beginning of
    the execution. As the input data grows continuously, the results provided by such
    batch systems become dated, quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, in stream processing, data is not collected over significant time
    periods before triggering the required processing. Commonly, the incoming data
    is moved to a queuing system, such as Apache Kafka or Amazon Kinesis. This data
    is then accessed by the stream processor, which executes certain computations
    on it to generate the resulting output. A typical stream processing pipeline creates
    incremental views, which are often updated based on the incremental data flowing
    into the system.
  prefs: []
  type: TYPE_NORMAL
- en: 'The incremental views are made available through a **Serving Layer** to support
    querying and real-time analytics requirements, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00311.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'There are two types of time that are important in stream processing systems:
    event time and processing time. Event time is the time at which the events actually
    occurred (at source), while processing time is the time when the events are observed
    in the processing system. Event time is typically embedded in the data itself,
    and for many use cases, it is the time you want to operate on. However, extracting
    event time from data, and handling late or out-of-order data can present significant
    challenges in streaming applications. Additionally, there is a skew between the
    event times and the processing times due to resource limitations, the distributed
    processing model, and so on. There are many use cases requiring aggregations by
    event time; for example, the number of system errors in one-hour windows.'
  prefs: []
  type: TYPE_NORMAL
- en: There can be other issues as well; for example, in windowing functionality,
    we need to determine whether all the data for a given event time has been observed
    yet. These systems need to be designed in a manner that allow them to function
    well in uncertain environments. For example, in Spark Structured Streaming, event-time,
    window-based aggregation queries can be defined consistently for a data stream
    because it can handle late arriving data, and update older aggregates appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: Fault tolerance is crucial when dealing with large data streaming applications,
    for example, a stream processing job that keeps a count of all the tuples it has
    seen so far. Here, each tuple may represent a stream of user activity, and the
    application may want to report the total activity seen so far. A node failure
    in such a system can result in an inaccurate count because of the unprocessed
    tuples (on the failed node).
  prefs: []
  type: TYPE_NORMAL
- en: A naive way to recover from this situation would be to replay the entire Dataset.
    This is a costly operation given the size of data involved. Checkpointing is a
    common technique used to avoid reprocessing the entire Dataset. In the case of
    failures, the application data state is reverted to the last checkpoint, and the
    tuples from that point on, are replayed. To prevent data loss in Spark Streaming
    applications, a **write-ahead log** (**WAL**) is used, from which data can be
    replayed after failures.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will introduce the Lambda architecture, which is a popular
    pattern implemented in Spark-centric applications, as it can address requirements
    of both, batch and stream processing, using very similar code.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Lambda architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Lambda architectural pattern attempts to combine the best of both worlds--batch
    processing and stream processing. This pattern consists of several layers: **Batch
    Layer** (ingests and processes data on persistent storage such as HDFS and S3),
    **Speed Layer** (ingests and processes streaming data that has not been processed
    by the **Batch Layer** yet), and the **Serving Layer** (combines outputs from
    the **Batch** and **Speed Layers** to present merged results). This is a very
    popular architecture in Spark environments because it can support both the **Batch**
    and **Speed Layer** implementations with minimal code differences between the
    two.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The given figure depicts the Lambda architecture as a combination of batch
    processing and stream processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00312.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The next figure shows an implementation of the Lambda architecture using AWS
    Cloud services (**Amazon Kinesis**, **Amazon S3** Storage, **Amazon EMR**, **Amazon
    DynamoDB**, and so on) and Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00313.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: For more details on the AWS implementation of Lambda architecture, refer to [https://d0.awsstatic.com/whitepapers/lambda-architecure-on-for-batch-aws.pdf](https://d0.awsstatic.com/whitepapers/lambda-architecure-on-for-batch-aws.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss a simpler architecture called Kappa Architecture,
    which dispenses with the **Batch Layer** entirely and works with stream processing
    in the **Speed Layer** only.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Kappa Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Kappa Architecture** is simpler than the Lambda pattern as it comprises
    the Speed and Serving Layers only. All the computations occur as stream processing
    and there are no batch re-computations done on the full Dataset. Recomputations
    are only done to support changes and new requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, the incoming real-time data stream is processed in memory and is
    persisted in a database or HDFS to support queries, as illustrated in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00314.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The Kappa Architecture can be realized by using Apache Spark combined with a
    queuing solution, such as Apache Kafka. If the data retention times are bound
    to several days to weeks, then Kafka could also be used to retain the data for
    the limited period of time.
  prefs: []
  type: TYPE_NORMAL
- en: In the next few sections, we will introduce a few hands-on exercises using Apache
    Spark, Scala, and Apache Kafka that are very useful in the real-world applications
    development context. We will start by using Spark SQL and Structured Streaming
    to implement a few streaming use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Design considerations for building scalable stream processing applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Building robust stream processing applications is challenging. The typical
    complexities associated with stream processing include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Complex Data**: Diverse data formats and the quality of data create significant
    challenges in streaming applications. Typically, the data is available in various
    formats, such as JSON, CSV, AVRO, and binary. Additionally, dirty data, or late
    arriving, and out-of-order data, can make the design of such applications extremely
    complex.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complex workloads**: Streaming applications need to support a diverse set
    of application requirements, including interactive queries, machine learning pipelines,
    and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complex systems**: With diverse storage systems, including Kafka, S3, Kinesis,
    and so on, system failures can lead to significant reprocessing or bad results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Steam processing using Spark SQL can be fast, scalable, and fault-tolerant.
    It provides an extensive set of high-level APIs to deal with complex data and
    workloads. For example, the data sources API can integrate with many storage systems
    and data formats.
  prefs: []
  type: TYPE_NORMAL
- en: For a detailed coverage of building scalable and fault-tolerant structured streaming applications,
    refer to [https://spark-summit.org/2017/events/easy-scalable-fault-tolerant-stream-processing-with-structured-streaming-in-apache-spark/](https://spark-summit.org/2017/events/easy-scalable-fault-tolerant-stream-processing-with-structured-streaming-in-apache-spark/).
  prefs: []
  type: TYPE_NORMAL
- en: A streaming query allows us to specify one or more data sources, transform the
    data using DataFrame/Dataset APIs or SQL, and specify various sinks to output
    the results. There is built-in support for several data sources, such as files,
    Kafka, and sockets, and we can also combine multiple data sources, if required.
  prefs: []
  type: TYPE_NORMAL
- en: The Spark SQL Catalyst optimizer figures out the mechanics of incrementally
    executing the transformations. The query is converted to a series of incremental
    execution plans that operate on the new batches of data. The sink accepts the
    output of each batch and the updates are completed within a transaction context.
    You can also specify various output modes (**Complete**, **Update**, or **Append**)
    and triggers to govern when to output the results. If no trigger is specified,
    the results are continuously updated. The progress of a given query, and restarts
    after failures are managed by persisting checkpoints.
  prefs: []
  type: TYPE_NORMAL
- en: Spark structured streaming enables streaming analytics without having to worry
    about the complex underlying mechanisms that make streaming work. In this model,
    the input can be thought of as data from an append-only table (that grows continuously).
    A trigger specifies the time interval for checking the input for the arrival of
    new data and the query represents operations such as map, filter, and reduce on
    the input. The result represents the final table that is updated in each trigger
    interval (as per the specified query operation).
  prefs: []
  type: TYPE_NORMAL
- en: For detailed illustrations on structured streaming internals, check out [http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html](http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html).
  prefs: []
  type: TYPE_NORMAL
- en: We can also execute sliding window operations on streaming data. Here, we define
    aggregations over a sliding window, in which we group the data and compute appropriate
    aggregations (for each group).
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss Spark SQL features that can help in building
    robust ETL pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Building robust ETL pipelines using Spark SQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ETL pipelines execute a series of transformations on source data to produce
    cleansed, structured, and ready-for-use output by subsequent processing components. The
    transformations required to be applied on the source will depend on nature of
    the data. The input or source data can be structured (RDBMS, Parquet, and so on),
    semi-structured (CSV, JSON, and so on) or unstructured data (text, audio, video,
    and so on).  After being processed through such pipelines, the data is ready for
    downstream data processing, modeling, analytics, reporting, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure illustrates an application architecture in which the input
    data from Kafka, and other sources such as application and server logs, are cleansed
    and transformed (using an ETL pipeline) before being stored in an enterprise data
    store. This data store can eventually feed other applications (via Kafka), support
    interactive queries, store subsets or views of the data in serving databases,
    train ML models, support reporting applications, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00315.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As the abbreviation (ETL) suggests, we need to retrieve the data from various
    sources (Extract), transform the data for downstream consumption (Transform),
    and transmit it to different destinations (Load).
  prefs: []
  type: TYPE_NORMAL
- en: Over the next few sections, we will use Spark SQL features to access and process
    various data sources and data formats for ETL purposes. Spark SQL's flexible APIs,
    combined with the Catalyst optimizer and tungsten execution engine, make it highly
    suitable for building end-to-end ETL pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code block, we present a simple skeleton of a single ETL query
    that combines all the three (Extract, Transform, and Load) functions. These queries
    can also be extended to execute complex joins between tables containing data from
    multiple sources and source formats:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will introduce a few criteria that can help you make
    appropriate choices regarding data formats to satisfy the requirements of your
    specific use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing appropriate data formats
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In enterprise settings, the data is available in many different data sources
    and formats. Spark SQL supports a set of built-in and third-party connectors.
    In addition, we can also define custom data source connectors. Data formats include
    structured, semi-structured, and unstructured formats, such as plain text, JSON,
    XML, CSV, RDBMS records, images, and video. More recently, big data formats such
    as Parquet, ORC, and Avro are becoming increasingly popular. In general, unstructured
    formats such as plain text files are more flexible, while structured formats such
    as Parquet and AVRO are more efficient from a storage and performance perspective.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of structured data formats, the data has a rigid, well-defined schema
    or structure associated with it. For example, columnar data formats make it more
    efficient to extract values from columns. However, this rigidity can make changes
    to the schema, or the structure, challenging. By contrast, unstructured data sources,
    such as free-form text, contain no markup or separators as in CSV or TSV files.
    Such data sources generally require some context around the data; for example,
    you need to know that the contents of files contain text from blogs.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, we need many transformations and feature extraction techniques to
    interpret diverse Datasets. Semi-structured data is structured at a record level,
    but not necessarily across all the records. As a result, each data record contains
    the associated schema information as well.
  prefs: []
  type: TYPE_NORMAL
- en: The JSON format is probably the most common example of semi-structured data.
    JSON records are in a human-readable form, making it more convenient for development
    and debugging purposes. However, these formats suffer from parsing-related overheads,
    and are typically not the best choice for supporting the ad hoc querying functionality.
  prefs: []
  type: TYPE_NORMAL
- en: Often, applications will have to be designed to span and traverse across varied
    data sources and formats to efficiently store and process the data. For example,
    Avro is a good choice when access is required to complete rows of data, as in
    the case of access to features in an ML pipeline. In cases where flexibility in
    the schema is required, using JSON may be the most appropriate choice for the
    data format. Furthermore, in cases where the data does not have a fixed schema,
    it is probably best to use the plain text file format.
  prefs: []
  type: TYPE_NORMAL
- en: Transforming data in ETL pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Typically, semi-structured formats such as JSON contain struct, map, and array
    data types; for example, request and/or response payloads for REST web services
    contain JSON data with nested fields and arrays.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will present examples of Spark SQL-based transformations
    on Twitter data. The input Dataset is a file (`cache-0.json.gz`) containing `10
    M` tweets from a set of Datasets containing over `170 M` tweets collected during
    the three months leading up to the 2012 US presidential elections. This file can
    be downloaded from [https://datahub.io/dataset/twitter-2012-presidential-election](https://datahub.io/dataset/twitter-2012-presidential-election).
  prefs: []
  type: TYPE_NORMAL
- en: 'Before starting with the following examples, start Zookeeper and the Kafka
    broker as described in [Chapter 5](part0085.html#2H1VQ0-e9cbc07f866e437b8aa14e841622275c),
    *Using Spark SQL in Streaming Applications*. Also, create a new Kafka topic, called
    tweetsa. We generate the schema from the input JSON Dataset, as shown. This schema
    definition will be used later in this section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Set up to read the streaming tweets from the Kafka topic (*tweetsa*), and then
    parse the JSON data using the schema from the previous step.
  prefs: []
  type: TYPE_NORMAL
- en: 'We select all the fields in the tweet by `specifying data.*` in this statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'You will need to use the following command repeatedly (as you work through
    the examples) to pipe the tweets contained in the input file to the Kafka topic,
    as illustrated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Given the size of the input file, this can potentially result in space-related
    issues on your machine. If this happens, use appropriate Kafka commands to delete
    and recreate the topic (refer to [https://kafka.apache.org/0102/documentation.html](https://kafka.apache.org/0102/documentation.html)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we reproduce a section of the schema to help understand the structure
    we are working with over the next few examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00316.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can select specific fields from nested columns in the JSON string. We use
    the . (dot) operator to choose the nested field, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we write the output stream to the screen to view the results. You will
    need to execute the following statement after each of the transformations in order
    to view and evaluate the results. Also, in the interests of saving time, you should
    execute `s5.stop()`, after you have seen sufficient output on the screen. Alternatively,
    you can always choose to work with a smaller set of data extracted from the original
    input file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00317.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'In the next example, we will flatten a struct using star (*) to select all
    the subfields in the struct:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The results can be viewed by writing the output stream, as shown in the preceding
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00318.gif)'
  prefs: []
  type: TYPE_IMG
- en: We can use the struct function to create a new struct (for nesting the columns)
    as illustrated in the following code snippet. We can select a specific field or
    fields for creating the new struct. We can also nest all the columns using star
    (*), if required.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we reproduce the section of the schema used in this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00319.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00320.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'In the next example, we select a single array (or map) element using `getItem()`.
    Here, we are operating on the following part of the schema:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00321.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00322.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00323.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'We can use the `explode()` function to create a new row for each element in
    an array, as shown. To illustrate the results of `explode()`, we first show the
    rows containing the arrays, and then show the results of applying the explode
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output is obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00324.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Note the separate rows created for the array elements after applying the explode
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00325.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Spark SQL also has functions such as `to_json()`, to transform a `struct` to
    a JSON string, and `from_json()`, to convert a JSON string to a `struct`. These
    functions are very useful to read from or write to Kafka topics. For example,
    if the "value" field contains data in a JSON string, then we can use the `from_json()`
    function to extract the data, transform it, and then push it out to a different
    Kafka topic, and/or write it out to a Parquet file or a serving database.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we use the `to_json()` function to convert a struct
    to a JSON string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00326.gif)'
  prefs: []
  type: TYPE_IMG
- en: We can use the `from_json()` function to convert a column containing JSON data
    into a `struct` data type. Further, we can flatten the preceding struct into separate
    columns. We show an example of using this function in a later section.
  prefs: []
  type: TYPE_NORMAL
- en: For more detailed coverage of transformation functions, refer to [https://databricks.com/blog/2017/02/23/working-complex-data-formats-structured-streaming-apache-spark-2-1.html](https://databricks.com/blog/2017/02/23/working-complex-data-formats-structured-streaming-apache-spark-2-1.html).
  prefs: []
  type: TYPE_NORMAL
- en: Addressing errors in ETL pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ETL tasks are usually considered to be complex, expensive, slow, and error-prone.
    Here, we will examine typical challenges in ETL processes, and how Spark SQL features
    assist in addressing them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark can automatically infer the schema from a JSON file. For example, for
    the following JSON data, the inferred schema includes all the labels and the data
    types based on the content. Here, the data types for all the elements in the input
    data are longs by default:'
  prefs: []
  type: TYPE_NORMAL
- en: '**test1.json**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'You can print the schema to verify the data types, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'However, in the following JSON data, if the value of `e` in the third row and
    the value of `b` in the last row are changed to include fractions, and the value
    of `f` in the second-from-last row is enclosed in quotes, the inferred schema
    changes the data types of `b` and `e` to double, and `f` to string type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'If we want to associate specific structure or data types to elements, we need
    to use a user-specified schema. In the next example, we use a CSV file with a
    header containing the field names. The field names in the schema are derived from
    the header, and the data types specified in the user-defined schema are used against
    them, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output is obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00327.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Issues can also occur in ETL pipelines due to file and data corruption. If the
    data is not mission-critical, and the corrupt files can be safely ignored, we
    can set `config property spark.sql.files.ignoreCorruptFiles = true`. This setting
    lets Spark jobs continue running even when corrupt files are encountered. Note
    that contents that are successfully read will continue to be returned.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, there is bad data for `b` in row `4`. We can still
    read the data using the `PERMISSIVE` mode. In this case, a new column, called
    `_corrupt_record`, is added to the DataFrame, and the contents of the corrupted
    rows appear in that column with the rest of the fields initialized to nulls. We
    can focus on the data issues by reviewing the data in this column and initiate
    suitable actions to fix them. By setting the `spark.sql.columnNameOfCorruptRecord`
    property, we can configure the default name of the corrupted contents column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00328.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we use the `DROPMALFORMED` option to drop all malformed records. Here,
    the fourth row is dropped due to the bad value for `b`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00329.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'For critical data, we can use the `FAILFAST` option to fail immediately upon
    encountering a bad record. For example, in the following example, due to the value
    of `b` in the fourth row, the operation throws an exception and exits immediately:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following example, we have a record that spans two rows; we can read
    this record by setting the `wholeFile` option to true:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: For more details on Spark SQL-based ETL pipelines and roadmaps, visit [https://spark-summit.org/2017/events/building-robust-etl-pipelines-with-apache-spark/](https://spark-summit.org/2017/events/building-robust-etl-pipelines-with-apache-spark/).
  prefs: []
  type: TYPE_NORMAL
- en: The preceding reference presents several higher-order SQL transformation functions,
    new formats for the DataframeWriter API, and a unified `Create Table` (as `Select`)
    constructs in Spark 2.2 and 2.3-Snapshot.
  prefs: []
  type: TYPE_NORMAL
- en: Other requirements addressed by Spark SQL include scalability and continuous
    ETL using structured streaming. We can use structured streaming to enable raw
    data to be available as structured data ready for analysis, reporting, and decision-making
    as soon as possible, instead of incurring the hours of delay typically associated
    with running periodic batch jobs. This type of processing is especially important
    in applications such as anomaly detection, fraud detection, and so on, where time
    is of the essence.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will shift our focus to building a scalable monitoring
    solution using Spark SQL.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a scalable monitoring solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building a scalable monitoring function for large-scale deployments can be challenging
    as there could be billions of data points captured each day. Additionally, the
    volume of logs and the number of metrics can be difficult to manage without a
    suitable big data platform with streaming and visualization support.
  prefs: []
  type: TYPE_NORMAL
- en: Voluminous logs collected from applications, servers, network devices, and so
    on are processed to provide real-time monitoring that help detect errors, warnings,
    failures, and other issues. Typically, various daemons, services, and tools are
    used to collect/send log records to the monitoring system. For example, log entries
    in the JSON format can be sent to Kafka queues or Amazon Kinesis. These JSON records
    can then be stored on S3 as files and/or streamed to be analyzed in real time
    (in a Lambda architecture implementation). Typically, an ETL pipeline is run to
    cleanse the log data, transform it into a more structured form, and then load
    it into files such as Parquet files or databases, for querying, alerting, and
    reporting purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure illustrates one such platform using **Spark Streaming
    Jobs**, a **Scalable Time Series Database** such as OpenTSDB or Graphite, and
    **Visualization Tools** such as Grafana:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00330.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: For more details on this solution, refer to [https://spark-summit.org/2017/events/scalable-monitoring-using-apache-spark-and-friends/](https://spark-summit.org/2017/events/scalable-monitoring-using-apache-spark-and-friends/).
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring and troubleshooting problems are challenging tasks in large distributed
    environments comprising of several Spark clusters with varying configurations
    and versions, running different types of workloads. In these environments hundreds
    of thousands metrics may be received. Additionally, hundreds of MBs of logs are
    generated per second. These metrics need to be tracked and the logs analyzed for
    anomalies, failures, bugs, environmental issues, and so on to support alerting
    and troubleshooting functions.
  prefs: []
  type: TYPE_NORMAL
- en: The following figure illustrates an AWS-based data pipeline that pushes all
    the metrics and logs (both structured and unstructured) to Kinesis. A structured
    streaming job can read the raw logs from Kinesis and save the data as Parquet
    files on S3.
  prefs: []
  type: TYPE_NORMAL
- en: 'The structured streaming query can strip known error patterns and raise suitable
    alerts, if a new error type is observed. Other Spark batch and streaming applications
    can use these Parquet files to perform additional processing and output their
    results as new Parquet files on S3:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00331.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In this architecture, discovering issues from unstructured logs may be required
    to determine their scope, duration, and impact. **Raw Logs** typically contain
    many near-duplicate error messages. For efficient processing of these logs, we
    need to normalize, deduplicate, and filter out well-known error conditions to
    discover and reveal new ones.
  prefs: []
  type: TYPE_NORMAL
- en: For details on a pipeline to process raw logs, refer to [https://spark-summit.org/2017/events/lessons-learned-from-managing-thousands-of-production-apache-spark-clusters-daily/](https://spark-summit.org/2017/events/lessons-learned-from-managing-thousands-of-production-apache-spark-clusters-daily/).
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will explore some of the features Spark SQL and Structured
    Streaming provide to create a scalable monitoring solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, start the Spark shell with the Kafka packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Download the traces for the month of July, 1995, containing HTTP requests to
    the NASA Kennedy Space Center WWW server in Florida from [http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html](http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the following packages for the hands-on exercises in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, define the schema for the records in the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'For simplicity, we read the input file as a CSV file with a space separator,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create a DataFrame containing the log events. As the timestamp changes
    to the local time zone (by default) in the preceding step, we also retain the
    original timestamp with time zone information in the `original_dateTime` column,
    as illustrated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We can check the results of the streaming read, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00332.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'We can save the streaming input to Parquet files, partitioned by date to support
    queries more efficiently, as demonstrated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We can read input so that the latest records are available first by specifying
    the `latestFirst` option:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also write out the output in the JSON format, partitioned by date, easily,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we show the use of Kafka for input and output in streaming Spark applications.
    Here, we have to specify the format parameter as `kafka`, and the kafka broker
    and topic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we are reading a stream of JSON data from Kafka. The starting offset is
    set to the earliest to specify the starting point for our query. This applies
    only when a new streaming query is started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We can print out the schema for records read from Kafka, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define the schema for input records, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can specify the schema, as illustrated. The star `*` operator is used
    to select all the `subfields` in a `struct`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we show an example of selecting specific fields. Here, we set the `outputMode`
    to append so that only the new rows appended to the result table are written out
    to external storage. This is applicable only on queries where the existing rows
    in the result table are not expected to change:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00333.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also specify `read` (not `readStream`) to read the records into a regular
    DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now do all the standard DataFrame operations against this DataFrame;
    for example, we create a table and query it, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00334.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, we read the records from Kafka and apply the schema:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We can execute the following query to check the contents of the records:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00335.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'We can select all the fields from the records, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also select specific fields of interest from the DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can use window operations, as demonstrated, and maintain counts for
    various HTTP codes. Here, we use `outputMode` set to `complete` since we want
    the entire updated result table to be written to the external storage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00336.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we show another example of using `groupBy` and computed counts for various
    page requests in these windows. This can be used to compute and report the top
    pages visited type metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00337.gif)'
  prefs: []
  type: TYPE_IMG
- en: Note that the examples presented earlier are instances of stateful processing.
    The counts have to be saved as a distributed state between triggers. Each trigger
    reads the previous state and writes the updated state. This state is stored in
    memory, and is backed by the persistent WAL, typically located on HDFS or S3 storage.
    This allows the streaming application to automatically handle data arriving late.
    Keeping this state allows the late data to update the counts of old windows.
  prefs: []
  type: TYPE_NORMAL
- en: However, the size of the state can increase indefinitely, if the old windows
    are not dropped. A watermarking approach is used to address this issue. A watermark
    is a moving threshold of how late data is expected to be and when to drop the
    old state. It trails behind the max seen event time. Data newer than watermark
    may be late, but is allowed into the aggregate, while data older than the watermark
    is considered "too late" and dropped. Additionally, windows older than the watermark
    are automatically deleted to limit the amount of intermediate state that is required
    to be maintained by the system.
  prefs: []
  type: TYPE_NORMAL
- en: 'A watermark specified for the previous query is given here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: For more details on watermarking, refer to [https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html](https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html).
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will shift our focus to deploying Spark-based machine
    learning pipelines in production.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Spark machine learning pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following figure illustrates a machine learning pipeline at a conceptual
    level. However, real-life ML pipelines are a lot more complicated, with several
    models being trained, tuned, combined, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00338.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The next figure shows the core elements of a typical machine learning application
    split into two parts: the modeling, including model training, and the deployed
    model (used on streaming data to output the results):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00339.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Typically, data scientists experiment or do their modeling work in Python and/or
    R. Their work is then reimplemented in Java/Scala before deployment in a production
    environment. Enterprise production environments often consist of web servers,
    application servers, databases, middleware, and so on. The conversion of prototypical
    models to production-ready models results in additional design and development
    effort that lead to delays in rolling out updated models.
  prefs: []
  type: TYPE_NORMAL
- en: We can use Spark MLlib 2.x model serialization to directly use the models and
    pipelines saved by data scientists (to disk) in production environments by loading
    them from the persisted model files.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example (source: [https://spark.apache.org/docs/latest/ml-pipeline.html](https://spark.apache.org/docs/latest/ml-pipeline.html)),
    we will illustrate creating and saving a ML pipeline in Python (using `pyspark`
    shell) and then retrieving it in a Scala environment (using Spark shell).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start the `pyspark` shell and execute the following sequence of Python statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Start the Spark shell and execute the following sequence of Scala statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create a `test` Dataset and run it through the ML pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The results of running the model on the `test` Dataset are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The key parameters of the saved logistic regression model are read into a DataFrame,
    as illustrated in the following code block. Earlier, when the model was saved
    in the `pyspark` shell, these parameters were saved to a Parquet file located
    in a subdirectory associated with the final stage of our pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output is obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00340.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00341.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: For more details on how to productionize ML models, refer to [https://spark-summit.org/2017/events/how-to-productionize-your-machine-learning-models-using-apache-spark-mllib-2x/](https://spark-summit.org/2017/events/how-to-productionize-your-machine-learning-models-using-apache-spark-mllib-2x/).
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the challenges in typical ML deployment environments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Production deployment environments for ML models can be very diverse and complex.
    For example, models may need to be deployed in web applications, portals, real-time
    and batch processing systems, and as an API or a REST service, embedded in devices
    or in large legacy environments.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, enterprise technology stacks can comprise of Java Enterprise,
    C/C++, legacy mainframe environments, relational databases, and so on. The non-functional
    requirements and customer SLAs with respect to response times, throughput, availability,
    and uptime can also vary widely. However, in almost all cases, our deployment
    process needs to support A/B testing, experimentation, model performance evaluation,
    and be agile and responsive to business needs.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, practitioners use various methods to benchmark and phase-in new or
    updated models to avoid high-risk, big bang production deployments.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explore a few model deployment architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding types of model scoring architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The simplest model is to precompute model results using Spark (batch processing),
    save the results to a database, and then serve the results to web and mobile applications
    from the database. Many large-scale recommendation engines and search engines
    use this architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00342.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'A second model scoring architecture computes the features and runs prediction
    algorithms using Spark Streaming. The prediction results can be cached using caching
    solutions, such as Redis, and can be made available via an API. Other applications
    can then use these APIs to obtain the prediction results from the deployed model.
    This option is illustrated in this figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00343.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In a third architectural model, we can use Spark for model training purposes
    only. The model is then copied into the production environment. For example, we
    can load the coefficients and intercept of a logistic regression model from a
    JSON file. This approach is resource-efficient and results in a high-performing
    system. It is also a lot easier to deploy in existing or complex environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is illustrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00344.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Continuing with our earlier example, we can read in the saved model parameters
    from a Parquet file and convert it to a JSON format that can, in turn, be conveniently
    imported into any application (inside or outside the Spark environment) and applied
    to new data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'We can display the intercept, coefficients, and other key parameters using
    standard OS commands, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00345.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As models are becoming bigger and more complex, it can be challenging to deploy
    and serve them. Models may not scale well, and their resource requirements can
    become very expensive. Databricks and Redis-ML provide solutions to deploy the
    trained model.
  prefs: []
  type: TYPE_NORMAL
- en: In the Redis-ML solution, the model is applied to the new data directly in the
    Redis environment.
  prefs: []
  type: TYPE_NORMAL
- en: This can provide the required overall performance, scalability, and availability
    at a much lower price point than running the model in a Spark environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows Redis-ML being used as a serving engine (implementing
    the third model scoring architectural pattern, as described earlier):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00346.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the next section, we will briefly discuss using Mesos and Kubernetes as cluster
    managers in production environments.
  prefs: []
  type: TYPE_NORMAL
- en: Using cluster managers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will briefly discuss Mesos and Kubernetes at a conceptual
    level. The Spark framework can be deployed through Apache **Mesos**, **YARN**,
    Spark Standalone, or the **Kubernetes** cluster manager, as depicted:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00347.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Mesos can enable easy scalability and replication of data, and is a good unified
    cluster management solution for heterogeneous workloads.
  prefs: []
  type: TYPE_NORMAL
- en: To use Mesos from Spark, the Spark binaries should be accessible by Mesos and
    the Spark driver configured to connect to Mesos. Alternatively, you can also install
    Spark binaries on all the Mesos slaves. The driver creates a job and then issues
    the tasks for scheduling, while Mesos determines the machines to handle them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark can run over Mesos in two modes: coarse-grained (the default) and fine-grained
    (deprecated in Spark 2.0.0). In the coarse-grained mode, each Spark executor runs
    as a single Mesos task. This mode has significantly lower start up overheads,
    but reserves Mesos resources for the duration of the application. Mesos also supports
    dynamic allocation where the number of executors is adjusted based on the statistics
    of the application.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure illustrates a deployment that collocates the **Mesos Master**
    and **Zookeeper** nodes. The **Mesos Slave** and **Cassandra Node** are also collocated
    for better data locality. In addition, the Spark binaries are deployed on all
    worker nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00348.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Another emerging Spark cluster management solution is Kubernetes, which is being
    developed as a native cluster manager for Spark. It is an open source system that
    can be used for automating the deployment, scaling, and management of containerized
    Spark applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next figure depicts a high-level view of Kubernetes. Each node contains
    a daemon, called a **Kublet**, which talks to the **Master** node. The users also
    talks to the Master to declaratively specify what they want to run. For example,
    a user can request running a specific number of web server instances. The Master
    will take the user''s request and schedule the workload on the nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00349.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The nodes run one or more pods. A pod is a higher-level abstraction on containers,
    and each pod can contain a set of colocated containers. Each pod has its own IP
    address and can communicate with the pods in other nodes. The storage volumes
    can be local or network-attached. This can be seen in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00350.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Kubernetes promotes resource sharing between different types of Spark workload
    to reduce operational costs and improve infrastructure utilization. In addition,
    several add-on services can be used out-of-the-box with Spark applications, including
    logging, monitoring, security, container-to-container communications, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: For more details on Spark on Kubernetes, visit [https://github.com/apache-spark-on-k8s/spark](https://github.com/apache-spark-on-k8s/spark).
  prefs: []
  type: TYPE_NORMAL
- en: In the following figure, the dotted line separates Kubernetes from Spark. Spark
    Core is responsible for getting new executors, pushing new configurations, removing
    executors, and so on. The **Kubernetes Scheduler Backend** takes the Spark Core
    requests and converts them into primitives that Kubernetes can understand. Additionally,
    it handles all resource requests and all communications with Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other services, such as a File Staging Server, can make your local files and
    JARs available to the Spark cluster, and the Spark shuffle service can store shuffle
    data for the dynamic allocation of resources; for example, it can enable elastically
    changing the number of executors in a particular stage. You can also extend the
    Kubernetes API to include custom or application-specific resources; for example,
    you can create dashboards to display the progress of jobs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00351.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Kubernetes also provides useful administrative features to help manage clusters,
    for example, RBAC and namespace-level resource quotas, audit logging, monitoring
    node, pod, cluster-level metrics, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we presented several Spark SQL-based application architectures
    for building highly-scalable applications. We explored the main concepts and challenges
    in batch processing and stream processing. We discussed the features of Spark
    SQL that can help in building robust ETL pipelines. We also presented some code
    towards building a scalable monitoring application. Additionally, we explored
    an efficient deployment technique for machine learning pipelines, and some basic
    concepts involved in using cluster managers such as Mesos and Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, this book attempts to help you build a strong foundation in Spark
    SQL and Scala. However, there are still many areas that you can explore in greater
    depth to build deeper expertise. Depending on your specific domain, the nature
    of data and problems could vary widely and your approach to solving them would
    typically encompass one or more areas described in this book. However, in all
    cases EDA and data munging skills will be required, and the more you practice,
    the more proficient you will become at it. Try downloading and working with different
    types of data covering structured, semi-structured and unstructured data. Additionally,
    read the references mentioned in all the chapters to get deeper insights into
    how other data science practitioners approach problems. Reference Apache Spark
    site for latest releases of the software, and explore other machine learning algorithms
    you could use in your ML pipelines. Finally, topics such as deep learning and
    cost-based optimizations are still evolving in Spark, try to keep up with the
    developments in these areas as they will be key to solving many interesting problems
    in the near future.
  prefs: []
  type: TYPE_NORMAL
