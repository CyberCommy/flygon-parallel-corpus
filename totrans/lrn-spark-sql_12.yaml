- en: Spark SQL in Large-Scale Application Architectures
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark SQL在大规模应用程序架构中的应用
- en: In this book, we started with the basics of Spark SQL and its components, and
    its role in Spark applications. Later, we presented a series of chapters focusing
    on its usage in various types of applications. With DataFrame/Dataset API and
    the Catalyst optimizer at the heart of Spark SQL, it is no surprise that it plays
    a key role in all applications based on the Spark technology stack. These applications
    include large-scale machine learning, large-scale graphs, and deep learning applications.
    Additionally, we presented Spark SQL-based Structured Streaming applications that
    operate in complex environments as continuous applications. In this chapter, we
    will explore application architectures that leverage Spark modules and Spark SQL
    in real-world applications.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们从Spark SQL及其组件的基础知识开始，以及它在Spark应用程序中的作用。随后，我们提出了一系列关于其在各种类型应用程序中的使用的章节。作为Spark
    SQL的核心，DataFrame/Dataset API和Catalyst优化器在所有基于Spark技术栈的应用程序中发挥关键作用，这并不奇怪。这些应用程序包括大规模机器学习、大规模图形和深度学习应用程序。此外，我们提出了基于Spark
    SQL的结构化流应用程序，这些应用程序作为连续应用程序在复杂环境中运行。在本章中，我们将探讨在现实世界应用程序中利用Spark模块和Spark SQL的应用程序架构。
- en: More specifically, we will cover key architectural components and patterns in
    large-scale applications that architects and designers will find useful as a starting
    point for their specific use-cases. We will describe the deployment of some of
    the main processing models being used for batch processing, streaming applications,
    and machine learning pipelines. The underlying architecture for these processing
    models is required to support ingesting very large volumes of various types of
    data arriving at high velocities at one end, while making the output data available
    for use by analytical tools, and reporting and modeling software at the other
    end. Additionally, we will present supporting code using Spark SQL for monitoring,
    troubleshooting, and gathering/reporting metrics.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地，我们将涵盖大规模应用程序中的关键架构组件和模式，这些对架构师和设计师来说将作为特定用例的起点。我们将描述一些用于批处理、流处理应用程序和机器学习管道的主要处理模型的部署。这些处理模型的基础架构需要支持在一端到达高速的各种类型数据的大量数据，同时在另一端使输出数据可供分析工具、报告和建模软件使用。此外，我们将使用Spark
    SQL提供支持代码，用于监控、故障排除和收集/报告指标。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章中涵盖以下主题：
- en: Understanding Spark-based batch and stream processing architectures
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解基于Spark的批处理和流处理架构
- en: Understanding Lambda and Kappa Architectures
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解Lambda和Kappa架构
- en: Implementing scalable stream processing with structured streaming
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用结构化流实现可扩展的流处理
- en: Building robust **Extract-Transform-Load** (**ETL**) pipelines using Spark SQL
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark SQL构建强大的ETL管道
- en: Implementing a scalable monitoring solution using Spark SQL
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark SQL实现可扩展的监控解决方案
- en: Deploying Spark machine learning pipelines
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署Spark机器学习管道
- en: 'Using cluster managers: Mesos and Kubernetes'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用集群管理器：Mesos和Kubernetes
- en: Understanding Spark-based application architectures
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解基于Spark的应用程序架构
- en: 'Apache Spark is an emerging platform that leverages distributed storage and
    processing frameworks to support querying, reporting, analytics, and intelligent
    applications at scale. Spark SQL has the necessary features, and supports the
    key mechanisms required, to access data across a diverse set of data sources and
    formats, and prepare it for downstream applications either with low-latency streaming
    data or high-throughput historical data stores. The following figure shows a high-level
    architecture that incorporates these requirements in typical Spark-based batch
    and streaming applications:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark是一个新兴的平台，利用分布式存储和处理框架来支持规模化的查询、报告、分析和智能应用。Spark SQL具有必要的功能，并支持所需的关键机制，以访问各种数据源和格式的数据，并为下游应用程序做准备，无论是低延迟的流数据还是高吞吐量的历史数据存储。下图显示了典型的基于Spark的批处理和流处理应用程序中包含这些要求的高级架构：
- en: '![](img/00307.jpeg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00307.jpeg)'
- en: 'Additionally, as organizations start employing big data and NoSQL-based solutions
    across a number of projects, a data layer comprising RDBMSes alone is no longer
    considered the best fit for all the use-cases in a modern enterprise application.
    RDBMS-only based architectures illustrated in the following figure are rapidly
    disappearing across the industry, in order to meet the requirements of typical
    big-data applications:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，随着组织开始在许多项目中采用大数据和NoSQL解决方案，仅由RDBMS组成的数据层不再被认为是现代企业应用程序所有用例的最佳选择。仅基于RDBMS的架构在下图所示的行业中迅速消失，以满足典型大数据应用程序的要求：
- en: '![](img/00308.jpeg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00308.jpeg)'
- en: 'A more typical scenario comprising of multiple types of data store is shown
    in the next figure. Applications today use several types of data store that represent
    the best fit for a given set of use cases. Using multiple data storage technologies,
    chosen based on the way, data is being used by applications, is called **polyglot
    persistence**. Spark SQL is an excellent enabler of this and other similar persistence
    strategies in the cloud or on-premise deployments:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了一个更典型的场景，其中包含多种类型的数据存储。如今的应用程序使用多种数据存储类型，这些类型最适合特定的用例。根据应用程序使用数据的方式选择多种数据存储技术，称为多语言持久性。Spark
    SQL在云端或本地部署中是这种和其他类似持久性策略的极好的实现者：
- en: '![](img/00309.jpeg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00309.jpeg)'
- en: 'Additionally, we observe that only a small fraction of real-world ML systems
    are composed of ML code (the smallest box in the following figure). However, the
    infrastructure surrounding this ML code is vast and complex. Later in this chapter,
    we will use Spark SQL to create some of the key parts in such applications, including
    scalable ETL pipelines and monitoring solutions. Subsequently, we will also discuss
    the production deployment of machine learning pipelines, and the use of cluster
    managers such as Mesos and Kubernetes:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们观察到，现实世界中只有一小部分ML系统由ML代码组成（下图中最小的方框）。然而，围绕这些ML代码的基础设施是庞大且复杂的。在本章的后面，我们将使用Spark
    SQL来创建这些应用程序中的一些关键部分，包括可扩展的ETL管道和监控解决方案。随后，我们还将讨论机器学习管道的生产部署，以及使用Mesos和Kubernetes等集群管理器：
- en: '![](img/00310.gif)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00310.gif)'
- en: 'Reference: "Hidden Technical Debt in Machine Learning Systems," Google NIPS
    2015'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 参考：“机器学习系统中的隐藏技术债务”，Google NIPS 2015
- en: In the next section, we will discuss the key concepts and challenges in Spark-based
    batch and stream processing architectures.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论基于Spark的批处理和流处理架构中的关键概念和挑战。
- en: Using Apache Spark for batch processing
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Apache Spark进行批处理
- en: Typically, batch processing is done on huge volumes of data to create batch
    views in order to support ad hoc querying and MIS reporting functionality, and/or
    to apply scalable machine learning algorithms for classification, clustering,
    collaborative filtering, and analytics applications.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，批处理是针对大量数据进行的，以创建批量视图，以支持特定查询和MIS报告功能，和/或应用可扩展的机器学习算法，如分类、聚类、协同过滤和分析应用。
- en: Due to the data volume involved in batch processing, these applications are
    typically long-running jobs and can easily extend over hours, days, or weeks,
    for example, aggregation queries such as count of daily visitors to a page, unique
    visitors to a website, and total sales per week.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 由于批处理涉及的数据量较大，这些应用通常是长时间运行的作业，并且很容易延长到几个小时、几天或几周，例如，聚合查询，如每日访问者数量、网站的独立访问者和每周总销售额。
- en: Increasingly, Apache Spark is becoming popular as the engine for large-scale
    data processing. It can run programs up to 100x faster than Hadoop MapReduce in
    memory, or 10x faster on disk. An important reason for the rapid adoption of Spark
    is the common/similar coding required to address both batch and stream processing
    requirements.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 越来越多的人开始将Apache Spark作为大规模数据处理的引擎。它可以在内存中运行程序，比Hadoop MapReduce快100倍，或者在磁盘上快10倍。Spark被迅速采用的一个重要原因是，它需要相似的编码来满足批处理和流处理的需求。
- en: In the next section, we will introduce the key characteristics and concepts
    of stream processing.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍流处理的关键特征和概念。
- en: Using Apache Spark for stream processing
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Apache Spark进行流处理
- en: Most modern businesses are trying to deal with high data volumes (and associated
    rapid and unbounded growth of such data), combined with low-latency processing
    requirements. Additionally, higher value is being associated with near real-time
    business insights derived from real-time streaming data than traditional batch
    processed MIS reports. In contrast to streaming systems, the traditional batch
    processing systems were designed to process large amounts of a set of bounded
    data. Such systems are provided with all the data they need at the beginning of
    the execution. As the input data grows continuously, the results provided by such
    batch systems become dated, quickly.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现代企业都在努力处理大量数据（以及相关数据的快速和无限增长），同时还需要低延迟的处理需求。此外，与传统的批处理MIS报告相比，从实时流数据中获得的近实时业务洞察力被赋予了更高的价值。与流处理系统相反，传统的批处理系统旨在处理一组有界数据的大量数据。这些系统在执行开始时就提供了它们所需的所有数据。随着输入数据的不断增长，这些批处理系统提供的结果很快就会过时。
- en: Typically, in stream processing, data is not collected over significant time
    periods before triggering the required processing. Commonly, the incoming data
    is moved to a queuing system, such as Apache Kafka or Amazon Kinesis. This data
    is then accessed by the stream processor, which executes certain computations
    on it to generate the resulting output. A typical stream processing pipeline creates
    incremental views, which are often updated based on the incremental data flowing
    into the system.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在流处理中，数据在触发所需处理之前不会在显著的时间段内收集。通常，传入的数据被移动到排队系统，例如Apache Kafka或Amazon Kinesis。然后，流处理器访问这些数据，并对其执行某些计算以生成结果输出。典型的流处理管道创建增量视图，这些视图通常根据流入系统的增量数据进行更新。
- en: 'The incremental views are made available through a **Serving Layer** to support
    querying and real-time analytics requirements, as shown in the following diagram:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 增量视图通过**Serving Layer**提供，以支持查询和实时分析需求，如下图所示：
- en: '![](img/00311.jpeg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00311.jpeg)'
- en: 'There are two types of time that are important in stream processing systems:
    event time and processing time. Event time is the time at which the events actually
    occurred (at source), while processing time is the time when the events are observed
    in the processing system. Event time is typically embedded in the data itself,
    and for many use cases, it is the time you want to operate on. However, extracting
    event time from data, and handling late or out-of-order data can present significant
    challenges in streaming applications. Additionally, there is a skew between the
    event times and the processing times due to resource limitations, the distributed
    processing model, and so on. There are many use cases requiring aggregations by
    event time; for example, the number of system errors in one-hour windows.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: There can be other issues as well; for example, in windowing functionality,
    we need to determine whether all the data for a given event time has been observed
    yet. These systems need to be designed in a manner that allow them to function
    well in uncertain environments. For example, in Spark Structured Streaming, event-time,
    window-based aggregation queries can be defined consistently for a data stream
    because it can handle late arriving data, and update older aggregates appropriately.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Fault tolerance is crucial when dealing with large data streaming applications,
    for example, a stream processing job that keeps a count of all the tuples it has
    seen so far. Here, each tuple may represent a stream of user activity, and the
    application may want to report the total activity seen so far. A node failure
    in such a system can result in an inaccurate count because of the unprocessed
    tuples (on the failed node).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: A naive way to recover from this situation would be to replay the entire Dataset.
    This is a costly operation given the size of data involved. Checkpointing is a
    common technique used to avoid reprocessing the entire Dataset. In the case of
    failures, the application data state is reverted to the last checkpoint, and the
    tuples from that point on, are replayed. To prevent data loss in Spark Streaming
    applications, a **write-ahead log** (**WAL**) is used, from which data can be
    replayed after failures.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will introduce the Lambda architecture, which is a popular
    pattern implemented in Spark-centric applications, as it can address requirements
    of both, batch and stream processing, using very similar code.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Lambda architecture
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Lambda architectural pattern attempts to combine the best of both worlds--batch
    processing and stream processing. This pattern consists of several layers: **Batch
    Layer** (ingests and processes data on persistent storage such as HDFS and S3),
    **Speed Layer** (ingests and processes streaming data that has not been processed
    by the **Batch Layer** yet), and the **Serving Layer** (combines outputs from
    the **Batch** and **Speed Layers** to present merged results). This is a very
    popular architecture in Spark environments because it can support both the **Batch**
    and **Speed Layer** implementations with minimal code differences between the
    two.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: 'The given figure depicts the Lambda architecture as a combination of batch
    processing and stream processing:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00312.jpeg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
- en: 'The next figure shows an implementation of the Lambda architecture using AWS
    Cloud services (**Amazon Kinesis**, **Amazon S3** Storage, **Amazon EMR**, **Amazon
    DynamoDB**, and so on) and Spark:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00313.jpeg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
- en: For more details on the AWS implementation of Lambda architecture, refer to [https://d0.awsstatic.com/whitepapers/lambda-architecure-on-for-batch-aws.pdf](https://d0.awsstatic.com/whitepapers/lambda-architecure-on-for-batch-aws.pdf).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss a simpler architecture called Kappa Architecture,
    which dispenses with the **Batch Layer** entirely and works with stream processing
    in the **Speed Layer** only.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Kappa Architecture
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Kappa Architecture** is simpler than the Lambda pattern as it comprises
    the Speed and Serving Layers only. All the computations occur as stream processing
    and there are no batch re-computations done on the full Dataset. Recomputations
    are only done to support changes and new requirements.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, the incoming real-time data stream is processed in memory and is
    persisted in a database or HDFS to support queries, as illustrated in the following
    figure:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00314.jpeg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
- en: The Kappa Architecture can be realized by using Apache Spark combined with a
    queuing solution, such as Apache Kafka. If the data retention times are bound
    to several days to weeks, then Kafka could also be used to retain the data for
    the limited period of time.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: In the next few sections, we will introduce a few hands-on exercises using Apache
    Spark, Scala, and Apache Kafka that are very useful in the real-world applications
    development context. We will start by using Spark SQL and Structured Streaming
    to implement a few streaming use cases.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Design considerations for building scalable stream processing applications
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Building robust stream processing applications is challenging. The typical
    complexities associated with stream processing include the following:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '**Complex Data**: Diverse data formats and the quality of data create significant
    challenges in streaming applications. Typically, the data is available in various
    formats, such as JSON, CSV, AVRO, and binary. Additionally, dirty data, or late
    arriving, and out-of-order data, can make the design of such applications extremely
    complex.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complex workloads**: Streaming applications need to support a diverse set
    of application requirements, including interactive queries, machine learning pipelines,
    and so on.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complex systems**: With diverse storage systems, including Kafka, S3, Kinesis,
    and so on, system failures can lead to significant reprocessing or bad results.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Steam processing using Spark SQL can be fast, scalable, and fault-tolerant.
    It provides an extensive set of high-level APIs to deal with complex data and
    workloads. For example, the data sources API can integrate with many storage systems
    and data formats.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: For a detailed coverage of building scalable and fault-tolerant structured streaming applications,
    refer to [https://spark-summit.org/2017/events/easy-scalable-fault-tolerant-stream-processing-with-structured-streaming-in-apache-spark/](https://spark-summit.org/2017/events/easy-scalable-fault-tolerant-stream-processing-with-structured-streaming-in-apache-spark/).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: A streaming query allows us to specify one or more data sources, transform the
    data using DataFrame/Dataset APIs or SQL, and specify various sinks to output
    the results. There is built-in support for several data sources, such as files,
    Kafka, and sockets, and we can also combine multiple data sources, if required.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: The Spark SQL Catalyst optimizer figures out the mechanics of incrementally
    executing the transformations. The query is converted to a series of incremental
    execution plans that operate on the new batches of data. The sink accepts the
    output of each batch and the updates are completed within a transaction context.
    You can also specify various output modes (**Complete**, **Update**, or **Append**)
    and triggers to govern when to output the results. If no trigger is specified,
    the results are continuously updated. The progress of a given query, and restarts
    after failures are managed by persisting checkpoints.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Spark structured streaming enables streaming analytics without having to worry
    about the complex underlying mechanisms that make streaming work. In this model,
    the input can be thought of as data from an append-only table (that grows continuously).
    A trigger specifies the time interval for checking the input for the arrival of
    new data and the query represents operations such as map, filter, and reduce on
    the input. The result represents the final table that is updated in each trigger
    interval (as per the specified query operation).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 选择适当的数据格式
- en: For detailed illustrations on structured streaming internals, check out [http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html](http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 有关结构化流内部的详细说明，请查看[http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html](http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)。
- en: We can also execute sliding window operations on streaming data. Here, we define
    aggregations over a sliding window, in which we group the data and compute appropriate
    aggregations (for each group).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Spark结构化流使得流式分析变得简单，无需担心使流式工作的复杂底层机制。在这个模型中，输入可以被视为来自一个不断增长的追加表的数据。触发器指定了检查输入是否到达新数据的时间间隔，查询表示对输入进行的操作，如映射、过滤和减少。结果表示在每个触发间隔中更新的最终表（根据指定的查询操作）。
- en: In the next section, we will discuss Spark SQL features that can help in building
    robust ETL pipelines.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论Spark SQL功能，这些功能可以帮助构建强大的ETL管道。
- en: Building robust ETL pipelines using Spark SQL
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark SQL构建强大的ETL管道
- en: ETL pipelines execute a series of transformations on source data to produce
    cleansed, structured, and ready-for-use output by subsequent processing components. The
    transformations required to be applied on the source will depend on nature of
    the data. The input or source data can be structured (RDBMS, Parquet, and so on),
    semi-structured (CSV, JSON, and so on) or unstructured data (text, audio, video,
    and so on).  After being processed through such pipelines, the data is ready for
    downstream data processing, modeling, analytics, reporting, and so on.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ETL管道在源数据上执行一系列转换，以生成经过清洗、结构化并准备好供后续处理组件使用的输出。需要应用在源数据上的转换将取决于数据的性质。输入或源数据可以是结构化的（关系型数据库，Parquet等），半结构化的（CSV，JSON等）或非结构化数据（文本，音频，视频等）。通过这样的管道处理后，数据就可以用于下游数据处理、建模、分析、报告等。
- en: 'The following figure illustrates an application architecture in which the input
    data from Kafka, and other sources such as application and server logs, are cleansed
    and transformed (using an ETL pipeline) before being stored in an enterprise data
    store. This data store can eventually feed other applications (via Kafka), support
    interactive queries, store subsets or views of the data in serving databases,
    train ML models, support reporting applications, and so on:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 下图说明了一个应用架构，其中来自Kafka和其他来源（如应用程序和服务器日志）的输入数据在存储到企业数据存储之前经过清洗和转换（使用ETL管道）。这个数据存储最终可以供其他应用程序使用（通过Kafka），支持交互式查询，将数据的子集或视图存储在服务数据库中，训练ML模型，支持报告应用程序等。
- en: '![](img/00315.jpeg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: 在下一节中，我们将介绍一些标准，可以帮助您选择适当的数据格式，以满足特定用例的要求。
- en: As the abbreviation (ETL) suggests, we need to retrieve the data from various
    sources (Extract), transform the data for downstream consumption (Transform),
    and transmit it to different destinations (Load).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 正如缩写（ETL）所示，我们需要从各种来源检索数据（提取），转换数据以供下游使用（转换），并将其传输到不同的目的地（加载）。
- en: Over the next few sections, we will use Spark SQL features to access and process
    various data sources and data formats for ETL purposes. Spark SQL's flexible APIs,
    combined with the Catalyst optimizer and tungsten execution engine, make it highly
    suitable for building end-to-end ETL pipelines.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将使用Spark SQL功能来访问和处理各种数据源和数据格式，以实现ETL的目的。Spark SQL灵活的API，结合Catalyst优化器和tungsten执行引擎，使其非常适合构建端到端的ETL管道。
- en: 'In the following code block, we present a simple skeleton of a single ETL query
    that combines all the three (Extract, Transform, and Load) functions. These queries
    can also be extended to execute complex joins between tables containing data from
    multiple sources and source formats:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码块中，我们提供了一个简单的单个ETL查询的框架，结合了所有三个（提取、转换和加载）功能。这些查询也可以扩展到执行包含来自多个来源和来源格式的数据的表之间的复杂连接：
- en: '[PRE0]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the next section, we will introduce a few criteria that can help you make
    appropriate choices regarding data formats to satisfy the requirements of your
    specific use cases.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以对流数据执行滑动窗口操作。在这里，我们定义了对滑动窗口的聚合，其中我们对数据进行分组并计算适当的聚合（对于每个组）。
- en: Choosing appropriate data formats
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '![](img/00315.jpeg)'
- en: In enterprise settings, the data is available in many different data sources
    and formats. Spark SQL supports a set of built-in and third-party connectors.
    In addition, we can also define custom data source connectors. Data formats include
    structured, semi-structured, and unstructured formats, such as plain text, JSON,
    XML, CSV, RDBMS records, images, and video. More recently, big data formats such
    as Parquet, ORC, and Avro are becoming increasingly popular. In general, unstructured
    formats such as plain text files are more flexible, while structured formats such
    as Parquet and AVRO are more efficient from a storage and performance perspective.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在企业设置中，数据以许多不同的数据源和格式可用。Spark SQL支持一组内置和第三方连接器。此外，我们还可以定义自定义数据源连接器。数据格式包括结构化、半结构化和非结构化格式，如纯文本、JSON、XML、CSV、关系型数据库记录、图像和视频。最近，Parquet、ORC和Avro等大数据格式变得越来越受欢迎。一般来说，纯文本文件等非结构化格式更灵活，而Parquet和AVRO等结构化格式在存储和性能方面更有效率。
- en: In the case of structured data formats, the data has a rigid, well-defined schema
    or structure associated with it. For example, columnar data formats make it more
    efficient to extract values from columns. However, this rigidity can make changes
    to the schema, or the structure, challenging. By contrast, unstructured data sources,
    such as free-form text, contain no markup or separators as in CSV or TSV files.
    Such data sources generally require some context around the data; for example,
    you need to know that the contents of files contain text from blogs.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在结构化数据格式的情况下，数据具有严格的、明确定义的模式或结构。例如，列式数据格式使得从列中提取值更加高效。然而，这种严格性可能会使对模式或结构的更改变得具有挑战性。相比之下，非结构化数据源，如自由格式文本，不包含CSV或TSV文件中的标记或分隔符。这样的数据源通常需要一些关于数据的上下文；例如，你需要知道文件的内容包含来自博客的文本。
- en: Typically, we need many transformations and feature extraction techniques to
    interpret diverse Datasets. Semi-structured data is structured at a record level,
    but not necessarily across all the records. As a result, each data record contains
    the associated schema information as well.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们需要许多转换和特征提取技术来解释不同的数据集。半结构化数据在记录级别上是结构化的，但不一定在所有记录上都是结构化的。因此，每个数据记录都包含相关的模式信息。
- en: The JSON format is probably the most common example of semi-structured data.
    JSON records are in a human-readable form, making it more convenient for development
    and debugging purposes. However, these formats suffer from parsing-related overheads,
    and are typically not the best choice for supporting the ad hoc querying functionality.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: JSON格式可能是半结构化数据最常见的例子。JSON记录以人类可读的形式呈现，这对于开发和调试来说更加方便。然而，这些格式受到解析相关的开销的影响，通常不是支持特定查询功能的最佳选择。
- en: Often, applications will have to be designed to span and traverse across varied
    data sources and formats to efficiently store and process the data. For example,
    Avro is a good choice when access is required to complete rows of data, as in
    the case of access to features in an ML pipeline. In cases where flexibility in
    the schema is required, using JSON may be the most appropriate choice for the
    data format. Furthermore, in cases where the data does not have a fixed schema,
    it is probably best to use the plain text file format.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，应用程序需要设计成能够跨越各种数据源和格式高效存储和处理数据。例如，当需要访问完整的数据行时，Avro是一个很好的选择，就像在ML管道中访问特征的情况一样。在需要模式的灵活性的情况下，使用JSON可能是数据格式的最合适选择。此外，在数据没有固定模式的情况下，最好使用纯文本文件格式。
- en: Transforming data in ETL pipelines
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ETL管道中的数据转换
- en: Typically, semi-structured formats such as JSON contain struct, map, and array
    data types; for example, request and/or response payloads for REST web services
    contain JSON data with nested fields and arrays.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，诸如JSON之类的半结构化格式包含struct、map和array数据类型；例如，REST Web服务的请求和/或响应负载包含具有嵌套字段和数组的JSON数据。
- en: In this section, we will present examples of Spark SQL-based transformations
    on Twitter data. The input Dataset is a file (`cache-0.json.gz`) containing `10
    M` tweets from a set of Datasets containing over `170 M` tweets collected during
    the three months leading up to the 2012 US presidential elections. This file can
    be downloaded from [https://datahub.io/dataset/twitter-2012-presidential-election](https://datahub.io/dataset/twitter-2012-presidential-election).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将展示基于Spark SQL的Twitter数据转换的示例。输入数据集是一个文件（`cache-0.json.gz`），其中包含了在2012年美国总统选举前三个月内收集的超过`1.7亿`条推文中的`1千万`条推文。这个文件可以从[https://datahub.io/dataset/twitter-2012-presidential-election](https://datahub.io/dataset/twitter-2012-presidential-election)下载。
- en: 'Before starting with the following examples, start Zookeeper and the Kafka
    broker as described in [Chapter 5](part0085.html#2H1VQ0-e9cbc07f866e437b8aa14e841622275c),
    *Using Spark SQL in Streaming Applications*. Also, create a new Kafka topic, called
    tweetsa. We generate the schema from the input JSON Dataset, as shown. This schema
    definition will be used later in this section:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始以下示例之前，按照[第5章](part0085.html#2H1VQ0-e9cbc07f866e437b8aa14e841622275c)中描述的方式启动Zookeeper和Kafka代理。另外，创建一个名为tweetsa的新Kafka主题。我们从输入JSON数据集生成模式，如下所示。这个模式定义将在本节后面使用：
- en: '[PRE1]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Set up to read the streaming tweets from the Kafka topic (*tweetsa*), and then
    parse the JSON data using the schema from the previous step.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 设置从Kafka主题（*tweetsa*）中读取流式推文，并使用上一步的模式解析JSON数据。
- en: 'We select all the fields in the tweet by `specifying data.*` in this statement:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个声明中，我们通过`指定数据.*`来选择推文中的所有字段：
- en: '[PRE2]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You will need to use the following command repeatedly (as you work through
    the examples) to pipe the tweets contained in the input file to the Kafka topic,
    as illustrated:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在你通过示例工作时，你需要反复使用以下命令将输入文件中包含的推文传输到Kafka主题中，如下所示：
- en: '[PRE3]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Given the size of the input file, this can potentially result in space-related
    issues on your machine. If this happens, use appropriate Kafka commands to delete
    and recreate the topic (refer to [https://kafka.apache.org/0102/documentation.html](https://kafka.apache.org/0102/documentation.html)).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到输入文件的大小，这可能会导致您的计算机出现空间相关的问题。如果发生这种情况，请使用适当的Kafka命令来删除并重新创建主题（参考[https://kafka.apache.org/0102/documentation.html](https://kafka.apache.org/0102/documentation.html)）。
- en: 'Here, we reproduce a section of the schema to help understand the structure
    we are working with over the next few examples:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们重现了一个模式的部分，以帮助理解我们在接下来的几个示例中要处理的结构：
- en: '![](img/00316.jpeg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00316.jpeg)'
- en: 'We can select specific fields from nested columns in the JSON string. We use
    the . (dot) operator to choose the nested field, as shown:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从JSON字符串中的嵌套列中选择特定字段。我们使用`.`（点）运算符来选择嵌套字段，如下所示：
- en: '[PRE4]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we write the output stream to the screen to view the results. You will
    need to execute the following statement after each of the transformations in order
    to view and evaluate the results. Also, in the interests of saving time, you should
    execute `s5.stop()`, after you have seen sufficient output on the screen. Alternatively,
    you can always choose to work with a smaller set of data extracted from the original
    input file:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将输出流写入屏幕以查看结果。您需要在每个转换之后执行以下语句，以查看和评估结果。此外，为了节省时间，您应该在看到足够的屏幕输出后执行`s5.stop()`。或者，您可以选择使用从原始输入文件中提取的较小数据集进行工作：
- en: '[PRE5]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](img/00317.gif)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00317.gif)'
- en: 'In the next example, we will flatten a struct using star (*) to select all
    the subfields in the struct:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个示例中，我们将使用星号（*）展平一个struct以选择struct中的所有子字段：
- en: '[PRE6]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The results can be viewed by writing the output stream, as shown in the preceding
    example:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过编写输出流来查看结果，如前面的示例所示：
- en: '![](img/00318.gif)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00318.gif)'
- en: We can use the struct function to create a new struct (for nesting the columns)
    as illustrated in the following code snippet. We can select a specific field or
    fields for creating the new struct. We can also nest all the columns using star
    (*), if required.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用struct函数创建一个新的struct（用于嵌套列），如下面的代码片段所示。我们可以选择特定字段或字段来创建新的struct。如果需要，我们还可以使用星号（*）嵌套所有列。
- en: 'Here, we reproduce the section of the schema used in this example:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们重现了此示例中使用的模式部分：
- en: '![](img/00319.jpeg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00319.jpeg)'
- en: '[PRE7]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](img/00320.gif)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00320.gif)'
- en: 'In the next example, we select a single array (or map) element using `getItem()`.
    Here, we are operating on the following part of the schema:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个示例中，我们使用`getItem()`选择单个数组（或映射）元素。在这里，我们正在操作模式的以下部分：
- en: '![](img/00321.jpeg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00321.jpeg)'
- en: '[PRE8]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](img/00322.jpeg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00322.jpeg)'
- en: '[PRE9]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](img/00323.gif)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00323.gif)'
- en: 'We can use the `explode()` function to create a new row for each element in
    an array, as shown. To illustrate the results of `explode()`, we first show the
    rows containing the arrays, and then show the results of applying the explode
    function:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`explode()`函数为数组中的每个元素创建新行，如所示。为了说明`explode()`的结果，我们首先展示包含数组的行，然后展示应用explode函数的结果：
- en: '[PRE10]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The following output is obtained:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 获得以下输出：
- en: '![](img/00324.jpeg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00324.jpeg)'
- en: 'Note the separate rows created for the array elements after applying the explode
    function:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在应用explode函数后，为数组元素创建了单独的行：
- en: '[PRE11]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The following is the output obtained:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 获得的输出如下：
- en: '![](img/00325.jpeg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00325.jpeg)'
- en: Spark SQL also has functions such as `to_json()`, to transform a `struct` to
    a JSON string, and `from_json()`, to convert a JSON string to a `struct`. These
    functions are very useful to read from or write to Kafka topics. For example,
    if the "value" field contains data in a JSON string, then we can use the `from_json()`
    function to extract the data, transform it, and then push it out to a different
    Kafka topic, and/or write it out to a Parquet file or a serving database.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL还具有诸如`to_json()`之类的函数，用于将`struct`转换为JSON字符串，以及`from_json()`，用于将JSON字符串转换为`struct`。这些函数对于从Kafka主题读取或写入非常有用。例如，如果“value”字段包含JSON字符串中的数据，则我们可以使用`from_json()`函数提取数据，转换数据，然后将其推送到不同的Kafka主题，并/或将其写入Parquet文件或服务数据库。
- en: 'In the following example, we use the `to_json()` function to convert a struct
    to a JSON string:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们使用`to_json()`函数将struct转换为JSON字符串：
- en: '[PRE12]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](img/00326.gif)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00326.gif)'
- en: We can use the `from_json()` function to convert a column containing JSON data
    into a `struct` data type. Further, we can flatten the preceding struct into separate
    columns. We show an example of using this function in a later section.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`from_json()`函数将包含JSON数据的列转换为`struct`数据类型。此外，我们可以将前述结构展平为单独的列。我们在后面的部分中展示了使用此函数的示例。
- en: For more detailed coverage of transformation functions, refer to [https://databricks.com/blog/2017/02/23/working-complex-data-formats-structured-streaming-apache-spark-2-1.html](https://databricks.com/blog/2017/02/23/working-complex-data-formats-structured-streaming-apache-spark-2-1.html).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 有关转换函数的更详细覆盖范围，请参阅[https://databricks.com/blog/2017/02/23/working-complex-data-formats-structured-streaming-apache-spark-2-1.html](https://databricks.com/blog/2017/02/23/working-complex-data-formats-structured-streaming-apache-spark-2-1.html)。
- en: Addressing errors in ETL pipelines
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决ETL管道中的错误
- en: ETL tasks are usually considered to be complex, expensive, slow, and error-prone.
    Here, we will examine typical challenges in ETL processes, and how Spark SQL features
    assist in addressing them.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ETL任务通常被认为是复杂、昂贵、缓慢和容易出错的。在这里，我们将研究ETL过程中的典型挑战，以及Spark SQL功能如何帮助解决这些挑战。
- en: 'Spark can automatically infer the schema from a JSON file. For example, for
    the following JSON data, the inferred schema includes all the labels and the data
    types based on the content. Here, the data types for all the elements in the input
    data are longs by default:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Spark可以自动从JSON文件中推断模式。例如，对于以下JSON数据，推断的模式包括基于内容的所有标签和数据类型。在这里，输入数据中所有元素的数据类型默认为长整型：
- en: '**test1.json**'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**test1.json**'
- en: '[PRE13]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'You can print the schema to verify the data types, as shown:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以打印模式以验证数据类型，如下所示：
- en: '[PRE14]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'However, in the following JSON data, if the value of `e` in the third row and
    the value of `b` in the last row are changed to include fractions, and the value
    of `f` in the second-from-last row is enclosed in quotes, the inferred schema
    changes the data types of `b` and `e` to double, and `f` to string type:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在以下JSON数据中，如果第三行中的`e`的值和最后一行中的`b`的值被更改以包含分数，并且倒数第二行中的`f`的值被包含在引号中，那么推断的模式将更改`b`和`e`的数据类型为double，`f`的数据类型为字符串：
- en: '[PRE15]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'If we want to associate specific structure or data types to elements, we need
    to use a user-specified schema. In the next example, we use a CSV file with a
    header containing the field names. The field names in the schema are derived from
    the header, and the data types specified in the user-defined schema are used against
    them, as shown:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要将特定结构或数据类型与元素关联起来，我们需要使用用户指定的模式。在下一个示例中，我们使用包含字段名称的标题的CSV文件。模式中的字段名称来自标题，并且用户定义的模式中指定的数据类型将用于它们，如下所示：
- en: '[PRE16]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The following output is obtained:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 获取以下输出：
- en: '![](img/00327.jpeg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00327.jpeg)'
- en: Issues can also occur in ETL pipelines due to file and data corruption. If the
    data is not mission-critical, and the corrupt files can be safely ignored, we
    can set `config property spark.sql.files.ignoreCorruptFiles = true`. This setting
    lets Spark jobs continue running even when corrupt files are encountered. Note
    that contents that are successfully read will continue to be returned.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 由于文件和数据损坏，ETL管道中也可能出现问题。如果数据不是关键任务，并且损坏的文件可以安全地忽略，我们可以设置`config property spark.sql.files.ignoreCorruptFiles
    = true`。此设置允许Spark作业继续运行，即使遇到损坏的文件。请注意，成功读取的内容将继续返回。
- en: 'In the following example, there is bad data for `b` in row `4`. We can still
    read the data using the `PERMISSIVE` mode. In this case, a new column, called
    `_corrupt_record`, is added to the DataFrame, and the contents of the corrupted
    rows appear in that column with the rest of the fields initialized to nulls. We
    can focus on the data issues by reviewing the data in this column and initiate
    suitable actions to fix them. By setting the `spark.sql.columnNameOfCorruptRecord`
    property, we can configure the default name of the corrupted contents column:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个示例中，第4行的`b`存在错误数据。我们仍然可以使用`PERMISSIVE`模式读取数据。在这种情况下，DataFrame中会添加一个名为`_corrupt_record`的新列，并且损坏行的内容将出现在该列中，其余字段初始化为null。我们可以通过查看该列中的数据来关注数据问题，并采取适当的措施来修复它们。通过设置`spark.sql.columnNameOfCorruptRecord`属性，我们可以配置损坏内容列的默认名称：
- en: '[PRE17]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](img/00328.gif)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00328.gif)'
- en: 'Now, we use the `DROPMALFORMED` option to drop all malformed records. Here,
    the fourth row is dropped due to the bad value for `b`:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们使用`DROPMALFORMED`选项来删除所有格式不正确的记录。在这里，由于`b`的坏值，第四行被删除：
- en: '[PRE18]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](img/00329.gif)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00329.gif)'
- en: 'For critical data, we can use the `FAILFAST` option to fail immediately upon
    encountering a bad record. For example, in the following example, due to the value
    of `b` in the fourth row, the operation throws an exception and exits immediately:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 对于关键数据，我们可以使用`FAILFAST`选项，在遇到坏记录时立即失败。例如，在以下示例中，由于第四行中`b`的值，操作会抛出异常并立即退出：
- en: '[PRE19]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'In the following example, we have a record that spans two rows; we can read
    this record by setting the `wholeFile` option to true:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个示例中，我们有一条跨越两行的记录；我们可以通过将`wholeFile`选项设置为true来读取此记录：
- en: '[PRE20]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: For more details on Spark SQL-based ETL pipelines and roadmaps, visit [https://spark-summit.org/2017/events/building-robust-etl-pipelines-with-apache-spark/](https://spark-summit.org/2017/events/building-robust-etl-pipelines-with-apache-spark/).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 有关基于Spark SQL的ETL管道和路线图的更多详细信息，请访问[https://spark-summit.org/2017/events/building-robust-etl-pipelines-with-apache-spark/](https://spark-summit.org/2017/events/building-robust-etl-pipelines-with-apache-spark/)。
- en: The preceding reference presents several higher-order SQL transformation functions,
    new formats for the DataframeWriter API, and a unified `Create Table` (as `Select`)
    constructs in Spark 2.2 and 2.3-Snapshot.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 上述参考介绍了几个高阶SQL转换函数，DataframeWriter API的新格式以及Spark 2.2和2.3-Snapshot中的统一`Create
    Table`（作为`Select`）构造。
- en: Other requirements addressed by Spark SQL include scalability and continuous
    ETL using structured streaming. We can use structured streaming to enable raw
    data to be available as structured data ready for analysis, reporting, and decision-making
    as soon as possible, instead of incurring the hours of delay typically associated
    with running periodic batch jobs. This type of processing is especially important
    in applications such as anomaly detection, fraud detection, and so on, where time
    is of the essence.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL解决的其他要求包括可扩展性和使用结构化流进行持续ETL。我们可以使用结构化流来使原始数据尽快可用作结构化数据，以进行分析、报告和决策，而不是产生通常与运行周期性批处理作业相关的几小时延迟。这种处理在应用程序中尤为重要，例如异常检测、欺诈检测等，时间至关重要。
- en: In the next section, we will shift our focus to building a scalable monitoring
    solution using Spark SQL.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将把重点转移到使用Spark SQL构建可扩展的监控解决方案。
- en: Implementing a scalable monitoring solution
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施可扩展的监控解决方案
- en: Building a scalable monitoring function for large-scale deployments can be challenging
    as there could be billions of data points captured each day. Additionally, the
    volume of logs and the number of metrics can be difficult to manage without a
    suitable big data platform with streaming and visualization support.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 为大规模部署构建可扩展的监控功能可能具有挑战性，因为每天可能捕获数十亿个数据点。此外，日志的数量和指标的数量可能难以管理，如果没有适当的具有流式处理和可视化支持的大数据平台。
- en: Voluminous logs collected from applications, servers, network devices, and so
    on are processed to provide real-time monitoring that help detect errors, warnings,
    failures, and other issues. Typically, various daemons, services, and tools are
    used to collect/send log records to the monitoring system. For example, log entries
    in the JSON format can be sent to Kafka queues or Amazon Kinesis. These JSON records
    can then be stored on S3 as files and/or streamed to be analyzed in real time
    (in a Lambda architecture implementation). Typically, an ETL pipeline is run to
    cleanse the log data, transform it into a more structured form, and then load
    it into files such as Parquet files or databases, for querying, alerting, and
    reporting purposes.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure illustrates one such platform using **Spark Streaming
    Jobs**, a **Scalable Time Series Database** such as OpenTSDB or Graphite, and
    **Visualization Tools** such as Grafana:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00330.jpeg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
- en: For more details on this solution, refer to [https://spark-summit.org/2017/events/scalable-monitoring-using-apache-spark-and-friends/](https://spark-summit.org/2017/events/scalable-monitoring-using-apache-spark-and-friends/).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring and troubleshooting problems are challenging tasks in large distributed
    environments comprising of several Spark clusters with varying configurations
    and versions, running different types of workloads. In these environments hundreds
    of thousands metrics may be received. Additionally, hundreds of MBs of logs are
    generated per second. These metrics need to be tracked and the logs analyzed for
    anomalies, failures, bugs, environmental issues, and so on to support alerting
    and troubleshooting functions.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: The following figure illustrates an AWS-based data pipeline that pushes all
    the metrics and logs (both structured and unstructured) to Kinesis. A structured
    streaming job can read the raw logs from Kinesis and save the data as Parquet
    files on S3.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'The structured streaming query can strip known error patterns and raise suitable
    alerts, if a new error type is observed. Other Spark batch and streaming applications
    can use these Parquet files to perform additional processing and output their
    results as new Parquet files on S3:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00331.jpeg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
- en: In this architecture, discovering issues from unstructured logs may be required
    to determine their scope, duration, and impact. **Raw Logs** typically contain
    many near-duplicate error messages. For efficient processing of these logs, we
    need to normalize, deduplicate, and filter out well-known error conditions to
    discover and reveal new ones.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: For details on a pipeline to process raw logs, refer to [https://spark-summit.org/2017/events/lessons-learned-from-managing-thousands-of-production-apache-spark-clusters-daily/](https://spark-summit.org/2017/events/lessons-learned-from-managing-thousands-of-production-apache-spark-clusters-daily/).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will explore some of the features Spark SQL and Structured
    Streaming provide to create a scalable monitoring solution.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'First, start the Spark shell with the Kafka packages:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Download the traces for the month of July, 1995, containing HTTP requests to
    the NASA Kennedy Space Center WWW server in Florida from [http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html](http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the following packages for the hands-on exercises in this chapter:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, define the schema for the records in the file:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'For simplicity, we read the input file as a CSV file with a space separator,
    as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Next, we create a DataFrame containing the log events. As the timestamp changes
    to the local time zone (by default) in the preceding step, we also retain the
    original timestamp with time zone information in the `original_dateTime` column,
    as illustrated:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We can check the results of the streaming read, as shown:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![](img/00332.gif)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
- en: 'We can save the streaming input to Parquet files, partitioned by date to support
    queries more efficiently, as demonstrated:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We can read input so that the latest records are available first by specifying
    the `latestFirst` option:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过指定`latestFirst`选项来读取输入，以便最新的记录首先可用：
- en: '[PRE28]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We can also write out the output in the JSON format, partitioned by date, easily,
    as follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以按日期将输出以JSON格式输出，如下所示：
- en: '[PRE29]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now, we show the use of Kafka for input and output in streaming Spark applications.
    Here, we have to specify the format parameter as `kafka`, and the kafka broker
    and topic:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们展示了在流式Spark应用程序中使用Kafka进行输入和输出的示例。在这里，我们必须将格式参数指定为`kafka`，并指定kafka代理和主题：
- en: '[PRE30]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now, we are reading a stream of JSON data from Kafka. The starting offset is
    set to the earliest to specify the starting point for our query. This applies
    only when a new streaming query is started:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们正在从Kafka中读取JSON数据流。将起始偏移设置为最早以指定查询的起始点。这仅适用于启动新的流式查询时：
- en: '[PRE31]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We can print out the schema for records read from Kafka, as follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按以下方式打印从Kafka读取的记录的模式：
- en: '[PRE32]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Next, we define the schema for input records, as shown:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义输入记录的模式，如下所示：
- en: '[PRE33]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Next, we can specify the schema, as illustrated. The star `*` operator is used
    to select all the `subfields` in a `struct`:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以指定模式，如所示。星号`*`运算符用于选择`struct`中的所有`subfields`：
- en: '[PRE34]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Next, we show an example of selecting specific fields. Here, we set the `outputMode`
    to append so that only the new rows appended to the result table are written out
    to external storage. This is applicable only on queries where the existing rows
    in the result table are not expected to change:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们展示选择特定字段的示例。在这里，我们将`outputMode`设置为append，以便只有追加到结果表的新行被写入外部存储。这仅适用于查询结果表中现有行不会发生变化的情况：
- en: '[PRE35]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![](img/00333.gif)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00333.gif)'
- en: 'We can also specify `read` (not `readStream`) to read the records into a regular
    DataFrame:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以指定`read`（而不是`readStream`）将记录读入常规DataFrame中：
- en: '[PRE36]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We can now do all the standard DataFrame operations against this DataFrame;
    for example, we create a table and query it, as shown:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以对这个DataFrame执行所有标准的DataFrame操作；例如，我们创建一个表并查询它，如下所示：
- en: '[PRE37]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '![](img/00334.jpeg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00334.jpeg)'
- en: 'Then, we read the records from Kafka and apply the schema:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们从Kafka中读取记录并应用模式：
- en: '[PRE38]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We can execute the following query to check the contents of the records:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以执行以下查询来检查记录的内容：
- en: '[PRE39]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '![](img/00335.gif)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00335.gif)'
- en: 'We can select all the fields from the records, as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从记录中选择所有字段，如下所示：
- en: '[PRE40]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We can also select specific fields of interest from the DataFrame:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以从DataFrame中选择感兴趣的特定字段：
- en: '[PRE41]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Next, we can use window operations, as demonstrated, and maintain counts for
    various HTTP codes. Here, we use `outputMode` set to `complete` since we want
    the entire updated result table to be written to the external storage:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以使用窗口操作，并为各种HTTP代码维护计数，如所示。在这里，我们将`outputMode`设置为`complete`，因为我们希望将整个更新后的结果表写入外部存储：
- en: '[PRE42]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '![](img/00336.gif)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00336.gif)'
- en: 'Next, we show another example of using `groupBy` and computed counts for various
    page requests in these windows. This can be used to compute and report the top
    pages visited type metrics:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们展示了另一个使用`groupBy`和计算各窗口中各种页面请求计数的示例。这可用于计算和报告访问类型指标中的热门页面：
- en: '[PRE43]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '![](img/00337.gif)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00337.gif)'
- en: Note that the examples presented earlier are instances of stateful processing.
    The counts have to be saved as a distributed state between triggers. Each trigger
    reads the previous state and writes the updated state. This state is stored in
    memory, and is backed by the persistent WAL, typically located on HDFS or S3 storage.
    This allows the streaming application to automatically handle data arriving late.
    Keeping this state allows the late data to update the counts of old windows.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，前面提到的示例是有状态处理的实例。计数必须保存为触发器之间的分布式状态。每个触发器读取先前的状态并写入更新后的状态。此状态存储在内存中，并由持久的WAL支持，通常位于HDFS或S3存储上。这使得流式应用程序可以自动处理延迟到达的数据。保留此状态允许延迟数据更新旧窗口的计数。
- en: However, the size of the state can increase indefinitely, if the old windows
    are not dropped. A watermarking approach is used to address this issue. A watermark
    is a moving threshold of how late data is expected to be and when to drop the
    old state. It trails behind the max seen event time. Data newer than watermark
    may be late, but is allowed into the aggregate, while data older than the watermark
    is considered "too late" and dropped. Additionally, windows older than the watermark
    are automatically deleted to limit the amount of intermediate state that is required
    to be maintained by the system.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果不丢弃旧窗口，状态的大小可能会无限增加。水印方法用于解决此问题。水印是预期数据延迟的移动阈值，以及何时丢弃旧状态。它落后于最大观察到的事件时间。水印之后的数据可能会延迟，但允许进入聚合，而水印之前的数据被认为是“太晚”，并被丢弃。此外，水印之前的窗口会自动删除，以限制系统需要维护的中间状态的数量。
- en: 'A watermark specified for the previous query is given here:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个查询中指定的水印在这里给出：
- en: '[PRE44]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: For more details on watermarking, refer to [https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html](https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 有关水印的更多详细信息，请参阅[https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html](https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html)。
- en: In the next section, we will shift our focus to deploying Spark-based machine
    learning pipelines in production.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将把重点转移到在生产环境中部署基于Spark的机器学习管道。
- en: Deploying Spark machine learning pipelines
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署Spark机器学习管道
- en: 'The following figure illustrates a machine learning pipeline at a conceptual
    level. However, real-life ML pipelines are a lot more complicated, with several
    models being trained, tuned, combined, and so on:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 下图以概念级别说明了机器学习管道。然而，现实生活中的ML管道要复杂得多，有多个模型被训练、调整、组合等：
- en: '![](img/00338.jpeg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00338.jpeg)'
- en: 'The next figure shows the core elements of a typical machine learning application
    split into two parts: the modeling, including model training, and the deployed
    model (used on streaming data to output the results):'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00339.jpeg)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
- en: Typically, data scientists experiment or do their modeling work in Python and/or
    R. Their work is then reimplemented in Java/Scala before deployment in a production
    environment. Enterprise production environments often consist of web servers,
    application servers, databases, middleware, and so on. The conversion of prototypical
    models to production-ready models results in additional design and development
    effort that lead to delays in rolling out updated models.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: We can use Spark MLlib 2.x model serialization to directly use the models and
    pipelines saved by data scientists (to disk) in production environments by loading
    them from the persisted model files.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example (source: [https://spark.apache.org/docs/latest/ml-pipeline.html](https://spark.apache.org/docs/latest/ml-pipeline.html)),
    we will illustrate creating and saving a ML pipeline in Python (using `pyspark`
    shell) and then retrieving it in a Scala environment (using Spark shell).'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 'Start the `pyspark` shell and execute the following sequence of Python statements:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Start the Spark shell and execute the following sequence of Scala statements:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Next, we create a `test` Dataset and run it through the ML pipeline:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The results of running the model on the `test` Dataset are shown here:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The key parameters of the saved logistic regression model are read into a DataFrame,
    as illustrated in the following code block. Earlier, when the model was saved
    in the `pyspark` shell, these parameters were saved to a Parquet file located
    in a subdirectory associated with the final stage of our pipeline:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The following output is obtained:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00340.jpeg)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
- en: '[PRE50]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The output is, as shown:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00341.jpeg)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
- en: For more details on how to productionize ML models, refer to [https://spark-summit.org/2017/events/how-to-productionize-your-machine-learning-models-using-apache-spark-mllib-2x/](https://spark-summit.org/2017/events/how-to-productionize-your-machine-learning-models-using-apache-spark-mllib-2x/).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the challenges in typical ML deployment environments
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Production deployment environments for ML models can be very diverse and complex.
    For example, models may need to be deployed in web applications, portals, real-time
    and batch processing systems, and as an API or a REST service, embedded in devices
    or in large legacy environments.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, enterprise technology stacks can comprise of Java Enterprise,
    C/C++, legacy mainframe environments, relational databases, and so on. The non-functional
    requirements and customer SLAs with respect to response times, throughput, availability,
    and uptime can also vary widely. However, in almost all cases, our deployment
    process needs to support A/B testing, experimentation, model performance evaluation,
    and be agile and responsive to business needs.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Typically, practitioners use various methods to benchmark and phase-in new or
    updated models to avoid high-risk, big bang production deployments.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explore a few model deployment architectures.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Understanding types of model scoring architectures
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The simplest model is to precompute model results using Spark (batch processing),
    save the results to a database, and then serve the results to web and mobile applications
    from the database. Many large-scale recommendation engines and search engines
    use this architecture:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00342.jpeg)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
- en: 'A second model scoring architecture computes the features and runs prediction
    algorithms using Spark Streaming. The prediction results can be cached using caching
    solutions, such as Redis, and can be made available via an API. Other applications
    can then use these APIs to obtain the prediction results from the deployed model.
    This option is illustrated in this figure:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00343.jpeg)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
- en: In a third architectural model, we can use Spark for model training purposes
    only. The model is then copied into the production environment. For example, we
    can load the coefficients and intercept of a logistic regression model from a
    JSON file. This approach is resource-efficient and results in a high-performing
    system. It is also a lot easier to deploy in existing or complex environments.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 'It is illustrated here:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00344.jpeg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
- en: 'Continuing with our earlier example, we can read in the saved model parameters
    from a Parquet file and convert it to a JSON format that can, in turn, be conveniently
    imported into any application (inside or outside the Spark environment) and applied
    to new data:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We can display the intercept, coefficients, and other key parameters using
    standard OS commands, as follows:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '![](img/00345.jpeg)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
- en: As models are becoming bigger and more complex, it can be challenging to deploy
    and serve them. Models may not scale well, and their resource requirements can
    become very expensive. Databricks and Redis-ML provide solutions to deploy the
    trained model.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: In the Redis-ML solution, the model is applied to the new data directly in the
    Redis environment.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: This can provide the required overall performance, scalability, and availability
    at a much lower price point than running the model in a Spark environment.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows Redis-ML being used as a serving engine (implementing
    the third model scoring architectural pattern, as described earlier):'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00346.jpeg)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
- en: In the next section, we will briefly discuss using Mesos and Kubernetes as cluster
    managers in production environments.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Using cluster managers
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will briefly discuss Mesos and Kubernetes at a conceptual
    level. The Spark framework can be deployed through Apache **Mesos**, **YARN**,
    Spark Standalone, or the **Kubernetes** cluster manager, as depicted:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00347.jpeg)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
- en: Mesos can enable easy scalability and replication of data, and is a good unified
    cluster management solution for heterogeneous workloads.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: To use Mesos from Spark, the Spark binaries should be accessible by Mesos and
    the Spark driver configured to connect to Mesos. Alternatively, you can also install
    Spark binaries on all the Mesos slaves. The driver creates a job and then issues
    the tasks for scheduling, while Mesos determines the machines to handle them.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark can run over Mesos in two modes: coarse-grained (the default) and fine-grained
    (deprecated in Spark 2.0.0). In the coarse-grained mode, each Spark executor runs
    as a single Mesos task. This mode has significantly lower start up overheads,
    but reserves Mesos resources for the duration of the application. Mesos also supports
    dynamic allocation where the number of executors is adjusted based on the statistics
    of the application.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure illustrates a deployment that collocates the **Mesos Master**
    and **Zookeeper** nodes. The **Mesos Slave** and **Cassandra Node** are also collocated
    for better data locality. In addition, the Spark binaries are deployed on all
    worker nodes:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00348.jpeg)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
- en: Another emerging Spark cluster management solution is Kubernetes, which is being
    developed as a native cluster manager for Spark. It is an open source system that
    can be used for automating the deployment, scaling, and management of containerized
    Spark applications.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: 'The next figure depicts a high-level view of Kubernetes. Each node contains
    a daemon, called a **Kublet**, which talks to the **Master** node. The users also
    talks to the Master to declaratively specify what they want to run. For example,
    a user can request running a specific number of web server instances. The Master
    will take the user''s request and schedule the workload on the nodes:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00349.jpeg)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
- en: 'The nodes run one or more pods. A pod is a higher-level abstraction on containers,
    and each pod can contain a set of colocated containers. Each pod has its own IP
    address and can communicate with the pods in other nodes. The storage volumes
    can be local or network-attached. This can be seen in the following figure:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00350.jpeg)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
- en: Kubernetes promotes resource sharing between different types of Spark workload
    to reduce operational costs and improve infrastructure utilization. In addition,
    several add-on services can be used out-of-the-box with Spark applications, including
    logging, monitoring, security, container-to-container communications, and so on.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: For more details on Spark on Kubernetes, visit [https://github.com/apache-spark-on-k8s/spark](https://github.com/apache-spark-on-k8s/spark).
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: In the following figure, the dotted line separates Kubernetes from Spark. Spark
    Core is responsible for getting new executors, pushing new configurations, removing
    executors, and so on. The **Kubernetes Scheduler Backend** takes the Spark Core
    requests and converts them into primitives that Kubernetes can understand. Additionally,
    it handles all resource requests and all communications with Kubernetes.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: 'Other services, such as a File Staging Server, can make your local files and
    JARs available to the Spark cluster, and the Spark shuffle service can store shuffle
    data for the dynamic allocation of resources; for example, it can enable elastically
    changing the number of executors in a particular stage. You can also extend the
    Kubernetes API to include custom or application-specific resources; for example,
    you can create dashboards to display the progress of jobs:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00351.jpeg)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
- en: Kubernetes also provides useful administrative features to help manage clusters,
    for example, RBAC and namespace-level resource quotas, audit logging, monitoring
    node, pod, cluster-level metrics, and so on.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we presented several Spark SQL-based application architectures
    for building highly-scalable applications. We explored the main concepts and challenges
    in batch processing and stream processing. We discussed the features of Spark
    SQL that can help in building robust ETL pipelines. We also presented some code
    towards building a scalable monitoring application. Additionally, we explored
    an efficient deployment technique for machine learning pipelines, and some basic
    concepts involved in using cluster managers such as Mesos and Kubernetes.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, this book attempts to help you build a strong foundation in Spark
    SQL and Scala. However, there are still many areas that you can explore in greater
    depth to build deeper expertise. Depending on your specific domain, the nature
    of data and problems could vary widely and your approach to solving them would
    typically encompass one or more areas described in this book. However, in all
    cases EDA and data munging skills will be required, and the more you practice,
    the more proficient you will become at it. Try downloading and working with different
    types of data covering structured, semi-structured and unstructured data. Additionally,
    read the references mentioned in all the chapters to get deeper insights into
    how other data science practitioners approach problems. Reference Apache Spark
    site for latest releases of the software, and explore other machine learning algorithms
    you could use in your ML pipelines. Finally, topics such as deep learning and
    cost-based optimizations are still evolving in Spark, try to keep up with the
    developments in these areas as they will be key to solving many interesting problems
    in the near future.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
