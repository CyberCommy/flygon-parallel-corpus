- en: Chapter 2.  Data Processing Using the DataStream API
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Real-time analytics is currently an important issue. Many different domains
    need to process data in real time. So far there have been multiple technologies
    trying to provide this capability. Technologies such as Storm and Spark have been
    on the market for a long time now. Applications derived from the **Internet of
    Things** (**IoT**) need data to be stored, processed, and analyzed in real or
    near real time. In order to cater for such needs, Flink provides a streaming data
    processing API called DataStream API.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to look at the details relating to DataStream
    API, covering the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Execution environment
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data sources
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformations
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data sinks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connectors
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use case - sensor data analytics
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Any Flink program works on a certain defined anatomy as follows:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '![Data Processing Using the DataStream API](img/image_02_001.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
- en: We will be looking at each step and how we can use DataStream API with this
    anatomy.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Execution environment
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to start writing a Flink program, we first need to get an existing
    execution environment or create one. Depending upon what you are trying to do,
    Flink supports:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Getting an already existing Flink environment
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a local environment
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a remote environment
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Typically, you only need to use `getExecutionEnvironment()`. This will do the
    right thing based on your context. If you are executing on a local environment
    in an IDE then it will start a local execution environment. Otherwise, if you
    are executing the JAR then the Flink cluster manager will execute the program
    in a distributed manner.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: If you want to create a local or remote environment on your own then you can
    also choose do so by using methods such as `createLocalEnvironment()` and `createRemoteEnvironment` (`String
    host`, `int port`, `String`, and `.jar` files).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Data sources
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sources are places where the Flink program expects to get its data from. This
    is a second step in the Flink program's anatomy. Flink supports a number of pre-implemented
    data source functions. It also supports writing custom data source functions so
    anything that is not supported can be programmed easily. First let's try to understand
    the built-in source functions.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Socket-based
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'DataStream API supports reading data from a socket. You just need to specify
    the host and port to read the data from and it will do the work:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You can also choose to specify the delimiter:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You can also specify the maximum number of times the API should try to fetch
    the data:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: File-based
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can also choose to stream data from a file source using file-based source
    functions in Flink. You can use `readTextFile(String path)` to stream data from
    a file specified in the path. By default it will read `TextInputFormat` and will
    read strings line by line.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'If the file format is other than text, you can specify the same using these
    functions:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Flink also supports reading file streams as they are produced using the `readFileStream()`
    function:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You just need to specify the file path, the polling interval in which the file
    path should be polled, and the watch type. Watch types consist of three types:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '`FileMonitoringFunction.WatchType.ONLY_NEW_FILES` is used when the system should
    process only new files'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FileMonitoringFunction.WatchType.PROCESS_ONLY_APPENDED` is used when the system
    should process only appended contents of files'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FileMonitoringFunction.WatchType.REPROCESS_WITH_APPENDED` is used when the
    system should re-process not only the appended contents of files but also the
    previous content in the file'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If the file is not a text file, then we do have an option to use following
    function, which lets us define the file input format:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Internally, it divides the reading file task into two sub-tasks. One sub task
    only monitors the file path based on the `WatchType` given. The second sub-task
    does the actual file reading in parallel. The sub-task which monitors the file
    path is a non-parallel sub-task. Its job is to keep scanning the file path based
    on the polling interval and report files to be processed, split the files, and
    assign the splits to the respective downstream threads:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '![File-based](img/image_02_002.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
- en: Transformations
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data transformations transform the data stream from one form into another. The
    input could be one or more data streams and the output could also be zero, or
    one or more data streams. Now let's try to understand each transformation one
    by one.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Map
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is one of the simplest transformations, where the input is one data stream
    and the output is also one data stream.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In Scala:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: FlatMap
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: FlatMap takes one record and outputs zero, one, or more than one record.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In Scala:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Filter
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Filter functions evaluate the conditions and then, if they result as true, only
    emit the record. Filter functions can output zero records.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In Scala:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: KeyBy
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: KeyBy logically partitions the stream-based on the key. Internally it uses hash
    functions to partition the stream. It returns `KeyedDataStream`.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In Scala:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Reduce
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reduce rolls out the `KeyedDataStream` by reducing the last reduced value with
    the current value. The following code does the sum reduce of a `KeyedDataStream`.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In Scala:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Fold
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fold rolls out the `KeyedDataStream` by combining the last folder stream with
    the current record. It emits a data stream back.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'In Scala:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The preceding given function when applied on a stream of (1,2,3,4,5) would
    emit a stream like this: `Start=1=2=3=4=5`'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Aggregations
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DataStream API supports various aggregations such as `min`, `max`, `sum`, and
    so on. These functions can be applied on `KeyedDataStream` in order to get rolling
    aggregations.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In Scala:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The difference between `max` and `maxBy` is that max returns the maximum value
    in a stream but `maxBy` returns a key that has a maximum value. The same applies
    to `min` and `minBy`.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Window
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `window` function allows the grouping of existing `KeyedDataStreams` by
    time or other conditions. The following transformation emits groups of records
    by a time window of 10 seconds.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In Scala:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Flink defines slices of data in order to process (potentially) infinite data
    streams. These slices are called windows. This slicing helps processing data in
    chunks by applying transformations. To do windowing on a stream, we need to assign
    a key on which the distribution can be made and a function which describes what
    transformations to perform on a windowed stream.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: To slice streams into windows, we can use pre-implemented Flink window assigners.
    We have options such as, tumbling windows, sliding windows, global and session
    windows. Flink also allows you to write custom window assigners by extending `WindowAssginer`
    class. Let's try to understand how these various assigners work.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Global windows
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Global windows are never-ending windows unless specified by a trigger. Generally
    in this case, each element is assigned to one single per-key global Window. If
    we don't specify any trigger, no computation will ever get triggered.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Tumbling windows
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tumbling windows are created based on certain times. They are fixed-length windows
    and non over lapping. Tumbling windows should be useful when you need to do computation
    of elements in specific time. For example, tumbling window of 10 minutes can be
    used to compute a group of events occurring in 10 minutes time.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Sliding windows
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sliding windows are like tumbling windows but they are overlapping. They are
    fixed-length windows overlapping the previous ones by a user given window slide
    parameter. This type of windowing is useful when you want to compute something
    out of a group of events occurring in a certain time frame.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Session windows
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Session windows are useful when windows boundaries need to be decided upon the
    input data. Session windows allows flexibility in window start time and window
    size. We can also provide session gap configuration parameter which indicates
    how long to wait before considering the session in closed.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: WindowAll
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `windowAll` function allows the grouping of regular data streams. Generally
    this is a non-parallel data transformation as it runs on non-partitioned streams
    of data.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'In Scala:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Similar to regular data stream functions, we have window data stream functions
    as well. The only difference is they work on windowed data streams. So window
    reduce works like the `Reduce` function, Window fold works like the `Fold` function,
    and there are aggregations as well.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Union
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `Union` function performs the union of two or more data streams together.
    This does the combining of data streams in parallel. If we combine one stream
    with itself then it outputs each record twice.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In Scala:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Window join
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can also join two data streams by some keys in a common window. The following
    example shows the joining of two streams in a Window of `5` seconds where the
    joining condition of the first attribute of the first stream is equal to the second
    attribute of the other stream.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In Scala:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Split
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This function splits the stream into two or more streams based on the criteria.
    This can be used when you get a mixed stream and you may want to process each
    data separately.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'In Scala:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Select
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This function allows you to select a specific stream from the split stream.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'In Scala:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Project
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `Project` function allows you to select a sub-set of attributes from the
    event stream and only sends selected elements to the next processing stream.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'In Scala:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The preceding function selects the attribute numbers `2` and `3` from the given
    records. The following is the sample input and output records:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Physical partitioning
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Flink allows us to perform physical partitioning of the stream data. You have
    an option to provide custom partitioning. Let us have a look at the different
    types of partitioning.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Custom partitioning
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned earlier, you can provide custom implementation of a partitioner.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'In Scala:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: While writing a custom partitioner you need make sure you implement an efficient
    hash function.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Random partitioning
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Random partitioning randomly partitions data streams in an evenly manner.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'In Scala:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Rebalancing partitioning
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This type of partitioning helps distribute the data evenly. It uses a round
    robin method for distribution. This type of partitioning is good when data is
    skewed.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'In Scala:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Rescaling
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Rescaling is used to distribute the data across operations, perform transformations
    on sub-sets of data and combine them together. This rebalancing happens over a
    single node only, hence it does not require any data transfer across networks.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the distribution:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '![Rescaling](img/image_02_003.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
- en: 'In Java:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'In Scala:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Broadcasting
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Broadcasting distributes all records to each partition. This fans out each and
    every element to all partitions.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'In Scala:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Data sinks
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After the data transformations are done, we need to save results into some
    place. The following are some options Flink provides us to save results:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '`writeAsText()`: Writes records one line at a time as strings.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`writeAsCsV()`: Writes tuples as comma separated value files. Row and fields
    delimiter can also be configured.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`print()/printErr()`: Writes records to the standard output. You can also choose
    to write to the standard error.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`writeUsingOutputFormat()`: You can also choose to provide a custom output
    format. While defining the custom format you need to extend the `OutputFormat`
    which takes care of serialization and deserialization.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`writeToSocket()`: Flink supports writing data to a specific socket as well.
    It is required to define `SerializationSchema` for proper serialization and formatting.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Event time and watermarks
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Flink Streaming API takes inspiration from Google Data Flow model. It supports
    different concepts of time for its streaming API. In general, there three places
    where we can capture time in a streaming environment. They are as follows
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Event time
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The time at which event occurred on its producing device. For example in IoT
    project, the time at which sensor captures a reading. Generally these event times
    needs to embed in the record before they enter Flink. At the time processing,
    these timestamps are extracted and considering for windowing. Event time processing
    can be used for out of order events.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Processing time
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Processing time is the time of machine executing the stream of data processing.
    Processing time windowing considers only that timestamps where event is getting
    processed. Processing time is simplest way of stream processing as it does not
    require any synchronization between processing machines and producing machines.
    In distributed asynchronous environment processing time does not provide determinism
    as it is dependent on the speed at which records flow in the system.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Ingestion time
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is time at which a particular event enters Flink. All time based operations
    refer to this timestamp. Ingestion time is more expensive operation than processing
    but it gives predictable results. Ingestion time programs cannot handle any out
    of order events as it assigs timestamp only after the event is entered the Flink
    system.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Here is an example which shows how to set event time and watermarks. In case
    of ingestion time and processing time, we just need to the time characteristics
    and watermark generation is taken care automatically. Following is a code snippet
    for the same.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'In Scala:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'In case of event time stream programs, we need to specify the way to assign
    watermarks and timestamps. There are two ways of assigning watermarks and timestamps:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Directly from data source attribute
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a timestamp assigner
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To work with event time streams, we need to assign the time characteristic as
    follows
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'In Scala:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: It is always best to store event time while storing the record in source. Flink
    also supports some pre-defined timestamp extractors and watermark generators.
    Refer to [https://ci.apache.org/projects/flink/flink-docs-release-1.2/dev/event_timestamp_extractors.html](https://ci.apache.org/projects/flink/flink-docs-release-1.2/dev/event_timestamp_extractors.html).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Connectors
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Flink supports various connectors that allow data read/writes across
    various technologies. Let's learn more about this.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Kafka connector
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kafka is a publish-subscribe, distributed, message queuing system that allows
    users to publish messages to a certain topic; this is then distributed to the
    subscribers of the topic. Flink provides options to define a Kafka consumer as
    a data source in Flink Streaming. In order to use the Flink Kafka connector, we
    need to use a specific JAR file.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows how the Flink Kafka connector works:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '![Kafka connector](img/image_02_004.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
- en: 'We need to use the following Maven dependency to use the connector. I have
    been using Kafka version 0.9 so I will be adding the following dependency in `pom.xml`:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Now let''s try to understand how to use the Kafka consumer as the Kafka source:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'In Scala:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: In the preceding code, we first set the properties of the Kafka host and the
    zookeeper host and port. Next we need to specify the topic name, in this case
    `mytopic`. So if any messages get published to the `mytopic` topic, they will
    be processed by the Flink streams.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: If you get data in a different format, then you can also specify your custom
    schema for deserialization. By default, Flink supports string and JSON deserializers.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: In order to enable fault tolerance, we need to enable checkpointing in Flink.
    Flink is keen on taking snapshots of the state in a periodic manner. In the case
    of failure, it will restore to the last checkpoint and then restart the processing.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also define the Kafka producer as a sink. This will write the data to
    a Kafka topic. The following is a way to write data to a Kafka topic:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 'In Scala:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'In Java:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Twitter connector
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'These days, it is very important to have the ability to fetch data from Twitter
    and process it. Many companies use Twitter data for doing sentiment analytics
    for various products, services, movies, reviews, and so on. Flink provides the
    Twitter connector as one data source. To use the connector, you need to have a
    Twitter account. Once you have a Twitter account, you need to create a Twitter
    application and generate authentication keys to be used by the connector. Here
    is a link that will help you to generate tokens: [https://dev.twitter.com/oauth/overview/application-owner-access-tokens](https://dev.twitter.com/oauth/overview/application-owner-access-tokens).'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'The Twitter connector can be used through the Java or Scala API:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '![Twitter connector](img/image_02_005.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
- en: 'Once tokens are generated, we can start writing the program to fetch data from
    Twitter. First we need to add a Maven dependency:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Next we add Twitter as a data source. The following is the sample code:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'In Scala:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'In the preceding code, we first set properties for the token we got. And then
    we add the `TwitterSource`. If the given information is correct then you will
    start fetching the data from Twitter. `TwitterSource` emits the data in a JSON
    string format. A sample Twitter JSON looks like the following:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '`TwitterSource` provides various endpoints. By default it uses `StatusesSampleEndpoint`,
    which returns a set of random tweets. If you need to add some filters and don''t
    want to use the default endpoint, you can implement the `TwitterSource.EndpointInitializer`
    interface.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to fetch data from Twitter, we can then decide what to
    do with this data depending upon our use case. We can process, store, or analyze
    the data.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: RabbitMQ connector
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RabbitMQ is a widely used distributed, high-performance, message queuing system.
    It is used as a message delivery system for high throughput operations. It allows
    you to create a distributed message queue and include publishers and subscribers
    in the queue. More reading on RabbitMQ can be done at following link [https://www.rabbitmq.com/](https://www.rabbitmq.com/)
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Flink supports fetching and publishing data to and from RabbitMQ. It provides
    a connector that can act as a data source of data streams.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: 'For the RabbitMQ connector to work, we need to provide following information:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: RabbitMQ configurations such as host, port, user credentials, and so on.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Queue, the RabbitMQ queue name which you wish to subscribe.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Correlation IDs is a RabbitMQ feature used for correlating the request and response
    by a unique ID in a distributed world. The Flink RabbitMQ connector provides an
    interface to set this to true or false depending on whether you are using it or
    not.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deserialization schema--RabbitMQ stores and transports the data in a serialized
    manner to avoid network traffic. So when the message is received, the subscriber
    should know how to deserialize the message. The Flink connector provides us with
    some default deserializers such as the string deserializer.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RabbitMQ source provides us with the following options on stream deliveries:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'Exactly once: Using RabbitMQ correlation IDs and the Flink check-pointing mechanism
    with RabbitMQ transactions'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At-least once: When Flink checkpointing is enabled but RabbitMQ correlation
    IDs are not set'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No strong delivery guarantees with the RabbitMQ auto-commit mode
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is a diagram to help you understand the RabbitMQ connector in better manner:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '![RabbitMQ connector](img/image_02_006.jpg)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
- en: 'Now let''s look at how to write code to get this connector working. Like other
    connectors, we need to add a Maven dependency to the code:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The following snippet shows how to use the RabbitMQ connector in Java:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Similarly, in Scala the code can written as follows:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'We can also use the RabbitMQ connector as a Flink sink. If you want to send
    processes back to some different RabbitMQ queue, the following is a way to do
    so. We need to provide three important configurations:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: RabbitMQ configurations
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Queue name--Where to send back the processed data
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serialization schema--Schema for RabbitMQ to convert the data into bytes
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is sample code in Java to show how to use this connector as a
    Flink sink:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'The same can be done in Scala:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: ElasticSearch connector
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ElasticSearch is a distributed, low-latency, full text search engine system
    that allows us to index documents of our choice and then allows us to do a full
    text search over the set of documents. More on ElasticSearch can be read here
    at [https://www.elastic.co/](https://www.elastic.co/).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: In many use cases, you may want to process data using Flink and then store it
    in ElasticSearch. To enable this, Flink supports the ElasticSearch connector.
    So far, ElasticSearch has had two major releases. Flink supports them both.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: 'For ElasticSearch 1.X, the following Maven dependency needs to be added:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The Flink connector provides a sink to write data to ElasticSearch. It uses
    two methods to connect to ElasticSearch:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: Embedded node
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transport client
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram illustrates this:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '![ElasticSearch connector](img/image_02_007.jpg)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
- en: Embedded node mode
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the embedded node mode, the sink uses BulkProcessor to send the documents
    to ElasticSearch. We can configure how many requests to buffer before sending
    documents to ElasticSearch.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the code snippet:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'In the preceding code snippet, we create a hash map with configurations such
    as the cluster name and how many documents to buffer before sending the request.
    Then we add the sink to the stream, specifying the index, type, and the document
    to store. Similar code in Scala follows:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Transport client mode
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ElasticSearch allows connections through the transport client on port 9300\.
    Flink supports connecting using those through its connector. The only thing we
    need to mention here is all the ElasticSearch nodes present in the cluster in
    configurations.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the snippet in Java:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Here as well we provide the details about the cluster name, nodes, ports, maximum
    requests to send in bulk, and so on. Similar code in Scala can be written as follows:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Cassandra connector
  id: totrans-293
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Cassandra is a distributed, low latency, NoSQL database. It is a key value-based
    database. Many high throughput applications use Cassandra as their primary database.
    Cassandra works with a distributed cluster mode, where there is no master-slave
    architecture. Reads and writes can be felicitated by any node. More on Cassandra
    can be found at: [http://cassandra.apache.org/](http://cassandra.apache.org/).'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: 'Apache Flink provides a connector which can write data to Cassandra. In many
    applications, people may want to store streaming data from Flink into Cassandra.
    The following diagram shows a simple design of the Cassandra sink:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '![Cassandra connector](img/image_02_008.jpg)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
- en: 'Like other connectors, to get this we need to add it as a maven dependency:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Once the dependency is added, we just need to add the Cassandra sink with its
    configurations, as follows:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'The preceding code writes stream of data into a table called **events**. The
    table expects an event ID and a message. Similarly in Scala:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Use case - sensor data analytics
  id: totrans-304
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have looked at various aspects of DataStream API, let's try to use
    these concepts to solve a real world use case. Consider a machine which has sensor
    installed on it and we wish to collect data from these sensors and calculate average
    temperature per sensor every five minutes.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: 'Following would be the architecture:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '![Use case - sensor data analytics](img/image_02_009.jpg)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
- en: In this scenario, we assume that sensors are sending information to Kafka topic
    called **temp** with information as (timestamp, temperature, sensor-ID). Now we
    need to write code to read data from Kafka topics and processing it using Flink
    transformation.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们假设传感器正在向名为**temp**的Kafka主题发送信息，信息格式为（时间戳，温度，传感器ID）。现在我们需要编写代码从Kafka主题中读取数据，并使用Flink转换进行处理。
- en: Here important thing to consider is as we already have timestamp values coming
    from sensor, we can use Event Time computations for time factors. This means we
    would be able to take care of events even if they reach out of order.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里需要考虑的重要事情是，由于我们已经从传感器那里得到了时间戳数值，我们可以使用事件时间计算来处理时间因素。这意味着即使事件到达时是无序的，我们也能够处理这些事件。
- en: We start with simple streaming execution environment which will be reading data
    from Kafka. Since we have timestamps in events, we will be writing a custom timestamp
    and watermark extractor to read the timestamp values and do window processing
    based on that. Here is code snippet for the same.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从简单的流执行环境开始，它将从Kafka中读取数据。由于事件中有时间戳，我们将编写自定义的时间戳和水印提取器来读取时间戳数值，并根据此进行窗口处理。以下是相同的代码片段。
- en: '[PRE71]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Here we assume that we receive events in Kafka topics as strings and in the
    format:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们假设我们从Kafka主题中以字符串格式接收事件，并且格式为：
- en: '[PRE72]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'The following an example code to extract timestamp from record:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是从记录中提取时间戳的示例代码：
- en: '[PRE73]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Now we simply created keyed data stream and perform average calculation on
    temperature values as shown in the following code snippet:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们简单地创建了分区数据流，并对温度数值进行了平均计算，如下面的代码片段所示：
- en: '[PRE74]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: When execute the preceding given code, and if proper sensor events are published
    on Kafka topics then we will get the average temperature per sensor every five
    minutes.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 当执行上述给定的代码时，如果在Kafka主题上发布了适当的传感器事件，那么我们将每五分钟得到每个传感器的平均温度。
- en: The complete code is available on GitHub at [https://github.com/deshpandetanmay/mastering-flink/tree/master/chapter02/flink-streaming](https://github.com/deshpandetanmay/mastering-flink/tree/master/chapter02/flink-streaming).
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的代码可以在GitHub上找到：[https://github.com/deshpandetanmay/mastering-flink/tree/master/chapter02/flink-streaming](https://github.com/deshpandetanmay/mastering-flink/tree/master/chapter02/flink-streaming)。
- en: Summary
  id: totrans-320
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we started with Flink''s most powerful API: DataStream API.
    We looked at how data sources, transformations, and sinks work together. Then
    we looked at various technology connectors such as ElasticSearch, Cassandra, Kafka,
    RabbitMQ, and so on.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们从Flink最强大的API开始：DataStream API。我们看了数据源、转换和接收器是如何一起工作的。然后我们看了各种技术连接器，比如ElasticSearch、Cassandra、Kafka、RabbitMQ等等。
- en: At the end, we also tried to apply our learning to solve a real-world sensor
    data analytics use case.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还尝试将我们的学习应用于解决真实世界的传感器数据分析用例。
- en: In the next chapter, we are going to learn about another very important API
    from Flink's ecosystem point of view the DataSet API.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习Flink生态系统中另一个非常重要的API，即DataSet API。
