- en: Chapter 2.  Data Processing Using the DataStream API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Real-time analytics is currently an important issue. Many different domains
    need to process data in real time. So far there have been multiple technologies
    trying to provide this capability. Technologies such as Storm and Spark have been
    on the market for a long time now. Applications derived from the **Internet of
    Things** (**IoT**) need data to be stored, processed, and analyzed in real or
    near real time. In order to cater for such needs, Flink provides a streaming data
    processing API called DataStream API.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to look at the details relating to DataStream
    API, covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Execution environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data sources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data sinks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use case - sensor data analytics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Any Flink program works on a certain defined anatomy as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Data Processing Using the DataStream API](img/image_02_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We will be looking at each step and how we can use DataStream API with this
    anatomy.
  prefs: []
  type: TYPE_NORMAL
- en: Execution environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to start writing a Flink program, we first need to get an existing
    execution environment or create one. Depending upon what you are trying to do,
    Flink supports:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting an already existing Flink environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a local environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a remote environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Typically, you only need to use `getExecutionEnvironment()`. This will do the
    right thing based on your context. If you are executing on a local environment
    in an IDE then it will start a local execution environment. Otherwise, if you
    are executing the JAR then the Flink cluster manager will execute the program
    in a distributed manner.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to create a local or remote environment on your own then you can
    also choose do so by using methods such as `createLocalEnvironment()` and `createRemoteEnvironment` (`String
    host`, `int port`, `String`, and `.jar` files).
  prefs: []
  type: TYPE_NORMAL
- en: Data sources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sources are places where the Flink program expects to get its data from. This
    is a second step in the Flink program's anatomy. Flink supports a number of pre-implemented
    data source functions. It also supports writing custom data source functions so
    anything that is not supported can be programmed easily. First let's try to understand
    the built-in source functions.
  prefs: []
  type: TYPE_NORMAL
- en: Socket-based
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'DataStream API supports reading data from a socket. You just need to specify
    the host and port to read the data from and it will do the work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also choose to specify the delimiter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also specify the maximum number of times the API should try to fetch
    the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: File-based
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can also choose to stream data from a file source using file-based source
    functions in Flink. You can use `readTextFile(String path)` to stream data from
    a file specified in the path. By default it will read `TextInputFormat` and will
    read strings line by line.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the file format is other than text, you can specify the same using these
    functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Flink also supports reading file streams as they are produced using the `readFileStream()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'You just need to specify the file path, the polling interval in which the file
    path should be polled, and the watch type. Watch types consist of three types:'
  prefs: []
  type: TYPE_NORMAL
- en: '`FileMonitoringFunction.WatchType.ONLY_NEW_FILES` is used when the system should
    process only new files'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FileMonitoringFunction.WatchType.PROCESS_ONLY_APPENDED` is used when the system
    should process only appended contents of files'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FileMonitoringFunction.WatchType.REPROCESS_WITH_APPENDED` is used when the
    system should re-process not only the appended contents of files but also the
    previous content in the file'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If the file is not a text file, then we do have an option to use following
    function, which lets us define the file input format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Internally, it divides the reading file task into two sub-tasks. One sub task
    only monitors the file path based on the `WatchType` given. The second sub-task
    does the actual file reading in parallel. The sub-task which monitors the file
    path is a non-parallel sub-task. Its job is to keep scanning the file path based
    on the polling interval and report files to be processed, split the files, and
    assign the splits to the respective downstream threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '![File-based](img/image_02_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data transformations transform the data stream from one form into another. The
    input could be one or more data streams and the output could also be zero, or
    one or more data streams. Now let's try to understand each transformation one
    by one.
  prefs: []
  type: TYPE_NORMAL
- en: Map
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is one of the simplest transformations, where the input is one data stream
    and the output is also one data stream.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: FlatMap
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: FlatMap takes one record and outputs zero, one, or more than one record.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Filter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Filter functions evaluate the conditions and then, if they result as true, only
    emit the record. Filter functions can output zero records.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: KeyBy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: KeyBy logically partitions the stream-based on the key. Internally it uses hash
    functions to partition the stream. It returns `KeyedDataStream`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Reduce
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reduce rolls out the `KeyedDataStream` by reducing the last reduced value with
    the current value. The following code does the sum reduce of a `KeyedDataStream`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Fold
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fold rolls out the `KeyedDataStream` by combining the last folder stream with
    the current record. It emits a data stream back.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding given function when applied on a stream of (1,2,3,4,5) would
    emit a stream like this: `Start=1=2=3=4=5`'
  prefs: []
  type: TYPE_NORMAL
- en: Aggregations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DataStream API supports various aggregations such as `min`, `max`, `sum`, and
    so on. These functions can be applied on `KeyedDataStream` in order to get rolling
    aggregations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The difference between `max` and `maxBy` is that max returns the maximum value
    in a stream but `maxBy` returns a key that has a maximum value. The same applies
    to `min` and `minBy`.
  prefs: []
  type: TYPE_NORMAL
- en: Window
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `window` function allows the grouping of existing `KeyedDataStreams` by
    time or other conditions. The following transformation emits groups of records
    by a time window of 10 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Flink defines slices of data in order to process (potentially) infinite data
    streams. These slices are called windows. This slicing helps processing data in
    chunks by applying transformations. To do windowing on a stream, we need to assign
    a key on which the distribution can be made and a function which describes what
    transformations to perform on a windowed stream.
  prefs: []
  type: TYPE_NORMAL
- en: To slice streams into windows, we can use pre-implemented Flink window assigners.
    We have options such as, tumbling windows, sliding windows, global and session
    windows. Flink also allows you to write custom window assigners by extending `WindowAssginer`
    class. Let's try to understand how these various assigners work.
  prefs: []
  type: TYPE_NORMAL
- en: Global windows
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Global windows are never-ending windows unless specified by a trigger. Generally
    in this case, each element is assigned to one single per-key global Window. If
    we don't specify any trigger, no computation will ever get triggered.
  prefs: []
  type: TYPE_NORMAL
- en: Tumbling windows
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tumbling windows are created based on certain times. They are fixed-length windows
    and non over lapping. Tumbling windows should be useful when you need to do computation
    of elements in specific time. For example, tumbling window of 10 minutes can be
    used to compute a group of events occurring in 10 minutes time.
  prefs: []
  type: TYPE_NORMAL
- en: Sliding windows
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sliding windows are like tumbling windows but they are overlapping. They are
    fixed-length windows overlapping the previous ones by a user given window slide
    parameter. This type of windowing is useful when you want to compute something
    out of a group of events occurring in a certain time frame.
  prefs: []
  type: TYPE_NORMAL
- en: Session windows
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Session windows are useful when windows boundaries need to be decided upon the
    input data. Session windows allows flexibility in window start time and window
    size. We can also provide session gap configuration parameter which indicates
    how long to wait before considering the session in closed.
  prefs: []
  type: TYPE_NORMAL
- en: WindowAll
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `windowAll` function allows the grouping of regular data streams. Generally
    this is a non-parallel data transformation as it runs on non-partitioned streams
    of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Similar to regular data stream functions, we have window data stream functions
    as well. The only difference is they work on windowed data streams. So window
    reduce works like the `Reduce` function, Window fold works like the `Fold` function,
    and there are aggregations as well.
  prefs: []
  type: TYPE_NORMAL
- en: Union
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `Union` function performs the union of two or more data streams together.
    This does the combining of data streams in parallel. If we combine one stream
    with itself then it outputs each record twice.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Window join
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can also join two data streams by some keys in a common window. The following
    example shows the joining of two streams in a Window of `5` seconds where the
    joining condition of the first attribute of the first stream is equal to the second
    attribute of the other stream.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Split
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This function splits the stream into two or more streams based on the criteria.
    This can be used when you get a mixed stream and you may want to process each
    data separately.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Select
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This function allows you to select a specific stream from the split stream.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Project
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `Project` function allows you to select a sub-set of attributes from the
    event stream and only sends selected elements to the next processing stream.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding function selects the attribute numbers `2` and `3` from the given
    records. The following is the sample input and output records:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Physical partitioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Flink allows us to perform physical partitioning of the stream data. You have
    an option to provide custom partitioning. Let us have a look at the different
    types of partitioning.
  prefs: []
  type: TYPE_NORMAL
- en: Custom partitioning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned earlier, you can provide custom implementation of a partitioner.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: While writing a custom partitioner you need make sure you implement an efficient
    hash function.
  prefs: []
  type: TYPE_NORMAL
- en: Random partitioning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Random partitioning randomly partitions data streams in an evenly manner.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Rebalancing partitioning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This type of partitioning helps distribute the data evenly. It uses a round
    robin method for distribution. This type of partitioning is good when data is
    skewed.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Rescaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Rescaling is used to distribute the data across operations, perform transformations
    on sub-sets of data and combine them together. This rebalancing happens over a
    single node only, hence it does not require any data transfer across networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Rescaling](img/image_02_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Broadcasting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Broadcasting distributes all records to each partition. This fans out each and
    every element to all partitions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Data sinks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After the data transformations are done, we need to save results into some
    place. The following are some options Flink provides us to save results:'
  prefs: []
  type: TYPE_NORMAL
- en: '`writeAsText()`: Writes records one line at a time as strings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`writeAsCsV()`: Writes tuples as comma separated value files. Row and fields
    delimiter can also be configured.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`print()/printErr()`: Writes records to the standard output. You can also choose
    to write to the standard error.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`writeUsingOutputFormat()`: You can also choose to provide a custom output
    format. While defining the custom format you need to extend the `OutputFormat`
    which takes care of serialization and deserialization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`writeToSocket()`: Flink supports writing data to a specific socket as well.
    It is required to define `SerializationSchema` for proper serialization and formatting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Event time and watermarks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Flink Streaming API takes inspiration from Google Data Flow model. It supports
    different concepts of time for its streaming API. In general, there three places
    where we can capture time in a streaming environment. They are as follows
  prefs: []
  type: TYPE_NORMAL
- en: Event time
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The time at which event occurred on its producing device. For example in IoT
    project, the time at which sensor captures a reading. Generally these event times
    needs to embed in the record before they enter Flink. At the time processing,
    these timestamps are extracted and considering for windowing. Event time processing
    can be used for out of order events.
  prefs: []
  type: TYPE_NORMAL
- en: Processing time
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Processing time is the time of machine executing the stream of data processing.
    Processing time windowing considers only that timestamps where event is getting
    processed. Processing time is simplest way of stream processing as it does not
    require any synchronization between processing machines and producing machines.
    In distributed asynchronous environment processing time does not provide determinism
    as it is dependent on the speed at which records flow in the system.
  prefs: []
  type: TYPE_NORMAL
- en: Ingestion time
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is time at which a particular event enters Flink. All time based operations
    refer to this timestamp. Ingestion time is more expensive operation than processing
    but it gives predictable results. Ingestion time programs cannot handle any out
    of order events as it assigs timestamp only after the event is entered the Flink
    system.
  prefs: []
  type: TYPE_NORMAL
- en: Here is an example which shows how to set event time and watermarks. In case
    of ingestion time and processing time, we just need to the time characteristics
    and watermark generation is taken care automatically. Following is a code snippet
    for the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'In case of event time stream programs, we need to specify the way to assign
    watermarks and timestamps. There are two ways of assigning watermarks and timestamps:'
  prefs: []
  type: TYPE_NORMAL
- en: Directly from data source attribute
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a timestamp assigner
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To work with event time streams, we need to assign the time characteristic as
    follows
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: It is always best to store event time while storing the record in source. Flink
    also supports some pre-defined timestamp extractors and watermark generators.
    Refer to [https://ci.apache.org/projects/flink/flink-docs-release-1.2/dev/event_timestamp_extractors.html](https://ci.apache.org/projects/flink/flink-docs-release-1.2/dev/event_timestamp_extractors.html).
  prefs: []
  type: TYPE_NORMAL
- en: Connectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Flink supports various connectors that allow data read/writes across
    various technologies. Let's learn more about this.
  prefs: []
  type: TYPE_NORMAL
- en: Kafka connector
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kafka is a publish-subscribe, distributed, message queuing system that allows
    users to publish messages to a certain topic; this is then distributed to the
    subscribers of the topic. Flink provides options to define a Kafka consumer as
    a data source in Flink Streaming. In order to use the Flink Kafka connector, we
    need to use a specific JAR file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows how the Flink Kafka connector works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Kafka connector](img/image_02_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We need to use the following Maven dependency to use the connector. I have
    been using Kafka version 0.9 so I will be adding the following dependency in `pom.xml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s try to understand how to use the Kafka consumer as the Kafka source:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we first set the properties of the Kafka host and the
    zookeeper host and port. Next we need to specify the topic name, in this case
    `mytopic`. So if any messages get published to the `mytopic` topic, they will
    be processed by the Flink streams.
  prefs: []
  type: TYPE_NORMAL
- en: If you get data in a different format, then you can also specify your custom
    schema for deserialization. By default, Flink supports string and JSON deserializers.
  prefs: []
  type: TYPE_NORMAL
- en: In order to enable fault tolerance, we need to enable checkpointing in Flink.
    Flink is keen on taking snapshots of the state in a periodic manner. In the case
    of failure, it will restore to the last checkpoint and then restart the processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also define the Kafka producer as a sink. This will write the data to
    a Kafka topic. The following is a way to write data to a Kafka topic:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Twitter connector
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'These days, it is very important to have the ability to fetch data from Twitter
    and process it. Many companies use Twitter data for doing sentiment analytics
    for various products, services, movies, reviews, and so on. Flink provides the
    Twitter connector as one data source. To use the connector, you need to have a
    Twitter account. Once you have a Twitter account, you need to create a Twitter
    application and generate authentication keys to be used by the connector. Here
    is a link that will help you to generate tokens: [https://dev.twitter.com/oauth/overview/application-owner-access-tokens](https://dev.twitter.com/oauth/overview/application-owner-access-tokens).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Twitter connector can be used through the Java or Scala API:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Twitter connector](img/image_02_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Once tokens are generated, we can start writing the program to fetch data from
    Twitter. First we need to add a Maven dependency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we add Twitter as a data source. The following is the sample code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we first set properties for the token we got. And then
    we add the `TwitterSource`. If the given information is correct then you will
    start fetching the data from Twitter. `TwitterSource` emits the data in a JSON
    string format. A sample Twitter JSON looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '`TwitterSource` provides various endpoints. By default it uses `StatusesSampleEndpoint`,
    which returns a set of random tweets. If you need to add some filters and don''t
    want to use the default endpoint, you can implement the `TwitterSource.EndpointInitializer`
    interface.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to fetch data from Twitter, we can then decide what to
    do with this data depending upon our use case. We can process, store, or analyze
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: RabbitMQ connector
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RabbitMQ is a widely used distributed, high-performance, message queuing system.
    It is used as a message delivery system for high throughput operations. It allows
    you to create a distributed message queue and include publishers and subscribers
    in the queue. More reading on RabbitMQ can be done at following link [https://www.rabbitmq.com/](https://www.rabbitmq.com/)
  prefs: []
  type: TYPE_NORMAL
- en: Flink supports fetching and publishing data to and from RabbitMQ. It provides
    a connector that can act as a data source of data streams.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the RabbitMQ connector to work, we need to provide following information:'
  prefs: []
  type: TYPE_NORMAL
- en: RabbitMQ configurations such as host, port, user credentials, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Queue, the RabbitMQ queue name which you wish to subscribe.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Correlation IDs is a RabbitMQ feature used for correlating the request and response
    by a unique ID in a distributed world. The Flink RabbitMQ connector provides an
    interface to set this to true or false depending on whether you are using it or
    not.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deserialization schema--RabbitMQ stores and transports the data in a serialized
    manner to avoid network traffic. So when the message is received, the subscriber
    should know how to deserialize the message. The Flink connector provides us with
    some default deserializers such as the string deserializer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RabbitMQ source provides us with the following options on stream deliveries:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exactly once: Using RabbitMQ correlation IDs and the Flink check-pointing mechanism
    with RabbitMQ transactions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At-least once: When Flink checkpointing is enabled but RabbitMQ correlation
    IDs are not set'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No strong delivery guarantees with the RabbitMQ auto-commit mode
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is a diagram to help you understand the RabbitMQ connector in better manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '![RabbitMQ connector](img/image_02_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now let''s look at how to write code to get this connector working. Like other
    connectors, we need to add a Maven dependency to the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The following snippet shows how to use the RabbitMQ connector in Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, in Scala the code can written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also use the RabbitMQ connector as a Flink sink. If you want to send
    processes back to some different RabbitMQ queue, the following is a way to do
    so. We need to provide three important configurations:'
  prefs: []
  type: TYPE_NORMAL
- en: RabbitMQ configurations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Queue name--Where to send back the processed data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serialization schema--Schema for RabbitMQ to convert the data into bytes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is sample code in Java to show how to use this connector as a
    Flink sink:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'The same can be done in Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: ElasticSearch connector
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ElasticSearch is a distributed, low-latency, full text search engine system
    that allows us to index documents of our choice and then allows us to do a full
    text search over the set of documents. More on ElasticSearch can be read here
    at [https://www.elastic.co/](https://www.elastic.co/).
  prefs: []
  type: TYPE_NORMAL
- en: In many use cases, you may want to process data using Flink and then store it
    in ElasticSearch. To enable this, Flink supports the ElasticSearch connector.
    So far, ElasticSearch has had two major releases. Flink supports them both.
  prefs: []
  type: TYPE_NORMAL
- en: 'For ElasticSearch 1.X, the following Maven dependency needs to be added:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'The Flink connector provides a sink to write data to ElasticSearch. It uses
    two methods to connect to ElasticSearch:'
  prefs: []
  type: TYPE_NORMAL
- en: Embedded node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transport client
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram illustrates this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ElasticSearch connector](img/image_02_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Embedded node mode
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the embedded node mode, the sink uses BulkProcessor to send the documents
    to ElasticSearch. We can configure how many requests to buffer before sending
    documents to ElasticSearch.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code snippet, we create a hash map with configurations such
    as the cluster name and how many documents to buffer before sending the request.
    Then we add the sink to the stream, specifying the index, type, and the document
    to store. Similar code in Scala follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Transport client mode
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ElasticSearch allows connections through the transport client on port 9300\.
    Flink supports connecting using those through its connector. The only thing we
    need to mention here is all the ElasticSearch nodes present in the cluster in
    configurations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the snippet in Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Here as well we provide the details about the cluster name, nodes, ports, maximum
    requests to send in bulk, and so on. Similar code in Scala can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Cassandra connector
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Cassandra is a distributed, low latency, NoSQL database. It is a key value-based
    database. Many high throughput applications use Cassandra as their primary database.
    Cassandra works with a distributed cluster mode, where there is no master-slave
    architecture. Reads and writes can be felicitated by any node. More on Cassandra
    can be found at: [http://cassandra.apache.org/](http://cassandra.apache.org/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Apache Flink provides a connector which can write data to Cassandra. In many
    applications, people may want to store streaming data from Flink into Cassandra.
    The following diagram shows a simple design of the Cassandra sink:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cassandra connector](img/image_02_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Like other connectors, to get this we need to add it as a maven dependency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the dependency is added, we just need to add the Cassandra sink with its
    configurations, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code writes stream of data into a table called **events**. The
    table expects an event ID and a message. Similarly in Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: Use case - sensor data analytics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have looked at various aspects of DataStream API, let's try to use
    these concepts to solve a real world use case. Consider a machine which has sensor
    installed on it and we wish to collect data from these sensors and calculate average
    temperature per sensor every five minutes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following would be the architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Use case - sensor data analytics](img/image_02_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this scenario, we assume that sensors are sending information to Kafka topic
    called **temp** with information as (timestamp, temperature, sensor-ID). Now we
    need to write code to read data from Kafka topics and processing it using Flink
    transformation.
  prefs: []
  type: TYPE_NORMAL
- en: Here important thing to consider is as we already have timestamp values coming
    from sensor, we can use Event Time computations for time factors. This means we
    would be able to take care of events even if they reach out of order.
  prefs: []
  type: TYPE_NORMAL
- en: We start with simple streaming execution environment which will be reading data
    from Kafka. Since we have timestamps in events, we will be writing a custom timestamp
    and watermark extractor to read the timestamp values and do window processing
    based on that. Here is code snippet for the same.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we assume that we receive events in Kafka topics as strings and in the
    format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'The following an example code to extract timestamp from record:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we simply created keyed data stream and perform average calculation on
    temperature values as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: When execute the preceding given code, and if proper sensor events are published
    on Kafka topics then we will get the average temperature per sensor every five
    minutes.
  prefs: []
  type: TYPE_NORMAL
- en: The complete code is available on GitHub at [https://github.com/deshpandetanmay/mastering-flink/tree/master/chapter02/flink-streaming](https://github.com/deshpandetanmay/mastering-flink/tree/master/chapter02/flink-streaming).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we started with Flink''s most powerful API: DataStream API.
    We looked at how data sources, transformations, and sinks work together. Then
    we looked at various technology connectors such as ElasticSearch, Cassandra, Kafka,
    RabbitMQ, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: At the end, we also tried to apply our learning to solve a real-world sensor
    data analytics use case.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to learn about another very important API
    from Flink's ecosystem point of view the DataSet API.
  prefs: []
  type: TYPE_NORMAL
