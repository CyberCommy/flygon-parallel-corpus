- en: Using Spark SQL in Machine Learning Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: n this chapter, we will present typical use cases for using Spark SQL in machine
    learning applications. We will focus on the Spark machine learning API called
    `spark.ml`, which is the recommended solution for implementing ML workflows. The
    `spark.ml` API is built on DataFrames and provides many ready-to-use packages,
    including feature extractors, Transformers, selectors, and machine learning algorithms,
    such as classification, regression, and clustering algorithms. We will also use
    Apache Spark to perform **exploratory data analysis** (**EDA**), data pre-processing,
    feature engineering, and developing machine learning pipelines using `spark.ml`
    APIs and algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'More specifically, in this chapter, you will learn the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Key components of Spark ML pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand Feature engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing machine learning pipelines/applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code examples using Spark MLlib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing machine learning applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning, predictive analytics, and related data science topics are
    becoming increasingly popular for solving real-world problems across varied business
    domains.
  prefs: []
  type: TYPE_NORMAL
- en: Today, machine learning applications are driving mission-critical business decision-making
    in many organizations. These applications include recommendation engines, targeted
    advertising, speech recognition, fraud detection, image recognition and categorization,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will introduce the key components of the Spark ML pipeline
    API.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Spark ML pipelines and their components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The machine learning pipeline API was introduced in Apache Spark 1.2\. Spark
    MLlib provides an API for developers to create and execute complex ML workflows.
    The Pipeline API lets developers quickly assemble distributed machine learning
    pipelines as the API has been standardized for applying different machine learning
    algorithms. Additionally, we can also combine multiple machine learning algorithms
    into a single pipeline. These pipelines consist of several key components that
    ease the implementation of data analytics and machine learning applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main components of an ML pipeline are listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Datasets**: Spark SQL DataFrames/Datasets are used for storing and processing
    data in an ML pipeline. The DataFrames/Datasets API provides a standard API and
    a common way of dealing with both static data (typically, for batch processing)
    as well as streaming data (typically, for online stream processing). As we will
    see in the following sections, these Datasets will be used for storing and processing
    input data, transformed input data, feature vectors, labels, predictions, and
    so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pipelines**: ML workflows are implemented as pipelines consisting of a sequence
    of stages. For example, you could have a text preprocessing pipeline for the "Complete
    Submission Text File" of a `10-K` filing on the Edgar website. Such a pipeline
    would take the lines from the document as input at one end and produce a list
    of words as output, after passing through a series of Transformers (that apply
    regexes and other filters to the data in a particular sequence). Several examples
    of data and ML pipelines are presented in this chapter, as well as in [Chapter
    9](part0166.html#4U9TC0-e9cbc07f866e437b8aa14e841622275c), *Developing Applications
    with Spark SQL*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pipeline stages**: Each pipeline stage comprises of a Transformer or an Estimator
    that is executed in a specified sequence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transformer**: This is an algorithm that transforms an input DataFrame into
    another DataFrame with one or more features added to it. There are several Transformers
    such as RegexTokenizer, Binarizer, OneHotEncoder, various indexers (for example,
    `StringIndexer` and `VectorIndexer`) and others available as a part of the library.
    You can also define your own custom Transformers like we do in [Chapter 9](part0166.html#4U9TC0-e9cbc07f866e437b8aa14e841622275c),
    *Developing Applications with Spark SQL.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Estimator**: This is a machine learning algorithm that learns from the input
    data provided. The input to an estimator is a DataFrame and the output is a Transformer.
    There are several Estimators available in the MLlib libraries such as `LogisticRegression`,
    `RandomForest`, and so on. The output Transformers from these Estimators are the
    corresponding models, such as the LogisticRegressionModel, RandomForestModel,
    and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the steps in a pipeline application development process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A machine learning pipeline application development process typically includes
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data ingestion**: The input data ingested by a typical machine learning pipeline
    comes from multiple data sources, often in several different formats (as described
    in [Chapter 2](part0035.html#11C3M0-e9cbc07f866e437b8aa14e841622275c), *Using
    Spark SQL for Processing Structured and SemiStructured Data*). These sources can
    include files, databases (RDBMSs, NoSQL, Graph, and so on), Web Services (for
    example, REST end-points), Kafka and Amazon Kinesis streams, and others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data cleansing and preprocessing**: Data cleansing is a critical step in
    the overall data analytics pipeline. This preprocessing step fixes data quality
    issues and makes it suitable for consumption by the machine learning models. For
    example, we may need to remove HTML tags and replace special characters (such
    as `&nbsp`; and others) from the source HTML documents. We might have to rename
    columns (or specify them) as per the required standardized formats for Spark MLlib
    pipelines. Most importantly, we will also need to combine various columns in the
    DataFrame into a single column containing the feature vector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature engineering**: In this step, we extract and generate specific features
    from the input data using various techniques. These are then combined into a feature
    vector and passed to the next step in the process. Typically, a `VectorAssembler`
    is used to create the feature vector from the specified DataFrame columns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model training**: Machine learning model training involves specifying an
    algorithm and some training data (which the model can learn from). Typically,
    we split our input Dataset into training and test Datasets, by randomly selecting
    a certain proportion of the input records for each of these Datasets. The model
    is trained by calling the `fit()` method on the training Dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model validation**: This step involves evaluating and tuning the ML model
    to assess how good the predictions are. In this step, the model is applied to
    the test Dataset, using the `transform()` method, and appropriate performance
    measures for the model are computed, for example, accuracy, error, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model selection**: In this step, we choose the parameters for the Transformers
    and Estimators that produce the best ML model. Typically, we create a parameter
    grid and execute a grid-search for the most suitable set of parameters for a given
    model using a process called cross-validation. The best model returned by the
    cross validator can be saved and later loaded into the production environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model Deployment**: Finally, we deploy the best model for production. For
    some models, it may be easier to convert the model parameters (such as coefficients,
    intercepts, or decision trees with branching logic) to other formats (such as
    JSON) for simpler and more efficient deployments in complex production environments.
    More details on such deployments can be got from [Chapter 12](part0216.html#6DVPG0-e9cbc07f866e437b8aa14e841622275c), *Spark
    SQL in Large-Scale Application Architectures*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The deployed models will need to be continuously maintained, upgraded, optimized,
    and so on, in the production environment.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing feature engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feature engineering is the process of using domain knowledge of the data to
    create features that are key to applying machine learning algorithms. Any attribute
    can be a feature, and choosing a good set of features that helps solve the problem
    and produce acceptable results is key to the whole process. This step is often
    the most challenging aspect of machine learning applications. Both the quality
    and quantity/number of features greatly influences the overall quality of the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Better features also means more flexibility because they can result in good
    results even when less than optimal models are used. Most ML models can pick up
    on the structure and patterns in the underlying data, reasonably well. The flexibility
    of good features allows us to use less complex models that are faster and easier
    to understand and maintain. Better features also typically result in simpler models.
    Such features make it easier to select the right models and the most optimized
    parameters.
  prefs: []
  type: TYPE_NORMAL
- en: For a good blog on feature engineering, refer: Discover Feature Engineering,
    How to Engineer Features and How to Get Good at It, Jason Brownlee, at: [https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/](https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/)
  prefs: []
  type: TYPE_NORMAL
- en: Producing a feature vector for every piece of information in a real-world Dataset
    is impractical from a processing and computing costs perspective. Typically, feature
    transformations, such as indexing and binning, are used to reduce the dimensionality
    of predictor variables. Additionally, irrelevant and low-frequency values are
    generally removed from the model, and continuous variables are grouped into a
    reasonable number of bins. Some of the original features may be highly correlated,
    or redundant, and can, therefore, be dropped from further consideration as well.
    Additionally, multiple features can be combined to yield new features (thereby
    reducing the overall dimensionality as well). Depending on the model, we may also
    have to normalize the values of some variables to avoid skewed results from using
    absolute values. We apply transformations on the training Dataset to obtain a
    feature vector that will be fed into a machine learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, feature engineering is an iterative process comprising of multiple cycles
    of data selection and model evaluation. If the problem is well defined then the
    iterative process can be stopped at an appropriate point, and other configurations,
    or models, attempted.
  prefs: []
  type: TYPE_NORMAL
- en: Creating new features from raw data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Selecting features from raw data could lead to many different feature sets however
    we need to keep the ones that are most relevant to the problem to be solved.
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection can reveal the importance of various features, but those features
    have to be identified first. The number of features can be limited by our ability
    to collect the data but once collected it is entirely dependent on our selection
    process. Usually, they need to be created manually, and this requires time, patience,
    creativity, and familiarity with the raw input data.
  prefs: []
  type: TYPE_NORMAL
- en: Transformations of the raw input data depends on the nature of the data. For
    example, with textual data, it could mean generating document vectors while with
    image data, it could mean applying various filters to extract the contours from
    an image. Hence, the process is largely manual, slow, iterative, and requires
    lots of domain expertise.
  prefs: []
  type: TYPE_NORMAL
- en: Estimating the importance of a feature
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have to choose a subset from among hundreds and thousands of potential features
    to include in the modeling process. Making these choices requires deeper insights
    about the features that may have the greatest impact upon model performance. Typically,
    the features under consideration are scored and then ranked as per their scores.
    Generally, the features with the highest scores are selected for inclusion in
    the training Dataset, while others are ignored. Additionally, we may generate
    new features from the raw data features, as well. How do we know whether these
    generated features are helpful to the task at hand?
  prefs: []
  type: TYPE_NORMAL
- en: Different approaches can be used for estimating the importance of features.
    For example, we can group sets of related features and compare the performance
    of the model without those features to the complete model (with the dropped features
    included). We can also execute a k-fold cross-validation for both, the complete
    and dropped models, and compare them on various statistical measures. However,
    this approach can be too expensive to run continuously in production as it requires
    building every model k-times (for a k-fold cross-validation) for every feature
    group, which can be many (depending on the level of grouping). So, in practice,
    this exercise is performed periodically on a representative sample of models.
  prefs: []
  type: TYPE_NORMAL
- en: Other effective techniques for feature engineering include visualization and
    applying specific methods known to work well on certain types of data. Visualization
    can be a powerful tool to quickly analyze relationships between features and evaluate
    the impact of generated features. Using well-known approaches and methods in various
    domains can help accelerate the feature engineering process. For example, for
    textual data, using n-grams, TF-IDF, feature hashing, and others, are well known
    and widely applied feature engineering methods.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding dimensionality reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Primarily, dimensionality reduction deals with achieving a suitable reduction
    in the number of predictor variables in the model. It helps by selecting features
    that become part of the training Dataset after limiting the number of resulting
    columns in the feature matrix using various transformations. The attributes that
    are evaluated to be largely irrelevant to the problem need to be removed.
  prefs: []
  type: TYPE_NORMAL
- en: There will be some features that will be more important to the model's accuracy
    than others. There will also be some features that become redundant in the presence
    of other features.
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection addresses these challenges by selecting a subset of features
    that is most useful in solving the problem. Feature selection algorithms may compute
    correlation coefficients, covariances, and other statistics for choosing a good
    set of features. A feature is generally included, if it is highly correlated with
    the dependent variable (the thing being predicted). We can also use **Principal
    Component Analysis** (**PCA**) and unsupervised clustering methods for feature
    selection. More advanced methods may search through various feature sets creating
    and evaluating models automatically in order to derive the best predictive subgroup
    of features.
  prefs: []
  type: TYPE_NORMAL
- en: Deriving good features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will provide additional tips for deriving good features
    and measures for assessing them. These features can be handcrafted by a domain
    expert or automated using methods such as PCA or deep learning (see [Chapter 10](part0178.html#59O440-e9cbc07f866e437b8aa14e841622275c),
    *Using Spark SQL in Deep Learning Applications*)*.* Each of these approaches can
    be used independently, or jointly, to arrive at the best set of features.
  prefs: []
  type: TYPE_NORMAL
- en: When working on machine learning projects, tasks such as data preparation and
    cleansing are key, in addition to the actual learning models and algorithms used
    to solve the business problems. In the absence of data pre-processing steps in
    machine learning applications, the resulting patterns will not be accurate or
    useful and/or the prediction results will have a lower accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we present a few general tips to derive a good set of pre-processed features
    and cleansed data:'
  prefs: []
  type: TYPE_NORMAL
- en: Explore grouping for categorical values and/or limiting the number of categorical
    values that become predictor variables in the feature matrix to only the most
    common ones.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate and add new features by computing polynomial features from the provided
    features. However, be careful to avoid overfitting, which can occur when the model
    closely fits the data containing excessive number of features. This results in
    a model that memorizes the data, rather than learns from it, which in turn reduces
    its ability to predict new data accurately.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating each feature to test its correlation with the classes independently
    by using a ranking metric, such as Pearson's correlation. We can then select a
    subset of features, such as the top 10%, or the top 20%, of the ranked features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating how good each feature is using criteria, such as Gini index and entropy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the covariance between features; for example, if two features are
    changing the same way, it will probably not serve the overall purpose to select
    both of them as features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A model may also underfit the training Dataset, which will result in lower model
    accuracy. When your model is underfitting a Dataset, you should consider introducing
    new features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A date-time field contains a lot of information that can be difficult for a
    model to take advantage of in its original format. Decompose date-time fields
    into separate fields for month, day, year, and so on, to allow models to leverage
    these relationships.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply linear Transformers to numerical fields, such as weights and distances,
    for use in regression and other algorithms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explore storing a quantity measure (such weights or distances) as a rate or
    an aggregate quantity over a time interval to expose structures, such as seasonality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will present a detailed code example of a Spark ML pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a Spark ML classification model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step in implementing a machine learning model is to perform EDA on
    input data. This analysis would typically involve data visualization using tools
    such as Zeppelin, assessing feature types (numeric/categorical), computing basic
    statistics, computing covariances, and correlation coefficients, creating pivot
    tables, and so on (for more details on EDA, see [Chapter 3](part0045.html#1AT9A0-e9cbc07f866e437b8aa14e841622275c),
    *Using Spark SQL for Data Exploration*).
  prefs: []
  type: TYPE_NORMAL
- en: The next step involves executing data pre-processing and/or data munging operations.
    In almost all cases, the real-world input data will not be high quality data ready
    for use in a model.  There will be several transformations required to convert
    the features from the source format to final variables; for example, categorical
    features may need to be transformed to a binary variable for each categorical
    value using one-hot encoding technique (for more details on data munging, see
    [Chapter 4](part0057.html#1MBG20-e9cbc07f866e437b8aa14e841622275c), *Using Spark
    SQL for Data Munging*).
  prefs: []
  type: TYPE_NORMAL
- en: Next is the feature engineering step. In this step, we will derive new features
    to include, along with other existing features, in the training data. Use the
    tips provided earlier in this chapter to derive a good set of features to be ultimately used
    in training the model.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will train the model using the selected features and test it using
    the test Dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a good blog containing a detailed step-by-step example of a classification
    model applied to the Kaggle knowledge challenge--Titanic: Machine Learning from
    Disaster, refer: *Building Classification model using Apache Spark* by Vishnu
    Viswanath at: [http://vishnuviswanath.com/spark_lr.html](http://vishnuviswanath.com/spark_lr.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'These steps, along with the operations and input/output at each stage, are
    shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00182.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We will use the publicly available diabetes Dataset that consists of 101,766
    rows, representing ten years of clinical care records from 130 US hospitals and
    integrated delivery networks. It includes over 50 features (attributes) representing
    patient and hospital outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: The Dataset can be downloaded from the UCI website at [https://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008](https://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008).
  prefs: []
  type: TYPE_NORMAL
- en: The source ZIP file contains two CSV files. The first file, `diabetic_data.csv`,
    is the main input Dataset, and the second file, `IDs_mapping.csv`, is the master
    data for `admission_type_id`, `discharge_disposition_id`, and `admission_source_id`.
    The second file is small enough to manually split into three parts, one for each
    set of ID mappings.
  prefs: []
  type: TYPE_NORMAL
- en: 'The example in this section closely follows the approach and analysis presented
    in: Impact of HbA1c measurement on hospital readmission rates: analysis of 70,000
    clinical database patient records, by Beata Strack, Jonathan P. DeShazo, Chris
    Gennings,Juan L. Olmo,Sebastian Ventura,Krzysztof J. Ciosand John N. Clore, Biomed
    Res Int. 2014; 2014: 781670, available at: [http://europepmc.org/articles/PMC3996476](http://europepmc.org/articles/PMC3996476).'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will import all the packages required for this coding exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00183.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Exploring the diabetes Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Dataset contains attributes/features originally selected by clinical experts
    based on their potential connection to the diabetic condition or management.
  prefs: []
  type: TYPE_NORMAL
- en: A full list of the features and their description is available at [https://www.hindawi.com/journals/bmri/2014/781670/tab1/](https://www.hindawi.com/journals/bmri/2014/781670/tab1/).
  prefs: []
  type: TYPE_NORMAL
- en: 'We load the input data into a Spark DataFrame, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We can display the schema of the DataFrame created in the previous step to
    list the columns or fields in the DataFrame, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00184.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we print out a few sample records to get a high-level sense of the values
    contained in the fields of the Dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00185.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also compute the basic statistics for numerical columns using `dataFrame.describe("column")`.
    For example, we display the count, mean, standard deviation, and min and max values
    of a few numeric data columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The original input Dataset contains incomplete, redundant, and noisy information,
    as expected in any real-world Dataset. There are several fields that have a high
    percentage of missing values.
  prefs: []
  type: TYPE_NORMAL
- en: 'We compute the number of records that have specific fields missing, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'As computed precedingly, the features with many missing values are identified
    to be weight, payer code, and medical specialty. We drop the weight and payer
    code columns, however, the medical specialty attribute ( potentially, a very relevant
    feature) is retained:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The Dataset also contains records of multiple inpatient visits by some of the
    patients. Here, we extract a set of patients with multiple inpatient visits.
  prefs: []
  type: TYPE_NORMAL
- en: 'We observe that the overall number of such patients is significant:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'As mentioned in the reference study/paper, such observations cannot be considered
    as statistically independent, hence, we include only the first encounter for each
    patient. After these operations are completed, we verify that there are no patient
    records remaining in the DataFrame corresponding to multiple visit records:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'As in the reference study/paper, we also remove records of encounters that
    resulted in a patient''s death to avoid bias:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: After performing the mentioned operations, we were left with `69,934` encounters
    that constitute the final Dataset used for further analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we execute a set of `JOIN` operations to understand the data better in
    terms of top categories of `discharge disposition`, `admission types`, and `admission
    sources`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00186.gif)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00187.gif)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00188.gif)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will execute a series of data munging or data pre-processing
    steps to improve the overall data quality.
  prefs: []
  type: TYPE_NORMAL
- en: Pre-processing the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several data munging steps required to preprocess the data. We will
    start by addressing the missing field, values. We have several options when dealing
    with null or missing values. We can drop them using `df.na.drop()`, or fill them
    with default values using `df.na.fill()`. Such fields can be replaced with the
    most commonly occurring values for that column, and in the case of numeric fields
    they can also be replaced with average values. Additionally, you can also train
    a regression model on the column and use it to predict the field values for rows
    where values are missing.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we have the missing fields represented with a `?` value in the Dataset,
    so we use the `df.na.replace()` function to replace them with the "Missing" string.
  prefs: []
  type: TYPE_NORMAL
- en: 'This operation is illustrated for the  `medical_specialty` field, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The `medical_specialty` field can have values, such as cardiology, internal
    medicine, family/general practice, surgeon, or Missing. To a model, the Missing
    value appears like any other choice for `medical_specialty`. We could have created
    a new binary feature called `has_ medical_specialty` and assigned it a value of
    `1` when a row contained the value and `0` when it was unknown or missing. Alternatively,
    we could also have created a binary feature for each value of `medical_specialty`,
    such as `Is_Cardiology`, `Is_Surgeon`, and `Is_Missing`. These additional features
    can then be used instead of, or in addition to, the `medical_specialty` feature
    in different models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, guided by the analysis contained in the original paper, we drop a set
    of columns from further analysis in this chapter, to keep the size of the problem
    reasonable, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00189.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'As in the referenced study/paper, we also consider four groups of encounters:
    no HbA1c test performed, HbA1c performed and in normal range, HbA1c performed
    and the result is greater than 8 percent, and HbA1c performed and the result is
    greater than 8 percent.'
  prefs: []
  type: TYPE_NORMAL
- en: 'These steps to accomplish this grouping are executed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Since our primary objective focuses on factors that lead to early readmission,
    the readmission attribute (or the outcome) has two values: `Readmitted`, if the
    patient was readmitted within 30 days of discharge or `Not Readmitted`, which
    covers both readmission after 30 days and no readmission at all.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We create a new ordinal feature, called `Readmitted`, with two values: `Readmitted`
    and `Not Readmitted`. You can use similar approaches for age categories as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We display the numbers of several features versus the values of the target
    variable, as follows. This will help identify skews in the number of records based
    on various attributes in the input Dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we group the various age ranges into various categories and adds it as
    a column to obtain our final version of the Dataset, as illustrated. Additionally,
    we remove the three rows where the `gender` is `Unknown/Invalid`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00190.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'The schema for the final DataFrame after the pre-processing steps is, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00191.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'We display a few sample records from the final DataFrame, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00192.gif)'
  prefs: []
  type: TYPE_IMG
- en: After completing the data preprocessing phase, we now shift our focus to building
    the machine learning pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Building the Spark ML pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our example ML pipeline, we will have a sequence of pipeline components,
    which are detailed in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Using StringIndexer for indexing categorical features and labels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this exercise, we will be training a random forest classifier. First, we
    will index the categorical features and labels as required by `spark.ml`. Next,
    we will assemble the feature columns into a vector column because every `spark.ml`
    machine learning algorithm expects it. Finally, we can train our random forest
    on a training Dataset. Optionally, we can also unindex the labels to make them
    more readable.
  prefs: []
  type: TYPE_NORMAL
- en: There are several ready-to-use Transformers available to index categorical features.
    We can assemble all the features into one vector (using `VectorAssembler`) and
    then use a `VectorIndexer` to index it. The drawback of `VectorIndexer` is that
    it will index every feature that has less than `maxCategories` number of different
    values. It does not differentiate whether a given feature is categorical or not.
    Alternatively, we can index every categorical feature one by one using a `StringIndexer`,
    as follows.
  prefs: []
  type: TYPE_NORMAL
- en: We use a `StringIndexer` to transform String features to `Double` values. For
    example, the `raceIndexer` is an estimator that transforms the race column, that
    is, it generates indices for the different races in the input column, and creates
    a new output column called `raceCat`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `fit()` method then converts the column into a `StringType` and counts
    the numbers of each race. These steps are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00193.gif)'
  prefs: []
  type: TYPE_IMG
- en: '`raceIndexer.transform()` assigns the generated index to each value of the
    race in the column. For example, `AfricanAmerican` is assigned `1.0`, `Caucasian`
    is assigned `0.0`, and so on, as shown.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, we create indexers for the gender, age groups, HbA1c test results,
    change of medications, and diabetes medications prescribed, and fit them to the
    resulting DataFrames at each step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We print the schema of the resulting DataFrame containing the columns for various
    indexers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00194.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also index the labels using `StringIndexer`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, we can also define our feature indexers, succinctly, as illustrated.
    The sequence of `StringIndexers` can then be concatenated with the numeric features
    to derive the features vector using a `VectorAssembler`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: We need not have explicitly called the `fit()` and `transform()` methods for
    each indexer, that can be handled by the pipeline, automatically.
  prefs: []
  type: TYPE_NORMAL
- en: 'The behavior of the pipeline can be summarized are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It will execute each stage and pass the result of the current stage to the next
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the stage is a Transformer, then the pipeline calls `transform()` on it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the stage is an Estimator, then the pipeline first calls `fit()` followed
    by `transform()`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If it is the last stage in the pipeline, then the Estimator will not call `transform()` (after `fit()` )
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using VectorAssembler for assembling features into one column
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that our indexing is done, we need to assemble all our feature columns
    into a single column containing a vector that groups all our features. To do that,
    we''ll use the `VectorAssembler` Transformer, as in the following steps. However,
    first, due to the significant skew in the number of records for each label, we
    sample the records in appropriate proportions to have nearly equal numbers of
    records for each label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, we can also achieve the same by following the steps listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We apply the `transform()` operation and print a few sample records of the
    resulting DataFrame, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00195.gif)'
  prefs: []
  type: TYPE_IMG
- en: '`VectorIndexer` is used for indexing the features. We will pass all the feature
    columns that are used for the prediction to create a new vector column called
    `indexedFeatures`, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will train a random forest classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Using a Spark ML classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that that data is in the proper format expected by `spark.ml` machine learning
    algorithms, we will create a `RandomForestClassifier` component, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The standardized DataFrame format, allows easy replacement of the `RandomForestClassifier`
    with other `spark.ml` classifiers, such as `DecisionTreeClassifier` and `GBTClassifier`,
    as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: In the following section, we will create our pipeline by assembling the label
    and feature indexers, and the random forest classifier as stages in the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Spark ML pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, we will create a pipeline object using all the components we have defined
    till now. Since all the different steps have been implemented, we can assemble
    our pipeline, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will create training and test Datasets from the input
    Dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the training and test Datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To train and evaluate the model, we split the input data, randomly, between
    two DataFrames: a training set (containing 80 percent of the records) and a test
    set(containing 20 percent of the records). We will train the model using the training
    set and then evaluate it using the test set. The following can be used to split
    input data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now use the pipeline to fit the training data. A `PipelineModel` object
    is returned as a result of fitting the pipeline to the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will make predictions on our test Dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Making predictions using the PipelineModel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `PipelineModel` object from the previous step is used for making predictions
    on the test Dataset, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00196.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we will evaluate our model by measuring the accuracy of the predictions,
    as shown in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can also print our random forest model to understand the logic
    being used in the ten trees created in our model, as illustrated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00197.gif)'
  prefs: []
  type: TYPE_IMG
- en: In the next section, we will show the process of cross-validation to pick the
    best predictive model from a set of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the best model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to select the best model, we will perform a grid search over a set
    of parameters. For each combination of parameters, we will perform cross-validation
    and retain the best model according to some performance indicator. This process
    can be tedious, but `spark.ml` simplifies it with an easy-to-use API.
  prefs: []
  type: TYPE_NORMAL
- en: For cross-validation, we choose a value `k` for the number of folds, for example,
    a value of `3` will split the Dataset into three parts. From those three parts,
    three different training and test data pairs will be generated (two-thirds of
    the data for training and one-third for test). The model is evaluated on the average
    of the chosen performance indicator over the three pairs.
  prefs: []
  type: TYPE_NORMAL
- en: There is a set of values assigned to each of the parameters. The parameters
    used in our example are `maxBins` (the maximum number of bins used for discretizing
    continuous features and for splitting features at each node), `maxDepth` (the
    maximum depth of a tree), and `impurity` (the criterion used for information gain
    calculations).
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create a grid of parameters, as illustrated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00198.gif)'
  prefs: []
  type: TYPE_IMG
- en: Next, we will define an Evaluator which, as its name implies, will evaluate
    our model according to some metric. There are built-in Evaluators available for
    regression, and binary and multi-class classification models.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, after choosing `k=2` (set to a higher number for real-world models),
    the number of folds the data will be split into during cross-validation, we can
    create a `CrossValidator` object, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: You have to be careful when running cross-validation, especially on bigger Datasets,
    as it will train **k x p models**, where *p* is the product of the number of values
    for each param in your grid. So, for a *p* of `18`, the cross-validation will
    train `36` different models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since our `CrossValidator` is an Estimator, we can obtain the best model for
    our data by calling the `fit()` method on it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now make predictions on `testData`, as illustrated here. Note a slight
    improvement in the accuracy value as compared to the values before cross-validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00199.gif)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will show you the power of common interfaces that Spark
    ML exposes to ease the development and testing of ML pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Changing the ML algorithm in the pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In an earlier section, we showed how easy it is replace the RandomForestClassifier
    with other classifiers, such as, the DecisionTreeClassifier or the GBTClassifer.  In
    this section, we will replace the random forest classifier with a logistic regression
    model. Logistic regression explains the relationship between a binary-valued dependent
    variable based on other variables called independent variables. The binary values,
    `0` or `1`, can represent prediction values such as pass/fail, yes/no, dead/alive,
    and so on. Based on the values of the independent variables, it predicts the probability
    that the dependent variable takes one of the categorical values, such as a `0`
    or a `1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will create a `LogtisticRegression` component, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use the label and feature indexers from before and combine them with
    the logistic regression component to create a new pipeline, as follows. Furthermore,
    we use the `fit()` and `transform()` methods to train and then make predictions
    on the test Dataset. Note that the code looks very similar to the approach used
    earlier for the random forest pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Over the next several sections, we will introduce a series of tools and utilities
    available in Spark, that can be used to achieve better ML models.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Spark ML tools and utilities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the following sections, we will explore various tools and utilities that
    Spark ML offers to select features and create superior ML models easily and efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Using Principal Component Analysis to select features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned earlier, we can derive new features using **Principal Component
    Analysis** (**PCA**) on the data. This approach depends on the problem, so it
    is imperative to have a good understanding about the domain.
  prefs: []
  type: TYPE_NORMAL
- en: This exercise typically requires creativity and common sense to choose a set
    of features that may be relevant to the problem. A more extensive exploratory
    data analysis is typically required to help understand the data better and/or
    to identify patterns that lead to a good set of features.
  prefs: []
  type: TYPE_NORMAL
- en: PCA is a statistical procedure that converts a set of potentially correlated
    variables into a, typically, reduced set of linearly uncorrelated variables. The
    resulting set of uncorrelated variables are called principal components. A `PCA`
    class trains a model to project vectors to a lower dimensional space. The following
    example shows how to project our multi-dimensional feature vector into three-dimensional
    principal components.
  prefs: []
  type: TYPE_NORMAL
- en: According to Wikipedia, [https://en.wikipedia.org/wiki/Principal_component_analysis](https://en.wikipedia.org/wiki/Principal_component_analysis),
    "This transformation is defined in such a way that the first principal component
    has the largest possible variance (that is, it accounts for as much of the variability
    in the data as possible), and each succeeding component, in turn, has the highest
    variance possible under the constraint that it is orthogonal to the preceding
    components."
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be building our model using the Dataset used to fit in the random forest
    algorithm used for classification earlier in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Using encoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we use one-hot encoding to map a column of label indices to
    a column of binary vectors with, at most, a single one-value. This encoding allows
    algorithms that expect continuous features, such as `LogisticRegression`, to use
    categorical features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00200.gif)'
  prefs: []
  type: TYPE_IMG
- en: Using Bucketizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bucketizer is used to transform a column of continuous features to a column
    of feature buckets. We specify the `n+1` splits parameter for mapping continuous
    features into n buckets. The splits should be in a strictly increasing order.
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, we add `Double.NegativeInfinity` and `Double.PositiveInfinity` as
    the outer bounds of the splits to prevent potential out of Bucketizer bounds exceptions.
    In the following example, we specify six splits, and then define a `bucketizer`
    for the `num_lab_procedures` feature (with values varying from `1` to `126` in
    our Dataset), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00201.gif)'
  prefs: []
  type: TYPE_IMG
- en: Using VectorSlicer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A `VectorSlicer` is a Transformer that takes a feature vector and returns a
    new feature vector that is a subset of the original features. It is useful for
    extracting features from a vector column. We can use a `VectorSlicer` to test
    our model with different numbers and combinations of features.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we use four features initially, and then drop one
    of them. These slices of features can be used to test the importance of including/excluding
    features from the set of features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Using Chi-squared selector
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`ChiSqSelector` enables chi-squared feature selection. It operates on labeled
    data with categorical features. `ChiSqSelector` uses the chi-squared test of independence
    to choose the features. In our example, we use the `numTopFeatures` to choose
    a fixed number of top features that yield features with the most predictive power:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00202.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Using a Normalizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can normalize the data using a Normalizer object (a Transformer). The input
    to a Normalizer is a column created by the `VectorAssembler.` It normalizes the
    values in the column to produce a new column containing the normalized values.a
    new feature vector with a subarray of the original features. It is useful for
    extracting features from a vector column This normalization can help standardize
    input data and improve the behavior of learning algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Retrieving our original labels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`IndexToString` is the reverse operation of `StringIndexer` that converts the
    indices back to their original labels. The random forests transform method of
    the model produced by the `RandomForestClassifier` produces a prediction column
    that contains indexed labels that we need unindexed to retrieve the original label
    values, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will switch focus to presenting an example of Spark
    ML clustering using the k-means algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a Spark ML clustering model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will explain clustering with Spark ML. We will use a publicly
    available Dataset about the student's knowledge status about a subject.
  prefs: []
  type: TYPE_NORMAL
- en: The Dataset is available for download from the UCI website at [https://archive.ics.uci.edu/ml/datasets/User+Knowledge+Modeling](https://archive.ics.uci.edu/ml/datasets/User+Knowledge+Modeling).
  prefs: []
  type: TYPE_NORMAL
- en: 'The attributes of the records contained in the Dataset have reproduced here
    from the UCI website mentioned previously for reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '**STG**: The degree of study time for goal object materials (input value)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SCG**: The degree of repetition number of users for goal object materials
    (input value)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**STR**: The degree of study time of users for related objects with the goal
    object (input value)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LPR**: The exam performance of a user for related objects with the goal object
    (input value)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PEG**: The exam performance of a user for goal objects (input value)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**UNS**: The knowledge level of the user (target value)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'First, we will write a UDF to create two levels representing the two categories
    of the students--beneath average and beyond average from the five contained in
    the original Dataset. We combine the training and test CSV files to obtain a sufficient
    number of input records:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'We will read the input Dataset and create a column called `label` that is populated
    by the UDF. This allows us to have nearly equal numbers of records for each category:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we cast the numeric fields to `Double` values, verify the number of records
    in the DataFrame, display a few sample records, and print the schema, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will use `VectorAssembler` to create the features column, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'You can use `explainParams` to list the details of the k-means model, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Evaluate the quality of clustering by computing the **Within Set Sum of Squared
    Errors** (**WSSSE**). The standard k-means algorithm aims at minimizing the sum
    of squares of the distance between the centroid to the points in each cluster.
    Increasing the value of `k` can reduce this error. The optimal `k` is usually
    one where there is an "elbow" in the WSSSE graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we will compute the number of differences between the labels and the
    predicted values in our Dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will separate the DataFrames containing the prediction values of `0`
    and `1` to display sample records:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00203.gif)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00204.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also use `describe` to see the summary statistics for each of the predicted
    labels, as illustrated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00205.gif)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00206.gif)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will feed in a couple of test input records, and the model will predict
    the cluster for them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Topics, such as streaming ML applications and architectures for large-scale
    processing, will be covered in detail in [Chapter 12](part0216.html#6DVPG0-e9cbc07f866e437b8aa14e841622275c),
    *Spark SQL in Large-Scale Application Architectures*.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced machine learning applications. We covered one
    of the most important topics in machine learning, called feature engineering.
    Additionally, we provided code examples using Spark ML APIs to build a classification
    pipeline and a clustering application. Additionally, we also introduced a few
    tools and utilities that can help select features and build models more easily
    and efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will introduce GraphFrame applications and provide examples
    of using Spark SQL DataFrame/Dataset APIs to build graph applications. We will
    also apply various graph algorithms to graph applications.
  prefs: []
  type: TYPE_NORMAL
