- en: Chapter 11. Optimization – General Principles and Profiling Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '|   | *"We should forget about small efficiencies, say about 97% of the time:
    premature optimization is the root of all evil."* |   |'
  prefs: []
  type: TYPE_TB
- en: '|   | --*Donald Knuth* |'
  prefs: []
  type: TYPE_TB
- en: This chapter is about optimization and provides a set of general principles
    and profiling techniques. It gives the three rules of optimization every developer
    should be aware of and provides guidelines on optimization. Last, it focuses on
    how to find bottlenecks.
  prefs: []
  type: TYPE_NORMAL
- en: The three rules of optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Optimization has a price, no matter what the results are. When a piece of code
    works, it might be better (sometimes) to leave it alone than to try making it
    faster at all costs. There are a few rules to keep in mind when doing any kind
    of optimization:'
  prefs: []
  type: TYPE_NORMAL
- en: Make it work first
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Work from the user's point of view
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep the code readable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make it work first
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A very common mistake is to try to optimize the code while you are writing it.
    This is mostly pointless because the real bottlenecks are often located where
    you would have never thought they would be.
  prefs: []
  type: TYPE_NORMAL
- en: An application is usually composed of very complex interactions, and it is impossible
    to get a full picture of what is going on before it is really used.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, this is not a reason to write a function or a method without trying
    to make it as fast as possible. You should be careful to lower its complexity
    as much as possible and avoid useless repetition. But the first goal is to make
    it work. This goal should not be hindered by optimization efforts.
  prefs: []
  type: TYPE_NORMAL
- en: For line-level code, the Python philosophy is that there's one, and preferably
    only one, way to do it. So, as long as you stick with a Pythonic syntax, described
    in [Chapter 2](ch02.html "Chapter 2. Syntax Best Practices – below the Class Level"),
    *Syntax Best Practices – below the Class Level*, and [Chapter 3](ch03.html "Chapter 3. Syntax
    Best Practices – above the Class Level"), *Syntax Best Practices – above the Class
    Level*, your code should be fine. Often, writing less code is better and faster
    than writing more code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Don''t do any of these things until you have gotten your code working and you
    are ready to profile:'
  prefs: []
  type: TYPE_NORMAL
- en: Start to write a global dictionary to cache data for a function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Think about externalizing a part of the code in C or hybrid languages such as
    Cython
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Look for external libraries to do some basic calculation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For very specialized areas, such as scientific calculation or games, the usage
    of specialized libraries and externalization might be unavoidable from the beginning.
    On the other hand, using libraries like NumPy might ease the development of specific
    features and produce simpler and faster code at the end. Furthermore, you should
    not rewrite a function if there is a good library that does it for you.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, Soya 3D, which is a game engine on top of OpenGL (see [http://home.gna.org/oomadness/en/soya3d/index.html](http://home.gna.org/oomadness/en/soya3d/index.html)),
    uses C and Pyrex for fast matrix operations when rendering real-time 3D.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Optimization is carried out on programs that already work.
  prefs: []
  type: TYPE_NORMAL
- en: As Kent Beck says, "Make it work, then make it right, then make it fast."
  prefs: []
  type: TYPE_NORMAL
- en: Work from the user's point of view
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I have seen teams working on optimizing the startup time of an application server
    that worked really fine when it was already up and running. Once they finished
    speeding it, they promoted that work to their customers. They were a bit frustrated
    to notice that the customers didn't really care about it. This was because the
    speed-up work was not motivated by the user feedback but by the developer's point
    of view. The people who built the system were launching the server multiple times
    every day. So the startup time meant a lot to them but not to their customers.
  prefs: []
  type: TYPE_NORMAL
- en: 'While making a program start faster is a good thing from an absolute point
    of view, teams should be careful to prioritize the optimization work and ask themselves
    the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Have I been asked to make it faster?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Who finds the program slow?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is it really slow, or acceptable?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How much will it cost to make it go faster and is it worth it?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What parts need to be fast?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remember that optimization has a cost and that the developer's point of view
    is meaningless to customers, unless you are writing a framework or a library and
    the customer is a developer too.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Optimization is not a game. It should be done only when necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Keep the code readable and maintainable
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even if Python tries to make the common code patterns the fastest, optimization
    work might obfuscate your code and make it really hard to read. There's a balance
    to keep between producing readable, and therefore maintainable, code and defacing
    it in order to make it faster.
  prefs: []
  type: TYPE_NORMAL
- en: When you have reached 90% of your optimization objectives and the 10% left to
    be done makes your code completely unreadable, it might be a good idea to stop
    the work there or to look for other solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Optimization should not make your code unreadable. If it happens, you should
    look for alternative solutions such as externalization or redesign. Look for a
    good compromise between readability and speed.
  prefs: []
  type: TYPE_NORMAL
- en: Optimization strategy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's say your program has a real speed problem you need to resolve. Do not
    try to guess how to make it faster. Bottlenecks are often hard to find by looking
    at the code, and a set of tools is needed to find the real problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'A good optimization strategy can start with three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Find another culprit**: Make sure a third-party server or resource is not
    faulty'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scale the hardware**: Make sure the resources are sufficient'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Write a speed test**: Create a scenario with speed objectives'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find another culprit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Often, a performance problem occurs at production level and the customer alerts
    you that it is not working as it used to when the software was being tested. Performance
    problems might occur because the application was not planned to work in the real
    world with a high number of users and an increase of data size.
  prefs: []
  type: TYPE_NORMAL
- en: But if the application interacts with other applications, the first thing to
    do is to check if the bottlenecks are located on those interactions. For instance,
    a database server or an LDAP server might be responsible for extra overhead and
    might make everything slower.
  prefs: []
  type: TYPE_NORMAL
- en: The physical links between applications should also be considered. Maybe the
    network link between your application server and another server in the intranet
    is really slow due to a misconfiguration or congestion.
  prefs: []
  type: TYPE_NORMAL
- en: The design documentation should provide a diagram of all interactions and the
    nature of each link to get an overall picture of the system and offer help when
    trying to resolve a speed problem.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If your application uses third-party servers of resources, every interaction
    should be audited to make sure the bottleneck is not located there.
  prefs: []
  type: TYPE_NORMAL
- en: Scale the hardware
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When there is no more volatile memory available, the system starts to use the
    hard disk to store data. This is swapping.
  prefs: []
  type: TYPE_NORMAL
- en: This involves a lot of overhead and the performances drop drastically. From
    a user's point of view, the system is considered dead at this stage. So, it is
    important to scale the hardware to prevent this.
  prefs: []
  type: TYPE_NORMAL
- en: While having enough memory on a system is important, it is also important to
    make sure that the applications are not acting crazy and eating too much memory.
    For instance, if a program works on big video files that can weigh in at several
    hundreds of megabytes, it should not load them entirely in memory but rather work
    on chunks or use disk streams.
  prefs: []
  type: TYPE_NORMAL
- en: Disk usage is also important. A full partition might really slow down your application
    if the I/O errors are hidden in the code that tries to write repeatedly on the
    disk. Furthermore, even if the code only tries to write once, the hardware and
    OS might try to write multiple times.
  prefs: []
  type: TYPE_NORMAL
- en: Note that scaling up the hardware (vertical scaling) has some obvious limitations.
    You cannot fit an infinite amount of hardware to a single rack. Also, highly efficient
    hardware is extremely expensive (law of diminishing returns), so there is also
    an economical bound to this approach. From this point of view, it is always better
    to have the system that can be scaled by adding new computation nodes or workers
    (horizontal scaling). This allows you to scale out your service with commodity
    software that has the best performance/price ratio.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, designing and maintaining highly scalable distributed systems
    is both hard and expensive. If your system cannot be easily scaled horizontally
    or it is faster and cheaper to scale it vertically, it may be better to do so
    instead of wasting time and resources on a total redesign of your system architecture.
    Remember that hardware invariably tends to be faster and cheaper with time. Many
    products stay in this sweet spot where their scaling needs align with the trend
    of raising hardware performance.
  prefs: []
  type: TYPE_NORMAL
- en: Writing a speed test
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When starting with optimization work, it is important to work using a workflow
    similar to test-driven development rather than running some manual tests continuously.
    A good practice is to dedicate a test module in the application where the sequence
    of calls that are to be optimized is written. Having this scenario will help you
    to track your progress while you are optimizing the application.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can even write a few assertions where you set some speed objectives. To
    prevent speed regression, these tests can be left after the code has been optimized:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Measuring the execution speed depends on the power of the CPU used. But we will
    see in the next section how to write universal duration measures.
  prefs: []
  type: TYPE_NORMAL
- en: Finding bottlenecks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finding bottlenecks is done by:'
  prefs: []
  type: TYPE_NORMAL
- en: Profiling CPU usage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Profiling memory usage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Profiling network usage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Profiling CPU usage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first source of bottlenecks is your code. The standard library provides
    all the tools needed to perform code profiling. They are based on a deterministic
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: A **deterministic profiler** measures the time spent in each function by adding
    a timer at the lowest level. This introduces a bit of overhead but provides a
    good idea on where the time is consumed. A **statistical profiler**, on the other
    hand, samples the instruction pointer usage and does not instrument the code.
    The latter is less accurate but allows running the target program at full speed.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two ways to profile the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Macro-profiling**: This profiles the whole program while it is being used
    and generates statistics'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Micro-profiling**: This measures a precise part of the program by instrumenting
    it manually'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Macro-profiling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Macro-profiling is done by running the application in a special mode where
    the interpreter is instrumented to collect statistics on the code usage. Python
    provides several tools for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '`profile`: This is a pure Python implementation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cProfile`: This is a C implementation that provides the same interface as
    that of the `profile` tool but has less overhead'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The recommended choice for most Python programmers is `cProfile` due to its
    reduced overhead. Anyway, if you need to extend the profiler in some way, then
    `profile` will probably be a better choice because it does not use C extensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Both tools have the same interface and usage, so we will use only one of them
    to show how they work. The following is a `myapp.py` module with a main function
    that we are going to test with `cProfile`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The module can be called directly from the prompt and the results are summarized
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The statistics provided are a print view of a statistic object filled by the
    profiler. A manual invocation of the tool can be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The statistics can also be saved in a file and then read by the `pstats` module.
    This module provides a class that knows how to handle profile files and gives
    a few helpers to play with them invocation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'From there, you can browse the code by printing out the callers and callees
    for each function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Being able to sort the output allows working on different views to find the
    bottlenecks. For instance, consider the following scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: When the number of calls is really high and takes up most of the global time,
    the function or method is probably in a loop. Possible optimization may be done
    by moving this call to different scope in order to reduce number of operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When one function is taking very long time, a cache might be a good option,
    if possible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Another great way to visualize bottlenecks from profiling data is to transform
    them into diagrams (see *Figure 1*). **Gprof2Dot** ([https://github.com/jrfonseca/gprof2dot](https://github.com/jrfonseca/gprof2dot))
    can be used to turn profiler data into a dot graph. You can download this simple
    script PyPI using `pip` and use it on the stats as long as Graphviz (see [http://www.graphviz.org/](http://www.graphviz.org/))
    is installed in your environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The advantage of `gprof2dot` is that it tries to be language agnostic. It is
    not limited to Python `profile` or `cProfile` output and can read from multiple
    other profiles such as Linux perf, xperf, gprof, Java HPROF, and many others.
  prefs: []
  type: TYPE_NORMAL
- en: '![Macro-profiling](graphics/B05295_11_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1 An example of profiling overview diagram generated with gprof2dot
  prefs: []
  type: TYPE_NORMAL
- en: Macro-profiling is a good way to detect the function that has a problem, or
    at least its neighborhood. When you have found it, you can jump to micro-profiling.
  prefs: []
  type: TYPE_NORMAL
- en: Micro-profiling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When the slow function is found, it is sometimes necessary to do more profiling
    work that tests just a part of the program. This is done by manually instrumenting
    a part of the code in a speed test.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, the `cProfile` module can be used from a decorator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This approach allows testing parts of the application and sharpens the statistics
    output. But at this stage, having a list of callees is probably not interesting,
    as the function has already been pointed out as the one to optimize. The only
    interesting information is to know how fast it is, and then enhance it.
  prefs: []
  type: TYPE_NORMAL
- en: '`timeit` fits this need better by providing a simple way to measure the execution
    time of a small code snippet with the best underlying timer the host system provides
    (`time.time` or `time.clock`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The module allows you to repeat the call and is oriented to try out isolated
    code snippets. This is very useful outside the application context, in a prompt,
    for instance, but is not really handy to use within an existing application.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A deterministic profiler will provide results depending on what the computer
    is doing, and so results may vary each time. Repeating the same test multiple
    times and making averages provides more accurate results. Furthermore, some computers
    have special CPU features, such as **SpeedStep**, that might change the results
    if the computer is idling when the test is launched (see [http://en.wikipedia.org/wiki/SpeedStep](http://en.wikipedia.org/wiki/SpeedStep)).
    So, continually repeating the test is a good practice for small code snippets.
    There are also various caches to keep in mind such as DNS caches or CPU caches.
  prefs: []
  type: TYPE_NORMAL
- en: 'But the results of `timeit` should be used with caution. It is a very good
    tool to objectively compare two short snippets of code but it also allows you
    to easily make dangerous mistakes that will lead you to confusing conclusions.
    Here, for example, is the comparison of two innocent snippets of code with the
    `timeit` module that could make you think that string concatenation by addition
    is faster than the `str.join()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'From [Chapter 2](ch02.html "Chapter 2. Syntax Best Practices – below the Class
    Level"), *Syntax Best Practices – below the Class Level*, we know that string
    concatenation by addition in not a good pattern. Despite there are some minor
    CPython micro-optimizations designed exactly for such use case, it will eventually
    lead to quadratic run time. The problem lies in nuances about the `setup` argument
    of `timeit` (`-s` parameter in the command line) and how the range in Python 3
    works. I won''t discuss the details of the problem but will leave it to you as
    an exercise. Anyway, here is the correct way to compare string concatenation in
    addition with the `str.join()` idiom under Python 3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Measuring Pystones
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When measuring execution time, the result depends on the computer hardware.
    To be able to produce a universal measure, the simplest way is to benchmark the
    speed of a fixed sequence of code and calculate a ratio out of it. From there,
    the time taken by a function can be translated to a universal value that can be
    compared on any computer.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A lot of generic benchmarking tools for the measurement of computer performance
    are available. Surprisingly, some of them that were created many years ago are
    still used today. For instance, Whetstone was created in 1972, and back then it
    provided a computer performance analyzer in Algol 60 (see [http://en.wikipedia.org/wiki/Whetstone_%28benchmark%29](http://en.wikipedia.org/wiki/Whetstone_%28benchmark%29)).
    It is used to measure the **Millions Of Whetstone Instructions Per Second** (**MWIPS**).
    A table of results for old and modern CPUs is maintained at [http://freespace.virgin.net/roy.longbottom/whetstone%20results.htm](http://freespace.virgin.net/roy.longbottom/whetstone%20results.htm).
  prefs: []
  type: TYPE_NORMAL
- en: 'Python provides a benchmark utility in its `test` package that measures the
    duration of a sequence of well-chosen operations. The result is a number of **pystones**
    per second the computer is able to perform and the time used to perform the benchmark,
    which is generally around one second on modern hardware:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The rate can be used to translate a profile duration into a number of pystones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The `seconds_to_kpystones` returns the number of **kilo pystones**. This conversion
    can be included in your test if you want to code some speed assertions.
  prefs: []
  type: TYPE_NORMAL
- en: Having pystones will allow you to use this decorator in tests so that you can
    set assertions on execution times. These tests will be runnable on any computer
    and will allow developers to prevent speed regressions. When a part of the application
    has been optimized, they will be able to set its maximum execution time in tests
    and make sure it won't be breached by further changes. This approach is, of course,
    not ideal and 100% accurate, but it is at least better than hardcoding execution
    time assertions in raw values expressed as seconds.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling memory usage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another problem you may encounter when optimizing an application is memory consumption.
    If a program starts to eat so much memory that the system begins to swap, there
    is probably a place in your application where too many objects are created or
    objects that you don't intend to keep are still kept alive by some unintended
    reference. This is often easy to detect through classical profiling because consuming
    enough memory to make a system swap involves a lot of CPU work that can be detected.
    But sometimes it is not obvious and the memory usage has to be profiled.
  prefs: []
  type: TYPE_NORMAL
- en: How Python deals with memory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Memory usage is probably the hardest thing to profile in Python when you use
    the CPython implementation. While languages such as C allow you to get the memory
    size of any element, Python will never let you know how much a given object consumes.
    This is due to the dynamic nature of the language, and the fact that memory management
    is not directly accessible to the language user.
  prefs: []
  type: TYPE_NORMAL
- en: Some raw details of memory management were already explained in [Chapter 7](ch07.html
    "Chapter 7. Python Extensions in Other Languages"), *Python Extensions in Other
    Languages*. We already know that CPython uses reference counting to manage object
    allocation. This is the deterministic algorithm which ensures that object deallocation
    will be triggered when the reference count of the object goes to zero. Despite
    being deterministic, this process is not easy to track manually and to reason
    about in complex codebases. Also, the deallocation of objects on a reference count
    level does not necessarily mean that the actual process heap memory is freed by
    the interpreter. Depending on CPython interpreter compilation flags, system environment,
    or runtime context, the internal memory manager layer might decide to leave some
    blocks of free memory for future reallocation instead of releasing it completely.
  prefs: []
  type: TYPE_NORMAL
- en: Additional micro-optimizations in CPython implementation also make it even harder
    to predict actual memory usage. For instance, two variables that point to the
    same short string or small integer value might or might not point to the same
    object instance in memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite being quite scary and seemingly complex, memory management in Python
    is very well documented (refer to [https://docs.python.org/3/c-api/memory.html](https://docs.python.org/3/c-api/memory.html)).
    Note that, micro-optimizations mentioned earlier can, in most cases, be ignored
    when debugging memory issues. Also, reference counting is roughly based on a simple
    statement—if a given object is not referenced anymore, it is removed. In other
    words, all local references in a function are removed after the interpreter:'
  prefs: []
  type: TYPE_NORMAL
- en: Leaves the function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Makes sure the object is not being used anymore
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, objects that remain in memory are:'
  prefs: []
  type: TYPE_NORMAL
- en: Global objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Objects that are still referenced in some way
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Be careful with the **argument** **inbound** **outbound** edge case. If an
    object is created within the arguments, the argument reference will still be alive
    if the function returns the object. This can lead to unexpected results if it
    is used as a default value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'That is why nonmutable objects should always be used, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Reference counting in Python is handy and frees you from the obligation of manually
    tracking object references of objects, and therefore you don't have to manually
    destroy them. Although this introduces another problem, since developers never
    clean up instances in memory, it might grow in an uncontrolled way if developers
    don't pay attention to the way they use their data structures.
  prefs: []
  type: TYPE_NORMAL
- en: 'The usual memory eaters are:'
  prefs: []
  type: TYPE_NORMAL
- en: Caches that grow uncontrolled
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object factories that register instances globally and do not keep track of their
    usage, such as a database connector creator used on the fly every time a query
    is called
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Threads that are not properly finished
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Objects with a `__del__` method and involved in a cycle are also memory eaters.
    In older versions of Python (prior to 3.4 version), the garbage collector will
    not break the cycle since it cannot be sure which object should be deleted first.
    Hence, you will leak memory. Using this method is a bad idea in most cases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unfortunately, the management of reference counts must be done manually in C
    extensions using Python/C API with `Py_INCREF()` and `Py_DECREF()` macros. We
    discussed caveats of handling reference counts and reference ownership earlier
    in [Chapter 7](ch07.html "Chapter 7. Python Extensions in Other Languages"), *Python
    Extensions in Other Languages*, so you should already know that it is a pretty
    hard topic riddled with various pitfalls. This is the reason why most memory issues
    are caused by C extensions that are not written properly.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling memory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before starting to hunt down memory issues in Python, you should know that the
    nature of memory leaks in Python is quite special. In some of the compiled languages
    such as C and C++, the memory leaks are almost exclusively caused by allocated
    memory blocks that are no longer referenced by any pointer. If you don't have
    reference to memory, you cannot release it, and this very situation is called
    a *memory leak*. In Python, there is no low level memory management available
    for the user, so we rather deal with leaking references—references to objects
    that are not needed anymore but were not removed. This stops the interpreter from
    releasing resources but is not the same situation as a memory leak in C. Of course,
    there is always the exceptional case of C extensions, but they are a different
    kind of beast that need completely different tool chains and cannot be easily
    inspected from Python code.
  prefs: []
  type: TYPE_NORMAL
- en: So, memory issues in Python are mostly caused by unexpected or unplanned resource
    acquiring patterns. It happens very rarely that this is an effect of real bugs
    caused by the mishandling of memory allocation and deallocation routines. Such
    routines are available to the developer only in CPython when writing C extension
    with Python/C APIs and you will deal with them very rarely, if ever. Thus, most
    so-called memory leaks in Python are mostly caused by the overblown complexity
    of the software and minor interactions between its components that are really
    hard to track. In order to spot and locate such deficiencies of your software,
    you need to know how an actual memory usage looks in the program.
  prefs: []
  type: TYPE_NORMAL
- en: Getting information about how many objects are controlled by the Python interpreter
    and about their real size is a bit tricky. For instance, knowing how much a given
    object weighs in bytes would involve crawling down all its attributes, dealing
    with cross-references and then summing up everything. It's a pretty difficult
    problem if you consider the way objects tend to refer to each other. The `gc`
    module does not provide high-level functions for this, and it would require Python
    to be compiled in debug mode to have a full set of information.
  prefs: []
  type: TYPE_NORMAL
- en: Often, programmers just ask the system about the memory usage of their application
    after and before a given operation has been performed. But this measure is an
    approximation and depends a lot on how the memory is managed at system level.
    Using the `top` command under Linux or the Task Manager under Windows, for instance,
    makes it possible to detect memory problems when they are obvious. But this approach
    is laborious and makes it really hard to track down the faulty code block.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, there are a few tools available to make memory snapshots and calculate
    the number and size of loaded objects. But let's keep in mind that Python does
    not release memory easily, preferring to hold on to it in case it is needed again.
  prefs: []
  type: TYPE_NORMAL
- en: 'For some time, one of most popular tools to use when debugging memory issues
    and usage in Python was Guppy-PE and its Heapy component. Unfortunately, it seems
    to be no longer maintained and it lacks Python 3 support. Luckily, there are some
    other alternatives that are Python 3 compatible to some extent:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Memprof** ([http://jmdana.github.io/memprof/](http://jmdana.github.io/memprof/)):
    It is declared to work on Python 2.6, 2.7, 3.1, 3.2, and 3.3 and some POSIX-compliant
    systems (Mac OS X and Linux)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**memory_profiler** ([https://pypi.python.org/pypi/memory_profiler](https://pypi.python.org/pypi/memory_profiler)):
    It is declared to support the same Python versions and systems as Memprof'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pympler** ([http://pythonhosted.org/Pympler/](http://pythonhosted.org/Pympler/)):
    It is declared to support Python 2.5, 2.6, 2.7, 3.1, 3.2, 3.3, and 3.4 and to
    be OS independent'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that the preceding information is based purely on trove classifiers used
    by the latest distributions of featured packages. This could easily change in
    the time after this book was written. Nevertheless, there is one package that
    currently supports the widest spectrum of Python versions and is also known to
    work flawlessly under Python 3.5\. It is `objgraph`. Its APIs seem to be a bit
    clumsy and have a very limited set of functionalities. But it works, does well
    what it needs to and is really easy to use. Memory instrumentation is not a thing
    that is added to the production code permanently, so this tool does not need to
    be pretty. Because of its wide support of Python versions in OS independence,
    we will focus only on `objgraph` when discussing examples of memory profiling.
    The other tools mentioned in this section are also exciting pieces of software
    but you need to research them by yourself.
  prefs: []
  type: TYPE_NORMAL
- en: objgraph
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`objgraph` (refer to [http://mg.pov.lt/objgraph/](http://mg.pov.lt/objgraph/))
    is a simple tool for creating diagrams of object references that should be useful
    when hunting memory leaks in Python. It is available on PyPI but it is not a completely
    standalone tool and requires Graphviz in order to create memory usage diagrams.
    For developer-friendly systems like Mac OS X or Linux, you can easily obtain it
    using your preferred system package manager. For Windows, you need to download
    the Graphviz installer from the project page (refer to [http://www.graphviz.org/](http://www.graphviz.org/))
    and install it manually.'
  prefs: []
  type: TYPE_NORMAL
- en: '`objgraph` provides multiple utilities that allow you to list and print various
    statistics about memory usage and object counts. An example of such utilities
    in use is shown in the following transcript of interpreter session.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: As already said, `objgraph` allows you to create diagrams of memory usage patterns
    and cross-references that link all the objects in the given namespace. The most
    useful diagramming utilities of that library are `objgraph.show_refs()` and `objgraph.show_backrefs()`.
    They both accept reference to the object being inspected and save a diagram image
    to file using the Graphviz package. Examples of such graphs are presented in *Figure
    2* and *Figure 3*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code that was used to create these diagrams:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 2* shows the diagram of all references hold by `x` and `y` objects.
    From top to bottom and left to right it presents exactly four objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '`y = [x, [x], dict(x=x)]` list instance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dict(x=x)` dictionary instance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`[x]` list instance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`x = []` list instance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![objgraph](graphics/B05295_11_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2 An example result of the show_refs() diagram from the example() function
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 3* shows not only references between `x` and `y` but also all the objects
    that hold references to these two instances. There are so-called back references
    and are really helpful in finding objects that stop other objects from being deallocated.'
  prefs: []
  type: TYPE_NORMAL
- en: '![objgraph](graphics/B05295_11_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3 An example result of the show_backrefs() diagram from the example()
    function
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to show how `objgraph` may be used in practice, let''s review some
    practical examples. As we have already noted a few times in this book, CPython
    has its own garbage collector that exists independently from its reference counting
    method. It''s not used for general purpose memory management but only to solve
    the problem of cyclic references. In many situations, objects may reference each
    other in a way that would make it impossible to remove them using simple techniques
    based on tracking the number of references. Here is the most simple example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Such a situation is visually presented in *Figure 4*. In the preceding case,
    even if all external references to `x` and `y` objects will be removed (for instance,
    by returning from local scope of a function), these two objects cannot be removed
    because there are still two cross-references owned by these two objects. This
    is the situation where Python garbage collector steps in. It can detect cyclic
    references to objects and trigger their deallocation if there are no other valid
    references to these objects outside the cycle.
  prefs: []
  type: TYPE_NORMAL
- en: '![objgraph](graphics/B05295_11_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4 An example diagram of cyclic references between two objects
  prefs: []
  type: TYPE_NORMAL
- en: 'The real problem starts when at least one of the objects in such a cycle has
    the custom `__del__()` method defined. It is a custom deallocation handler that
    will be called when the object''s reference count finally goes to zero. It can
    execute any arbitrary Python code and so can also create new references to featured
    object. This is the reason why garbage collector prior to Python 3.4 version could
    not break reference cycles if at least one of the objects provided the custom
    `__del__()` method implementation. PEP 442 introduced safe object finalization
    to Python and became a part of the standard starting from Python 3.4\. Anyway,
    this may still be a problem for packages that worry about backwards compatibility
    and target a wide spectrum of Python interpreter versions. The following snippet
    of code shows you the differences in behavior of cyclic garbage collector in different
    Python versions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code, when executed under Python 3.3, shows that
    the cyclic garbage collector in the older versions of Python cannot collect objects
    that have the `__del__()` method defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'With a newer version of Python, the garbage collector can safely deal with
    finalization of objects even if they have the `__del__()` method defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Although custom finalization is no longer tricky in the latest Python releases,
    it still poses a problem for applications that need to work under different environments.
    As mentioned earlier, the `objgraph.show_refs()` and `objgraph.show_backrefs()`
    functions allow you to easily spot problematic class instances. For instance,
    we can easily modify the `main()` function to show all back references to the
    `WithDel` instances in order to see if we have leaking resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Running the preceding example under Python 3.3 will result in a diagram (see
    *Figure 5*), which shows that `gc.collect()` could not succeed in removing `x`,
    `y`, and `z` object instances. Additionally, `objgraph` highlights all the objects
    that have the custom `__del__()` method defined to make spotting such issues easier.
  prefs: []
  type: TYPE_NORMAL
- en: '![objgraph](graphics/B05295_11_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5 The diagram showing an example of cyclic references that can't be picked
    by the Python garbage collector prior to version 3.4
  prefs: []
  type: TYPE_NORMAL
- en: C code memory leaks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If the Python code seems perfectly fine and the memory still increases when
    you loop through the isolated function, the leak might be located on the C side.
    This happens, for instance, when a `Py_DECREF` call is missing.
  prefs: []
  type: TYPE_NORMAL
- en: The Python core code is pretty robust and tested for leaks. If you use packages
    that have C extensions, they might be a good place to look first. Because you
    will be dealing with code operating on a much lower level of abstraction than
    Python, you need to use completely different tools to resolve such memory issues.
  prefs: []
  type: TYPE_NORMAL
- en: 'Memory debugging is not easy in C, so before diving into extension internals
    make sure to properly diagnose the source of your problem. It is a very popular
    approach to isolate a suspicious package with code similar in nature to unit tests:'
  prefs: []
  type: TYPE_NORMAL
- en: Write a separate test for each API unit or functionality of an extension you
    are suspecting to leak memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform the test in a loop for an arbitrarily long time in isolation (one test
    per run)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Observe from outside which of the tested functionalities increase memory usage
    over time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using such an approach, you can isolate the faulty part of the extension and
    this will reduce the time required later to inspect and fix its code. This process
    may seem burdensome because it requires a lot of additional time and coding, but
    it really pays off in the long run. You can always ease your work by reusing some
    testing tools introduced in [Chapter 10](ch10.html "Chapter 10. Test-Driven Development"),
    *Test-Driven Development*. Utilities such as tox were perhaps not designed exactly
    for this case, but they can at least reduce the time required to run multiple
    tests in isolated environments.
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully, you have isolated the part of the extension that is leaking memory
    and can finally start actual debugging. If you're lucky, a simple manual inspection
    of the source code may give the desired results. In many cases, the problem is
    as simple as adding the missing `Py_DECREF` call. Nevertheless, in most cases,
    our work is not that simple. In such situations, you need to bring out some bigger
    guns. One of the notable generic tools for fighting memory leaks in compiled code
    that should be in every programmer's toolbelt is **Valgrind**. It is a whole instrumentation
    framework for building dynamic analysis tools. Because of this, it may not be
    easy to learn and master, but you should definitely know the basics.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling network usage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As I said earlier, an application that communicates with third-party programs
    such as databases, caches, web services, or an LDAP server can be slowed down
    when those applications are slow. This can be tracked with a regular code profiling
    method on the application side. But if the third-party software works fine on
    its own, the culprit is probably the network.
  prefs: []
  type: TYPE_NORMAL
- en: The problem might be a misconfigured hub, a low-bandwidth network link, or even
    a high number of traffic collisions that make computers send the same packets
    several times.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few elements to get you in. To find out what is going on, there
    are three fields to investigate at first:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Watch the network traffic using tools such as:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ntop`: [http://www.ntop.org](http://www.ntop.org) (Linux only)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`wireshark`: [www.wireshark.org](http://www.wireshark.org) (previously named
    Ethereal)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Track down unhealthy or misconfigured devices with `net-snmp` ([http://www.net-snmp.org](http://www.net-snmp.org)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimate the bandwidth between two computers using `Pathrate`, a statistical
    tool. See [http://www.cc.gatech.edu/~dovrolis/bw-est/pathrate.html](http://www.cc.gatech.edu/~dovrolis/bw-est/pathrate.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to go further on network performance issues, you might also want
    to read *Network Performance Open Source Toolkit*, *Wiley*, by Richard Blum. This
    book exposes strategies to tune the applications that are heavily using the network
    and provides a tutorial to scan complex network problems.
  prefs: []
  type: TYPE_NORMAL
- en: '*High Performance MySQL*, *O''Reilly Media*, by Jeremy Zawodny is also a good
    book to read when writing an application that uses MySQL.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we have seen:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The three rules of optimization:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make it work first
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Take the user's point of view
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep the code readable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An optimization strategy based on writing a scenario with speed objectives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to profile CPU or memory usage and a few tips for network profiling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that you know how to locate your performance problems, the next chapter
    provides some popular and generic strategies to get rid of them.
  prefs: []
  type: TYPE_NORMAL
