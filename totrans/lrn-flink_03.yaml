- en: Chapter 3.  Data Processing Using the Batch Processing API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even though many people appreciate the potential value of streaming data processing
    in most industries, there are many use cases where people don't feel it is necessary
    to process the data in a streaming manner. In all such cases, batch processing
    is the way to go. So far Hadoop has been the default choice for data processing.
    However, Flink also supports batch data processing by DataSet API.
  prefs: []
  type: TYPE_NORMAL
- en: For Flink, batch processing is a special case of stream processing. Here is
    a very interesting article explaining this thought in detail at [http://data-artisans.com/batch-is-a-special-case-of-streaming/](http://data-artisans.com/batch-is-a-special-case-of-streaming/).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to look at the details regarding DataSet API.
    This includes the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Data sources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data sinks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As we learnt in the previous chapter, any Flink program works on a certain
    defined anatomy as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Data Processing Using the Batch Processing API](img/image_03_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The DataSet API is not an exception to this flow. We will look at each step
    in detail. We already discussed in the previous chapter how to obtain the execution
    environment. So we will directly move to the details of data sources supported
    by DataSet API.
  prefs: []
  type: TYPE_NORMAL
- en: Data sources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sources are places where the DataSet API expects to get its data from. It could
    in the form of a file or from Java collections. This is the second step in the
    Flink program's anatomy. DataSet API supports a number of pre-implemented data
    source functions. It also supports writing custom data source functions so anything
    that is not supported can be programmed easily. First let's try to understand
    the built-in source functions.
  prefs: []
  type: TYPE_NORMAL
- en: File-based
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Flink supports reading data from files. It reads data line by line and returns
    it as strings. The following are built-in functions you can use to read data:'
  prefs: []
  type: TYPE_NORMAL
- en: '`readTextFile(Stringpath)`: This reads data from a file specified in the path.
    By default it will read `TextInputFormat` and will read strings line by line.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`readTextFileWithValue(Stringpath)`: This reads data from a file specified
    in the path. It returns `StringValues`. `StringValues` are mutable strings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`readCsvFile(Stringpath`): This reads data from comma separated files. It returns
    the Java POJOs or tuples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`readFileofPremitives(path, delimiter, class)`: This parses the new line into
    primitive data types such as strings or integers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`readHadoopFile(FileInputFormat, Key, Value, path)`: This reads files from
    a specified path with the given `FileInputFormat`, `Key` class and `Value` class.
    It returns the parsed values as tuples `Tuple2<Key,Value>`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`readSequenceFile(Key, Value, path)`: This reads files from a specified path
    with the given `SequenceFileInputFormat`, `Key` class and `Value` class. It returns
    the parsed values as tuples `Tuple2<Key,Value>`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For file-based inputs, Flink supports the recursive traversal of folders specified
    in a given path. In order to use this facility, we need to set an environment
    variable and pass it as a parameter while reading the data. The variable to set
    is `recursive.file.enumeration`. We need to set this variable to `true` in order
    to enable recursive traversal.
  prefs: []
  type: TYPE_NORMAL
- en: Collection-based
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With Flink DataSet API, we can also read data from Java-based collections.
    The following are some functions we can use to read the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '`fromCollection(Collection)`: This creates a dataset from Java-based collections.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fromCollection(Iterator, Class)`: This creates a dataset from an iterator.
    The elements of the iterator are of a type given by the class parameter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fromElements(T)`: This creates a dataset of a sequence of objects. The object
    type is specified in the function itself.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fromParallelCollection(SplittableIterator, Class)`: This creates a dataset
    from the iterator in parallel. Class represents the object types.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generateSequence(from, to)`: This generates the sequence of numbers between
    given limits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generic sources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'DataSet API supports a couple of generic functions to read data:'
  prefs: []
  type: TYPE_NORMAL
- en: '`readFile(inputFormat, path)`: This creates a dataset of the type `FileInputFormat`
    from a given path'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`createInput(inputFormat)`: This creates a dataset of the generic input format'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compressed files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Flink supports the decompression of files while reading if they are marked with
    proper extensions. We don't need to do any different configurations to read the
    compressed files. If a file with a proper extension is detected then Flink automatically
    decompresses it and sends it for further processing.
  prefs: []
  type: TYPE_NORMAL
- en: One thing to note here is that decompression of files cannot be done in parallel
    so this might take a bit of time before the actual data processing starts.
  prefs: []
  type: TYPE_NORMAL
- en: At this stage, it is recommended to avoid using compressed files as decompression
    is not a scalable activity in Flink.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are supported algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Compression algorithm** | **Extension** | **Is parallel?** |'
  prefs: []
  type: TYPE_TB
- en: '| Gzip | `.gz`, `.gzip` | No |'
  prefs: []
  type: TYPE_TB
- en: '| Deflate | `.deflate` | No |'
  prefs: []
  type: TYPE_TB
- en: Transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data transformations transform the dataset from one form into another. The input
    could be one or more datasets and the output could also be zero, or one or more
    data streams. Now let's try to understand each transformation one by one.
  prefs: []
  type: TYPE_NORMAL
- en: Map
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is one of the simplest transformations where the input is one dataset and
    output is also one dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Flat map
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The flat map takes one record and outputs zero, or one or more than one records.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Filter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Filter functions evaluate the conditions and then if returned `true` only emit
    the record. Filter functions can output zero records.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Project
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Project transformations remove or move the elements of a tuple into another.
    This can be used to do selective processing on specific elements.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In Scala, this transformation is not supported.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Reduce on grouped datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reduce transformations reduce each group into a single element based on the
    user-defined reduce function.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In Python, the code is not supported.
  prefs: []
  type: TYPE_NORMAL
- en: Reduce on grouped datasets by field position key
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For datasets with tuples, we can also group by the field positions. The following
    is an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Group combine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In some applications, it is important to do intermediate operations before doing
    some more transformations. Group combine operations can be very handy in this
    case. Intermediate transformations could be reducing the size and so on.
  prefs: []
  type: TYPE_NORMAL
- en: This is performed in memory with a greedy strategy that gets performed in multiple
    steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In Python, this code is not supported.
  prefs: []
  type: TYPE_NORMAL
- en: Aggregate on a grouped tuple dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Aggregate transformations are very common. We can easily perform common aggregations
    such as `sum`, `min`, and `max` on tuple datasets. The following is the way we
    do it.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'In Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Please note here that in DataSet API, if we need to apply multiple aggregations,
    we need to use the `and` keyword.
  prefs: []
  type: TYPE_NORMAL
- en: MinBy on a grouped tuple dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `minBy` function selects a single tuple from each group of tuple datasets
    for which the value is the minimum. The fields used for comparison must be comparable.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: In Python, this code is not supported.
  prefs: []
  type: TYPE_NORMAL
- en: MaxBy on a grouped tuple dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `MaxBy` function selects a single tuple from each group of tuple datasets
    for which the value is the maximum. The fields used for comparison must be comparable.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: In Python, this code is not supported.
  prefs: []
  type: TYPE_NORMAL
- en: Reduce on full dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The reduce transformation allows for the application of a user-defined function
    on a full dataset. Here is an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'In Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Group reduce on a full dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The group reduce transformation allows for the application of a user-defined
    function on a full dataset. Here is an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'In Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Aggregate on a full tuple dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can run common aggregation functions on full datasets. So far Flink supports
    `MAX`, `MIN`, and `SUM`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'In Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: MinBy on a full tuple dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `MinBy` function selects a single tuple from the full dataset for which
    the value is the minimum. The fields used for comparison must be comparable.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: In Python, this code is not supported.
  prefs: []
  type: TYPE_NORMAL
- en: MaxBy on a full tuple dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`MaxBy` selects a single tuple full dataset for which the value is maximum.
    The fields used for comparison must be comparable.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: In Python, this code is not supported.
  prefs: []
  type: TYPE_NORMAL
- en: Distinct
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The distinct transformation emits distinct values from the source dataset. This
    is used for removing duplicate values from the source.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: In Python, this code is not supported.
  prefs: []
  type: TYPE_NORMAL
- en: Join
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The join transformation joins two datasets into one dataset. The joining condition
    can be defined as one of the keys from each dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: In Python
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are various other ways in which two datasets can be joined. Here is a
    link where you can read more about all such joining options: [https://ci.apache.org/projects/flink/flink-docs-master/dev/batch/dataset_transformations.html#join](https://ci.apache.org/projects/flink/flink-docs-master/dev/batch/dataset_transformations.html#join).'
  prefs: []
  type: TYPE_NORMAL
- en: Cross
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The cross transformation does the cross product of two datasets by applying
    a user-defined function.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'In Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Union
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The union transformation combines two similar datasets. We can also union multiple
    datasets in one go.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'In Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Rebalance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This transformation evenly rebalances parallel partitions. This helps in achieving
    better performance as it helps in removing data skews.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: In Python, this code is not supported.
  prefs: []
  type: TYPE_NORMAL
- en: Hash partition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This transformation partitions the dataset on a given key.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: In Python, this code is not supported.
  prefs: []
  type: TYPE_NORMAL
- en: Range partition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This transformation range partitions the dataset on a given key.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: In Python, this code is not supported.
  prefs: []
  type: TYPE_NORMAL
- en: Sort partition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This transformation locally sorts the partitions dataset on a given key and
    in the given order.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: In Python, this code is not supported.
  prefs: []
  type: TYPE_NORMAL
- en: First-n
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This transformation arbitrarily returns the first-n elements of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: In Python, this code is not supported.
  prefs: []
  type: TYPE_NORMAL
- en: Broadcast variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Broadcast variables allow user to access certain dataset as collection to all
    operators. Generally, broadcast variables are used when we you want to refer a
    small amount of data frequently in a certain operation. Those who are familiar
    with Spark broadcast variables will be able use the same feature in Flink as well.
  prefs: []
  type: TYPE_NORMAL
- en: We just need to broadcast a dataset with a specific name and it will be available
    on each executors handy. The broadcast variables are kept in memory so we have
    to be cautious in using them. The following code snippet shows how to broadcast
    a dataset and use it as needed.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Broadcast variables are useful when we have look up conditions to be used for
    transformation and the lookup dataset is comparatively small.
  prefs: []
  type: TYPE_NORMAL
- en: Data sinks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After the data transformations are done, we need to save the results somewhere.
    The following are some options that Flink DataSet API provides to save the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '`writeAsText()`: This writes records one line at a time as strings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`writeAsCsV()`: This writes tuples as comma-separated value files. Row and
    field delimiters can also be configured.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`print()`/`printErr()`: This writes records to the standard output. You can
    also choose to write to a standard error.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`write()`: This supports writing data in a custom `FileOutputFormat`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output()`: This is used for datasets which are not file-based. This can be
    used where we want to write data to some database.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Flink's DataSet API supports various connectors, allowing data read/writes
    across various systems. Let's try to explore more on this.
  prefs: []
  type: TYPE_NORMAL
- en: Filesystems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Flink allows connecting to various distributed filesystems such as HDFS, S3,
    Google Cloud Storage, Alluxio, and so on, by default. In this section, we will
    see how to connect to these filesystems.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to connect to these systems, we need to add the following dependency
    in `pom.xml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: This allows us to use Hadoop data types, input formats, and output formats.
    Flink supports writable and writable comparable out-of-the-box, so we don't need
    the compatibility dependency for that.
  prefs: []
  type: TYPE_NORMAL
- en: HDFS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To read data from an HDFS file, we create a data source using the `readHadoopFile()`
    or `createHadoopInput()` method. In order to use this connector, we first need
    to configure `flink-conf.yaml` and set `fs.hdfs.hadoopconf` to the proper Hadoop
    configuration directory.
  prefs: []
  type: TYPE_NORMAL
- en: The resulting dataset would be a tuple of the type that matches with the HDFS
    data types. The following code snippet shows how to do this.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: We can also use this connector to write back the processed data to HDFS. The
    `OutputFormat` wrapper expects the dataset to be in `Tuple2` format. The following
    code snippet shows how to write back processed data to HDFS.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'In Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Amazon S3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As stated earlier, Flink supports reading data from Amazon S3 by default. But
    we need to do some configurations in Hadoop''s `core-site.xml`. We need to set
    the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Once done, we can access the S3 filesystem as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Alluxio
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Alluxio is an open source, memory speed virtual distributed storage. Many companies
    have been using Alluxio for high-speed data storage and processing. You can read
    more about Alluxio at: [http://www.alluxio.org/](http://www.alluxio.org/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Flink supports reading data from Alluxio by default. But we need to do some
    configurations in Hadoop `core-site.xml`. We need to set the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Once done, we can access the Alluxio filesystem as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Avro
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Flink has built-in support for Avro files. It allows easy reads and writes
    to Avro files. In order to read Avro files, we need to use `AvroInputFormat`.
    The following code snippet shows how to read Avro files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the dataset is ready we can easily perform various transformations, such
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Microsoft Azure storage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Microsoft Azure Storage is a cloud-based storage that allows storing data in
    a durable and scalable manner. Flink supports managing data stored on Microsoft
    Azure table storage. The following explains how we do this.
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, we need to download the `azure-tables-hadoop` project from `git`
    and then compile it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we add the following dependencies in `pom.xml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we write the following code to access Azure storage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: Now we are all set do any processing of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: MongoDB
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Through open source contributions, developers have been able to connect Flink
    to MongoDB. In this section, we are going to talk about one such project.
  prefs: []
  type: TYPE_NORMAL
- en: 'The project is open source and can be downloaded from GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we use the preceding connector in the Java program to connect to MongoDB:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the data is available as a dataset, we can easily do the desired transformation.
    We can also write back the data to the MongoDB collection as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: Iterations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the unique features Flink supports is iterations. These days a lot developers
    want to run iterative machine-learning and graph-processing algorithms using big
    data technologies. To cater to these needs, Flink supports running iterative data
    processing by defining a step function.
  prefs: []
  type: TYPE_NORMAL
- en: Iterator operator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An iterator operator consists of the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Iterator operator](img/image_03_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Iteration Input**: This is either the initial dataset received or the output
    of the previous iteration'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Step Function**: This is the function that needs to be applied on input dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Next Partial Solution**: This is the output of the step function which needs
    to be fed back to the next iteration'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Iteration Result**:After all iterations are completed, we get the result
    of iterations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of iterations can be controlled by various ways. One way could be
    setting up the number of iterations to perform or we can also put conditional
    termination.
  prefs: []
  type: TYPE_NORMAL
- en: Delta iterator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The delta operator iterates over the set of elements for incremental iteration
    operations. The main difference between that the delta iterator and regular iterator
    is, delta iterator works on updating the solution set rather than fully re-computing
    it every iteration.
  prefs: []
  type: TYPE_NORMAL
- en: This leads to more efficient operations as it allows us to focus on the important
    parts of the solution in less time. The following diagram shows flow of delta
    iterator in Flink.
  prefs: []
  type: TYPE_NORMAL
- en: '![Delta iterator](img/image_03_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Iteration Input**: We have to read the work set and solution set for the
    delta iterator from some files'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Step Function**: Step function is the function that needs to be applied on
    the input dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Next Work Set/ Update Solution**: Here after every iteration solution set
    it is updated with the latest results and the next work set is fed to the next
    iteration'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Iteration Result**: After all iterations are completed, we get the result
    of the iterations in the form of a solution set'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since delta iterators work on hot dataset itself, the performance and efficiency
    are great. Here is a detailed article that talks about using Flink iterations
    for the PageRank algorithm. [http://data-artisans.com/data-analysis-with-flink-a-case-study-and-tutorial/](http://data-artisans.com/data-analysis-with-flink-a-case-study-and-tutorial/).
  prefs: []
  type: TYPE_NORMAL
- en: Use case - Athletes data insights using Flink batch API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have learnt the details of DataSet API, let''s try to apply this
    knowledge to a real-life use case. Let''s say we have a dataset with us, which
    has information about the Olympics athletes and their performance in various games.
    The sample data looks like the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Player** | **Country** | **Year** | **Game** | **Gold** | **Silver** |
    **Bronze** | **Total** |'
  prefs: []
  type: TYPE_TB
- en: '| Yang Yilin | China | 2008 | Gymnastics | 1 | 0 | 2 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| Leisel Jones | Australia | 2000 | Swimming | 0 | 2 | 0 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| Go Gi-Hyeon | South Korea | 2002 | Short-Track Speed Skating | 1 | 1 | 0
    | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| Chen Ruolin | China | 2008 | Diving | 2 | 0 | 0 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| Katie Ledecky | United States | 2012 | Swimming | 1 | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Ruta Meilutyte | Lithuania | 2012 | Swimming | 1 | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| DAiniel Gyurta | Hungary | 2004 | Swimming | 0 | 1 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Arianna Fontana | Italy | 2006 | Short-Track Speed Skating | 0 | 0 | 1 |
    1 |'
  prefs: []
  type: TYPE_TB
- en: '| Olga Glatskikh | Russia | 2004 | Rhythmic Gymnastics | 1 | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Kharikleia Pantazi | Greece | 2000 | Rhythmic Gymnastics | 0 | 0 | 1 | 1
    |'
  prefs: []
  type: TYPE_TB
- en: '| Kim Martin | Sweden | 2002 | Ice Hockey | 0 | 0 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Kyla Ross | United States | 2012 | Gymnastics | 1 | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Gabriela Dragoi | Romania | 2008 | Gymnastics | 0 | 0 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Tasha Schwikert-Warren | United States | 2000 | Gymnastics | 0 | 0 | 1 |
    1 |'
  prefs: []
  type: TYPE_TB
- en: Now we want to get answers to the questions such as, How many players per country
    participated in the games? Or how many players participated for each game? As
    the data is at rest, we will be using Flink Batch API to analyze it.
  prefs: []
  type: TYPE_NORMAL
- en: The data available is in the CSV format. So we will be using a CSV reader provided
    by Flink API as shown in the following code snippet.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the data is parse properly, it is easy to move ahead and use it as required.
    The following code snippet shows how to get information of no. of players per
    country:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code snippet, we are first creating datasets with the key
    as the player''s country and value as `1` and then we group it and sum up the
    values to get the total count. Once we execute the code, here is how the output
    looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly we can apply the same logic to find the number of players per game
    as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code snippet would look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: This way you can run various other transformations to get the desired output.
    The complete code for this use case is available at [https://github.com/deshpandetanmay/mastering-flink/tree/master/chapter03/flink-batch](https://github.com/deshpandetanmay/mastering-flink/tree/master/chapter03/flink-batch).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learnt about DataSet API. It enabled us to do the batch
    processing. We learnt various transformations in order to do data processing.
    Later we also explored the various file-based connectors to read/write data from
    HDFS, Amazon S3, MS Azure, Alluxio, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: In the last section, we looked a use case where we applied the knowledge learnt
    in the earlier sections.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to learn another very important API from Flink's
    ecosystem point of view that is, Table API.
  prefs: []
  type: TYPE_NORMAL
