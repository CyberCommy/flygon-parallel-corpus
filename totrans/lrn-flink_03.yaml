- en: Chapter 3.  Data Processing Using the Batch Processing API
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章。使用批处理API进行数据处理
- en: Even though many people appreciate the potential value of streaming data processing
    in most industries, there are many use cases where people don't feel it is necessary
    to process the data in a streaming manner. In all such cases, batch processing
    is the way to go. So far Hadoop has been the default choice for data processing.
    However, Flink also supports batch data processing by DataSet API.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管许多人欣赏流数据处理在大多数行业中的潜在价值，但也有许多用例，人们认为不需要以流式方式处理数据。在所有这些情况下，批处理是前进的方式。到目前为止，Hadoop一直是数据处理的默认选择。但是，Flink也通过DataSet
    API支持批处理数据处理。
- en: For Flink, batch processing is a special case of stream processing. Here is
    a very interesting article explaining this thought in detail at [http://data-artisans.com/batch-is-a-special-case-of-streaming/](http://data-artisans.com/batch-is-a-special-case-of-streaming/).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Flink，批处理是流处理的一种特殊情况。在[http://data-artisans.com/batch-is-a-special-case-of-streaming/](http://data-artisans.com/batch-is-a-special-case-of-streaming/)上有一篇非常有趣的文章详细解释了这个想法。
- en: 'In this chapter, we are going to look at the details regarding DataSet API.
    This includes the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将详细了解DataSet API的详细信息。这包括以下主题：
- en: Data sources
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据源
- en: Transformations
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换
- en: Data sinks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据接收器
- en: Connectors
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接器
- en: 'As we learnt in the previous chapter, any Flink program works on a certain
    defined anatomy as follows:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一章中学到的，任何Flink程序都遵循以下定义的解剖结构：
- en: '![Data Processing Using the Batch Processing API](img/image_03_001.jpg)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![使用批处理API进行数据处理](img/image_03_001.jpg)'
- en: The DataSet API is not an exception to this flow. We will look at each step
    in detail. We already discussed in the previous chapter how to obtain the execution
    environment. So we will directly move to the details of data sources supported
    by DataSet API.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: DataSet API也不例外。我们将详细了解每个步骤。我们已经在上一章中讨论了如何获取执行环境。因此，我们将直接转向DataSet API支持的数据源的详细信息。
- en: Data sources
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据源
- en: Sources are places where the DataSet API expects to get its data from. It could
    in the form of a file or from Java collections. This is the second step in the
    Flink program's anatomy. DataSet API supports a number of pre-implemented data
    source functions. It also supports writing custom data source functions so anything
    that is not supported can be programmed easily. First let's try to understand
    the built-in source functions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 源是DataSet API期望从中获取数据的地方。它可以是文件形式，也可以是来自Java集合。这是Flink程序解剖的第二步。DataSet API支持许多预先实现的数据源函数。它还支持编写自定义数据源函数，因此可以轻松地编程任何不受支持的内容。首先让我们尝试理解内置的源函数。
- en: File-based
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于文件的
- en: 'Flink supports reading data from files. It reads data line by line and returns
    it as strings. The following are built-in functions you can use to read data:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Flink支持从文件中读取数据。它逐行读取数据并将其作为字符串返回。以下是您可以使用的内置函数来读取数据：
- en: '`readTextFile(Stringpath)`: This reads data from a file specified in the path.
    By default it will read `TextInputFormat` and will read strings line by line.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`readTextFile(Stringpath)`: 从指定路径读取文件中的数据。默认情况下，它将读取`TextInputFormat`并逐行读取字符串。'
- en: '`readTextFileWithValue(Stringpath)`: This reads data from a file specified
    in the path. It returns `StringValues`. `StringValues` are mutable strings.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`readTextFileWithValue(Stringpath)`: 从指定路径读取文件中的数据。它返回`StringValues`。`StringValues`是可变字符串。'
- en: '`readCsvFile(Stringpath`): This reads data from comma separated files. It returns
    the Java POJOs or tuples.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`readCsvFile(Stringpath)`: 从逗号分隔的文件中读取数据。它返回Java POJOs或元组。'
- en: '`readFileofPremitives(path, delimiter, class)`: This parses the new line into
    primitive data types such as strings or integers.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`readFileofPremitives(path, delimiter, class)`: 将新行解析为原始数据类型，如字符串或整数。'
- en: '`readHadoopFile(FileInputFormat, Key, Value, path)`: This reads files from
    a specified path with the given `FileInputFormat`, `Key` class and `Value` class.
    It returns the parsed values as tuples `Tuple2<Key,Value>`.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`readHadoopFile(FileInputFormat, Key, Value, path)`: 从指定路径使用给定的`FileInputFormat`、`Key`类和`Value`类读取文件。它将解析后的值返回为元组`Tuple2<Key,Value>`。'
- en: '`readSequenceFile(Key, Value, path)`: This reads files from a specified path
    with the given `SequenceFileInputFormat`, `Key` class and `Value` class. It returns
    the parsed values as tuples `Tuple2<Key,Value>`.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`readSequenceFile(Key, Value, path)`: 从指定路径使用给定的`SequenceFileInputFormat`、`Key`类和`Value`类读取文件。它将解析后的值返回为元组`Tuple2<Key,Value>`。'
- en: Note
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For file-based inputs, Flink supports the recursive traversal of folders specified
    in a given path. In order to use this facility, we need to set an environment
    variable and pass it as a parameter while reading the data. The variable to set
    is `recursive.file.enumeration`. We need to set this variable to `true` in order
    to enable recursive traversal.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于文件的输入，Flink支持递归遍历指定路径中的文件夹。为了使用这个功能，我们需要设置一个环境变量，并在读取数据时将其作为参数传递。要设置的变量是`recursive.file.enumeration`。我们需要将此变量设置为`true`以启用递归遍历。
- en: Collection-based
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于集合的
- en: 'With Flink DataSet API, we can also read data from Java-based collections.
    The following are some functions we can use to read the data:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Flink DataSet API，我们还可以从基于Java的集合中读取数据。以下是一些我们可以使用的函数来读取数据：
- en: '`fromCollection(Collection)`: This creates a dataset from Java-based collections.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fromCollection(Collection)`: 从基于Java的集合创建数据集。'
- en: '`fromCollection(Iterator, Class)`: This creates a dataset from an iterator.
    The elements of the iterator are of a type given by the class parameter.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fromCollection(Iterator, Class)`: 从迭代器创建数据集。迭代器的元素由类参数给定的类型。'
- en: '`fromElements(T)`: This creates a dataset of a sequence of objects. The object
    type is specified in the function itself.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fromElements(T)`: 创建一个包含一系列对象的数据集。对象类型在函数本身中指定。'
- en: '`fromParallelCollection(SplittableIterator, Class)`: This creates a dataset
    from the iterator in parallel. Class represents the object types.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fromParallelCollection(SplittableIterator, Class)`: 这将并行从迭代器创建数据集。Class代表对象类型。'
- en: '`generateSequence(from, to)`: This generates the sequence of numbers between
    given limits.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generateSequence(from, to)`: 生成给定范围内的数字序列。'
- en: Generic sources
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'DataSet API supports a couple of generic functions to read data:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '`readFile(inputFormat, path)`: This creates a dataset of the type `FileInputFormat`
    from a given path'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`createInput(inputFormat)`: This creates a dataset of the generic input format'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compressed files
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Flink supports the decompression of files while reading if they are marked with
    proper extensions. We don't need to do any different configurations to read the
    compressed files. If a file with a proper extension is detected then Flink automatically
    decompresses it and sends it for further processing.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: One thing to note here is that decompression of files cannot be done in parallel
    so this might take a bit of time before the actual data processing starts.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: At this stage, it is recommended to avoid using compressed files as decompression
    is not a scalable activity in Flink.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are supported algorithms:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '| **Compression algorithm** | **Extension** | **Is parallel?** |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
- en: '| Gzip | `.gz`, `.gzip` | No |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
- en: '| Deflate | `.deflate` | No |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
- en: Transformations
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data transformations transform the dataset from one form into another. The input
    could be one or more datasets and the output could also be zero, or one or more
    data streams. Now let's try to understand each transformation one by one.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Map
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is one of the simplest transformations where the input is one dataset and
    output is also one dataset.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In Scala:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In Python:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Flat map
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The flat map takes one record and outputs zero, or one or more than one records.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In Scala:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In Python:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Filter
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Filter functions evaluate the conditions and then if returned `true` only emit
    the record. Filter functions can output zero records.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In Python:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Project
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Project transformations remove or move the elements of a tuple into another.
    This can be used to do selective processing on specific elements.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In Scala, this transformation is not supported.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 'In Python:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Reduce on grouped datasets
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reduce transformations reduce each group into a single element based on the
    user-defined reduce function.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In Scala:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In Python, the code is not supported.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Reduce on grouped datasets by field position key
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For datasets with tuples, we can also group by the field positions. The following
    is an example.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In Scala:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In Python:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Group combine
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In some applications, it is important to do intermediate operations before doing
    some more transformations. Group combine operations can be very handy in this
    case. Intermediate transformations could be reducing the size and so on.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: This is performed in memory with a greedy strategy that gets performed in multiple
    steps.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In Scala:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In Python, this code is not supported.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Aggregate on a grouped tuple dataset
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Aggregate transformations are very common. We can easily perform common aggregations
    such as `sum`, `min`, and `max` on tuple datasets. The following is the way we
    do it.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'In Scala:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In Python:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Please note here that in DataSet API, if we need to apply multiple aggregations,
    we need to use the `and` keyword.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: MinBy on a grouped tuple dataset
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `minBy` function selects a single tuple from each group of tuple datasets
    for which the value is the minimum. The fields used for comparison must be comparable.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In Scala:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In Python, this code is not supported.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: MaxBy on a grouped tuple dataset
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `MaxBy` function selects a single tuple from each group of tuple datasets
    for which the value is the maximum. The fields used for comparison must be comparable.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'In Scala:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In Python, this code is not supported.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Reduce on full dataset
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The reduce transformation allows for the application of a user-defined function
    on a full dataset. Here is an example.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In Scala:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'In Python:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Group reduce on a full dataset
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The group reduce transformation allows for the application of a user-defined
    function on a full dataset. Here is an example.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'In Scala:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'In Python:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Aggregate on a full tuple dataset
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can run common aggregation functions on full datasets. So far Flink supports
    `MAX`, `MIN`, and `SUM`.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'In Scala:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'In Python:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: MinBy on a full tuple dataset
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `MinBy` function selects a single tuple from the full dataset for which
    the value is the minimum. The fields used for comparison must be comparable.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'In Scala:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: In Python, this code is not supported.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: MaxBy on a full tuple dataset
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`MaxBy` selects a single tuple full dataset for which the value is maximum.
    The fields used for comparison must be comparable.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'In Scala:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: In Python, this code is not supported.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Distinct
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The distinct transformation emits distinct values from the source dataset. This
    is used for removing duplicate values from the source.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'In Scala:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: In Python, this code is not supported.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Join
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The join transformation joins two datasets into one dataset. The joining condition
    can be defined as one of the keys from each dataset.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'In Scala:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: In Python
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Note
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are various other ways in which two datasets can be joined. Here is a
    link where you can read more about all such joining options: [https://ci.apache.org/projects/flink/flink-docs-master/dev/batch/dataset_transformations.html#join](https://ci.apache.org/projects/flink/flink-docs-master/dev/batch/dataset_transformations.html#join).'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Cross
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The cross transformation does the cross product of two datasets by applying
    a user-defined function.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'In Scala:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'In Python:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Union
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The union transformation combines two similar datasets. We can also union multiple
    datasets in one go.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'In Scala:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'In Python:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Rebalance
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This transformation evenly rebalances parallel partitions. This helps in achieving
    better performance as it helps in removing data skews.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'In Scala:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: In Python, this code is not supported.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Hash partition
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This transformation partitions the dataset on a given key.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'In Scala:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: In Python, this code is not supported.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Range partition
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This transformation range partitions the dataset on a given key.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'In Scala:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: In Python, this code is not supported.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Sort partition
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This transformation locally sorts the partitions dataset on a given key and
    in the given order.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'In Scala:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: In Python, this code is not supported.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: First-n
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This transformation arbitrarily returns the first-n elements of the dataset.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 'In Java:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'In Scala:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: In Python, this code is not supported.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Broadcast variables
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Broadcast variables allow user to access certain dataset as collection to all
    operators. Generally, broadcast variables are used when we you want to refer a
    small amount of data frequently in a certain operation. Those who are familiar
    with Spark broadcast variables will be able use the same feature in Flink as well.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: We just need to broadcast a dataset with a specific name and it will be available
    on each executors handy. The broadcast variables are kept in memory so we have
    to be cautious in using them. The following code snippet shows how to broadcast
    a dataset and use it as needed.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Broadcast variables are useful when we have look up conditions to be used for
    transformation and the lookup dataset is comparatively small.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Data sinks
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After the data transformations are done, we need to save the results somewhere.
    The following are some options that Flink DataSet API provides to save the results:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '`writeAsText()`: This writes records one line at a time as strings.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`writeAsCsV()`: This writes tuples as comma-separated value files. Row and
    field delimiters can also be configured.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`print()`/`printErr()`: This writes records to the standard output. You can
    also choose to write to a standard error.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`write()`: This supports writing data in a custom `FileOutputFormat`.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output()`: This is used for datasets which are not file-based. This can be
    used where we want to write data to some database.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connectors
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Flink's DataSet API supports various connectors, allowing data read/writes
    across various systems. Let's try to explore more on this.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Filesystems
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Flink allows connecting to various distributed filesystems such as HDFS, S3,
    Google Cloud Storage, Alluxio, and so on, by default. In this section, we will
    see how to connect to these filesystems.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: Flink允许默认连接到各种分布式文件系统，如HDFS、S3、Google Cloud Storage、Alluxio等。在本节中，我们将看到如何连接到这些文件系统。
- en: 'In order to connect to these systems, we need to add the following dependency
    in `pom.xml`:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 为了连接到这些系统，我们需要在`pom.xml`中添加以下依赖项：
- en: '[PRE59]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: This allows us to use Hadoop data types, input formats, and output formats.
    Flink supports writable and writable comparable out-of-the-box, so we don't need
    the compatibility dependency for that.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们能够使用Hadoop数据类型、输入格式和输出格式。Flink支持开箱即用的可写和可比较可写，因此我们不需要兼容性依赖项。
- en: HDFS
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: HDFS
- en: To read data from an HDFS file, we create a data source using the `readHadoopFile()`
    or `createHadoopInput()` method. In order to use this connector, we first need
    to configure `flink-conf.yaml` and set `fs.hdfs.hadoopconf` to the proper Hadoop
    configuration directory.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 要从HDFS文件中读取数据，我们使用`readHadoopFile()`或`createHadoopInput()`方法创建数据源。为了使用此连接器，我们首先需要配置`flink-conf.yaml`并将`fs.hdfs.hadoopconf`设置为正确的Hadoop配置目录。
- en: The resulting dataset would be a tuple of the type that matches with the HDFS
    data types. The following code snippet shows how to do this.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的数据集将是与HDFS数据类型匹配的元组类型。以下代码片段显示了如何做到这一点。
- en: 'In Java:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE60]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'In Scala:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE61]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: We can also use this connector to write back the processed data to HDFS. The
    `OutputFormat` wrapper expects the dataset to be in `Tuple2` format. The following
    code snippet shows how to write back processed data to HDFS.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用此连接器将处理后的数据写回HDFS。`OutputFormat`包装器期望数据集以`Tuple2`格式。以下代码片段显示了如何将处理后的数据写回HDFS。
- en: 'In Java:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java中：
- en: '[PRE62]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'In Scala:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala中：
- en: '[PRE63]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Amazon S3
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Amazon S3
- en: 'As stated earlier, Flink supports reading data from Amazon S3 by default. But
    we need to do some configurations in Hadoop''s `core-site.xml`. We need to set
    the following properties:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Flink默认支持从Amazon S3读取数据。但是，我们需要在Hadoop的`core-site.xml`中进行一些配置。我们需要设置以下属性：
- en: '[PRE64]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Once done, we can access the S3 filesystem as shown here:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，我们可以像这样访问S3文件系统：
- en: '[PRE65]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Alluxio
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Alluxio
- en: 'Alluxio is an open source, memory speed virtual distributed storage. Many companies
    have been using Alluxio for high-speed data storage and processing. You can read
    more about Alluxio at: [http://www.alluxio.org/](http://www.alluxio.org/).'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: Alluxio是一个开源的、内存速度的虚拟分布式存储。许多公司都在使用Alluxio进行高速数据存储和处理。您可以在[http://www.alluxio.org/](http://www.alluxio.org/)上了解更多关于Alluxio的信息。
- en: 'Flink supports reading data from Alluxio by default. But we need to do some
    configurations in Hadoop `core-site.xml`. We need to set the following properties:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: Flink默认支持从Alluxio读取数据。但是，我们需要在Hadoop的`core-site.xml`中进行一些配置。我们需要设置以下属性：
- en: '[PRE66]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Once done, we can access the Alluxio filesystem as shown here:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，我们可以像这样访问Alluxio文件系统：
- en: '[PRE67]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Avro
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Avro
- en: 'Flink has built-in support for Avro files. It allows easy reads and writes
    to Avro files. In order to read Avro files, we need to use `AvroInputFormat`.
    The following code snippet shows how to read Avro files:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: Flink内置支持Avro文件。它允许轻松读写Avro文件。为了读取Avro文件，我们需要使用`AvroInputFormat`。以下代码片段显示了如何读取Avro文件：
- en: '[PRE68]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Once the dataset is ready we can easily perform various transformations, such
    as:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集准备好后，我们可以轻松执行各种转换，例如：
- en: '[PRE69]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Microsoft Azure storage
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Microsoft Azure存储
- en: Microsoft Azure Storage is a cloud-based storage that allows storing data in
    a durable and scalable manner. Flink supports managing data stored on Microsoft
    Azure table storage. The following explains how we do this.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: Microsoft Azure Storage是一种基于云的存储，允许以持久且可扩展的方式存储数据。Flink支持管理存储在Microsoft Azure表存储上的数据。以下解释了我们如何做到这一点。
- en: 'First of all, we need to download the `azure-tables-hadoop` project from `git`
    and then compile it:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要从`git`下载`azure-tables-hadoop`项目，然后编译它：
- en: '[PRE70]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Next we add the following dependencies in `pom.xml`:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在`pom.xml`中添加以下依赖项：
- en: '[PRE71]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Next we write the following code to access Azure storage:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们编写以下代码来访问Azure存储：
- en: '[PRE72]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Now we are all set do any processing of the dataset.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好处理数据集了。
- en: MongoDB
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MongoDB
- en: Through open source contributions, developers have been able to connect Flink
    to MongoDB. In this section, we are going to talk about one such project.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 通过开源贡献，开发人员已经能够将Flink连接到MongoDB。在本节中，我们将讨论这样一个项目。
- en: 'The project is open source and can be downloaded from GitHub:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 该项目是开源的，可以从GitHub下载：
- en: '[PRE73]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Next we use the preceding connector in the Java program to connect to MongoDB:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们在Java程序中使用前面的连接器连接到MongoDB：
- en: '[PRE74]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Once the data is available as a dataset, we can easily do the desired transformation.
    We can also write back the data to the MongoDB collection as shown here:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据作为数据集可用，我们可以轻松进行所需的转换。我们还可以像这样将数据写回MongoDB集合：
- en: '[PRE75]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Iterations
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迭代
- en: One of the unique features Flink supports is iterations. These days a lot developers
    want to run iterative machine-learning and graph-processing algorithms using big
    data technologies. To cater to these needs, Flink supports running iterative data
    processing by defining a step function.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: Flink支持的一个独特功能是迭代。如今，许多开发人员希望使用大数据技术运行迭代的机器学习和图处理算法。为了满足这些需求，Flink支持通过定义步骤函数来运行迭代数据处理。
- en: Iterator operator
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 迭代器操作符
- en: 'An iterator operator consists of the following components:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代器操作符由以下组件组成：
- en: '![Iterator operator](img/image_03_002.jpg)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![迭代器操作符](img/image_03_002.jpg)'
- en: '**Iteration Input**: This is either the initial dataset received or the output
    of the previous iteration'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**迭代输入**：这是接收到的初始数据集或上一次迭代的输出'
- en: '**Step Function**: This is the function that needs to be applied on input dataset'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤函数**：这是需要应用于输入数据集的函数'
- en: '**Next Partial Solution**: This is the output of the step function which needs
    to be fed back to the next iteration'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**下一个部分解**：这是需要反馈到下一次迭代的步骤函数的输出'
- en: '**Iteration Result**:After all iterations are completed, we get the result
    of iterations'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**迭代结果**：在完成所有迭代后，我们得到迭代的结果'
- en: The number of iterations can be controlled by various ways. One way could be
    setting up the number of iterations to perform or we can also put conditional
    termination.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: Delta iterator
  id: totrans-299
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The delta operator iterates over the set of elements for incremental iteration
    operations. The main difference between that the delta iterator and regular iterator
    is, delta iterator works on updating the solution set rather than fully re-computing
    it every iteration.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: This leads to more efficient operations as it allows us to focus on the important
    parts of the solution in less time. The following diagram shows flow of delta
    iterator in Flink.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '![Delta iterator](img/image_03_003.jpg)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
- en: '**Iteration Input**: We have to read the work set and solution set for the
    delta iterator from some files'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Step Function**: Step function is the function that needs to be applied on
    the input dataset'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Next Work Set/ Update Solution**: Here after every iteration solution set
    it is updated with the latest results and the next work set is fed to the next
    iteration'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Iteration Result**: After all iterations are completed, we get the result
    of the iterations in the form of a solution set'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since delta iterators work on hot dataset itself, the performance and efficiency
    are great. Here is a detailed article that talks about using Flink iterations
    for the PageRank algorithm. [http://data-artisans.com/data-analysis-with-flink-a-case-study-and-tutorial/](http://data-artisans.com/data-analysis-with-flink-a-case-study-and-tutorial/).
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: Use case - Athletes data insights using Flink batch API
  id: totrans-308
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have learnt the details of DataSet API, let''s try to apply this
    knowledge to a real-life use case. Let''s say we have a dataset with us, which
    has information about the Olympics athletes and their performance in various games.
    The sample data looks like the following table:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '| **Player** | **Country** | **Year** | **Game** | **Gold** | **Silver** |
    **Bronze** | **Total** |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
- en: '| Yang Yilin | China | 2008 | Gymnastics | 1 | 0 | 2 | 3 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
- en: '| Leisel Jones | Australia | 2000 | Swimming | 0 | 2 | 0 | 2 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
- en: '| Go Gi-Hyeon | South Korea | 2002 | Short-Track Speed Skating | 1 | 1 | 0
    | 2 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
- en: '| Chen Ruolin | China | 2008 | Diving | 2 | 0 | 0 | 2 |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
- en: '| Katie Ledecky | United States | 2012 | Swimming | 1 | 0 | 0 | 1 |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
- en: '| Ruta Meilutyte | Lithuania | 2012 | Swimming | 1 | 0 | 0 | 1 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
- en: '| DAiniel Gyurta | Hungary | 2004 | Swimming | 0 | 1 | 0 | 1 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
- en: '| Arianna Fontana | Italy | 2006 | Short-Track Speed Skating | 0 | 0 | 1 |
    1 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
- en: '| Olga Glatskikh | Russia | 2004 | Rhythmic Gymnastics | 1 | 0 | 0 | 1 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
- en: '| Kharikleia Pantazi | Greece | 2000 | Rhythmic Gymnastics | 0 | 0 | 1 | 1
    |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
- en: '| Kim Martin | Sweden | 2002 | Ice Hockey | 0 | 0 | 1 | 1 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
- en: '| Kyla Ross | United States | 2012 | Gymnastics | 1 | 0 | 0 | 1 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
- en: '| Gabriela Dragoi | Romania | 2008 | Gymnastics | 0 | 0 | 1 | 1 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
- en: '| Tasha Schwikert-Warren | United States | 2000 | Gymnastics | 0 | 0 | 1 |
    1 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
- en: Now we want to get answers to the questions such as, How many players per country
    participated in the games? Or how many players participated for each game? As
    the data is at rest, we will be using Flink Batch API to analyze it.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: The data available is in the CSV format. So we will be using a CSV reader provided
    by Flink API as shown in the following code snippet.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Once the data is parse properly, it is easy to move ahead and use it as required.
    The following code snippet shows how to get information of no. of players per
    country:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'In the preceding code snippet, we are first creating datasets with the key
    as the player''s country and value as `1` and then we group it and sum up the
    values to get the total count. Once we execute the code, here is how the output
    looks:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Similarly we can apply the same logic to find the number of players per game
    as shown in the following code snippet:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'The output of the preceding code snippet would look as follows:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: This way you can run various other transformations to get the desired output.
    The complete code for this use case is available at [https://github.com/deshpandetanmay/mastering-flink/tree/master/chapter03/flink-batch](https://github.com/deshpandetanmay/mastering-flink/tree/master/chapter03/flink-batch).
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，您可以运行各种其他转换以获得所需的输出。此用例的完整代码可在[https://github.com/deshpandetanmay/mastering-flink/tree/master/chapter03/flink-batch](https://github.com/deshpandetanmay/mastering-flink/tree/master/chapter03/flink-batch)上找到。
- en: Summary
  id: totrans-337
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learnt about DataSet API. It enabled us to do the batch
    processing. We learnt various transformations in order to do data processing.
    Later we also explored the various file-based connectors to read/write data from
    HDFS, Amazon S3, MS Azure, Alluxio, and so on.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了DataSet API。它使我们能够进行批处理。我们学习了各种转换以进行数据处理。后来，我们还探索了各种基于文件的连接器，以从HDFS、Amazon
    S3、MS Azure、Alluxio等读取/写入数据。
- en: In the last section, we looked a use case where we applied the knowledge learnt
    in the earlier sections.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一节中，我们看了一个用例，在这个用例中，我们应用了在前几节中学到的知识。
- en: In the next chapter, we are going to learn another very important API from Flink's
    ecosystem point of view that is, Table API.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习另一个非常重要的API，即Table API，从Flink的生态系统角度来看。
