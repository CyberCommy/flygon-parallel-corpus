- en: Apache Spark - Machine Learning on Big Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far in this book we've talked about a lot of general data mining and machine
    learning techniques that you can use in your data science career, but they've
    all been running on your desktop. As such, you can only run as much data as a
    single machine can process using technologies such as Python and scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, everyone talks about big data, and odds are you might be working for a
    company that does in fact have big data to process. Big data meaning that you
    can''t actually control it all, you can''t actually wrangle it all on just one
    system. You need to actually compute it using the resources of an entire cloud,
    a cluster of computing resources. And that''s where Apache Spark comes in. Apache
    Spark is a very powerful tool for managing big data, and doing machine learning
    on large Datasets. By the end of the chapter, you will have an in-depth knowledge
    of the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Installing and working with Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resilient Distributed Datasets** (**RDDs**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **MLlib** (**Machine Learning Library**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision Trees in Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-Means Clustering in Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, I'm going to get you set up using Apache Spark, and show you
    some examples of actually using Apache Spark to solve some of the same problems
    that we solved using a single computer in the past in this book. The first thing
    we need to do is get Spark set up on your computer. So, we're going to walk you
    through how to do that in the next couple of sections. It's pretty straightforward
    stuff, but there are a few gotchas. So, don't just skip these sections; there
    are a few things you need to pay special attention to get Spark running successfully,
    especially on a Windows system. Let's get Apache Spark set up on your system,
    so you can actually dive in and start playing around with it.
  prefs: []
  type: TYPE_NORMAL
- en: We're going to be running this just on your own desktop for now. But, the same
    programs that we're going to write in this chapter could be run on an actual Hadoop
    cluster. So, you can take these scripts that we're writing and running locally
    on your desktop in Spark standalone mode, and actually run them from the master
    node of an actual Hadoop cluster, then let it scale up to the entire power of
    a Hadoop cluster and process massive Datasets that way. Even though we're going
    to set things up to run locally on your own computer, keep in mind that these
    same concepts will scale up to running on a cluster as well.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Spark on Windows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Getting Spark installed on Windows involves several steps that we''ll walk
    you through here. I''m just going to assume that you''re on Windows because most
    people use this book at home. We''ll talk a little bit about dealing with other
    operating systems in a moment. If you''re already familiar with installing stuff
    and dealing with environment variables on your computer, then you can just take
    the following little cheat sheet and go off and do it. If you''re not so familiar
    with Windows internals, I will walk you through it one step at a time in the upcoming
    sections. Here are the quick steps for those Windows pros:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Install a JDK**: You need to first install a JDK, that''s a Java Development
    Kit. You can just go to Sun''s website and download that and install it if you
    need to. We need the JDK because, even though we''re going to be developing in
    Python during this course, that gets translated under the hood to Scala code,
    which is what Spark is developed in natively. And, Scala, in turn, runs on top
    of the Java interpreter. So, in order to run Python code, you need a Scala system,
    which will be installed by default as part of Spark. Also, we need Java, or more
    specifically Java''s interpreter, to actually run that Scala code. It''s like
    a technology layer cake.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Install Python**: Obviously you''re going to need Python, but if you''ve
    gotten to this point in the book, you should already have a Python environment
    set up, hopefully with Enthought Canopy. So, we can skip this step.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Install a prebuilt version of Spark for Hadoop**: Fortunately, the Apache
    website makes available prebuilt versions of Spark that will just run out of the
    box that are precompiled for the latest Hadoop version. You don''t have to build
    anything, you can just download that to your computer and stick it in the right
    place and be good to go for the most part.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Create a conf/log4j.properties file**: We have a few configuration things
    to take care of. One thing we want to do is adjust our warning level so we don''t
    get a bunch of warning spam when we run our jobs. We''ll walk through how to do
    that. Basically, you need to rename one of the properties files, and then adjust
    the error setting within it.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Add a SPARK_HOME environment variable**: Next, we need to set up some environment
    variables to make sure that you can actually run Spark from any path that you
    might have. We''re going to add a SPARK_HOME environment variable pointing to
    where you installed Spark, and then we will add `%SPARK_HOME%\bin` to your system
    path, so that when you run Spark Submit, or PySpark or whatever Spark command
    you need, Windows will know where to find it.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Set a HADOOP_HOME variable**: On Windows there''s one more thing we need
    to do, we need to set a `HADOOP_HOME` variable as well because it''s going to
    expect to find one little bit of Hadoop, even if you''re not using Hadoop on your
    standalone system.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Install winutils.exe**: Finally, we need to install a file called `winutils.exe`.
    There''s a link to `winutils.exe` within the resources for this book, so you can
    get that there.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you want to walk through the steps in more detail, you can refer to the upcoming
    sections.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Spark on other operating systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A quick note on installing Spark on other operating systems: the same steps
    will basically apply on them too. The main difference is going to be in how you
    set environment variables on your system, in such a way that they will automatically
    be applied whenever you log in. That''s going to vary from OS to OS. macOS does
    it differently from various flavors of Linux, so you''re going to have to be at
    least a little bit familiar with using a Unix terminal command prompt, and how
    to manipulate your environment to do that. But most macOS or Linux users who are
    doing development already have those fundamentals under their belt. And of course,
    you''re not going to need `winutils.exe` if you''re not on Windows. So, those
    are the main differences for installing on different OSes.'
  prefs: []
  type: TYPE_NORMAL
- en: Installing the Java Development Kit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For installing the Java Development Kit, go back to the browser, open a new
    tab, and just search for `jdk` (short for Java Development Kit). This will bring
    you to the Oracle site, from where you can download Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dfda3f71-6e92-47de-8042-5809c986c13b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'On the Oracle website, click on JDK DOWNLOAD. Now, click on Accept License
    Agreement and then you can select the download option for your operating system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/874e8bfe-a3b6-413a-a33f-c477b3acb6f3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For me, that''s going to be Windows 64-bit, and a wait for 198 MB of goodness
    to download:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4fa3b0cf-a3f7-4032-89e9-81b98807069a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once the download is finished, locate the installer and start it running. Note
    that we can''t just accept the default settings in the installer on Windows here.
    So, this is a Windows-specific workaround, but as of the writing of this book,
    the current version of Spark is 2.1.1 and it turns out there''s an issue with
    Spark 2.1.1 with Java on Windows. The issue is that if you''ve installed Java
    to a path that has a space in it, it doesn''t work, so we need to make sure that
    Java is installed to a path that does not have a space in it. This means that
    you can''t skip this step even if you have Java installed already, so let me show
    you how to do that. On the installer, click on Next, and you will see, as in the
    following screen, that it wants to install by default to the `C:\Program Files\Java\jdk`
    path, whatever the version is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e23f5481-70e9-419a-a294-9d84965a3314.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The space in the `Program Files` path is going to cause trouble, so let''s
    click on the Change... button and install to `c:\jdk`, a nice simple path, easy
    to remember, and with no spaces in it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b84c94ed-6ede-43cb-82cb-fdee49f881b5.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, it also wants to install the Java Runtime environment, so just to be safe,
    I'm also going to install that to a path with no spaces.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the second step of the JDK installation, we should have this showing on
    our screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7023d123-557d-4ad3-afcc-126c9ca354e0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'I will change that destination folder as well, and we will make a new folder
    called `C:\jre` for that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/05f43aab-96ee-4ed9-acbe-e6c5323e0a8e.png)'
  prefs: []
  type: TYPE_IMG
- en: Alright, successfully installed. Woohoo!
  prefs: []
  type: TYPE_NORMAL
- en: Now, you'll need to remember the path that we installed the JDK into, which
    in our case was `C:\jdk`. We still have a few more steps to go here. Next, we
    need to install Spark itself.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s get back to a new browser tab here, head to [spark.apache.org](http://spark.apache.org),
    and click on the Download Spark button:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/86baa4bb-df53-4641-ae95-0e5de592efc2.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, we have used Spark 2.1.1 in this book, but anything beyond 2.0 should work
    just fine.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7b1ca113-5535-438e-88fd-dea58a24ef55.png)'
  prefs: []
  type: TYPE_IMG
- en: Make sure you get a prebuilt version, and select the Direct Download option
    so all these defaults are perfectly fine. Go ahead and click on the link next
    to instruction number 4 to download that package.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, it downloads a **TGZ** (**Tar in GZip**) file, which you might not be
    familiar with. Windows is kind of an afterthought with Spark quite honestly because
    on Windows, you''re not going to have a built-in utility for actually decompressing
    TGZ files. This means that you might need to install one, if you don''t have one
    already. The one I use is called WinRAR, and you can pick that up from [www.rarlab.com](http://www.rarlab.com).
    Go to the Downloads page if you need it, and download the installer for WinRAR
    32-bit or 64-bit, depending on your operating system. Install WinRAR as normal,
    and that will allow you to actually decompress TGZ files on Windows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a3e884d4-be31-4274-b858-9c7ed7e14ed6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'So, let''s go ahead and decompress the TGZ files. I''m going to open up my
    `Downloads` folder to find the Spark archive that we downloaded, and let''s go
    ahead and right-click on that archive and extract it to a folder of my choosing
    - I''m just going to put it in my `Downloads` folder for now. Again, WinRAR is
    doing this for me at this point:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/edaa679b-44dd-4811-bbed-52b39f6488bc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, I should now have a folder in my `Downloads` folder associated with that
    package. Let''s open that up and there is Spark itself. You should see something
    like the folder content shown below. So, you need to install that in some place
    that you can remember:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/417d9e6f-4b8c-4a51-bb1c-694e1cf23530.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You don''t want to leave it in your `Downloads` folder obviously, so let''s
    go ahead and open up a new file explorer window here. I go to my `C` drive and
    create a new folder, and let''s just call it `spark`. So, my Spark installation
    is going to live in `C:\spark`. Again, nice and easy to remember. Open that folder.
    Now, I go back to my downloaded `spark` folder and use *Ctrl* + *A* to select
    everything in the Spark distribution, *Ctrl* + *C* to copy it, and then go back
    to `C:\spark`, where I want to put it, and *Ctrl* + *V* to paste it in:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e5a2e48a-10c4-4849-9ef2-11833945eaa8.png)'
  prefs: []
  type: TYPE_IMG
- en: Remembering to paste the contents of the `spark` folder, not the `spark` folder
    itself is very important. So, what I should have now is my `C` drive with a `spark`
    folder that contains all of the files and folders from the Spark distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, there are still a few things we need to configure. So, while we''re in
    `C:\spark` let''s open up the `conf` folder, and in order to make sure that we
    don''t get spammed to death by log messages, we''re going to change the logging
    level setting here. So to do that, right-click on the `log4j.properties.template`
    file and select Rename:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/82bf6b9d-9ba1-40c3-b8e0-c60efbc92f77.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Delete the `.template` part of the filename to make it an actual `log4j.properties`
    file. Spark will use this to configure its logging:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9277e9dc-cb05-4d4c-95ef-415e6c0ed53d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, open this file in a text editor of some sort. On Windows, you might need
    to right-click there and select Open with and then WordPad. In the file, locate
    `log4j.rootCategory=INFO`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/36f4f644-c41d-4647-a05d-2c54a8151c23.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's change this to `log4j.rootCategory=ERROR` and this will just remove the
    clutter of all the log spam that gets printed out when we run stuff. Save the
    file, and exit your editor.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we installed Python, Java, and Spark. Now the next thing we need to
    do is to install something that will trick your PC into thinking that Hadoop exists,
    and again this step is only necessary on Windows. So, you can skip this step if
    you're on Mac or Linux.
  prefs: []
  type: TYPE_NORMAL
- en: 'I have a little file available that will do the trick. Let''s go to [http://media.sundog-soft.com/winutils.exe](http://media.sundog-soft.com/winutils.exe).
    Downloading `winutils.exe` will give you a copy of a little snippet of an executable,
    which can be used to trick Spark into thinking that you actually have Hadoop:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6257dac5-b9be-45d2-a9dc-58ad1ac9d705.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, since we''re going to be running our scripts locally on our desktop, it''s
    not a big deal, we don''t need to have Hadoop installed for real. This just gets
    around another quirk of running Spark on Windows. So, now that we have that, let''s
    find it in the `Downloads` folder, *Ctrl* + *C* to copy it, and let''s go to our
    `C` drive and create a place for it to live:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dccddda5-a8d8-47c0-b4ec-d9c636fe4513.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, create a new folder again in the root `C` drive, and we will call it `winutils`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ca38647b-576c-4e16-ba62-027fdf776511.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now let''s open this `winutils` folder and create a `bin` folder inside it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/240ed035-0eef-48dc-b8b7-a0ff9e7bbc98.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now in this `bin` folder, I want you to paste the `winutils.exe` file we downloaded.
    So you should have `C:\winutils\bin` and then `winutils.exe`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/329665c8-62c3-4959-88f5-6f7a64587d6c.png)'
  prefs: []
  type: TYPE_IMG
- en: This next step is only required on some systems, but just to be safe, open Command
    Prompt on Windows. You can do that by going to your Start menu and going down
    to Windows System, and then clicking on Command Prompt. Here, I want you to type
    `cd c:\winutils\bin`, which is where we stuck our `winutils.exe` file. Now if
    you type `dir`, you should see that file there. Now type `winutils.exe chmod 777
    \tmp\hive`. This just makes sure that all the file permissions you need to actually
    run Spark successfully are in place without any errors. You can close Command
    Prompt now that you're done with that step. Wow, we're almost done, believe it
    or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we need to set some environment variables for things to work. I''ll show
    you how to do that on Windows. On Windows 10, you''ll need to open up the Start
    menu and go to Windows System | Control Panel to open up Control Panel:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a2776899-1253-46d2-9de2-19bb4e45e982.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In Control Panel, click on System and Security:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a0db3138-5c95-463d-be21-087a8b379cfc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, click on System:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b19bf633-a93e-45f9-9614-dae791b66324.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then click on Advanced system settings from the list on the left-hand side:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a07b0a50-a59b-4438-9d8a-7d876d861c14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From here, click on Environment Variables...:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4ded259c-ba10-44cf-acdb-da295c9959c2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will get these options:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bbce5217-fcad-4153-a54c-c5ac2fc3a5c7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, this is a very Windows-specific way of setting environment variables.
    On other operating systems, you''ll use different processes, so you''ll have to
    look at how to install Spark on them. Here, we''re going to set up some new user
    variables. Click on the first New... button for a new user variable and call it
    `SPARK_HOME`, as shown below, all uppercase. This is going to point to where we
    installed Spark, which for us is `c:\spark`, so type that in as the Variable value
    and click on OK:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/99ecb254-bc78-45ff-9608-c74514df43f9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We also need to set up `JAVA_HOME`, so click on New... again and type in `JAVA_HOME`
    as Variable name. We need to point that to where we installed Java, which for
    us is `c:\jdk`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f84bdcce-2d85-4a17-888e-6e7020a1d3ed.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We also need to set up `HADOOP_HOME`, and that''s where we installed the `winutils`
    package, so we''ll point that to `c:\winutils`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/65b2e661-efd4-4c80-b6a6-d7ad02f60df2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So far, so good. The last thing we need to do is to modify our path. You should
    have a PATH environment variable here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/24ed1020-3dc3-4d51-b234-8dcb95deb08c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Click on the PATH environment variable, then on Edit..., and add a new path.
    This is going to be `%SPARK_HOME%\bin`, and I''m going to add another one, `%JAVA_HOME%\bin`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cdba2eb0-1f24-483c-a034-b7128e4e0e2d.png)'
  prefs: []
  type: TYPE_IMG
- en: Basically, this makes all the binary executables of Spark available to Windows,
    wherever you're running it from. Click on OK on this menu and on the previous
    two menus. We have finally everything set up.
  prefs: []
  type: TYPE_NORMAL
- en: Spark introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's get started with a high-level overview of Apache Spark and see what it's
    all about, what it's good for, and how it works.
  prefs: []
  type: TYPE_NORMAL
- en: What is Spark? Well, if you go to the Spark website, they give you a very high-level,
    hand-wavy answer, "A fast and general engine for large-scale data processing."
    It slices, it dices, it does your laundry. Well, not really. But it is a framework
    for writing jobs or scripts that can process very large amounts of data, and it
    manages distributing that processing across a cluster of computing for you. Basically,
    Spark works by letting you load your data into these large objects called Resilient
    Distributed Data stores, RDDs. It can automatically perform operations that transform
    and create actions based on those RDDs, which you can think of as large data frames.
  prefs: []
  type: TYPE_NORMAL
- en: The beauty of it is that Spark will automatically and optimally spread that
    processing out amongst an entire cluster of computers, if you have one available.
    You are no longer restricted to what you can do on a single machine or a single
    machine's memory. You can actually spread that out to all the processing capabilities
    and memory that's available to a cluster of machines, and, in this day and age,
    computing is pretty cheap. You can actually rent time on a cluster through things
    like Amazon's Elastic MapReduce service, and just rent some time on a whole cluster
    of computers for just a few dollars, and run your job that you couldn't run on
    your own desktop.
  prefs: []
  type: TYPE_NORMAL
- en: It's scalable
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How is Spark scalable? Well, let's get a little bit more specific here in how
    it all works.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3e61f40b-aae0-4ce4-a3bf-9ae6e2948d6c.png)'
  prefs: []
  type: TYPE_IMG
- en: The way it works is, you write a driver program, which is just a little script
    that looks just like any other Python script really, and it uses the Spark library
    to actually write your script with. Within that library, you define what's called
    a Spark Context, which is sort of the root object that you work within when you're
    developing in Spark.
  prefs: []
  type: TYPE_NORMAL
- en: From there, the Spark framework kind of takes over and distributes things for
    you. So if you're running in standalone mode on your own computer, like we're
    going to be doing in these upcoming sections, it all just stays there on your
    computer, obviously. However, if you are running on a cluster manager, Spark can
    figure that out and automatically take advantage of it. Spark actually has its
    own built-in cluster manager, you can actually use it on its own without even
    having Hadoop installed, but if you do have a Hadoop cluster available to you,
    it can use that as well.
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop is more than MapReduce; there's actually a component of Hadoop called
    YARN that separates out the entire cluster management piece of Hadoop. Spark can
    interface with YARN to actually use that to optimally distribute the components
    of your processing amongst the resources available to that Hadoop cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Within a cluster, you might have individual executor tasks that are running.
    These might be running on different computers, or they might be running on different
    cores of the same computer. They each have their own individual cache and their
    own individual tasks that they run. The driver program, the Spark Context and
    the cluster manager work together to coordinate all this effort and return the
    final result back to you.
  prefs: []
  type: TYPE_NORMAL
- en: The beauty of it is, all you have to do is write the initial little script,
    the driver program, which uses a Spark Context to describe at a high level the
    processing you want to do on this data. Spark, working together with the cluster
    manager that you're using, figures out how to spread that out and distribute it
    so you don't have to worry about all those details. Well, if it doesn't work,
    obviously, you might have to do some troubleshooting to figure out if you have
    enough resources available for the task at hand, but, in theory, it's all just
    magic.
  prefs: []
  type: TYPE_NORMAL
- en: It's fast
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What's the big deal about Spark? I mean, there are similar technologies like
    MapReduce that have been around longer. Spark is fast though, and on the website
    they claim that Spark is "up to 100x faster than MapReduce when running a job
    in memory, or 10 times faster on disk." Of course, the key words here are "up
    to," your mileage may vary. I don't think I've ever seen anything, actually, run
    that much faster than MapReduce. Some well-crafted MapReduce code can actually
    still be pretty darn efficient. But I will say that Spark does make a lot of common
    operations easier. MapReduce forces you to really break things down into mappers
    and reducers, whereas Spark is a little bit higher level. You don't have to always
    put as much thought into doing the right thing with Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Part of that leads to another reason why Spark is so fast. It has a DAG engine,
    a directed acyclic graph. Wow, that's another fancy word. What does it mean? The
    way Spark works is, you write a script that describes how to process your data,
    and you might have an RDD that's basically like a data frame. You might do some
    sort of transformation on it, or some sort of action on it. But nothing actually
    happens until you actually perform an action on that data. What happens at that
    point is, Spark will say "hmm, OK. So, this is the end result you want on this
    data. What are all the other things I had to do to get up this point, and what's
    the optimal way to lay out the strategy for getting to that point?" So, under
    the hood, it will figure out the best way to split up that processing, and distribute
    that information to get the end result that you're looking for. So, the key inside
    here, is that Spark waits until you tell it to actually produce a result, and
    only at that point does it actually go and figure out how to produce that result.
    So, it's kind of a cool concept there, and that's the key to a lot of its efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: It's young
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark is a very hot technology, and is relatively young, so it's still very
    much emerging and changing quickly, but a lot of big people are using it. Amazon,
    for example, has claimed they're using it, eBay, NASA's Jet Propulsional Laboratories,
    Groupon, TripAdvisor, Yahoo, and many, many others have too. I'm sure there's
    a lot of companies using it that don't confess up to it, but if you go to the
    Spark Apache Wiki page at [http://spark.apache.org/powered-by.html](http://spark.apache.org/powered-by.html).
  prefs: []
  type: TYPE_NORMAL
- en: There's actually a list you can look up of known big companies that are using
    Spark to solve real-world data problems. If you are worried that you're getting
    into the bleeding edge here, fear not, you're in very good company with some very
    big people that are using Spark in production for solving real problems. It is
    pretty stable stuff at this point.
  prefs: []
  type: TYPE_NORMAL
- en: It's not difficult
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It's also not that hard. You have your choice of programming in Python, Java,
    or Scala, and they're all built around the same concept that I just described
    earlier, that is, the Resilient Distributed Dataset, RDD for short. We'll talk
    about that in a lot more detail in the coming sections of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Components of Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark actually has many different components that it's built up of. So there
    is a Spark Core that lets you do pretty much anything you can dream up just using
    Spark Core functions alone, but there are these other things built on top of Spark
    that are also useful.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ed75debb-a33c-49d3-99f3-e69d183abf4f.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Spark Streaming**: Spark Streaming is a library that lets you actually process
    data in real time. Data can be flowing into a server continuously, say, from weblogs,
    and Spark Streaming can help you process that data in real time as you go, forever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spark SQL**: This lets you actually treat data as a SQL database, and actually
    issue SQL queries on it, which is kind of cool if you''re familiar with SQL already.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MLlib**: This is what we''re going to be focusing on in this section. It
    is actually a machine learning library that lets you perform common machine learning
    algorithms, with Spark underneath the hood to actually distribute that processing
    across a cluster. You can perform machine learning on much larger Datasets than
    you could have otherwise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GraphX**: This is not for making pretty charts and graphs. It refers to graph
    in the network theory sense. Think about a social network; that''s an example
    of a graph. GraphX just has a few functions that let you analyze the properties
    of a graph of information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python versus Scala for Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I do get some flack sometimes about using Python when I'm teaching people about
    Apache Spark, but there's a method to my madness. It is true that a lot of people
    use Scala when they're writing Spark code, because that's what Spark is developed
    in natively. So, you are incurring a little bit of overhead by forcing Spark to
    translate your Python code into Scala and then into Java interpreter commands
    at the end of the day.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, Python''s a lot easier, and you don''t need to compile things. Managing
    dependencies is also a lot easier. You can really focus your time on the algorithms
    and what you''re doing, and less on the minutiae of actually getting it built,
    and running, and compiling, and all that nonsense. Plus, obviously, this book
    has been focused on Python so far, and it makes sense to keep using what we''ve
    learned and stick with Python throughout these lectures. Here''s a quick summary
    of the pros and cons of the two languages:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Python** | **Scala** |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: No need to compile, manage dependencies, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Less coding overhead
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You already know Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lets us focus on the concepts instead of a new language
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Scala is probably a more popular choice with Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark is built in Scala, so coding in Scala is "native" to Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: New features, libraries tend to be Scala-first
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, I will say that if you were to do some Spark programming in the real
    world, there''s a good chance people are using Scala. Don''t worry about it too
    much, though, because in Spark the Python and Scala code ends up looking very
    similar because it''s all around the same RDD concept. The syntax is very slightly
    different, but it''s not that different. If you can figure out how to do Spark
    using Python, learning how to use it in Scala isn''t that big of a leap, really.
    Here''s a quick example of the same code in the two languages:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6a204753-e6f0-415f-a359-b8d9a68c5a8e.png)'
  prefs: []
  type: TYPE_IMG
- en: So, that's the basic concepts of Spark itself, why it's such a big deal, and
    how it's so powerful in letting you run machine learning algorithms on very large
    Datasets, or any algorithm really. Let's now talk in a little bit more detail
    about how it does that, and the core concept of the Resilient Distributed Dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Spark and Resilient Distributed Datasets (RDD)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's get a little bit deeper into how Spark works. We're going to talk about
    Resilient Distributed Datasets, known as RDDs. It's sort of the core that you
    use when programming in Spark, and we'll have a few code snippets to try to make
    it real. We're going to give you a crash course in Apache Spark here. There's
    a lot more depth to it than what we're going to cover in the next few sections,
    but I'm just going to give you the basics you need to actually understand what's
    going on in these examples, and hopefully get you started and pointed in the right
    direction.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, the most fundamental piece of Spark is called the Resilient Distributed
    Dataset, an RDD, and this is going to be the object that you use to actually load
    and transform and get the answers you want out of the data that you're trying
    to process. It's a very important thing to understand. The final letter in RDD
    stands for Dataset, and at the end of the day that's all it is; it's just a bunch
    of rows of information that can contain pretty much anything. But the key is the
    R and the first D.
  prefs: []
  type: TYPE_NORMAL
- en: '**Resilient**: It is resilient in that Spark makes sure that if you''re running
    this on a cluster and one of those clusters goes down, it can automatically recover
    from that and retry. Now, that resilience only goes so far, mind you. If you don''t
    have enough resources available to the job that you''re trying to run, it will
    still fail, and you will have to add more resources to it. There''s only so many
    things it can recover from; there is a limit to how many times it will retry a
    given task. But it does make its best effort to make sure that in the face of
    an unstable cluster or an unstable network it will still continue to try its best
    to run through to completion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distributed**: Obviously, it is distributed. The whole point of using Spark
    is that you can use it for big data problems where you can actually distribute
    the processing across the entire CPU and memory power of a cluster of computers.
    That can be distributed horizontally, so you can throw as many computers as you
    want to a given problem. The larger the problem, the more computers; there''s
    really no upper bound to what you can do there.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The SparkContext object
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You always start your Spark scripts by getting a SparkContext object, and this
    is the object that embodies the guts of Spark. It is what is going to give you
    your RDDs to process on, so it is what generates the objects that you use in your
    processing.
  prefs: []
  type: TYPE_NORMAL
- en: You know, you don't actually think about the SparkContext very much when you're
    actually writing Spark programs, but it is sort of the substrate that is running
    them for you under the hood. If you're running in the Spark shell interactively,
    it has an `sc` object already available for you that you can use to create RDDs.
    In a standalone script, however, you will have to create that SparkContext explicitly,
    and you'll have to pay attention to the parameters that you use because you can
    actually tell the Spark context how you want that to be distributed. Should I
    take advantage of every core that I have available to me? Should I be running
    on a cluster or just standalone on my local computer? So, that's where you set
    up the fundamental settings of how Spark will operate.
  prefs: []
  type: TYPE_NORMAL
- en: Creating RDDs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's look at some little code snippets of actually creating RDDs, and I think
    it will all start to make a little bit more sense.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an RDD using a Python list
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is a very simple example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If I just want to make an RDD out of a plain old Python list, I can call the
    `parallelize()` function in Spark. That will convert a list of stuff, in this
    case, just the numbers, 1, 2, 3, 4, into an RDD object called `nums`.
  prefs: []
  type: TYPE_NORMAL
- en: That is the simplest case of creating an RDD, just from a hard-coded list of
    stuff. That list could come from anywhere; it doesn't have to be hard-coded either,
    but that kind of defeats the purpose of big data. I mean, if I have to load the
    entire Dataset into memory before I can create an RDD from it, what's the point?
  prefs: []
  type: TYPE_NORMAL
- en: Loading an RDD from a text file
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I can also load an RDD from a text file, and that could be anywhere.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In this example, I have a giant text file that's the entire encyclopedia or
    something. I'm reading that from my local disk here, but I could also use s3n
    if I want to host this file on a distributed AmazonS3 bucket, or hdfs if I want
    to refer to data that's stored on a distributed HDFS cluster (that stands for
    Hadoop Distributed File System if you're not familiar with HDFS). When you're
    dealing with big data and working with a Hadoop cluster, usually that's where
    your data will live.
  prefs: []
  type: TYPE_NORMAL
- en: That line of code will actually convert every line of that text file into its
    own row in an RDD. So, you can think of the RDD as a database of rows, and, in
    this example, it will load up my text file into an RDD where every line, every
    row, contains one line of text. I can then do further processing in that RDD to
    parse or break out the delimiters in that data. But that's where I start from.
  prefs: []
  type: TYPE_NORMAL
- en: Remember when we talked about ETL and ELT earlier in the book? This is a good
    example of where you might actually be loading raw data into a system and doing
    the transform on the system itself that you used to query your data. You can take
    raw text files that haven't been processed at all and use the power of Spark to
    actually transform those into more structured data.
  prefs: []
  type: TYPE_NORMAL
- en: 'It can also talk to things like Hive, so if you have an existing Hive database
    set up at your company, you can create a Hive context that''s based on your Spark
    context. How cool is that? Take a look at this example code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You can actually create an RDD, in this case called rows, that's generated by
    actually executing a SQL query on your Hive database.
  prefs: []
  type: TYPE_NORMAL
- en: More ways to create RDDs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are more ways to create RDDs as well. You can create them from a JDBC
    connection. Basically any database that supports JDBC can also talk to Spark and
    have RDDs created from it. Cassandra, HBase, Elasticsearch, also files in JSON
    format, CSV format, sequence files object files, and a bunch of other compressed
    files like ORC can be used to create RDDs. I don't want to get into the details
    of all those, you can get a book and look those up if you need to, but the point
    is that it's very easy to create an RDD from data, wherever it might be, whether
    it's on a local filesystem or a distributed data store.
  prefs: []
  type: TYPE_NORMAL
- en: Again, RDD is just a way of loading and maintaining very large amounts of data
    and keeping track of it all at once. But, conceptually within your script, an
    RDD is just an object that contains a bunch of data. You don't have to think about
    the scale, because Spark does that for you.
  prefs: []
  type: TYPE_NORMAL
- en: RDD operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, there are two different types of classes of things you can do on RDDs once
    you have them, you can do transformations, and you can do actions.
  prefs: []
  type: TYPE_NORMAL
- en: Transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s talk about transformations first. Transformations are exactly what they
    sound like. It''s a way of taking an RDD and transforming every row in that RDD
    to a new value, based on a function you provide. Let''s look at some of those
    functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**map() and flatmap()**: `map` and `flatmap` are the functions you''ll see
    the most often. Both of these will take any function that you can dream up, that
    will take, as input, a row of an RDD, and it will output a transformed row. For
    example, you might take raw input from a CSV file, and your `map` operation might
    take that input and break it up into individual fields based on the comma delimiter,
    and return back a Python list that has that data in a more structured format that
    you can perform further processing on. You can chain map operations together,
    so the output of one `map` might end up creating a new RDD that you then do another
    transformation on, and so on, and so forth. Again, the key is, Spark can distribute
    those transformations across the cluster, so it might take part of your RDD and
    transform it on one machine, and another part of your RDD and transform it on
    another.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Like I said, `map` and `flatmap` are the most common transformations you'll
    see. The only difference is that `map` will only allow you to output one value
    for every row, whereas `flatmap` will let you actually output multiple new rows
    for a given row. So you can actually create a larger RDD or a smaller RDD than
    you started with using `flatmap.`
  prefs: []
  type: TYPE_NORMAL
- en: '**filter()**: `filter` can be used if what you want to do is just create a
    Boolean function that says "should this row be preserved or not? Yes or no."'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**distinct()**: `distinct` is a less commonly used transformation that will
    only return back distinct values within your RDD.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**sample()**: This function lets you take a random sample from your RDD'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**union(), intersection(), subtract() and Cartesian()**: You can perform intersection
    operations like union, intersection, subtract, or even produce every cartesian
    combination that exists within an RDD.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using map()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here''s a little example of how you might use the map function in your work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Let's say I created an RDD just from the list 1, 2, 3, 4\. I can then call `rdd.map()`
    with a lambda function of x that takes in each row, that is, each value of that
    RDD, calls it x, and then it applies the function x multiplied by x to square
    it. If I were to then collect the output of this RDD, it would be 1, 4, 9 and
    16, because it would take each individual entry of that RDD and square it, and
    put that into a new RDD.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you don''t remember what lambda functions are, we did talk about it a little
    bit earlier in this book, but as a refresher, the lambda function is just a shorthand
    for defining a function in line. So `rdd.map(lambda x: x*x)` is exactly the same
    thing as a separate function `def squareIt(x): return x*x`, and saying `rdd.map(squareIt)`.'
  prefs: []
  type: TYPE_NORMAL
- en: It's just a shorthand for very simple functions that you want to pass in as
    a transformation. It eliminates the need to actually declare this as a separate
    named function of its own. That's the whole idea of functional programming. So
    you can say you understand functional programming now, by the way! But really,
    it's just shorthand notation for defining a function inline as part of the parameters
    to a `map()` function, or any transformation for that matter.
  prefs: []
  type: TYPE_NORMAL
- en: Actions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can also perform actions on an RDD, when you want to actually get a result.
    Here are some examples of what you can do:'
  prefs: []
  type: TYPE_NORMAL
- en: '`collect()`: You can call collect() on an RDD, which will give you back a plain
    old Python object that you can then iterate through and print out the results,
    or save them to a file, or whatever you want to do.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`count()`: You can also call `count()`, which will force it to actually go
    count how many entries are in the RDD at this point.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`countByValue()`: This function will give you a breakdown of how many times
    each unique value within that RDD occurs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`take()`: You can also sample from the RDD using `take()`, which will take
    a random number of entries from the RDD.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`top()`: `top()` will give you the first few entries in that RDD if you just
    want to get a little peek into what''s in there for debugging purposes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reduce()`: The more powerful action is `reduce()` which will actually let
    you combine values together for the same common key value. You can also use RDDs
    in the context of key-value data. The `reduce()` function lets you define a way
    of combining together all the values for a given key. It is very much similar
    in spirit to MapReduce. `reduce()` is basically the analogous operation to a `reducer()`
    in MapReduce, and `map()` is analogous to a `mapper()`. So, it''s often very straightforward
    to actually take a MapReduce job and convert it to Spark by using these functions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remember, too, that nothing actually happens in Spark until you call an action.
    Once you call one of those action methods, that's when Spark goes out and does
    its magic with directed acyclic graphs, and actually computes the optimal way
    to get the answer you want. But remember, nothing really occurs until that action
    happens. So, that can sometimes trip you up when you're writing Spark scripts,
    because you might have a little print statement in there, and you might expect
    to get an answer, but it doesn't actually appear until the action is actually
    performed.
  prefs: []
  type: TYPE_NORMAL
- en: That is Spark 101 in a nutshell. Those are the basics you need for Spark programming.
    Basically, what is an RDD and what are the things you can do to an RDD. Once you
    get those concepts, then you can write some Spark code. Let's change tack now
    and talk about MLlib, and some specific features in Spark that let you do machine
    learning algorithms using Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing MLlib
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fortunately, you don't have to do things the hard way in Spark when you're doing
    machine learning. It has a built-in component called MLlib that lives on top of
    Spark Core, and this makes it very easy to perform complex machine learning algorithms
    using massive Datasets, and distributing that processing across an entire cluster
    of computers. So, very exciting stuff. Let's learn more about what it can do.
  prefs: []
  type: TYPE_NORMAL
- en: Some MLlib Capabilities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, what are some of the things MLlib can do? Well, one is feature extraction.
  prefs: []
  type: TYPE_NORMAL
- en: One thing you can do at scale is term frequency and inverse document frequency
    stuff, and that's useful for creating, for example, search indexes. We will actually
    go through an example of that later in the chapter. The key, again, is that it
    can do this across a cluster using massive Datasets, so you could make your own
    search engine for the web with this, potentially. It also offers basic statistics
    functions, chi-squared tests, Pearson or Spearman correlation, and some simpler
    things like min, max, mean, and variance. Those aren't terribly exciting in and
    of themselves, but what is exciting is that you can actually compute the variance
    or the mean or whatever, or the correlation score, across a massive Dataset, and
    it would actually break that Dataset up into various chunks and run that across
    an entire cluster if necessary.
  prefs: []
  type: TYPE_NORMAL
- en: So, even if some of these operations aren't terribly interesting, what's interesting
    about it is the scale at which it can operate at. It can also support things like
    linear regression and logistic regression, so if you need to fit a function to
    a massive set of data and use that for predictions, you can do that too. It also
    supports Support Vector Machines. We're getting into some of the more fancy algorithms
    here, some of the more advanced stuff, and that too can scale up to massive Datasets
    using Spark's MLlib. There is a Naive Bayes classifier built into MLlib, so, remember
    that spam classifier that we built earlier in the book? You could actually do
    that for an entire e-mail system using Spark, and scale that up as far as you
    want to.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees, one of my favorite things in machine learning, are also supported
    by Spark, and we'll actually have an example of that later in this chapter. We'll
    also look at K-Means clustering, and you can do clustering using K-Means and massive
    Datasets with Spark and MLlib. Even principal component analysis and **SVD** (**Singular
    Value Decomposition**) can be done with Spark as well, and we'll have an example
    of that too. And, finally, there's a built-in recommendations algorithm called
    Alternating Least Squares that's built into MLlib. Personally, I've had kind of
    mixed results with it, you know, it's a little bit too much of a black box for
    my taste, but I am a recommender system snob, so take that with a grain of salt!
  prefs: []
  type: TYPE_NORMAL
- en: Special MLlib data types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using MLlib is usually pretty straightforward, there are just some library functions
    you need to call. It does introduce a few new data types; however, that you need
    to know about, and one is the vector.
  prefs: []
  type: TYPE_NORMAL
- en: The vector data type
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Remember when we were doing movie similarities and movie recommendations earlier
    in the book? An example of a vector might be a list of all the movies that a given
    user rated. There are two types of vector, sparse and dense. Let's look at an
    example of those. There are many, many movies in the world, and a dense vector
    would actually represent data for every single movie, whether or not a user actually
    watched it. So, for example, let's say I have a user who watched Toy Story, obviously
    I would store their rating for Toy Story, but if they didn't watch the movie Star
    Wars, I would actually store the fact that there is not a number for Star Wars.
    So, we end up taking up space for all these missing data points with a dense vector.
    A sparse vector only stores the data that exists, so it doesn't waste any memory
    space on missing data, OK. So, it's a more compact form of representing a vector
    internally, but obviously that introduces some complexity while processing. So,
    it's a good way to save memory if you know that your vectors are going to have
    a lot of missing data in them.
  prefs: []
  type: TYPE_NORMAL
- en: LabeledPoint data type
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There's also a `LabeledPoint` data type that comes up, and that's just what
    it sounds like, a point that has some sort of label associated with it that conveys
    the meaning of this data in human readable terms.
  prefs: []
  type: TYPE_NORMAL
- en: Rating data type
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, there is a `Rating` data type that you'll encounter if you're using
    recommendations with MLlib. This data type can take in a rating that represents
    a 1-5 or 1-10, whatever star rating a person might have, and use that to inform
    product recommendations automatically.
  prefs: []
  type: TYPE_NORMAL
- en: So, I think you finally have everything you need to get started, let's dive
    in and actually look at some real MLlib code and run it, and then it will make
    a lot more sense.
  prefs: []
  type: TYPE_NORMAL
- en: Decision Trees in Spark with MLlib
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Alright, let''s actually build some decision trees using Spark and the MLlib
    library, this is very cool stuff. Wherever you put the course materials for this
    book, I want you to go to that folder now. Make sure you''re completely closed
    out of Canopy, or whatever environment you''re using for Python development, because
    I want to make sure you''re starting it from this directory, OK? And find the
    `SparkDecisionTree` script, and double-click that to open up Canopy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ad29a0a7-b494-4b34-9ab7-0430ffb3c225.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, up until this point we've been using IPython notebooks for our code, but
    you can't really use those very well with Spark. With Spark scripts, you need
    to actually submit them to the Spark infrastructure and run them in a very special
    way, and we'll see how that works shortly.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring decision trees code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, we are just looking at a raw Python script file now, without any of the
    usual embellishment of the IPython notebook stuff. let's walk through what's going
    on in the script.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d667f6b-c68c-489a-a94b-5582b37634f0.png)'
  prefs: []
  type: TYPE_IMG
- en: We'll go through it slowly, because this is your first Spark script that you've
    seen in this book.
  prefs: []
  type: TYPE_NORMAL
- en: First, we're going to import, from `pyspark.mllib`, the bits that we need from
    the machine learning library for Spark.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We need the `LabeledPoint` class, which is a data type required by the `DecisionTree`
    class, and the `DecisionTree` class itself, imported from `mllib.tree`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, pretty much every Spark script you see is going to include this line,
    where we import `SparkConf` and `SparkContext`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This is needed to create the `SparkContext` object that is kind of the root
    of everything you do in Spark.
  prefs: []
  type: TYPE_NORMAL
- en: 'And finally, we''re going to import the array library from `numpy`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Yes, you can still use `NumPy`, and `scikit-learn`, and whatever you want within
    Spark scripts. You just have to make sure, first of all, that these libraries
    are installed on every machine that you intend to run it on.
  prefs: []
  type: TYPE_NORMAL
- en: If you're running on a cluster, you need to make sure that those Python libraries
    are already in place somehow, and you also need to understand that Spark will
    not make the scikit-learn methods, for example, magically scalable. You can still
    call these functions in the context of a given map function, or something like
    that, but it's only going to run on that one machine within that one process.
    Don't lean on that stuff too heavily, but, for simple things like managing arrays,
    it's totally an okay thing to do.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the SparkContext
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we'll start by setting up our `SparkContext`, and giving it a `SparkConf`,
    a configuration.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This configuration object says, I'm going to set the master node to "`local`",
    and this means that I'm just running on my own local desktop, I'm not actually
    running on a cluster at all, and I'm just going to run in one process. I'm also
    going to give it an app name of "`SparkDecisionTree`," and you can call that whatever
    you want, Fred, Bob, Tim, whatever floats your boat. It's just what this job will
    appear as if you were to look at it in the Spark console later on.
  prefs: []
  type: TYPE_NORMAL
- en: 'And then we will create our `SparkContext` object using that configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: That gives us an `sc` object we can use for creating RDDs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we have a bunch of functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Let's just get down these functions for now, and we'll come back to them later.
  prefs: []
  type: TYPE_NORMAL
- en: Importing and cleaning our data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's go to the first bit of Python code that actually gets executed in this
    script.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ba509b70-e811-4149-b770-49de93e80b5b.png)'
  prefs: []
  type: TYPE_IMG
- en: The first thing we're going to do is load up this `PastHires.csv` file, and
    that's the same file we used in the decision tree exercise that we did earlier
    in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Let's pause quickly to remind ourselves of the content of that file. If you
    remember right, we have a bunch of attributes of job candidates, and we have a
    field of whether or not we hired those people. What we're trying to do is build
    up a decision tree that will predict - would we hire or not hire a person given
    those attributes?
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's take a quick peek at the `PastHires.csv`, which will be an Excel
    file.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/26da7937-34a5-4c2b-9f83-e42cb12a44ab.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see that Excel actually imported this into a table, but if you were
    to look at the raw text you'd see that it's made up of comma-separated values.
  prefs: []
  type: TYPE_NORMAL
- en: The first line is the actual headings of each column, so what we have above
    are the number of years of prior experience, is the candidate currently employed
    or not, number of previous employers, the level of education, whether they went
    to a top-tier school, whether they had an internship while they were in school,
    and finally, the target that we're trying to predict on, whether or not they got
    a job offer in the end of the day. Now, we need to read that information into
    an RDD so we can do something with it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go back to our script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The first thing we need to do is read that CSV data in, and we're going to throw
    away that first row, because that's our header information, remember. So, here's
    a little trick for doing that. We start off by importing every single line from
    that file into a raw data RDD, and I could call that anything I want, but we're
    calling it `sc.textFile`. SparkContext has a `textFile` function that will take
    a text file and create a new RDD, where each entry, each line of the RDD, consists
    of one line of input.
  prefs: []
  type: TYPE_NORMAL
- en: Make sure you change the path to that file to wherever you actually installed
    it, otherwise it won't work.
  prefs: []
  type: TYPE_NORMAL
- en: Now, I'm going to extract the first line, the first row from that RDD, by using
    the `first` function. So, now the header RDD will contain one entry that is just
    that row of column headers. And now, look what's going on in the above code, I'm
    using `filter` on my original data that contains all of the information in that
    CSV file, and I'm defining a `filter` function that will only let lines through
    if that line is not equal to the contents of that initial header row. What I've
    done here is, I've taken my raw CSV file and I've stripped out the first line
    by only allowing lines that do not equal that first line to survive, and I'm returning
    that back to the `rawData` RDD variable again. So, I'm taking `rawData`, filtering
    out that first line, and creating a new `rawData` that only contains the data
    itself. With me so far? It's not that complicated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we''re going to use a `map` function. What we need to do next is start
    to make more structure out of this information. Right now, every row of my RDD
    is just a line of text, it is comma-delimited text, but it''s still just a giant
    line of text, and I want to take that comma-separated value list and actually
    split it up into individual fields. At the end of the day, I want each RDD to
    be transformed from a line of text that has a bunch of information separated by
    commas into a Python list that has actual individual fields for each column of
    information that I have. So, that''s what this lambda function does:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: It calls the built-in Python function `split`, which will take a row of input,
    and split it on comma characters, and divide that into a list of every field delimited
    by commas.
  prefs: []
  type: TYPE_NORMAL
- en: The output of this map function, where I passed in a lambda function that just
    splits every line into fields based on commas, is a new RDD called `csvData`.
    And, at this point, `csvData` is an RDD that contains, on every row, a list where
    every element is a column from my source data. Now, we're getting close.
  prefs: []
  type: TYPE_NORMAL
- en: 'It turns out that in order to use a decision tree with MLlib, a couple of things
    need to be true. First of all, the input has to be in the form of LabeledPoint
    data types, and it all has to be numeric in nature. So, we need to transform all
    of our raw data into data that can actually be consumed by MLlib, and that''s
    what the `createLabeledPoints` function that we skipped past earlier does. We''ll
    get to that in just a second, first here''s the call to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We''re going to call a map on `csvData`, and we are going to pass it the `createLabeledPoints`
    function, which will transform every input row into something even closer to what
    we want at the end of the day. So, let''s look at what `createLabeledPoints` does:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'It takes in a list of fields, and just to remind you again what that looks
    like, let''s pull up that `.csv` Excel file again:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1a544f69-91c3-4053-9c95-790c728122e3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, at this point, every RDD entry has a field, it''s a Python list, where
    the first element is the years of experience, second element is employed, so on
    and so forth. The problems here are that we want to convert those lists to Labeled
    Points, and we want to convert everything to numerical data. So, all these yes
    and no answers need to be converted to ones and zeros. These levels of experience
    need to be converted from names of degrees to some numeric ordinal value. Maybe
    we''ll assign the value zero to no education, one can mean BS, two can mean MS,
    and three can mean PhD, for example. Again, all these yes/no values need to be
    converted to zeros and ones, because at the end of the day, everything going into
    our decision tree needs to be numeric, and that''s what `createLabeledPoints`
    does. Now, let''s go back to the code and run through it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'First, it takes in our list of `StringFields` ready to convert it into `LabeledPoints`,
    where the label is the target value-was this person hired or not? 0 or 1-followed
    by an array that consists of all the other fields that we care about. So, this
    is how you create a `LabeledPoint` that the `DecisionTree MLlib` class can consume.
    So, you see in the above code that we''re converting years of experience from
    a string to an integer value, and for all the yes/no fields, we''re calling this
    `binary` function, that I defined up at the top of the code, but we haven''t discussed
    yet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'All it does is convert the character yes to 1, otherwise it returns 0\. So,
    Y will become 1, N will become 0\. Similarly, I have a `mapEducation` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: As we discussed earlier, this simply converts different types of degrees to
    an ordinal numeric value in exactly the same way as our yes/no fields.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a reminder, this is the line of code that sent us running through those
    functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: At this point, after mapping our RDD using that `createLabeledPoints` function,
    we now have a `trainingData` RDD, and this is exactly what MLlib wants for constructing
    a decision tree.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a test candidate and building our decision tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s create a little test candidate we can use, so we can use our model to
    actually predict whether someone new would be hired or not. What we''re going
    to do is create a test candidate that consists of an array of the same values
    for each field as we had in the CSV file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s quickly compare that code with the Excel document so you can see the
    array mapping:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d0370539-6f0f-49a4-b9c0-5adc827ce00c.png)'
  prefs: []
  type: TYPE_IMG
- en: Again, we need to map these back to their original column representation, so
    that 10, 1, 3, 1, 0, 0 means 10 years of prior experience, currently employed,
    three previous employers, a BS degree, did not go to a top-tier school and did
    not do an internship. We could actually create an entire RDD full of candidates
    if we wanted to, but we'll just do one for now.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we''ll use parallelize to convert that list into an RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Nothing new there. Alright, now for the magic let''s move to the next code
    block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We are going to call `DecisionTree.trainClassifier`, and this is what will
    actually build our decision tree itself. We pass in our `trainingData`, which
    is just an RDD full of `LabeledPoint` arrays, `numClasses=2`, because we have,
    basically, a yes or no prediction that we''re trying to make, will this person
    be hired or not? The next parameter is called `categoricalFeaturesInfo`, and this
    is a Python dictionary that maps fields to the number of categories in each field.
    So, if you have a continuous range available to a given field, like the number
    of years of experience, you wouldn''t specify that at all in here, but for fields
    that are categorical in nature, such as what degree do they have, for example,
    that would say fieldID3, mapping to the degree attained, which has four different
    possibilities: no education, BS, MS, and PhD. For all of the yes/no fields, we''re
    mapping those to 2 possible categories, yes/no or 0/1 is what we converted those
    to.'
  prefs: []
  type: TYPE_NORMAL
- en: Continuing to move through our `DecisionTree.trainClassifier` call, we are going
    to use the `'gini'` impurity metric as we measure the entropy. We have a `maxDepth`
    of 5, which is just an upper boundary on how far we're going to go, that can be
    larger if you wish. Finally, `maxBins` is just a way to trade off computational
    expense if you can, so it just needs to at least be the maximum number of categories
    you have in each feature. Remember, nothing really happens until we call an action,
    so we're going to actually use this model to make a prediction for our test candidate.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use our `DecisionTree` model, which contains a decision tree that was trained
    on our test training data, and we tell that to make a prediction on our test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll get back a list of predictions that we can then iterate through. So,
    `predict` returns a plain old Python object and is an action that I can `collect`.
    Let me rephrase that a little bit: `collect` will return a Python object on our
    predictions, and then we can iterate through every item in that list and print
    the result of the prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also print out the decision tree itself by using `toDebugString`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: That will actually print out a little representation of the decision tree that
    it created internally, that you can follow through in your own head. So, that's
    kind of cool too.
  prefs: []
  type: TYPE_NORMAL
- en: Running the script
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Alright, feel free to take some time, stare at this script a little bit more,
    digest what's going on, but, if you're ready, let's move on and actually run this
    beast. So, to do so, you can't just run it directly from Canopy. We're going to
    go to the Tools menu and open up a Canopy Command Prompt, and this just opens
    up a Windows command prompt with all the necessary environment variables in place
    for running Python scripts in Canopy. Make sure that the working directory is
    the directory that you installed all of the course materials into.
  prefs: []
  type: TYPE_NORMAL
- en: All we need to do is call `spark-submit`, so this is a script that lets you
    run Spark scripts from Python, and then the name of the script, `SparkDecisionTree.py`.
    That's all I have to do.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Hit Return, and off it will go. Again, if I were doing this on a cluster and
    I created my `SparkConf` accordingly, this would actually get distributed to the
    entire cluster, but, for now, we''re just going to run it on my computer. When
    it''s finished, you should see the below output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5982c23d-99f3-4cb4-818a-6d9b5041176f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, in the above image, you can see in the test person that we put in above,
    we have a prediction that this person would be hired, and I''ve also printed out
    the decision tree itself, so it''s kind of cool. Now, let''s bring up that Excel
    document once more so we can compare it to the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ad3fb46a-ef3e-442a-91c1-1971c3616db4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can walk through this and see what it means. So, in our output decision
    tree we actually end up with a depth of four, with nine different nodes, and,
    again, if we remind ourselves what these different fields correlate to, the way
    to read this is: If (feature 1 in 0), so that means if the employed is No, then
    we drop down to feature 5\. This list is zero-based, so feature 5 in our Excel
    document is internships. We can run through the tree like that: this person is
    not currently employed, did not do an internship, has no prior years of experience
    and has a Bachelor''s degree, we would not hire this person. Then we get to the
    Else clauses. If that person had an advanced degree, we would hire them, just
    based on the data that we had that we trained it on. So, you can work out what
    these different feature IDs mean back to your original source data, remember,
    you always start counting at 0, and interpret that accordingly. Note that all
    the categorical features are expressed in Boolean in this list of possible categories
    that it saw, whereas continuous data is expressed numerically as less than or
    greater than relationships.'
  prefs: []
  type: TYPE_NORMAL
- en: And there you have it, an actual decision tree built using Spark and MLlib that
    actually works and makes sense. Pretty awesome stuff.
  prefs: []
  type: TYPE_NORMAL
- en: K-Means Clustering in Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Alright, let's look at another example of using Spark in MLlib, and this time
    we're going to look at k-means clustering, and just like we did with decision
    trees, we're going to take the same example that we did using scikit-learn and
    we're going to do it in Spark instead, so it can actually scale up to a massive
    Dataset. So, again, I've made sure to close out of everything else, and I'm going
    to go into my book materials and open up the `SparkKMeans` Python script, and
    let's study what's going on in.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/375a2436-ca5b-41bf-94d4-a80bedf814e1.png)'
  prefs: []
  type: TYPE_IMG
- en: Alright, so again, we begin with some boilerplate stuff.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: We're going to import the `KMeans` package from the clustering `MLlib` package,
    we're going to import array and random from `numpy`, because, again, we're free
    to use whatever you want, this is a Python script at the end of the day, and `MLlib`
    often does require `numpy` arrays as input. We're going to import the `sqrt` function
    and the usual boilerplate stuff, we need `SparkConf` and `SparkContext`, pretty
    much every time from `pyspark`. We're also going to import the scale function
    from `scikit-learn`. Again, it's OK to use `scikit-learn` as long as you make
    sure its installed in every machine that you're going to be running this job on,
    and also don't assume that `scikit-learn` will magically scale itself up just
    because you're running it on Spark. But, since I'm only using it for the scaling
    function, it's OK. Alright, let's go ahead and set things up.
  prefs: []
  type: TYPE_NORMAL
- en: 'I''m going to create a global variable first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'I''m going to run k-means clustering in this example with a K of 5, meaning
    with five different clusters. I''m then going to go ahead and set up a local `SparkConf`
    just running on my own desktop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: I'm going to set the name of my application to `SparkKMeans` and create a `SparkContext`
    object that I can then use to create RDDs that run on my local machine. We'll
    skip past the `createClusteredData` function for now, and go to the first line
    of code that gets run.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The first thing we're going to do is create an RDD by parallelizing in some
    fake data that I'm creating, and that's what the `createClusteredData` function
    does. Basically, I'm telling you to create 100 data points clustered around K
    centroids, and this is pretty much identical to the code that we looked at when
    we played with k-means clustering earlier in the book. If you want a refresher,
    go ahead and look back at that chapter. Basically, what we're going to do is create
    a bunch of random centroids around which we normally distribute some age and income
    data. So, what we're doing is trying to cluster people based on their age and
    income, and we are fabricating some data points to do that. That returns a `numpy`
    array of our fake data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once that result comes back from `createClusteredData`, I'm calling `scale`
    on it, and that will ensure that my ages and incomes are on comparable scales.
    Now, remember the section we studied saying you have to remember about data normalization?
    This is one of those examples where it is important, so we are normalizing that
    data with `scale` so that we get good results from k-means.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: And finally, we parallelize the resulting list of arrays into an RDD using `parallelize`.
    Now our data RDD contains all of our fake data. All we have to do, and this is
    even easier than a decision tree, is call `KMeans.train` on our training data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: We pass in the number of clusters we want, our K value, a parameter that puts
    an upper boundary on how much processing it's going to do; we then tell it to
    use the default initialization mode of k-means where we just randomly pick our
    initial centroids for our clusters before we start iterating on them, and back
    comes the model that we can use. We're going to call that `clusters`.
  prefs: []
  type: TYPE_NORMAL
- en: Alright, now we can play with that cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by printing out the cluster assignments for each one of our points.
    So, we''re going to take our original data and transform it using a lambda function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: This function is just going to transform each point into the cluster number
    that is predicted from our model. Again, we're just taking our RDD of data points.
    We're calling `clusters.predict` to figure out which cluster our k-means model
    is assigning them to, and we're just going to put the results in our `resultRDD`.
    Now, one thing I want to point out here is this cache call, in the above code.
  prefs: []
  type: TYPE_NORMAL
- en: An important thing when you're doing Spark is that any time you're going to
    call more than one action on an RDD, it's important to cache it first, because
    when you call an action on an RDD, Spark goes off and figures out the DAG for
    it, and how to optimally get to that result.
  prefs: []
  type: TYPE_NORMAL
- en: It will go off and actually execute everything to get that result. So, if I
    call two different actions on the same RDD, it will actually end up evaluating
    that RDD twice, and if you want to avoid all of that extra work, you can cache
    your RDD in order to make sure that it does not recompute it more than once.
  prefs: []
  type: TYPE_NORMAL
- en: 'By doing that, we make sure these two subsequent operations do the right thing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: In order to get an actual result, what we're going to do is use `countByValue`,
    and what that will do is give us back an RDD that has how many points are in each
    cluster. Remember, `resultRDD` currently has mapped every individual point to
    the cluster it ended up with, so now we can use `countByValue` to just count up
    how many values we see for each given cluster ID. We can then easily print that
    list out. And we can actually look at the raw results of that RDD as well, by
    calling `collect` on it, and that will give me back every single points cluster
    assignment, and we can print out all of them.
  prefs: []
  type: TYPE_NORMAL
- en: Within set sum of squared errors (WSSSE)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, how do we measure how good our clusters are? Well, one metric for that
    is called the Within Set Sum of Squared Errors, wow, that sounds fancy! It''s
    such a big term that we need an abbreviation for it, WSSSE. All it is, we look
    at the distance from each point to its centroid, the final centroid in each cluster,
    take the square of that error and sum it up for the entire Dataset. It''s just
    a measure of how far apart each point is from its centroid. Obviously, if there''s
    a lot of error in our model then they will tend to be far apart from the centroids
    that might apply, so for that we need a higher value of K, for example. We can
    go ahead and compute that value and print it out with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: First of all, we define this `error` function that computes the squared error
    for each point. It just takes the distance from the point to the centroid center
    of each cluster and sums it up. To do that, we're taking our source data, calling
    a lambda function on it that actually computes the error from each centroid center
    point, and then we can chain different operations together here.
  prefs: []
  type: TYPE_NORMAL
- en: First, we call `map` to compute the error for each point. Then to get a final
    total that represents the entire Dataset, we're calling `reduce` on that result.
    So, we're doing `data.map` to compute the error for each point, and then `reduce`
    to take all of those errors and add them all together. And that's what the little
    lambda function does. This is basically a fancy way of saying, "I want you to
    add up everything in this RDD into one final result." `reduce` will take the entire
    RDD, two things at a time, and combine them together using whatever function you
    provide. The function I'm providing it above is "take the two rows that I'm combining
    together and just add them up."
  prefs: []
  type: TYPE_NORMAL
- en: If we do that throughout every entry of the RDD, we end up with a final summed-up
    total. It might seem like a little bit of a convoluted way to just sum up a bunch
    of values, but by doing it this way we are able to make sure that we can actually
    distribute this operation if we need to. We could actually end up computing the
    sum of one piece of the data on one machine, and a sum of a different piece over
    on another machine, and then take those two sums and combine them together into
    a final result. This `reduce` function is saying, how do I take any two intermediate
    results from this operation, and combine them together?
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, feel free to take a moment and stare at this a little bit longer if
    you want it to sink in. Nothing really fancy going on here, but there are a few
    important points:'
  prefs: []
  type: TYPE_NORMAL
- en: We introduced the use of a cache if you want to make sure that you don't do
    unnecessary recomputations on an RDD that you're going to use more than once.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We introduced the use of the `reduce` function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have a couple of interesting mapper functions as well here, so there's a
    lot to learn from in this example.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the end of the day, it will just do k-means clustering, so let's go ahead
    and run it.
  prefs: []
  type: TYPE_NORMAL
- en: Running the code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Go to the Tools menu, Canopy Command Prompt, and type in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Hit Return, and off it will go. In this situation, you might have to wait a
    few moments for the output to appear in front of you, but you should see something
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eebf988f-bcec-4c78-ab13-d495aba80c3a.png)'
  prefs: []
  type: TYPE_IMG
- en: It worked, awesome! So remember, the output that we asked for was, first of
    all, a count of how many points ended up in each cluster. So, this is telling
    us that cluster 0 had 21 points in it, cluster 1 had 20 points in it, and so on
    and so forth. It ended up pretty evenly distributed, so that's a good sign.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we printed out the cluster assignments for each individual point, and,
    if you remember, the original data that fabricated this data did it sequentially,
    so it's actually a good thing that you see all of the 3s together, and all the
    1s together, and all the 4s together, it looks like it started to get a little
    bit confused with the 0s and 2s, but by and large, it seems to have done a pretty
    good job of uncovering the clusters that we created the data with originally.
  prefs: []
  type: TYPE_NORMAL
- en: And finally, we computed the WSSSE metric, it came out to 19.97 in this example.
    So, if you want to play around with this a little bit, I encourage you to do so.
    You can see what happens to that error metric as you increase or decrease the
    values of K, and think about why that may be. You can also experiment with what
    happens if you don't normalize all the data, does that actually affect your results
    in a meaningful way? Is that actually an important thing to do? And you can also
    experiment with the `maxIterations` parameter on the model itself and get a good
    feel of what that actually does to the final results, and how important it is.
    So, feel free to mess around with it and experiment away. That's k-means clustering
    done with MLlib and Spark in a scalable manner. Very cool stuff.
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, our final example of MLlib is going to be using something called Term Frequency
    Inverse Document Frequency, or TF-IDF, which is the fundamental building block
    of many search algorithms. As usual, it sounds complicated, but it's not as bad
    as it sounds.
  prefs: []
  type: TYPE_NORMAL
- en: So, first, let's talk about the concepts of TF-IDF, and how we might go about
    using that to solve a search problem. And what we're actually going to do with
    TF-IDF is create a rudimentary search engine for Wikipedia using Apache Spark
    in MLlib. How awesome is that? Let's get started.
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF stands for Term Frequency and Inverse Document Frequency, and these are
    basically two metrics that are closely interrelated for doing search and figuring
    out the relevancy of a given word to a document, given a larger body of documents.
    So, for example, every article on Wikipedia might have a term frequency associated
    with it, every page on the Internet could have a term frequency associated with
    it for every word that appears in that document. Sounds fancy, but, as you'll
    see, it's a fairly simple concept.
  prefs: []
  type: TYPE_NORMAL
- en: '**All Term Frequency** means is how often a given word occurs in a given document.
    So, within one web page, within one Wikipedia article, within one whatever, how
    common is a given word within that document? You know, what is the ratio of that
    word''s occurrence rate throughout all the words in that document? That''s it.
    That''s all term frequency is.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Document frequency**, is the same idea, but this time it is the frequency
    of that word across the entire corpus of documents. So, how often does this word
    occur throughout all of the documents that I have, all the web pages, all of the
    articles on Wikipedia, whatever. For example, common words like "a" or "the" would
    have a very high document frequency, and I would expect them to also have a very
    high term frequency, but that doesn''t necessarily mean they''re relevant to a
    given document.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can kind of see where we're going with this. So, let's say we have a very
    high term frequency and a very low document frequency for a given word. The ratio
    of these two things can give me a measure of the relevance of that word to the
    document. So, if I see a word that occurs very often in a given document, but
    not very often in the overall space of documents, then I know that this word probably
    conveys some special meaning to this particular document. It might convey what
    this document is actually about.
  prefs: []
  type: TYPE_NORMAL
- en: So, that's TF-IDF. It just stands for Term Frequency x Inverse Document Frequency,
    which is just a fancy way of saying term frequency over document frequency, which
    is just a fancy way of saying how often does this word occur in this document
    compared to how often it occurs in the entire body of documents? It's that simple.
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF in practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In practice, there are a few little nuances to how we use this. For example,
    we use the actual log of the inverse document frequency instead of the raw value,
    and that's because word frequencies in reality tend to be distributed exponentially.
    So, by taking the log, we end up with a slightly better weighting of words, given
    their overall popularity. There are some limitations to this approach, obviously,
    one is that we basically assume a document is nothing more than a bagful of words,
    we assume there are no relationships between the words themselves. And, obviously,
    that's not always the case, and actually parsing them out can be a good part of
    the work, because you have to deal with things like synonyms and various tenses
    of words, abbreviations, capitalizations, misspellings, and so on. This gets back
    to the idea of cleaning your data being a large part of your job as a data scientist,
    and it's especially true when you're dealing with natural language processing
    stuff. Fortunately, there are some libraries out there that can help you with
    this, but it is a real problem and it will affect the quality of your results.
  prefs: []
  type: TYPE_NORMAL
- en: Another implementation trick that we use with TF-IDF is, instead of storing
    actual string words with their term frequencies and inverse document frequency,
    to save space and make things more efficient, we actually map every word to a
    numerical value, a hash value we call it. The idea is that we have a function
    that can take any word, look at its letters, and assign that, in some fairly well-distributed
    manner, to a set of numbers in a range. That way, instead of using the word "represented",
    we might assign that a hash value of 10, and we can then refer to the word "represented"
    as "10" from now on. Now, if the space of your hash values isn't large enough,
    you could end up with different words being represented by the same number, which
    sounds worse than it is. But, you know, you want to make sure that you have a
    fairly large hash space so that is unlikely to happen. Those are called hash collisions.
    They can cause issues, but, in reality, there's only so many words that people
    commonly use in the English language. You can get away with 100,000 or so and
    be just fine.
  prefs: []
  type: TYPE_NORMAL
- en: Doing this at scale is the hard part. If you want to do this over all of Wikipedia,
    then you're going to have to run this on a cluster. But for the sake of argument,
    we are just going to run this on our own desktop for now, using a small sample
    of Wikipedia data.
  prefs: []
  type: TYPE_NORMAL
- en: Using TF- IDF
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How do we turn that into an actual search problem? Once we have TF-IDF, we have
    this measure of each word's relevancy to each document. What do we do with it?
    Well, one thing you could do is compute TF-IDF for every word that we encounter
    in the entire body of documents that we have, and then, let's say we want to search
    for a given term, a given word. Let's say we want to search for "what Wikipedia
    article in my set of Wikipedia articles is most relevant to Gettysburg?" I could
    sort all the documents by their TF-IDF score for Gettysburg, and just take the
    top results, and those are my search results for Gettysburg. That's it. Just take
    your search word, compute TF-IDF, take the top results. That's it.
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, in the real world there's a lot more to search than that. Google
    has armies of people working on this problem and it's way more complicated in
    practice, but this will actually give you a working search engine algorithm that
    produces reasonable results. Let's go ahead and dive in and see how it all works.
  prefs: []
  type: TYPE_NORMAL
- en: Searching wikipedia with Spark MLlib
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We're going to build an actual working search algorithm for a piece of Wikipedia
    using Apache Spark in MLlib, and we're going to do it all in less than 50 lines
    of code. This might be the coolest thing we do in this entire book!
  prefs: []
  type: TYPE_NORMAL
- en: 'Go into your course materials and open up the `TF-IDF.py` script, and that
    should open up Canopy with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/87797105-16f4-4546-bb34-a435f27ece1f.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, step back for a moment and let it sink in that we're actually creating
    a working search algorithm, along with a few examples of using it in less than
    50 lines of code here, and it's scalable. I could run this on a cluster. It's
    kind of amazing. Let's step through the code.
  prefs: []
  type: TYPE_NORMAL
- en: Import statements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We're going to start by importing the `SparkConf` and `SparkContext` libraries
    that we need for any Spark script that we run in Python, and then we're going
    to import `HashingTF` and `IDF` using the following commands.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: So, this is what computes the term frequencies (`TF`) and inverse document frequencies
    (`IDF`) within our documents.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the initial RDD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We'll start off with our boilerplate Spark stuff that creates a local `SparkConfiguration`
    and a `SparkContext`, from which we can then create our initial RDD.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Next, we're going to use our `SparkContext` to create an RDD from `subset-small.tsv`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: This is a file containing tab-separated values, and it represents a small sample
    of Wikipedia articles. Again, you'll need to change your path as shown in the
    preceding code as necessary for wherever you installed the course materials for
    this book.
  prefs: []
  type: TYPE_NORMAL
- en: That gives me back an RDD where every document is in each line of the RDD. The
    `tsv` file contains one entire Wikipedia document on every line, and I know that
    each one of those documents is split up into tabular fields that have various
    bits of metadata about each article.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next thing I''m going to do is split those up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: I'm going to split up each document based on their tab delimiters into a Python
    list, and create a new `fields` RDD that, instead of raw input data, now contains
    Python lists of each field in that input data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, I''m going to map that data, take in each list of fields, extract
    field number three `x[3]`, which I happen to know is the body of the article itself,
    the actual article text, and I''m in turn going to split that based on spaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: What `x[3]` does is extract the body of the text from each Wikipedia article,
    and split it up into a list of words. My new `documents` RDD has one entry for
    every document, and every entry in that RDD contains a list of words that appear
    in that document. Now, we actually know what to call these documents later on
    when we're evaluating the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'I''m also going to create a new RDD that stores the document names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: All that does is take that same `fields` RDD and uses this `map` function to
    extract the document name, which I happen to know is in field number one.
  prefs: []
  type: TYPE_NORMAL
- en: So, I now have two RDDs, `documents`, which contains lists of words that appear
    in each document, and `documentNames`, which contains the name of each document.
    I also know that these are in the same order, so I can actually combine these
    together later on to look up the name for a given document.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and transforming a HashingTF object
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, the magic happens. The first thing we''re going to do is create a `HashingTF`
    object, and we''re going to pass in a parameter of 100,000\. This means that I''m
    going to hash every word into one of 100,000 numerical values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Instead of representing words internally as strings, which is very inefficient,
    it's going to try to, as evenly as possible, distribute each word to a unique
    hash value. I'm giving it up to 100,000 hash values to choose from. Basically,
    this is mapping words to numbers at the end of the day.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, I''m going to call `transform` on `hashingTF` with my actual RDD of documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: That's going to take my list of words in every document and convert it to a
    list of hash values, a list of numbers that represent each word instead.
  prefs: []
  type: TYPE_NORMAL
- en: This is actually represented as a sparse vector at this point to save even more
    space. So, not only have we converted all of our words to numbers, but we've also
    stripped out any missing data. In the event that a word does not appear in a document
    where you're not storing the fact that word does not appear explicitly, it saves
    even more space.
  prefs: []
  type: TYPE_NORMAL
- en: Computing the TF-IDF score
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To actually compute the TF-IDF score for each word in each document, we first
    cache this `tf` RDD.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We do that because we''re going to use it more than once. Next, we use `IDF(minDocFreq=2)`,
    meaning that we''re going to ignore any word that doesn''t appear at least twice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We call `fit` on `tf`, and then in the next line we call `transform` on `tf`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: What we end up with here is an RDD of the TF-IDF score for each word in each
    document.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Wikipedia search engine algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s try and put the algorithm to use. Let''s try to look up the best article
    for the word **Gettysburg**. If you''re not familiar with US history, that''s
    where Abraham Lincoln gave a famous speech. So, we can transform the word Gettysburg
    into its hash value using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then extract the TF-IDF score for that hash value into a new RDD for
    each document:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: What this does is extract the TF-IDF score for Gettysburg, from the hash value
    it maps to for every document, and stores that in this `gettysburgRelevance` RDD.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then combine that with the `documentNames` so we can see the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can print out the answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Running the algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, let's go run that and see what happens. As usual, to run the Spark script,
    we're not going to just hit the play icon. We have to go to Tools>Canopy Command
    Prompt. In the Command Prompt that opens up, we will type in `spark-submit TF-IDF.py`,
    and off it goes.
  prefs: []
  type: TYPE_NORMAL
- en: We are asking it to chunk through quite a bit of data, even though it's a small
    sample of Wikipedia it's still a fair chunk of information, so it might take a
    while. Let's see what comes back for the best document match for Gettysburg, what
    document has the highest TF-IDF score?
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b7dfb9f6-8d3a-4af9-a230-6d0fccb02e54.png)'
  prefs: []
  type: TYPE_IMG
- en: It's Abraham Lincoln! Isn't that awesome? We just made an actual search engine
    that actually works, in just a few lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: And there you have it, an actual working search algorithm for a little piece
    of Wikipedia using Spark in MLlib and TF-IDF. And the beauty is we can actually
    scale that up to all of Wikipedia if we wanted to, if we had a cluster large enough
    to run it.
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully we got your interest up there in Spark, and you can see how it can
    be applied to solve what can be pretty complicated machine learning problems in
    a distributed manner. So, it's a very important tool, and I want to make sure
    you don't get through this book on data science without at least knowing the concepts
    of how Spark can be applied to big data problems. So, when you need to move beyond
    what one computer can do, remember, Spark is at your disposal.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Spark 2.0 DataFrame API for MLlib
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter was originally produced for Spark 1, so let's talk about what's
    new in Spark 2, and what new capabilities exist in MLlib now.
  prefs: []
  type: TYPE_NORMAL
- en: So, the main thing with Spark 2 is that they moved more and more toward Dataframes
    and Datasets. Datasets and Dataframes are kind of used interchangeably sometimes.
    Technically a dataframe is a Dataset of row objects, they're kind of like RDDs,
    but the only difference is that, whereas an RDD just contains unstructured data,
    a Dataset has a defined schema to it.
  prefs: []
  type: TYPE_NORMAL
- en: A Dataset knows ahead of time exactly what columns of information exists in
    each row, and what types those are. Because it knows about the actual structure
    of that Dataset ahead of time, it can optimize things more efficiently. It also
    lets us think of the contents of this Dataset as a little, mini database, well,
    actually, a very big database if it's on a cluster. That means we can do things
    like issue SQL queries on it.
  prefs: []
  type: TYPE_NORMAL
- en: This creates a higher-level API with which we can query and analyze massive
    Datasets on a Spark cluster. It's pretty cool stuff. It's faster, it has more
    opportunities for optimization, and it has a higher-level API that's often easier
    to work with.
  prefs: []
  type: TYPE_NORMAL
- en: How Spark 2.0 MLlib works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Going forward in Spark 2.0, MLlib is pushing dataframes as its primary API.
    This is the way of the future, so let''s take a look at how it works. I''ve gone
    ahead and opened up the `SparkLinearRegression.py` file in Canopy, as shown in
    the following figure, so let''s walk through it a little bit:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/99003c6b-bbd0-4c0a-84c5-9c12af88be01.png)'
  prefs: []
  type: TYPE_IMG
- en: As you see, for one thing, we're using `ml` instead of `MLlib`, and that's because
    the new dataframe-based API is in there.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, what we're going to do is implement linear regression, and
    linear regression is just a way of fitting a line to a set of data. What we're
    going to do in this exercise is take a bunch of fabricated data that we have in
    two dimensions, and try to fit a line to it with a linear model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''re going to separate our data into two sets, one for building the model
    and one for evaluating the model, and we''ll compare how well this linear model
    does at actually predicting real values. First of all, in Spark 2, if you''re
    going to be doing stuff with the `SparkSQL` interface and using Datasets, you''ve
    got to be using a `SparkSession` object instead of a `SparkContext`. To set one
    up, you do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the middle bit is only necessary on Windows and in Spark 2.0\. It
    kind of works around a little bug that they have, to be honest. So, if you''re
    on Windows, make sure you have a `C:/temp` folder. If you want to run this, go
    create that now if you need to. If you''re not on Windows, you can delete that
    whole middle section to leave: `spark = SparkSession.builder.appName("LinearRegression").getOrCreate()`.'
  prefs: []
  type: TYPE_NORMAL
- en: Okay, so you can say `spark`, give it an `appName` and `getOrCreate()`.
  prefs: []
  type: TYPE_NORMAL
- en: This is interesting, because once you've created a Spark session, if it terminates
    unexpectedly, you can actually recover from that the next time that you run it.
    So, if we have a checkpoint directory, it can actually restart where it left off
    using `getOrCreate`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we''re going to use this `regression.txt` file that I have included with
    the course materials:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: That is just a text file that has comma-delimited values of two columns, and
    they're just two columns of, more or less randomly, linearly correlated data.
    It can represent whatever you want. Let's imagine that it represents heights and
    weights, for example. So, the first column might represent heights, the second
    column might represent weights.
  prefs: []
  type: TYPE_NORMAL
- en: In the lingo of machine learning, we talk about labels and features, where labels
    are usually the thing that you're trying to predict, and features are a set of
    known attributes of the data that you use to make a prediction from.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, maybe heights are the labels and the features are the weights.
    Maybe we're trying to predict heights based on your weight. It can be anything,
    it doesn't matter. This is all normalized down to data between -1 and 1\. There's
    no real meaning to the scale of the data anywhere, you can pretend it means anything
    you want, really.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use this with MLlib, we need to transform our data into the format it expects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: The first thing we're going to do is split that data up with this `map` function
    that just splits each line into two distinct values in a list, and then we're
    going to map that to the format that MLlib expects. That's going to be a floating
    point label, and then a dense vector of the feature data.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we only have one bit of feature data, the weight, so we have a
    vector that just has one thing in it, but even if it's just one thing, the MLlib
    linear regression model requires a dense vector there. This is like a `labeledPoint`
    in the older API, but we have to do it the hard way here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to actually assign names to those columns. Here''s the syntax
    for doing that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: We're going to tell MLlib that these two columns in the resulting RDD actually
    correspond to the label and the features, and then I can convert that RDD to a
    DataFrame object. At this point, I have an actual dataframe or, if you will, a
    Dataset that contains two columns, label and features, where the label is a floating
    point height, and the features column is a dense vector of floating point weights.
    That is the format required by MLlib, and MLlib can be pretty picky about this
    stuff, so it's important that you pay attention to these formats.
  prefs: []
  type: TYPE_NORMAL
- en: Now, like I said, we're going to split our data in half.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: We're going to do a 50/50 split between training data and test data. This returns
    back two dataframes, one that I'm going to use to actually create my model, and
    one that I'm going to use to evaluate my model.
  prefs: []
  type: TYPE_NORMAL
- en: I will next create my actual linear regression model with a few standard parameters
    here that I've set.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'We''re going to call `lir = LinearRegression`, and then I will fit that model
    to the set of data that I held aside for training, the training data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: That gives me back a model that I can use to make predictions from.
  prefs: []
  type: TYPE_NORMAL
- en: Let's go ahead and do that.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: I will call `model.transform(testDF)`, and what that's going to do is predict
    the heights based on the weights in my testing Dataset. I actually have the known
    labels, the actual, correct heights, and this is going to add a new column to
    that dataframe called predictions, that has the predicted values based on that
    linear model.
  prefs: []
  type: TYPE_NORMAL
- en: 'I''m going to cache those results, and now I can just extract them and compare
    them together. So, let''s pull out the prediction column, just using `select`
    like you would in SQL, and then I''m going to actually transform that dataframe
    and pull out the RDD from it, and use that to map it to just a plain old RDD full
    of floating point heights in this case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'These are the predicted heights. Next, we''re going to get the actual heights
    from the label column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can zip them back together and just print them out side by side
    and see how well it does:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: This is kind of a convoluted way of doing it; I did this to be more consistent
    with the previous example, but a simpler approach would be to just actually select
    prediction and label together into a single RDD that maps out those two columns
    together and then I don't have to zip them up, but either way it works. You'll
    also note that right at the end there we need to stop the Spark session.
  prefs: []
  type: TYPE_NORMAL
- en: So let's see if it works. Let's go up to Tools, Canopy Command Prompt, and we'll
    type in `spark-submit SparkLinearRegression.py` and let's see what happens.
  prefs: []
  type: TYPE_NORMAL
- en: There's a little bit more upfront time to actually run these APIs with Datasets,
    but once they get going, they're very fast. Alright, there you have it.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/93ec3caf-319d-47fa-8712-266370ee6bcb.png)'
  prefs: []
  type: TYPE_IMG
- en: Here we have our actual and predicted values side by side, and you can see that
    they're not too bad. They tend to be more or less in the same ballpark. There
    you have it, a linear regression model in action using Spark 2.0, using the new
    dataframe-based API for MLlib. More and more, you'll be using these APIs going
    forward with MLlib in Spark, so make sure you opt for these when you can. Alright,
    that's MLlib in Spark, a way of actually distributing massive computing tasks
    across an entire cluster for doing machine learning on big Datasets. So, good
    skill to have. Let's move on.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we started with installing Spark, then moved to introducing
    Spark in depth while understanding how Spark works in combination with RDDs. We
    also walked through various ways of creating RDDs while exploring different operations.
    We then introduced MLlib, and stepped through some detailed examples of decision
    trees and K-Means Clustering in Spark. We then pulled off our masterstroke of
    creating a search engine in just a few lines of code using TF-IDF. Finally, we
    looked at the new features of Spark 2.0.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll take a look at A/B testing and experimental design.
  prefs: []
  type: TYPE_NORMAL
