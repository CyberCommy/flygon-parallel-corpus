- en: Apache Spark - Machine Learning on Big Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark - 大数据上的机器学习
- en: So far in this book we've talked about a lot of general data mining and machine
    learning techniques that you can use in your data science career, but they've
    all been running on your desktop. As such, you can only run as much data as a
    single machine can process using technologies such as Python and scikit-learn.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在这本书中，我们已经讨论了许多通用的数据挖掘和机器学习技术，你可以在数据科学职业中使用，但它们都在你的桌面上运行。因此，你只能使用诸如Python和scikit-learn等技术来处理单台机器可以处理的数据量。
- en: 'Now, everyone talks about big data, and odds are you might be working for a
    company that does in fact have big data to process. Big data meaning that you
    can''t actually control it all, you can''t actually wrangle it all on just one
    system. You need to actually compute it using the resources of an entire cloud,
    a cluster of computing resources. And that''s where Apache Spark comes in. Apache
    Spark is a very powerful tool for managing big data, and doing machine learning
    on large Datasets. By the end of the chapter, you will have an in-depth knowledge
    of the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，每个人都在谈论大数据，很可能你正在为一家实际上有大数据需要处理的公司工作。大数据意味着你实际上无法控制所有数据，你无法在一个系统上处理所有数据。你需要使用整个云、一组计算资源的集群来计算它。这就是Apache
    Spark的用武之地。Apache Spark是一个非常强大的工具，用于管理大数据，并在大规模数据集上进行机器学习。到本章结束时，你将对以下主题有深入的了解：
- en: Installing and working with Spark
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装和使用Spark
- en: '**Resilient Distributed Datasets** (**RDDs**)'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**弹性分布式数据集**（**RDDs**）'
- en: The **MLlib** (**Machine Learning Library**)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MLlib**（**机器学习库**）'
- en: Decision Trees in Spark
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark中的决策树
- en: K-Means Clustering in Spark
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark中的K均值聚类
- en: Installing Spark
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Spark
- en: In this section, I'm going to get you set up using Apache Spark, and show you
    some examples of actually using Apache Spark to solve some of the same problems
    that we solved using a single computer in the past in this book. The first thing
    we need to do is get Spark set up on your computer. So, we're going to walk you
    through how to do that in the next couple of sections. It's pretty straightforward
    stuff, but there are a few gotchas. So, don't just skip these sections; there
    are a few things you need to pay special attention to get Spark running successfully,
    especially on a Windows system. Let's get Apache Spark set up on your system,
    so you can actually dive in and start playing around with it.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我将帮助你使用Apache Spark，并向你展示一些实际使用Apache Spark解决与本书中过去在单台计算机上解决的相同问题的示例。我们需要做的第一件事是在你的计算机上设置Spark。因此，我们将在接下来的几节中为你介绍如何做到这一点。这是相当简单的事情，但有一些需要特别注意的地方。所以，不要只是跳过这些部分；有一些东西你需要特别注意，才能成功地运行Spark，尤其是在Windows系统上。让我们在你的系统上设置Apache
    Spark，这样你就可以真正地投入其中并开始尝试一些东西。
- en: We're going to be running this just on your own desktop for now. But, the same
    programs that we're going to write in this chapter could be run on an actual Hadoop
    cluster. So, you can take these scripts that we're writing and running locally
    on your desktop in Spark standalone mode, and actually run them from the master
    node of an actual Hadoop cluster, then let it scale up to the entire power of
    a Hadoop cluster and process massive Datasets that way. Even though we're going
    to set things up to run locally on your own computer, keep in mind that these
    same concepts will scale up to running on a cluster as well.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将在你自己的桌面上运行这个。但是，我们在本章中要编写的相同程序可以在实际的Hadoop集群上运行。因此，你可以将我们正在编写并在Spark独立模式下在你的桌面上运行的这些脚本，实际上从实际的Hadoop集群的主节点上运行它们，然后让它扩展到整个Hadoop集群的强大处理大规模数据集的能力。即使我们要在你自己的计算机上本地运行这些东西，也要记住这些相同的概念也可以扩展到在集群上运行。
- en: Installing Spark on Windows
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Windows上安装Spark
- en: 'Getting Spark installed on Windows involves several steps that we''ll walk
    you through here. I''m just going to assume that you''re on Windows because most
    people use this book at home. We''ll talk a little bit about dealing with other
    operating systems in a moment. If you''re already familiar with installing stuff
    and dealing with environment variables on your computer, then you can just take
    the following little cheat sheet and go off and do it. If you''re not so familiar
    with Windows internals, I will walk you through it one step at a time in the upcoming
    sections. Here are the quick steps for those Windows pros:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在Windows上安装Spark涉及几个步骤，我们将在这里为你逐步介绍。我假设你在Windows上，因为大多数人在家里使用这本书。我们稍后会谈一下如何处理其他操作系统。如果你已经熟悉在计算机上安装东西和处理环境变量，那么你可以使用以下简短的提示表并开始操作。如果你对Windows内部不太熟悉，我将在接下来的几节中逐步为你介绍。以下是那些Windows专家的快速步骤：
- en: '**Install a JDK**: You need to first install a JDK, that''s a Java Development
    Kit. You can just go to Sun''s website and download that and install it if you
    need to. We need the JDK because, even though we''re going to be developing in
    Python during this course, that gets translated under the hood to Scala code,
    which is what Spark is developed in natively. And, Scala, in turn, runs on top
    of the Java interpreter. So, in order to run Python code, you need a Scala system,
    which will be installed by default as part of Spark. Also, we need Java, or more
    specifically Java''s interpreter, to actually run that Scala code. It''s like
    a technology layer cake.'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**安装JDK**：你需要首先安装JDK，即Java开发工具包。如果需要，你可以直接去Sun的网站下载并安装。我们需要JDK，因为即使在这门课程中我们将使用Python进行开发，但在底层，它会被转换为Scala代码，而Spark就是用Scala原生开发的。而Scala又是在Java解释器之上运行的。因此，为了运行Python代码，你需要一个Scala系统，这将作为Spark的一部分默认安装。此外，我们需要Java，或者更具体地说，需要Java的解释器来实际运行那些Scala代码。就像是一个技术层的蛋糕。'
- en: '**Install Python**: Obviously you''re going to need Python, but if you''ve
    gotten to this point in the book, you should already have a Python environment
    set up, hopefully with Enthought Canopy. So, we can skip this step.'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**安装Python**：显然，你需要Python，但如果你已经阅读到这本书的这一部分，你应该已经设置好了Python环境，希望是Enthought
    Canopy。所以，我们可以跳过这一步。'
- en: '**Install a prebuilt version of Spark for Hadoop**: Fortunately, the Apache
    website makes available prebuilt versions of Spark that will just run out of the
    box that are precompiled for the latest Hadoop version. You don''t have to build
    anything, you can just download that to your computer and stick it in the right
    place and be good to go for the most part.'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Create a conf/log4j.properties file**: We have a few configuration things
    to take care of. One thing we want to do is adjust our warning level so we don''t
    get a bunch of warning spam when we run our jobs. We''ll walk through how to do
    that. Basically, you need to rename one of the properties files, and then adjust
    the error setting within it.'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Add a SPARK_HOME environment variable**: Next, we need to set up some environment
    variables to make sure that you can actually run Spark from any path that you
    might have. We''re going to add a SPARK_HOME environment variable pointing to
    where you installed Spark, and then we will add `%SPARK_HOME%\bin` to your system
    path, so that when you run Spark Submit, or PySpark or whatever Spark command
    you need, Windows will know where to find it.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Set a HADOOP_HOME variable**: On Windows there''s one more thing we need
    to do, we need to set a `HADOOP_HOME` variable as well because it''s going to
    expect to find one little bit of Hadoop, even if you''re not using Hadoop on your
    standalone system.'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Install winutils.exe**: Finally, we need to install a file called `winutils.exe`.
    There''s a link to `winutils.exe` within the resources for this book, so you can
    get that there.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you want to walk through the steps in more detail, you can refer to the upcoming
    sections.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Installing Spark on other operating systems
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A quick note on installing Spark on other operating systems: the same steps
    will basically apply on them too. The main difference is going to be in how you
    set environment variables on your system, in such a way that they will automatically
    be applied whenever you log in. That''s going to vary from OS to OS. macOS does
    it differently from various flavors of Linux, so you''re going to have to be at
    least a little bit familiar with using a Unix terminal command prompt, and how
    to manipulate your environment to do that. But most macOS or Linux users who are
    doing development already have those fundamentals under their belt. And of course,
    you''re not going to need `winutils.exe` if you''re not on Windows. So, those
    are the main differences for installing on different OSes.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Installing the Java Development Kit
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For installing the Java Development Kit, go back to the browser, open a new
    tab, and just search for `jdk` (short for Java Development Kit). This will bring
    you to the Oracle site, from where you can download Java:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dfda3f71-6e92-47de-8042-5809c986c13b.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
- en: 'On the Oracle website, click on JDK DOWNLOAD. Now, click on Accept License
    Agreement and then you can select the download option for your operating system:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/874e8bfe-a3b6-413a-a33f-c477b3acb6f3.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
- en: 'For me, that''s going to be Windows 64-bit, and a wait for 198 MB of goodness
    to download:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4fa3b0cf-a3f7-4032-89e9-81b98807069a.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
- en: 'Once the download is finished, locate the installer and start it running. Note
    that we can''t just accept the default settings in the installer on Windows here.
    So, this is a Windows-specific workaround, but as of the writing of this book,
    the current version of Spark is 2.1.1 and it turns out there''s an issue with
    Spark 2.1.1 with Java on Windows. The issue is that if you''ve installed Java
    to a path that has a space in it, it doesn''t work, so we need to make sure that
    Java is installed to a path that does not have a space in it. This means that
    you can''t skip this step even if you have Java installed already, so let me show
    you how to do that. On the installer, click on Next, and you will see, as in the
    following screen, that it wants to install by default to the `C:\Program Files\Java\jdk`
    path, whatever the version is:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e23f5481-70e9-419a-a294-9d84965a3314.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
- en: 'The space in the `Program Files` path is going to cause trouble, so let''s
    click on the Change... button and install to `c:\jdk`, a nice simple path, easy
    to remember, and with no spaces in it:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b84c94ed-6ede-43cb-82cb-fdee49f881b5.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
- en: Now, it also wants to install the Java Runtime environment, so just to be safe,
    I'm also going to install that to a path with no spaces.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: 'At the second step of the JDK installation, we should have this showing on
    our screen:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7023d123-557d-4ad3-afcc-126c9ca354e0.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
- en: 'I will change that destination folder as well, and we will make a new folder
    called `C:\jre` for that:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/05f43aab-96ee-4ed9-acbe-e6c5323e0a8e.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
- en: Alright, successfully installed. Woohoo!
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Now, you'll need to remember the path that we installed the JDK into, which
    in our case was `C:\jdk`. We still have a few more steps to go here. Next, we
    need to install Spark itself.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Installing Spark
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s get back to a new browser tab here, head to [spark.apache.org](http://spark.apache.org),
    and click on the Download Spark button:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/86baa4bb-df53-4641-ae95-0e5de592efc2.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
- en: Now, we have used Spark 2.1.1 in this book, but anything beyond 2.0 should work
    just fine.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7b1ca113-5535-438e-88fd-dea58a24ef55.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
- en: Make sure you get a prebuilt version, and select the Direct Download option
    so all these defaults are perfectly fine. Go ahead and click on the link next
    to instruction number 4 to download that package.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, it downloads a **TGZ** (**Tar in GZip**) file, which you might not be
    familiar with. Windows is kind of an afterthought with Spark quite honestly because
    on Windows, you''re not going to have a built-in utility for actually decompressing
    TGZ files. This means that you might need to install one, if you don''t have one
    already. The one I use is called WinRAR, and you can pick that up from [www.rarlab.com](http://www.rarlab.com).
    Go to the Downloads page if you need it, and download the installer for WinRAR
    32-bit or 64-bit, depending on your operating system. Install WinRAR as normal,
    and that will allow you to actually decompress TGZ files on Windows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a3e884d4-be31-4274-b858-9c7ed7e14ed6.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
- en: 'So, let''s go ahead and decompress the TGZ files. I''m going to open up my
    `Downloads` folder to find the Spark archive that we downloaded, and let''s go
    ahead and right-click on that archive and extract it to a folder of my choosing
    - I''m just going to put it in my `Downloads` folder for now. Again, WinRAR is
    doing this for me at this point:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/edaa679b-44dd-4811-bbed-52b39f6488bc.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: 'So, I should now have a folder in my `Downloads` folder associated with that
    package. Let''s open that up and there is Spark itself. You should see something
    like the folder content shown below. So, you need to install that in some place
    that you can remember:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/417d9e6f-4b8c-4a51-bb1c-694e1cf23530.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
- en: 'You don''t want to leave it in your `Downloads` folder obviously, so let''s
    go ahead and open up a new file explorer window here. I go to my `C` drive and
    create a new folder, and let''s just call it `spark`. So, my Spark installation
    is going to live in `C:\spark`. Again, nice and easy to remember. Open that folder.
    Now, I go back to my downloaded `spark` folder and use *Ctrl* + *A* to select
    everything in the Spark distribution, *Ctrl* + *C* to copy it, and then go back
    to `C:\spark`, where I want to put it, and *Ctrl* + *V* to paste it in:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e5a2e48a-10c4-4849-9ef2-11833945eaa8.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
- en: Remembering to paste the contents of the `spark` folder, not the `spark` folder
    itself is very important. So, what I should have now is my `C` drive with a `spark`
    folder that contains all of the files and folders from the Spark distribution.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, there are still a few things we need to configure. So, while we''re in
    `C:\spark` let''s open up the `conf` folder, and in order to make sure that we
    don''t get spammed to death by log messages, we''re going to change the logging
    level setting here. So to do that, right-click on the `log4j.properties.template`
    file and select Rename:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，还有一些东西我们需要配置。所以，当我们在`C:\spark`中时，让我们打开`conf`文件夹，为了确保我们不会被日志消息淹没，我们将在这里更改日志级别设置。因此，右键单击`log4j.properties.template`文件，然后选择重命名：
- en: '![](img/82bf6b9d-9ba1-40c3-b8e0-c60efbc92f77.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/82bf6b9d-9ba1-40c3-b8e0-c60efbc92f77.png)'
- en: 'Delete the `.template` part of the filename to make it an actual `log4j.properties`
    file. Spark will use this to configure its logging:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 删除文件名中的`.template`部分，使其成为一个真正的`log4j.properties`文件。Spark将使用这个文件来配置它的日志记录：
- en: '![](img/9277e9dc-cb05-4d4c-95ef-415e6c0ed53d.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9277e9dc-cb05-4d4c-95ef-415e6c0ed53d.png)'
- en: 'Now, open this file in a text editor of some sort. On Windows, you might need
    to right-click there and select Open with and then WordPad. In the file, locate
    `log4j.rootCategory=INFO`:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，用某种文本编辑器打开这个文件。在Windows上，你可能需要右键单击，然后选择“打开方式”，然后选择“WordPad”。在文件中，找到`log4j.rootCategory=INFO`：
- en: '![](img/36f4f644-c41d-4647-a05d-2c54a8151c23.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/36f4f644-c41d-4647-a05d-2c54a8151c23.png)'
- en: Let's change this to `log4j.rootCategory=ERROR` and this will just remove the
    clutter of all the log spam that gets printed out when we run stuff. Save the
    file, and exit your editor.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把这个改成`log4j.rootCategory=ERROR`，这样就可以消除运行时打印出的所有日志垃圾。保存文件，然后退出编辑器。
- en: So far, we installed Python, Java, and Spark. Now the next thing we need to
    do is to install something that will trick your PC into thinking that Hadoop exists,
    and again this step is only necessary on Windows. So, you can skip this step if
    you're on Mac or Linux.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们安装了Python、Java和Spark。现在我们需要做的下一件事是安装一些东西，让你的电脑认为Hadoop是存在的，这一步在Windows上是必要的。所以，如果你在Mac或Linux上，可以跳过这一步。
- en: 'I have a little file available that will do the trick. Let''s go to [http://media.sundog-soft.com/winutils.exe](http://media.sundog-soft.com/winutils.exe).
    Downloading `winutils.exe` will give you a copy of a little snippet of an executable,
    which can be used to trick Spark into thinking that you actually have Hadoop:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我有一个小文件可以解决问题。让我们去[http://media.sundog-soft.com/winutils.exe](http://media.sundog-soft.com/winutils.exe)。下载`winutils.exe`将给你一个可执行文件的一小部分副本，可以用来欺骗Spark，让它认为你实际上安装了Hadoop：
- en: '![](img/6257dac5-b9be-45d2-a9dc-58ad1ac9d705.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6257dac5-b9be-45d2-a9dc-58ad1ac9d705.png)'
- en: 'Now, since we''re going to be running our scripts locally on our desktop, it''s
    not a big deal, we don''t need to have Hadoop installed for real. This just gets
    around another quirk of running Spark on Windows. So, now that we have that, let''s
    find it in the `Downloads` folder, *Ctrl* + *C* to copy it, and let''s go to our
    `C` drive and create a place for it to live:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，因为我们将在我们的桌面上本地运行我们的脚本，这并不是什么大不了的事，我们不需要真正安装Hadoop。这只是绕过在Windows上运行Spark的另一个怪癖。所以，现在我们有了这个，让我们在“下载”文件夹中找到它，*Ctrl*
    + *C*复制它，然后让我们去我们的`C`驱动器，为它创建一个位置。
- en: '![](img/dccddda5-a8d8-47c0-b4ec-d9c636fe4513.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dccddda5-a8d8-47c0-b4ec-d9c636fe4513.png)'
- en: 'So, create a new folder again in the root `C` drive, and we will call it `winutils`:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，在`C`驱动器的根目录中再次创建一个新文件夹，我们将称之为`winutils`：
- en: '![](img/ca38647b-576c-4e16-ba62-027fdf776511.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ca38647b-576c-4e16-ba62-027fdf776511.png)'
- en: 'Now let''s open this `winutils` folder and create a `bin` folder inside it:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们打开这个`winutils`文件夹，并在其中创建一个`bin`文件夹：
- en: '![](img/240ed035-0eef-48dc-b8b7-a0ff9e7bbc98.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/240ed035-0eef-48dc-b8b7-a0ff9e7bbc98.png)'
- en: 'Now in this `bin` folder, I want you to paste the `winutils.exe` file we downloaded.
    So you should have `C:\winutils\bin` and then `winutils.exe`:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在在这个`bin`文件夹中，我希望你把我们下载的`winutils.exe`文件粘贴进去。所以你应该有`C:\winutils\bin`，然后`winutils.exe`：
- en: '![](img/329665c8-62c3-4959-88f5-6f7a64587d6c.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/329665c8-62c3-4959-88f5-6f7a64587d6c.png)'
- en: This next step is only required on some systems, but just to be safe, open Command
    Prompt on Windows. You can do that by going to your Start menu and going down
    to Windows System, and then clicking on Command Prompt. Here, I want you to type
    `cd c:\winutils\bin`, which is where we stuck our `winutils.exe` file. Now if
    you type `dir`, you should see that file there. Now type `winutils.exe chmod 777
    \tmp\hive`. This just makes sure that all the file permissions you need to actually
    run Spark successfully are in place without any errors. You can close Command
    Prompt now that you're done with that step. Wow, we're almost done, believe it
    or not.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这个下一步只在一些系统上需要，但为了安全起见，在Windows上打开命令提示符。你可以通过转到开始菜单，然后转到Windows系统，然后点击命令提示符来做到这一点。在这里，我希望你输入`cd
    c:\winutils\bin`，这是我们放置`winutils.exe`文件的地方。现在如果你输入`dir`，你应该会看到那个文件。现在输入`winutils.exe
    chmod 777 \tmp\hive`。这只是确保你需要成功运行Spark的所有文件权限都已经放置好，没有任何错误。现在你可以关闭命令提示符了，因为你已经完成了这一步。哇，我们几乎完成了，信不信由你。
- en: 'Now we need to set some environment variables for things to work. I''ll show
    you how to do that on Windows. On Windows 10, you''ll need to open up the Start
    menu and go to Windows System | Control Panel to open up Control Panel:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要设置一些环境变量才能让事情正常运行。我将向你展示如何在Windows上做到这一点。在Windows 10上，你需要打开开始菜单，然后转到Windows系统
    | 控制面板来打开控制面板：
- en: '![](img/a2776899-1253-46d2-9de2-19bb4e45e982.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a2776899-1253-46d2-9de2-19bb4e45e982.png)'
- en: 'In Control Panel, click on System and Security:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在控制面板中，点击系统和安全：
- en: '![](img/a0db3138-5c95-463d-be21-087a8b379cfc.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a0db3138-5c95-463d-be21-087a8b379cfc.png)'
- en: 'Then, click on System:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，点击系统：
- en: '![](img/b19bf633-a93e-45f9-9614-dae791b66324.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b19bf633-a93e-45f9-9614-dae791b66324.png)'
- en: 'Then click on Advanced system settings from the list on the left-hand side:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 然后从左侧的列表中点击高级系统设置：
- en: '![](img/a07b0a50-a59b-4438-9d8a-7d876d861c14.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a07b0a50-a59b-4438-9d8a-7d876d861c14.png)'
- en: 'From here, click on Environment Variables...:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，点击环境变量...：
- en: '![](img/4ded259c-ba10-44cf-acdb-da295c9959c2.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4ded259c-ba10-44cf-acdb-da295c9959c2.png)'
- en: 'We will get these options:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将得到这些选项：
- en: '![](img/bbce5217-fcad-4153-a54c-c5ac2fc3a5c7.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bbce5217-fcad-4153-a54c-c5ac2fc3a5c7.png)'
- en: 'Now, this is a very Windows-specific way of setting environment variables.
    On other operating systems, you''ll use different processes, so you''ll have to
    look at how to install Spark on them. Here, we''re going to set up some new user
    variables. Click on the first New... button for a new user variable and call it
    `SPARK_HOME`, as shown below, all uppercase. This is going to point to where we
    installed Spark, which for us is `c:\spark`, so type that in as the Variable value
    and click on OK:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/99ecb254-bc78-45ff-9608-c74514df43f9.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
- en: 'We also need to set up `JAVA_HOME`, so click on New... again and type in `JAVA_HOME`
    as Variable name. We need to point that to where we installed Java, which for
    us is `c:\jdk`:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f84bdcce-2d85-4a17-888e-6e7020a1d3ed.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
- en: 'We also need to set up `HADOOP_HOME`, and that''s where we installed the `winutils`
    package, so we''ll point that to `c:\winutils`:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/65b2e661-efd4-4c80-b6a6-d7ad02f60df2.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
- en: 'So far, so good. The last thing we need to do is to modify our path. You should
    have a PATH environment variable here:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/24ed1020-3dc3-4d51-b234-8dcb95deb08c.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
- en: 'Click on the PATH environment variable, then on Edit..., and add a new path.
    This is going to be `%SPARK_HOME%\bin`, and I''m going to add another one, `%JAVA_HOME%\bin`:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cdba2eb0-1f24-483c-a034-b7128e4e0e2d.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
- en: Basically, this makes all the binary executables of Spark available to Windows,
    wherever you're running it from. Click on OK on this menu and on the previous
    two menus. We have finally everything set up.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Spark introduction
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's get started with a high-level overview of Apache Spark and see what it's
    all about, what it's good for, and how it works.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: What is Spark? Well, if you go to the Spark website, they give you a very high-level,
    hand-wavy answer, "A fast and general engine for large-scale data processing."
    It slices, it dices, it does your laundry. Well, not really. But it is a framework
    for writing jobs or scripts that can process very large amounts of data, and it
    manages distributing that processing across a cluster of computing for you. Basically,
    Spark works by letting you load your data into these large objects called Resilient
    Distributed Data stores, RDDs. It can automatically perform operations that transform
    and create actions based on those RDDs, which you can think of as large data frames.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: The beauty of it is that Spark will automatically and optimally spread that
    processing out amongst an entire cluster of computers, if you have one available.
    You are no longer restricted to what you can do on a single machine or a single
    machine's memory. You can actually spread that out to all the processing capabilities
    and memory that's available to a cluster of machines, and, in this day and age,
    computing is pretty cheap. You can actually rent time on a cluster through things
    like Amazon's Elastic MapReduce service, and just rent some time on a whole cluster
    of computers for just a few dollars, and run your job that you couldn't run on
    your own desktop.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: It's scalable
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How is Spark scalable? Well, let's get a little bit more specific here in how
    it all works.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3e61f40b-aae0-4ce4-a3bf-9ae6e2948d6c.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
- en: The way it works is, you write a driver program, which is just a little script
    that looks just like any other Python script really, and it uses the Spark library
    to actually write your script with. Within that library, you define what's called
    a Spark Context, which is sort of the root object that you work within when you're
    developing in Spark.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: From there, the Spark framework kind of takes over and distributes things for
    you. So if you're running in standalone mode on your own computer, like we're
    going to be doing in these upcoming sections, it all just stays there on your
    computer, obviously. However, if you are running on a cluster manager, Spark can
    figure that out and automatically take advantage of it. Spark actually has its
    own built-in cluster manager, you can actually use it on its own without even
    having Hadoop installed, but if you do have a Hadoop cluster available to you,
    it can use that as well.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 从那里开始，Spark框架会接管并为您分配任务。因此，如果您在自己的计算机上以独立模式运行，就像我们将在接下来的部分中进行的那样，所有任务都会留在您的计算机上。然而，如果您在集群管理器上运行，Spark可以识别并自动利用它。Spark实际上有自己内置的集群管理器，您甚至可以在没有安装Hadoop的情况下单独使用它，但如果您有可用的Hadoop集群，它也可以使用。
- en: Hadoop is more than MapReduce; there's actually a component of Hadoop called
    YARN that separates out the entire cluster management piece of Hadoop. Spark can
    interface with YARN to actually use that to optimally distribute the components
    of your processing amongst the resources available to that Hadoop cluster.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop不仅仅是MapReduce；实际上，Hadoop有一个名为YARN的组件，它将Hadoop的整个集群管理部分分离出来。Spark可以与YARN接口，实际上使用它来在Hadoop集群中有效地分配处理组件的资源。
- en: Within a cluster, you might have individual executor tasks that are running.
    These might be running on different computers, or they might be running on different
    cores of the same computer. They each have their own individual cache and their
    own individual tasks that they run. The driver program, the Spark Context and
    the cluster manager work together to coordinate all this effort and return the
    final result back to you.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在集群中，您可能有正在运行的个别执行器任务。这些可能在不同的计算机上运行，也可能在同一台计算机的不同核心上运行。它们各自有自己的缓存和自己的任务。驱动程序、Spark
    Context和集群管理器共同协调所有这些工作，并将最终结果返回给您。
- en: The beauty of it is, all you have to do is write the initial little script,
    the driver program, which uses a Spark Context to describe at a high level the
    processing you want to do on this data. Spark, working together with the cluster
    manager that you're using, figures out how to spread that out and distribute it
    so you don't have to worry about all those details. Well, if it doesn't work,
    obviously, you might have to do some troubleshooting to figure out if you have
    enough resources available for the task at hand, but, in theory, it's all just
    magic.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 它的美妙之处在于，您只需要编写最初的小脚本，即驱动程序，它使用Spark Context在高层次上描述您想要对这些数据进行的处理。Spark与您使用的集群管理器一起工作，找出如何分散和分发，因此您不必担心所有这些细节。当然，如果不起作用，显然，您可能需要进行一些故障排除，以找出您手头的任务是否有足够的资源可用，但理论上，这都只是魔术。
- en: It's fast
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它很快
- en: What's the big deal about Spark? I mean, there are similar technologies like
    MapReduce that have been around longer. Spark is fast though, and on the website
    they claim that Spark is "up to 100x faster than MapReduce when running a job
    in memory, or 10 times faster on disk." Of course, the key words here are "up
    to," your mileage may vary. I don't think I've ever seen anything, actually, run
    that much faster than MapReduce. Some well-crafted MapReduce code can actually
    still be pretty darn efficient. But I will say that Spark does make a lot of common
    operations easier. MapReduce forces you to really break things down into mappers
    and reducers, whereas Spark is a little bit higher level. You don't have to always
    put as much thought into doing the right thing with Spark.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Spark有什么了不起的？我的意思是，有类似的技术，比如MapReduce已经存在很长时间了。不过，Spark很快，网站上声称Spark在内存中运行作业时比MapReduce快100倍，或者在磁盘上快10倍。当然，这里的关键词是“最多”，您的情况可能有所不同。我从来没有见过任何东西实际上比MapReduce快那么多。一些精心设计的MapReduce代码实际上仍然可以非常高效。但我会说，Spark确实使许多常见操作更容易。MapReduce迫使您真正将事情分解为映射器和减速器，而Spark则更高级一些。您不必总是那么费心地使用Spark做正确的事情。
- en: Part of that leads to another reason why Spark is so fast. It has a DAG engine,
    a directed acyclic graph. Wow, that's another fancy word. What does it mean? The
    way Spark works is, you write a script that describes how to process your data,
    and you might have an RDD that's basically like a data frame. You might do some
    sort of transformation on it, or some sort of action on it. But nothing actually
    happens until you actually perform an action on that data. What happens at that
    point is, Spark will say "hmm, OK. So, this is the end result you want on this
    data. What are all the other things I had to do to get up this point, and what's
    the optimal way to lay out the strategy for getting to that point?" So, under
    the hood, it will figure out the best way to split up that processing, and distribute
    that information to get the end result that you're looking for. So, the key inside
    here, is that Spark waits until you tell it to actually produce a result, and
    only at that point does it actually go and figure out how to produce that result.
    So, it's kind of a cool concept there, and that's the key to a lot of its efficiency.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分原因之一是Spark为何如此快的原因。它有一个DAG引擎，即有向无环图。哇，这是另一个花哨的词。这是什么意思？Spark的工作方式是，您编写一个描述如何处理数据的脚本，您可能有一个RDD，基本上就像一个数据框架。您可能对其进行某种转换或某种操作。但直到您对该数据执行某种操作之前，实际上什么都不会发生。在那一点上发生的是，Spark会说“嗯，好吧。所以，这是您在这些数据上想要的最终结果。我为了达到这一点必须做的所有其他事情是什么，以及达到这一点的最佳策略是什么？”因此，在幕后，它将找出最佳的方式来分割处理，并分发信息以获得您所寻找的最终结果。因此，这里的关键是，Spark等到您告诉它实际产生结果，只有在那一点上它才会去找出如何产生那个结果。因此，这是一个很酷的概念，这是它效率的关键。
- en: It's young
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它很年轻
- en: Spark is a very hot technology, and is relatively young, so it's still very
    much emerging and changing quickly, but a lot of big people are using it. Amazon,
    for example, has claimed they're using it, eBay, NASA's Jet Propulsional Laboratories,
    Groupon, TripAdvisor, Yahoo, and many, many others have too. I'm sure there's
    a lot of companies using it that don't confess up to it, but if you go to the
    Spark Apache Wiki page at [http://spark.apache.org/powered-by.html](http://spark.apache.org/powered-by.html).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Spark是一种非常炙手可热的技术，而且相对年轻，所以它仍然在不断发展和迅速变化，但很多大公司都在使用它。例如，亚马逊声称他们在使用它，eBay，NASA的喷气推进实验室，Groupon，TripAdvisor，雅虎，还有许多其他公司也在使用。我相信有很多公司在使用它，但他们不会承认，但如果你去Spark
    Apache Wiki页面[http://spark.apache.org/powered-by.html](http://spark.apache.org/powered-by.html)。
- en: There's actually a list you can look up of known big companies that are using
    Spark to solve real-world data problems. If you are worried that you're getting
    into the bleeding edge here, fear not, you're in very good company with some very
    big people that are using Spark in production for solving real problems. It is
    pretty stable stuff at this point.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上有一个你可以查阅的已知大公司使用Spark解决实际数据问题的列表。如果你担心自己正在接触最前沿的技术，不用担心，有一些非常大的公司正在使用Spark来解决实际问题，你是和一些非常重要的人一起使用Spark来解决实际问题。在这一点上，它是相当稳定的东西。
- en: It's not difficult
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这并不困难
- en: It's also not that hard. You have your choice of programming in Python, Java,
    or Scala, and they're all built around the same concept that I just described
    earlier, that is, the Resilient Distributed Dataset, RDD for short. We'll talk
    about that in a lot more detail in the coming sections of this chapter.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这也不难。你可以选择用Python、Java或Scala编程，它们都是围绕我之前描述的相同概念构建的，即弹性分布式数据集，简称RDD。我们将在本章的后续部分详细讨论这一点。
- en: Components of Spark
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark的组件
- en: Spark actually has many different components that it's built up of. So there
    is a Spark Core that lets you do pretty much anything you can dream up just using
    Spark Core functions alone, but there are these other things built on top of Spark
    that are also useful.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Spark实际上有许多不同的组件构成。因此，有一个Spark核心，只需使用Spark核心功能就可以做出几乎任何你可以想象的事情，但还有其他一些构建在Spark之上的东西也很有用。
- en: '![](img/ed75debb-a33c-49d3-99f3-e69d183abf4f.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ed75debb-a33c-49d3-99f3-e69d183abf4f.png)'
- en: '**Spark Streaming**: Spark Streaming is a library that lets you actually process
    data in real time. Data can be flowing into a server continuously, say, from weblogs,
    and Spark Streaming can help you process that data in real time as you go, forever.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark Streaming**：Spark Streaming是一个库，它让你实际上可以实时处理数据。数据可以持续地流入服务器，比如来自网络日志，Spark
    Streaming可以帮助你实时处理数据，一直进行下去。'
- en: '**Spark SQL**: This lets you actually treat data as a SQL database, and actually
    issue SQL queries on it, which is kind of cool if you''re familiar with SQL already.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark SQL**：这让你实际上可以将数据视为SQL数据库，并在其上发出SQL查询，如果你已经熟悉SQL，这是很酷的。'
- en: '**MLlib**: This is what we''re going to be focusing on in this section. It
    is actually a machine learning library that lets you perform common machine learning
    algorithms, with Spark underneath the hood to actually distribute that processing
    across a cluster. You can perform machine learning on much larger Datasets than
    you could have otherwise.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MLlib**：这是我们在本节中要重点关注的内容。它实际上是一个机器学习库，让你可以执行常见的机器学习算法，底层使用Spark来实际分布式处理集群中的数据。你可以对比以前能处理的更大的数据集进行机器学习。'
- en: '**GraphX**: This is not for making pretty charts and graphs. It refers to graph
    in the network theory sense. Think about a social network; that''s an example
    of a graph. GraphX just has a few functions that let you analyze the properties
    of a graph of information.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GraphX**：这不是用来制作漂亮的图表和图形的。它是指网络理论意义上的图。想想一个社交网络；这就是图的一个例子。GraphX只有一些函数，让你分析信息图的属性。'
- en: Python versus Scala for Spark
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python与Scala在Spark中的比较
- en: I do get some flack sometimes about using Python when I'm teaching people about
    Apache Spark, but there's a method to my madness. It is true that a lot of people
    use Scala when they're writing Spark code, because that's what Spark is developed
    in natively. So, you are incurring a little bit of overhead by forcing Spark to
    translate your Python code into Scala and then into Java interpreter commands
    at the end of the day.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候我在教授Apache Spark时会遇到一些批评，因为我使用Python，但我的做法是有道理的。的确，很多人在编写Spark代码时使用Scala，因为Spark是本地开发的。因此，通过强制Spark将你的Python代码转换为Scala，然后在最后一天转换为Java解释器命令，你会增加一些开销。
- en: 'However, Python''s a lot easier, and you don''t need to compile things. Managing
    dependencies is also a lot easier. You can really focus your time on the algorithms
    and what you''re doing, and less on the minutiae of actually getting it built,
    and running, and compiling, and all that nonsense. Plus, obviously, this book
    has been focused on Python so far, and it makes sense to keep using what we''ve
    learned and stick with Python throughout these lectures. Here''s a quick summary
    of the pros and cons of the two languages:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Python要容易得多，而且你不需要编译东西。管理依赖项也要容易得多。你可以真正把时间集中在算法和你正在做的事情上，而不是在实际构建、运行、编译和所有那些废话上。此外，显然，这本书到目前为止一直都在关注Python，继续使用我们学到的东西并在这些讲座中坚持使用Python是有意义的。以下是两种语言的优缺点的快速总结：
- en: '| **Python** | **Scala** |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| **Python** | **Scala** |'
- en: '|'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: No need to compile, manage dependencies, etc.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无需编译、管理依赖等
- en: Less coding overhead
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码开销更少
- en: You already know Python
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你已经了解Python
- en: Lets us focus on the concepts instead of a new language
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让我们专注于概念而不是新语言
- en: '|'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Scala is probably a more popular choice with Spark
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scala可能是Spark的更受欢迎的选择
- en: Spark is built in Scala, so coding in Scala is "native" to Spark
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark是用Scala构建的，所以在Scala中编码对于Spark来说是“本地”的
- en: New features, libraries tend to be Scala-first
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新功能、库往往是首先使用Scala
- en: '|'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'However, I will say that if you were to do some Spark programming in the real
    world, there''s a good chance people are using Scala. Don''t worry about it too
    much, though, because in Spark the Python and Scala code ends up looking very
    similar because it''s all around the same RDD concept. The syntax is very slightly
    different, but it''s not that different. If you can figure out how to do Spark
    using Python, learning how to use it in Scala isn''t that big of a leap, really.
    Here''s a quick example of the same code in the two languages:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我要说的是，如果您在现实世界中进行一些Spark编程，很有可能人们正在使用Scala。不过不要太担心，因为在Spark中，Python和Scala代码最终看起来非常相似，因为它们都围绕着相同的RDD概念。语法略有不同，但并不是很大的不同。如果您能够弄清楚如何使用Python进行Spark编程，学习如何在Scala中使用它并不是一个很大的飞跃。这里有两种语言中相同代码的快速示例：
- en: '![](img/6a204753-e6f0-415f-a359-b8d9a68c5a8e.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6a204753-e6f0-415f-a359-b8d9a68c5a8e.png)'
- en: So, that's the basic concepts of Spark itself, why it's such a big deal, and
    how it's so powerful in letting you run machine learning algorithms on very large
    Datasets, or any algorithm really. Let's now talk in a little bit more detail
    about how it does that, and the core concept of the Resilient Distributed Dataset.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这就是Spark本身的基本概念，为什么它如此重要，以及它如何在让您在非常大的数据集上运行机器学习算法或任何算法方面如此强大。现在让我们更详细地讨论一下它是如何做到这一点的，以及弹性分布式数据集的核心概念。
- en: Spark and Resilient Distributed Datasets (RDD)
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark和弹性分布式数据集（RDD）
- en: Let's get a little bit deeper into how Spark works. We're going to talk about
    Resilient Distributed Datasets, known as RDDs. It's sort of the core that you
    use when programming in Spark, and we'll have a few code snippets to try to make
    it real. We're going to give you a crash course in Apache Spark here. There's
    a lot more depth to it than what we're going to cover in the next few sections,
    but I'm just going to give you the basics you need to actually understand what's
    going on in these examples, and hopefully get you started and pointed in the right
    direction.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解一下Spark的工作原理。我们将谈论弹性分布式数据集，即RDD。这是您在Spark编程中使用的核心，我们将提供一些代码片段来尝试使其变得真实。我们将在这里为您提供Apache
    Spark的速成课程。比我们接下来要涵盖的内容更加深入，但我只会为您提供实际理解这些示例所需的基础知识，并希望能够让您开始并指向正确的方向。
- en: As mentioned, the most fundamental piece of Spark is called the Resilient Distributed
    Dataset, an RDD, and this is going to be the object that you use to actually load
    and transform and get the answers you want out of the data that you're trying
    to process. It's a very important thing to understand. The final letter in RDD
    stands for Dataset, and at the end of the day that's all it is; it's just a bunch
    of rows of information that can contain pretty much anything. But the key is the
    R and the first D.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Spark最基本的部分称为弹性分布式数据集，即RDD，这将是您实际用来加载、转换和获取您想要的数据的对象。这是一个非常重要的理解。RDD中的最后一个字母代表数据集，最终它只是一堆包含几乎任何内容的信息行。但关键是R和第一个D。
- en: '**Resilient**: It is resilient in that Spark makes sure that if you''re running
    this on a cluster and one of those clusters goes down, it can automatically recover
    from that and retry. Now, that resilience only goes so far, mind you. If you don''t
    have enough resources available to the job that you''re trying to run, it will
    still fail, and you will have to add more resources to it. There''s only so many
    things it can recover from; there is a limit to how many times it will retry a
    given task. But it does make its best effort to make sure that in the face of
    an unstable cluster or an unstable network it will still continue to try its best
    to run through to completion.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**弹性**：它是弹性的，因为Spark确保如果您在集群上运行此任务并且其中一个集群出现故障，它可以自动从中恢复并重试。不过，请注意，这种弹性是有限的。如果您没有足够的资源可用于您要运行的作业，它仍然会失败，您将不得不为其添加更多资源。它只能从许多事情中恢复；它会尝试多少次重新尝试给定的任务是有限的。但它会尽最大努力确保在面对不稳定的集群或不稳定的网络时，仍然会继续尽最大努力运行到完成。'
- en: '**Distributed**: Obviously, it is distributed. The whole point of using Spark
    is that you can use it for big data problems where you can actually distribute
    the processing across the entire CPU and memory power of a cluster of computers.
    That can be distributed horizontally, so you can throw as many computers as you
    want to a given problem. The larger the problem, the more computers; there''s
    really no upper bound to what you can do there.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式**：显然，它是分布式的。使用Spark的整个目的是，您可以将其用于可以横向分布到整个计算机集群的CPU和内存功率的大数据问题。这可以水平分布，因此您可以将尽可能多的计算机投入到给定的问题中。问题越大，使用的计算机就越多；在这方面真的没有上限。'
- en: The SparkContext object
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SparkContext对象
- en: You always start your Spark scripts by getting a SparkContext object, and this
    is the object that embodies the guts of Spark. It is what is going to give you
    your RDDs to process on, so it is what generates the objects that you use in your
    processing.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 您始终通过获取SparkContext对象来启动Spark脚本，这个对象体现了Spark的核心。它将为您提供要在其上处理的RDD，因此它生成了您在处理中使用的对象。
- en: You know, you don't actually think about the SparkContext very much when you're
    actually writing Spark programs, but it is sort of the substrate that is running
    them for you under the hood. If you're running in the Spark shell interactively,
    it has an `sc` object already available for you that you can use to create RDDs.
    In a standalone script, however, you will have to create that SparkContext explicitly,
    and you'll have to pay attention to the parameters that you use because you can
    actually tell the Spark context how you want that to be distributed. Should I
    take advantage of every core that I have available to me? Should I be running
    on a cluster or just standalone on my local computer? So, that's where you set
    up the fundamental settings of how Spark will operate.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道吗，当你实际编写Spark程序时，你并不会非常关注SparkContext，但它实际上是在幕后为你运行它们的基础。如果你在Spark shell中交互式运行，它已经为你提供了一个`sc`对象，你可以用它来创建RDD。然而，在独立脚本中，你将不得不显式创建SparkContext，并且你将不得不注意你使用的参数，因为你实际上可以告诉Spark上下文你希望它如何分布。我应该利用我可用的每个核心吗？我应该在集群上运行还是只在我的本地计算机上独立运行？所以，这就是你设置Spark操作的基本设置的地方。
- en: Creating RDDs
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建RDD
- en: Let's look at some little code snippets of actually creating RDDs, and I think
    it will all start to make a little bit more sense.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一些实际创建RDD的小代码片段，我认为这一切都会开始变得更加清晰。
- en: Creating an RDD using a Python list
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Python列表创建RDD
- en: 'The following is a very simple example:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个非常简单的例子：
- en: '[PRE0]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If I just want to make an RDD out of a plain old Python list, I can call the
    `parallelize()` function in Spark. That will convert a list of stuff, in this
    case, just the numbers, 1, 2, 3, 4, into an RDD object called `nums`.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我只想从一个普通的Python列表中创建RDD，我可以在Spark中调用`parallelize()`函数。这将把一系列东西，比如这里的数字1、2、3、4，转换为一个名为`nums`的RDD对象。
- en: That is the simplest case of creating an RDD, just from a hard-coded list of
    stuff. That list could come from anywhere; it doesn't have to be hard-coded either,
    but that kind of defeats the purpose of big data. I mean, if I have to load the
    entire Dataset into memory before I can create an RDD from it, what's the point?
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这是创建RDD的最简单情况，只是从一个硬编码的列表中创建。该列表可以来自任何地方；它也不必是硬编码的，但这有点违背了大数据的目的。我的意思是，如果我必须在创建RDD之前将整个数据集加载到内存中，那还有什么意义呢？
- en: Loading an RDD from a text file
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从文本文件加载RDD
- en: I can also load an RDD from a text file, and that could be anywhere.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我还可以从文本文件中加载RDD，它可以是任何地方。
- en: '[PRE1]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In this example, I have a giant text file that's the entire encyclopedia or
    something. I'm reading that from my local disk here, but I could also use s3n
    if I want to host this file on a distributed AmazonS3 bucket, or hdfs if I want
    to refer to data that's stored on a distributed HDFS cluster (that stands for
    Hadoop Distributed File System if you're not familiar with HDFS). When you're
    dealing with big data and working with a Hadoop cluster, usually that's where
    your data will live.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我有一个巨大的文本文件，整个百科全书之类的东西。我正在从我的本地磁盘读取它，但如果我想要将这个文件托管在分布式的AmazonS3存储桶上，我也可以使用s3n，或者如果我想引用存储在分布式HDFS集群上的数据，我可以使用hdfs（如果您对HDFS不熟悉，它代表Hadoop分布式文件系统）。当你处理大数据并使用Hadoop集群时，通常你的数据会存储在那里。
- en: That line of code will actually convert every line of that text file into its
    own row in an RDD. So, you can think of the RDD as a database of rows, and, in
    this example, it will load up my text file into an RDD where every line, every
    row, contains one line of text. I can then do further processing in that RDD to
    parse or break out the delimiters in that data. But that's where I start from.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这行代码实际上会将文本文件的每一行转换为RDD中的一行。所以，你可以把RDD看作是一行的数据库，在这个例子中，它将我的文本文件加载到一个RDD中，其中每一行，每一行，包含一行文本。然后我可以在那个RDD中进行进一步的处理，解析或分解数据中的分隔符。但这是我开始的地方。
- en: Remember when we talked about ETL and ELT earlier in the book? This is a good
    example of where you might actually be loading raw data into a system and doing
    the transform on the system itself that you used to query your data. You can take
    raw text files that haven't been processed at all and use the power of Spark to
    actually transform those into more structured data.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 还记得我们之前在书中讨论ETL和ELT吗？这是一个很好的例子，你可能实际上正在将原始数据加载到系统中，并在系统本身上进行转换，用于查询数据的系统。你可以拿未经任何处理的原始文本文件，并利用Spark的强大功能将其转换为更结构化的数据。
- en: 'It can also talk to things like Hive, so if you have an existing Hive database
    set up at your company, you can create a Hive context that''s based on your Spark
    context. How cool is that? Take a look at this example code:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 它还可以与Hive等东西通信，所以如果你的公司已经设置了现有的Hive数据库，你可以创建一个基于你的Spark上下文的Hive上下文。这是多么酷啊？看看这个例子代码：
- en: '[PRE2]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You can actually create an RDD, in this case called rows, that's generated by
    actually executing a SQL query on your Hive database.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 你实际上可以创建一个RDD，这里称为rows，它是通过在你的Hive数据库上实际执行SQL查询来生成的。
- en: More ways to create RDDs
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建RDD的更多方法
- en: There are more ways to create RDDs as well. You can create them from a JDBC
    connection. Basically any database that supports JDBC can also talk to Spark and
    have RDDs created from it. Cassandra, HBase, Elasticsearch, also files in JSON
    format, CSV format, sequence files object files, and a bunch of other compressed
    files like ORC can be used to create RDDs. I don't want to get into the details
    of all those, you can get a book and look those up if you need to, but the point
    is that it's very easy to create an RDD from data, wherever it might be, whether
    it's on a local filesystem or a distributed data store.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 还有更多创建RDD的方法。您可以从JDBC连接创建它们。基本上，任何支持JDBC的数据库也可以与Spark通信，并从中创建RDD。Cassandra、HBase、Elasticsearch，还有JSON格式、CSV格式、序列文件对象文件以及一堆其他压缩文件（如ORC）都可以用来创建RDD。我不想深入讨论所有这些细节，如果需要，您可以找一本书查看，但重点是很容易从数据中创建RDD，无论数据是在本地文件系统还是分布式数据存储中。
- en: Again, RDD is just a way of loading and maintaining very large amounts of data
    and keeping track of it all at once. But, conceptually within your script, an
    RDD is just an object that contains a bunch of data. You don't have to think about
    the scale, because Spark does that for you.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，RDD只是一种加载和维护大量数据并一次跟踪所有数据的方法。但是，在脚本中，概念上，RDD只是包含大量数据的对象。您不必考虑规模，因为Spark会为您处理。
- en: RDD operations
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RDD操作
- en: Now, there are two different types of classes of things you can do on RDDs once
    you have them, you can do transformations, and you can do actions.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，一旦您拥有RDD，您可以对其执行两种不同类型的操作，即转换和操作。
- en: Transformations
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转换
- en: 'Let''s talk about transformations first. Transformations are exactly what they
    sound like. It''s a way of taking an RDD and transforming every row in that RDD
    to a new value, based on a function you provide. Let''s look at some of those
    functions:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先谈谈转换。转换就是它听起来的样子。这是一种将RDD中的每一行根据您提供的函数转换为新值的方法。让我们看看其中一些函数：
- en: '**map() and flatmap()**: `map` and `flatmap` are the functions you''ll see
    the most often. Both of these will take any function that you can dream up, that
    will take, as input, a row of an RDD, and it will output a transformed row. For
    example, you might take raw input from a CSV file, and your `map` operation might
    take that input and break it up into individual fields based on the comma delimiter,
    and return back a Python list that has that data in a more structured format that
    you can perform further processing on. You can chain map operations together,
    so the output of one `map` might end up creating a new RDD that you then do another
    transformation on, and so on, and so forth. Again, the key is, Spark can distribute
    those transformations across the cluster, so it might take part of your RDD and
    transform it on one machine, and another part of your RDD and transform it on
    another.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**map() 和 flatmap()**: `map`和`flatmap`是您经常看到的函数。这两个函数都将接受您可以想象的任何函数，该函数将以RDD的一行作为输入，并输出一个转换后的行。例如，您可以从CSV文件中获取原始输入，您的`map`操作可能会将该输入根据逗号分隔符拆分为单独的字段，并返回一个包含以更结构化格式的数据的Python列表，以便您可以进行进一步的处理。您可以链接map操作，因此一个`map`的输出可能最终创建一个新的RDD，然后您可以对其进行另一个转换，依此类推。再次强调，关键是，Spark可以在集群上分发这些转换，因此它可能会在一台机器上转换RDD的一部分，然后在另一台机器上转换RDD的另一部分。'
- en: Like I said, `map` and `flatmap` are the most common transformations you'll
    see. The only difference is that `map` will only allow you to output one value
    for every row, whereas `flatmap` will let you actually output multiple new rows
    for a given row. So you can actually create a larger RDD or a smaller RDD than
    you started with using `flatmap.`
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我说的，`map`和`flatmap`是您将看到的最常见的转换。唯一的区别是`map`只允许您为每一行输出一个值，而`flatmap`将允许您实际上为给定的行输出多个新行。因此，您实际上可以使用`flatmap`创建一个比您开始时更大或更小的RDD。
- en: '**filter()**: `filter` can be used if what you want to do is just create a
    Boolean function that says "should this row be preserved or not? Yes or no."'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**filter()**: 如果您只想创建一个布尔函数来判断“是否应该保留此行？是或否。”'
- en: '**distinct()**: `distinct` is a less commonly used transformation that will
    only return back distinct values within your RDD.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**distinct()**: `distinct`是一个不太常用的转换，它将仅返回RDD中的不同值。'
- en: '**sample()**: This function lets you take a random sample from your RDD'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**sample()**: 此函数允许您从RDD中随机抽取样本'
- en: '**union(), intersection(), subtract() and Cartesian()**: You can perform intersection
    operations like union, intersection, subtract, or even produce every cartesian
    combination that exists within an RDD.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**union(), intersection(), subtract() 和 Cartesian()**: 您可以执行诸如并集、交集、差集，甚至生成RDD中存在的每个笛卡尔组合的操作。'
- en: Using map()
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用map()
- en: 'Here''s a little example of how you might use the map function in your work:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是您如何在工作中使用map函数的一个小例子：
- en: '[PRE3]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Let's say I created an RDD just from the list 1, 2, 3, 4\. I can then call `rdd.map()`
    with a lambda function of x that takes in each row, that is, each value of that
    RDD, calls it x, and then it applies the function x multiplied by x to square
    it. If I were to then collect the output of this RDD, it would be 1, 4, 9 and
    16, because it would take each individual entry of that RDD and square it, and
    put that into a new RDD.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我只是从列表1、2、3、4创建了一个RDD。然后我可以使用一个lambda函数x调用`rdd.map()`，该函数接受每一行，也就是RDD的每个值，将其称为x，然后将函数x乘以x应用于平方。如果我然后收集此RDD的输出，它将是1、4、9和16，因为它将获取该RDD的每个单独条目并对其进行平方，然后将其放入新的RDD中。
- en: 'If you don''t remember what lambda functions are, we did talk about it a little
    bit earlier in this book, but as a refresher, the lambda function is just a shorthand
    for defining a function in line. So `rdd.map(lambda x: x*x)` is exactly the same
    thing as a separate function `def squareIt(x): return x*x`, and saying `rdd.map(squareIt)`.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '如果您不记得lambda函数是什么，我们在本书的前面稍微谈到过，但是作为提醒，lambda函数只是定义一个内联函数的简写。因此，`rdd.map(lambda
    x: x*x)`与一个单独的函数`def squareIt(x): return x*x`是完全相同的，并且说`rdd.map(squareIt)`。'
- en: It's just a shorthand for very simple functions that you want to pass in as
    a transformation. It eliminates the need to actually declare this as a separate
    named function of its own. That's the whole idea of functional programming. So
    you can say you understand functional programming now, by the way! But really,
    it's just shorthand notation for defining a function inline as part of the parameters
    to a `map()` function, or any transformation for that matter.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个非常简单的函数的简写，您希望将其作为转换传递。它消除了实际将其声明为自己的单独命名函数的需要。这就是函数式编程的整个理念。所以你现在可以说你理解函数式编程了！但实际上，这只是定义一个内联函数作为`map()`函数的参数之一，或者任何转换的简写符号。
- en: Actions
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 行动
- en: 'You can also perform actions on an RDD, when you want to actually get a result.
    Here are some examples of what you can do:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以对RDD执行操作，当您真正想要获得结果时。以下是一些您可以执行的示例：
- en: '`collect()`: You can call collect() on an RDD, which will give you back a plain
    old Python object that you can then iterate through and print out the results,
    or save them to a file, or whatever you want to do.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`collect()`: 您可以在RDD上调用collect()，这将为您提供一个普通的Python对象，然后您可以遍历并打印结果，或将其保存到文件，或者您想做的任何其他事情。'
- en: '`count()`: You can also call `count()`, which will force it to actually go
    count how many entries are in the RDD at this point.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`count()`: 您还可以调用`count()`，这将强制其实际上计算此时RDD中有多少条目。'
- en: '`countByValue()`: This function will give you a breakdown of how many times
    each unique value within that RDD occurs.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`countByValue()`: 此函数将为您提供RDD中每个唯一值出现的次数的统计。'
- en: '`take()`: You can also sample from the RDD using `take()`, which will take
    a random number of entries from the RDD.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`take()`: 您还可以使用`take()`从RDD中进行抽样，它将从RDD中获取随机数量的条目。'
- en: '`top()`: `top()` will give you the first few entries in that RDD if you just
    want to get a little peek into what''s in there for debugging purposes.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`top()`: 如果您只想为了调试目的查看RDD中的前几个条目，`top()`将为您提供这些条目。'
- en: '`reduce()`: The more powerful action is `reduce()` which will actually let
    you combine values together for the same common key value. You can also use RDDs
    in the context of key-value data. The `reduce()` function lets you define a way
    of combining together all the values for a given key. It is very much similar
    in spirit to MapReduce. `reduce()` is basically the analogous operation to a `reducer()`
    in MapReduce, and `map()` is analogous to a `mapper()`. So, it''s often very straightforward
    to actually take a MapReduce job and convert it to Spark by using these functions.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reduce()`: 更强大的操作是`reduce()`，它实际上允许您将相同的公共键值的值组合在一起。您还可以在键-值数据的上下文中使用RDD。`reduce()`函数允许您定义一种将给定键的所有值组合在一起的方式。它在精神上与MapReduce非常相似。`reduce()`基本上是MapReduce中`reducer()`的类似操作，而`map()`类似于`mapper()`。因此，通过使用这些函数，实际上很容易将MapReduce作业转换为Spark。'
- en: Remember, too, that nothing actually happens in Spark until you call an action.
    Once you call one of those action methods, that's when Spark goes out and does
    its magic with directed acyclic graphs, and actually computes the optimal way
    to get the answer you want. But remember, nothing really occurs until that action
    happens. So, that can sometimes trip you up when you're writing Spark scripts,
    because you might have a little print statement in there, and you might expect
    to get an answer, but it doesn't actually appear until the action is actually
    performed.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 还记得，在Spark中实际上什么都不会发生，直到您调用一个操作。一旦调用其中一个操作方法，Spark就会出去并使用有向无环图进行其魔术，并实际计算获得所需答案的最佳方式。但请记住，直到发生那个操作，实际上什么都不会发生。因此，当您编写Spark脚本时，有时可能会遇到问题，因为您可能在其中有一个小的打印语句，并且您可能期望得到一个答案，但实际上直到执行操作时才会出现。
- en: That is Spark 101 in a nutshell. Those are the basics you need for Spark programming.
    Basically, what is an RDD and what are the things you can do to an RDD. Once you
    get those concepts, then you can write some Spark code. Let's change tack now
    and talk about MLlib, and some specific features in Spark that let you do machine
    learning algorithms using Spark.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是Spark编程的基础。基本上，什么是RDD以及您可以对RDD执行哪些操作。一旦掌握了这些概念，您就可以编写一些Spark代码。现在让我们改变方向，谈谈MLlib，以及Spark中一些特定的功能，让您可以使用Spark进行机器学习算法。
- en: Introducing MLlib
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍MLlib
- en: Fortunately, you don't have to do things the hard way in Spark when you're doing
    machine learning. It has a built-in component called MLlib that lives on top of
    Spark Core, and this makes it very easy to perform complex machine learning algorithms
    using massive Datasets, and distributing that processing across an entire cluster
    of computers. So, very exciting stuff. Let's learn more about what it can do.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，在进行机器学习时，您不必在Spark中以困难的方式进行操作。它有一个名为MLlib的内置组件，它位于Spark Core之上，这使得使用大规模数据集执行复杂的机器学习算法变得非常容易，并将该处理分布到整个计算机集群中。非常令人兴奋的事情。让我们更多地了解它可以做什么。
- en: Some MLlib Capabilities
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一些MLlib功能
- en: So, what are some of the things MLlib can do? Well, one is feature extraction.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，MLlib可以做些什么？其中之一是特征提取。
- en: One thing you can do at scale is term frequency and inverse document frequency
    stuff, and that's useful for creating, for example, search indexes. We will actually
    go through an example of that later in the chapter. The key, again, is that it
    can do this across a cluster using massive Datasets, so you could make your own
    search engine for the web with this, potentially. It also offers basic statistics
    functions, chi-squared tests, Pearson or Spearman correlation, and some simpler
    things like min, max, mean, and variance. Those aren't terribly exciting in and
    of themselves, but what is exciting is that you can actually compute the variance
    or the mean or whatever, or the correlation score, across a massive Dataset, and
    it would actually break that Dataset up into various chunks and run that across
    an entire cluster if necessary.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在规模上执行词频和逆文档频率等操作，这对于创建搜索索引非常有用。我们稍后将实际上通过本章的一个示例来进行说明。关键是，它可以使用大规模数据集在整个集群中执行此操作，因此您可以使用它来为网络创建自己的搜索引擎。它还提供基本的统计函数，卡方检验，皮尔逊或斯皮尔曼相关性，以及一些更简单的东西，如最小值，最大值，平均值和方差。这些本身并不是非常令人兴奋，但令人兴奋的是，您实际上可以计算大规模数据集的方差或平均值，或者相关性得分，如果必要，它实际上会将该数据集分解成各种块，并在整个集群中运行。
- en: So, even if some of these operations aren't terribly interesting, what's interesting
    about it is the scale at which it can operate at. It can also support things like
    linear regression and logistic regression, so if you need to fit a function to
    a massive set of data and use that for predictions, you can do that too. It also
    supports Support Vector Machines. We're getting into some of the more fancy algorithms
    here, some of the more advanced stuff, and that too can scale up to massive Datasets
    using Spark's MLlib. There is a Naive Bayes classifier built into MLlib, so, remember
    that spam classifier that we built earlier in the book? You could actually do
    that for an entire e-mail system using Spark, and scale that up as far as you
    want to.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees, one of my favorite things in machine learning, are also supported
    by Spark, and we'll actually have an example of that later in this chapter. We'll
    also look at K-Means clustering, and you can do clustering using K-Means and massive
    Datasets with Spark and MLlib. Even principal component analysis and **SVD** (**Singular
    Value Decomposition**) can be done with Spark as well, and we'll have an example
    of that too. And, finally, there's a built-in recommendations algorithm called
    Alternating Least Squares that's built into MLlib. Personally, I've had kind of
    mixed results with it, you know, it's a little bit too much of a black box for
    my taste, but I am a recommender system snob, so take that with a grain of salt!
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Special MLlib data types
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using MLlib is usually pretty straightforward, there are just some library functions
    you need to call. It does introduce a few new data types; however, that you need
    to know about, and one is the vector.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: The vector data type
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Remember when we were doing movie similarities and movie recommendations earlier
    in the book? An example of a vector might be a list of all the movies that a given
    user rated. There are two types of vector, sparse and dense. Let's look at an
    example of those. There are many, many movies in the world, and a dense vector
    would actually represent data for every single movie, whether or not a user actually
    watched it. So, for example, let's say I have a user who watched Toy Story, obviously
    I would store their rating for Toy Story, but if they didn't watch the movie Star
    Wars, I would actually store the fact that there is not a number for Star Wars.
    So, we end up taking up space for all these missing data points with a dense vector.
    A sparse vector only stores the data that exists, so it doesn't waste any memory
    space on missing data, OK. So, it's a more compact form of representing a vector
    internally, but obviously that introduces some complexity while processing. So,
    it's a good way to save memory if you know that your vectors are going to have
    a lot of missing data in them.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: LabeledPoint data type
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There's also a `LabeledPoint` data type that comes up, and that's just what
    it sounds like, a point that has some sort of label associated with it that conveys
    the meaning of this data in human readable terms.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Rating data type
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, there is a `Rating` data type that you'll encounter if you're using
    recommendations with MLlib. This data type can take in a rating that represents
    a 1-5 or 1-10, whatever star rating a person might have, and use that to inform
    product recommendations automatically.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: So, I think you finally have everything you need to get started, let's dive
    in and actually look at some real MLlib code and run it, and then it will make
    a lot more sense.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Decision Trees in Spark with MLlib
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Alright, let''s actually build some decision trees using Spark and the MLlib
    library, this is very cool stuff. Wherever you put the course materials for this
    book, I want you to go to that folder now. Make sure you''re completely closed
    out of Canopy, or whatever environment you''re using for Python development, because
    I want to make sure you''re starting it from this directory, OK? And find the
    `SparkDecisionTree` script, and double-click that to open up Canopy:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，让我们使用Spark和MLlib库实际构建一些决策树，这是非常酷的东西。无论你把这本书的课程材料放在哪里，我希望你现在就去那个文件夹。确保你完全关闭了Canopy，或者你用于Python开发的任何环境，因为我想确保你是从这个目录开始的，好吗？然后找到`SparkDecisionTree`脚本，双击打开Canopy：
- en: '![](img/ad29a0a7-b494-4b34-9ab7-0430ffb3c225.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ad29a0a7-b494-4b34-9ab7-0430ffb3c225.png)'
- en: Now, up until this point we've been using IPython notebooks for our code, but
    you can't really use those very well with Spark. With Spark scripts, you need
    to actually submit them to the Spark infrastructure and run them in a very special
    way, and we'll see how that works shortly.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在这一点上，我们一直在使用IPython笔记本来编写我们的代码，但是你不能真正很好地使用它们与Spark。对于Spark脚本，你需要实际将它们提交到Spark基础设施并以非常特殊的方式运行它们，我们很快就会看到它是如何工作的。
- en: Exploring decision trees code
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索决策树代码
- en: So, we are just looking at a raw Python script file now, without any of the
    usual embellishment of the IPython notebook stuff. let's walk through what's going
    on in the script.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，现在我们只是看一个原始的Python脚本文件，没有IPython笔记本的通常修饰。让我们来看看脚本中发生了什么。
- en: '![](img/6d667f6b-c68c-489a-a94b-5582b37634f0.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6d667f6b-c68c-489a-a94b-5582b37634f0.png)'
- en: We'll go through it slowly, because this is your first Spark script that you've
    seen in this book.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们会慢慢来，因为这是你在本书中看到的第一个Spark脚本。
- en: First, we're going to import, from `pyspark.mllib`, the bits that we need from
    the machine learning library for Spark.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将从`pyspark.mllib`中导入我们在Spark机器学习库中需要的部分。
- en: '[PRE4]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We need the `LabeledPoint` class, which is a data type required by the `DecisionTree`
    class, and the `DecisionTree` class itself, imported from `mllib.tree`.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要`LabeledPoint`类，这是`DecisionTree`类所需的数据类型，以及从`mllib.tree`导入的`DecisionTree`类本身。
- en: 'Next, pretty much every Spark script you see is going to include this line,
    where we import `SparkConf` and `SparkContext`:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你会看到几乎每个Spark脚本都会包含这一行，我们在其中导入`SparkConf`和`SparkContext`：
- en: '[PRE5]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This is needed to create the `SparkContext` object that is kind of the root
    of everything you do in Spark.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这是创建`SparkContext`对象所需的，它是你在Spark中做任何事情的根本。
- en: 'And finally, we''re going to import the array library from `numpy`:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将从`numpy`中导入数组库：
- en: '[PRE6]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Yes, you can still use `NumPy`, and `scikit-learn`, and whatever you want within
    Spark scripts. You just have to make sure, first of all, that these libraries
    are installed on every machine that you intend to run it on.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，你仍然可以在Spark脚本中使用`NumPy`、`scikit-learn`或者任何你想要的东西。你只需要确保首先这些库在你打算在其上运行的每台机器上都已安装好。
- en: If you're running on a cluster, you need to make sure that those Python libraries
    are already in place somehow, and you also need to understand that Spark will
    not make the scikit-learn methods, for example, magically scalable. You can still
    call these functions in the context of a given map function, or something like
    that, but it's only going to run on that one machine within that one process.
    Don't lean on that stuff too heavily, but, for simple things like managing arrays,
    it's totally an okay thing to do.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在集群上运行，你需要确保这些Python库已经以某种方式安装好了，并且你还需要明白，Spark不会使scikit-learn的方法等变得可扩展。你仍然可以在给定map函数的上下文中调用这些函数，但它只会在那一个机器的一个进程中运行。不要过分依赖这些东西，但是对于像管理数组这样的简单事情，这是完全可以的。
- en: Creating the SparkContext
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建SparkContext
- en: Now, we'll start by setting up our `SparkContext`, and giving it a `SparkConf`,
    a configuration.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将开始设置我们的`SparkContext`，并给它一个`SparkConf`，一个配置。
- en: '[PRE7]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This configuration object says, I'm going to set the master node to "`local`",
    and this means that I'm just running on my own local desktop, I'm not actually
    running on a cluster at all, and I'm just going to run in one process. I'm also
    going to give it an app name of "`SparkDecisionTree`," and you can call that whatever
    you want, Fred, Bob, Tim, whatever floats your boat. It's just what this job will
    appear as if you were to look at it in the Spark console later on.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配置对象表示，我将把主节点设置为"`local`"，这意味着我只是在自己的本地桌面上运行，我实际上根本不是在集群上运行，我只会在一个进程中运行。我还会给它一个应用程序名称"`SparkDecisionTree`"，你可以随意命名它，Fred、Bob、Tim，随你喜欢。这只是当你稍后在Spark控制台中查看时，这个作业将显示为什么。
- en: 'And then we will create our `SparkContext` object using that configuration:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将使用该配置创建我们的`SparkContext`对象：
- en: '[PRE8]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: That gives us an `sc` object we can use for creating RDDs.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 这给了我们一个`sc`对象，我们可以用它来创建RDDs。
- en: 'Next, we have a bunch of functions:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们有一堆函数：
- en: '[PRE9]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Let's just get down these functions for now, and we'll come back to them later.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 现在先记住这些函数，稍后我们会回来再讨论它们。
- en: Importing and cleaning our data
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入和清理我们的数据
- en: Let's go to the first bit of Python code that actually gets executed in this
    script.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下这个脚本中实际执行的第一部分Python代码。
- en: '![](img/ba509b70-e811-4149-b770-49de93e80b5b.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ba509b70-e811-4149-b770-49de93e80b5b.png)'
- en: The first thing we're going to do is load up this `PastHires.csv` file, and
    that's the same file we used in the decision tree exercise that we did earlier
    in this book.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要做的第一件事是加载`PastHires.csv`文件，这是我们在本书早期做决策树练习时使用的同一个文件。
- en: Let's pause quickly to remind ourselves of the content of that file. If you
    remember right, we have a bunch of attributes of job candidates, and we have a
    field of whether or not we hired those people. What we're trying to do is build
    up a decision tree that will predict - would we hire or not hire a person given
    those attributes?
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们暂停一下，回顾一下那个文件的内容。如果你记得的话，我们有一堆求职者的属性，还有一个字段，表示我们是否雇佣了这些人。我们要做的是建立一个决策树，来预测
    - 根据这些属性，我们是否会雇佣这个人。
- en: Now, let's take a quick peek at the `PastHires.csv`, which will be an Excel
    file.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/26da7937-34a5-4c2b-9f83-e42cb12a44ab.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
- en: You can see that Excel actually imported this into a table, but if you were
    to look at the raw text you'd see that it's made up of comma-separated values.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: The first line is the actual headings of each column, so what we have above
    are the number of years of prior experience, is the candidate currently employed
    or not, number of previous employers, the level of education, whether they went
    to a top-tier school, whether they had an internship while they were in school,
    and finally, the target that we're trying to predict on, whether or not they got
    a job offer in the end of the day. Now, we need to read that information into
    an RDD so we can do something with it.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go back to our script:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The first thing we need to do is read that CSV data in, and we're going to throw
    away that first row, because that's our header information, remember. So, here's
    a little trick for doing that. We start off by importing every single line from
    that file into a raw data RDD, and I could call that anything I want, but we're
    calling it `sc.textFile`. SparkContext has a `textFile` function that will take
    a text file and create a new RDD, where each entry, each line of the RDD, consists
    of one line of input.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Make sure you change the path to that file to wherever you actually installed
    it, otherwise it won't work.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Now, I'm going to extract the first line, the first row from that RDD, by using
    the `first` function. So, now the header RDD will contain one entry that is just
    that row of column headers. And now, look what's going on in the above code, I'm
    using `filter` on my original data that contains all of the information in that
    CSV file, and I'm defining a `filter` function that will only let lines through
    if that line is not equal to the contents of that initial header row. What I've
    done here is, I've taken my raw CSV file and I've stripped out the first line
    by only allowing lines that do not equal that first line to survive, and I'm returning
    that back to the `rawData` RDD variable again. So, I'm taking `rawData`, filtering
    out that first line, and creating a new `rawData` that only contains the data
    itself. With me so far? It's not that complicated.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we''re going to use a `map` function. What we need to do next is start
    to make more structure out of this information. Right now, every row of my RDD
    is just a line of text, it is comma-delimited text, but it''s still just a giant
    line of text, and I want to take that comma-separated value list and actually
    split it up into individual fields. At the end of the day, I want each RDD to
    be transformed from a line of text that has a bunch of information separated by
    commas into a Python list that has actual individual fields for each column of
    information that I have. So, that''s what this lambda function does:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: It calls the built-in Python function `split`, which will take a row of input,
    and split it on comma characters, and divide that into a list of every field delimited
    by commas.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: The output of this map function, where I passed in a lambda function that just
    splits every line into fields based on commas, is a new RDD called `csvData`.
    And, at this point, `csvData` is an RDD that contains, on every row, a list where
    every element is a column from my source data. Now, we're getting close.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: 'It turns out that in order to use a decision tree with MLlib, a couple of things
    need to be true. First of all, the input has to be in the form of LabeledPoint
    data types, and it all has to be numeric in nature. So, we need to transform all
    of our raw data into data that can actually be consumed by MLlib, and that''s
    what the `createLabeledPoints` function that we skipped past earlier does. We''ll
    get to that in just a second, first here''s the call to it:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We''re going to call a map on `csvData`, and we are going to pass it the `createLabeledPoints`
    function, which will transform every input row into something even closer to what
    we want at the end of the day. So, let''s look at what `createLabeledPoints` does:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'It takes in a list of fields, and just to remind you again what that looks
    like, let''s pull up that `.csv` Excel file again:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1a544f69-91c3-4053-9c95-790c728122e3.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
- en: 'So, at this point, every RDD entry has a field, it''s a Python list, where
    the first element is the years of experience, second element is employed, so on
    and so forth. The problems here are that we want to convert those lists to Labeled
    Points, and we want to convert everything to numerical data. So, all these yes
    and no answers need to be converted to ones and zeros. These levels of experience
    need to be converted from names of degrees to some numeric ordinal value. Maybe
    we''ll assign the value zero to no education, one can mean BS, two can mean MS,
    and three can mean PhD, for example. Again, all these yes/no values need to be
    converted to zeros and ones, because at the end of the day, everything going into
    our decision tree needs to be numeric, and that''s what `createLabeledPoints`
    does. Now, let''s go back to the code and run through it:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'First, it takes in our list of `StringFields` ready to convert it into `LabeledPoints`,
    where the label is the target value-was this person hired or not? 0 or 1-followed
    by an array that consists of all the other fields that we care about. So, this
    is how you create a `LabeledPoint` that the `DecisionTree MLlib` class can consume.
    So, you see in the above code that we''re converting years of experience from
    a string to an integer value, and for all the yes/no fields, we''re calling this
    `binary` function, that I defined up at the top of the code, but we haven''t discussed
    yet:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'All it does is convert the character yes to 1, otherwise it returns 0\. So,
    Y will become 1, N will become 0\. Similarly, I have a `mapEducation` function:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As we discussed earlier, this simply converts different types of degrees to
    an ordinal numeric value in exactly the same way as our yes/no fields.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: 'As a reminder, this is the line of code that sent us running through those
    functions:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: At this point, after mapping our RDD using that `createLabeledPoints` function,
    we now have a `trainingData` RDD, and this is exactly what MLlib wants for constructing
    a decision tree.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Creating a test candidate and building our decision tree
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s create a little test candidate we can use, so we can use our model to
    actually predict whether someone new would be hired or not. What we''re going
    to do is create a test candidate that consists of an array of the same values
    for each field as we had in the CSV file:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let''s quickly compare that code with the Excel document so you can see the
    array mapping:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d0370539-6f0f-49a4-b9c0-5adc827ce00c.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
- en: Again, we need to map these back to their original column representation, so
    that 10, 1, 3, 1, 0, 0 means 10 years of prior experience, currently employed,
    three previous employers, a BS degree, did not go to a top-tier school and did
    not do an internship. We could actually create an entire RDD full of candidates
    if we wanted to, but we'll just do one for now.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we''ll use parallelize to convert that list into an RDD:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Nothing new there. Alright, now for the magic let''s move to the next code
    block:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We are going to call `DecisionTree.trainClassifier`, and this is what will
    actually build our decision tree itself. We pass in our `trainingData`, which
    is just an RDD full of `LabeledPoint` arrays, `numClasses=2`, because we have,
    basically, a yes or no prediction that we''re trying to make, will this person
    be hired or not? The next parameter is called `categoricalFeaturesInfo`, and this
    is a Python dictionary that maps fields to the number of categories in each field.
    So, if you have a continuous range available to a given field, like the number
    of years of experience, you wouldn''t specify that at all in here, but for fields
    that are categorical in nature, such as what degree do they have, for example,
    that would say fieldID3, mapping to the degree attained, which has four different
    possibilities: no education, BS, MS, and PhD. For all of the yes/no fields, we''re
    mapping those to 2 possible categories, yes/no or 0/1 is what we converted those
    to.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: Continuing to move through our `DecisionTree.trainClassifier` call, we are going
    to use the `'gini'` impurity metric as we measure the entropy. We have a `maxDepth`
    of 5, which is just an upper boundary on how far we're going to go, that can be
    larger if you wish. Finally, `maxBins` is just a way to trade off computational
    expense if you can, so it just needs to at least be the maximum number of categories
    you have in each feature. Remember, nothing really happens until we call an action,
    so we're going to actually use this model to make a prediction for our test candidate.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 'We use our `DecisionTree` model, which contains a decision tree that was trained
    on our test training data, and we tell that to make a prediction on our test data:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We''ll get back a list of predictions that we can then iterate through. So,
    `predict` returns a plain old Python object and is an action that I can `collect`.
    Let me rephrase that a little bit: `collect` will return a Python object on our
    predictions, and then we can iterate through every item in that list and print
    the result of the prediction.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also print out the decision tree itself by using `toDebugString`:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: That will actually print out a little representation of the decision tree that
    it created internally, that you can follow through in your own head. So, that's
    kind of cool too.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: Running the script
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Alright, feel free to take some time, stare at this script a little bit more,
    digest what's going on, but, if you're ready, let's move on and actually run this
    beast. So, to do so, you can't just run it directly from Canopy. We're going to
    go to the Tools menu and open up a Canopy Command Prompt, and this just opens
    up a Windows command prompt with all the necessary environment variables in place
    for running Python scripts in Canopy. Make sure that the working directory is
    the directory that you installed all of the course materials into.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: All we need to do is call `spark-submit`, so this is a script that lets you
    run Spark scripts from Python, and then the name of the script, `SparkDecisionTree.py`.
    That's all I have to do.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Hit Return, and off it will go. Again, if I were doing this on a cluster and
    I created my `SparkConf` accordingly, this would actually get distributed to the
    entire cluster, but, for now, we''re just going to run it on my computer. When
    it''s finished, you should see the below output:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5982c23d-99f3-4cb4-818a-6d9b5041176f.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
- en: 'So, in the above image, you can see in the test person that we put in above,
    we have a prediction that this person would be hired, and I''ve also printed out
    the decision tree itself, so it''s kind of cool. Now, let''s bring up that Excel
    document once more so we can compare it to the output:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ad3fb46a-ef3e-442a-91c1-1971c3616db4.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
- en: 'We can walk through this and see what it means. So, in our output decision
    tree we actually end up with a depth of four, with nine different nodes, and,
    again, if we remind ourselves what these different fields correlate to, the way
    to read this is: If (feature 1 in 0), so that means if the employed is No, then
    we drop down to feature 5\. This list is zero-based, so feature 5 in our Excel
    document is internships. We can run through the tree like that: this person is
    not currently employed, did not do an internship, has no prior years of experience
    and has a Bachelor''s degree, we would not hire this person. Then we get to the
    Else clauses. If that person had an advanced degree, we would hire them, just
    based on the data that we had that we trained it on. So, you can work out what
    these different feature IDs mean back to your original source data, remember,
    you always start counting at 0, and interpret that accordingly. Note that all
    the categorical features are expressed in Boolean in this list of possible categories
    that it saw, whereas continuous data is expressed numerically as less than or
    greater than relationships.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: And there you have it, an actual decision tree built using Spark and MLlib that
    actually works and makes sense. Pretty awesome stuff.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: K-Means Clustering in Spark
  id: totrans-301
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Alright, let's look at another example of using Spark in MLlib, and this time
    we're going to look at k-means clustering, and just like we did with decision
    trees, we're going to take the same example that we did using scikit-learn and
    we're going to do it in Spark instead, so it can actually scale up to a massive
    Dataset. So, again, I've made sure to close out of everything else, and I'm going
    to go into my book materials and open up the `SparkKMeans` Python script, and
    let's study what's going on in.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/375a2436-ca5b-41bf-94d4-a80bedf814e1.png)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
- en: Alright, so again, we begin with some boilerplate stuff.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We're going to import the `KMeans` package from the clustering `MLlib` package,
    we're going to import array and random from `numpy`, because, again, we're free
    to use whatever you want, this is a Python script at the end of the day, and `MLlib`
    often does require `numpy` arrays as input. We're going to import the `sqrt` function
    and the usual boilerplate stuff, we need `SparkConf` and `SparkContext`, pretty
    much every time from `pyspark`. We're also going to import the scale function
    from `scikit-learn`. Again, it's OK to use `scikit-learn` as long as you make
    sure its installed in every machine that you're going to be running this job on,
    and also don't assume that `scikit-learn` will magically scale itself up just
    because you're running it on Spark. But, since I'm only using it for the scaling
    function, it's OK. Alright, let's go ahead and set things up.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: 'I''m going to create a global variable first:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'I''m going to run k-means clustering in this example with a K of 5, meaning
    with five different clusters. I''m then going to go ahead and set up a local `SparkConf`
    just running on my own desktop:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: I'm going to set the name of my application to `SparkKMeans` and create a `SparkContext`
    object that I can then use to create RDDs that run on my local machine. We'll
    skip past the `createClusteredData` function for now, and go to the first line
    of code that gets run.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The first thing we're going to do is create an RDD by parallelizing in some
    fake data that I'm creating, and that's what the `createClusteredData` function
    does. Basically, I'm telling you to create 100 data points clustered around K
    centroids, and this is pretty much identical to the code that we looked at when
    we played with k-means clustering earlier in the book. If you want a refresher,
    go ahead and look back at that chapter. Basically, what we're going to do is create
    a bunch of random centroids around which we normally distribute some age and income
    data. So, what we're doing is trying to cluster people based on their age and
    income, and we are fabricating some data points to do that. That returns a `numpy`
    array of our fake data.
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once that result comes back from `createClusteredData`, I'm calling `scale`
    on it, and that will ensure that my ages and incomes are on comparable scales.
    Now, remember the section we studied saying you have to remember about data normalization?
    This is one of those examples where it is important, so we are normalizing that
    data with `scale` so that we get good results from k-means.
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: And finally, we parallelize the resulting list of arrays into an RDD using `parallelize`.
    Now our data RDD contains all of our fake data. All we have to do, and this is
    even easier than a decision tree, is call `KMeans.train` on our training data.
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We pass in the number of clusters we want, our K value, a parameter that puts
    an upper boundary on how much processing it's going to do; we then tell it to
    use the default initialization mode of k-means where we just randomly pick our
    initial centroids for our clusters before we start iterating on them, and back
    comes the model that we can use. We're going to call that `clusters`.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: Alright, now we can play with that cluster.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by printing out the cluster assignments for each one of our points.
    So, we''re going to take our original data and transform it using a lambda function:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: This function is just going to transform each point into the cluster number
    that is predicted from our model. Again, we're just taking our RDD of data points.
    We're calling `clusters.predict` to figure out which cluster our k-means model
    is assigning them to, and we're just going to put the results in our `resultRDD`.
    Now, one thing I want to point out here is this cache call, in the above code.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: An important thing when you're doing Spark is that any time you're going to
    call more than one action on an RDD, it's important to cache it first, because
    when you call an action on an RDD, Spark goes off and figures out the DAG for
    it, and how to optimally get to that result.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: It will go off and actually execute everything to get that result. So, if I
    call two different actions on the same RDD, it will actually end up evaluating
    that RDD twice, and if you want to avoid all of that extra work, you can cache
    your RDD in order to make sure that it does not recompute it more than once.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: 'By doing that, we make sure these two subsequent operations do the right thing:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: In order to get an actual result, what we're going to do is use `countByValue`,
    and what that will do is give us back an RDD that has how many points are in each
    cluster. Remember, `resultRDD` currently has mapped every individual point to
    the cluster it ended up with, so now we can use `countByValue` to just count up
    how many values we see for each given cluster ID. We can then easily print that
    list out. And we can actually look at the raw results of that RDD as well, by
    calling `collect` on it, and that will give me back every single points cluster
    assignment, and we can print out all of them.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: Within set sum of squared errors (WSSSE)
  id: totrans-327
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, how do we measure how good our clusters are? Well, one metric for that
    is called the Within Set Sum of Squared Errors, wow, that sounds fancy! It''s
    such a big term that we need an abbreviation for it, WSSSE. All it is, we look
    at the distance from each point to its centroid, the final centroid in each cluster,
    take the square of that error and sum it up for the entire Dataset. It''s just
    a measure of how far apart each point is from its centroid. Obviously, if there''s
    a lot of error in our model then they will tend to be far apart from the centroids
    that might apply, so for that we need a higher value of K, for example. We can
    go ahead and compute that value and print it out with the following code:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: First of all, we define this `error` function that computes the squared error
    for each point. It just takes the distance from the point to the centroid center
    of each cluster and sums it up. To do that, we're taking our source data, calling
    a lambda function on it that actually computes the error from each centroid center
    point, and then we can chain different operations together here.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: First, we call `map` to compute the error for each point. Then to get a final
    total that represents the entire Dataset, we're calling `reduce` on that result.
    So, we're doing `data.map` to compute the error for each point, and then `reduce`
    to take all of those errors and add them all together. And that's what the little
    lambda function does. This is basically a fancy way of saying, "I want you to
    add up everything in this RDD into one final result." `reduce` will take the entire
    RDD, two things at a time, and combine them together using whatever function you
    provide. The function I'm providing it above is "take the two rows that I'm combining
    together and just add them up."
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: If we do that throughout every entry of the RDD, we end up with a final summed-up
    total. It might seem like a little bit of a convoluted way to just sum up a bunch
    of values, but by doing it this way we are able to make sure that we can actually
    distribute this operation if we need to. We could actually end up computing the
    sum of one piece of the data on one machine, and a sum of a different piece over
    on another machine, and then take those two sums and combine them together into
    a final result. This `reduce` function is saying, how do I take any two intermediate
    results from this operation, and combine them together?
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, feel free to take a moment and stare at this a little bit longer if
    you want it to sink in. Nothing really fancy going on here, but there are a few
    important points:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: We introduced the use of a cache if you want to make sure that you don't do
    unnecessary recomputations on an RDD that you're going to use more than once.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We introduced the use of the `reduce` function.
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have a couple of interesting mapper functions as well here, so there's a
    lot to learn from in this example.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the end of the day, it will just do k-means clustering, so let's go ahead
    and run it.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: Running the code
  id: totrans-338
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Go to the Tools menu, Canopy Command Prompt, and type in:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Hit Return, and off it will go. In this situation, you might have to wait a
    few moments for the output to appear in front of you, but you should see something
    like this:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eebf988f-bcec-4c78-ab13-d495aba80c3a.png)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
- en: It worked, awesome! So remember, the output that we asked for was, first of
    all, a count of how many points ended up in each cluster. So, this is telling
    us that cluster 0 had 21 points in it, cluster 1 had 20 points in it, and so on
    and so forth. It ended up pretty evenly distributed, so that's a good sign.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: Next, we printed out the cluster assignments for each individual point, and,
    if you remember, the original data that fabricated this data did it sequentially,
    so it's actually a good thing that you see all of the 3s together, and all the
    1s together, and all the 4s together, it looks like it started to get a little
    bit confused with the 0s and 2s, but by and large, it seems to have done a pretty
    good job of uncovering the clusters that we created the data with originally.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: And finally, we computed the WSSSE metric, it came out to 19.97 in this example.
    So, if you want to play around with this a little bit, I encourage you to do so.
    You can see what happens to that error metric as you increase or decrease the
    values of K, and think about why that may be. You can also experiment with what
    happens if you don't normalize all the data, does that actually affect your results
    in a meaningful way? Is that actually an important thing to do? And you can also
    experiment with the `maxIterations` parameter on the model itself and get a good
    feel of what that actually does to the final results, and how important it is.
    So, feel free to mess around with it and experiment away. That's k-means clustering
    done with MLlib and Spark in a scalable manner. Very cool stuff.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF
  id: totrans-346
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, our final example of MLlib is going to be using something called Term Frequency
    Inverse Document Frequency, or TF-IDF, which is the fundamental building block
    of many search algorithms. As usual, it sounds complicated, but it's not as bad
    as it sounds.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: So, first, let's talk about the concepts of TF-IDF, and how we might go about
    using that to solve a search problem. And what we're actually going to do with
    TF-IDF is create a rudimentary search engine for Wikipedia using Apache Spark
    in MLlib. How awesome is that? Let's get started.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF stands for Term Frequency and Inverse Document Frequency, and these are
    basically two metrics that are closely interrelated for doing search and figuring
    out the relevancy of a given word to a document, given a larger body of documents.
    So, for example, every article on Wikipedia might have a term frequency associated
    with it, every page on the Internet could have a term frequency associated with
    it for every word that appears in that document. Sounds fancy, but, as you'll
    see, it's a fairly simple concept.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: '**All Term Frequency** means is how often a given word occurs in a given document.
    So, within one web page, within one Wikipedia article, within one whatever, how
    common is a given word within that document? You know, what is the ratio of that
    word''s occurrence rate throughout all the words in that document? That''s it.
    That''s all term frequency is.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Document frequency**, is the same idea, but this time it is the frequency
    of that word across the entire corpus of documents. So, how often does this word
    occur throughout all of the documents that I have, all the web pages, all of the
    articles on Wikipedia, whatever. For example, common words like "a" or "the" would
    have a very high document frequency, and I would expect them to also have a very
    high term frequency, but that doesn''t necessarily mean they''re relevant to a
    given document.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can kind of see where we're going with this. So, let's say we have a very
    high term frequency and a very low document frequency for a given word. The ratio
    of these two things can give me a measure of the relevance of that word to the
    document. So, if I see a word that occurs very often in a given document, but
    not very often in the overall space of documents, then I know that this word probably
    conveys some special meaning to this particular document. It might convey what
    this document is actually about.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: So, that's TF-IDF. It just stands for Term Frequency x Inverse Document Frequency,
    which is just a fancy way of saying term frequency over document frequency, which
    is just a fancy way of saying how often does this word occur in this document
    compared to how often it occurs in the entire body of documents? It's that simple.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF in practice
  id: totrans-354
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In practice, there are a few little nuances to how we use this. For example,
    we use the actual log of the inverse document frequency instead of the raw value,
    and that's because word frequencies in reality tend to be distributed exponentially.
    So, by taking the log, we end up with a slightly better weighting of words, given
    their overall popularity. There are some limitations to this approach, obviously,
    one is that we basically assume a document is nothing more than a bagful of words,
    we assume there are no relationships between the words themselves. And, obviously,
    that's not always the case, and actually parsing them out can be a good part of
    the work, because you have to deal with things like synonyms and various tenses
    of words, abbreviations, capitalizations, misspellings, and so on. This gets back
    to the idea of cleaning your data being a large part of your job as a data scientist,
    and it's especially true when you're dealing with natural language processing
    stuff. Fortunately, there are some libraries out there that can help you with
    this, but it is a real problem and it will affect the quality of your results.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: Another implementation trick that we use with TF-IDF is, instead of storing
    actual string words with their term frequencies and inverse document frequency,
    to save space and make things more efficient, we actually map every word to a
    numerical value, a hash value we call it. The idea is that we have a function
    that can take any word, look at its letters, and assign that, in some fairly well-distributed
    manner, to a set of numbers in a range. That way, instead of using the word "represented",
    we might assign that a hash value of 10, and we can then refer to the word "represented"
    as "10" from now on. Now, if the space of your hash values isn't large enough,
    you could end up with different words being represented by the same number, which
    sounds worse than it is. But, you know, you want to make sure that you have a
    fairly large hash space so that is unlikely to happen. Those are called hash collisions.
    They can cause issues, but, in reality, there's only so many words that people
    commonly use in the English language. You can get away with 100,000 or so and
    be just fine.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: Doing this at scale is the hard part. If you want to do this over all of Wikipedia,
    then you're going to have to run this on a cluster. But for the sake of argument,
    we are just going to run this on our own desktop for now, using a small sample
    of Wikipedia data.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: Using TF- IDF
  id: totrans-358
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How do we turn that into an actual search problem? Once we have TF-IDF, we have
    this measure of each word's relevancy to each document. What do we do with it?
    Well, one thing you could do is compute TF-IDF for every word that we encounter
    in the entire body of documents that we have, and then, let's say we want to search
    for a given term, a given word. Let's say we want to search for "what Wikipedia
    article in my set of Wikipedia articles is most relevant to Gettysburg?" I could
    sort all the documents by their TF-IDF score for Gettysburg, and just take the
    top results, and those are my search results for Gettysburg. That's it. Just take
    your search word, compute TF-IDF, take the top results. That's it.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, in the real world there's a lot more to search than that. Google
    has armies of people working on this problem and it's way more complicated in
    practice, but this will actually give you a working search engine algorithm that
    produces reasonable results. Let's go ahead and dive in and see how it all works.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: Searching wikipedia with Spark MLlib
  id: totrans-361
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We're going to build an actual working search algorithm for a piece of Wikipedia
    using Apache Spark in MLlib, and we're going to do it all in less than 50 lines
    of code. This might be the coolest thing we do in this entire book!
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: 'Go into your course materials and open up the `TF-IDF.py` script, and that
    should open up Canopy with the following code:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/87797105-16f4-4546-bb34-a435f27ece1f.png)'
  id: totrans-364
  prefs: []
  type: TYPE_IMG
- en: Now, step back for a moment and let it sink in that we're actually creating
    a working search algorithm, along with a few examples of using it in less than
    50 lines of code here, and it's scalable. I could run this on a cluster. It's
    kind of amazing. Let's step through the code.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: Import statements
  id: totrans-366
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We're going to start by importing the `SparkConf` and `SparkContext` libraries
    that we need for any Spark script that we run in Python, and then we're going
    to import `HashingTF` and `IDF` using the following commands.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: So, this is what computes the term frequencies (`TF`) and inverse document frequencies
    (`IDF`) within our documents.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: Creating the initial RDD
  id: totrans-370
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We'll start off with our boilerplate Spark stuff that creates a local `SparkConfiguration`
    and a `SparkContext`, from which we can then create our initial RDD.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Next, we're going to use our `SparkContext` to create an RDD from `subset-small.tsv`.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: This is a file containing tab-separated values, and it represents a small sample
    of Wikipedia articles. Again, you'll need to change your path as shown in the
    preceding code as necessary for wherever you installed the course materials for
    this book.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: That gives me back an RDD where every document is in each line of the RDD. The
    `tsv` file contains one entire Wikipedia document on every line, and I know that
    each one of those documents is split up into tabular fields that have various
    bits of metadata about each article.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: 'The next thing I''m going to do is split those up:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: I'm going to split up each document based on their tab delimiters into a Python
    list, and create a new `fields` RDD that, instead of raw input data, now contains
    Python lists of each field in that input data.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, I''m going to map that data, take in each list of fields, extract
    field number three `x[3]`, which I happen to know is the body of the article itself,
    the actual article text, and I''m in turn going to split that based on spaces:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: What `x[3]` does is extract the body of the text from each Wikipedia article,
    and split it up into a list of words. My new `documents` RDD has one entry for
    every document, and every entry in that RDD contains a list of words that appear
    in that document. Now, we actually know what to call these documents later on
    when we're evaluating the results.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: 'I''m also going to create a new RDD that stores the document names:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: All that does is take that same `fields` RDD and uses this `map` function to
    extract the document name, which I happen to know is in field number one.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: So, I now have two RDDs, `documents`, which contains lists of words that appear
    in each document, and `documentNames`, which contains the name of each document.
    I also know that these are in the same order, so I can actually combine these
    together later on to look up the name for a given document.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: Creating and transforming a HashingTF object
  id: totrans-387
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, the magic happens. The first thing we''re going to do is create a `HashingTF`
    object, and we''re going to pass in a parameter of 100,000\. This means that I''m
    going to hash every word into one of 100,000 numerical values:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Instead of representing words internally as strings, which is very inefficient,
    it's going to try to, as evenly as possible, distribute each word to a unique
    hash value. I'm giving it up to 100,000 hash values to choose from. Basically,
    this is mapping words to numbers at the end of the day.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, I''m going to call `transform` on `hashingTF` with my actual RDD of documents:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: That's going to take my list of words in every document and convert it to a
    list of hash values, a list of numbers that represent each word instead.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: This is actually represented as a sparse vector at this point to save even more
    space. So, not only have we converted all of our words to numbers, but we've also
    stripped out any missing data. In the event that a word does not appear in a document
    where you're not storing the fact that word does not appear explicitly, it saves
    even more space.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: Computing the TF-IDF score
  id: totrans-395
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To actually compute the TF-IDF score for each word in each document, we first
    cache this `tf` RDD.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We do that because we''re going to use it more than once. Next, we use `IDF(minDocFreq=2)`,
    meaning that we''re going to ignore any word that doesn''t appear at least twice:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We call `fit` on `tf`, and then in the next line we call `transform` on `tf`:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: What we end up with here is an RDD of the TF-IDF score for each word in each
    document.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: Using the Wikipedia search engine algorithm
  id: totrans-403
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s try and put the algorithm to use. Let''s try to look up the best article
    for the word **Gettysburg**. If you''re not familiar with US history, that''s
    where Abraham Lincoln gave a famous speech. So, we can transform the word Gettysburg
    into its hash value using the following code:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We will then extract the TF-IDF score for that hash value into a new RDD for
    each document:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: What this does is extract the TF-IDF score for Gettysburg, from the hash value
    it maps to for every document, and stores that in this `gettysburgRelevance` RDD.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: 'We then combine that with the `documentNames` so we can see the results:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Finally, we can print out the answer:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Running the algorithm
  id: totrans-413
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, let's go run that and see what happens. As usual, to run the Spark script,
    we're not going to just hit the play icon. We have to go to Tools>Canopy Command
    Prompt. In the Command Prompt that opens up, we will type in `spark-submit TF-IDF.py`,
    and off it goes.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: We are asking it to chunk through quite a bit of data, even though it's a small
    sample of Wikipedia it's still a fair chunk of information, so it might take a
    while. Let's see what comes back for the best document match for Gettysburg, what
    document has the highest TF-IDF score?
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b7dfb9f6-8d3a-4af9-a230-6d0fccb02e54.png)'
  id: totrans-416
  prefs: []
  type: TYPE_IMG
- en: It's Abraham Lincoln! Isn't that awesome? We just made an actual search engine
    that actually works, in just a few lines of code.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: And there you have it, an actual working search algorithm for a little piece
    of Wikipedia using Spark in MLlib and TF-IDF. And the beauty is we can actually
    scale that up to all of Wikipedia if we wanted to, if we had a cluster large enough
    to run it.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully we got your interest up there in Spark, and you can see how it can
    be applied to solve what can be pretty complicated machine learning problems in
    a distributed manner. So, it's a very important tool, and I want to make sure
    you don't get through this book on data science without at least knowing the concepts
    of how Spark can be applied to big data problems. So, when you need to move beyond
    what one computer can do, remember, Spark is at your disposal.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: Using the Spark 2.0 DataFrame API for MLlib
  id: totrans-420
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter was originally produced for Spark 1, so let's talk about what's
    new in Spark 2, and what new capabilities exist in MLlib now.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: So, the main thing with Spark 2 is that they moved more and more toward Dataframes
    and Datasets. Datasets and Dataframes are kind of used interchangeably sometimes.
    Technically a dataframe is a Dataset of row objects, they're kind of like RDDs,
    but the only difference is that, whereas an RDD just contains unstructured data,
    a Dataset has a defined schema to it.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: A Dataset knows ahead of time exactly what columns of information exists in
    each row, and what types those are. Because it knows about the actual structure
    of that Dataset ahead of time, it can optimize things more efficiently. It also
    lets us think of the contents of this Dataset as a little, mini database, well,
    actually, a very big database if it's on a cluster. That means we can do things
    like issue SQL queries on it.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: This creates a higher-level API with which we can query and analyze massive
    Datasets on a Spark cluster. It's pretty cool stuff. It's faster, it has more
    opportunities for optimization, and it has a higher-level API that's often easier
    to work with.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: How Spark 2.0 MLlib works
  id: totrans-425
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Going forward in Spark 2.0, MLlib is pushing dataframes as its primary API.
    This is the way of the future, so let''s take a look at how it works. I''ve gone
    ahead and opened up the `SparkLinearRegression.py` file in Canopy, as shown in
    the following figure, so let''s walk through it a little bit:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/99003c6b-bbd0-4c0a-84c5-9c12af88be01.png)'
  id: totrans-427
  prefs: []
  type: TYPE_IMG
- en: As you see, for one thing, we're using `ml` instead of `MLlib`, and that's because
    the new dataframe-based API is in there.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: Implementing linear regression
  id: totrans-429
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, what we're going to do is implement linear regression, and
    linear regression is just a way of fitting a line to a set of data. What we're
    going to do in this exercise is take a bunch of fabricated data that we have in
    two dimensions, and try to fit a line to it with a linear model.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: 'We''re going to separate our data into two sets, one for building the model
    and one for evaluating the model, and we''ll compare how well this linear model
    does at actually predicting real values. First of all, in Spark 2, if you''re
    going to be doing stuff with the `SparkSQL` interface and using Datasets, you''ve
    got to be using a `SparkSession` object instead of a `SparkContext`. To set one
    up, you do the following:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Note that the middle bit is only necessary on Windows and in Spark 2.0\. It
    kind of works around a little bug that they have, to be honest. So, if you''re
    on Windows, make sure you have a `C:/temp` folder. If you want to run this, go
    create that now if you need to. If you''re not on Windows, you can delete that
    whole middle section to leave: `spark = SparkSession.builder.appName("LinearRegression").getOrCreate()`.'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: Okay, so you can say `spark`, give it an `appName` and `getOrCreate()`.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: This is interesting, because once you've created a Spark session, if it terminates
    unexpectedly, you can actually recover from that the next time that you run it.
    So, if we have a checkpoint directory, it can actually restart where it left off
    using `getOrCreate`.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we''re going to use this `regression.txt` file that I have included with
    the course materials:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-437
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: That is just a text file that has comma-delimited values of two columns, and
    they're just two columns of, more or less randomly, linearly correlated data.
    It can represent whatever you want. Let's imagine that it represents heights and
    weights, for example. So, the first column might represent heights, the second
    column might represent weights.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: In the lingo of machine learning, we talk about labels and features, where labels
    are usually the thing that you're trying to predict, and features are a set of
    known attributes of the data that you use to make a prediction from.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: In this example, maybe heights are the labels and the features are the weights.
    Maybe we're trying to predict heights based on your weight. It can be anything,
    it doesn't matter. This is all normalized down to data between -1 and 1\. There's
    no real meaning to the scale of the data anywhere, you can pretend it means anything
    you want, really.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: 'To use this with MLlib, we need to transform our data into the format it expects:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: The first thing we're going to do is split that data up with this `map` function
    that just splits each line into two distinct values in a list, and then we're
    going to map that to the format that MLlib expects. That's going to be a floating
    point label, and then a dense vector of the feature data.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we only have one bit of feature data, the weight, so we have a
    vector that just has one thing in it, but even if it's just one thing, the MLlib
    linear regression model requires a dense vector there. This is like a `labeledPoint`
    in the older API, but we have to do it the hard way here.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to actually assign names to those columns. Here''s the syntax
    for doing that:'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-446
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: We're going to tell MLlib that these two columns in the resulting RDD actually
    correspond to the label and the features, and then I can convert that RDD to a
    DataFrame object. At this point, I have an actual dataframe or, if you will, a
    Dataset that contains two columns, label and features, where the label is a floating
    point height, and the features column is a dense vector of floating point weights.
    That is the format required by MLlib, and MLlib can be pretty picky about this
    stuff, so it's important that you pay attention to these formats.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: Now, like I said, we're going to split our data in half.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-449
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: We're going to do a 50/50 split between training data and test data. This returns
    back two dataframes, one that I'm going to use to actually create my model, and
    one that I'm going to use to evaluate my model.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: I will next create my actual linear regression model with a few standard parameters
    here that I've set.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-452
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We''re going to call `lir = LinearRegression`, and then I will fit that model
    to the set of data that I held aside for training, the training data frame:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: That gives me back a model that I can use to make predictions from.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: Let's go ahead and do that.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: I will call `model.transform(testDF)`, and what that's going to do is predict
    the heights based on the weights in my testing Dataset. I actually have the known
    labels, the actual, correct heights, and this is going to add a new column to
    that dataframe called predictions, that has the predicted values based on that
    linear model.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: 'I''m going to cache those results, and now I can just extract them and compare
    them together. So, let''s pull out the prediction column, just using `select`
    like you would in SQL, and then I''m going to actually transform that dataframe
    and pull out the RDD from it, and use that to map it to just a plain old RDD full
    of floating point heights in this case:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-460
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'These are the predicted heights. Next, we''re going to get the actual heights
    from the label column:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-462
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Finally, we can zip them back together and just print them out side by side
    and see how well it does:'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-464
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: This is kind of a convoluted way of doing it; I did this to be more consistent
    with the previous example, but a simpler approach would be to just actually select
    prediction and label together into a single RDD that maps out those two columns
    together and then I don't have to zip them up, but either way it works. You'll
    also note that right at the end there we need to stop the Spark session.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
- en: So let's see if it works. Let's go up to Tools, Canopy Command Prompt, and we'll
    type in `spark-submit SparkLinearRegression.py` and let's see what happens.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
- en: There's a little bit more upfront time to actually run these APIs with Datasets,
    but once they get going, they're very fast. Alright, there you have it.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/93ec3caf-319d-47fa-8712-266370ee6bcb.png)'
  id: totrans-468
  prefs: []
  type: TYPE_IMG
- en: Here we have our actual and predicted values side by side, and you can see that
    they're not too bad. They tend to be more or less in the same ballpark. There
    you have it, a linear regression model in action using Spark 2.0, using the new
    dataframe-based API for MLlib. More and more, you'll be using these APIs going
    forward with MLlib in Spark, so make sure you opt for these when you can. Alright,
    that's MLlib in Spark, a way of actually distributing massive computing tasks
    across an entire cluster for doing machine learning on big Datasets. So, good
    skill to have. Let's move on.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-470
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we started with installing Spark, then moved to introducing
    Spark in depth while understanding how Spark works in combination with RDDs. We
    also walked through various ways of creating RDDs while exploring different operations.
    We then introduced MLlib, and stepped through some detailed examples of decision
    trees and K-Means Clustering in Spark. We then pulled off our masterstroke of
    creating a search engine in just a few lines of code using TF-IDF. Finally, we
    looked at the new features of Spark 2.0.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll take a look at A/B testing and experimental design.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
