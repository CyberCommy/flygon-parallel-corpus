- en: Optimization - Going Down the Hill with Gradient Descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing a quadratic cost function and finding the minima using just math
    to gain insight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coding a quadratic cost function optimization using Gradient Descent (GD) from
    scratch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coding Gradient Descent optimization to solve Linear regression from scratch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normal equations as an alternative to solve Linear regression in Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Understanding how optimization works is fundamental for a successful career
    in machine learning. We picked the **Gradient Descent** (**GD**) method for an
    end-to-end deep dive to demonstrate the inner workings of an optimization technique.
    We will develop the concept using three recipes that walk the developer from scratch
    to a fully developed code to solve an actual problem with real-world data. The
    fourth recipe explores an alternative to GD using Spark and normal equations (limited
    scaling for big data problems) to solve a regression problem.
  prefs: []
  type: TYPE_NORMAL
- en: Let's get started. How does a machine learn anyway? Does it really learn from
    its mistakes? What does it mean when the machine finds a solution using optimization?
  prefs: []
  type: TYPE_NORMAL
- en: 'At a high level, machines learn based on one of the following five techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Error based learning**: In this technique, we search the domain space for
    a combination of parameter values (weights) that minimize the total error (predicted
    versus actual) over the training data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Information theory learning**: This method uses concepts such as entropy
    and information gain that are found in classical Shannon Information theory. The
    tree-based ML system, rooted classically in the ID3 algorithm, fits well in this
    category. The ensemble tree models will be the crowning achievement of this category.
    We will discuss tree models in [Chapter 10](part0460.html#DMM2O0-4d291c9fed174a6992fd24938c2f9c77),
    *Building Machine Learning Systems with Decision Tree and Ensemble Models*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Probability Space Learning**: This branch of machine learning is based on
    Bayes theorem ([https://en.wikipedia.org/wiki/Bayes''_theorem)](https://en.wikipedia.org/wiki/Bayes''_theorem)).
    The most well-known method in machine learning is Naïve Bayes (multiple variations).
    Naïve Bayes culminates with an introduction of the Bayes Network which allows
    for more control over the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Similarity measure learning***:* This method works by attempting to define
    a measure of similarity and then fitting an observation''s grouping based on that
    measure. The best known method is KNN (nearest neighbor), which is the standard
    in any ML toolkit. Spark ML implements K-means++ with parallelism referred to
    as K-Means|| (K Means Parallel).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Genetic algorithms (GA) and evolutionary learning**: This can be viewed as
    Darwin''s theory (Origin of the Species) applied to optimization and machine learning.
    The idea behind GA is to use a recursive generative algorithm to create a set
    of initial candidates and then use feedback (fitness landscape) to eliminate distant
    candidates, fold similar ones while randomly introducing mutations (numerical
    or symbolic jitter) to unlikely candidates, and then repeat until the solution
    is found.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some of the data scientist and ML engineers prefer to think of optimization
    as maximizing the log likelihood rather than minimizing the cost function - they
    are really the two sides of the same coin! In this chapter, we will focus on error-based
    learning, especially **Gradient Descent**.
  prefs: []
  type: TYPE_NORMAL
- en: To provide a solid understanding, we will deep dive into Gradient Descent (GD)
    by going through three GD recipes as they apply to optimization. We will then
    provide Spark's Normal Equation recipe as an alternative to numerical optimization
    methods, such as Gradient Descent (GD) or the **Limited-memory Broyden-Fletcher-
    Goldfarb-Shanno** (**LBFGS**) algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark provides an excellent coverage for all categories. The following
    figure depicts a taxonomy that will guide you through your journey in the field
    of numerical optimization, which is fundamental to achieving excellence in ML.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00188.gif)'
  prefs: []
  type: TYPE_IMG
- en: How do machines learn using an error-based system?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machines learn pretty much the same way we do--they learn from their mistakes.
    They first start by making an initial guess (random weights for parameters). Second,
    they use their model (for example, GLM, RRN, isotonic regression) to make a prediction
    (for example, a number). Third, they look at what the answer should have been
    (training set). Fourth, they measure the difference between actual versus predicted
    answers using a variety of techniques (such as least squares, similarity, and
    so on).
  prefs: []
  type: TYPE_NORMAL
- en: Once all these mechanics are in place, they keep repeating the process over
    the entire training dataset, while trying to come up with a combination of parameters
    that has the minimal error when they consider the entire training dataset. What
    makes this interesting is that each branch of machine learning uses math or domain
    known facts to avoid a brute-force combinatorics approach which will not terminate
    in real-world settings.
  prefs: []
  type: TYPE_NORMAL
- en: The error-based ML optimization is a branch of mathematical programming (MP)
    which is implemented algorithmically, but with limited accuracy (varying accuracy
    of 10^(-2) to 10^(-6)). Most, if not all, the methods in this category take advantage
    of simple calculus facts like first derivative (slope), such as the GD technique,
    and second derivative (curvature), such as the BFGS technique, to minimize a cost
    function. In the case of BFGS, the invisible hands are the updater function (L1
    updater), rank (second rank update), approximating the final answer/solution using
    a Hessian free technique ([https://en.wikipedia.org/wiki/Hessian_matrix](https://en.wikipedia.org/wiki/Hessian_matrix))
    without the actual second derivative matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure depicts some of the facilities that touch on optimization
    in Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '**![](img/00189.gif)**'
  prefs: []
  type: TYPE_NORMAL
- en: The functions to do SGD and LBFGS optimization are available by themselves in
    Spark. To utilize them, you should be able to write and supply your own cost function.
    The functions, such as `runMiniBatchSGD()`, are not only marked private, but also
    require good understanding of the implementation of both algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since this is a cookbook and we cannot go deep into optimization theory, for
    background and reference we recommend the following books from our library:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Optimization (2013)**:[https://www.amazon.com/Optimization-Springer-Texts-Statistics-Kenneth/dp/1461458374/ref=sr_1_8?ie=UTF8&qid=1485744639&sr=8-8&keywords=optimization](https://www.amazon.com/Optimization-Springer-Texts-Statistics-Kenneth/dp/1461458374/ref=sr_1_8?ie=UTF8&qid=1485744639&sr=8-8&keywords=optimization)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimization for Machine Learning (2011)**:[https://www.amazon.com/Optimization-Machine-Learning-Information-Processing/dp/026201646X/ref=sr_1_1?ie=UTF8&qid=1485744817&sr=8-1&keywords=optimization+for+machine+learning](https://www.amazon.com/Optimization-Machine-Learning-Information-Processing/dp/026201646X/ref=sr_1_1?ie=UTF8&qid=1485744817&sr=8-1&keywords=optimization+for+machine+learning)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Convex Optimization (2004)**:[https://www.amazon.com/Convex-Optimization-Stephen-Boyd/dp/0521833787/ref=pd_sim_14_2?_encoding=UTF8&psc=1&refRID=7T88DJY5ZWBEREGJ4WT4](https://www.amazon.com/Convex-Optimization-Stephen-Boyd/dp/0521833787/ref=pd_sim_14_2?_encoding=UTF8&psc=1&refRID=7T88DJY5ZWBEREGJ4WT4)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Genetic Algorithm in Search, Optimization and Machine Learning (1989) - a
    classic!**:[https://www.amazon.com/Genetic-Algorithms-Optimization-Machine-Learning/dp/0201157675/ref=sr_1_5?s=books&ie=UTF8&qid=1485745151&sr=1-5&keywords=genetic+programming](https://www.amazon.com/Genetic-Algorithms-Optimization-Machine-Learning/dp/0201157675/ref=sr_1_5?s=books&ie=UTF8&qid=1485745151&sr=1-5&keywords=genetic+programming)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Swarm Intelligence from Natural to Artificial Systems (1999)**:[https://www.amazon.com/Swarm-Intelligence-Artificial-Institute-Complexity/dp/0195131592/ref=sr_1_3?s=books&ie=UTF8&qid=1485745559&sr=1-3&keywords=swarm+intelligence](https://www.amazon.com/Swarm-Intelligence-Artificial-Institute-Complexity/dp/0195131592/ref=sr_1_3?s=books&ie=UTF8&qid=1485745559&sr=1-3&keywords=swarm+intelligence)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing a quadratic cost function and finding the minima using just math
    to gain insight
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will explore the fundamental concept behind mathematical
    optimization using simple derivatives before introducing Gradient Descent (first
    order derivative) and L-BFGS, which is a Hessian free quasi-Newton method.
  prefs: []
  type: TYPE_NORMAL
- en: We will examine a sample quadratic cost/error function and show how to find
    the minimum or maximum with just math.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00190.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We will use both the closed form (vertex formula) and derivative method (slope)
    to find the minima, but we will defer to later recipes in this chapter to introduce
    numerical optimization techniques, such Gradient Descent and its application to
    regression.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s assume we have a quadratic cost function and we find its minima:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00191.gif)'
  prefs: []
  type: TYPE_IMG
- en: The cost function in statistical machine learning algorithms acts as a proxy
    for the level of difficulty, energy spent, or total error as we move around in
    our search space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first thing we do is to graph the function and inspect it visually.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00192.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Upon visual inspection, we see that ![](img/00193.gif)  is a concave function
    with its minima at (2,1).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Our next step would be to find the minima by optimizing the function. Some examples
    of presenting the cost or error function in machine learning could be squared
    error, Euclidian distance, MSSE, or any other similarity measure that can capture
    how far we are from an optimal numeric answer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The next step is to search for best parameter values that minimize errors (for
    example, cost) in our ML technique. For example, by optimizing a linear regression
    cost function (sum of squared errors), we arrive at best values for its parameter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Derivative method: Set the first derivative to zero and solve'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vertex method: Use closed algebraic form'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First, we solve for minima using the derivative method by computing the first
    derivative, setting it to zero, and solving for *x* and *y*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00194.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Given f(x) = 2x² - 8x +9 as our cost/error function, the derivative can be
    computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00195.jpeg)![](img/00196.gif)'
  prefs: []
  type: TYPE_IMG
- en: '[Power rule: ![](img/00197.gif)]'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00198.gif) [We set the derivative equal to 0 and solve for![](img/00199.gif)]'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00200.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We now verify the minima using the vertex formula method. To compute the minima
    using the algebraic method please see the following steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the function, ![](img/00201.gif), the vertex can be found at:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00202.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s compute the minima using the vertex algebraic formula:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00203.gif)![](img/00204.gif)![](img/00205.gif)![](img/00206.gif)![](img/00207.gif)![](img/00208.gif)2(2)2
    + (-8) (2) +9![](img/00209.gif)'
  prefs: []
  type: TYPE_IMG
- en: As the last step, we check the result of steps 4 and 5 to make sure that our
    answer using a closed algebraic form yielding the minimum of (2, 1), is consistent
    with the derivative method which also yields (2, 1).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the last step, we show a pictorial view of *f(x)* in the left panel along
    with its derivative on the right panel, so you can visually inspect the answer
    for yourself.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00210.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, a casual inspection depicts that the minima vertex is at (2,1)
    on the left hand side *{ x=2, f(x)=1 }* while the right hand side chart shows
    the derivative of the function with respect to *X* (only the parameter) with its
    minima at *X=2*. As seen in the previous steps, we set the derivative of the function
    to zero and solve for *X* which results in number 2\. You can also visually inspect
    the two panels and equations to make sure *X=2* is true and makes sense in both
    cases.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have two techniques at our disposal for finding the minima of a quadratic
    function without using a numerical method. In real-life statistical machine learning
    optimization, we use the derivatives to find the minima of a convex function.
    If the function is convex (or the optimization is bonded), there is only one local
    minima, so the work is much simpler than non-linear/non-convex problems that are
    present in deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the derivative method in the preceding recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we found the derivative by applying the derivative rules (for example,
    exponent).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Second, we took advantage of the fact that, for a given simple quadratic function
    (convex optimization), the minima occurs when the slope of the first derivative
    is zero.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Third, we simply found the derivative by following and applying mechanical calculus
    rules.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fourth, we set the derivative of the function to zero ![](img/00211.gif)and
    solved for x
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fifth, we used the x value and plugged it into the original equation to find
    y. Using steps 1 through 5, we ended up with the minima at point (2, 1).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most statistical machine learning algorithms define and search a domain space
    while using a cost or error function to arrive at a best numerically approximated
    solution (for example, parameters of a regression). The point at which the function
    has its minima (minimizing cost/error) or its maxima (maximizing log likelihood)
    is where the best solution with minimal error (the best approximation) exists.
  prefs: []
  type: TYPE_NORMAL
- en: A quick refresher for differentiation rules can be found at: [https://en.wikipedia.org/wiki/Differentiation_rules](https://en.wikipedia.org/wiki/Differentiation_rules) [and ](https://en.wikipedia.org/wiki/Differentiation_rules)[https://www.math.ucdavis.edu/~kouba/Math17BHWDIRECTORY/Derivatives.pdf](https://www.math.ucdavis.edu/~kouba/Math17BHWDIRECTORY/Derivatives.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: A more mathematical write up for minimizing quadratic functions can be found
    at: [http://www.cis.upenn.edu/~cis515/cis515-11-sl12.pdf](http://www.cis.upenn.edu/~cis515/cis515-11-sl12.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: A scientific write up for quadratic function optimization and forms from MIT
    can be found at: [https://ocw.mit.edu/courses/sloan-school-of-management/15-084j-nonlinear-programming-spring-2004/lecture-notes/lec4_quad_form.pdf](https://ocw.mit.edu/courses/sloan-school-of-management/15-084j-nonlinear-programming-spring-2004/lecture-notes/lec4_quad_form.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A lengthy write up for quadratic equation from UCSC can be found at: [https://people.ucsc.edu/~miglior/chapter%20pdf/Ch08_SE.pdf](https://people.ucsc.edu/~miglior/chapter%20pdf/Ch08_SE.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Quadratic functions can be expressed as one of the following forms:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| **Quadratic function ax² + bx + c form** | **Standard form of quadratic function**
    |'
  prefs: []
  type: TYPE_TB
- en: '| ![](img/00212.gif) | ![](img/00213.gif) |'
  prefs: []
  type: TYPE_TB
- en: Where *a, b* and *c* are real numbers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure provides a quick reference to minima/maxima and the parameters
    regulating the convex/concave look and feel of the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00214.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Coding a quadratic cost function optimization using Gradient Descent (GD) from
    scratch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will code an iterative numerical optimization technique called
    gradient descent (GD) to find the minimum of a quadratic function *f(x) = 2x²
    - 8x +9*.
  prefs: []
  type: TYPE_NORMAL
- en: The focus here shifts from using math to solve for the minima (setting the first
    derivative to zero) to an iterative numerical method called Gradient Descent (GD)
    which starts with a guess and then gets closer to the solution in each iteration
    using a cost/error function as the guideline.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up the path using the package directive: `package spark.ml.cookbook.chapter9`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Import the necessary packages.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `scala.util.control.Breaks` will allow us to break out of the program. We
    use this during the debugging phase only when the program fails to converge or
    gets stuck in a never ending process (for example, when the step size is too large).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This step defines the actual quadratic function that we are trying to minimize:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This step defines the derivative of the function. It is what is referred to
    as the gradient at point x. It is the first order derivative of function *f(x)
    = 2x^2 - 8x + 9*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In this step, we setup a random starting point (set to 13 here). This would
    become our initial starting point on the *x* axis.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We proceed to set up the actual minima calculated from the previous recipe,
    *O**ptimizing a quadratic cost function and finding the minima using just math
    to gain insight*, so we can compute our estimate with each iteration versus actual.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This point is trying to act as the labels that you would provide during the
    training phase of an ML algorithm. In a real-life setting, we would have a training
    dataset with labels and would let the algorithm train and adjust its parameters
    accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Set up bookkeeping variables and declare `ArrayBuffer` data structures to store
    the cost (error) plus the estimated minima for inspection and graphing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The internal control variables for the gradient descent algorithm get set in
    this step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `stepSize`, also referred to as the learning rate, guides the program in
    how much to move each time, while tolerance helps the algorithm to stop when we
    get sufficiently close to the minima.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first set up a loop to iterate and stop when we are close enough to the
    minima, based on the desired tolerance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We update the minima each time and call on the function to calculate and return
    the derivative value at the current updated point:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We decide how much to move by first taking the derivative value returned by
    the last step and multiplying that by the step size (that is, we scale it). We
    then proceed to update the current minima and decrease it by the movement (derivative
    value x step size):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We compute our cost function value (error) by using a very simple square distance
    formula. In real life, the actual minima will be derived from training, but here
    we use the value from the previous recipe, *Optimizing a quadratic cost function
    and finding the minima using just math to gain insight*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We produce some intermediate output results so you can observe the behavior
    of currentMinimum at each iteration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The following statement is included as a reminder that, regardless of how an
    optimization algorithm is implemented, it should always provide means for exiting
    from a non-converging algorithm (that is, it should guard against user input and
    edge cases):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output cost and minima vectors that we collected in each iteration for
    later analysis and graphing are:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We define and set the variables that are the final minima and the actual function
    value *f(minima)*. They act as (X,Y) for where the minima is located:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We print our final results that match our computation in the recipe, *Optimizing
    a quadratic cost function and finding the minima using just math to gain insight*,
    using the iterative method. The final output should read as our minimum is located
    as (2,1), which can be either visually or computationally checked via the recipe,
    *Optimizing a quadratic cost function and finding the minima using just math to
    gain insight*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The process finished with the exit code 0
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Gradient Descent technique takes advantage of the fact that the gradient
    of the function (first derivative in this case) points to the direction of the
    descent. Conceptually, Gradient Descent (GD) optimizes a cost or error function
    to search for the best parameter for the model. The following figure demonstrates
    the iterative nature of the gradient descent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00215.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We started the recipe by defining the step size (the learning rate), tolerance,
    the function to be differentiated, and the function's first derivative, and then
    proceeded to iterate and get closer to the target minima of zero from the initial
    guess (13 in this case).
  prefs: []
  type: TYPE_NORMAL
- en: At each iteration, we calculated the gradient of the point (the first derivative
    at that point) and then scaled it using the step size to regulate the amount of
    each move. Since we are descending, we subtracted the scaled gradient from the
    old point to find the next point closer to the solution (to minimize the error).
  prefs: []
  type: TYPE_NORMAL
- en: There is some confusion as to whether the gradient value should be added or
    subtracted to arrive at the new point, which we try to clarify next. The guiding
    principle should be whether the slope is negative or positive. To move in the
    right direction, you must move in the direction of the first derivative (gradient).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table and figure provide a guideline for the GD update step:'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![](img/00216.gif) ***< 0****Negative Gradient* | ![](img/00217.gif) ***>
    0****Positive Gradient* |'
  prefs: []
  type: TYPE_TB
- en: '| ![](img/00218.gif) | ![](img/00219.gif) |'
  prefs: []
  type: TYPE_TB
- en: '![](img/00220.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The following figure depicts the inner workings of a single step (the negative
    slope depicted) in which we either subtract or add the gradient from the starting
    point to arrive at the next point that will put us one step closer to the minimum
    of the quadratic function. For example, in this recipe, we start from 13 and after
    200+ iterations (depends on the learning rate), we end up at the minima of (2,1),
    which matches the solution found in the recipe, *Optimizing a quadratic cost function
    and finding the minima using just math to gain insight* of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00221.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: To understand the steps better, let's try to follow a step from the left-hand
    side of the preceding graph for a simple ![](img/00222.gif) function. In this
    case, we are on the left-hand side of the curve (the original guess was a negative
    number) and we are trying to climb down and increase X with each iteration in
    the direction of the gradient (the first derivative)
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will walk you through the next figure to demonstrate the
    core concept and the steps in the recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the derivative at the given point--the gradient.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the gradient from step 1 and scale it by step size--the amount of the move.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Find the new position by subtracting the amount of movement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Negative gradient case**: In the following figure, we subtract the negative
    gradient (effectively adding the gradient) to the original point so end up climbing
    down toward the minima of ![](img/00223.gif)at zero. The graph depicted in the
    figure matches this case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Positive gradient case**: If we were on the other side of the curve with
    the positive gradient, we then subtract the positive gradient number from the
    previous position (effectively subtracting the gradient) to climb down toward
    the minima. The code in this recipe matches this case, in which we are trying
    to start from a positive number 13 (the initial guess) and move toward the minima
    at 0 in an iterative fashion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update the parameters and move to the new point.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We keep repeating these steps untill we converge to a solution and, hence, minimize
    the function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00224.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: It is important to note that Gradient Descent (GD) and its variations use the
    first order derivative, which means that they are curvature ignorant, while the
    second order derivative algorithms such as Newton or quasi-Newton (BFGS, LBFGS)
    methods use both gradient and curvature with or without a Hessian matrix (a partial
    directives matrix with respect to each variable).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The alternative to GD would be to search the entire domain space for the optimal
    setting, which is neither practical, nor will it ever terminate in a practical
    sense, due to the size and scale of real-life big data ML problems.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The step size or learning rate is very important to master when you first start
    with GD. If the step size is too small, it results in computational wastage and
    gives the appearance that the gradient descent is not converging to a solution.
    While setting the step size is trivial for demos and small projects, setting it
    to a wrong value can lead to a high computational loss on large ML projects. On
    the other hand, if the step size is too large, we end up with a ping-pong situation
    or moving away from convergences that usually shows up as a blown up error curve,
    meaning that the error increases rather than decreasing with each iteration.
  prefs: []
  type: TYPE_NORMAL
- en: In our experience, it is best to look at the error versus iteration chart and
    use the knee to pinpoint the right value. The alternative is to try .01, .001,......0001
    and see how the convergence proceeds with each iteration (steps too small or too
    large). It is helpful to remember that the step size is just a scaling factor,
    since the actual gradient at the point might be too large for the movement (it
    will jump over the minimum).
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize:'
  prefs: []
  type: TYPE_NORMAL
- en: If the step size is too small, then you have slow convergence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the step size is too large, you will end up skipping the minima (over-shooting),
    resulting in either slow computation or a ping-pong effect (getting stuck).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following figure depicts variations based on different step sizes to demonstrate
    the points mentioned previously.
  prefs: []
  type: TYPE_NORMAL
- en: '**Scenario 1**: Step size = .01 - The step size is appropriate - just a bit
    too small but it gets the job done in around 200 iterations. We don''t like to
    see anything under 200 because it must be general purpose enough to survive in
    real life.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scenario 2**: Step size = .001 - The step size is too small and results in
    slow convergence. While it does not seem to be that bad (1,500+ iterations), it
    might be considered too fine grained.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scenario 3**: Step size = .05 - The step size is too large. In this case,
    the algorithm gets stuck and keeps going back and forth without ever converging.
    It can''t be emphasized enough that you must think of an exit policy in case this
    occurs in real life (the nature and distribution of data changes a lot, so be
    prepared).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scenario 4**: Step size = .06 - The step size is too large resulting in non-convergence
    and blow up. The error curve blows up (it increases in a non-linear way) meaning
    that errors get larger with each iteration rather than getting smaller. In practice,
    we see more of this case (scenario 4) than the previous scenario, but both can
    happen so you should be ready for both. As you can see, a small .01 difference
    in step size between scenarios 3 and 4 made a difference in how the GD behaved.
    This is the same problem (optimization) that makes algorithmic trading difficult.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00225.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: It is worth mentioning that, for this type of smooth convex optimization problem,
    the local minima are often the same as the global minima. You can think of local
    minima/maxima as extreme values within a given range. For the same function, the
    global minima/maxima refers to the global or the most absolute value in the entire
    range of the function.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Stochastic Gradient Descent: There are multiple variations of Gradient Descent
    (GD), with Stochastic Gradient Descent (SGD) being the most talked about. Apache
    Spark supports the Stochastic Gradient Descent (SGD) variation, in which we update
    the parameters with a subset of training data - which is a bit challenging since
    we need to update the parameters simultaneously. There are two main differences
    between SGD and GD. The first difference is that SGD is an online learning/optimization
    technique while GD is more of an offline learning/optimization technique. The
    second difference between SGD versus GD is the speed of convergence due to not
    needing to examine the entire dataset before updating any parameter. This difference
    is depicted in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00226.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We can set the batch-window size in Apache Spark to make the algorithm more
    responsive to massive datasets (there is no need to traverse the whole dataset
    at once). There will be some randomness associated with SGD, but overall it is
    the "de facto" method used these days. It is a lot faster and can converge much
    faster.
  prefs: []
  type: TYPE_NORMAL
- en: In both GD and SGD cases, you search for the best parameter for the model by
    updating the original parameters. The difference is that, in the core GD, you
    have to run through all your data points to do a single update for a parameter
    in a given iteration as opposed to SGD, in which you look at each single (or mini-batch)
    sample from the training dataset to update the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a short general purpose write up, a good place to start is with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: GD :[https://en.wikipedia.org/wiki/Gradient_descent](https://en.wikipedia.org/wiki/Gradient_descent)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SGD:[https://en.wikipedia.org/wiki/Stochastic_gradient_descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A more mathematical treatment from CMU, Microsoft, and the Journal of Statistical
    software can be found at:'
  prefs: []
  type: TYPE_NORMAL
- en: 'CMU: [https://www.cs.cmu.edu/~ggordon/10725-F12/slides/05-gd-revisited.pdf](https://www.cs.cmu.edu/~ggordon/10725-F12/slides/05-gd-revisited.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[MS :](https://www.cs.cmu.edu/~ggordon/10725-F12/slides/05-gd-revisited.pdf)
    [http://cilvr.cs.nyu.edu/diglib/lsml/bottou-sgd-tricks-2012.pdf](http://cilvr.cs.nyu.edu/diglib/lsml/bottou-sgd-tricks-2012.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jstat:[https://arxiv.org/pdf/1509.06459v1.pdf](https://arxiv.org/pdf/1509.06459v1.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coding Gradient Descent optimization to solve Linear Regression from scratch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will explore how to code Gradient Descent to solve a Linear
    Regression problem. In the previous recipe, we demonstrated how to code GD to
    find the minimum of a quadratic function.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe demonstrates a more realistic optimization problem in which we optimize
    (minimize) the least square cost function to solve the linear regression problem
    in Scala on Apache Spark 2.0+. We will use real data and run our algorithm and
    compare the result to a tier-1 commercially available statistic software to demonstrate
    accuracy and speed.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We start by downloading the file from Princeton University which contains the
    following data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00227.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Princeton University'
  prefs: []
  type: TYPE_NORMAL
- en: Download source: [http://data.princeton.edu/wws509/datasets/#salary](http://data.princeton.edu/wws509/datasets/#salary).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To keep things simple, we then select the `yr` and `sl` to study how the number
    of years in rank influences the salary. To cut down on data wrangling code, we
    save those two columns in a file (`Year_Salary.csv`), as depicted in the following
    table, to study their linear relationship:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00228.gif)'
  prefs: []
  type: TYPE_IMG
- en: We visually inspect the data by using a scatter plot from the IBM SPSS package.
    It cannot be emphasized enough that a visual inspection should be the first step
    in any data science project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00229.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We use the import package to place the code in the desired place:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`package spark.ml.cookbook.chapter9`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first four statements import the necessary packages for the JFree chart
    package so we can graph the GD error and convergence in the same code base. The
    fifth import takes care of `ArrayBuffer`, which we use to store intermediate results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the data structure to hold intermediate results as we minimize errors
    and converge to a solution for the slope (`mStep`) and intercept (`bStep`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the functions for graphing via the JFree chart. The first one just displays
    the chart and the second one sets the chart properties. This is a boiler-plate
    code that you can customize based on your preference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This function computes the error based on the least square principle which we
    minimize to find the best fitting solution. The function finds the difference
    between what we predict and what the actual value (the salary) is available via
    the training data. After finding the difference, it squares them to compute the
    total error. The pow() function is a Scala math function to compute the square.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00230.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Wikipedia'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The next function computes two gradients (the first derivative) of the *f(x)=
    b + mx* and averages them over the domain (all points). It is the same process
    as in the second recipe, except that we need partial derivatives (gradient) because
    we are minimizing two parameters `m` and `b` (slope and intercept) and not just
    one.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the last two lines, we scale the gradient by multiplying it by the learning
    rate (step size). The reason we do that is to make sure we don't end up with large
    step sizes and overshoot the minimum, resulting in either a ping-pong scenario
    or error blow up, as discussed in the previous recipe.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This function reads and parses the CSV file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The following is a wrapper function that loops for N number of iterations and
    calls on the `step_gradient()` function to compute the gradient at a given point.
    We then proceed to store the results from every step one by one for processing
    later (for example, graphing).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Noteworthy is the use of `Tuple2()` to hold the return value from the `step_gradient()`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: In the last steps of the function we call on the `compute_error_for_line_given_points()`
    function to compute the error for a given combination of the slope and intercept
    and store it in `gradientStepError`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The last and final step is the main program, which sets up the initial starting
    point for the slope, intercept, number of iterations, and learning rate. We purposely
    choose a smaller learning rate and larger number of iterations to demonstrate
    accuracy and speed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: First, we start by the initialization of the key controlling variables for GD
    (learning rate, number of iterations, and starting point).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Second, we proceed to show the starting point (0,0) and call on `compute_error_for_line_given_points()`
    to show the starting error. It should be noted that the error should be lower
    after we are done running through the GD and display the result in the final step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Third, we set up necessary calls and structures for the JFree chart to display
    two charts that depict the slope, intercept, and error behavior as we merge toward
    an optimized solution (the best combination of the slope and intercept to minimize
    the error).
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The following is the output for this recipe.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First we display the starting point of 0,0 with the error of 6.006 and then
    allow the algorithm to run and display the result after completing the number
    of iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00231.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Noteworthy is the starting and ending error number and how it reduced over time
    due to optimization.
  prefs: []
  type: TYPE_NORMAL
- en: We used IBM SPSS as a control point to show that the GD algorithm that we put
    together matches the result (almost 1:1) produced by the SPSS package - it is
    almost exact!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following figure shows the output from IBM SPSS for comparing the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00232.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the last step, two charts are produced by the program side by side.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following figure shows how slope *(m)* and intercept (*b*) converge toward
    the best combination that minimizes the error as we run through the iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00233.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The following figure shows how slope (*m*) and intercept (*b*) converge toward
    the best combination that minimizes the error as we run through the iterations.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00234.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gradient Descent is an iterative numerical method that starts from an initial
    guess and then asks itself how badly am I doing by looking at an error function
    that is the squared distance of predicted versus actual data in the training file.
  prefs: []
  type: TYPE_NORMAL
- en: In this program, we selected a simple linear line *f(x) = b + mx* equation as
    our model. To optimize and come up with the best combination of slope m, intercept
    b for our model, we had 52 actual pairs of data (age, salary) that we can plug
    into our linear model (*Predicted Salary = Slope x Age + Intercept*). In short,
    we wanted to find the best combination of the slope and intercept that helped
    us fit a linear line that minimizes the squared distance. The squared function
    gives us all positive values and lets us concentrate on the magnitude of the error
    only.
  prefs: []
  type: TYPE_NORMAL
- en: '`ReadCSV()`: Reads and parses the data file in to our datasets:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*(x[1], y[1]), (x[2], y[2]), (x[3], y[4]), ... (x[52], y[52])*'
  prefs: []
  type: TYPE_NORMAL
- en: '`Compute_error_for_line_given_points()`: This function implements the cost
    or error function. We use a linear model (equation of a line) to predict, and
    then measure, the squared distance from the actual number. After adding up the
    errors, we average and return the total error:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00235.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'y[i] = mx[i] + b : for all data pair (*x, y)*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00236.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Noteworthy code within the function: the first line of code calculates the
    squared distance between predicted (*m * x + b*) and the actual (*y*). The second
    line of code averages it and returns it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00237.gif)*totalError += math.pow(y - (m * x + b), 2)**....**return
    totalError / points.length*'
  prefs: []
  type: TYPE_NORMAL
- en: The following figure shows the basic concept of least squares. In short, we
    take the distance between what the actual training data was for a point versus
    what our model predicts, and square them and then add them up. The reason we square
    them is to avoid using the absolute value function `abs()` which is not computationally
    desirable. The squared difference has better mathematical properties by providing
    a continuously differentiable property which is preferred when you want to minimize
    it.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00238.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '`step_gradient()`: This function is where the gradient (the first derivative)
    is calculated using the current point we are iterating over (*x[i],y[i]).* It
    should be noted that, unlike the previous recipe, we have two parameters so we
    need to calculate partial derivatives for the intercept (`b_gradient`) and then
    the slope (`m_gradient`). We then need to divide by the number of points to average.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00239.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Using a partial derivative with respect to Intercept(*b*):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*b_gradient += -(2 / N) * (y - ((m_current * x) + b_current))*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using a partial derivative with respect to Slope(*m*):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*m_gradient += -(2 / N) * x * (y - ((m_current * x) + b_current))*'
  prefs: []
  type: TYPE_NORMAL
- en: The last step is to scale the calculated gradient by the learning-rate (step
    size) and then move to a newly estimated position for the slope (m_current) and
    intercept (b_current):*result(0) = b_current - (learningRate * b_gradient)**result(1)
    = m_current - (learningRate * m_gradient)*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gradient_descent_runner()`: This is the driver that executes the `step_gradient()`
    and `compute_error_for_line_given_points()` for the number of iterations defined:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While this recipe was able to handle real-life data and match the estimation
    from a commercial package, in practice you need to implement Stochastic Gradient
    Descent.
  prefs: []
  type: TYPE_NORMAL
- en: Spark 2.0 offers Stochastic Gradient Descent (SGD) with a mini-batch window
    (for efficiency control).
  prefs: []
  type: TYPE_NORMAL
- en: Spark provides two approaches toward utilizing SGD. The first alternative is
    to use a standalone optimization technique in which you pass in your optimization
    function. See the following links: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.optimization.Optimizer](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.optimization.Optimizer) and [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.optimization.GradientDescent](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.optimization.GradientDescent)
  prefs: []
  type: TYPE_NORMAL
- en: 'The second alternative would be to use specialized APIs that have SGD built-in
    already as their optimization technique:'
  prefs: []
  type: TYPE_NORMAL
- en: '`LogisticRegressionWithSGD()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`StreamingLogisticRegressionWithSGD()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LassoWithSGD()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LinearRegressionWithSGD()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RidgeRegressionWithSGD()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SVMWithSGD()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As of Spark 2.0, all RDD-based regression is in maintenance mode only.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Optimization with Spark 2.0: [https://spark.apache.org/docs/latest/mllib-optimization.html#stochastic-gradient-descent-sgd](https://spark.apache.org/docs/latest/mllib-optimization.html#stochastic-gradient-descent-sgd)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normal equations as an alternative for solving Linear Regression in Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we present an alternative to Gradient Descent (GD) and LBFGS
    by using Normal Equations to solve linear regression. In the case of normal equations,
    you are setting up your regression as a matrix of features and vector of labels
    (dependent variables) while trying to solve it by using matrix operations such
    as inverse, transpose, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: The emphasis here is to highlight Spark's facility for using Normal Equations
    to solve Linear Regression and not the details of the model or generated coefficients.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We use the same housing dataset which we extensively covered in [Chapter 5](part0282.html#8CTUK0-4d291c9fed174a6992fd24938c2f9c77),
    *Practical Machine Learning with Regression and Classification in Spark 2.0 -
    Part I* and [Chapter 6](part0329.html#9PO920-4d291c9fed174a6992fd24938c2f9c77),
    *Practical Machine Learning with Regression and Classification in Spark 2.0 -
    Part II*, which relate various attributes (for example number of rooms, and so
    on) to the price of the house.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The data is available as `housing8.csv` under the `Chapter 9` data directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the package directive to take care of the placement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We then import the necessary libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Reduce the extra output generated by Spark by setting the Logger information
    level to `Level.ERROR`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up SparkSession with the appropriate attributes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Read the input file and parse it into a dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Display the following dataset contents, but limit them to first three rows
    for inspection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00240.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'We create a LinearRegression object and set the number of iterations, ElasticNet,
    and Regularization parameters. The last step is to set the right solver methods
    by choosing `setSolver("normal")`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Please be sure to set the ElasticNet parameter to 0.0 for the "normal" solver
    to work.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fit the `LinearRegressionModel` to the data using:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output is generated when you run the program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Readers can output more information, but the model summary was covered in [Chapter
    5](part0282.html#8CTUK0-4d291c9fed174a6992fd24938c2f9c77), *Practical Machine
    Learning with Regression and Classification in Spark 2.0 - Part I* and [Chapter
    6](part0329.html#9PO920-4d291c9fed174a6992fd24938c2f9c77), *Practical Machine
    Learning with Regression and Classification in Spark 2.0 - Part II* via other
    techniques.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are ultimately trying to solve the following equation for linear regression
    using the closed form formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00241.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Spark provides an out-of-the box fully parallel method for solving this equation
    by allowing you to set the `setSolver("normal")`.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you fail to set the ElasticNet parameter to 0.0, you will get an error because
    L2 regularization is used when solving through normal equations in Spark (as of
    this writing).
  prefs: []
  type: TYPE_NORMAL
- en: Documentation for Spark 2.0 related to isotonic regression can be found at: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.LinearRegression](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.LinearRegression) and [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.LinearRegressionModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.LinearRegressionModel)
  prefs: []
  type: TYPE_NORMAL
- en: The model summary can be found at: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.LinearRegressionSummary](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.LinearRegressionSummary)
  prefs: []
  type: TYPE_NORMAL
- en: "[\uFEFF](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.regression.LinearRegressionSummary)"
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Also refer to the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Iterative methods(SGD, LBFGS) | Closed formNormal Equation |'
  prefs: []
  type: TYPE_TB
- en: '| Choosing learning Rate | No parameter |'
  prefs: []
  type: TYPE_TB
- en: '| Iterations can be large | Does not iterate |'
  prefs: []
  type: TYPE_TB
- en: '| Good performance on large feature sets | Slow and impractical on large feature
    sets |'
  prefs: []
  type: TYPE_TB
- en: '| Error prone: getting stuck due to poor parameter selection | (x^Tx)^(-1)
    is computationally expensive - in the order of n³ |'
  prefs: []
  type: TYPE_TB
- en: Here is a quick reference on configuration of the LinearRegression object, but
    be sure to see [Chapter 5](part0282.html#8CTUK0-4d291c9fed174a6992fd24938c2f9c77), *Practical
    Machine Learning with Regression and Classification in Spark 2.0 - Part I* and [Chapter
    6](part0329.html#9PO920-4d291c9fed174a6992fd24938c2f9c77), *Practical Machine
    Learning with Regression and Classification in Spark 2.0 - Part II* for more details.
  prefs: []
  type: TYPE_NORMAL
- en: 'L1: Lasso regression'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'L2: Ridge regression'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'L1 - L2: Elastic net in which you can adjust the dial'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following link is a write-up from Columbia University that explains normal
    equations as they relate to solving Linear Regression problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.stat.columbia.edu/~fwood/Teaching/w4315/Fall2009/lecture_11](http://www.stat.columbia.edu/~fwood/Teaching/w4315/Fall2009/lecture_11)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Octave (](http://www.stat.columbia.edu/~fwood/Teaching/w4315/Fall2009/lecture_11)
    [https://www.gnu.org/software/octave/](https://www.gnu.org/software/octave/)[)
    from GNU is a popular matrix manipulation software and you should have it in your
    toolkit.](http://www.stat.columbia.edu/~fwood/Teaching/w4315/Fall2009/lecture_11)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following link contains a quick tutorial to get you started: [http://www.lauradhamilton.com/tutorial-linear-regression-with-octave](http://www.lauradhamilton.com/tutorial-linear-regression-with-octave)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
