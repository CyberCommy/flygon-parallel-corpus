- en: Chapter 4. Exploratory Data Analysis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章 探索性数据分析
- en: '**Exploratory Data Analysis** (**EDA**) performed in commercial settings is
    generally commissioned as part of a larger piece of work that is organized and
    executed along the lines of a feasibility assessment. The aim of this feasibility
    assessment, and thus the focus of what we can term an *extended EDA*, is to answer
    a broad set of questions about whether the data examined is fit for purpose and
    thus worthy of further investment.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在商业环境中进行的**探索性数据分析**（EDA）通常作为更大的工作的一部分委托，这个工作是按照可行性评估的线索组织和执行的。这种可行性评估的目的，因此也是我们可以称之为*扩展EDA*的焦点，是回答关于所检查的数据是否合适并且值得进一步投资的一系列广泛问题。
- en: Under this general remit, the data investigations are expected to cover several
    aspects of feasibility that include the practical aspects of using the data in
    production, such as its timeliness, quality, complexity, and coverage, as well
    as being appropriate for the intended hypothesis to be tested. While some of these
    aspects are potentially less fun from a data science perspective, these data quality
    led investigations are no less important than purely statistical insights. This
    is especially true when the datasets in question are very large and complex and
    when the investment needed to prepare the data for the data science might be significant.
    To illustrate this point, and to bring the topic to life, we present methods for
    doing an EDA of the vast and complex **Global Knowledge Graph** (**GKG**) data
    feeds, made available by the **Global Database of Events, Language and Tone**
    (**GDELT**) project.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个一般性的任务下，数据调查预计将涵盖可行性的几个方面，包括在生产中使用数据的实际方面，如及时性、质量、复杂性和覆盖范围，以及适合于测试的预期假设。虽然其中一些方面从数据科学的角度来看可能不那么有趣，但这些以数据质量为主导的调查与纯粹的统计洞察力一样重要。特别是当涉及的数据集非常庞大和复杂，以及为数据科学准备数据所需的投资可能是巨大的时候。为了阐明这一点，并使主题更加生动，我们提出了对全球事件、语言和语调全球数据库（GDELT）项目提供的大规模和复杂的全球知识图（GKG）数据源进行探索的方法。
- en: 'In this chapter, we will create and interpret an EDA while covering the following
    topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将创建和解释一个EDA，同时涵盖以下主题：
- en: Understanding the problems and design goals for planning and structuring an
    Extended Exploratory Data Analysis
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解规划和构建扩展探索性数据分析的问题和设计目标
- en: What data profiling is, with examples, and how a general framework for data
    quality can be formed around the technique for continuous data quality monitoring
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据概要分析是什么，举例说明，并且如何围绕连续数据质量监控的技术形成一个通用框架
- en: How to construct a general *mask-based* data profiler around the method
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何构建一个围绕该方法的通用*基于掩码的*数据概要分析器
- en: How to store the exploratory metrics to a standard schema, to facilitate the
    study of data drift in the metrics over time, with examples
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何将探索性指标存储到标准模式中，以便随时间研究指标的数据漂移，附有示例
- en: How to use Apache Zeppelin notebooks for quick EDA work, and for plotting charts
    and graphs
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用Apache Zeppelin笔记本进行快速探索性数据分析工作，以及绘制图表和图形
- en: How to extract and study the GCAM sentiments in GDELT, both as time series and
    as spatio-temporal datasets
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何提取和研究GDELT中的GCAM情感，包括时间序列和时空数据集
- en: How to extend Apache Zeppelin to generate custom charts using the `plot.ly`
    library
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何扩展Apache Zeppelin以使用`plot.ly`库生成自定义图表
- en: The problem, principles and planning
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题、原则和规划
- en: In this section, we will explore why an EDA might be required and discuss the
    important considerations for creating one.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨为什么可能需要进行探索性数据分析，并讨论创建探索性数据分析时的重要考虑因素。
- en: Understanding the EDA problem
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解探索性数据分析问题
- en: A difficult question that precedes an EDA project is: *Can you give me an estimate
    and breakdown of your proposed EDA costs, please?*
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行EDA项目之前的一个困难问题是：*你能给我一个关于你提议的EDA成本的估算和分解吗？*
- en: 'How we answer this question ultimately shapes our EDA strategy and tactics.
    In days gone by, the answer to this question typically started like this: *Basically
    you pay by the column....* This rule of thumb is based on the premise that there
    is *an iterable unit of data exploration work*, and these units of work drive
    the estimate of effort and thus the rough price of performing the EDA.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何回答这个问题最终塑造了我们的探索性数据分析策略和战术。过去，对这个问题的回答通常是这样开始的：*基本上你按列付费……*这个经验法则是基于这样的前提：*数据探索工作的可迭代单元*，这些工作单元驱动了工作量的估算，从而决定了进行探索性数据分析的大致价格。
- en: What's interesting about this idea is that the units of work are quoted in terms
    of the *data structures to investigate* rather than *functions that need writing*.
    The reason for this is simple. Data processing pipelines of functions are assumed
    to exist already, rather than being new work, and so the quotation offered is
    actually the implied cost of configuring the new inputs' data structures to our
    standard data processing pipelines for exploring data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法有趣的地方在于，工作单元的报价是以*需要调查的数据结构*而不是*需要编写的函数*来报价的。其原因很简单。假定数据处理管道的函数已经存在，而不是新的工作，因此所提供的报价实际上是配置新输入数据结构到我们标准的数据处理管道以探索数据的隐含成本。
- en: This thinking brings us to the main EDA problem, that *exploring* seems hard
    to pin down in terms of planning tasks and estimating timings. The recommended
    approach is to consider explorations as configuration driven tasks. This helps
    us to structure and estimate the work more effectively, as well as helping to
    shape the thinking around the effort so that configuration is the central challenge,
    rather than the writing of a lot of ad hoc throw-away code.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这种思维方式使我们面临的主要探索性数据分析问题是，*探索*在规划任务和估算时间方面似乎很难确定。建议的方法是将探索视为配置驱动的任务。这有助于我们更有效地组织和估算工作，同时有助于塑造围绕配置的思维，而不是编写大量临时代码。
- en: The process of configuring data exploration also drives us to consider the processing
    templates we might need. We would need to configure these based on the form of
    the data we explore. For instance, we would need a standard exploration pipeline
    for structured data, for text data, for graph shaped data, for image data, for
    sound data, for time series data, and for spatial data. Once we have these templates,
    we need to simply map our input data to them and configure our ingestion filters
    to deliver a focused lens over the data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 配置数据探索的过程也促使我们考虑可能需要的处理模板。我们需要根据我们探索的数据形式进行配置。例如，我们需要为结构化数据、文本数据、图形数据、图像数据、声音数据、时间序列数据和空间数据配置标准的探索流程。一旦我们有了这些模板，我们只需要将输入数据映射到它们，并配置摄入过滤器，以便对数据进行聚焦。
- en: Design principles
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设计原则
- en: 'Modernizing these ideas for Apache Spark based EDA processing means that we
    need to design our configurable EDA functions and code with some general principles
    in mind:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为基于Apache Spark的EDA处理现代化这些想法意味着我们需要设计具有一些通用原则的可配置EDA函数和代码：
- en: '**Easily reusable functions/features***:* We need to define our functions to
    work on general data structures in general ways so they produce good exploratory
    features and deliver them in ways that minimize the effort needed to configure
    them for new datasets'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**易重用的函数/特性***：我们需要定义我们的函数以一般方式处理一般数据结构，以便它们产生良好的探索特性，并以最小的配置工作量将它们交付给新数据集'
- en: '**Minimize intermediate data structures***:* We need to avoid proliferating
    intermediate schemas, helping to minimize intermediate configurations, and where
    possible create reusable data structures'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最小化中间数据结构***：我们需要避免大量中间模式，帮助最小化中间配置，并在可能的情况下创建可重复使用的数据结构'
- en: '**Data driven configuration***:* Where possible, we need to have configurations
    that can be generated from metadata to reduce the manual boilerplate work'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据驱动配置***：在可能的情况下，我们需要生成可以从元数据中生成的配置，以减少手动样板工作'
- en: '**Templated visualizations**: General reusable visualizations driven from common
    input schemas and metadata'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模板化可视化***：从常见输入模式和元数据驱动的通用可重复使用的可视化'
- en: Lastly, although it is not a strict principle per se, we need to construct exploratory
    tools that are flexible enough to discover data structures rather than depend
    on rigid pre-defined configurations. This helps when things go wrong, by helping
    us to reverse engineer the file content, the encodings, or the potential errors
    in the file definitions when we come across them.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，虽然这不是一个严格的原则，但我们需要构建灵活的探索工具，以便发现数据结构，而不是依赖于严格预定义的配置。当出现问题时，这有助于我们通过帮助我们反向工程文件内容、编码或文件定义中的潜在错误来解决问题。
- en: General plan of exploration
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索的一般计划
- en: The early stages of all EDA work are invariably based on the simple goal of
    establishing whether the data is of good quality. If we focus here, to create
    a general *getting started* plan that is widely applicable, then we can lay down
    a general set of tasks.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 所有EDA工作的早期阶段都不可避免地基于建立数据质量的简单目标。如果我们在这里集中精力，创建一个广泛适用的通用*入门*计划，那么我们可以制定一般的任务集。
- en: 'These tasks create the general shape of a proposed EDA project plan, which
    is as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这些任务构成了拟议的EDA项目计划的一般形状，如下所示：
- en: Prepare source tools, source our input datasets, review the documentation, and
    so on. Review security of data where necessary.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备源工具，获取我们的输入数据集，审查文档等。必要时审查数据的安全性。
- en: Obtain, decrypt, and stage the data in HDFS; collect **non-functional requirements**
    (**NFRs**) for planning.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取、解密并在HDFS中分阶段存储数据；收集用于规划的非功能性需求（NFRs）。
- en: Run code point level frequency reports on the file content.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在文件内容上运行代码点级别的频率报告。
- en: Run a population check on the amount of missing data in the files' fields.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在文件字段中运行缺失数据量的人口统计检查。
- en: Run a low grain format profiler to check on the high cardinality fields in the
    files.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行低粒度格式分析器，检查文件中基数较高的字段。
- en: Run a high grain format profiler check on format-controlled fields in the files.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在文件中对受格式控制字段运行高粒度格式分析器检查。
- en: Run referential integrity checks, where appropriate.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行参照完整性检查，必要时。
- en: Run in-dictionary checks, to verify external dimensions.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行字典检查，验证外部维度。
- en: Run basic numeric and statistical explorations of numeric data.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对数值数据进行基本的数字和统计探索。
- en: Run more visualization-based explorations of key data of interest.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对感兴趣的关键数据进行更多基于可视化的探索。
- en: Note
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: In character encoding terminology, a code point or code position is any of the
    numerical values that make up the code space. Many code points represent single
    characters, but they can also have other meanings, such as for formatting.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在字符编码术语中，代码点或代码位置是构成代码空间的任何数字值。许多代码点代表单个字符，但它们也可以具有其他含义，例如用于格式化。
- en: Preparation
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: Now that we have a general plan of action, before exploring our data, we must
    first invest in building the reusable tools for conducting the early mundane parts
    of the exploration pipeline that help us validate data; then as a second step
    investigate GDELT's content.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个行动的一般计划，在探索数据之前，我们必须首先投资于构建可重复使用的工具，用于进行探索流程的早期单调部分，帮助我们验证数据；然后作为第二步调查GDELT的内容。
- en: Introducing mask based data profiling
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引入基于掩码的数据分析
- en: A simple but effective method for quickly exploring new types of data is to
    make use of mask based data profiling. A *mask* in this context is a transformation
    function for a string that generalizes a data item into a feature, that, as a
    collection of masks, will have a lower cardinality than the original values in
    the field of study.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 快速探索新类型数据的一种简单而有效的方法是利用基于掩码的数据分析。在这种情况下，*掩码*是将数据项泛化为特征的字符串转换函数，作为掩码集合，其基数将低于研究领域中原始值的基数。
- en: 'When a column of data is summarized into mask frequency counts, a process commonly
    called *data profiling*, it can offer rapid insights into the common structures
    and content of the strings, and hence reveal how the raw data was encoded. Consider
    the following mask for exploring data:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据列被总结为掩码频率计数时，通常称为*数据概要*，它可以快速洞察字符串的常见结构和内容，从而揭示原始数据的编码方式。考虑以下用于探索数据的掩码：
- en: Translate uppercase letters to *A*
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将大写字母翻译为*A*
- en: Translate lowercase letters to *a*
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将小写字母翻译为*a*
- en: Translate numbers, 0 through 9, to *9*
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数字0到9翻译为*9*
- en: It seems like a very simple transformation at first glance. As an example, let's
    apply this mask to a high cardinality field of data, such as the GDELT GKG file's
    *V2.1 Source Common Name* field. The documentation suggests it records the common
    name of the source of the news article being studied, which typically is the name
    of the website the news article was scraped from. Our expectation is that it contains
    domain names, such as [nytimes.com](https://www.nytimes.com/).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 乍一看，这似乎是一个非常简单的转换。例如，让我们将此掩码应用于数据的高基数字段，例如GDELT GKG文件的*V2.1 Source Common Name*字段。文档建议它记录了正在研究的新闻文章的来源的常见名称，通常是新闻文章被抓取的网站的名称，我们期望它包含域名，例如[nytimes.com](https://www.nytimes.com/)。
- en: 'Before implementing the production solution in Spark, let''s prototype a profiler
    on the Unix command line to provide an example that we can run anywhere:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中实施生产解决方案之前，让我们在Unix命令行上原型化一个概要工具，以提供一个我们可以在任何地方运行的示例：
- en: '[PRE0]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The output is a sorted count of records found in the Source Common Name column
    alongside the mask generated by the regular expression (regex). It should be very
    clear looking at the results of this *profiled data* that the field contains domain
    names - or does it? As we have only looked at the most common masks (the top 20
    in this case) perhaps the long tail of masks at the other end of the sorted list
    holds potential data quality issues at a lower frequency.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是在Source Common Name列中找到的记录的排序计数，以及正则表达式（regex）生成的掩码。通过查看这个*概要数据*的结果，应该很清楚该字段包含域名-或者是吗？因为我们只看了最常见的掩码（在这种情况下是前20个），也许在排序列表的另一端的长尾部分可能存在潜在的数据质量问题。
- en: 'Rather than looking at just the top 20 masks, or even the bottom 20, we can
    introduce a subtle change to improve the generalization ability of our mask function.
    By making the regex collapse multiple adjacent occurrences of lower case letters
    into a single `a` character, the mask''s cardinality can be reduced without really
    diminishing our ability to interpret the results. We can prototype this improvement
    with just a small change to our regex and hopefully view all the masks in one
    page of output:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以引入一个微妙的改变来提高我们的掩码函数的泛化能力，而不是只看前20个掩码，甚至是后20个。通过使正则表达式将小写字母的多个相邻出现折叠成一个`a`字符，掩码的基数可以减少，而不会真正减少我们解释结果的能力。我们可以通过对我们的正则表达式进行微小的改进来原型化这个改进，并希望在一个输出页面上查看所有的掩码：
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Very quickly, we have prototyped a mask that reduces the three thousand or so
    raw values down to a very short list of 22 values that are easily inspected by
    eye. As the long tail is now a much shorter tail, we can easily spot any possible
    outliers in this data field that could represent quality issues or special cases.
    This type of inspection, although manual, can be very powerful.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 非常快地，我们原型化了一个掩码，将三千多个原始值缩减为一个非常短的列表，可以轻松地通过眼睛检查。由于长尾现在变得更短了，我们可以很容易地发现这个数据字段中可能的异常值，这些异常值可能代表质量问题或特殊情况。尽管是手动的，但这种类型的检查可能非常有力。
- en: Notice, for instance, there is a particular mask in the output, `AAA Aa`, which
    doesn't have a *dot* within it, as we would expect in a domain name. We interpret
    this finding to mean we've spotted two rows of raw data that are not valid domain
    names, but perhaps general descriptors. Perhaps this is an error, or an example
    of what is known as, *illogical field use*, meaning there could be other values
    slipping into this column that perhaps should logically go elsewhere.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，例如输出中有一个特定的掩码，`AAA Aa`，其中没有*点*，这与我们在域名中所期望的不符。我们解释这一发现意味着我们发现了两行不是有效域名的原始数据，而可能是一般描述符。也许这是一个错误，或者是所谓的*不合逻辑的字段使用*的例子，这意味着可能有其他值滑入了这一列，也许应该逻辑上属于其他地方。
- en: This is worth investigating, and it is easy to inspect those exact two records.
    We do so by generating the masks alongside the original data, then filtering on
    the offending mask to locate the original strings for manual inspection.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这值得调查，而且很容易检查这两条记录。我们可以通过在原始数据旁边生成掩码，然后过滤掉有问题的掩码来定位原始字符串，以进行手动检查。
- en: 'Rather than code a very long one liner on the command line, we can inspect
    these records using a legacy data profiler called `bytefreq` (short for *byte
    frequencies*) written in awk. It has switches to generate formatted reports, database
    ready metrics, and also a switch to output masks and data side by side. We have
    open-sourced `bytefreq` specifically for readers of this book, and suggest you
    play with it to really understand how useful this technique can be: [https://bitbucket.org/bytesumo/bytefreq](https://bitbucket.org/bytesumo/bytefreq).'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用一个名为`bytefreq`（*字节频率*的缩写）的传统数据概要工具来检查这些记录，而不是在命令行上编写一个非常长的一行代码。它有开关来生成格式化报告、数据库准备的指标，还有一个开关来输出掩码和数据并排。我们已经为本书的读者开源了`bytefreq`，建议您尝试一下，以真正理解这种技术有多有用：[https://bitbucket.org/bytesumo/bytefreq](https://bitbucket.org/bytesumo/bytefreq)。
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: When we inspect the odd mask, `A Aa`, we can see the offending text found is
    `BBC Monitoring`, and in re-reading the GDELT documentation we will see that this
    is not an error, but a known special case. It means when using this field, we
    must remember to handle this special case. One way to handle it could be by including
    a correction rule to swap this string value for a value that works better, for
    example, the valid domain name [www.monitor.bbc.co.uk](http://www.monitor.bbc.co.uk),
    which is the data source to which the text string refers.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们检查奇怪的掩码“A Aa”时，我们可以看到找到的有问题的文本是“BBC Monitoring”，在重新阅读GDELT文档时，我们会发现这不是一个错误，而是一个已知的特殊情况。这意味着在使用这个字段时，我们必须记住处理这个特殊情况。处理它的一种方法可能是包括一个更正规则，将这个字符串值替换为一个更好的值，例如有效的域名[www.monitor.bbc.co.uk](http://www.monitor.bbc.co.uk)，这是文本字符串所指的数据源。
- en: 'The idea we are introducing here is that a mask can be used as a *key* to retrieve
    offending records in particular fields. This logic leads us to the next major
    benefit of mask based profiling: the output masks are a form of *Data Quality
    Error Code*. These error codes can fall into two categories: a whitelist of *good* masks,
    and a blacklist of *bad* masks that are used to find poor quality data. Thought
    of this way, masks then form the basis for searching and retrieving data cleansing
    methods, or perhaps for throwing an alarm or rejecting a record.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里介绍的想法是，掩码可以用作检索特定字段中有问题记录的*关键*。这种逻辑引导我们到基于掩码的分析的下一个主要好处：输出的掩码是一种*数据质量错误代码*。这些错误代码可以分为两类：*好*掩码的白名单，和用于查找低质量数据的*坏*掩码的黑名单。这样考虑，掩码就成为搜索和检索数据清理方法的基础，或者用于发出警报或拒绝记录。
- en: 'The lesson is that we can create *Treatment functions* to remediate raw strings
    that are found using a particular mask calculated over data in a particular field.
    This thinking leads to the following conclusion: we can create a general framework
    around mask based profiling for doing data quality control and remediation *as
    we read data within our data reading pipeline*. This has some really advantageous
    solution properties:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这个教训是，我们可以创建*处理函数*来纠正使用特定掩码计算在特定字段数据上找到的原始字符串。这种思路导致了以下结论：我们可以创建一个围绕基于掩码的分析框架，用于在数据读取管道中进行数据质量控制和纠正。这具有一些非常有利的解决方案特性：
- en: Generating data quality masks is an *on read* process; we can accept new raw
    data and write it to disk then, on read, we can generate masks only when needed
    at query time - so data cleansing can be a dynamic process.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成数据质量掩码是一个*读取*过程；我们可以接受新的原始数据并将其写入磁盘，然后在读取时，我们只在查询时需要时生成掩码 - 因此数据清理可以是一个动态过程。
- en: Treatment functions can then be dynamically applied to targeting remediation
    efforts that help to cleanse our data at the time of read.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理函数可以动态应用于目标纠正工作，帮助在读取数据时清理我们的数据。
- en: Because previously unseen strings are generalized into masks, new strings can
    be flagged as having quality issues even if that exact string has never been seen
    before. This generality helps us to reduce complexity, simplify our processes,
    and create reusable smart solutions - even across subject areas.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因为以前未见过的字符串被概括为掩码，即使从未见过这个确切的字符串，新字符串也可以被标记为存在质量问题。这种普遍性帮助我们减少复杂性，简化我们的流程，并创建可重用的智能解决方案
    - 即使跨学科领域也是如此。
- en: Data items that create masks that do not fall either into mask white-lists,
    fix-lists, or blacklists can potentially be quarantined for attention; human analysts
    can inspect the records and hopefully whitelist them, or perhaps create new Treatments
    Functions that help to get the data out of quarantine and back into production.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建掩码的数据项如果不属于掩码白名单、修复列表或黑名单，可能会被隔离以供关注；人类分析师可以检查记录，并希望将它们列入白名单，或者创建新的处理函数，帮助将数据从隔离状态中取出并重新投入生产。
- en: Data quarantines can be implemented simply as an on-read filter, and when new
    remediation functions are created to cleanse or fix data, the dynamic treatments
    applied at read time will automatically *release* the corrected data to users
    without long delays.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据隔离可以简单地作为一个读取过滤器实施，当新的纠正函数被创建来清理或修复数据时，读取时自动应用的动态处理将自动将更正后的数据释放给用户，而不会有长时间的延迟。
- en: Eventually a data quality Treatment library will be created that stabilizes
    over time. New work is mainly done by mapping and applying the existing treatments
    to new data. A phone number reformatting Treatment function, for example, can
    be widely reused over many datasets and projects.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终将创建一个随时间稳定的数据质量处理库。新的工作主要是通过将现有处理映射并应用到新数据上来完成的。例如，电话号码重新格式化处理函数可以在许多数据集和项目中广泛重复使用。
- en: 'With the method and architectural benefits now explained, the requirements
    for building a generalized mask based profiler should be clearer. Note that the
    mask generation process is a classic Hadoop MapReduce process: map input''s data
    out to masks, and reduce those masks back down to summarized frequency counts.
    Note also how, even in this short example, we have already used two types of masks
    and each is made up of a pipeline of underlying transformations. It suggests we
    need a tool that supports a library of predefined masks as well as allowing for
    user defined masks that can be created quickly and on demand. It also suggests
    there should be ways to *stack* the masks to build them up into complex pipelines.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在解释了方法和架构的好处，构建一个通用的基于掩码的分析器的要求应该更清晰了。请注意，掩码生成过程是一个经典的Hadoop MapReduce过程：将输入数据映射到掩码，然后将这些掩码减少到总结的频率计数。还要注意的是，即使在这个简短的例子中，我们已经使用了两种类型的掩码，每种掩码都由一系列基础转换组成。这表明我们需要一个支持预定义掩码库的工具，同时也允许用户定义的掩码可以快速创建和按需使用。它还表明应该有方法来*堆叠*这些掩码，将它们组合成复杂的管道。
- en: What may not be so obvious yet is that all data profiling done in this way can
    write profiler metrics to *a common output format.* This helps to improve reusability
    of our code through simplifying the logging, storing, retrieval, and consumption
    of the profiling data.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 也许还不那么明显的是，以这种方式进行的所有数据概要都可以将概要度量写入*一个通用输出格式*。这有助于通过简化概要数据的记录、存储、检索和使用来提高我们代码的可重用性。
- en: 'As an example we should be able to report all mask based profiler metrics using
    the following schema:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们应该能够使用以下模式报告所有基于掩码的概要度量：
- en: '[PRE3]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Once our metrics are captured in this single schema format, we can then build
    secondary reports using a user interface, such as Zeppelin notebook.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们的度量被捕获在这个单一的模式格式中，我们就可以使用用户界面（如Zeppelin笔记本）构建辅助报告。
- en: Before we walk through implementing these functions, an introduction to the
    character class masks is needed as these differ slightly from the normal profiling
    masks.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们逐步实现这些函数之前，需要介绍一下字符类掩码，因为这些与普通的概要掩码略有不同。
- en: Introducing character class masks
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引入字符类掩码
- en: There is another simple type of data profiling that we can also apply that helps with
    file inspection. It involves profiling the actual bytes that make up a whole file.
    It is an old method, one that originally comes from cryptography where frequency
    analysis of letters in texts was used to gain an edge on deciphering substitution
    codes.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一种简单的数据概要类型，我们也可以应用它来帮助文件检查。它涉及对构成整个文件的实际字节进行概要。这是一种古老的方法，最初来自密码学，其中对文本中字母的频率进行分析用于在解密替换代码时获得优势。
- en: 'While not a common technique in data science circles today, byte level analysis
    is surprisingly useful when it''s needed. In the past, data encodings were a massive
    problem. Files were encoded in a range of code pages, across ASCII and EBCDIC
    standards. Byte frequency reporting was often critical to discover the actual
    encoding, delimiters, and line endings used in the files. Back, then the number
    of people who could create files, but not technically describe them, waqs surprising.
    Today, as the world moves increasingly to Unicode-based character encodings, these
    old methods need updating. In Unicode, the concept of a byte is modernized to
    multi-byte *code points*, which can be revealed in Scala using the following function:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在今天的数据科学圈中并不常见，但在需要时，字节级分析是令人惊讶地有用。过去，数据编码是一个巨大的问题。文件以一系列代码页编码，跨ASCII和EBCDIC标准。字节频率报告通常是发现实际编码、分隔符和文件中使用的行结束的关键。那时，能够创建文件但在技术上无法描述它们的人数是令人惊讶的。如今，随着世界越来越多地转向基于Unicode的字符编码，这些古老的方法需要更新。在Unicode中，字节的概念被现代化为多字节*代码点*，可以使用以下函数在Scala中揭示。
- en: '[PRE4]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Using this function, we can begin to profile any international character level
    data we receive in our GDELT dataset and start to understand the complexities
    we might face in exploiting the data. But, unlike the other masks, to create interpretable
    results from code points, we require a dictionary that we can use to look up meaningful
    contextual information, such as unicode category and the unicode character names.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 利用这个函数，我们可以开始对我们在GDELT数据集中收到的任何国际字符级数据进行分析，并开始了解我们在利用数据时可能面临的复杂性。但是，与其他掩码不同，为了从代码点创建可解释的结果，我们需要一个字典，我们可以用它来查找有意义的上下文信息，比如Unicode类别和Unicode字符名称。
- en: 'To generate a contextual lookup, we can use this quick command line hack to
    generate a reduced dictionary from the main one found at [unicode.org](http://unicode.org),
    which should help us to better report on our findings:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成一个上下文查找，我们可以使用这个快速的命令行技巧从主要的[unicode.org](http://unicode.org)找到的字典中生成一个缩小的字典，这应该有助于我们更好地报告我们的发现：
- en: '[PRE5]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We will use this dictionary, joined to our discovered code points, to report
    on the character class frequencies of each byte in the file. While it seems like
    a simple form of analysis, the results can often be surprising and offer a forensic
    level of understanding of the data we are handling, its source, and the types
    of algorithms and methods we can apply successfully to it. We will also look up
    the general Unicode Category to simplify our reports using the following lookup
    table:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这个字典，与我们发现的代码点结合起来，报告文件中每个字节的字符类频率。虽然这似乎是一种简单的分析形式，但结果往往会令人惊讶，并提供对我们处理的数据、其来源以及我们可以成功应用的算法和方法类型的法医级别的理解。我们还将查找一般的Unicode类别，以简化我们的报告，使用以下查找表：
- en: '[PRE6]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Building a mask based profiler
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建基于掩码的概要度量
- en: 'Let''s walk through creating a notebook-based toolkit for profiling data in
    Spark. The mask functions we will implement are set out over several grains of
    detail, moving from file level to row level, and then to field level:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过创建一个基于笔记本的工具包来逐步分析Spark中的数据。我们将实现的掩码函数在几个细节粒度上设置，从文件级别到行级别，然后到字段级别：
- en: 'Character level masks applied across whole files are:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用于整个文件的字符级掩码是：
- en: Unicode Frequency, UTF-16 multi-byte representation (aka Code Points), at file
    level
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Unicode频率，UTF-16多字节表示（也称为代码点），在文件级别
- en: UTF Character Class Frequency, at file level
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UTF字符类频率，文件级别
- en: Delimiter Frequency, at row level
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分隔符频率，行级别
- en: 'String level masks applied to fields within files are:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用于文件中字段的字符串级掩码是：
- en: ASCII low grain profile, per field
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ASCII低粒度概要，每个字段
- en: ASCII high grain profile, per field
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ASCII高粒度概要，每个字段
- en: Population checks, per field
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人口检查，每个字段
- en: Setting up Apache Zeppelin
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置Apache Zeppelin
- en: As we are going to be exploring our data visually, a product that could be very
    useful for mixing and matching technologies with relative ease is Apache Zeppelin.
    Apache Zeppelin is an Apache Incubator product that enables us to create a notebook,
    or worksheet, containing a mix of a number of different languages including Python,
    Scala, SQL, and Bash, which makes it ideal for working with Spark for running
    exploratory data analysis.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将要通过可视化方式探索我们的数据，一个非常有用的产品是Apache Zeppelin，它可以非常方便地混合和匹配技术。Apache Zeppelin是Apache孵化器产品，使我们能够创建一个包含多种不同语言的笔记本或工作表，包括Python、Scala、SQL和Bash，这使其非常适合使用Spark进行探索性数据分析。
- en: Code is written in a notebook style using *paragraphs* (or cells) where each
    cell can be independently executed making it easy to work on a small piece of
    code without having to repeatedly compile and run entire programs. It also serves
    as a record of the code used to produce any given output, and helps us to integrate
    visualizations.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 代码以笔记本风格编写，使用*段落*（或单元格），其中每个单元格可以独立执行，这样可以轻松地处理小段代码，而无需反复编译和运行整个程序。它还作为生成任何给定输出所使用的代码的记录，并帮助我们集成可视化。
- en: 'Zeppelin can be installed and run very quickly, a minimal installation process
    is explained as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Zeppelin可以快速安装和运行，最小安装过程如下所述：
- en: 'Download and extract Zeppelin from here: [https://zeppelin.incubator.apache.org/download.html](https://zeppelin.incubator.apache.org/download.html)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从这里下载并提取Zeppelin：[https://zeppelin.incubator.apache.org/download.html](https://zeppelin.incubator.apache.org/download.html)
- en: Find the conf directory and make a copy of `zeppelin-env.sh.template` named
    `zeppelin-env.sh`.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到conf目录并复制`zeppelin-env.sh.template`，命名为`zeppelin-env.sh`。
- en: Alter the `zeppelin-env.sh` file, uncomment and set the `JAVA_HOME` and `SPARK_HOME`
    entries to the relevant locations on your machine.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修改`zeppelin-env.sh`文件，取消注释并设置`JAVA_HOME`和`SPARK_HOME`条目为您机器上的相关位置。
- en: Should you want Zeppelin to use HDFS in Spark, set the `HADOOP_CONF_DIR` entry
    to the location of your Hadoop files; `hdfs-site.xml`, `core-site.xml`, and so
    on.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您希望Zeppelin在Spark中使用HDFS，请将`HADOOP_CONF_DIR`条目设置为您的Hadoop文件的位置；`hdfs-site.xml`，`core-site.xml`等。
- en: 'Start the Zeppelin service: `bin/zeppelin-daemon.sh start`. This will automatically
    pick up the changes made in `conf/zeppelin-env.sh`.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启动Zeppelin服务：`bin/zeppelin-daemon.sh start`。这将自动获取`conf/zeppelin-env.sh`中所做的更改。
- en: On our test cluster, we are using Hortonworks HDP 2.6, and Zeppelin comes as
    part of the installation.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的测试集群上，我们使用的是Hortonworks HDP 2.6，Zeppelin作为安装的一部分。
- en: 'One thing to note when using Zeppelin is that the first paragraph should always
    be a declaration of external packages. Any Spark dependencies can be added in
    this way using the `ZeppelinContext`, to be run right after each restart of the
    interpreter in Zeppelin; for example:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用Zeppelin时需要注意的一点是，第一段应始终声明外部包。任何Spark依赖项都可以使用`ZeppelinContext`以这种方式添加，以便在Zeppelin中的每次解释器重新启动后立即运行；例如：
- en: '[PRE7]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: After this we can write code in any of the available languages. We are going
    to use a mix of Scala, SQL, and Bash across the notebook by declaring each cell
    using a type of interpreter, that is, `%spark`, `%sql`, and `%shell`. Zeppelin
    defaults to Scala Spark if no interpreter is given `(%spark`).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们可以在任何可用的语言中编写代码。我们将通过声明每个单元格的解释器类型（`%spark`，`%sql`和`%shell`）在笔记本中使用Scala，SQL和Bash的混合。如果没有给出解释器，Zeppelin默认为Scala
    Spark（`%spark`）。
- en: You can find the Zeppelin notebooks to accompany this chapter, as well as others
    in our code repository.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在我们的代码库中找到与本章配套的Zeppelin笔记本，以及其他笔记本。
- en: Constructing a reusable notebook
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建可重用的笔记本
- en: 'In our code repository we have created a simple, extensible, open source data
    profiler library that can also be found here: [https://bytesumo@bitbucket.org/gzet_io/profilers.git](https://bytesumo@bitbucket.org/gzet_io/profilers.git)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的代码库中，我们创建了一个简单、可扩展、开源的数据分析库，也可以在这里找到：[https://bytesumo@bitbucket.org/gzet_io/profilers.git](https://bytesumo@bitbucket.org/gzet_io/profilers.git)
- en: The library takes care of the framework needed to apply masks to data frames,
    including the special case where raw lines of a file are cast to a data frame
    of just one column. We won't go through all the details of that framework line
    by line, but the class of most interest is found in the file `MaskBasedProfiler.scala`,
    which also contains the definitions of each of the available mask functions.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 该库负责应用掩码到数据框架所需的框架，包括将文件的原始行转换为仅有一列的数据框架的特殊情况。我们不会逐行介绍该框架的所有细节，但最感兴趣的类在文件`MaskBasedProfiler.scala`中找到，该文件还包含每个可用掩码函数的定义。
- en: A great way to use this library is by constructing a user-friendly notebook
    application that allows for visual exploration of data. We have prepared just
    such a notebook for our profiling using Apache Zeppelin. Next, we will walk through
    how to build our own notebook using the preceding section as a starting point.
    The data in our examples is the GDELT `event` files, which have a simple tab delimited
    format.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此库的一个很好的方法是构建一个用户友好的笔记本应用程序，允许对数据进行可视化探索。我们已经为使用Apache Zeppelin进行分析准备了这样的笔记本。接下来，我们将演示如何使用前面的部分构建我们自己的笔记本。我们示例中的数据是GDELT
    `event`文件，格式为简单的制表符分隔。
- en: 'The first step to building up a notebook (or even just to play with our readymade
    one), is to copy the `profilers-1.0.0.jar` file from our library into a local
    directory that the Zeppelin user on our cluster can access, which on a Hortonworks
    installation is the Zeppelin user''s home directory on the Namenode:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 构建笔记本的第一步（甚至只是玩弄我们准备好的笔记本）是将`profilers-1.0.0.jar`文件从我们的库复制到集群上Zeppelin用户可以访问的本地目录中，对于Hortonworks安装来说，这是Namenode上Zeppelin用户的主目录。
- en: '[PRE8]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Then we can visit `http://{main.install.hostname}:9995` to access the Apache
    Zeppelin homepage. From that page, we can upload our notebook and follow along,
    or we can create a new one and build our own by clicking **Create new note**.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以访问`http://{main.install.hostname}:9995`来访问Apache Zeppelin主页。从该页面，我们可以上传我们的笔记本并跟随，或者我们可以创建一个新的笔记本，并通过单击**创建新笔记**来构建我们自己的笔记本。
- en: 'In Zeppelin, the first paragraph of a notebook is where we execute our Spark
    code dependencies. We''ll import the profiler jars that we''ll need later:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在Zeppelin中，笔记本的第一段是我们执行Spark代码依赖关系的地方。我们将导入稍后需要的分析器jar包：
- en: '[PRE9]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In paragraph two, we include a small shell script to inspect the file(s) we
    want to profile to verify that we''re picking up the right ones. Note the use
    of `column` and `colrm`, both very handy Unix commands for inspecting columnar
    table data on the command line:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二段中，我们包括一个小的shell脚本来检查我们想要分析的文件，以验证我们是否选择了正确的文件。请注意`column`和`colrm`的使用，它们都是非常方便的Unix命令，用于在命令行上检查列式表数据：
- en: '[PRE10]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In paragraph 3, 4, 5, and 6, we use Zeppelin''s facility for user input boxes
    to allow the user to configure the EDA notebook like it''s a proper web-based
    application. This allows users to configure four variables that can be reused
    in the notebook to drive further investigations: **YourMask**, **YourDelimiter**,
    **YourFilePath**, and **YourHeaders**. These look great when we hide the editors
    and adjust the alignment and size of the windows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3、4、5和6段中，我们使用Zeppelin的用户输入框功能，允许用户配置EDA笔记本，就像它是一个真正的基于Web的应用程序一样。这允许用户配置四个变量，可以在笔记本中重复使用，以驱动进一步的调查：**YourMask**，**YourDelimiter**，**YourFilePath**和**YourHeaders**。当我们隐藏编辑器并调整窗口的对齐和大小时，这看起来很棒：
- en: '![Constructing a reusable notebook](img/image_04_001.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![构建可重用的笔记本](img/image_04_001.jpg)'
- en: 'If we open the prepared notebook and click on **show editor** on any of these
    input paragraphs, we''ll see how we set those up to provide drop-down boxes in
    Zeppelin, for example:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们打开准备好的笔记本并点击任何这些输入段落上的**显示编辑器**，我们将看到我们如何设置它们以在Zeppelin中提供下拉框，例如：
- en: '[PRE11]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, we have a paragraph that is used to import the functions we need:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们有一个用于导入我们需要的函数的段落：
- en: '[PRE12]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Then we move on to a new paragraph that configures and ingests the data we
    read in:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们继续到一个新的段落，配置和导入我们读取的数据：
- en: '[PRE13]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now that we''ve done the configuration steps, we can start to examine our tabular
    data and discover if our reported column names match our input data. In a new
    paragraph window, we use the SQL context to simplify calling SparkSQL and running
    a query:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了配置步骤，我们可以开始检查我们的表格数据，并发现我们报告的列名是否与我们的输入数据匹配。在一个新的段落窗口中，我们使用SQL上下文来简化调用SparkSQL并运行查询：
- en: '[PRE14]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The great thing about Zeppelin is that the output is formatted into a proper
    HTML table, which we can easily use to inspect wide files having many columns
    (for example, GDELT Event files):'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Zeppelin的一个很棒的地方是，输出被格式化为一个合适的HTML表，我们可以轻松地用它来检查具有许多列的宽文件（例如GDELT事件文件）：
- en: '![Constructing a reusable notebook](img/image_04_002.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![构建可重用的笔记本](img/image_04_002.jpg)'
- en: We can see from this displayed data that our columns match the input data; therefore
    we can proceed with our analysis.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从显示的数据中看到，我们的列与输入数据匹配；因此我们可以继续进行分析。
- en: Note
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: If you wish to read the GDELT event files, you can find the header file in our
    code repository.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望读取GDELT事件文件，您可以在我们的代码存储库中找到头文件。
- en: If there are errors in the data alignment between columns and content at this
    point, it is also possible to select the first 10 rows of the RawLines Dataframe,
    configured earlier, which will display just the first 10 rows of the raw string
    based data inputs. If the data happens to be tab delimited, we'll see immediately
    a further benefit that the Zeppelin formatted output will align the columns for
    us on the raw strings automatically, much like the way that we did earlier using
    the bash command *column.*
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如果此时列与内容之间的数据对齐存在错误，还可以选择之前配置的RawLines Dataframe的前10行，它将仅显示原始字符串数据输入的前10行。如果数据恰好是制表符分隔的，我们将立即看到另一个好处，即Zeppelin格式化输出将自动对齐原始字符串的列，就像我们之前使用bash命令*column*那样。
- en: 'Now we will move on to study the file''s bytes, to discover details about the
    encodings within it. To do so we load our lookup tables, and then join them to
    the output of our profiler functions, which we registered earlier as a table.
    Notice how the output of the profiler can be treated directly as an SQL callable
    table:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将继续研究文件的字节，以发现其中的编码细节。为此，我们加载我们的查找表，然后将它们与我们之前注册为表的分析器函数的输出进行连接。请注意，分析器的输出可以直接作为可调用的SQL表处理：
- en: '[PRE15]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In a new paragraph, we can use the SQLContext to visualize the output. To help
    view the values that are skewed, we can use the SQL statement to calculate the
    log of the counts. This produces a graphic, which we could include in a final
    report, where we can toggle between raw frequencies and log frequencies.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在新的段落中，我们可以使用SQLContext来可视化输出。为了帮助查看偏斜的值，我们可以使用SQL语句来计算计数的对数。这将产生一个图形，我们可以在最终报告中包含，我们可以在原始频率和对数频率之间切换。
- en: '![Constructing a reusable notebook](img/image_04_003.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![构建可重用的笔记本](img/image_04_003.jpg)'
- en: 'Because we have loaded the category of character classes, we can also adjust
    the visualization to further simplify the chart:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们已经加载了字符类别，我们还可以调整可视化以进一步简化图表：
- en: '![Constructing a reusable notebook](img/image_04_004.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![构建可重用的笔记本](img/image_04_004.jpg)'
- en: 'A basic check we must always run when doing an EDA is population checks, which
    we calculate using POPCHECKS. POPCHECKS is a special mask we defined in our Scala
    code that returns a `1` if a field is populated, or a `0` if it is not. When we
    inspect the result, we notice we''ll need to do some final report writing to present
    the numbers in a more directly interpretable way:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行EDA时，我们必须始终运行的基本检查是人口普查，我们使用POPCHECKS进行计算。 POPCHECKS是我们在Scala代码中定义的特殊掩码，如果字段有值则返回`1`，如果没有则返回`0`。当我们检查结果时，我们注意到我们需要进行一些最终报告写作，以更直接地解释数字：
- en: '[PRE16]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![Constructing a reusable notebook](img/image_04_005.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![构建可重用的笔记本](img/image_04_005.jpg)'
- en: We can do that in two steps. Firstly, we can use an SQL case expression to convert
    the data into values of *populated* or *missing,* which should help. Then we can
    pivot this aggregate dataset by performing a `groupby` on the filename, `metricDescriptor`,
    and `fieldname` while performing a sum over the populated and the missing values.
    When we do this we can also include default values of zero where the profiler
    did not find any cases of data either being populated or missing. It's important
    to do this when we calculate percentages, to ensure that we never have null numerators
    or denominators. While this code is not as short as it could be, it illustrates
    a number of techniques for manipulating data in `SparkSQL`.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice also that in `SparkSQL` we can use the SQL `coalesce` statement, which
    is not to be confused with Spark native `coalesce` functionality, for manipulating
    RDDs. In the SQL sense this function converts nulls into default values, and it
    is often used gratuitously to trap special cases in production grade code where
    data is not particularly trusted. Notable also is that sub-selects are well supported
    in `SparkSQL`. You can even make heavy use of these and Spark will not complain.
    This is particularly useful as they are the most natural way to program for many
    traditional database engineers as well as people with experience of databases
    of all kinds:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output of the preceding code is a clean reporting table about field level
    population counts in our data:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '![Constructing a reusable notebook](img/image_04_006.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
- en: 'When graphically displayed in our Zeppelin notebook using the `stacked` bar
    chart functionality, the data produces excellent visualizations that instantly
    tell us about the levels of data population in our files:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '![Constructing a reusable notebook](img/image_04_007.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
- en: As Zeppelin's bar charts support tooltips, we can use the pointer to observe
    the full names of the columns, even if they display poorly in the default view.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, we can also include further paragraphs in our notebook to reveal the
    results of the `ASCII_HighGrain` and `ASCII_LowGrain` masks, explained earlier.
    This can be done by simply viewing the profiler outputs as a table, or using more
    advanced functionality in Zeppelin. As a table, we can try the following:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![Constructing a reusable notebook](img/image_04_008.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
- en: To build an interactive viewer, which is useful when we look at ASCII_HighGrain
    masks that may have very high cardinalities, we can set up an SQL statement that
    accepts the value of a Zeppelin user input box, where users can type in the column
    number or the field name to retrieve just the relevant section of the metrics
    we collected.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'We do that in a new SQL paragraph like this, with the SQL predicate being `x.fieldName
    like ''%${ColumnName}%''`:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This creates an interactive user window that refreshes on user input, creating
    a dynamic profiling report having several output configurations. Here we show
    the output not as a table, but as a chart of the log of the frequency counts for
    a field that should have low cardinality, the longitude of *Action* identified
    in the event file:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '![Constructing a reusable notebook](img/image_04_009.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
- en: The result shows us that even a simple field like Longitude has a large spread
    of formats in the data.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: The techniques reviewed so far should help create a very reusable notebook for
    performing exploratory data profiling on all our input data, both quickly and
    efficiently, producing graphical outputs that we can use to produce great reports
    and documentation about input file quality.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Exploring GDELT
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A large part of the EDA journey is obtaining and documenting the sources of
    data, and GDELT content is no exception. After researching the GKG datasets, we
    discovered that it was challenging just to document the actual sources of data
    we should be using. In the following sections, we provide a comprehensive listing
    of the resources we located for use, which will need to be run in the examples.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A cautionary note on download times: using a typical 5 Mb home broadband, 2000
    GKG files takes approximately 3.5 hours to download. Given that the GKG English
    language files alone have over 40,000 files, this could take a while to download.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: GDELT GKG datasets
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We should be using the latest GDELT data feed, version 2.1 as of December 2016\.
    The main documentation for this data is here:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/documentation/GDELT-Global_Knowledge_Graph_Codebook-V2.1.pdf](http://data.gdeltproject.org/documentation/GDELT-Global_Knowledge_Graph_Codebook-V2.1.pdf)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we have included the data and secondary references
    to look up tables, and further documentation.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: The files
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: GKG-English Language Global Knowledge Graph (v2.1)
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/gdeltv2/masterfilelist.txt](http://data.gdeltproject.org/gdeltv2/masterfilelist.txt)'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/gdeltv2/lastupdate.txt](http://data.gdeltproject.org/gdeltv2/lastupdate.txt)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: GKG-Translated - Non-English Global Knowledge Graph
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/gdeltv2/lastupdate-translation.txt](http://data.gdeltproject.org/gdeltv2/lastupdate-translation.txt)'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/gdeltv2/masterfilelist-translation.txt](http://data.gdeltproject.org/gdeltv2/masterfilelist-translation.txt)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: GKG-TV (Internet Archive - American Television Global Knowledge Graph)
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/gdeltv2_iatelevision/lastupdate.txt](http://data.gdeltproject.org/gdeltv2_iatelevision/lastupdate.txt)'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/gdeltv2_iatelevision/masterfilelist.txt](http://data.gdeltproject.org/gdeltv2_iatelevision/masterfilelist.txt)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: GKG-Visual - CloudVision
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/gdeltv2_cloudvision/lastupdate.txt](http://data.gdeltproject.org/gdeltv2_cloudvision/lastupdate.txt)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Special collections
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: GKG-AME - Africa And Middle East Global Knowledge Graph
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/gkgv2_specialcollections/AME-GKG.CIA.gkgv2.csv.zip](http://data.gdeltproject.org/gkgv2_specialcollections/AME-GKG.CIA.gkgv2.csv.zip)'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/gkgv2_specialcollections/AME-GKG.CORE.gkgv2.csv.zip](http://data.gdeltproject.org/gkgv2_specialcollections/AME-GKG.CORE.gkgv2.csv.zip)'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/gkgv2_specialcollections/AME-GKG.DTIC.gkgv2.csv.zip](http://data.gdeltproject.org/gkgv2_specialcollections/AME-GKG.DTIC.gkgv2.csv.zip)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/gkgv2_specialcollections/AME-GKG.IADISSERT.gkgv2.csv.zip](http://data.gdeltproject.org/gkgv2_specialcollections/AME-GKG.IADISSERT.gkgv2.csv.zip)'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/gkgv2_specialcollections/AME-GKG.IANONDISSERT.gkgv2.csv.zip](http://data.gdeltproject.org/gkgv2_specialcollections/AME-GKG.IANONDISSERT.gkgv2.csv.zip)'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/gkgv2_specialcollections/AME-GKG.JSTOR.gkgv2.csv.zip](http://data.gdeltproject.org/gkgv2_specialcollections/AME-GKG.JSTOR.gkgv2.csv.zip)'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: GKG-HR (Human Rights Collection)
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/gkgv2_specialcollections/HR-GKG.AMNESTY.gkgv2.csv.zip](http://data.gdeltproject.org/gkgv2_specialcollections/HR-GKG.AMNESTY.gkgv2.csv.zip)'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/gkgv2_specialcollections/HR-GKG.CRISISGROUP.gkgv2.csv.zip](http://data.gdeltproject.org/gkgv2_specialcollections/HR-GKG.CRISISGROUP.gkgv2.csv.zip)'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/gkgv2_specialcollections/HR-GKG.FIDH.gkgv2.csv.zip](http://data.gdeltproject.org/gkgv2_specialcollections/HR-GKG.FIDH.gkgv2.csv.zip)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/gkgv2_specialcollections/HR-GKG.HRW.gkgv2.csv.zip](http://data.gdeltproject.org/gkgv2_specialcollections/HR-GKG.HRW.gkgv2.csv.zip)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/gkgv2_specialcollections/HR-GKG.ICC.gkgv2.csv.zip](http://data.gdeltproject.org/gkgv2_specialcollections/HR-GKG.ICC.gkgv2.csv.zip)'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/gkgv2_specialcollections/HR-GKG.OHCHR.gkgv2.csv.zip](http://data.gdeltproject.org/gkgv2_specialcollections/HR-GKG.OHCHR.gkgv2.csv.zip)'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/gkgv2_specialcollections/HR-GKG.USSTATE.gkgv2.csv.zip](http://data.gdeltproject.org/gkgv2_specialcollections/HR-GKG.USSTATE.gkgv2.csv.zip)'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Reference data
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/documentation/GCAM-MASTER-CODEBOOK.TXT](http://data.gdeltproject.org/documentation/GCAM-MASTER-CODEBOOK.TXT)'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/supportingdatasets/GNS-GAUL-ADM2-CROSSWALK.TXT.zip](http://data.gdeltproject.org/supportingdatasets/GNS-GAUL-ADM2-CROSSWALK.TXT.zip)'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/supportingdatasets/DOMAINSBYCOUNTRY-ENGLISH.TXT](http://data.gdeltproject.org/supportingdatasets/DOMAINSBYCOUNTRY-ENGLISH.TXT)'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/supportingdatasets/DOMAINSBYCOUNTRY-ALLLANGUAGES.TXT](http://data.gdeltproject.org/supportingdatasets/DOMAINSBYCOUNTRY-ALLLANGUAGES.TXT)'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.unicode.org/Public/UNIDATA/UnicodeData.txt](http://www.unicode.org/Public/UNIDATA/UnicodeData.txt)'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.geonames.org/about.html](http://www.geonames.org/about.html)'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the GKG v2.1
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we review existing articles that explore the GDELT data feeds, we find
    many studies that focus on the people, themes, and tone of the articles, and some
    that focus on the earlier event files. But there is not much published that explores
    the **Global Content Analysis Measures** (**GCAM**) content that is now included
    in the GKG files. When we try to use the data quality workbook we've built to
    examine the GDELT data feed, we discover that the Global Knowledge Graph is hard
    to work with, as the files are encoded using multiple nested delimiters. Working
    with this nested format data quickly is the key challenge in working with the
    GKG, and indeed the GCAM, and is the focus of the rest of this chapter.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some obvious questions we need to answer as part of exploring the
    GCAM data in the GKG files:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: What are the differences between the English language GKG files and the translated
    *Translingual* international files? Are there differences in how the data is populated
    between these feeds, given that some of the entity recognition algorithms might
    not work well on translated files?
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the translated data is well populated for the GCAM sentiment metrics dataset,
    included in the GKG files, can it (or indeed the English versions) be trusted?
    How can we access and normalize this data, and does it hold valuable signals rather
    than noise?
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we can answer just these two questions alone, we will have established much
    about the usefulness of GDELT as a source of signals from which to perform data
    science. However, *how* we answer those questions is important, and we need to
    try and template our code as we obtain those answers, to create reusable configuration
    driven EDA components. If we can create re-purposable explorations in line with
    our principles, we will drive out far more value than hardcoding our analysis.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: The Translingual files
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's reuse our earlier work to reveal some of the quality issues, then extend
    our explorations to these more detailed and complex questions. By running some
    of the population count (POPCHECK) metrics to a temporary file, for both the normal
    GKG data and the translated files, we can import and union the results together.
    This is a benefit of having a standardized metrics format that we reuse; we can
    easily perform comparisons across datasets!
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: 'Rather than go through the code in detail, we''ll deliver some headline answers.
    When we examine the population counts between the English and the translated GKG
    files we do expose some differences in the content available:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '![The Translingual files](img/image_04_010.jpg)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
- en: We see here that the translated GKG translingual files have no Quotations data
    at all and that they are very under populated when identifying Persons versus
    the population counts we are seeing in the general English language news feed.
    So there are definitely some differences to be mindful of.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: As a consequence, we should examine carefully any content in the translingual
    data feeds that we wish to rely on in production. Later we'll see how the translated
    information in the GCAM sentiment content measures up against the native English
    language sentiments.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: A configurable GCAM time series EDA
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The GCAM's content is primarily made up of *Word Counts*, created by filtering
    news articles using dictionary filters and doing word counts on the synonyms that
    characterize the theme of interest. The resulting count can be normalized through
    dividing the count by the total words in the document. It also includes *Scored
    Values* delivering sentiment scores that appear to be based on directly studying
    the original language text.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'We can quickly summarize the range of sentiment variables to study and explore
    in the GCAM in a couple of lines of code, the output of which is annotated with
    the name of the language:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The GCAM word count based time series seems to be most fully developed, especially
    in the English language where there are 2441 sentiment measures! Working with
    such a large number of measures seems hard, even to do simple analysis. We'll
    need some tools to simplify things, and we'll need to focus our scope.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: To help, we've created a simple SparkSQL-based explorer to extract and visualize
    time series data from the GCAM block of data, which specifically targets the word
    count based sentiments. It's created by cloning and adjusting our original data
    quality explorer in Zeppelin.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'It works by adjusting it to read in the GKG file glob using a defined schema,
    and previewing just the raw data we want to focus on:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The results of the early column selections isolate our content on the areas
    to explore; time (`V21Date`), sentiment(`V2GCAM`), and Source URL (`V2DocID`):'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: In a new Zeppelin paragraph, we create an SQLContext and carefully unravel the
    nested structure of the GCAM records. Notice the first of the inner comma delimited
    rows in the V2GCAM field hold the `wc` dimension and a measure representing the
    word count of the story for this GkgRecordID, then the other sentiment measures
    are listed. We need to unfurl this data into actual rows, as well as divide all
    word count based sentiments by the total word count for the article in `wc` to
    normalize the scores.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: In the following snippets, we have designed a `SparkSQL` statement to do this
    in a typical *onion* fashion, using subselects. This is a coding style you may
    wish to learn to read if you don't know it already. It works like this - create
    the innermost selection/query and then run it to test it, then wrap it in brackets
    and continue by selecting the data into the next query process, and so on. Then
    the catalyst optimizer does its magic and optimizes the whole pipeline. It results
    in an ETL process that is both declarative and readable, and which also offers
    an ability to troubleshoot and isolate issues in any part of the pipeline, if
    that's needed. If we want to understand how to handle the nested array process,
    we can easily rebuild the following SQL, running the innermost fragment first,
    then reviewing its outputs, then expanding on it to include the next query that
    wraps it, and so on. Step by step we can then review the staged outputs to review
    how the whole statement works together to deliver the final result.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: The key trick in the following query is how to apply the word count denominator
    to each of the other sentiment word counts, to normalize the values. This method
    of normalization is actually suggested in the GKG documentation, although no implementation
    hints are provided.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'Also of note, is how the V21Date field is converted from an integer to a date,
    which is needed to plot the time series effectively. The conversion requires that
    we pre-import the following library in addition to the others imported in the
    notebook:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Using the `Unix_timestamp` functions, we convert the V21Date into a `Unix_timestamp`,
    which is an integer, and then convert that integer again into a date field, all
    using native Spark libraries to configure the formatting and temporal resolution.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'The following SQL query achieves our desired investigation:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The results of the query are illustrated here using Zeppelin''s time series
    viewer. It shows that the time series data is building up properly and that it
    looks very credible, having a short-lived peak on November 8 2016: the day of
    the US presidential election:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '![A configurable GCAM time series EDA](img/image_04_011.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
- en: Now we have a working SQL statement to examine the GCAM sentiment scores, perhaps
    we should double-check some other measures, for example on a different but related
    topic, such as the Brexit vote in the UK.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ve selected three GCAM sentiment measures that look interesting, in addition
    to the *Election Fraud* measure, which hopefully will provide an interesting comparison
    to the results we''ve seen for the US election. The measures we''ll look at are:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '''c18.101'' -- Immigration'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '''c18.100'' -- Democracy'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '''c18.140'' -- Election'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To include them, we need to extend our query to pick up multiple normalized
    Series, and we may also need to be mindful that the results may not all fit into
    Zeppelin''s viewer, which defaults to only taking in the first 1000 results, so
    we may need to further summarize to hours or days. While not a large change, it
    will be interesting to see how extensible our existing work is:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'In this second example, we further refine our base query, removing the unnecessary
    GKGRecordIDs that we didn''t use. This query also demonstrates how to filter results
    against many `Series` names using a simple set of predicates. Notice we have also
    added in a pre-grouping step using the following:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This random number is used to create a partition prefix key that we use in our
    inner group by statement, before going on to group again without this prefix.
    The query is written in this way as it helps to subdivide and pre-summarize *hotspotting* data
    and smooth out any pipeline bottlenecks.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 'When we look at the results of this query in Zeppelin''s time series viewer
    we have the chance to further summarize up to hourly counts, and to translate
    the cryptic GCAM series codes into proper names using a case statement. We can
    do this in a new query, helping to isolate *specific* reporting configurations
    away from the general dataset construction query:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This final query reduces the data to hourly values, which is less than the
    default 1000 row maximum that Zeppelin handles by default, additionally it generates
    a comparative time series chart:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '![A configurable GCAM time series EDA](img/image_04_012.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
- en: The resulting chart illustrates that there is almost no discussion at all about
    *Election Fraud* preceding the Brexit vote, but there are however spikes on *Election,*
    and that Immigration is a hotter theme than Democracy. Again, the GCAM English
    language sentiment data seems to hold real signal.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have shed some light on the English language records, we can extend
    our work to explore them against the translated data in GCAM.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'As a final way to complete the analysis in this notebook, we can comment out
    the filters on the specific `Series` and write a timeseries database of all the
    GCAM series data for Brexit to a parquet file in our HDFS filesystem. This allows
    us to permanently store our GCAM data to disk and even to append new data to it
    over time. The following is the code needed to either overwrite, or to append
    to a parquet file:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: With the parquet files written to disk, we have now built a lightweight GCAM
    time series data store that allows us to quickly retrieve a GCAM sentiment, for
    exploration across language groups.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: Plot.ly charting on Apache Zeppelin
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For our next exploration we will also extend our use of Apache Zeppelin notebooks
    to include producing `%pyspark` charts using an external charting library called
    plotly, open sourced by [https://plot.ly/](https://plot.ly/), which can be used
    to create print quality visualizations. To use plotly in our notebook, we can
    upgrade our Apache Zeppelin installation, using the code found at [https://github.com/beljun/zeppelin-plotly](https://github.com/beljun/zeppelin-plotly),
    which provides the integration needed. On its GitHub page, there are detailed
    installation instructions, and within their code base, they provide a very helpful
    example notebook. Here are some tips for installing plotly for use on an HDP cluster
    with Zeppelin:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: 'Log into the Namenode as the Zeppelin user and change the directory to the
    Zeppelin home directory at `/home/zeppelin` where we will download the external
    code:'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Change the directory to where the Zeppelin `*.war` file is kept. This location
    is revealed in the Zeppelin **Configuration** tab. For example:'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now, as per the instructions, we need to edit the index.html document found
    in the Zeppelin `war` file:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Once the `index.html` page is extracted we can use an editor such as vim to
    insert the `plotly-latest.min.js` script tag (as per the instructions), just before
    the body tag, and save and execute the document.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Put the edited `index.html` document back into the war file using:'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Finally, log into Ambari, and use it to restart the Zeppelin service.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Follow the rest of the instructions to generate a test chart in Zeppelin.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We may need to install or update old libraries if there are issues. Log into
    the Namenode and use pip to install the packages:'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: With the installation complete, we should now be able to create Zeppelin notebooks
    that generate inline plot.ly charts from the `%pyspark` paragraphs, and these
    will be created offline using the local libraries rather than the online service.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Exploring translation sourced GCAM sentiment with plot.ly
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For this comparison, let''s focus on an interesting measure found in the GCAM
    documentation: *c6.6*; *Financial Uncertainty*. This measure counts word-matches
    made between a news story and a financially oriented *uncertainty dictionary*.
    If we trace its provenance online, we can discover the academic paper and actual
    dictionary driving the metric. However, will that dictionary based measure work
    with translated news text? To investigate this, we can review how this financial
    *Uncertainty* metric differs across six major European language groups: English,
    French, German, Spanish, Italian, and Polish with respect to the subject of Brexit.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: 'We create a new notebook, include a *pyspark* paragraph to load plot.ly libraries
    and set them to run in offline mode:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Then we create a paragraph to read in our cached data from parquet:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We then can create an SQL query that reads and prepares it for plot, and registers
    it for use:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Now that we''ve defined an adaptor, we can create the query that summarizes
    the data in our parquet file to something that will fit more easily into memory:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'This main payload query generates a set of data that we can load to a `pandas`
    array in `pyspark`, and which has timestamps with a plot.ly ready format:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'To feed this data to plot.ly we must convert the Spark Dataframe that we generated
    into a `pandas` one:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'When we perform this step, we must remember to `collect()` the dataframe, as
    well as reset the column names for `pandas` to pick up. With a `pandas` array
    now in our Python environment, we can pivot the data easily into a form that will
    facilitate time series plotting:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Lastly, we include a call to generate the chart:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '![Exploring translation sourced GCAM sentiment with plot.ly](img/image_04_013.jpg)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
- en: 'Now that we have produced a working plot.ly chart of our data, we should create
    a custom visualization, which was not possible with the standard Zeppelin notebook,
    to illustrate the value that the plotly library brings to our exploration. A simple
    example is to generate some *small multiples* like this:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Which generates the following chart:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '![Exploring translation sourced GCAM sentiment with plot.ly](img/image_04_014.jpg)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
- en: This small multiple chart helps us to see that, in the Italian press, there
    seems to have been a local spike in financial Uncertainty on June 15 2016; just
    a week or so before the election. This is something we might wish to investigate
    as it is also present, to a lesser degree, in Spanish language news too.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: Plotly also offers many other interesting visualizations. If you have been carefully
    reading the code snippets, you may have noticed that the parquet file includes
    the FIPS10-4 country code from the GKG files. We should be able to leverage these
    location codes to plot a choropleth map of the Uncertainty metric, using Plotly,
    and at the same time leverage our previous data processing.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: 'To create this geographical map, we reuse our parquet file reader query that
    we registered earlier. Unfortunately, the GKG files use FIPS 10-4 two-character
    country encoding, and Plotly uses ISO-3166 three-character country codes to automatically
    geotag the user records it processes for plotting. We can address this by using
    a case statement in our SQL to remap our codes, before summarizing them over the
    whole period of enquiry:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'With our data now prepared in a `pandas` dataframe, we can invoke the visualization
    using the following line of Python:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The final result is an interactive, zoomable map of the world. We will leave
    its political interpretation to the reader, but conclude technically that perhaps
    this map shows an effect to do with news volume that we could later normalize
    on; for instance by dividing our values by total stories per country.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '![Exploring translation sourced GCAM sentiment with plot.ly](img/image_04_015.jpg)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
- en: Concluding remarks
  id: totrans-303
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It is worth pointing out that there are a number of parameters that drove our
    EDA across all of our investigations, and we could consider how these might be
    parameterized to build proper exploration products for monitoring GDELT. Parameters
    for consideration are as follows:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: We can select a non-GCAM field to filter on. In the preceding examples, it is
    configured to the V2DocID, which is the URL of the story. Finding words in the
    URL such as BREXIT or TRUMP will help to scope our investigations to stories that
    are relevant to particular subject areas. We could also reuse this technique to
    filter on BBC or NYTIMES, for example. Alternatively, if we swapped this column
    for another, such as Theme or Person, then these columns would offer new ways
    to focus our study on particular subjects or people of interest.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have converted and generalized the granularity of the timestamp, V21Date,
    to deliver hourly time series increments, but we could reconfigure this to create
    our time series on a monthly, weekly, or daily basis - or indeed on any other
    increment.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We first selected and scoped our investigation to one timeseries of interest,
    *c18_134*, which is *Election Fraud*, but we can easily reconfigure this to look
    at *Immigration* or *Hate Speech* or any of the other 2400+ sentiment scores that
    are word count based.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have introduced a file glob at the start of our notebook, which scopes the
    amount of time that we include in the summary output. To keep costs low, we've
    kept it small to start with, but we could refocus this time range on key events,
    or even open it up to all of the available files, given enough processing budget
    (time and money).
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have now illustrated that our code can be easily adjusted to build a notebook-based
    GCAM time-series explorer, from which we would be able to construct huge numbers
    of focused investigations on demand; each exploring the content of the GCAM data
    in a configurable way.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have been carefully following the SQL code throughout the notebook,
    and were wondering why it had not been written using the Python API, or perhaps
    using idiomatic Scala, we will complete this section with one final observation:
    it is precisely because it is constructed from SQL that it can be moved between
    the Python, R, or Scala contexts with almost no cost in code refactoring. Should
    a new charting facility in R become available, it can be ported easily over to
    R, and then effort can be focused solely on the visualization. Indeed, with the
    arrival of Spark 2.0+, it is perhaps the SQL code that requires the least review
    when porting. The importance of code portability cannot be stressed enough. The
    most valuable benefit of using SQL in the EDA context, however, is that it makes
    generating parameter driven notebooks in Zeppelin so easy, as we have seen in
    the earlier profiler section. Drop-down boxes, and other UI widgets, can all be
    created in conjunction with string processing to customize the code before execution,
    irrespective of backend language. This is an extremely fast way to build interactivity
    and configuration into our analysis, without dipping into complex meta programming
    methods. It also helps us to avoid solving those meta programming complexities
    across the different language back ends available in Apache Zeppelin/Spark.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: With respect to building broad data explorations, if we wished to use our cached
    results in parquet more broadly, there is also an opportunity to remove the need
    for "eyeballs looking at charts" altogether. See [Chapter 12](ch12.xhtml "Chapter 12. TrendCalculus"),
    *TrendCalculus* to get an idea for how we could programmatically study trends
    across all of the data in GKG programmatically.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: A final trick of note when using Zeppelin, to produce graphics for EDA reports,
    is one that is purely practical. If we wish to extract our graphics to files,
    to include them in our final report for example, rather than taking screenshots
    of our notebook, we can directly extract the scalable vector graphics files (SVG)
    from Zeppelin and download them to files using the *bookmarklet* found here [http://nytimes.github.io/svg-crowbar/](http://nytimes.github.io/svg-crowbar/).
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: A configurable GCAM Spatio-Temporal EDA
  id: totrans-313
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another question about the GCAM remains unanswered; how do we start to understand
    how it subdivides spatially? Could a geospatial pivot of the GCAM expose how the
    global news media presents its aggregate geopolitical views, as detailed geographies
    that get beneath country level analysis?
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: If we can construct such a dataset as part of our EDA, it would have many and
    varied applications. At a city level for example, it would be a general geopolitical
    signals library that could enrich a wide range of other data science projects.
    Consider holiday travel booking patterns, shown against the backdrop of geopolitical
    themes emerging in the news. Would we discover that global news signals at city
    level predicts rising or falling tourism rates in places of media interest? The
    possibilities for this type of data are nearly endless when we consider the resulting
    information as a source of geopolitical situational awareness.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: With such an opportunity in front of us, we need to consider carefully our investment
    in this more complex EDA. It will need, as before, a common data structure from
    which to start our explorations.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: 'As a target, we will aim to construct the following dataframe from which to
    explore the geopolitical trends, which we will call "*GeoGcam*":'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Introducing GeoGCAM
  id: totrans-319
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: GeoGcam is a global spatio-temporal signals dataset that is derived from the
    raw GDELT Global Knowledge Graph (2.1). It enables the exploration of evolving
    geopolitical trends in global news media sentiment quickly and easily. The data
    itself is created using a transformation pipeline that casts the raw GKG files
    into a standard, reusable, global time/space/sentiment signals format that allows
    for direct downstream spatio-temporal analysis, cartographic visualization, and
    further wide scale geopolitical trend analysis.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: It can be used as a source of external covariates for predictive models, especially
    ones that require improved geopolitical situational awareness.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: It is constructed by recasting the GKG's GCAM sentiment data into a spatially
    oriented schema. This is performed by *placing* each news story's sentiments against
    each of the fine-grained city/town level locations identified in its GKG record.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: The data is then aggregated by city, across all of the indexed stories in a
    15 minute GKG time window. The result is a file that delivers an aggregate news
    media *sentiment consensus* across all stories in that space and time window,
    for that place. Although there will be noise, our hypothesis is that big broad
    geopolitical themes will emerge.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: 'A sample of the dataset (which matches the target schema) is:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Technical notes on the dataset:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: Only news articles tagged with specific city locations are included, meaning
    only those tagged by GKG as having a location type code of 3=USCITY or 4=WORLDCITY.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have calculated and included the full GeoHash for each city (see [Chapter
    5](ch05.xhtml "Chapter 5. Spark for Geographic Analysis")*, Spark for Geographic
    Analysis* for more information), simplifying how the data can be indexed and summarized
    for larger geographic regions.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The granularity of the file is based on the aggregation key used to produce
    the dataset, which is: `V21Date`, `LocCountryCode`, `Lat`, `Long`, `GeoHash`,
    `Language`, `Series.`'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have carried forward the primary location country code field, identified
    in the GKG feed, into the city level aggregation function; this allows us to quickly
    examine the data by countries without having to perform complex lookups.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The provided data is un-normalized. We should later normalize it via the total
    article word count for the location, which is available in the series called `wc`.
    But this should only be done for word count based sentiment measures. We also
    carry a count of the articles so we can test different types of normalization.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The feed is built from the English language GKG records, but we plan to include
    the international *Translingual* feeds in the same data format. In readiness,
    we've included a field denoting the original news story language.
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have an ingestion routine for this dataset to GeoMesa, a scalable data store
    that allows us to geographically explore the resulting data; this is available
    in our code repository. For an in-depth exploration of GeoMesa, see [Chapter 5](ch05.xhtml
    "Chapter 5. Spark for Geographic Analysis"), *Spark for Geographic Analysis*.
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is a pipeline to build up the GeoGCAM files:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'This query essentially does the following: It builds a Cartesian join between
    all the GCAM sentiments and the granular locations identified in the records (cities
    / places), and then proceeds to *place* the Tone and sentiment values on those
    locations for all news stories in a 15 minute window. The output is a spatio-temporal
    dataset that allows us to geographically map the GCAM sentiments. For instance,
    it is possible to quickly export and plot this data in QGIS, which is an open
    source mapping tool.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: Does our spatial pivot work?
  id: totrans-337
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When the preceding GeoGCAM dataset is filtered to look at the GCAM *immigration* sentiment
    as a theme over the first two weeks of GKG data in February 2015, we can generate
    the following map:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: '![Does our spatial pivot work?](img/image_04_016.jpg)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
- en: This illustrates the tone of global English language news media, using light (positive
    average tone) and dark (negative average tone), which is found in the GKG files
    over that period, and explores how that tone maps over each geographic tile on
    the map (the pixel size calculated mirrors fairly accurately the size of the truncated
    GeoHash that is grouped on) with respect to the sentiment for immigration as a
    theme.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: We can see very clearly on this map that immigration is not only a hot topic
    associated with places in the UK, but also has strong spatial concentrations in
    other places too. For instance, we can see the strong negative tone associated
    with parts of the Middle East that clearly stands out in a concentrated dark block.
    We also see details that we perhaps would have missed before. For example, there
    is a concentrated negative tone on immigration around Dublin, which is not immediately
    explainable, and something seems to be happening in the north east of Nigeria
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: The map shows that there may also be an English language bias to watch out for,
    as there is little discussion in non-English speaking places, which seems odd,
    until we realize we've not yet included the Translingual GKG feed. This suggests
    that we should extend our processing to include the translingual data source,
    in order to obtain a more rounded and full set of signals including non-English
    news media.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: 'The full list of the GCAM time series available is listed in the GCAM master
    codebook found here: [http://data.gdeltproject.org/documentation/GCAM-MASTER-CODEBOOK.TXT](http://data.gdeltproject.org/documentation/GCAM-MASTER-CODEBOOK.TXT).'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: For the moment, the English language news data examined in the GeoGCAM format
    provides a fascinating view of the world, and we discover that GDELT does offer
    real signals we can leverage. Using the GeoGCAM formatted data developed in this
    chapter, you should now be able to construct your own specific geopolitical explorations
    easily and quickly, even integrating this content with your own datasets.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-345
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've reviewed many ideas for exploring data quality and data
    content. We have also introduced the reader to tools and techniques for working
    with GDELT, which are aimed at encouraging the reader to expand their own investigations.
    We have demonstrated rapid development in Zeppelin, and written much of our code
    in SparkSQL to demonstrate the excellent portability of this method. As the GKG
    files are so complex in terms of content, much of the rest of this book is dedicated
    to in-depth analyses that move beyond exploration, and we step away from SparkSQL
    as we dig deeper into the Spark codebase.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter,that is, [Chapter 5](ch05.xhtml "Chapter 5. Spark for Geographic
    Analysis"), *Spark for Geographic Analysis*, we will explore GeoMesa; an ideal
    tool for managing and exploring the GeoGCAM dataset created in this chapter, as
    well as GeoServer and the GeoTools toolsets to further expand our knowledge of
    spatio-temporal exploration and visualization.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
