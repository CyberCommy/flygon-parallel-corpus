- en: Chapter 4. Exploratory Data Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Exploratory Data Analysis** (**EDA**) performed in commercial settings is
    generally commissioned as part of a larger piece of work that is organized and
    executed along the lines of a feasibility assessment. The aim of this feasibility
    assessment, and thus the focus of what we can term an *extended EDA*, is to answer
    a broad set of questions about whether the data examined is fit for purpose and
    thus worthy of further investment.'
  prefs: []
  type: TYPE_NORMAL
- en: Under this general remit, the data investigations are expected to cover several
    aspects of feasibility that include the practical aspects of using the data in
    production, such as its timeliness, quality, complexity, and coverage, as well
    as being appropriate for the intended hypothesis to be tested. While some of these
    aspects are potentially less fun from a data science perspective, these data quality
    led investigations are no less important than purely statistical insights. This
    is especially true when the datasets in question are very large and complex and
    when the investment needed to prepare the data for the data science might be significant.
    To illustrate this point, and to bring the topic to life, we present methods for
    doing an EDA of the vast and complex **Global Knowledge Graph** (**GKG**) data
    feeds, made available by the **Global Database of Events, Language and Tone**
    (**GDELT**) project.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will create and interpret an EDA while covering the following
    topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the problems and design goals for planning and structuring an
    Extended Exploratory Data Analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What data profiling is, with examples, and how a general framework for data
    quality can be formed around the technique for continuous data quality monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to construct a general *mask-based* data profiler around the method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to store the exploratory metrics to a standard schema, to facilitate the
    study of data drift in the metrics over time, with examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use Apache Zeppelin notebooks for quick EDA work, and for plotting charts
    and graphs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to extract and study the GCAM sentiments in GDELT, both as time series and
    as spatio-temporal datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to extend Apache Zeppelin to generate custom charts using the `plot.ly`
    library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The problem, principles and planning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will explore why an EDA might be required and discuss the
    important considerations for creating one.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the EDA problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A difficult question that precedes an EDA project is: *Can you give me an estimate
    and breakdown of your proposed EDA costs, please?*
  prefs: []
  type: TYPE_NORMAL
- en: 'How we answer this question ultimately shapes our EDA strategy and tactics.
    In days gone by, the answer to this question typically started like this: *Basically
    you pay by the column....* This rule of thumb is based on the premise that there
    is *an iterable unit of data exploration work*, and these units of work drive
    the estimate of effort and thus the rough price of performing the EDA.'
  prefs: []
  type: TYPE_NORMAL
- en: What's interesting about this idea is that the units of work are quoted in terms
    of the *data structures to investigate* rather than *functions that need writing*.
    The reason for this is simple. Data processing pipelines of functions are assumed
    to exist already, rather than being new work, and so the quotation offered is
    actually the implied cost of configuring the new inputs' data structures to our
    standard data processing pipelines for exploring data.
  prefs: []
  type: TYPE_NORMAL
- en: This thinking brings us to the main EDA problem, that *exploring* seems hard
    to pin down in terms of planning tasks and estimating timings. The recommended
    approach is to consider explorations as configuration driven tasks. This helps
    us to structure and estimate the work more effectively, as well as helping to
    shape the thinking around the effort so that configuration is the central challenge,
    rather than the writing of a lot of ad hoc throw-away code.
  prefs: []
  type: TYPE_NORMAL
- en: The process of configuring data exploration also drives us to consider the processing
    templates we might need. We would need to configure these based on the form of
    the data we explore. For instance, we would need a standard exploration pipeline
    for structured data, for text data, for graph shaped data, for image data, for
    sound data, for time series data, and for spatial data. Once we have these templates,
    we need to simply map our input data to them and configure our ingestion filters
    to deliver a focused lens over the data.
  prefs: []
  type: TYPE_NORMAL
- en: Design principles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Modernizing these ideas for Apache Spark based EDA processing means that we
    need to design our configurable EDA functions and code with some general principles
    in mind:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Easily reusable functions/features***:* We need to define our functions to
    work on general data structures in general ways so they produce good exploratory
    features and deliver them in ways that minimize the effort needed to configure
    them for new datasets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Minimize intermediate data structures***:* We need to avoid proliferating
    intermediate schemas, helping to minimize intermediate configurations, and where
    possible create reusable data structures'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data driven configuration***:* Where possible, we need to have configurations
    that can be generated from metadata to reduce the manual boilerplate work'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Templated visualizations**: General reusable visualizations driven from common
    input schemas and metadata'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lastly, although it is not a strict principle per se, we need to construct exploratory
    tools that are flexible enough to discover data structures rather than depend
    on rigid pre-defined configurations. This helps when things go wrong, by helping
    us to reverse engineer the file content, the encodings, or the potential errors
    in the file definitions when we come across them.
  prefs: []
  type: TYPE_NORMAL
- en: General plan of exploration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The early stages of all EDA work are invariably based on the simple goal of
    establishing whether the data is of good quality. If we focus here, to create
    a general *getting started* plan that is widely applicable, then we can lay down
    a general set of tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'These tasks create the general shape of a proposed EDA project plan, which
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare source tools, source our input datasets, review the documentation, and
    so on. Review security of data where necessary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Obtain, decrypt, and stage the data in HDFS; collect **non-functional requirements**
    (**NFRs**) for planning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run code point level frequency reports on the file content.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run a population check on the amount of missing data in the files' fields.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run a low grain format profiler to check on the high cardinality fields in the
    files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run a high grain format profiler check on format-controlled fields in the files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run referential integrity checks, where appropriate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run in-dictionary checks, to verify external dimensions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run basic numeric and statistical explorations of numeric data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run more visualization-based explorations of key data of interest.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In character encoding terminology, a code point or code position is any of the
    numerical values that make up the code space. Many code points represent single
    characters, but they can also have other meanings, such as for formatting.
  prefs: []
  type: TYPE_NORMAL
- en: Preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have a general plan of action, before exploring our data, we must
    first invest in building the reusable tools for conducting the early mundane parts
    of the exploration pipeline that help us validate data; then as a second step
    investigate GDELT's content.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing mask based data profiling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A simple but effective method for quickly exploring new types of data is to
    make use of mask based data profiling. A *mask* in this context is a transformation
    function for a string that generalizes a data item into a feature, that, as a
    collection of masks, will have a lower cardinality than the original values in
    the field of study.
  prefs: []
  type: TYPE_NORMAL
- en: 'When a column of data is summarized into mask frequency counts, a process commonly
    called *data profiling*, it can offer rapid insights into the common structures
    and content of the strings, and hence reveal how the raw data was encoded. Consider
    the following mask for exploring data:'
  prefs: []
  type: TYPE_NORMAL
- en: Translate uppercase letters to *A*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Translate lowercase letters to *a*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Translate numbers, 0 through 9, to *9*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It seems like a very simple transformation at first glance. As an example, let's
    apply this mask to a high cardinality field of data, such as the GDELT GKG file's
    *V2.1 Source Common Name* field. The documentation suggests it records the common
    name of the source of the news article being studied, which typically is the name
    of the website the news article was scraped from. Our expectation is that it contains
    domain names, such as [nytimes.com](https://www.nytimes.com/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Before implementing the production solution in Spark, let''s prototype a profiler
    on the Unix command line to provide an example that we can run anywhere:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The output is a sorted count of records found in the Source Common Name column
    alongside the mask generated by the regular expression (regex). It should be very
    clear looking at the results of this *profiled data* that the field contains domain
    names - or does it? As we have only looked at the most common masks (the top 20
    in this case) perhaps the long tail of masks at the other end of the sorted list
    holds potential data quality issues at a lower frequency.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rather than looking at just the top 20 masks, or even the bottom 20, we can
    introduce a subtle change to improve the generalization ability of our mask function.
    By making the regex collapse multiple adjacent occurrences of lower case letters
    into a single `a` character, the mask''s cardinality can be reduced without really
    diminishing our ability to interpret the results. We can prototype this improvement
    with just a small change to our regex and hopefully view all the masks in one
    page of output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Very quickly, we have prototyped a mask that reduces the three thousand or so
    raw values down to a very short list of 22 values that are easily inspected by
    eye. As the long tail is now a much shorter tail, we can easily spot any possible
    outliers in this data field that could represent quality issues or special cases.
    This type of inspection, although manual, can be very powerful.
  prefs: []
  type: TYPE_NORMAL
- en: Notice, for instance, there is a particular mask in the output, `AAA Aa`, which
    doesn't have a *dot* within it, as we would expect in a domain name. We interpret
    this finding to mean we've spotted two rows of raw data that are not valid domain
    names, but perhaps general descriptors. Perhaps this is an error, or an example
    of what is known as, *illogical field use*, meaning there could be other values
    slipping into this column that perhaps should logically go elsewhere.
  prefs: []
  type: TYPE_NORMAL
- en: This is worth investigating, and it is easy to inspect those exact two records.
    We do so by generating the masks alongside the original data, then filtering on
    the offending mask to locate the original strings for manual inspection.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rather than code a very long one liner on the command line, we can inspect
    these records using a legacy data profiler called `bytefreq` (short for *byte
    frequencies*) written in awk. It has switches to generate formatted reports, database
    ready metrics, and also a switch to output masks and data side by side. We have
    open-sourced `bytefreq` specifically for readers of this book, and suggest you
    play with it to really understand how useful this technique can be: [https://bitbucket.org/bytesumo/bytefreq](https://bitbucket.org/bytesumo/bytefreq).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: When we inspect the odd mask, `A Aa`, we can see the offending text found is
    `BBC Monitoring`, and in re-reading the GDELT documentation we will see that this
    is not an error, but a known special case. It means when using this field, we
    must remember to handle this special case. One way to handle it could be by including
    a correction rule to swap this string value for a value that works better, for
    example, the valid domain name [www.monitor.bbc.co.uk](http://www.monitor.bbc.co.uk),
    which is the data source to which the text string refers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea we are introducing here is that a mask can be used as a *key* to retrieve
    offending records in particular fields. This logic leads us to the next major
    benefit of mask based profiling: the output masks are a form of *Data Quality
    Error Code*. These error codes can fall into two categories: a whitelist of *good* masks,
    and a blacklist of *bad* masks that are used to find poor quality data. Thought
    of this way, masks then form the basis for searching and retrieving data cleansing
    methods, or perhaps for throwing an alarm or rejecting a record.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The lesson is that we can create *Treatment functions* to remediate raw strings
    that are found using a particular mask calculated over data in a particular field.
    This thinking leads to the following conclusion: we can create a general framework
    around mask based profiling for doing data quality control and remediation *as
    we read data within our data reading pipeline*. This has some really advantageous
    solution properties:'
  prefs: []
  type: TYPE_NORMAL
- en: Generating data quality masks is an *on read* process; we can accept new raw
    data and write it to disk then, on read, we can generate masks only when needed
    at query time - so data cleansing can be a dynamic process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Treatment functions can then be dynamically applied to targeting remediation
    efforts that help to cleanse our data at the time of read.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because previously unseen strings are generalized into masks, new strings can
    be flagged as having quality issues even if that exact string has never been seen
    before. This generality helps us to reduce complexity, simplify our processes,
    and create reusable smart solutions - even across subject areas.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data items that create masks that do not fall either into mask white-lists,
    fix-lists, or blacklists can potentially be quarantined for attention; human analysts
    can inspect the records and hopefully whitelist them, or perhaps create new Treatments
    Functions that help to get the data out of quarantine and back into production.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data quarantines can be implemented simply as an on-read filter, and when new
    remediation functions are created to cleanse or fix data, the dynamic treatments
    applied at read time will automatically *release* the corrected data to users
    without long delays.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eventually a data quality Treatment library will be created that stabilizes
    over time. New work is mainly done by mapping and applying the existing treatments
    to new data. A phone number reformatting Treatment function, for example, can
    be widely reused over many datasets and projects.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With the method and architectural benefits now explained, the requirements
    for building a generalized mask based profiler should be clearer. Note that the
    mask generation process is a classic Hadoop MapReduce process: map input''s data
    out to masks, and reduce those masks back down to summarized frequency counts.
    Note also how, even in this short example, we have already used two types of masks
    and each is made up of a pipeline of underlying transformations. It suggests we
    need a tool that supports a library of predefined masks as well as allowing for
    user defined masks that can be created quickly and on demand. It also suggests
    there should be ways to *stack* the masks to build them up into complex pipelines.'
  prefs: []
  type: TYPE_NORMAL
- en: What may not be so obvious yet is that all data profiling done in this way can
    write profiler metrics to *a common output format.* This helps to improve reusability
    of our code through simplifying the logging, storing, retrieval, and consumption
    of the profiling data.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example we should be able to report all mask based profiler metrics using
    the following schema:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Once our metrics are captured in this single schema format, we can then build
    secondary reports using a user interface, such as Zeppelin notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Before we walk through implementing these functions, an introduction to the
    character class masks is needed as these differ slightly from the normal profiling
    masks.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing character class masks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is another simple type of data profiling that we can also apply that helps with
    file inspection. It involves profiling the actual bytes that make up a whole file.
    It is an old method, one that originally comes from cryptography where frequency
    analysis of letters in texts was used to gain an edge on deciphering substitution
    codes.
  prefs: []
  type: TYPE_NORMAL
- en: 'While not a common technique in data science circles today, byte level analysis
    is surprisingly useful when it''s needed. In the past, data encodings were a massive
    problem. Files were encoded in a range of code pages, across ASCII and EBCDIC
    standards. Byte frequency reporting was often critical to discover the actual
    encoding, delimiters, and line endings used in the files. Back, then the number
    of people who could create files, but not technically describe them, waqs surprising.
    Today, as the world moves increasingly to Unicode-based character encodings, these
    old methods need updating. In Unicode, the concept of a byte is modernized to
    multi-byte *code points*, which can be revealed in Scala using the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Using this function, we can begin to profile any international character level
    data we receive in our GDELT dataset and start to understand the complexities
    we might face in exploiting the data. But, unlike the other masks, to create interpretable
    results from code points, we require a dictionary that we can use to look up meaningful
    contextual information, such as unicode category and the unicode character names.
  prefs: []
  type: TYPE_NORMAL
- en: 'To generate a contextual lookup, we can use this quick command line hack to
    generate a reduced dictionary from the main one found at [unicode.org](http://unicode.org),
    which should help us to better report on our findings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use this dictionary, joined to our discovered code points, to report
    on the character class frequencies of each byte in the file. While it seems like
    a simple form of analysis, the results can often be surprising and offer a forensic
    level of understanding of the data we are handling, its source, and the types
    of algorithms and methods we can apply successfully to it. We will also look up
    the general Unicode Category to simplify our reports using the following lookup
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Building a mask based profiler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s walk through creating a notebook-based toolkit for profiling data in
    Spark. The mask functions we will implement are set out over several grains of
    detail, moving from file level to row level, and then to field level:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Character level masks applied across whole files are:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unicode Frequency, UTF-16 multi-byte representation (aka Code Points), at file
    level
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: UTF Character Class Frequency, at file level
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Delimiter Frequency, at row level
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'String level masks applied to fields within files are:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ASCII low grain profile, per field
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ASCII high grain profile, per field
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Population checks, per field
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up Apache Zeppelin
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we are going to be exploring our data visually, a product that could be very
    useful for mixing and matching technologies with relative ease is Apache Zeppelin.
    Apache Zeppelin is an Apache Incubator product that enables us to create a notebook,
    or worksheet, containing a mix of a number of different languages including Python,
    Scala, SQL, and Bash, which makes it ideal for working with Spark for running
    exploratory data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Code is written in a notebook style using *paragraphs* (or cells) where each
    cell can be independently executed making it easy to work on a small piece of
    code without having to repeatedly compile and run entire programs. It also serves
    as a record of the code used to produce any given output, and helps us to integrate
    visualizations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Zeppelin can be installed and run very quickly, a minimal installation process
    is explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Download and extract Zeppelin from here: [https://zeppelin.incubator.apache.org/download.html](https://zeppelin.incubator.apache.org/download.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find the conf directory and make a copy of `zeppelin-env.sh.template` named
    `zeppelin-env.sh`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alter the `zeppelin-env.sh` file, uncomment and set the `JAVA_HOME` and `SPARK_HOME`
    entries to the relevant locations on your machine.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Should you want Zeppelin to use HDFS in Spark, set the `HADOOP_CONF_DIR` entry
    to the location of your Hadoop files; `hdfs-site.xml`, `core-site.xml`, and so
    on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Start the Zeppelin service: `bin/zeppelin-daemon.sh start`. This will automatically
    pick up the changes made in `conf/zeppelin-env.sh`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On our test cluster, we are using Hortonworks HDP 2.6, and Zeppelin comes as
    part of the installation.
  prefs: []
  type: TYPE_NORMAL
- en: 'One thing to note when using Zeppelin is that the first paragraph should always
    be a declaration of external packages. Any Spark dependencies can be added in
    this way using the `ZeppelinContext`, to be run right after each restart of the
    interpreter in Zeppelin; for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: After this we can write code in any of the available languages. We are going
    to use a mix of Scala, SQL, and Bash across the notebook by declaring each cell
    using a type of interpreter, that is, `%spark`, `%sql`, and `%shell`. Zeppelin
    defaults to Scala Spark if no interpreter is given `(%spark`).
  prefs: []
  type: TYPE_NORMAL
- en: You can find the Zeppelin notebooks to accompany this chapter, as well as others
    in our code repository.
  prefs: []
  type: TYPE_NORMAL
- en: Constructing a reusable notebook
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In our code repository we have created a simple, extensible, open source data
    profiler library that can also be found here: [https://bytesumo@bitbucket.org/gzet_io/profilers.git](https://bytesumo@bitbucket.org/gzet_io/profilers.git)'
  prefs: []
  type: TYPE_NORMAL
- en: The library takes care of the framework needed to apply masks to data frames,
    including the special case where raw lines of a file are cast to a data frame
    of just one column. We won't go through all the details of that framework line
    by line, but the class of most interest is found in the file `MaskBasedProfiler.scala`,
    which also contains the definitions of each of the available mask functions.
  prefs: []
  type: TYPE_NORMAL
- en: A great way to use this library is by constructing a user-friendly notebook
    application that allows for visual exploration of data. We have prepared just
    such a notebook for our profiling using Apache Zeppelin. Next, we will walk through
    how to build our own notebook using the preceding section as a starting point.
    The data in our examples is the GDELT `event` files, which have a simple tab delimited
    format.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step to building up a notebook (or even just to play with our readymade
    one), is to copy the `profilers-1.0.0.jar` file from our library into a local
    directory that the Zeppelin user on our cluster can access, which on a Hortonworks
    installation is the Zeppelin user''s home directory on the Namenode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Then we can visit `http://{main.install.hostname}:9995` to access the Apache
    Zeppelin homepage. From that page, we can upload our notebook and follow along,
    or we can create a new one and build our own by clicking **Create new note**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Zeppelin, the first paragraph of a notebook is where we execute our Spark
    code dependencies. We''ll import the profiler jars that we''ll need later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In paragraph two, we include a small shell script to inspect the file(s) we
    want to profile to verify that we''re picking up the right ones. Note the use
    of `column` and `colrm`, both very handy Unix commands for inspecting columnar
    table data on the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In paragraph 3, 4, 5, and 6, we use Zeppelin''s facility for user input boxes
    to allow the user to configure the EDA notebook like it''s a proper web-based
    application. This allows users to configure four variables that can be reused
    in the notebook to drive further investigations: **YourMask**, **YourDelimiter**,
    **YourFilePath**, and **YourHeaders**. These look great when we hide the editors
    and adjust the alignment and size of the windows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Constructing a reusable notebook](img/image_04_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If we open the prepared notebook and click on **show editor** on any of these
    input paragraphs, we''ll see how we set those up to provide drop-down boxes in
    Zeppelin, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we have a paragraph that is used to import the functions we need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we move on to a new paragraph that configures and ingests the data we
    read in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we''ve done the configuration steps, we can start to examine our tabular
    data and discover if our reported column names match our input data. In a new
    paragraph window, we use the SQL context to simplify calling SparkSQL and running
    a query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The great thing about Zeppelin is that the output is formatted into a proper
    HTML table, which we can easily use to inspect wide files having many columns
    (for example, GDELT Event files):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Constructing a reusable notebook](img/image_04_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can see from this displayed data that our columns match the input data; therefore
    we can proceed with our analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you wish to read the GDELT event files, you can find the header file in our
    code repository.
  prefs: []
  type: TYPE_NORMAL
- en: If there are errors in the data alignment between columns and content at this
    point, it is also possible to select the first 10 rows of the RawLines Dataframe,
    configured earlier, which will display just the first 10 rows of the raw string
    based data inputs. If the data happens to be tab delimited, we'll see immediately
    a further benefit that the Zeppelin formatted output will align the columns for
    us on the raw strings automatically, much like the way that we did earlier using
    the bash command *column.*
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we will move on to study the file''s bytes, to discover details about the
    encodings within it. To do so we load our lookup tables, and then join them to
    the output of our profiler functions, which we registered earlier as a table.
    Notice how the output of the profiler can be treated directly as an SQL callable
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In a new paragraph, we can use the SQLContext to visualize the output. To help
    view the values that are skewed, we can use the SQL statement to calculate the
    log of the counts. This produces a graphic, which we could include in a final
    report, where we can toggle between raw frequencies and log frequencies.
  prefs: []
  type: TYPE_NORMAL
- en: '![Constructing a reusable notebook](img/image_04_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Because we have loaded the category of character classes, we can also adjust
    the visualization to further simplify the chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Constructing a reusable notebook](img/image_04_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'A basic check we must always run when doing an EDA is population checks, which
    we calculate using POPCHECKS. POPCHECKS is a special mask we defined in our Scala
    code that returns a `1` if a field is populated, or a `0` if it is not. When we
    inspect the result, we notice we''ll need to do some final report writing to present
    the numbers in a more directly interpretable way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![Constructing a reusable notebook](img/image_04_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can do that in two steps. Firstly, we can use an SQL case expression to convert
    the data into values of *populated* or *missing,* which should help. Then we can
    pivot this aggregate dataset by performing a `groupby` on the filename, `metricDescriptor`,
    and `fieldname` while performing a sum over the populated and the missing values.
    When we do this we can also include default values of zero where the profiler
    did not find any cases of data either being populated or missing. It's important
    to do this when we calculate percentages, to ensure that we never have null numerators
    or denominators. While this code is not as short as it could be, it illustrates
    a number of techniques for manipulating data in `SparkSQL`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice also that in `SparkSQL` we can use the SQL `coalesce` statement, which
    is not to be confused with Spark native `coalesce` functionality, for manipulating
    RDDs. In the SQL sense this function converts nulls into default values, and it
    is often used gratuitously to trap special cases in production grade code where
    data is not particularly trusted. Notable also is that sub-selects are well supported
    in `SparkSQL`. You can even make heavy use of these and Spark will not complain.
    This is particularly useful as they are the most natural way to program for many
    traditional database engineers as well as people with experience of databases
    of all kinds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is a clean reporting table about field level
    population counts in our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Constructing a reusable notebook](img/image_04_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'When graphically displayed in our Zeppelin notebook using the `stacked` bar
    chart functionality, the data produces excellent visualizations that instantly
    tell us about the levels of data population in our files:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Constructing a reusable notebook](img/image_04_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As Zeppelin's bar charts support tooltips, we can use the pointer to observe
    the full names of the columns, even if they display poorly in the default view.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, we can also include further paragraphs in our notebook to reveal the
    results of the `ASCII_HighGrain` and `ASCII_LowGrain` masks, explained earlier.
    This can be done by simply viewing the profiler outputs as a table, or using more
    advanced functionality in Zeppelin. As a table, we can try the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![Constructing a reusable notebook](img/image_04_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: To build an interactive viewer, which is useful when we look at ASCII_HighGrain
    masks that may have very high cardinalities, we can set up an SQL statement that
    accepts the value of a Zeppelin user input box, where users can type in the column
    number or the field name to retrieve just the relevant section of the metrics
    we collected.
  prefs: []
  type: TYPE_NORMAL
- en: 'We do that in a new SQL paragraph like this, with the SQL predicate being `x.fieldName
    like ''%${ColumnName}%''`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This creates an interactive user window that refreshes on user input, creating
    a dynamic profiling report having several output configurations. Here we show
    the output not as a table, but as a chart of the log of the frequency counts for
    a field that should have low cardinality, the longitude of *Action* identified
    in the event file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Constructing a reusable notebook](img/image_04_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The result shows us that even a simple field like Longitude has a large spread
    of formats in the data.
  prefs: []
  type: TYPE_NORMAL
- en: The techniques reviewed so far should help create a very reusable notebook for
    performing exploratory data profiling on all our input data, both quickly and
    efficiently, producing graphical outputs that we can use to produce great reports
    and documentation about input file quality.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring GDELT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A large part of the EDA journey is obtaining and documenting the sources of
    data, and GDELT content is no exception. After researching the GKG datasets, we
    discovered that it was challenging just to document the actual sources of data
    we should be using. In the following sections, we provide a comprehensive listing
    of the resources we located for use, which will need to be run in the examples.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A cautionary note on download times: using a typical 5 Mb home broadband, 2000
    GKG files takes approximately 3.5 hours to download. Given that the GKG English
    language files alone have over 40,000 files, this could take a while to download.'
  prefs: []
  type: TYPE_NORMAL
- en: GDELT GKG datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We should be using the latest GDELT data feed, version 2.1 as of December 2016\.
    The main documentation for this data is here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/documentation/GDELT-Global_Knowledge_Graph_Codebook-V2.1.pdf](http://data.gdeltproject.org/documentation/GDELT-Global_Knowledge_Graph_Codebook-V2.1.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we have included the data and secondary references
    to look up tables, and further documentation.
  prefs: []
  type: TYPE_NORMAL
- en: The files
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: GKG-English Language Global Knowledge Graph (v2.1)
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/gdeltv2/masterfilelist.txt](http://data.gdeltproject.org/gdeltv2/masterfilelist.txt)'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/gdeltv2/lastupdate.txt](http://data.gdeltproject.org/gdeltv2/lastupdate.txt)'
  prefs: []
  type: TYPE_NORMAL
- en: GKG-Translated - Non-English Global Knowledge Graph
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/gdeltv2/lastupdate-translation.txt](http://data.gdeltproject.org/gdeltv2/lastupdate-translation.txt)'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/gdeltv2/masterfilelist-translation.txt](http://data.gdeltproject.org/gdeltv2/masterfilelist-translation.txt)'
  prefs: []
  type: TYPE_NORMAL
- en: GKG-TV (Internet Archive - American Television Global Knowledge Graph)
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/gdeltv2_iatelevision/lastupdate.txt](http://data.gdeltproject.org/gdeltv2_iatelevision/lastupdate.txt)'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/gdeltv2_iatelevision/masterfilelist.txt](http://data.gdeltproject.org/gdeltv2_iatelevision/masterfilelist.txt)'
  prefs: []
  type: TYPE_NORMAL
- en: GKG-Visual - CloudVision
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/gdeltv2_cloudvision/lastupdate.txt](http://data.gdeltproject.org/gdeltv2_cloudvision/lastupdate.txt)'
  prefs: []
  type: TYPE_NORMAL
- en: Special collections
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: GKG-AME - Africa And Middle East Global Knowledge Graph
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/gkgv2_specialcollections/AME-GKG.CIA.gkgv2.csv.zip](http://data.gdeltproject.org/gkgv2_specialcollections/AME-GKG.CIA.gkgv2.csv.zip)'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/gkgv2_specialcollections/AME-GKG.CORE.gkgv2.csv.zip](http://data.gdeltproject.org/gkgv2_specialcollections/AME-GKG.CORE.gkgv2.csv.zip)'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/gkgv2_specialcollections/AME-GKG.DTIC.gkgv2.csv.zip](http://data.gdeltproject.org/gkgv2_specialcollections/AME-GKG.DTIC.gkgv2.csv.zip)'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/gkgv2_specialcollections/AME-GKG.IADISSERT.gkgv2.csv.zip](http://data.gdeltproject.org/gkgv2_specialcollections/AME-GKG.IADISSERT.gkgv2.csv.zip)'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/gkgv2_specialcollections/AME-GKG.IANONDISSERT.gkgv2.csv.zip](http://data.gdeltproject.org/gkgv2_specialcollections/AME-GKG.IANONDISSERT.gkgv2.csv.zip)'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/gkgv2_specialcollections/AME-GKG.JSTOR.gkgv2.csv.zip](http://data.gdeltproject.org/gkgv2_specialcollections/AME-GKG.JSTOR.gkgv2.csv.zip)'
  prefs: []
  type: TYPE_NORMAL
- en: GKG-HR (Human Rights Collection)
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/gkgv2_specialcollections/HR-GKG.AMNESTY.gkgv2.csv.zip](http://data.gdeltproject.org/gkgv2_specialcollections/HR-GKG.AMNESTY.gkgv2.csv.zip)'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/gkgv2_specialcollections/HR-GKG.CRISISGROUP.gkgv2.csv.zip](http://data.gdeltproject.org/gkgv2_specialcollections/HR-GKG.CRISISGROUP.gkgv2.csv.zip)'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/gkgv2_specialcollections/HR-GKG.FIDH.gkgv2.csv.zip](http://data.gdeltproject.org/gkgv2_specialcollections/HR-GKG.FIDH.gkgv2.csv.zip)'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/gkgv2_specialcollections/HR-GKG.HRW.gkgv2.csv.zip](http://data.gdeltproject.org/gkgv2_specialcollections/HR-GKG.HRW.gkgv2.csv.zip)'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/gkgv2_specialcollections/HR-GKG.ICC.gkgv2.csv.zip](http://data.gdeltproject.org/gkgv2_specialcollections/HR-GKG.ICC.gkgv2.csv.zip)'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/gkgv2_specialcollections/HR-GKG.OHCHR.gkgv2.csv.zip](http://data.gdeltproject.org/gkgv2_specialcollections/HR-GKG.OHCHR.gkgv2.csv.zip)'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/gkgv2_specialcollections/HR-GKG.USSTATE.gkgv2.csv.zip](http://data.gdeltproject.org/gkgv2_specialcollections/HR-GKG.USSTATE.gkgv2.csv.zip)'
  prefs: []
  type: TYPE_NORMAL
- en: Reference data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/documentation/GCAM-MASTER-CODEBOOK.TXT](http://data.gdeltproject.org/documentation/GCAM-MASTER-CODEBOOK.TXT)'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/supportingdatasets/GNS-GAUL-ADM2-CROSSWALK.TXT.zip](http://data.gdeltproject.org/supportingdatasets/GNS-GAUL-ADM2-CROSSWALK.TXT.zip)'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/supportingdatasets/DOMAINSBYCOUNTRY-ENGLISH.TXT](http://data.gdeltproject.org/supportingdatasets/DOMAINSBYCOUNTRY-ENGLISH.TXT)'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.gdeltproject.org/supportingdatasets/DOMAINSBYCOUNTRY-ALLLANGUAGES.TXT](http://data.gdeltproject.org/supportingdatasets/DOMAINSBYCOUNTRY-ALLLANGUAGES.TXT)'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.unicode.org/Public/UNIDATA/UnicodeData.txt](http://www.unicode.org/Public/UNIDATA/UnicodeData.txt)'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.geonames.org/about.html](http://www.geonames.org/about.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the GKG v2.1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we review existing articles that explore the GDELT data feeds, we find
    many studies that focus on the people, themes, and tone of the articles, and some
    that focus on the earlier event files. But there is not much published that explores
    the **Global Content Analysis Measures** (**GCAM**) content that is now included
    in the GKG files. When we try to use the data quality workbook we've built to
    examine the GDELT data feed, we discover that the Global Knowledge Graph is hard
    to work with, as the files are encoded using multiple nested delimiters. Working
    with this nested format data quickly is the key challenge in working with the
    GKG, and indeed the GCAM, and is the focus of the rest of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some obvious questions we need to answer as part of exploring the
    GCAM data in the GKG files:'
  prefs: []
  type: TYPE_NORMAL
- en: What are the differences between the English language GKG files and the translated
    *Translingual* international files? Are there differences in how the data is populated
    between these feeds, given that some of the entity recognition algorithms might
    not work well on translated files?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the translated data is well populated for the GCAM sentiment metrics dataset,
    included in the GKG files, can it (or indeed the English versions) be trusted?
    How can we access and normalize this data, and does it hold valuable signals rather
    than noise?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we can answer just these two questions alone, we will have established much
    about the usefulness of GDELT as a source of signals from which to perform data
    science. However, *how* we answer those questions is important, and we need to
    try and template our code as we obtain those answers, to create reusable configuration
    driven EDA components. If we can create re-purposable explorations in line with
    our principles, we will drive out far more value than hardcoding our analysis.
  prefs: []
  type: TYPE_NORMAL
- en: The Translingual files
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's reuse our earlier work to reveal some of the quality issues, then extend
    our explorations to these more detailed and complex questions. By running some
    of the population count (POPCHECK) metrics to a temporary file, for both the normal
    GKG data and the translated files, we can import and union the results together.
    This is a benefit of having a standardized metrics format that we reuse; we can
    easily perform comparisons across datasets!
  prefs: []
  type: TYPE_NORMAL
- en: 'Rather than go through the code in detail, we''ll deliver some headline answers.
    When we examine the population counts between the English and the translated GKG
    files we do expose some differences in the content available:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Translingual files](img/image_04_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We see here that the translated GKG translingual files have no Quotations data
    at all and that they are very under populated when identifying Persons versus
    the population counts we are seeing in the general English language news feed.
    So there are definitely some differences to be mindful of.
  prefs: []
  type: TYPE_NORMAL
- en: As a consequence, we should examine carefully any content in the translingual
    data feeds that we wish to rely on in production. Later we'll see how the translated
    information in the GCAM sentiment content measures up against the native English
    language sentiments.
  prefs: []
  type: TYPE_NORMAL
- en: A configurable GCAM time series EDA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The GCAM's content is primarily made up of *Word Counts*, created by filtering
    news articles using dictionary filters and doing word counts on the synonyms that
    characterize the theme of interest. The resulting count can be normalized through
    dividing the count by the total words in the document. It also includes *Scored
    Values* delivering sentiment scores that appear to be based on directly studying
    the original language text.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can quickly summarize the range of sentiment variables to study and explore
    in the GCAM in a couple of lines of code, the output of which is annotated with
    the name of the language:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The GCAM word count based time series seems to be most fully developed, especially
    in the English language where there are 2441 sentiment measures! Working with
    such a large number of measures seems hard, even to do simple analysis. We'll
    need some tools to simplify things, and we'll need to focus our scope.
  prefs: []
  type: TYPE_NORMAL
- en: To help, we've created a simple SparkSQL-based explorer to extract and visualize
    time series data from the GCAM block of data, which specifically targets the word
    count based sentiments. It's created by cloning and adjusting our original data
    quality explorer in Zeppelin.
  prefs: []
  type: TYPE_NORMAL
- en: 'It works by adjusting it to read in the GKG file glob using a defined schema,
    and previewing just the raw data we want to focus on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The results of the early column selections isolate our content on the areas
    to explore; time (`V21Date`), sentiment(`V2GCAM`), and Source URL (`V2DocID`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: In a new Zeppelin paragraph, we create an SQLContext and carefully unravel the
    nested structure of the GCAM records. Notice the first of the inner comma delimited
    rows in the V2GCAM field hold the `wc` dimension and a measure representing the
    word count of the story for this GkgRecordID, then the other sentiment measures
    are listed. We need to unfurl this data into actual rows, as well as divide all
    word count based sentiments by the total word count for the article in `wc` to
    normalize the scores.
  prefs: []
  type: TYPE_NORMAL
- en: In the following snippets, we have designed a `SparkSQL` statement to do this
    in a typical *onion* fashion, using subselects. This is a coding style you may
    wish to learn to read if you don't know it already. It works like this - create
    the innermost selection/query and then run it to test it, then wrap it in brackets
    and continue by selecting the data into the next query process, and so on. Then
    the catalyst optimizer does its magic and optimizes the whole pipeline. It results
    in an ETL process that is both declarative and readable, and which also offers
    an ability to troubleshoot and isolate issues in any part of the pipeline, if
    that's needed. If we want to understand how to handle the nested array process,
    we can easily rebuild the following SQL, running the innermost fragment first,
    then reviewing its outputs, then expanding on it to include the next query that
    wraps it, and so on. Step by step we can then review the staged outputs to review
    how the whole statement works together to deliver the final result.
  prefs: []
  type: TYPE_NORMAL
- en: The key trick in the following query is how to apply the word count denominator
    to each of the other sentiment word counts, to normalize the values. This method
    of normalization is actually suggested in the GKG documentation, although no implementation
    hints are provided.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also of note, is how the V21Date field is converted from an integer to a date,
    which is needed to plot the time series effectively. The conversion requires that
    we pre-import the following library in addition to the others imported in the
    notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Using the `Unix_timestamp` functions, we convert the V21Date into a `Unix_timestamp`,
    which is an integer, and then convert that integer again into a date field, all
    using native Spark libraries to configure the formatting and temporal resolution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following SQL query achieves our desired investigation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The results of the query are illustrated here using Zeppelin''s time series
    viewer. It shows that the time series data is building up properly and that it
    looks very credible, having a short-lived peak on November 8 2016: the day of
    the US presidential election:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A configurable GCAM time series EDA](img/image_04_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now we have a working SQL statement to examine the GCAM sentiment scores, perhaps
    we should double-check some other measures, for example on a different but related
    topic, such as the Brexit vote in the UK.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ve selected three GCAM sentiment measures that look interesting, in addition
    to the *Election Fraud* measure, which hopefully will provide an interesting comparison
    to the results we''ve seen for the US election. The measures we''ll look at are:'
  prefs: []
  type: TYPE_NORMAL
- en: '''c18.101'' -- Immigration'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '''c18.100'' -- Democracy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '''c18.140'' -- Election'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To include them, we need to extend our query to pick up multiple normalized
    Series, and we may also need to be mindful that the results may not all fit into
    Zeppelin''s viewer, which defaults to only taking in the first 1000 results, so
    we may need to further summarize to hours or days. While not a large change, it
    will be interesting to see how extensible our existing work is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'In this second example, we further refine our base query, removing the unnecessary
    GKGRecordIDs that we didn''t use. This query also demonstrates how to filter results
    against many `Series` names using a simple set of predicates. Notice we have also
    added in a pre-grouping step using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: This random number is used to create a partition prefix key that we use in our
    inner group by statement, before going on to group again without this prefix.
    The query is written in this way as it helps to subdivide and pre-summarize *hotspotting* data
    and smooth out any pipeline bottlenecks.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we look at the results of this query in Zeppelin''s time series viewer
    we have the chance to further summarize up to hourly counts, and to translate
    the cryptic GCAM series codes into proper names using a case statement. We can
    do this in a new query, helping to isolate *specific* reporting configurations
    away from the general dataset construction query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This final query reduces the data to hourly values, which is less than the
    default 1000 row maximum that Zeppelin handles by default, additionally it generates
    a comparative time series chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A configurable GCAM time series EDA](img/image_04_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The resulting chart illustrates that there is almost no discussion at all about
    *Election Fraud* preceding the Brexit vote, but there are however spikes on *Election,*
    and that Immigration is a hotter theme than Democracy. Again, the GCAM English
    language sentiment data seems to hold real signal.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have shed some light on the English language records, we can extend
    our work to explore them against the translated data in GCAM.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a final way to complete the analysis in this notebook, we can comment out
    the filters on the specific `Series` and write a timeseries database of all the
    GCAM series data for Brexit to a parquet file in our HDFS filesystem. This allows
    us to permanently store our GCAM data to disk and even to append new data to it
    over time. The following is the code needed to either overwrite, or to append
    to a parquet file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: With the parquet files written to disk, we have now built a lightweight GCAM
    time series data store that allows us to quickly retrieve a GCAM sentiment, for
    exploration across language groups.
  prefs: []
  type: TYPE_NORMAL
- en: Plot.ly charting on Apache Zeppelin
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For our next exploration we will also extend our use of Apache Zeppelin notebooks
    to include producing `%pyspark` charts using an external charting library called
    plotly, open sourced by [https://plot.ly/](https://plot.ly/), which can be used
    to create print quality visualizations. To use plotly in our notebook, we can
    upgrade our Apache Zeppelin installation, using the code found at [https://github.com/beljun/zeppelin-plotly](https://github.com/beljun/zeppelin-plotly),
    which provides the integration needed. On its GitHub page, there are detailed
    installation instructions, and within their code base, they provide a very helpful
    example notebook. Here are some tips for installing plotly for use on an HDP cluster
    with Zeppelin:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Log into the Namenode as the Zeppelin user and change the directory to the
    Zeppelin home directory at `/home/zeppelin` where we will download the external
    code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Change the directory to where the Zeppelin `*.war` file is kept. This location
    is revealed in the Zeppelin **Configuration** tab. For example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, as per the instructions, we need to edit the index.html document found
    in the Zeppelin `war` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Once the `index.html` page is extracted we can use an editor such as vim to
    insert the `plotly-latest.min.js` script tag (as per the instructions), just before
    the body tag, and save and execute the document.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Put the edited `index.html` document back into the war file using:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Finally, log into Ambari, and use it to restart the Zeppelin service.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Follow the rest of the instructions to generate a test chart in Zeppelin.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We may need to install or update old libraries if there are issues. Log into
    the Namenode and use pip to install the packages:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: With the installation complete, we should now be able to create Zeppelin notebooks
    that generate inline plot.ly charts from the `%pyspark` paragraphs, and these
    will be created offline using the local libraries rather than the online service.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring translation sourced GCAM sentiment with plot.ly
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For this comparison, let''s focus on an interesting measure found in the GCAM
    documentation: *c6.6*; *Financial Uncertainty*. This measure counts word-matches
    made between a news story and a financially oriented *uncertainty dictionary*.
    If we trace its provenance online, we can discover the academic paper and actual
    dictionary driving the metric. However, will that dictionary based measure work
    with translated news text? To investigate this, we can review how this financial
    *Uncertainty* metric differs across six major European language groups: English,
    French, German, Spanish, Italian, and Polish with respect to the subject of Brexit.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We create a new notebook, include a *pyspark* paragraph to load plot.ly libraries
    and set them to run in offline mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we create a paragraph to read in our cached data from parquet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We then can create an SQL query that reads and prepares it for plot, and registers
    it for use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we''ve defined an adaptor, we can create the query that summarizes
    the data in our parquet file to something that will fit more easily into memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'This main payload query generates a set of data that we can load to a `pandas`
    array in `pyspark`, and which has timestamps with a plot.ly ready format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'To feed this data to plot.ly we must convert the Spark Dataframe that we generated
    into a `pandas` one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'When we perform this step, we must remember to `collect()` the dataframe, as
    well as reset the column names for `pandas` to pick up. With a `pandas` array
    now in our Python environment, we can pivot the data easily into a form that will
    facilitate time series plotting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, we include a call to generate the chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '![Exploring translation sourced GCAM sentiment with plot.ly](img/image_04_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that we have produced a working plot.ly chart of our data, we should create
    a custom visualization, which was not possible with the standard Zeppelin notebook,
    to illustrate the value that the plotly library brings to our exploration. A simple
    example is to generate some *small multiples* like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Which generates the following chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Exploring translation sourced GCAM sentiment with plot.ly](img/image_04_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This small multiple chart helps us to see that, in the Italian press, there
    seems to have been a local spike in financial Uncertainty on June 15 2016; just
    a week or so before the election. This is something we might wish to investigate
    as it is also present, to a lesser degree, in Spanish language news too.
  prefs: []
  type: TYPE_NORMAL
- en: Plotly also offers many other interesting visualizations. If you have been carefully
    reading the code snippets, you may have noticed that the parquet file includes
    the FIPS10-4 country code from the GKG files. We should be able to leverage these
    location codes to plot a choropleth map of the Uncertainty metric, using Plotly,
    and at the same time leverage our previous data processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create this geographical map, we reuse our parquet file reader query that
    we registered earlier. Unfortunately, the GKG files use FIPS 10-4 two-character
    country encoding, and Plotly uses ISO-3166 three-character country codes to automatically
    geotag the user records it processes for plotting. We can address this by using
    a case statement in our SQL to remap our codes, before summarizing them over the
    whole period of enquiry:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'With our data now prepared in a `pandas` dataframe, we can invoke the visualization
    using the following line of Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The final result is an interactive, zoomable map of the world. We will leave
    its political interpretation to the reader, but conclude technically that perhaps
    this map shows an effect to do with news volume that we could later normalize
    on; for instance by dividing our values by total stories per country.
  prefs: []
  type: TYPE_NORMAL
- en: '![Exploring translation sourced GCAM sentiment with plot.ly](img/image_04_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Concluding remarks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It is worth pointing out that there are a number of parameters that drove our
    EDA across all of our investigations, and we could consider how these might be
    parameterized to build proper exploration products for monitoring GDELT. Parameters
    for consideration are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We can select a non-GCAM field to filter on. In the preceding examples, it is
    configured to the V2DocID, which is the URL of the story. Finding words in the
    URL such as BREXIT or TRUMP will help to scope our investigations to stories that
    are relevant to particular subject areas. We could also reuse this technique to
    filter on BBC or NYTIMES, for example. Alternatively, if we swapped this column
    for another, such as Theme or Person, then these columns would offer new ways
    to focus our study on particular subjects or people of interest.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have converted and generalized the granularity of the timestamp, V21Date,
    to deliver hourly time series increments, but we could reconfigure this to create
    our time series on a monthly, weekly, or daily basis - or indeed on any other
    increment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We first selected and scoped our investigation to one timeseries of interest,
    *c18_134*, which is *Election Fraud*, but we can easily reconfigure this to look
    at *Immigration* or *Hate Speech* or any of the other 2400+ sentiment scores that
    are word count based.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have introduced a file glob at the start of our notebook, which scopes the
    amount of time that we include in the summary output. To keep costs low, we've
    kept it small to start with, but we could refocus this time range on key events,
    or even open it up to all of the available files, given enough processing budget
    (time and money).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have now illustrated that our code can be easily adjusted to build a notebook-based
    GCAM time-series explorer, from which we would be able to construct huge numbers
    of focused investigations on demand; each exploring the content of the GCAM data
    in a configurable way.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have been carefully following the SQL code throughout the notebook,
    and were wondering why it had not been written using the Python API, or perhaps
    using idiomatic Scala, we will complete this section with one final observation:
    it is precisely because it is constructed from SQL that it can be moved between
    the Python, R, or Scala contexts with almost no cost in code refactoring. Should
    a new charting facility in R become available, it can be ported easily over to
    R, and then effort can be focused solely on the visualization. Indeed, with the
    arrival of Spark 2.0+, it is perhaps the SQL code that requires the least review
    when porting. The importance of code portability cannot be stressed enough. The
    most valuable benefit of using SQL in the EDA context, however, is that it makes
    generating parameter driven notebooks in Zeppelin so easy, as we have seen in
    the earlier profiler section. Drop-down boxes, and other UI widgets, can all be
    created in conjunction with string processing to customize the code before execution,
    irrespective of backend language. This is an extremely fast way to build interactivity
    and configuration into our analysis, without dipping into complex meta programming
    methods. It also helps us to avoid solving those meta programming complexities
    across the different language back ends available in Apache Zeppelin/Spark.'
  prefs: []
  type: TYPE_NORMAL
- en: With respect to building broad data explorations, if we wished to use our cached
    results in parquet more broadly, there is also an opportunity to remove the need
    for "eyeballs looking at charts" altogether. See [Chapter 12](ch12.xhtml "Chapter 12. TrendCalculus"),
    *TrendCalculus* to get an idea for how we could programmatically study trends
    across all of the data in GKG programmatically.
  prefs: []
  type: TYPE_NORMAL
- en: A final trick of note when using Zeppelin, to produce graphics for EDA reports,
    is one that is purely practical. If we wish to extract our graphics to files,
    to include them in our final report for example, rather than taking screenshots
    of our notebook, we can directly extract the scalable vector graphics files (SVG)
    from Zeppelin and download them to files using the *bookmarklet* found here [http://nytimes.github.io/svg-crowbar/](http://nytimes.github.io/svg-crowbar/).
  prefs: []
  type: TYPE_NORMAL
- en: A configurable GCAM Spatio-Temporal EDA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another question about the GCAM remains unanswered; how do we start to understand
    how it subdivides spatially? Could a geospatial pivot of the GCAM expose how the
    global news media presents its aggregate geopolitical views, as detailed geographies
    that get beneath country level analysis?
  prefs: []
  type: TYPE_NORMAL
- en: If we can construct such a dataset as part of our EDA, it would have many and
    varied applications. At a city level for example, it would be a general geopolitical
    signals library that could enrich a wide range of other data science projects.
    Consider holiday travel booking patterns, shown against the backdrop of geopolitical
    themes emerging in the news. Would we discover that global news signals at city
    level predicts rising or falling tourism rates in places of media interest? The
    possibilities for this type of data are nearly endless when we consider the resulting
    information as a source of geopolitical situational awareness.
  prefs: []
  type: TYPE_NORMAL
- en: With such an opportunity in front of us, we need to consider carefully our investment
    in this more complex EDA. It will need, as before, a common data structure from
    which to start our explorations.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a target, we will aim to construct the following dataframe from which to
    explore the geopolitical trends, which we will call "*GeoGcam*":'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Introducing GeoGCAM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: GeoGcam is a global spatio-temporal signals dataset that is derived from the
    raw GDELT Global Knowledge Graph (2.1). It enables the exploration of evolving
    geopolitical trends in global news media sentiment quickly and easily. The data
    itself is created using a transformation pipeline that casts the raw GKG files
    into a standard, reusable, global time/space/sentiment signals format that allows
    for direct downstream spatio-temporal analysis, cartographic visualization, and
    further wide scale geopolitical trend analysis.
  prefs: []
  type: TYPE_NORMAL
- en: It can be used as a source of external covariates for predictive models, especially
    ones that require improved geopolitical situational awareness.
  prefs: []
  type: TYPE_NORMAL
- en: It is constructed by recasting the GKG's GCAM sentiment data into a spatially
    oriented schema. This is performed by *placing* each news story's sentiments against
    each of the fine-grained city/town level locations identified in its GKG record.
  prefs: []
  type: TYPE_NORMAL
- en: The data is then aggregated by city, across all of the indexed stories in a
    15 minute GKG time window. The result is a file that delivers an aggregate news
    media *sentiment consensus* across all stories in that space and time window,
    for that place. Although there will be noise, our hypothesis is that big broad
    geopolitical themes will emerge.
  prefs: []
  type: TYPE_NORMAL
- en: 'A sample of the dataset (which matches the target schema) is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Technical notes on the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: Only news articles tagged with specific city locations are included, meaning
    only those tagged by GKG as having a location type code of 3=USCITY or 4=WORLDCITY.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have calculated and included the full GeoHash for each city (see [Chapter
    5](ch05.xhtml "Chapter 5. Spark for Geographic Analysis")*, Spark for Geographic
    Analysis* for more information), simplifying how the data can be indexed and summarized
    for larger geographic regions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The granularity of the file is based on the aggregation key used to produce
    the dataset, which is: `V21Date`, `LocCountryCode`, `Lat`, `Long`, `GeoHash`,
    `Language`, `Series.`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have carried forward the primary location country code field, identified
    in the GKG feed, into the city level aggregation function; this allows us to quickly
    examine the data by countries without having to perform complex lookups.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The provided data is un-normalized. We should later normalize it via the total
    article word count for the location, which is available in the series called `wc`.
    But this should only be done for word count based sentiment measures. We also
    carry a count of the articles so we can test different types of normalization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The feed is built from the English language GKG records, but we plan to include
    the international *Translingual* feeds in the same data format. In readiness,
    we've included a field denoting the original news story language.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have an ingestion routine for this dataset to GeoMesa, a scalable data store
    that allows us to geographically explore the resulting data; this is available
    in our code repository. For an in-depth exploration of GeoMesa, see [Chapter 5](ch05.xhtml
    "Chapter 5. Spark for Geographic Analysis"), *Spark for Geographic Analysis*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is a pipeline to build up the GeoGCAM files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'This query essentially does the following: It builds a Cartesian join between
    all the GCAM sentiments and the granular locations identified in the records (cities
    / places), and then proceeds to *place* the Tone and sentiment values on those
    locations for all news stories in a 15 minute window. The output is a spatio-temporal
    dataset that allows us to geographically map the GCAM sentiments. For instance,
    it is possible to quickly export and plot this data in QGIS, which is an open
    source mapping tool.'
  prefs: []
  type: TYPE_NORMAL
- en: Does our spatial pivot work?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When the preceding GeoGCAM dataset is filtered to look at the GCAM *immigration* sentiment
    as a theme over the first two weeks of GKG data in February 2015, we can generate
    the following map:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Does our spatial pivot work?](img/image_04_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This illustrates the tone of global English language news media, using light (positive
    average tone) and dark (negative average tone), which is found in the GKG files
    over that period, and explores how that tone maps over each geographic tile on
    the map (the pixel size calculated mirrors fairly accurately the size of the truncated
    GeoHash that is grouped on) with respect to the sentiment for immigration as a
    theme.
  prefs: []
  type: TYPE_NORMAL
- en: We can see very clearly on this map that immigration is not only a hot topic
    associated with places in the UK, but also has strong spatial concentrations in
    other places too. For instance, we can see the strong negative tone associated
    with parts of the Middle East that clearly stands out in a concentrated dark block.
    We also see details that we perhaps would have missed before. For example, there
    is a concentrated negative tone on immigration around Dublin, which is not immediately
    explainable, and something seems to be happening in the north east of Nigeria
  prefs: []
  type: TYPE_NORMAL
- en: The map shows that there may also be an English language bias to watch out for,
    as there is little discussion in non-English speaking places, which seems odd,
    until we realize we've not yet included the Translingual GKG feed. This suggests
    that we should extend our processing to include the translingual data source,
    in order to obtain a more rounded and full set of signals including non-English
    news media.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full list of the GCAM time series available is listed in the GCAM master
    codebook found here: [http://data.gdeltproject.org/documentation/GCAM-MASTER-CODEBOOK.TXT](http://data.gdeltproject.org/documentation/GCAM-MASTER-CODEBOOK.TXT).'
  prefs: []
  type: TYPE_NORMAL
- en: For the moment, the English language news data examined in the GeoGCAM format
    provides a fascinating view of the world, and we discover that GDELT does offer
    real signals we can leverage. Using the GeoGCAM formatted data developed in this
    chapter, you should now be able to construct your own specific geopolitical explorations
    easily and quickly, even integrating this content with your own datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've reviewed many ideas for exploring data quality and data
    content. We have also introduced the reader to tools and techniques for working
    with GDELT, which are aimed at encouraging the reader to expand their own investigations.
    We have demonstrated rapid development in Zeppelin, and written much of our code
    in SparkSQL to demonstrate the excellent portability of this method. As the GKG
    files are so complex in terms of content, much of the rest of this book is dedicated
    to in-depth analyses that move beyond exploration, and we step away from SparkSQL
    as we dig deeper into the Spark codebase.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter,that is, [Chapter 5](ch05.xhtml "Chapter 5. Spark for Geographic
    Analysis"), *Spark for Geographic Analysis*, we will explore GeoMesa; an ideal
    tool for managing and exploring the GeoGCAM dataset created in this chapter, as
    well as GeoServer and the GeoTools toolsets to further expand our knowledge of
    spatio-temporal exploration and visualization.
  prefs: []
  type: TYPE_NORMAL
