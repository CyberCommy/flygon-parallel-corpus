- en: Image Recognition with Vue.js and TensorFlow.js
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the hottest topics in computing at the moment is machine learning. In
    this chapter, we are going to step into the world of machine learning and look
    at using the popular `TensorFlow.js` package to perform image classification,
    as well as to detect poses. As a break from Angular and React, we will move on
    to Vue.js to provide our client implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: What machine learning is and how it relates to AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to install Vue
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating an application with Vue
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Showing a home page with the Vue template
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using routing in Vue
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What **Convolutional Neural Networks** (**CNNs**) are
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How models are trained in TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building an image classification class using pre-trained TensorFlow models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The image types that TensorFlow supports for image classification and pose detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using pose detection to show body joints
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The finished project can be downloaded from [https://github.com/PacktPublishing/Advanced-TypeScript-3-Programming-Projects/tree/master/chapter09](https://github.com/PacktPublishing/Advanced-TypeScript-3-Programming-Projects/tree/master/chapter09).
    This project uses TensorFlow, so the following additional components will be used
    in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '`@tensorflow-models/mobilenet`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`@tensorflow-models/posenet`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`@tensorflow/tfjs`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will also use Bootstrap with Vue, so we will need to install the following
    Bootstrap components:'
  prefs: []
  type: TYPE_NORMAL
- en: '`bootstrap`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bootstrap-vue`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After downloading the project, you will have to install the package requirements
    using the `npm install` command.
  prefs: []
  type: TYPE_NORMAL
- en: What is machine learning and how does TensorFlow fit in?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It's very hard nowadays to get away from the idea of artificially intelligent
    machines. People have become accustomed to having access to tools such as Siri,
    Alexa, and Cortana, which create the appearance that the technology understands
    us and is able to interact with us. These voice-activated systems use natural
    language processing to recognize sentences such as, *What's the weather like in
    Kos today?*
  prefs: []
  type: TYPE_NORMAL
- en: The magic behind these systems is machine learning. To pick one of these systems,
    we will quickly look at what Alexa does behind the scenes before we look at how
    machine learning relates to AI.
  prefs: []
  type: TYPE_NORMAL
- en: When we ask Alexa a question, *she* recognizes *her* name so that she knows
    that she should start listening to what comes after to start processing. This
    is the software equivalent of tapping someone on their shoulder to get their attention.
    Alexa then records the following sentence until a point is reached where Alexa
    can transmit the recording via the internet to the Alexa voice service. This incredibly
    sophisticated service parses the recording as best it can (sometimes, heavy accents
    can confuse the service). The service then acts on the parsed recording and sends
    the results back to your Alexa device.
  prefs: []
  type: TYPE_NORMAL
- en: As well as answering questions about the weather, there is an exhausting number
    of Alexa skills that users can use, and Amazon encourages developers to create
    skills that go beyond what they have time to come up with. This means that it's
    as easy to order a pizza as it is to check the latest racing results.
  prefs: []
  type: TYPE_NORMAL
- en: This preamble leads us to the point where we start to touch on what machine
    learning has to do with Alexa. The software behind Alexa uses machine learning
    to continually update itself, so every time it makes a mistake, this is fed back
    in so that the system is *smarter* the next time around and doesn't make that
    mistake in the future.
  prefs: []
  type: TYPE_NORMAL
- en: As you can imagine, interpreting speech is a hugely complicated task. It's something
    that we, as humans, learn from an early age, and the analogy with machine learning
    is quite breathtaking because we also learn speech through repetition and reinforcement.
    So, when a baby randomly says *dada*, the baby has learned to make the sounds,
    but does not know the correct context for the sound yet. Reinforcement, usually
    provided by the parents pointing to themselves, is used to link the sound to a
    person. Similar reinforcement takes place when we use picture books; when we teach
    a baby the word *cow*, we point to a picture of a cow. That way, the baby learns
    to associate the word with the picture.
  prefs: []
  type: TYPE_NORMAL
- en: Since speech interpretation is so complicated, it takes a huge amount of processing
    power, and it also requires a huge pre-trained dataset. Imagine how frustrating
    it would be if we had to teach Alexa everything. This is partly why machine learning
    systems are only really coming into their own now. We now have enough infrastructure
    in place to offload the computations to reliable, powerful, and dedicated machines.
    Additionally, we now have internet that is, by and large, powerful and fast enough
    to cope with the vast amounts of data that is being transmitted to these machine
    learning systems. We certainly wouldn't have been able to do half of what we can
    do now if we were still running on 56K modems.
  prefs: []
  type: TYPE_NORMAL
- en: What is machine learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We know that computers are good at yes and no answers, or 1s and 0s, if you
    like. This means that a computer fundamentally cannot reply with an *-ish* answer,
    so it cannot say yes-ish to a question. Bear with me for a moment, as this will
    become clear shortly.
  prefs: []
  type: TYPE_NORMAL
- en: At its most basic level, we can say that machine learning boils down to teaching
    computers to learn in the same way that we do. They learn to interpret data from
    all sorts of sources and use this learning to classify that data. The machine
    will learn from successes and failures, which will, in turn, make it more accurate
    and capable of making even more complex inferences.
  prefs: []
  type: TYPE_NORMAL
- en: Getting back to the idea of computers working with yes or no answers, when we
    come up with an answer that amounts to *well, it depends*, we are largely coming
    up with multiple answers based on the same input—the equivalent of multiple routes
    through to yes or no answers. Machine learning systems are becoming much better
    at learning, so the algorithms behind them are able to draw on more and more data,
    along with more and more reinforcement to make deeper connections.
  prefs: []
  type: TYPE_NORMAL
- en: Behind the scenes, machine learning applies an incredible array of algorithms
    and statistical models so that systems can perform set tasks without having to
    be given detailed instructions on how to accomplish those tasks. This level of
    inference is light years away from the way we have traditionally built applications,
    and this draws on the fact that, given the right mathematical models, computers
    are very, very good at spotting patterns. Along with that, they are doing a huge
    number of related tasks simultaneously, meaning that the mathematical models underpinning
    the learning can take the results of their calculations back in as feeds to themselves
    in order to build a better understanding of the world.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we must mention that AI and machine learning are not the same.
    Machine learning is an application of AI based on the ability to automatically
    learn without being programmed to deal with a particular task. The success of
    machine learning is based on having a sufficient amount of data for the system
    to learn for itself. There are a number of algorithm types that can be applied.
    Some are known as unsupervised learning algorithms, while others are known as
    supervised learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised algorithms take in data that has not been classified or labeled
    previously. The algorithms are run on such datasets to look for underlying or
    hidden patterns, which can be used to create inferences.
  prefs: []
  type: TYPE_NORMAL
- en: A supervised learning algorithm takes its previous learning and applies it to
    new data using labeled examples. These labeled examples help it learn the correct
    answers. Behind the scenes, there is a training dataset that learning algorithms
    use to refine their knowledge and learn from. The greater the level of training
    data, the more likely the algorithm is to be able to produce correct answers.
  prefs: []
  type: TYPE_NORMAL
- en: There are other types of algorithms, including reinforcement learning algorithms
    and semi-supervised learning algorithms, but these are outside the scope of this
    book.
  prefs: []
  type: TYPE_NORMAL
- en: What is TensorFlow and how does it relate to machine learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have talked about what machine learning is and that it could seem very daunting
    if were to try and implement it ourselves. Fortunately, there are libraries that
    help us create our own machine learning implementations. Originally created by
    the Google Brain team, TensorFlow is one such library designed to support large-scale
    machine learning and numerical computation. Initially, TensorFlow was written
    as a hybrid Python/C++ library, where Python provided the frontend API for building
    learning applications and the C++ side executed them. TensorFlow brings a number
    of machine learning and neural networking (sometimes called **deep learning**)
    algorithms together.
  prefs: []
  type: TYPE_NORMAL
- en: Given the success of the original Python implementation, we now have an implementation
    of TensorFlow (properly called `TensorFlow.js`) written in TypeScript that we
    can use in our applications. This is the version we are going to use in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Project overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The project we are going to write in this chapter is the one that excited me
    the most when I was writing the proposal for this book. I have a long-term love
    affair with all things AI; the topic fascinates me. With the rise of frameworks
    such as `TensorFlow.js` (I'll be shortening this to just TensorFlow), the ability
    to perform sophisticated machine learning has never been more readily available
    outside of academia. As I said, this chapter really got me excited, so we aren't
    going to use just one machine learning operation—we are going to use image classification
    to determine what is in a picture and we will use pose detection to draw the key
    points, such as the major joints and major facial landmarks of a person.
  prefs: []
  type: TYPE_NORMAL
- en: 'Working alongside the GitHub code, this topic should take about an hour to
    complete and, when you have finished it, it should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/55d8a477-ed6c-4ab5-bcd7-752183ee41b7.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we know what project we are going to build, we are ready to start the
    implementation. In the next section, we are going to start off by installing Vue.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with TensorFlow in Vue
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you do not already have Vue installed, the first step is to install the
    Vue **Command-Line Interface** (**CLI**). This is installed using `npm` with the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Creating our Vue-based application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our TensorFlow application is going to run entirely in the client browser. This
    means that we need to write an application to host the TensorFlow functionality.
    We are going to use Vue to provide our client, so the following steps are needed
    to automatically build our Vue application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating our client is as simple as running the `vue create` command, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This starts off the process of creating the application. There are a number
    of decision points that need to happen when going through the client creation
    process, starting off with choosing whether to accept the defaults or to manually
    select the features we want to add. Since we want to add TypeScript support, we
    need to choose the Manually select features preset. The following screenshot shows
    the steps that we will go through to select the features for our Vue application:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/56bc69f3-88ed-4234-a191-fd175d22128c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are a number of features that we could add to our project, but we are
    only interested in a few of them, so deselect Babel and choose to add TypeScript,
    Router, VueX, and Linter / Formatter from the list. Selection/deselection is accomplished
    by using the spacebar:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/f775f672-aba1-4246-bd40-b253649f4616.png)'
  prefs: []
  type: TYPE_IMG
- en: When we press *Enter*, a number of other options will be presented. Pressing
    *Enter* will set the default value for the first three options. When we get to
    the option for selecting the **linter** (short for **Lexical INTERpreter**), choose
    TSLint from the list, and then keep pressing *Enter* for the other options. A
    linter is a tool that automatically parses your code, looking for potential issues.
    It does this by looking at our code to see if it breaches a set of predefined
    rules, which could indicate that there are bugs or code-styling issues.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we have gone all the way through this process, our client will be created;
    this will take some time to complete as there is a lot of code being downloaded
    and installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/b4cb17e8-85fa-472a-afd6-a6971ed16521.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that our application has been created, we can run it by running `npm run
    serve` in the root of the client folder. Unlike Angular and React, the browser
    won''t display the page by default, so we will need to open the page for ourselves
    using `http://localhost:8080`. When we do so, the page will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/82e894fd-191f-4704-a6b0-d7c39d47429c.png)'
  prefs: []
  type: TYPE_IMG
- en: We are going to make our life easier when we write the image classifier since
    we are going to reuse some of the existing infrastructure that the Vue CLI created
    for us by modifying the home page to show our image classifier in action.
  prefs: []
  type: TYPE_NORMAL
- en: Showing a home page with the Vue template
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a similar way to React giving us the special `.jsx`/`.tsx` extensions to
    allow us to put our code and web page together, Vue provides us with single-file
    components that are created as `.vue` files. These files allow us to mix our code
    and web templates together to build up our page. Let's open up our `Home.vue`
    page and analyze it before we go on to create our first TensorFlow component.
  prefs: []
  type: TYPE_NORMAL
- en: We can see that our `.vue` component is broken up into two separate parts. There's
    a template section that defines the layout of the HTML that will be displayed
    on the screen, and there's a separate script section where we include our code.
    Since we are using TypeScript, the language for our `script` section is `ts`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The script section starts off by defining the `import` section in much the
    same way that we would see in a standard `.ts` file. Where we see `@` in the import,
    this tells us that the import path is relative to the `src` directory, so the
    `HelloWorld.vue` component is located in the `src/components` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The next thing we need to do is create a class that extends from the `Vue`
    class. What we are doing with this is using `@Component` to create a component
    registration called `Home` that can be used elsewhere:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'There is something else that we need to do. Our template is going to reference
    an external `HelloWorld` component. We must decorate our class with the components
    that the template is going to use, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The template is very straightforward. It consists of a single `div` class that
    we are going to render the `HelloWorld` component into:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: From the previous code template, we can see that, unlike React, Vue does not
    give us an explicit `render` function to deal with rendering HTML and state. Instead,
    the build-up of rendering is a lot closer to the Angular model, in which a template
    is parsed into content that can be served up.
  prefs: []
  type: TYPE_NORMAL
- en: The reason that we mentioned Angular here is because Vue.js started off being
    developed by Evan You, who was working on the AngularJS project at Google; he
    wanted to create a more performant library. While AngularJS was a great framework,
    it did require a complete buy-in to the Angular ecosystem in order to work with it
    (the Angular team is working to rectify this). So, while Vue leverages Angular
    features such as templates, it has a very light touch in that you can just add
    a script tag to your existing code and start slowly migrating your existing code
    base over to Angular.
  prefs: []
  type: TYPE_NORMAL
- en: Vue borrows concepts from React such as the use of the Virtual DOM (as we discussed
    when we introduced React). Vue also uses a virtual DOM but implements it in a
    slightly different fashion, primarily with Vue only re-rendering out components
    that have a change, where React, by default, would also re-render child components.
  prefs: []
  type: TYPE_NORMAL
- en: 'What we want to do now is modify the `HelloWorld` component so that we can
    work with TensorFlow. But, before we do that, we need to write a couple of supporting
    classes that will do the heavy lifting with TensorFlow. These aren''t big classes
    in terms of code, but they are extremely important. Our `ImageClassifier` class
    starts off with a standard class definition, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is optional, but it has a major impact on the stability of our
    application if it is running on a Windows client. Underneath the cover, TensorFlow
    uses WebGLTextures, but there is a problem with creating WebGLTextures on Windows
    platforms. To get around this issue, our constructor needs to be modified to look
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we can run the image classification any number of times, we are going
    to add a private variable that represents the standard `MobileNet` TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Introducing MobileNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, we need to take a small detour into the world of CNNs. `MobileNet`
    is a CNN model, so having a little understanding of what a CNN is helps us to
    understand how it relates to the problem we are solving here. Don't worry, we
    aren't going to dig into the mathematics behind CNNs, but knowing a little bit
    about what they do will help us appreciate what they bring to the table for us.
  prefs: []
  type: TYPE_NORMAL
- en: CNN classifiers work by taking in an input image (potentially from a video stream),
    processing the image, and classifying it under predefined categories. In order
    to understand how they work, we need to take a step back and think about the problem
    from a computer's point of view. Suppose we have a picture of a horse. To a computer,
    that picture is just a series of pixels, so if we were to show a picture of a
    slightly different horse, the computer cannot say that they match just by comparing
    pixels.
  prefs: []
  type: TYPE_NORMAL
- en: A CNN breaks the images down into pieces (say, into 3x3 grids of pixels) and
    compares those pieces. Simplistically speaking, what it is looking for is the
    number of matches that it can make with these pieces. The greater the number of
    matches, the greater the confidence that we have a match. This is a very simplified
    description of what a CNN does, which involves a number of steps and filters,
    but it should serve to provide an understanding of why we would want to use a
    CNN such as `MobileNet` in TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: '`MobileNet` is a specialist CNN that, among other things, provides us with
    image classification that has been trained against images in the ImageNet database
    ([http://www.image-net.org/](http://www.image-net.org/)). When we load the model,
    we are loading a pre-trained model that has been created for us. The reason we
    are using a pre-trained network is that it has been trained on a large dataset
    on the server. We would not want to run image classification training in the browser
    because it would require too much load being carried over from the server to the
    browser in order to perform the training. So, no matter how powerful your client
    PC is, copying over the training datasets will be too much.'
  prefs: []
  type: TYPE_NORMAL
- en: We have mentioned `MobileNetV1` and `MobileNetV2` without going into what they
    are and what datasets they were trained on. Basically, the `MobileNet` models
    were developed by Google and trained on the ImageNet dataset, a dataset containing
    1.4 million images broken down into 1,000 classes of images. The reason these
    are called `MobileNet` models is because they were trained with mobile devices
    in mind, so they are designed to run on low power and/or low storage devices.
  prefs: []
  type: TYPE_NORMAL
- en: With a pre-trained model, we can use it as it is or we could customize it to
    use it for transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: The Classify method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we understand a little bit about CNNs, we are ready to put this knowledge
    into practice. We are going to create an asynchronous classification method. TensorFlow
    can work with a number of formats when it needs to detect images, so we are going
    to generalize our method to only accept the appropriate types:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Only one of these types is specific to TensorFlow—the `Tensor3D` type. All the
    other types are standard DOM types, so this can be easily consumed in a web page
    without having to jump through numerous hoops to convert the image into a suitable
    format.
  prefs: []
  type: TYPE_NORMAL
- en: 'We haven''t introduced our `TensorInformation` interface yet. When we receive
    classifications back from `MobileNet`, we receive a classification name and a
    confidence level for the classification. This comes back as `Promise<Array<[string,
    number]>>` from the classification operation, so we convert this into something
    more meaningful for our consuming code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We now know that we are going to be returning an array of classifications and
    a probability (the confidence level). Getting back to our `Classify` method, we
    need to load `MobileNet` if it has not previously been loaded. This operation
    can take a while, which is why we cache it so that we don''t have to reload it
    the next time we call this method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We have accepted the defaults for the `load` operation. There are a number
    of options that we could have supplied if we needed to:'
  prefs: []
  type: TYPE_NORMAL
- en: '`version`: This sets the `MobileNet` version number, and defaults to 1\. Right
    now, there are two values that can be set: `1` means that we use `MobileNetV1`,
    and `2` means that we use `MobileNetV2`. Practically, for us, the difference between
    versions relates to the accuracy and performance of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`alpha`: This can be set to `0.25`, `0.5`, `0.75`, or `1`. Surprisingly, this
    has nothing to do with the `alpha` channel on an image. Instead, it refers to
    the width of the network that will be used, effectively trading accuracy for performance.
    The higher the number, the greater the accuracy. Conversely, the higher the number,
    the slower the performance. The default for the `alpha` is `1`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`modelUrl`: If we wanted to work with a custom model, we could supply this
    here.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If the model loads successfully, then we can now perform the image classification.
    This is a straightforward call to the `classify` method, taking in the `image`
    that has been passed into our method. Following the completion of this operation,
    we return the array of classification results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The `model.classify` method returns three classifications by default, but if
    we wanted to, we could pass a parameter to return a different number of classifications.
    If we wanted to retrieve the top five results, we would change the `model.classify`
    line, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, in the unlikely event that the model failed to load, we return `null`.
    With this in place, our completed `Classify` method looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: TensorFlow really can be that simple. Obviously, behind the scenes, a great
    deal of complexity has been hidden, but that is the beauty of well-designed libraries.
    They should shield us from the complexities while leaving us with room to get
    into the more complex operations and customization if we need to.
  prefs: []
  type: TYPE_NORMAL
- en: So, that's our image classification component written. How do we use it in our
    Vue application, though? In the next section, we are going to see how we modify
    the `HelloWorld` component to use this class.
  prefs: []
  type: TYPE_NORMAL
- en: Modifying the HelloWorld component to support image classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we created our Vue application, the CLI helpfully created a `HelloWorld.vue`
    file for us containing the `HelloWorld` component. We are going to take advantage
    of the fact that we already have this component, and we are going to use it to
    classify a pre-loaded image. If we wanted to, we could use it to load images using
    a file upload component and drive the classifications when this changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s take a look at what our `HelloWorld` TypeScript code looks like.
    Obviously, we are going to start with a class definition. Just like we saw earlier,
    we have marked this with the `@Component` decorator to say that this is a component:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We have two member variables that we want to declare in our class. We know
    that we want to use the `ImageClassifier` class that we have just written, so
    we will bring that one in. We also want to create an array of the `TensorInformation`
    results from the classification operation. The reason that we are going to store
    them as a class-level value is that we are going to have to bind to this when
    the operation finishes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we finish writing our class, we need to see what our template will look
    like. We start off with the `template` definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, we are using Bootstrap, so we are going to use a `div` container
    to lay out our content. The first thing we are going to add to our container is
    an image. I have chosen to use an image of a group of Border Collie dogs here,
    largely because I am a fan of dogs. In order for us to read this image inside
    TensorFlow, we need to set `crossorigin` to `anonymous`. Pay particular attention
    to `ref="dogId"` in this part because we are going to need it again shortly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Following the image, we are going to add further Bootstrap support with the `row`
    and `col` classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Inside this row, we are going to create a Bootstrap list. We saw that Vue has
    its own Bootstrap support, so we are going to use its version for list support,
    `b-list-group`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we finally get to the meat of the template. The reason that we exposed
    the array of tensors in our class was so that we could iterate over each result
    in the array when it is populated. In the following code, we create a dynamic
    number of `b-list-group-item` by using `v-for` to automatically iterate over each
    tensor item. This creates the `b-list-group-item` entry, but we still need to
    display the individual `className` and `probability` items. With Vue, we bind
    text items such as this using `{{ <<item>> }}`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The reason we have added `v-bind:key` alongside `v-for` is because Vue provides
    something it calls an **in-place patch** by default. This means that Vue uses
    this key as a hint to uniquely track the item so that it can keep the values up
    to date on changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'That''s it; our template is complete. As we can see, the following is a simple
    template, but there''s a lot going on with it. We have a Bootstrap container showing
    an image and, following that, letting Vue dynamically bind in the `tensor` details:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Getting back to our TypeScript code, we are going to write the method that
    takes the image and then uses it to call our `ImageClassifier.Classify` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we are loading an image onto our client, we have to wait for the page
    to render with the image so that we can retrieve it. We are going to call our
    `Classify` method from the constructor so, as it runs while the page is being
    created, we need to use a little trick to wait for the image to load. Specifically,
    we are going to use a Vue function called `nextTick`. It is important to understand
    that updates to the DOM happen asynchronously. When a value changes, the change
    isn''t rendered immediately. Instead, Vue requests a DOM update, which is then
    triggered by a timer. So, by using `nextTick`, we wait for the next DOM update
    tick and perform the relevant operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The reason why we mark the `async` function inside the `then` block is that
    we are going to perform an await inside this section, which means that we have
    to scope this as `async` as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the template, we defined our image with a `ref` statement because we want
    to access this from inside our class. To do that, we query the map of `ref` statements that
    Vue maintains for us here, and since we have set our own reference up with `dogId`,
    we can now access the image. This trick saves us from having to use `getElementById`
    to retrieve our HTML element:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: When we built our Vue application, the CLI automatically set up TSLint rules
    for us. One of these rules related to accessing elements via string literals.
    We disable the rule temporarily by using `tslint:disable:no-string-literal`. To
    re-enable the rule, we use `tslint:enable:no-string-literal`. There is an alternative
    way to disable this rule for a single line, which is to use `/* tslint:disable-next-line:no-string-literal
    */`. The approach you take doesn't really matter; what matters is the end result.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have a reference to the dog image, we can now cast the image to `HTMLImageElement`
    and use this in the call to our `Classify` method in the `ImageClassifier` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: When the call to `Classify` comes back, as long as the model has loaded and
    successfully found classifications, it will populate our onscreen list through
    the power of binding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Throughout our examples, I have tried to keep our code base as clean and as
    simple as possible. The code has been separated into separate classes just so
    we can create small and powerful pieces of functionality. To see why I like to
    do this, this is what our `HelloWorld` code looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: In total, including `tslint` formatters and white space, this code is only 20
    lines long. Our `ImageClassifier` class is only 22 lines long, and that's an `ImageClassifier`
    class that could be used elsewhere without modification. By keeping classes simple,
    we decrease the number of ways that they could go wrong and we increase our chances
    of being able to reuse them. More importantly, we stick to the rather unfriendly
    named **Keep It Simple, Stupid** (**KISS**) principle, which states that systems
    work best if they are inherently as simple as they can possibly be.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have seen image classification in action, we can think about adding
    pose detection to our application. Before we do so, we need to look at a couple
    of other Vue areas that are going to be important to us.
  prefs: []
  type: TYPE_NORMAL
- en: The Vue application entry point
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Something that we haven't touched on yet is what the entry point to our Vue
    application is. We have seen the `Home.vue` page, but that is just a component
    that gets rendered somewhere else. We need to take a step back and see how our
    Vue application actually handles loading itself and showing the relevant components.
    While we are doing this, we will also touch on routing in Vue so that we can see
    how that all hangs together.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our starting point is found inside the `public` folder. In there, we have an
    `index.html` file, which we can think of as being the master template for our
    application. It''s a fairly standard HTML file—we might want to give it a more
    suitable `title` (here, we''re going with `Advanced TypeScript - Machine Learning`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The important element here is the `div` with its `id` property set to `app`.
    This is the element into which we are going to render our components. The way
    we do this is controlled from the `main.ts` file. Let''s start by adding Bootstrap
    support, both by adding the Bootstrap CSS files and by registering the `BootstrapVue`
    plugin using `Vue.use`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Even though we have Bootstrap support in place, we don''t have anything that
    hooks our components into `app div`. The reason why we add this support is to
    create a new Vue application. This accepts a router, a Vue store that is used
    to contain things such as Vue state and mutations, and a `render` function, which
    is called when the component is being rendered. The `App` component that''s passed
    into our `render` method is the top-level `App` component that we will use to
    render all the other components into. When the Vue application finishes being
    created, it is mounted into the `app` div from `index.html`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Our `App.vue` template consists of two separate areas. Before we add those
    areas, let''s define the `template` element and containing `div` tag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Inside this `div` tag, we are going to add our first logical section—our good
    old friend, the navigation bar. Since these come in from the Vue Bootstrap implementation,
    they are all prefixed with `b-`, but they should need no dissection now, as they
    should be very familiar by this point:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'When the user navigates to a page, we need to display the appropriate component.
    Under the cover, the component that is displayed is controlled by the Vue router,
    but we need somewhere to display it. This is accomplished by using the following
    tag below our navigation bar:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This is what our `App` template looks like when it has been completed. As we
    can see, if we want to route to other pages, we will need to add separate `b-nav-item`
    entries to this list. If we wanted to, we could dynamically create this navigation
    list using `v-for` in a similar way to what we saw when we were building up the
    classifications with the image classifier view:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'When we first started looking at routing, all those chapters ago, you possibly
    thought that routing was a highly complicated thing to add to our applications.
    By now, you should be a lot more comfortable with routing and it isn''t going
    to be much of a surprise that it is straightforward and simple to add routing
    support in Vue. We start off by registering the `Router` plugin inside Vue using
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'With this in place, we are now ready to build routing support. We export an
    instance of `Router` that can be used in our `new Vue` call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now at the point where we need to add our routing options. The first
    option we are going to set up is the routing mode. We are going to use the HTML5
    `history` API to manage our links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: We could use URL hashing for routing. This works in all the browsers that Vue
    supports and is a good choice if the HTML5 `history` API is unavailable. Alternatively,
    there is an abstract routing mode that works across all JavaScript environments,
    including Node. If the browser API is not present, no matter what we set the mode
    to, the router will automatically be forced to use this.
  prefs: []
  type: TYPE_NORMAL
- en: The reason we want to use the `history` API is that it allows us to modify the
    URL without triggering full-page refreshes. Since we know that we only want to
    replace components, rather than replacing the whole `index.html` page, we end
    up leveraging this API to only reload the component parts of the page, without
    doing a full-page reload.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also want to set the base URL of our application. If we wanted to override
    this location to serve everything from the `deploy` folder, for instance, then
    we would set this to `/deploy/`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'While it is all well and good setting up the routing mode and the base URL,
    we are missing out on the important part here—setting the routes themselves. At
    a minimum, each route contains a path and a component. The path relates to the
    path in the URL, and the component identifies what component will be displayed
    as a result of that path. Our routes look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: We have a special path match in our route. If the user types in a URL that doesn't
    exist, then we use `*` to capture that and redirect it to a particular component.
    We must put this as the last entry because otherwise, it would take precedence
    over the exact matches. The eagle-eyed reader will notice that, strictly speaking,
    we don't need the first path because our routing would still show the `Home` component
    due to our `*` fallback.
  prefs: []
  type: TYPE_NORMAL
- en: One thing we added to our route was a reference to a component that doesn't
    exist yet. We're going to address that now by adding the `Pose` component.
  prefs: []
  type: TYPE_NORMAL
- en: Adding pose detection capabilities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we start addressing pose detection, we are going to add a component
    that will play host to the relevant functionality. As this is our first component
    *from scratch*, we''ll also cover it from scratch. Inside our `views` folder,
    create a file called `Pose.vue`. This file is going to contain three logical elements,
    so we will start off by adding those and setting up our template to use Bootstrap:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The only one of these that we haven't looked at so far is the `style` section.
    Scoped styles allow us to apply styling that just applies to the current component.
    We will apply local styling shortly, but first, we need to set up the image that
    we are going to display.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our example code, I have selected a picture that is 1,200 pixels wide and
    675 pixels high. This information is important because, when we do our pose detection,
    we are going to draw these points on the image, which means that we need to do
    a little bit of styling arrangement to put a canvas in place on which we can draw
    the points that match the locations on the image. We start off with two containers
    to hold our image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now going to add some CSS inside our style-scoped section to hardwire
    the dimensions in place. We start by setting the outside wrapper to have the dimensions
    I have just described. We then position our inside wrapper relative to our outer
    one, and set the width and height to 100% so that they fill the bounds exactly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Going back to `insideWrapper`, we need to add our image inside this. The image
    I chose for our example was a neutral pose, showing the key body points. The format
    of our image tag should look familiar, having done this already with the image
    classification code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'In the same `insideWrapper` `div` tag, just below our image, we need to add
    a canvas. We will use the canvas when we want to draw the key body points. The
    key thing with this is that the width and height of the canvas match the container
    dimensions exactly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, our `template` looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'We have added classes to the image and canvas, but we haven''t added their
    definitions. We could use one class to cover both, but I''m happy enough for us
    to have separate ones in place to set the width and height to 100%, and to position
    them absolutely inside the container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Our completed, the styling section will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: We have a couple of helper classes that we need to write at this point—one to
    do the pose detection, and the other to draw the points on the image.
  prefs: []
  type: TYPE_NORMAL
- en: Drawing the key points on the canvas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whenever we detect a pose, we receive a number of key points back with it. Each
    key point is made up of a position (the *x* and *y* coordinates), the score (or
    confidence), and the actual part that the key point represents. We want to loop
    over the points and draw them on the canvas.
  prefs: []
  type: TYPE_NORMAL
- en: 'As always, let''s start off with our class definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'We only need to get the canvas element once as it''s not going to change. This
    indicates that we could pass this as our canvas and, because we are interested
    in the two-dimensional element of the canvas, we can extract the drawing context
    directly from the canvas. With this context, we clear off any previously drawn
    elements on the canvas and set a `fillStyle` color to `#ff0300`, which we will
    use to color in our pose points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to draw our key points, we write a method that loops over each `Keypoint` instance
    and calls `fillRect` to draw the point. The rectangle is offset from the *x* and
    *y* coordinates by 2.5 pixels so that drawing a 5-pixel rectangle actually draws
    a rectangle that is roughly centered on the point:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Once finished, our `DrawPose` class looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Using pose detection on the image
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Earlier, we created an `ImageClassifier` class to perform our image classification.
    In keeping with the spirit of this class, we are now going to write a `PoseClassifier`
    class to manage the physical pose detection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'We are going to set up two private members for our class. The model is a `PoseNet`
    model, which will be populated when we call the relevant load method. `DrawPose`
    is the class we just defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Before we go any further into our pose detection code, we should start to get
    an understanding of what pose detection is, some of the things it is good for,
    and what some of the constraints are.
  prefs: []
  type: TYPE_NORMAL
- en: A brief aside about pose detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are using the term **pose detection** here, but this is also known as **pose
    estimation**. If you have not come across pose estimation, this simply refers
    to a computer vision operation where human figures are detected, either from an
    image or from a video. Once the figure(s) have been detected, the model is able
    to determine roughly where the key joints and body segments (such as the left
    ear) are.
  prefs: []
  type: TYPE_NORMAL
- en: The growth of pose detection has been rapid, and it has some obvious usages.
    For instance, we could use pose detection to perform motion capture for animation;
    studios are increasingly turning to motion capture to capture live-action performances
    and convert them into 3D images. Another usage lives in the field of sports; in
    fact, sports have many potential usages of motion capture. Suppose you are a pitcher
    in a major league baseball team. Pose detection could be used to determine whether
    your stance was correct at the point of releasing the ball; perhaps you were leaning
    over too far, or your elbow positioning was incorrect. With pose detection, it
    becomes easier for coaches to work with players to correct potential problems.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, it's worth noting that pose detection is not the same as person
    recognition. I know it seems obvious, but there are people who have been confused
    by this technology into thinking that this somehow identified who a person is.
    That's a completely different form of machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: How does PoseNet work?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even with camera-based input, the process of performing pose detection does
    not change. We start with an input image (a single still of a video is good enough
    for this). The image is passed through a CNN to do the first part and identify
    where the people are in the scene. The next step takes the output from the CNN,
    passes it through a pose decoding algorithm (we'll come back to this in a moment),
    and uses this to decode poses.
  prefs: []
  type: TYPE_NORMAL
- en: The reason we said *pose decoding algorithm* was to gloss over the fact that
    we actually have two decoding algorithms. We can detect single poses, or if there
    are multiple people, we can detect multiple poses.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have opted to go with the single pose algorithm because it is the simpler
    and faster algorithm. If there are multiple people in the picture, there is potential
    for the algorithm to merge key points from different people together; therefore,
    things such as occlusion could mean that the algorithm detects person 2''s right
    shoulder as person 1''s left elbow. In the following image, we can see how the
    elbow of the girl on the right obscures the left elbow of the person in the middle:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/2d6da8aa-084f-4de7-aa17-509e2f214ad3.png)'
  prefs: []
  type: TYPE_IMG
- en: Occlusion is when one part of an image hides another part.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key points that are detected by `PoseNet` are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Nose
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Left eye
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Right eye
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Left ear
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Right ear
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Left shoulder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Right shoulder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Left elbow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Right elbow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Left wrist
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Right wrist
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Left hip
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Right hip
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Left knee
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Right knee
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Left ankle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Right ankle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can see where these are placed in our application. When it has finished
    detecting the points, we get an overlay of images, such as this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/8bbfe2de-3b60-43e1-98b1-2075ccf2e1ba.png)'
  prefs: []
  type: TYPE_IMG
- en: Back to our pose detection code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Getting back to our `PoseClassifier` class, our constructor deals with exactly
    the same WebGLTexture issue that we discussed for our `ImageClassifier` implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now going to write an asynchronous `Pose` method that returns either
    an array of `Keypoint` items, or `null` if the `PoseNet` model fails to load or
    find any poses. As well as accepting an image, this method will also accept the
    canvas that provides the context that we are going to draw our points on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'In the same way that `ImageClassifier` retrieved the `MobileNet` model as a
    cached operation, we are going to retrieve the `PoseNet` model and cache it. We
    will take this opportunity to instantiate the `DrawPose` instance as well. The
    point behind performing logic such as this is to ensure that this is something
    that we only do once, no matter how many times we call this method. Once the model
    is not null, the code prevents us from attempting to load `PoseNet` again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'When we load the model, we can supply the following option:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Multiplier**: This is the float multiplier for the number of channels (the
    depth) for all convolution operations. Choose from 1.01, 1.0, 0.75, or 0.50\.
    There is a trade-off of speed and accuracy here, with the larger values being
    more accurate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, if the model loads successfully, we are going to call `estimateSinglePose`
    with our image to retrieve the `Pose` prediction, which also contains the `keypoints`
    that we will draw:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, putting this all together to show how we don''t have to write huge amounts
    of code to accomplish all this work, and how separating the code out into small,
    self-contained logical chunks, makes our code so much simpler to understand, as
    well as easier to write. This is the full `PoseClassifier` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Completing our pose detection component
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Getting back to our `Pose.vue` component, we now have to fill in the `script`
    section. We are going to need the following `import` statements and class definition
    for our component (remember that I promised we would build this class up from
    scratch). Again, we can see the use of `@Component` to give us a component registration.
    We see this time and time again with Vue components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'We have reached the point where we can write our `Classify` method, which will
    retrieve the image and canvas when they have been created and pass this through
    to the `PoseClassifier` class. We need a couple of private fields to hold the
    `PoseClassifier` instance and the returned `Keypoint` array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Inside our `Classify` code, we are going to employ the same life cycle trick
    of waiting for `nextTick` before we retrieve the image referenced as `poseId`,
    and the canvas referenced as `posecanvas`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have the image reference, we cast them to the appropriate `HTMLImageElement`
    and `HTMLCanvasElement` types, before we call the `Pose` method and populate our
    `keypoints` member with the resulting values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we can run the application. It''s very satisfying seeing the
    `keypoints` results being overlaid onto the image, but we can go further. With
    just a little bit of extra effort, we can display the `keypoints` results in a
    Bootstrap table. Go back to our template and add the following `div` statements
    to add a Bootstrap row and column below the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we have already exposed the `keypoints` results, we can simply create
    a Vue Bootstrap table using `b-table`. We set the binding to the items using `:items`,
    setting it to the `keypoints` results that we defined in our class. This means
    that, whenever the `keypoints` entry gets new values, the table will be updated
    to display these values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Refreshing our application adds the table below the image, with the table looking
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/0e3e8bd3-753d-4108-bd61-fa17d61ffe87.png)'
  prefs: []
  type: TYPE_IMG
- en: While this is a reasonable start, it would be good if we could take a bit more
    control of the table. Right now, the fields are picked up and automatically formatted
    by `b-table`. With a small change, we can separate the `Position` instance out
    into two separate entries and make the `Score` and `Part` fields sortable.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our `Pose` class, we will create a `fields` entry. The `fields` entry maps
    the score entry to use the `Confidence` label and sets it to be `sortable`. The
    `part` field maps to a `label` value of `Part` and is also set to be `sortable`.
    We break `position` into two separate mapped entries labeled `X` and `Y`, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'The last thing we need to do is hook the `fields` entry into `b-table`. We
    do this using the `:fields` property, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Refreshing our application shows us the effect of such little changes. This
    is a much more attractive screen, and the fact that the user can sort the `Confidence`
    (originally called `score`) and `Part` fields with such little effort shows just
    how powerful Vue really is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/afc205c4-af29-4c59-b390-b0afb12d4398.png)'
  prefs: []
  type: TYPE_IMG
- en: That's it—we have reached the end of introducing TensorFlow and Vue. We have
    steered clear of the mathematical aspects behind CNNs because although they can
    appear intimidating at first glance, they aren't really as bad as all of that,
    but there are a lot of parts to a typical CNN. There is also a lot more that we
    can do with Vue; for such a small library, it is incredibly powerful, and this
    combination of small size and power is one of the reasons that it is becoming
    ever more popular.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have taken our first steps into writing machine learning
    applications using the popular `TensorFlow.js` library. As well as learning about
    what machine learning is, we have also seen how it fits into the AI space. While
    we wrote our classes to hook up to `MobileNet` and pose detection libraries, we
    also covered what CNNs are.
  prefs: []
  type: TYPE_NORMAL
- en: As well as looking at `TensorFlow.js`, we have started a journey into using
    Vue.js, the up-and-coming client side library that is rapidly gaining popularity
    alongside Angular and React. We saw how to use `.vue` files and how to hook our
    TypeScript alongside the web templates, including using Vue's binding syntax.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to take a radical step sideways and see how
    we can incorporate TypeScript alongside ASP.NET Core to build a music library
    combining C# with TypeScript.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What languages was TensorFlow originally released in?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is supervised machine learning?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is `MobileNet`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How many classifications are returned to us by default?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What command do we use to create our Vue application?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we denote a component in Vue?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Packt has an extensive number of TensorFlow books and videos, should you want
    to improve your knowledge of TensorFlow. These books aren''t just limited to `TensorFlow.js`,
    so there is an incredible depth of topics that go back to the original implementation
    of TensorFlow. Here are some of the ones I recommend:'
  prefs: []
  type: TYPE_NORMAL
- en: '*TensorFlow Reinforcement Learning Quick Start Guide* ([https://www.packtpub.com/in/big-data-and-business-intelligence/tensorflow-reinforcement-learning-quick-start-guide](https://www.packtpub.com/in/big-data-and-business-intelligence/tensorflow-reinforcement-learning-quick-start-guide)):
    Get up and running with training and deploying intelligent and self-learning agents
    using Python by Kaushik Balakrishnan: ISBN 978-1789533583.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*TensorFlow Machine Learning Projects* ([https://www.packtpub.com/big-data-and-business-intelligence/tensorflow-machine-learning-projects](https://www.packtpub.com/big-data-and-business-intelligence/tensorflow-machine-learning-projects)):
    Build 13 real-world projects with advanced numerical computations using the Python
    ecosystem by Ankit Jain and Amita Kapoor: ISBN 978-1789132212.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hands-On Computer Vision with TensorFlow 2* ([https://www.packtpub.com/in/application-development/hands-computer-vision-tensorflow-2](https://www.packtpub.com/in/application-development/hands-computer-vision-tensorflow-2)):
    Leverage deep learning to create powerful image processing apps with TensorFlow
    2.0 and Keras by Benjamin Planche and Eliot Andres: ISBN 978-1788830645.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As well as TensorFlow, we have also looked at using Vue, so the following will
    also be helpful for furthering your knowledge:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Vue CLI 3 Quick Start Guide* ([https://www.packtpub.com/in/web-development/vue-cli-3-quick-start-guide](https://www.packtpub.com/in/web-development/vue-cli-3-quick-start-guide))
    by Ajdin Imsirovic: ISBN 978-1789950342'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
