- en: Word2vec for Prediction and Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we covered some basic NLP steps, such as tokenization,
    stoplist removal, and feature creation, by creating a **Term Frequency - Inverse
    Document Frequency** (**TF-IDF**) matrix with which we performed a supervised
    learning task of predicting the sentiment of movie reviews. In this chapter, we
    are going to extend our previous example to now include the amazing power of word
    vectors, popularized by Google researchers, Tomas Mikolov and Ilya Sutskever,
    in their paper, *Distributed Representations of Words and Phrases and their Compositionality.*
  prefs: []
  type: TYPE_NORMAL
- en: We will start with a brief overview of the motivation behind word vectors, drawing
    on our understanding of the previous NLP feature extraction techniques, and we'll
    then explain the concept behind the family of algorithms that represent the word2vec
    framework (indeed, word2vec is not just one single algorithm). Then, we will discuss
    a very popular extension of word2vec called doc2vec, whereby we are interested
    in *vectorizing* entire documents into a single fixed array of N numbers. We'll
    further research on this hugely popular field of NLP, or cognitive computing research.
    Next, we will apply a word2vec algorithm to our movie review dataset, examine
    the resulting word vectors, and create document vectors by taking the average
    of the individual word vectors in order to perform a supervised learning task.
    Finally, we will use these document vectors to run a clustering algorithm to see
    how well our movie review vectors group together.
  prefs: []
  type: TYPE_NORMAL
- en: 'The power of word vectors is an exploding area of research that companies such
    as Google and Facebook have invested in heavily, given its power of encoding the
    semantic and syntactic meaning of individual words, which we will discuss shortly.
    It''s no coincidence that Spark implemented its own version of word2vec, which
    can also be found in Google''s Tensorflow library and Facebook''s Torch. More
    recently, Facebook announced a new real-time text processing called deep text, using
    their pretrained word vectors, in which they showcased their belief in this amazing
    technology and the implications it has or is having on their business applications.
    However, in this chapter, we''ll just cover a small portion of this exciting area,
    including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Explanation of the word2vec algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generalization of the word2vec idea, resulting in doc2vec
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application of both the algorithms on the movie reviews dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Motivation of word vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Similar to the work we did in the previous chapter, traditional NLP approaches
    rely on converting individual words--which we created via tokenization--into a
    format that a computer algorithm can learn (that is, predicting the movie sentiment).
    Doing this required us to convert a single review of *N* tokens into a fixed representation
    by creating a TF-IDF matrix. In doing so, we did two important things *behind
    the scenes*:'
  prefs: []
  type: TYPE_NORMAL
- en: Individual words were assigned an integer ID (for example, a hash). For example,
    the word *friend* might be assigned to 39,584, while the word *bestie* might be
    assigned to 99,928,472\. Cognitively, we know that *friend* is very similar to
    *bestie*; however, any notion of similarity is lost by converting these tokens
    into integer IDs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By converting each token into an integer ID, we consequently lose the context
    with which the token was used. This is important because in order to understand
    the cognitive meaning of words, and thereby train a computer to learn that *friend*
    and *bestie* are similar, we need to understand how the two tokens are used (for
    example, their respective contexts).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Given this limited functionality of traditional NLP techniques with respect
    to encoding the semantic and syntactic meaning of words, Tomas Mikolov and other
    researchers explored methods that employ neural networks to better *encode* the
    meaning of words as a vector of *N* numbers (for example, vector *bestie* = [0.574,
    0.821, 0.756, ... , 0.156]). When calculated properly, we will discover that the
    vectors for *bestie* and *friend* are close in space, whereby closeness is defined
    as a cosine similarity. It turns out that these vector representations (often
    referred to as *word embeddings*) give us the ability to capture a richer understanding
    of text.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, using word embeddings also gives us the ability to learn the
    same semantics across multiple languages despite differences in the written form
    (for example, Japanese and English). For example, the Japanese word for movie
    is *eiga* (![](img/00112.jpeg)); therefore, it follows that using word vectors,
    these two words, *movie* and ![](img/00113.jpeg)*,* should be close in the vector
    space despite their differences in appearance. Thus, the word embeddings allow
    for applications to be language-agnostic--yet another reason why this technology
    is hugely popular!
  prefs: []
  type: TYPE_NORMAL
- en: Word2vec explained
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First things first: word2vec does not represent a *single* algorithm but rather
    a family of algorithms that attempt to encode the semantic and syntactic *meaning*
    of words as a vector of *N* numbers (hence, word-to-vector = word2vec). We will
    explore each of these algorithms in depth in this chapter, while also giving you
    the opportunity to read/research other areas of *vectorization* of text, which
    you may find helpful.'
  prefs: []
  type: TYPE_NORMAL
- en: What is a word vector?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In its simplest form, a word vector is merely a one-hot-encoding, whereby every
    element in the vector represents a word in our vocabulary, and the given word
    is *encoded* with `1` while all the other words elements are encoded with `0`.
    Suppose our vocabulary only has the following movie terms: **Popcorn**, **Candy**,
    **Soda**, **Tickets**, and **Blockbuster**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Following the logic we just explained, we could encode the term **Tickets** as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00114.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Using this simplistic form of encoding, which is what we do when we create a
    bag-of-words matrix, there is no meaningful comparison we can make between words
    (for example, *is Popcorn related to Soda; is Candy similar to Tickets?)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given these obvious limitations, word2vec attempts to remedy this via distributed
    representations for words. Suppose that for each word, we have a distributed vector
    of, say, 300 numbers that represent a single word, whereby each word in our vocabulary
    is also represented by a distribution of weights across those 300 elements. Now,
    our picture would drastically change to look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00115.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, given this distributed representation of individual words as 300 numeric
    values, we can make meaningful comparisons among words using a cosine similarity,
    for example. That is, using the vectors for **Tickets** and **Soda**, we can determine
    that the two terms are not related, given their vector representations and their
    cosine similarity to one another. And that''s not all we can do! In their ground-breaking
    paper, Mikolov et. al also performed mathematical functions of word vectors to
    make some incredible findings; in particular, the authors give the following *math
    problem* to their word2vec dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '*V(King) - V(Man) + V(Woman) ~ V(Queen)*'
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that these distributed vector representations of words are extremely
    powerful in comparison questions (for example, is A related to B?), which is all
    the more remarkable when you consider that this semantic and syntactic learned
    knowledge comes from observing lots of words and their context with no other information
    necessary. That is, we did not have to tell our machine that *Popcorn* is a food,
    noun, singular, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: How is this made possible? Word2vec employs the power of neural networks in
    a supervised fashion to learn the vector representation of words (which is an
    unsupervised task). If that sounds a bit like an oxymoron at first, fear not!
    Everything will be made clearer with a few examples, starting first with the **Continuous
    Bag-of-Words** model, commonly referred to as just the **CBOW** model.
  prefs: []
  type: TYPE_NORMAL
- en: The CBOW model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, let''s consider a simple movie review, which will act as our base example
    in the next few sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00116.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, imagine that we have a window that acts as a slider, which includes the
    main word currently in focus (highlighted in red in the following image), in addition
    to the five words before and after the focus word (highlighted in yellow):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00117.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The words in yellow form the context that surrounds the current focus word,
    *ideas*. These context words act as inputs to our feed-forward neural network,
    whereby each word is encoded via one-hot-encoding (all other elements are zeroed
    out) with one hidden layer and one output layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00118.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, the total size of our vocabulary (for example, post-tokenization)
    is denoted by a capital C, whereby we perform one-hot-encoding of each word within
    the context window--in this case, the five words before and after our focus word,
    *ideas*. At this point, we propagate our encoded vectors to our hidden layer via
    weighted sum--just like a *normal* feed-forward neural network--whereby, we specify
    beforehand the number of weights in our hidden layer. Finally, a sigmoid function
    is applied from the single hidden layer to the output layer, which attempts to
    predict the current focus word. This is achieved by maximizing the conditional
    probability of observing the focus word (*idea*), given the context of its surrounding
    words (**film**, **with**, **plenty**, **of**, **smart**, **regarding**, **the**,
    **impact**, **of**, and **alien**). Notice that the output layer is also of the
    same size as our initial vocabulary, C.
  prefs: []
  type: TYPE_NORMAL
- en: 'Herein lies the interesting property of both the families of the word2vec algorithm:
    it''s an unsupervised learning algorithm at heart and relies on supervised learning
    to learn individual word vectors. This is true for the CBOW model and also the
    skip-gram model, which we will cover next. Note that at the time of writing this
    book, Spark''s MLlib only incorporates the skip-gram model of word2vec.'
  prefs: []
  type: TYPE_NORMAL
- en: The skip-gram model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous model, we used a window of words before and after the focus
    word to predict the focus word. The skip-gram model takes a similar approach but
    reverses the architecture of the neural network. That is, we are going to start
    with the focus word as our input into our network and then try to predict the
    surrounding contextual words using a single hidden layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00119.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the skip-gram model is the exact opposite of the CBOW model.
    The training goal of the network is to minimize the summed prediction error across
    all the context words in the output layer, which, in our example, is an input
    of *ideas* and an output layer that predicts *film*, *with*, *plenty*, *of*, *smart*,
    *regarding*, *the*, *impact*, *of*, and *alien*.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, you saw that we used a tokenization function that removed
    stopwords, such as *the*, *with*, *to*, and so on, which we have not shown here
    intentionally to clearly convey our examples without losing the reader. In the
    example that follows, we will perform the same tokenization function as [Chapter
    4](part0080.html#2C9D00-d18ba71168a441bd917775fac13ca893), *Predicting Movie Reviews
    Using NLP and Spark Streaming*, which will remove the stopwords.
  prefs: []
  type: TYPE_NORMAL
- en: Fun with word vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have condensed words (tokens) into vectors of numbers, we can have
    some fun with them. A few classic examples from the original Google paper that
    you can try for yourself are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mathematical operations**: As mentioned earlier, the canonical example of
    this is *v(King) - v(Man) + v(Woman) ~ v(Queen)*. Using simple addition, such
    as *v(software) + v(engineer)*, we can come up with some fascinating relationships;
    here are a few more examples:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00120.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '**Similarity**: Given that we are working with a vector space, we can use the
    cosine similarity to compare one token against many in order to see similar tokens.
    For example, similar words to *v(Spark)* might be *v(MLlib)*, *v(scala)*, *v(graphex)*,
    and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Matches/Not Matches**: Which words from a given list do not go together?
    For example, *doesn''t_match[v(lunch, dinner, breakfast, Tokyo)] == v(Tokyo)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A is to B as C is to ?**: As per the Google paper, here is a list of word
    comparisons that are made possible by using the skip-gram implementation of word2vec:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00121.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Cosine similarity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Word similarity/dissimilarity is measured via cosine similarity, which has a
    very nice property of being bound between `-1` and `1`. Perfect similarity between
    two words will yield a score of `1`, no relation will yield `0`, and `-1` means
    that they are opposites.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the cosine similarity function for the word2vec algorithm (again,
    just the CBOW implementation in Spark, for now) is already baked into MLlib, which
    we will see shortly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00122.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: For those who are interested in other measures of similarity, a recent research
    has been published that makes a strong case for using **Earth-Mover's Distance**
    (**EMD**), which is a different method from cosine similarity, requiring some
    additional calculation, but shows promising early results.
  prefs: []
  type: TYPE_NORMAL
- en: Doc2vec explained
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we mentioned in the chapter's introduction, there is an extension of word2vec
    that encodes entire *documents* as opposed to individual words. In this case,
    a document is what you make of it, be it a sentence, a paragraph, an article,
    an essay, and so on. Not surprisingly, this paper came out after the original
    word2vec paper but was also, not surprisingly, coauthored by Tomas Mikolov and
    Quoc Le. Even though MLlib has yet to introduce doc2vec into their stable of algorithms,
    we feel it is necessary for a data science practitioner to know about this extension
    of word2vec, given its promise of and results with supervised learning and information
    retrieval tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Like word2vec, doc2vec (sometimes referred to as *paragraph vectors*) relies
    on a supervised learning task to learn distributed representations of documents
    based on contextual words. Doc2vec is also a family of algorithms, whereby the
    architecture will look extremely similar to the CBOW and skip-gram models of word2vec
    that you learned in the previous sections. As you will see next, implementing
    doc2vec will require a parallel training of both individual word vectors and document
    vectors that represent what we deem as a *document*.
  prefs: []
  type: TYPE_NORMAL
- en: The distributed-memory model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This particular flavor of doc2vec closely resembles the CBOW model of word2vec,
    whereby the algorithm tries to predict a *focus word* given its surrounding *context
    words* but with the addition of a paragraph ID. Think of this as another individual
    contextual word vector that helps with the prediction task but is constant throughout
    what we consider to be a document. Continuing our previous example, if we have
    this movie review (we define one document as one movie review) and our focus word
    is *ideas*, we will now have the following architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00123.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that as we move down the document and change the *focus word* from *ideas* to
    *regarding*, our context words will obviously change; however, **Document ID:
    456** remains the same. This is a crucial point in doc2vec, as the document ID
    is used in the prediction task:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00124.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The distributed bag-of-words model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The last algorithm in doc2vec is modeled after the word2vec skip-gram model,
    with one exception--instead of using the *focus* word as the input, we will now
    take the document ID as the input and try to predict *randomly sampled* words
    from the document. That is, we will completely ignore the context words in our
    output altogether:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00125.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Like word2vec, we can take similarities of documents of N words using these
    *paragraph vectors*, which have proven hugely successful in both supervised and
    unsupervised tasks. Here are some of the experiments that Mikolov et. al ran using,
    notably, the supervised task that leverages the same dataset we used in the last
    two chapters!
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00126.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Information-retrieval tasks (three paragraphs, the first should *sound* closer
    to the second than the third paragraph):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00127.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the subsequent sections, we are going to create a *poor man's document vector* by
    taking the average of individual word vectors to form our document vector, which
    will encode entire movie reviews of n-length into vectors of dimension, 300.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing this book, Spark's MLlib does not have an implementation
    of doc2vec; however, there are many projects that are leveraging this technology,
    which are in the incubation phase and which you can test out.
  prefs: []
  type: TYPE_NORMAL
- en: Applying word2vec and exploring our data with vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that you have a good understanding of word2vec, doc2vec, and the incredible
    power of word vectors, it''s time we turned our focus to our original IMDB dataset,
    whereby we will perform the following preprocessing:'
  prefs: []
  type: TYPE_NORMAL
- en: Split words in each movie review by a space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remove punctuation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remove stopwords and all alphanumeric words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using our tokenization function from the previous chapter, we will end with
    an array of comma-separated words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because we have already covered the preceding steps in [Chapter 4](part0080.html#2C9D00-d18ba71168a441bd917775fac13ca893),
    *Predicting Movie Reviews Using NLP and Spark Streaming*, we'll quickly reproduce
    them in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, we begin with starting the Spark shell, which is our working environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In the prepared environment, we can directly load the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also define the tokenization function to split the reviews into tokens,
    removing all the common words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'With all the building blocks ready, we just apply them to the loaded input
    data, augmenting them by a new column, `reviewTokens`, which holds a list of words
    extracted from the review:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The `reviewTokens` column is a perfect input for the word2vec model. We can
    build it using the Spark ML library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The Spark implementation has several additional hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`setMinCount`: This is the minimum frequency with which we can create a word.
    It is another processing step so that the model is not running on super rare terms
    with low counts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`setNumIterations`: Typically, we see that a higher number of iterations leads
    to more *accurate* word vectors (think of these as the number of epochs in a traditional
    feed-forward neural network). The default value is set to `1`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`setVectorSize`: This is where we declare the size of our vectors. It can be
    any integer with a default size of `100`. Many of the *public* word vectors that
    come pretrained tend to favor larger vector sizes; however, this is purely application-dependent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`setLearningRate`: Just like a *regular* neural network, which we learned about
    in [Chapter 2](part0038.html#147LC0-d18ba71168a441bd917775fac13ca893), *Detecting
    Dark Matter- The Higgs-Boson Particle,* discretion is needed in part by the data
    scientist--too little a learning rate and the model will take forever-and-a-day
    to converge. However, if the learning rate is too large, one risks a non-optimal
    set of learned weights in the network. The default value is `0`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now that our model has finished, it''s time to inspect some of our word vectors!
    Recall that whenever you are unsure of what values your model can produce, always
    hit the *tab* button, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00128.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Let's take a step back and consider what we just did here. First, we condensed
    the word, *funny*, to a vector composed of an array of 100 floating point numbers
    (recall that this is the default value for the Spark implementation of the word2vec
    algorithm). Because we have reduced all the words in our corpus of reviews to
    the same distributed representation of 100 numbers, we can make comparisons using
    the cosine similarity, which is what the second number in our result set reflects
    (the highest cosine similarity in this case is for the word *nutty*)*.*
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we can also access the vector for *funny *or any other word in our
    dictionary using the `getVectors` function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00129.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: A lot of interesting research has been done on clustering similar words together
    based on these representations as well. We will revisit clustering later in this
    chapter when we'll try to cluster similar movie reviews after we perform a hacked
    version of doc2vec in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Creating document vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So, now that we can create vectors that encode the *meaning* of words, and
    we know that any given movie review post tokenization is an array of *N* words,
    we can begin creating a poor man''s doc2vec by taking the average of all the words
    that make up the review. That is, for each review, by averaging the individual
    word vectors, we lose the specific sequencing of words, which, depending on the
    sensitivity of your application, can make a difference:'
  prefs: []
  type: TYPE_NORMAL
- en: '*v(word_1) + v(word_2) + v(word_3) ... v(word_Z) / count(words in review)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ideally, one would use a flavor of doc2vec to create document vectors; however,
    doc2vec has yet to be implemented in MLlib at the time of writing this book, so
    for now, we are going to use this simple version, which, as you will see, has
    surprising results. Fortunately, the Spark ML implementation of the word2vec model
    already averages word vectors if the model contains a list of tokens. For example,
    we can show that the phrase, *funny movie*, has a vector that is equal to the
    average of the vectors of the `funny` and `movie` tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00130.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Hence, we can prepare our simple version of doc2vec by a simple model transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As practitioners in this field, we have had the unique opportunity to work with
    various flavors of document vectors, including word averaging, doc2vec, LSTM auto-encoders,
    and skip-thought vectors. What we have found is that for small word snippets,
    where the sequencing of words isn't crucial, the simple word averaging does a
    surprisingly good job as supervised learning tasks. That is, not to say that it
    could be improved with doc2vec and other variants but is rather an observation
    based on the many use cases we have seen across various customer applications.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning task
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Like in the previous chapter, we need to prepare the training and validation
    data. In this case, we''ll reuse the Spark API to split the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s perform a grid search using a simple decision tree and a few hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now inspect the result and show the best model AUC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00131.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Using this simple grid search on a decision tree, we can see that our *poor
    man''s doc2vec* produces an AUC of 0.7054\. Let''s also expose our exact training
    and test data to H2O and try a deep learning algorithm using the Flow UI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have successfully published our dataset as H2O frames, let''s open
    the Flow UI and run a deep learning algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'First, note that if we run the `getFrames` command, we will see the two RDDs
    that we seamlessly passed from Spark to H2O:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00132.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We need to change the type of column label from a numeric column to a categorical
    one by clicking on Convert to enum for both the frames:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00133.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we will run a deep learning model with all of the hyperparameters set
    to their default value and the first column set to be our label:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00134.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'If you did not explicitly create a train/test dataset, you can also perform
    an *n folds cross-validation* using the *nfolds* hyperparameter previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00135.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'After running the model training, we can view the model output by clicking
    View to see the AUC on both the training and validation datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00136.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We see a higher AUC for our simple deep learning model of ~ 0.8289\. This is
    a result without any tuning or hyperparameter searching.
  prefs: []
  type: TYPE_NORMAL
- en: 'What are some other steps that we can perform to improve the AUC even more?
    We could certainly try a new algorithm with grid searching for hyperparameters,
    but more interestingly, can we tune the document vectors? The answer is yes and
    no! It''s a partial *no* because, as you will recall, word2vec is an unsupervised
    learning task at heart; however, we can get an idea of the strength of our vectors
    by observing some of the similar words returned. For example, let''s take the
    word `drama`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00137.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Intuitively, we can look at the results and ask whether these five words are
    *really the best* synonyms (that is, the best cosine similarities) of the the
    word *drama*. Let''s now try rerunning our word2vec model by modifying its input
    parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00138.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: You should immediately notice that the synonyms are *better* in terms of similarity
    to the word in question, but also note that the cosine similarities are significantly
    higher for the terms as well. Recall that the default number of iterations for
    word2vec is 1 and now we have set it to `250`, allowing our network to really
    triangulate on some quality word vectors, which can further be improved with more
    preprocessing steps and further tweaking of our hyperparameters for word2vec,
    which should produce document vectors of better quality.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Many companies such as Google freely give pretrained word vectors (trained
    on a subset of Google News, incorporating the top three million words/phrases)
    for various vector dimensions: for example, 25d, 50d, 100d, 300d, and so on. You
    can find the code (and the resulting word vectors) here. In addition to Google
    News, there are other sources of trained word vectors, which use Wikipedia and
    various languages. One question you might have is that if companies such as Google
    freely provide pretrained word vectors, why bother building your own? The answer
    to the question is, of course, application-dependent; Google''s pretrained dictionary
    has three different vectors for the word *java* based on capitalization (JAVA,
    Java, and java mean different things), but perhaps, your application is just about
    coffee, so only one *version* of java is all that is really needed.'
  prefs: []
  type: TYPE_NORMAL
- en: Our goal for this chapter was to give you a clear and concise explanation of
    the word2vec algorithms and very popular extensions of this algorithm, such as
    doc2vec and sequence-to-sequence learning models, which employ various flavors
    of recurrent neural networks. As always, one chapter is hardly enough time to
    cover this extremely exciting field in natural language processing, but hopefully,
    this is enough to whet your appetite for now!
  prefs: []
  type: TYPE_NORMAL
- en: As practitioners and researchers in this field, we (the authors) are constantly
    thinking of new ways of representing documents as fixed vectors, and there are
    a plenty of papers dedicated to this problem. You can consider *LDA2vec* and *Skip-thought
    Vectors* for further reading on the subject.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some other blogs to add to your reading list regarding **Natural Language Processing**
    (**NLP**) and *Vectorizing* are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Google's research blog ([https://research.googleblog.com/](https://research.googleblog.com/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The NLP blog (always well thought out posts with a lot of links for further
    reading,) ([http://nlpers.blogspot.com/](http://nlpers.blogspot.com/)))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Stanford NLP blog ([http://nlp.stanford.edu/blog/](http://nlp.stanford.edu/blog/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next chapter, we will see word vectors again, where we'll combine all
    of what you have learned so far to tackle a problem that requires *the kitchen
    sink* with respect to the various processing tasks and model inputs. Stick around!
  prefs: []
  type: TYPE_NORMAL
