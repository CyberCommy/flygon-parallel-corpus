- en: Chapter 5. Spark for Geographic Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Geographic processing is a powerful use case for Spark and therefore the aim
    of this chapter is to explain how data scientists can process geographic data
    using Spark to produce powerful, map-based views of very large datasets. We will
    demonstrate how to process spatio-temporal datasets easily via Spark integrations
    with GeoMesa, which helps turn Spark into a sophisticated geographic processing
    engine. As the **Internet of Things** (**IoT**) and other location-aware datasets
    become ever more common, and *moving objects* data volumes climb, Spark will become
    a critical tool that closes the geoprocessing gap that exists between spatial
    functionality and processing scalability. This chapter reveals how to conduct
    advanced geopolitical analysis of global news with a view to leveraging the data
    to analyze and perform data science on oil prices.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Using Spark to ingest and preprocess geolocated data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing geodata which is appropriately indexed, using Geohash indexing inside
    GeoMesa
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running complex spatio-temporal queries, filtering data across time and space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Spark and GeoMesa together to perform advanced geographic processing in
    order to study change over time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Spark to calculate density maps and to visualize changes in these maps
    over time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Querying and integrating spatial data across map layers to build new insights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GDELT and oil
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The premise of this chapter is that we can manipulate GDELT data to determine,
    to a greater or lesser extent, the price of oil based on historic events. The
    accuracy of our predictor will depend on many variables including the detail of
    our events, the number used and our hypotheses surrounding the nature of the relationship
    between oil and these events.
  prefs: []
  type: TYPE_NORMAL
- en: The oil industry is very complex and is driven by many factors. It has been
    found however, that most major oil price fluctuations are largely explained by
    shifts in the demand of crude oil. The price also increases during times of greater
    demand for stock, and historically has been high in times of geopolitical tension
    in the Middle East. In particular, political events have a strong influence on
    the oil price and it is this aspect that we will concentrate on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Crude oil is produced by many countries around the world; there are however,
    three main benchmarks that are used by producers for pricing:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Brent: Produced by various entities in the North Sea'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'WTI: **West Texas Intermediate** (**WTI**) covering entities in the mid-west
    and Gulf Coast regions of North America'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OPEC: Produced by members of OPEC:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Algeria, Angola, Ecuador, Gabon, Indonesia, Iran, Iraq, Kuwait, Libya, Nigeria,
    Qatar, Saudi Arabia, UAE, and Venezuela
  prefs: []
  type: TYPE_NORMAL
- en: 'It becomes clear that the first thing we need to do is to obtain the historical
    pricing data for the three baselines. By searching the Internet, downloadable
    data can be found in many places, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: Brent: [https://fred.stlouisfed.org/](https://fred.stlouisfed.org/series/DCOILBRENTEU)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: WTI: [https://fred.stlouisfed.org/](https://fred.stlouisfed.org/series/DCOILBRENTEU)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OPEC: [http://opec.org](http://opec.org)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now we know that oil prices are primarily determined by supply and demand, our
    first hypothesis will be that the supply and demand is affected, to a greater
    extent, by world events and thus we can predict what that supply and demand is
    likely to be.
  prefs: []
  type: TYPE_NORMAL
- en: We want to try and determine whether the oil price will rise or fall during
    the next day, week, or month and, as we have used GDELT throughout the book, we
    will take that knowledge and expand it to run some very large processing jobs.
    Before we start, it's worth discussing the path we are going to take, and the
    reasons for the decisions made. The first area of concern is how GDELT relates
    to oil; this will define the scope of the initial work, and provide a base upon
    which we can build later. It is important here that we decide how to leverage
    GDELT and what the consequences of that decision will be; for example, we could
    decide to use all of the data for all of the time, but the processing time required
    for that is very large indeed since just one day of GDELT events data can average
    15 MB, and 1.5 GB for GKG. Therefore, we should analyze the contents of the two
    sets and try to establish what our initial data input will be.
  prefs: []
  type: TYPE_NORMAL
- en: GDELT events
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Looking through the GDELT schema, there are a number of points that could be
    useful; the events schema primarily revolves around identifying the two primary
    actors in a story and relating an event to them. There is also the ability to
    look at events at different levels, so we will have good flexibility to work at
    higher or lower levels of complexity, depending upon how our results work out.
    For example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `EventCode` field is a CAMEO action code: 0251 (Appeal for easing of administrative
    sanctions) and can also be used at the levels 02 (Appeal) and 025 (Appeal to yield).'
  prefs: []
  type: TYPE_NORMAL
- en: Our second hypothesis is therefore, that the level of detail of the event will
    provide better or worse accuracy from our algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Other interesting labels are `GoldsteinScale`, `NumMentions` and `Lat`/`Lon`.
    The `GoldsteinScale` label is a number from -10 to +10 and it attempts to capture
    the theoretical potential impact that type of event can have on the stability
    of a country; a great match based on what we have already established about the
    stability of oil prices. The `NumMentions` label gives us an indication of how
    often the event has appeared across all source documents; this could help us to
    assign an importance to events if we find that we need to reduce the number of
    assessed events in our processing. For example, we could process the data and
    find the top 10, 100, or 1000 events in the last hour, day, or week based upon
    how often they have been mentioned. Finally, the `lat`/`lon` label information
    attempts to assign a geographical point of reference for the event, making this
    very useful for when we want to produce maps in GeoMesa.
  prefs: []
  type: TYPE_NORMAL
- en: GDELT GKG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The GKG schema is related to summarizing the content of the events and providing
    enhanced information specific to that content. Areas of interest for our purposes
    include `Counts`, `Themes`, `GCAM`, and `Locations`; the `Counts` field maps any
    numeric mention, thus potentially allowing us to calculate a severity, for example
    KILLS=47\. The `Themes` field lists all of the themes based on the GDELT category
    list; this could help us to machine learn particular areas, over time, which affect
    oil prices. The `GCAM` field is the result of content analysis of the event; a
    quick perusal of the GCAM list shows us that there are some possibly useful dimensions
    to look out for:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: And finally, we have the `Locations` field, which provides similar information
    to the Events, and thus also can be used for visualization of maps.
  prefs: []
  type: TYPE_NORMAL
- en: Formulating a plan of action
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Having inspected the GDELT schemas, we now need to make some decisions around
    what data we are going to use, and make sure we justify that usage based on our
    hypotheses. This is a critical stage as there are many areas to consider, and
    at the very least we need to:'
  prefs: []
  type: TYPE_NORMAL
- en: Ensure that our hypotheses are clear so that we have a known starting point
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure that we are clear about how we are going to implement the hypotheses,
    and determine an action plan
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure that we use enough appropriate data to meet our action plan; scope the
    data usage to ensure we can produce a conclusion within a given time frame, for
    example, using all GDELT data would be great, but is probably not reasonable unless
    a large processing cluster is available. On the other hand using one day is clearly
    not enough to gauge any patterns over time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Formulate a plan B in case our initial results are not conclusive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our second hypothesis is about the detail of the events; for the purposes of
    clarity, in this chapter, we are going to choose just one of the data sources
    initially, with a view to adding further complexity if our model does not perform
    well. Therefore, we can choose the GDELT events as the fields mentioned above
    provide for an excellent base upon which to prove our algorithms; in particular,
    the `gcam` field will be very useful to determine the nature of an event and the
    `NumMentions` field will be quick to implement when considering the importance
    of an event. While the GKG data also looks useful, we want to try and use general
    events at this stage; so, the GCAM oil data, for example, is considered too specific
    as there is a good chance that articles related to these fields will often be
    about the reaction to oil price change, and therefore too late to consider for
    our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our initial processing flow (action plan) will involve the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Obtain oil price data for the last 5 years
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Obtain GDELT events for the last 5 years
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Install GeoMesa and related tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load the GDELT data to GeoMesa
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build a visualization to show some of the events on a world map
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use an appropriate machine learning algorithm to learn event types against oil
    price rise/fall
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the model to predict the rise or fall in price of oil
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GeoMesa
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GeoMesa is an open source product designed to leverage the distributed nature
    of storage systems, such as Accumulo and Cassandra, to hold a distributed spatio-temporal
    database. With this design, GeoMesa is capable of running the large-scale geospatial
    analytics that are required for very large data sets, including GDELT.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to use GeoMesa to store GDELT data and run our analytics across
    a large proportion of that data; this should give us access to enough data to
    train our model so that we can predict the future rise and fall of oil prices.
    Also, GeoMesa will enable us to plot large amounts of points on a map, so that
    we can visualize GDELT and any other useful data.
  prefs: []
  type: TYPE_NORMAL
- en: Installing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There is a very good tutorial on the GeoMesa website ([www.geomesa.org](http://www.geomesa.org))
    that guides the user through the installation process. Therefore, it is not our
    intention here to produce another how-to guide; there are, however, a few points
    worth noting that may save you time in getting everything up and running:'
  prefs: []
  type: TYPE_NORMAL
- en: GeoMesa has a lot of components, and many of these have a lot of versions. It
    is very important to ensure that all of the versions of the software stack match
    exactly with the versions specified in the GeoMesa maven POMs. Of particular interest
    are Hadoop, Zookeeper, and Accumulo; the version locations can be found in the
    root `pom.xml` file in the GeoMesa tutorial and other related downloads.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the time of writing, there are some additional issues when integrating GeoMesa
    with some of the Hadoop vendor stacks. If you are able, use GeoMesa with your
    own stack of Hadoop/Accumulo and so on, to ensure version compatibility.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The GeoMesa version dependency labeling has changed from version 1.3.0\. It
    is very important that you ensure all of the versions line up with your chosen
    version of GeoMesa; if there are any conflicting classes then there will definitely
    be problems at some point down the line.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you have not used Accumulo before, we have discussed it in detail in other
    chapters within this book. An initial familiarization will help greatly when using
    GeoMesa (see [Chapter 7](ch07.xhtml "Chapter 7. Building Communities"), *Building
    Communities*).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When using Accumulo 1.6 or greater with GeoMesa, there is the option to use
    Accumulo namespaces. If you are unfamiliar with this, then opt to not use namespaces
    and simply copy the GeoMesa runtime JAR into `/lib/text` in your Accumulo root
    folder.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GeoMesa uses a few shell scripts; due to the nature of operating systems there
    may be the odd problem with running these scripts, depending upon your platform.
    The issues are minor and can be fixed with some quick Internet searches; for example
    when running `jai-image.sh` there was a minor issue with user confirmation on
    an Mac OSX.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The GeoMesa maven repository can be found at [https://repo.locationtech.org/content/repositories/releases/org/locationtech/geomesa/](https://repo.locationtech.org/content/repositories/releases/org/locationtech/geomesa/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once you are able to successfully run GeoMesa from the command line, we can
    move on to the next section.
  prefs: []
  type: TYPE_NORMAL
- en: GDELT Ingest
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The next stage is to obtain the GDELT data and load it into GeoMesa. There
    are a number of options here, depending upon how you plan to proceed; if you are
    just working through this chapter, then you can use a script to download the data
    in one go:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This will download and verify all of the GDELT events data for 2015 and 2016\.
    The amount of data required is something we need to estimate at this stage, as
    we do not know how our algorithm is going to work out, so we have chosen two years
    worth to start with.
  prefs: []
  type: TYPE_NORMAL
- en: 'An alternative to the script is to read [Chapter 2](ch02.xhtml "Chapter 2. Data
    Acquisition"), *Data Acquisitio*n, which explains in detail how to configure Apache
    NiFi to download the GDELT data in real time, and further it loads it to HDFS
    ready for use. Otherwise, a script to allow the preceding data to be transferred
    to HDFS is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: HDFS uses data blocks; we want to ensure that files are stored as efficiently
    as possible. Writing a method to aggregate files to the HDFS block size (64 MB
    by default) will ensure the NameNode memory is not filled with many entries for
    lots of small files, and will make processing more efficient also. Large files
    that use more than one block (file size > 64 MB) are known as split files.
  prefs: []
  type: TYPE_NORMAL
- en: We have a substantial amount of data in HDFS (approximately 48 GB for 2015/16).
    Now, we will load this to Accumulo via GeoMesa.
  prefs: []
  type: TYPE_NORMAL
- en: GeoMesa Ingest
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The GeoMesa tutorials discuss the idea of loading the data from HDFS to Accumulo
    using a `MapReduce` job. Let's take a look at this and create a Spark equivalent.
  prefs: []
  type: TYPE_NORMAL
- en: MapReduce to Spark
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since **MapReduce** (**MR**) is generally considered dead, or at least dying,
    it is very useful to know how to create Spark jobs from those existing in MR.
    The following method can be applied to any MR job. We will consider the GeoMesa
    Accumulo loading job described in the GeoMesa tutorial (`geomesa-examples-gdelt`)
    for this case.
  prefs: []
  type: TYPE_NORMAL
- en: 'An MR job is typically made up of three parts: the mapper, the reducer, and
    the driver. The GeoMesa example is a map-only job and therefore requires no reducer.
    The job takes a GDELT input line, creates a (Key,Value) pair from an empty `Text`
    object and the created GeoMesa `SimpleFeature`, and uses the `GeoMesaOutputFormat`
    to load the data to Accumulo. The full code of the MR job can be found in our
    repository; next this we will work through the key parts and suggest the changes
    required for Spark.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The job is initiated from the `main` method; the first few lines are related
    to parsing the required options from the command line, such as the Accumulo username
    and password. We then reach:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The GeoMesa `SimpleFeatureType` is the primary mechanism used to store data
    in a GeoMesa data store and it needs to be initialized once, along with the data
    store initialization. Once this is done we execute the MR job itself. In Spark,
    we can pass the arguments via the command line as before, and then do the one-off
    setup:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The contents of the jar contain a standard Spark job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Parse the command line arguments as before, as well as performing the initialization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can load the data from HDFS, using wildcards if required. This creates
    one partition for each block of the file (64 MB default), resulting in an `RDD[String]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Or we can fix the number of partitions, depending upon our available resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we can perform the map, where we can embed the function to replace the
    process in the original MR `map` method. We create a tuple (Text,SimpleFeatureType)
    to replicate a (Key, Value) pair so that we can use the `OutputFormat` in the
    next step. When Scala Tuples are created in this way, the resulting RDD gains
    extra methods, such as `ReduceByKey`, which is functionally equivalent to the
    MR Reducer (see below for further information on what we should really be using,
    `mapPartitions`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can finally output to Accumulo using the `GeomesaOutputFormat` from
    the original job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: At this stage, we have not mentioned the `setup` method in the MR job; this
    method is called before any input is processed to allocate an expensive resource
    like a database connection, or in our case, a reusable object, and a `cleanup`
    method is then used to release that resource if it were to persist when out of
    scope. In our case, the `setup` method is used to create a `SimpleFeatureBuilder`
    which can be reused during each call of the mapper to build `SimpleFeatures` for
    output; there is no `cleanup` method as the memory is automatically released when
    the object is out of scope (the code has completed).
  prefs: []
  type: TYPE_NORMAL
- en: 'The Spark `map` function only operates on one input at a time, and provides
    no means to execute code before or after transforming a batch of values. It looks
    reasonable to simply put the setup and cleanup code before and after a call to
    `map`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'But, this fails for several reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: It puts any objects used in `map` into the map function's closure, which requires
    that it be serializable (for example, by implementing `java.io.Serializable`).
    Not all objects will be serializable, thus exceptions may be thrown.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `map` function is a transformation, rather than an operation, and is lazily
    evaluated. Thus, instructions after the `map` function are not guaranteed to be
    executed immediately.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even if the preceding issues were covered for a particular implementation, we
    would only be executing code on the driver, not necessarily freeing resources
    allocated by serialized copies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The closest counterpart to a mapper in Spark is the `mapPartitions` method.
    This method does not map just one value to another value, but maps an Iterator
    of values to an Iterator of other values, akin to a bulk-map method. This means
    that the `mapPartitions` can allocate resources locally at its start:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'However, releasing resources (`cleanup`) is not straightforward as we still
    experience the lazy evaluation problem; if resources are freed after the `map`,
    then the iterator may not have evaluated before the disappearance of those resources.
    One solution to this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the Spark code for ingest, there is an additional change that
    we could make, which is to add a `Geohash` field (see the following for more information
    on how to produce this field). To insert this field into the code, we will need
    an additional entry at the end of the GDELT attributes list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'And a line to set the value of the `simpleFeature` type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can run our Spark job to load the GeoMesa Accumulo instance with
    the GDELT data from HDFS. The two years of GDELT is around 100 million entries!
    You can check how much data is in Accumulo by using the Accumulo shell, run from
    the `accumulo/bin` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Geohash
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Geohash is a geocoding system invented by Gustavo Niemeyer. It is a hierarchical,
    spatial data structure that subdivides space into buckets of grid shape, which
    is one of the many applications of what is known as a Z-order curve and generally
    space-filling curves.
  prefs: []
  type: TYPE_NORMAL
- en: Geohashes offer properties like arbitrary precision and the possibility of gradually
    removing characters from the end of the code to reduce its size (and gradually
    lose precision).
  prefs: []
  type: TYPE_NORMAL
- en: As a consequence of the gradual precision degradation, nearby geographical locations
    will often (but not always) present similar prefixes. The longer a shared prefix
    is, the closer the two locations are; this is very useful in GeoMesa should we
    want to use points from a particular area, as we can use the `Geohash` field added
    in the preceding ingest code .
  prefs: []
  type: TYPE_NORMAL
- en: 'The main usages of Geohashes are:'
  prefs: []
  type: TYPE_NORMAL
- en: As a unique identifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To represent point data, for example, in databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When used in a database, the structure of geo-hashed data has two advantages.
    First, data indexed by Geohash will have all points for a given rectangular area
    in contiguous slices (the number of slices depends on the precision required and
    the presence of Geohash *fault lines*). This is especially useful in database
    systems where queries on a single index are much easier or faster than multiple-index
    queries: Accumulo, for example. Second, this index structure can be used for a
    quick-and-dirty proximity search: the closest points are often among the closest
    Geohashes. These advantages make Geohashes ideal for use in GeoMesa. The following
    is an extract of code from David Allsopp''s excellent Geohash scala implementation
    [https://github.com/davidallsopp/geohash-scala](https://github.com/davidallsopp/geohash-scala).
    This code can be used to produce Geohashes based on a `lat`/`lon` input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: One limitation of the Geohash algorithm is in attempting to utilize it to find
    points in proximity to each other based on a common prefix. Edge case locations
    that are close to each other, but on opposite sides of the 180 degrees meridian,
    will result in Geohash codes with no common prefix (different longitudes for near
    physical locations). Points that are close by at the North and South poles will
    have very different Geohashes (different longitudes for near physical locations).
  prefs: []
  type: TYPE_NORMAL
- en: Also, two close locations on either side of the equator (or Greenwich meridian)
    will not have a long common prefix since they belong to different halves of the
    world; one location's binary latitude (or longitude) will be 011111... and the
    other 100000... so they will not have a common prefix and most bits will be flipped.
  prefs: []
  type: TYPE_NORMAL
- en: In order to do a proximity search, we could compute the southwest corner (low
    Geohash with low latitude and longitude) and northeast corner (high Geohash with
    high latitude and longitude) of a bounding box and search for Geohashes between
    those two. This will retrieve all points in the Z-order curve between the two
    corners; this also breaks down at the 180 meridians and the poles.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, since a Geohash (in this implementation) is based on coordinates of
    longitude and latitude, the distance between two Geohashes reflects the distance
    in latitude/longitude coordinates between two points, which does not translate
    to actual distance. In this case, we can use the **Haversine** formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Geohash](img/B05261_05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This gives us the actual distance between the two points taking into account
    the curvature of the earth, where:'
  prefs: []
  type: TYPE_NORMAL
- en: '**r** is the radius of the sphere,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**φ1**, **φ2**: latitude of point 1 and latitude of point 2, in radians'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**λ1**, **λ2**: longitude of point 1 and longitude of point 2, in radians'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GeoServer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have successfully loaded GDELT data to Accumulo via GeoMesa, we
    can work towards visualizing that data on a map; this feature is very useful for
    plotting results of analytics on world maps, for example. GeoMesa integrates well
    with GeoServer for this purpose. GeoServer is an **Open Geospatial Consortium**
    (**OGC**) compliant with the implementation of a number of standards including
    **Web Feature Service** (**WFS**) and **Web Map Service** (**WMS**). "It publishes
    data from any major spatial data source".
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to use GeoServer to view the results from our analytics in a clean,
    presentable way. Again, we are not going to delve into getting GeoServer up and
    running, as there is a very good tutorial in the GeoMesa documentation that enables
    the integration of the two. A couple of common points to watch out for are as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The system uses **Java Advanced Imaging** (**JAI**) libraries; if you have
    issues with these, specifically on a Mac, then these can often be fixed by removing
    the libraries from the default Java installation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This will then allow the GeoServer versions to be used, located in `$GEOSERVER_HOME/webapps/geoserver/WEB-INF/lib/`
  prefs: []
  type: TYPE_NORMAL
- en: Again, we cannot stress the importance of versions. You must be very clear about
    which versions of the main modules you are using, for example, Hadoop, Accumulo,
    Zookeeper, and most importantly, GeoMesa. If you mix versions you will see problems
    and the stack traces often mask the true issue. If you do have exceptions, check
    and double-check your versions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Map layers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once GeoServer is running, we can create a layer for visualization. GeoServer
    enables us to publish a single or a group of layers to produce a graphic. When
    we create a layer, we can specify the bounding box, view the feature (which is
    the `SimpleFeature` we created in the Spark code previously), and even run a **Common
    Query Language** (**CQL**) query to filter the data (more about this as follows).
    After a layer has been created, selecting layer preview and the JPG option will
    produce a URL with a graphic similar to the following; temporal bounding here
    is for January 2016 so that the map is not overcrowded:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Map layers](img/image_05_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The URL can be used to produce other graphics, simply by manipulating the arguments.
    A brief breakdown of the URL is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `geoserver` URL with the standard:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The `request` type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The `layers` and `styles`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the layer `transparency`, if required:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The `cql` statement, in this case any row that has an entry with `GoldsteinScale>8`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The bounding box `bbox`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The `height` and `width` of the graphic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Source and `image` type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Filter the content by temporal query bounds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The final step for this section is to attach a world map to this layer so that
    the image becomes more readable. If you search the Internet for world map shape
    files, there are a number of options; we have used one from [http://thematicmapping.org](http://thematicmapping.org).
    Adding one of these into GeoServer as a shape-file store, and then creating and
    publishing a layer before creating a layer group of our GDELT data and the shape-file,
    will produce an image similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Map layers](img/image_05_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: To make things a bit more interesting, we have filtered the events based on
    the `GoldsteinScale` field in the `FeatureType`. By adding `cql_filter=GoldsteinScale
    > 8` to the URL, we can plot all of the points where the `GoldsteinScale` score
    was greater than eight; so essentially, the above image shows us where the highest
    levels of positive sentiment were located in the world, in January 2016!
  prefs: []
  type: TYPE_NORMAL
- en: CQL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Common Query Language** (**CQL**) is a plain text query language created
    by the OGC for the [Catalogue Web Services specification](http://www.opengeospatial.org/standards/cat).
    It is a human-readable query language (unlike, for example, [OGC filters](http://www.opengeospatial.org/standards/filter))
    and uses a similar syntax to SQL. Although similar to SQL, CQL has much less functionality;
    for example, it is quite strict in requiring an attribute to be on the left side
    of any comparison operator.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following lists the CQL supported operators:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Comparison operators: =, <>, >, >=, <, <='
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ID, list and other operators: BETWEEN, BEFORE, AFTER, LIKE, IS, EXISTS, NOT,
    IN'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Arithmetic expression operators: +, -, *, /'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Geometric operators: EQUALS, DISJOINT, INTERSECTS, TOUCHES, CROSSES, WITHIN,
    CONTAINS, OVERLAPS, RELATE, DWITHIN, BEYOND'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Due to the limitations of CQL, GeoServer provides an extended version of CQL
    called ECQL. ECQL provides much of the missing functionality of CQL, providing
    a more flexible language that has more in common with SQL. GeoServer supports
    the use of both CQL and ECQL in WMS and WFS requests.
  prefs: []
  type: TYPE_NORMAL
- en: The quickest way to test CQL queries is to amend the URL of a layer such as
    the one we created above, when using JPGs for example, or to use the CQL box at
    the bottom of the layer option within GeoMesa.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we have several layers defined in one WMS request, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we may want to filter just one of those layers with the CQL query. In
    this case, CQL filters must be ordered in the same way that the layers are; we
    use the `INCLUDE` keyword for the layers that we don''t want to filter and delimit
    them using a ";". For example, to filter only `layer2` in our example, the WMS
    request would appear thus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Be aware when using columns of type `Date`; we need to determine their format
    before attempting any CQL with them. Usually they will be in ISO8601 format; 2012-01-01T00:00:00Z.
    However, different formats may be present depending upon how the data was loaded.
    In our example, we have ensured the SQLDATE is in the correct format.
  prefs: []
  type: TYPE_NORMAL
- en: Gauging oil prices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have a substantial amount of data in our data store (we can always
    add more data using the preceding Spark job) we will proceed to query that data,
    using the GeoMesa API, to get the rows ready for application to our learning algorithm.
    We could of course use raw GDELT files, but the following method is a useful tool
    to have available.
  prefs: []
  type: TYPE_NORMAL
- en: Using the GeoMesa query API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The GeoMesa query API enables us to query for results based upon spatio-temporal
    attributes, whilst also leveraging the parallelization of the data store, in this
    case Accumulo with its iterators. We can use the API to build `SimpleFeatureCollections`,
    which we can then parse to realize GeoMesa `SimpleFeatures` and ultimately the
    raw data that matches our query.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this stage we should build code that is generic, such that we can change
    it easily should we decide later that we have not used enough data, or perhaps
    if we need to change the output fields. Initially, we will extract a few fields;
    `SQLDATE`, `Actor1Name`, `Actor2Name`, and `EventCode`. We should also decide
    on the bounding box for our queries; as we are looking at three different oil
    indexes there is a decision to be made about how we suppose the geographical influence
    of events relates to the oil price itself. This is one of the most difficult variables
    to evaluate, as there are so many factors involved in the price determination;
    arguably the bounding box is the whole world. However, as we are using three indexes,
    we are going to make the assumption that each index has its own geographic limitations,
    based on research regarding the areas of oil supply and the areas of demand. We
    can always vary these bounds later should we have more relevant information, or
    if the results are not favorable and we need to re-evaluate. The proposed initial
    bounding boxes are:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Brent: North Sea and the UK (Supply) and Central Europe (Demand): 34.515610,
    -21.445313 - 69.744748, 36.914063'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'WTI: America (Supply) and Western Europe (Demand): -58.130121, -162.070313,
    71.381635, -30.585938'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OPEC: The Middle East (Supply) and Europe (Demand): -38.350273, -20.390625,
    38.195022, 149.414063'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code to extract our results from GeoMesa is as follows (Brent Oil):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The `RDD[Row]` collection can be written to disk for future use as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We should read in as much data as possible at this point in order to provide
    our algorithm with a large amount of training data. We will split our input data
    between training and test data at a later stage. Therefore, there is no need to
    hold any data back.
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At this stage, we have obtained our data from GeoMesa based on the bounding
    box, and the date range, for a particular oil index. The output has been organized
    such that we have a collection of rows, each one containing the supposed important
    details for one event. We are not sure whether the fields we have chosen for each
    event are entirely relevant in providing enough information to build a reliable
    model so, depending upon our results, this is something that we may have to experiment
    with at a later date. We next need to transform the data into something that can
    be used by our learning process. In this case, we will aggregate the data into
    one-week blocks and transform the data into a typical *bag of words*, starting
    by loading the data from the previous step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Within this RDD, we have the `EventCodes` (CAMEO codes): these will need to
    be transformed into their respective descriptions, so that the bag of words can
    be built. By downloading the CAMEO codes from [http://gdeltproject.org/data/lookups/CAMEO.eventcodes.txt](http://gdeltproject.org/data/lookups/CAMEO.eventcodes.txt),
    we can create a `Map` object for use in the next step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Note that we normalize the output by removing any non-standard characters; the
    aim of this is to try and avoid erroneous characters affecting our training model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now create our `bagOfWordsRDD` by appending the actor codes either side
    of the `EventCode` mapped description, and create a DataFrame from the date and
    formed sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We have previously mentioned that we could work with our data at a daily, weekly,
    or even yearly level; by choosing weekly, we will next need to group our DataFrame
    by week. In Spark 2.0, we can achieve this easily using window functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'As we will produce the oil price data for the end of each week, we should ensure
    that our sentence data is grouped for the days Friday to Thursday, so that we
    can later join this with the price data for that Friday. This is achieved by altering
    the fourth argument of the `window` function; in this case, one day provided the
    correct grouping. If we run the command `sentencesDF.printSchema`, we will see
    that the `sentenceArray` column is an array of strings, while we need just a `String`
    for the input to our learning algorithms. The next code extract demonstrates this
    change, as well as producing the column `commonFriday`, which gives us a reference
    for the date we are working around for each row, as well as a unique key that
    we can join with later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The next step is to collect our data and label it for use in the next stage.
    In order to label it, we must normalize the oil price data we downloaded. Earlier
    in this chapter we mentioned the frequency of data points; at the moment the data
    contains a date and the price at the end of that day. We need to transform our
    data into tuples of (Date, change) where the Date is the Friday of that week and
    the change is a rise or fall based on the average of the daily prices from the
    previous Monday onwards; if the price stays the same, we'll take this to be a
    fall so that we can implement binary value learning algorithms later.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can again use the window feature in Spark DataFrames to easily group the
    data by week; we will also reformat the date as follows, so that the window group
    function performs correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce something similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can calculate the rise or fall from the previous week; first by adding
    the previous week''s `last(PRICE)` to each row (using the Spark `lag` function),
    and then by calculating the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'You will notice the use of the `signum` function; this is very useful for comparison
    as it produces the following outcomes:'
  prefs: []
  type: TYPE_NORMAL
- en: If the first value is less than the second, output -1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the first value is greater than the second, output +1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the two values are equal, output 0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now that we have the two DataFrames, `aggSentenceDF` and `oilPriceChangeDF`,
    we can join the two using the `commonFriday` column to produce a labeled dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'We also drop the window and `sentenceArray` columns, as well as add an ID column,
    so that we can uniquely reference each row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Machine learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We now have input data and the weekly price change; next, we will turn our
    GeoMesa data into numerical vectors that a machine-learning model can work with.
    The Spark machine learning library, MLlib, has a utility called `HashingTF` to
    do just that. `HashingTF` transforms a bag of words into a vector of term frequencies
    by applying a hash function to each term. Because the vector has a finite number
    of elements, it''s possible that two terms will map to the same, hashed term;
    the hashed, vectorized features may not exactly represent the actual content of
    the input text. So, we''ll set up a relatively large feature vector, accommodating
    10,000 different hashed values, to reduce the chance of these collisions. The
    logic behind this is that there are only so many possible events (regardless of
    their size) and therefore a repeat of a previously seen event should produce a
    similar outcome. Of course, the combination of events may change this, which is
    accounted for by initially taking one-week blocks. To format the input data correctly
    for `HashingTF`, we will also execute a `Tokenizer` over the input text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The final preparation step is to implement an **Inverse Document Frequency**
    (**IDF**), this is a numerical measure of how much information each term provides:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: For the purposes of this exercise, we will implement a Naive Bayes implementation
    to perform the machine learning part of our functionality. This algorithm is a
    good initial fit to learn outcomes from a series of inputs; in our case, we hope
    to learn an increase or decrease in oil price given a set of events from the previous
    week.
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Naive Bayes is a simple technique for constructing classifiers: models that
    assign class labels to problem instances, represented as vectors of feature values,
    where the class labels are drawn from some finite set. Naive Bayes is available
    in Spark MLlib, thus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We can tie all of the above steps together using an MLlib Pipeline; a Pipeline
    can be thought of as a workflow that simplifies the combination of multiple algorithms.
    From the Spark documentation some definitions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'DataFrame: This ML API uses DataFrames from Spark SQL as an ML dataset, which
    can hold a variety of data types. For example, a DataFrame could have different
    columns storing text, feature vectors, true labels, and predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Transformer: A Transformer is an algorithm that can transform one DataFrame
    into another DataFrame. For example, an ML model is a Transformer that transforms
    a DataFrame with features into a DataFrame with predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Estimator: An Estimator is an algorithm that can "fit" a DataFrame to produce
    a Transformer. For example, a learning algorithm is an Estimator that trains on
    a DataFrame and produces a model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pipeline: A Pipeline chains multiple Transformers and Estimators together to
    specify an ML workflow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `pipeline` is declared thus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'We noted previously, that all of the available data should be read from GeoMesa,
    as we would split the data at a later stage in order to provide training and test
    data sets. This is performed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'And finally, we can execute the full model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The model can be saved and loaded easily:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To test our model, we should execute the `model` transformer, mentioned as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'This provides a prediction for each of the input rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The results, having been taken from the resultant DataFrame (`model.transform(testDF).select("rawPrediction",
    "probability", "prediction").show`), are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a problem space such as oil price prediction, it is always going to be very
    difficult/near impossible to create a truly successful algorithm, so this chapter
    was always geared towards more of a demonstration piece. However, we have results
    and their legitimacy is not irrelevant; we trained the above algorithms with several
    years of data from the oil indexes and GDELT, and then gleaned the results from
    the outcome of the model execution before comparing it to the correct label.
  prefs: []
  type: TYPE_NORMAL
- en: In tests, the previous model showed a 51% accuracy. This is marginally better
    than what we would expect from simply selecting results at random, but provides
    a firm base upon which to make improvements. With the ability to save data sets
    and models, it would be straightforward to make changes to the model during efforts
    to improve accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: There are many areas of improvement that can be made and we have already mentioned
    some of them during this chapter. In order to improve our model, we should address
    the specific areas in a systematic manner. As we can only make an educated guess
    as to which changes will affect an improvement, it is important to try and address
    the areas of greatest concern first. Following, is a brief summary of how we might
    approach these changes. We should always visit our hypotheses and determine whether
    they are still valid, or where changes should be made.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hypothesis 1: *"The supply and demand [of oil] is affected, to a greater extent,
    by world events and thus we can predict what that supply and demand is likely
    to be."* Our initial attempt at a model has shown 51% accuracy; although this
    is not enough to determine that this hypothesis is valid, it is worth continuing
    with other areas of the model and to attempt to improve accuracy before discounting
    the hypothesis altogether.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hypothesis 2: *"The level of detail of the event will provide better or worse
    accuracy from our algorithm."* We have huge scope for change here; there are several
    areas where we could amend the code and re-run the model quickly, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Number of events: does an increase affect accuracy?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Daily/Weekly/Monthly data roundups: weekly round-ups may not ever give good
    results'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Limited data sets: we currently only use a few fields from GDELT, would more
    fields help with the accuracy?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Preclusion of any other types of data: would the introduction of GKG data help
    with accuracy?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In conclusion, we perhaps have more questions than we started with; however,
    we have now done the ground work to produce an initial model upon which we can
    build, hopefully improving accuracy and leading to a further understanding of
    the data and its potential effect on oil prices.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have introduced the concepts of storing data in a spatio-temporal
    way so that we can use GeoMesa and GeoServer to create and run queries. We have
    shown these queries executed in both the tools themselves and in a programmatic
    way, leveraging GeoServer to display results. Further, we have demonstrated how
    to merge different artifacts to create insights purely from the raw GDELT events,
    before any follow-on processing. Following on from GeoMesa, we have touched upon
    the highly complex world of oil pricing and worked on a simple algorithm to estimate
    weekly oil changes. Whilst it is not reasonable to create an accurate model with
    the time and resources available, we have explored a number of areas of concern
    and attempted to address these, at least at a high level, in order to give an
    insight into possible approaches that can be made in this problem space.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the chapter, we have introduced a number of key Spark libraries and
    functions, the key area being MLlib which we will see in further detail during
    the course of the rest of this book.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, [Chapter 6](ch06.xhtml "Chapter 6. Scraping Link-Based
    External Data"), *Scraping Link-Based External Data*, we further implement the
    GDELT dataset to build a web scale news scanner for tracking trends.
  prefs: []
  type: TYPE_NORMAL
