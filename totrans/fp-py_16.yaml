- en: Chapter 16. Optimizations and Improvements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we'll look at a few optimizations that we can make to create
    high performance functional programs. We'll expand on the `@lru_cache` decorator
    from [Chapter 10](ch10.html "Chapter 10. The Functools Module"), *The Functools
    Module*. We have a number of ways to implement the memoization algorithm. We'll
    also discuss how to write our own decorators. More importantly, we'll see how
    we use a `Callable` object to cache memoized results.
  prefs: []
  type: TYPE_NORMAL
- en: We'll also look at some optimization techniques that were presented in [Chapter
    6](ch06.html "Chapter 6. Recursions and Reductions"), *Recursions and Reductions*.
    We'll review the general approach to tail recursion optimization. For some algorithms,
    we can combine memoization with a recursive implementation and achieve good performance.
    For other algorithms, memoization isn't really very helpful and we have to look
    elsewhere for performance improvements.
  prefs: []
  type: TYPE_NORMAL
- en: In most cases, small changes to a program will lead to small improvements in
    performance. Replacing a function with a `lambda` object will have a tiny impact
    on performance. If we have a program that is unacceptably slow, we often have
    to locate a completely new algorithm or data structure. Some algorithms have bad
    "big-O" complexity; nothing will make them magically run faster.
  prefs: []
  type: TYPE_NORMAL
- en: One place to start is [http://www.algorist.com](http://www.algorist.com). This
    is a resource that may help to locate better algorithms for a given problem.
  prefs: []
  type: TYPE_NORMAL
- en: Memoization and caching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we saw in [Chapter 10](ch10.html "Chapter 10. The Functools Module"), *The
    Functools Module*, many algorithms can benefit from memoization. We'll start with
    a review of some previous examples to characterize the kinds of functions that
    can be helped with memoization.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 6](ch06.html "Chapter 6. Recursions and Reductions"), *Recursions
    and Reductions*, we looked at a few common kinds of recursions. The simplest kind
    of recursion is a tail recursion with arguments that can be easily matched to
    values in a cache. If the arguments are integers, strings, or materialized collections,
    then we can compare arguments quickly to determine if the cache has a previously
    computed result.
  prefs: []
  type: TYPE_NORMAL
- en: We can see from these examples that integer numeric calculations such as computing
    factorial or locating a Fibonacci number will be obviously improved. Locating
    prime factors and raising integers to powers are more examples of numeric algorithms
    that apply to integer values.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we looked at the recursive version of a Fibonacci number calculator, we
    saw that it contained two tail-call recursions. Here''s the definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Memoization and caching](graphics/B03652_16_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This can be turned into a loop, but the design change requires some thinking.
    The memoized version of this can be quite fast and doesn't require quite so much
    thinking to design.
  prefs: []
  type: TYPE_NORMAL
- en: The Syracuse function, shown in [Chapter 6](ch06.html "Chapter 6. Recursions
    and Reductions"), *Recursions and Reductions*, is an example of the kind of function
    used to compute fractal values. It contains a simple rule that's applied recursively.
    Exploring the Collatz conjecture ("does the Syracuse function always lead to 1?")
    requires memoized intermediate results.
  prefs: []
  type: TYPE_NORMAL
- en: The recursive application of the Syracuse function is an example of a function
    with an "attractor," where the value is attracted to 1\. In some higher dimensional
    functions, the attractor can be a line or perhaps a fractal. When the attractor
    is a point, memoization can help; otherwise, memoization may actually be a hindrance,
    since each fractal value is unique.
  prefs: []
  type: TYPE_NORMAL
- en: 'When working with collections, the benefits of caching may vanish. If the collection
    happens to have the same number of integer values, strings, or tuples, then there''s
    a chance that the collection is a duplicate and time can be saved. However, if
    a calculation on a collection will be needed more than once, manual optimization
    is best: do the calculation once and assign the results to a variable.'
  prefs: []
  type: TYPE_NORMAL
- en: When working with iterables, generator functions, and other lazy objects, caching
    of an overall object is essentially impossible. In these cases, memoization is
    not going to help at all.
  prefs: []
  type: TYPE_NORMAL
- en: Raw data that includes measurements often use floating point values. Since an
    exact equality comparison between floating point values may not work out well,
    memoizing intermediate results may not work out well either.
  prefs: []
  type: TYPE_NORMAL
- en: Raw data that includes counts, however, may benefit from memoization. These
    are integers, and we can trust exact integer comparisons to (potentially) save
    recalculating a previous value. Some statistical functions, when applied to counts,
    can benefit from using the `fractions` module instead of floating point values.
    When we replace `x/y` with the `Fraction(x,y)` method, we've preserved the ability
    to do exact value matching. We can produce the final result using the `float(some_fraction)`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: Specializing memoization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The essential idea of memoization is so simple that it can be captured by the
    `@lru_cache` decorator. This decorator can be applied to any function to implement
    memoization. In some cases, we might be able to improve on the generic idea with
    something more specialized. There are a large number of potentially optimizable
    multivalued functions. We'll pick one here and look at another in a more complex
    case study.
  prefs: []
  type: TYPE_NORMAL
- en: 'The binomial, ![Specializing memoization](graphics/B03652_16_33.jpg), shows
    the number of ways *n* different things can be arranged in groups of size *m*.
    The value is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Specializing memoization](graphics/B03652_16_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Clearly, we should cache the factorial calculations rather than redo all those
    multiplications. However, we may also benefit from caching the overall binomial
    calculation, too.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll create a C`allable` object that contains multiple internal caches. Here''s
    a helper function that we''ll need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The `prod()` function computes the product of an iterable of numbers. It's defined
    as a reduction using the `*` operator.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a C`allable` object with two caches that uses this `prod()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We created two caches: one for factorial values and one for binomial coefficient
    values. The internal `fact()` method uses the `fact_cache` attribute. If the value
    isn''t in the cache, it''s computed and added to the cache. The external `__call__()`
    method uses the `bin_cache` attribute in a similar way: if a particular binomial
    has already been calculated, the answer is simply returned. If not, the internal
    `fact()` method is used to compute a new value.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the preceding `Callable` class like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This shows how we can create a C`allable` object from our class and then invoke
    the object on a particular set of arguments. There are a number of ways that a
    52-card deck can be dealt into 5-card hands. There are 2.6 million possible hands.
  prefs: []
  type: TYPE_NORMAL
- en: Tail recursion optimizations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In [Chapter 6](ch06.html "Chapter 6. Recursions and Reductions"), *Recursions
    and Reductions*, among many others, we looked at how a simple recursion can be
    optimized into a `for` loop. The general approach is this:'
  prefs: []
  type: TYPE_NORMAL
- en: Design the recursion. This means the base case and the recursive cases. For
    example, this is a definition of computing:![Tail recursion optimizations](graphics/B03652_16_04.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To design the recursion execute the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'If the recursion has a simple call at the end, replace the recursive case with
    a `for` loop. The command is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: When the recursion appears at the end of a simple function, it's described as
    a tail–call optimization. Many compilers will optimize this into a loop. Python—lacking
    this optimization in its compiler—doesn't do this kind of tail-call transformation.
  prefs: []
  type: TYPE_NORMAL
- en: This pattern is very common. Performing the tail-call optimization improves
    performance and removes any upper bound on the number of recursions that can be
    done.
  prefs: []
  type: TYPE_NORMAL
- en: 'Prior to doing any optimization, it''s absolutely essential that the function
    already works. For this, a simple `doctest` string is often sufficient. We might
    use annotation on our factorial functions like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We added two edge cases: the explicit base case and the first item beyond the
    base case. We also added another item that would involve multiple iterations.
    This allows us to tweak the code with confidence.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When we have a more complex combination of functions, we might need to execute
    commands like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `__test__` variable is used by the `doctest.testmod()` function. All of
    the values in the dictionary associated with the `__test__` variable are examined
    for the `doctest` strings. This is a handy way to test features that come from
    compositions of functions. This is also called integration testing, since it tests
    the integration of multiple software components.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having working code with a set of tests gives us the confidence to make optimizations.
    We can easily confirm the correctness of the optimization. Here''s a popular quote
    that is used to describe optimization:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | *"Making a wrong program worse is no sin."* |   |'
  prefs: []
  type: TYPE_TB
- en: '|   | --*Jon Bentley* |'
  prefs: []
  type: TYPE_TB
- en: This appeared in the *Bumper Sticker Computer Science* chapter of *More Programming
    Pearls*, published by Addison-Wesley, Inc. What's important here is that we should
    only optimize code that's actually correct.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There''s no general rule for optimization. We often focus on optimizing performance
    because we have tools like the Big O measure of complexity that show us whether
    or not an algorithm is an effective solution to a given problem. Optimizing storage
    is usually tackled separately: we can look at the steps in an algorithm and estimate
    the size of the storage required for the various storage structures.'
  prefs: []
  type: TYPE_NORMAL
- en: In many cases, the two considerations are opposed. In some cases, an algorithm
    that has outstandingly good performance requires a large data structure. This
    algorithm can't scale without dramatic increases in the amount of storage required.
    Our goal is to design an algorithm that is reasonably fast and also uses an acceptable
    amount of storage.
  prefs: []
  type: TYPE_NORMAL
- en: 'We may have to spend time researching algorithmic alternatives to locate a
    way to make the space-time trade off properly. There are some common optimization
    techniques. We can often follow links from Wikipedia: [http://en.wikipedia.org/wiki/Space–time_tradeoff](http://en.wikipedia.org/wiki/Space–time_tradeoff).'
  prefs: []
  type: TYPE_NORMAL
- en: One memory optimization technique we have in Python is to use an iterable. This
    has some properties of a proper materialized collection, but doesn't necessarily
    occupy storage. There are few operations (such as the `len()` function) that can't
    work on an iterable. For other operations, the memory saving feature can allow
    a program to work with very large collections.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing accuracy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a few cases, we need to optimize the accuracy of a calculation. This can
    be challenging and may require some fairly advanced math to determine the limits
    on the accuracy of a given approach.
  prefs: []
  type: TYPE_NORMAL
- en: An interesting thing we can do in Python is replace floating point approximations
    with `fractions.Fraction` value. For some applications, this can create more accurate
    answers than floating point, because more bits are used for numerator and denominator
    than a floating point mantissa.
  prefs: []
  type: TYPE_NORMAL
- en: It's important to use `decimal.Decimal` values to work with currency. It's a
    common error to use a `float` value. When using a `float` value, additional noise
    bits are introduced because of the mismatch between `Decimal` values provided
    as input and the binary approximation used by floating point values. Using `Decimal`
    values prevents the introduction of tiny inaccuracies.
  prefs: []
  type: TYPE_NORMAL
- en: In many cases, we can make small changes to a Python application to switch from
    `float` values to `Fraction` or `Decimal` values. When working with transcendental
    functions, this change isn't necessarily beneficial. Transcendental functions—by
    definition—involve irrational numbers.
  prefs: []
  type: TYPE_NORMAL
- en: Reducing accuracy based on audience requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For some calculations, a fraction value may be more intuitively meaningful than
    a floating point value. This is part of presenting statistical results in a way
    that an audience can understand and take action on.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the chi-squared test generally involves computing the ![Reducing
    accuracy based on audience requirements](graphics/B03652_16_35.jpg) comparison
    between actual values and expected values. We can then subject this comparison
    value to a test against the ![Reducing accuracy based on audience requirements](graphics/B03652_16_35.jpg)
    cumulative distribution function. When the expected and actual values have no
    particular relationship—we can call this a null relationship—the variation will
    be random; ![Reducing accuracy based on audience requirements](graphics/B03652_16_35.jpg)
    the value tends to be small. When we accept the null hypothesis, then we'll look
    elsewhere for a relationship. When the actual values are significantly different
    from the expected values, we may reject the null hypothesis. By rejecting the
    null hypothesis, we can explore further to determine the precise nature of the
    relationship.
  prefs: []
  type: TYPE_NORMAL
- en: The decision is often based on the table of the ![Reducing accuracy based on
    audience requirements](graphics/B03652_16_35.jpg) **Cumulative Distribution Function**
    (**CDF**) for selected ![Reducing accuracy based on audience requirements](graphics/B03652_16_35.jpg)
    values and given degrees of freedom. While the tabulated CDF values are mostly
    irrational values, we don't usually use more than two or three decimal places.
    This is merely a decision-making tool, there's no practical difference in meaning
    between 0.049 and 0.05.
  prefs: []
  type: TYPE_NORMAL
- en: A widely used probability is 0.05 for rejecting the null hypothesis. This is
    a `Fraction` object less than 1/20\. When presenting data to an audience, it sometimes
    helps to characterize results as fractions. A value like 0.05 is hard to visualize.
    Describing a relationship has having 1 chance in 20 can help to characterize the
    likelihood of a correlation.
  prefs: []
  type: TYPE_NORMAL
- en: Case study – making a chi-squared decision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We'll look at a common statistical decision. The decision is described in detail
    at [http://www.itl.nist.gov/div898/handbook/prc/section4/prc45.htm](http://www.itl.nist.gov/div898/handbook/prc/section4/prc45.htm).
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a chi-squared decision on whether or not data is distributed randomly.
    In order to make this decision, we''ll need to compute an expected distribution
    and compare the observed data to our expectations. A significant difference means
    there''s something that needs further investigation. An insignificant difference
    means we can use the null hypothesis that there''s nothing more to study: the
    differences are simply random variation.'
  prefs: []
  type: TYPE_NORMAL
- en: We'll show how we can process the data with Python. We'll start with some backstory—some
    details that are not part of the case study, but often features an **Exploratory
    Data Analysis** (**EDA**) application. We need to gather the raw data and produce
    a useful summary that we can analyze.
  prefs: []
  type: TYPE_NORMAL
- en: 'Within the production quality assurance operations, silicon wafer defect data
    is collected into a database. We might use SQL queries to extract defect details
    for further analysis. For example, a query could look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The output from this query could be a CSV file with individual defect details:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We need to summarize the preceding data. We might summarize at the SQL query
    level using the `COUNT` and `GROUP BY` statements. We might also summarize at
    the Python application level. While a pure database summary is often described
    as being more efficient, this isn't always true. In some cases, a simple extract
    of raw data and a Python application to summarize can be faster than a SQL summary.
    If performance is important, both alternatives must be measured, rather than hoping
    that the database is fastest.
  prefs: []
  type: TYPE_NORMAL
- en: 'In some cases, we may be able to get summary data from the database efficiently.
    This summary must have three attributes: the shift, type of defect, and a count
    of defects observed. The summary data looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The output will show all of the 12 combinations of shift and defect type.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next section, we''ll focus on reading the raw data to create summaries.
    This is the kind of context in which Python is particularly powerful: working
    with raw source data.'
  prefs: []
  type: TYPE_NORMAL
- en: We need to observe and compare shift and defect counts with an overall expectation.
    If the difference between observed counts and expected counts can be attributed
    to random fluctuation, we have to accept the null hypothesis that nothing interesting
    is going wrong. If, on the other hand, the numbers don't fit with random variation,
    then we have a problem that requires further investigation.
  prefs: []
  type: TYPE_NORMAL
- en: Filtering and reducing the raw data with a Counter object
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ll represent the essential defect counts as a `collections.Counter` parameter.
    We will build counts of defects by shift and defect type from the detailed raw
    data. Here''s a function to read some raw data from a CSV file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The preceding function will create a dictionary reader based on an open file
    provided via the `input` parameter. We've confirmed that the column names match
    the three expected column names. In some cases, we'll have extra columns in the
    file; in this case, the assertion will be something like `all((c in rdr.fieldnames)
    for c in […])`. Given a tuple of column names, this will assure that all of the
    required columns are present in the source. We can also use sets to assure that
    `set(rdr.fieldnames) <= set([...])`.
  prefs: []
  type: TYPE_NORMAL
- en: We created a `types.SimpleNamespace` parameter for each row. In the preceding
    example, the supplied column names are valid Python variable names that allow
    us to easily turn a dictionary into a namespace. In some cases, we'll need to
    map column names to Python variable names to make this work.
  prefs: []
  type: TYPE_NORMAL
- en: A `SimpleNamespace` parameter allows us to use slightly simpler syntax to refer
    to items within the row. Specifically, the next generator expression uses references
    such as `row.shift` and `row.defect_type` instead of the bulkier `row['shift']`
    or `row['defect_type']` references.
  prefs: []
  type: TYPE_NORMAL
- en: We can use a more complex generator expression to do a map-filter combination.
    We'll filter each row to ignore rows with no defect code. For rows with a defect
    code, we're mapping an expression which creates a two tuple from the `row.shift`
    and `row.defect_type` references.
  prefs: []
  type: TYPE_NORMAL
- en: In some applications, the filter won't be a trivial expression such as `row.defect_type`.
    It may be necessary to write a more sophisticated condition. In this case, it
    may be helpful to use the `filter()` function to apply the complex condition to
    the generator expression that provides the data.
  prefs: []
  type: TYPE_NORMAL
- en: Given a generator that will produce a sequence of `(shift, defect)` tuples,
    we can summarize them by creating a `Counter` object from the generator expression.
    Creating this `Counter` object will process the lazy generator expressions, which
    will read the source file, extract fields from the rows, filter the rows, and
    summarize the counts.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll use the `defect_reduce()` function to gather and summarize the data
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We can open a file, gather the defects, and display them to be sure that we've
    properly summarized by shift and defect type. Since the result is a `Counter`
    object, we can combine it with other `Counter` objects if we have other sources
    of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `defects` value looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We have defect counts organized by shift and defect types. We'll look at alternative
    input of summarized data next. This reflects a common use case where data is available
    at the summary level.
  prefs: []
  type: TYPE_NORMAL
- en: Once we've read the data, the next step is to develop two probabilities so that
    we can properly compute expected defects for each shift and each type of defect.
    We don't want to divide the total defect count by 12, since that doesn't reflect
    the actual deviations by shift or defect type. The shifts may be more or less
    equally productive. The defect frequencies are certainly not going to be similar.
    We expect some defects to be very rare and others to be more common.
  prefs: []
  type: TYPE_NORMAL
- en: Reading summarized data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As an alternative to reading all of the raw data, we can look at processing
    only the summary counts. We want to create a `Counter` object similar to the previous
    example; this will have defect counts as a value with a key of shift and defect
    code. Given summaries, we simply create a `Counter` object from the input dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a function that will read our summary data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We require an open file as the input. We'll create a `csv.DictReader()` function
    that helps parse the raw CSV data that we got from the database. We included an
    `assert` statement to confirm that the file really has the expected data.
  prefs: []
  type: TYPE_NORMAL
- en: We defined a `lambda` object that creates a two tuple with the key and the integer
    conversion of the count. The key is itself a two tuple with the shift and defect
    information. The result will be a sequence such as `((shift,defect), count), ((shift,defect),
    count), …)`. When we map the `lambda` to the `DictReader` parameter, we'll have
    a generator function that can emit the sequence of two tuples.
  prefs: []
  type: TYPE_NORMAL
- en: We will create a dictionary from the collection of two tuples and use this dictionary
    to build a `Counter` object. The `Counter` object can easily be combined with
    other `Counter` objects. This allows us to combine details acquired from several
    sources. In this case, we only have a single source.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can assign this single source to the variable `defects`. The value looks
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This matches the detail summary shown previously. The source data, however,
    was already summarized. This is often the case when data is extracted from a database
    and SQL is used to do group-by operations.
  prefs: []
  type: TYPE_NORMAL
- en: Computing probabilities from a Counter object
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We need to compute the probabilities of defects by shift and defects by type.
    In order to compute the expected probabilities, we need to start with some simple
    sums. The first is the overall sum of all defects, which can be calculated by
    executing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This is done directly from the values in the `Counter` object assigned to the
    `defects` variable. This will show that there are 309 total defects in the sample
    set.
  prefs: []
  type: TYPE_NORMAL
- en: We need to get defects by shift as well as defects by type. This means that
    we'll extract two kinds of subsets from the raw defect data. The "by-shift" extract
    will use just one part of the `(shift,defect type)` key in the `Counter` object.
    The "by-type" will use the other half of the key pair.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can summarize by creating additional `Counter` objects extracted from the
    initial set of the `Counter` objects assigned to the `defects` variable. Here''s
    the by-shift summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We've created a collection of individual `Counter` objects that have a shift,
    `s`, as the key and the count of defects associated with that shift `defects[s,d]`.
    The generator expression will create 12 such `Counter` objects to extract data
    for all combinations of four defect types and three shifts. We'll combine the
    `Counter` objects with a `sum()` function to get three summaries organized by
    shift.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can't use the default initial value of 0 for the `sum()` function. We must
    provide an empty `Counter()` function as an initial value.
  prefs: []
  type: TYPE_NORMAL
- en: 'The type totals are created with an expression similar to the one used to create
    shift totals:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We created a dozen `Counter` objects using the defect type, `d`, as the key
    instead of shift type; otherwise, the processing is identical.
  prefs: []
  type: TYPE_NORMAL
- en: 'The shift totals look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The defect type totals look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We've kept the summaries as `Counter` objects, rather than creating simple `dict`
    objects or possibly even `list` instances. We'll generally use them as simple
    dicts from this point forward. However, there are some situations where we will
    want proper `Counter` objects instead of reductions.
  prefs: []
  type: TYPE_NORMAL
- en: Alternative summary approaches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We've read the data and computed summaries in two separate steps. In some cases,
    we may want to create the summaries while reading the initial data. This is an
    optimization that might save a little bit of processing time. We could write a
    more complex input reduction that emitted the grand total, the shift totals, and
    the defect type totals. These `Counter` objects would be built one item at a time.
  prefs: []
  type: TYPE_NORMAL
- en: We've focused on using the `Counter` instances, because they seem to allow us
    flexibility. Any changes to the data acquisition will still create `Counter` instances
    and won't change the subsequent analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s how we can compute the probabilities of defect by shift and by defect
    type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ve created two dictionaries: `P_shift` and `P_type`. The `P_shift` dictionary
    maps a shift to a `Fraction` object that shows the shift''s contribution to the
    overall number of defects. Similarly, the `P_type` dictionary maps a defect type
    to a `Fraction` object that shows the type''s contribution to the overall number
    of defects.'
  prefs: []
  type: TYPE_NORMAL
- en: We've elected to use `Fraction` objects to preserve all of the precision of
    the input values. When working with counts like this, we may get probability values
    that make more intuitive sense to people reviewing the data.
  prefs: []
  type: TYPE_NORMAL
- en: We've elected to use `dict` objects because we've switched modes. At this point
    in the analysis, we're no longer accumulating details; we're using reductions
    to compare actual and observed data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `P_shift` data looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The `P_type` data looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: A value such as 32/103 or 96/309 might be more meaningful to some people than
    0.3106\. We can easily get `float` values from `Fraction` objects, as we'll see
    later.
  prefs: []
  type: TYPE_NORMAL
- en: The shifts all seem to be approximately at the same level of defect production.
    The defect types vary, which is typical. It appears that the defect `C` is a relatively
    common problem, whereas the defect `B` is much less common. Perhaps the second
    defect requires a more complex situation to arise.
  prefs: []
  type: TYPE_NORMAL
- en: Computing expected values and displaying a contingency table
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The expected defect production is a combined probability. We'll compute the
    shift defect probability multiplied by the probability based on defect type. This
    will allow us to compute all 12 probabilities from all combinations of shift and
    defect type. We can weight these with the observed numbers and compute the detailed
    expectation for defects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the calculation of expected values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We'll create a dictionary that parallels the initial `defects Counter` object.
    This dictionary will have a sequence of two tuples with keys and values. The keys
    will be two tuples of shift and defect type. Our dictionary is built from a generator
    expression that explicitly enumerates all combinations of keys from the `P_shift`
    and `P_type` dictionaries.
  prefs: []
  type: TYPE_NORMAL
- en: 'The value of the `expected` dictionary looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Each item of the mapping has a key with shift and defect type. This is associated
    with a `Fraction` value based on the probability of defect based on shift times,
    the probability of a defect based on defect type times the overall number of defects.
    Some of the fractions are reduced, for example, a value of 6624/309 can be simplified
    to 2208/103.
  prefs: []
  type: TYPE_NORMAL
- en: Large numbers are awkward as proper fractions. Displaying large values as `float`
    values is often easier. Small values (such as probabilities) are sometimes easier
    to understand as fractions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll print the observed and expected times in pairs. This will help us visualize
    the data. We''ll create something that looks like the following to help summarize
    what we''ve observed and what we expect:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This shows 12 cells. Each cell has values with the observed number of defects
    and an expected number of defects. Each row ends with the shift totals, and each
    column has a footer with the defect totals.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, we might export this data in CSV notation and build a spreadsheet.
    In other cases, we'll build an HTML version of the contingency table and leave
    the layout details to a browser. We've shown a pure text version here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a sequence of statements to create the contingency table shown previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: This spreads the defect types across each line. We've written enough `obs exp`
    column titles to cover all defect types. For each shift, we'll emit a line of
    observed and actual pairs, followed by a shift total. At the bottom, we'll emit
    a line of footers with just the defect type totals and the grand total.
  prefs: []
  type: TYPE_NORMAL
- en: A contingency table like this one helps us to visualize the comparison between
    observed and expected values. We can compute a chi-squared value for these two
    sets of values. This will help us decide if the data is random or if there's something
    that deserves further investigation.
  prefs: []
  type: TYPE_NORMAL
- en: Computing the chi-squared value
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The ![Computing the chi-squared value](graphics/B03652_16_35.jpg) value is based
    on ![Computing the chi-squared value](graphics/B03652_16_06.jpg), where the *e*
    values are the expected values and the *o* values are the observed values.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can compute the specified formula''s value as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: We've defined a small `lambda` to help us optimize the calculation. This allows
    us to execute the `expected[s,t]` and `defects[s,t]` attributes just once, even
    though the expected value is used in two places. For this dataset, the final ![Computing
    the chi-squared value](graphics/B03652_16_35.jpg) value is 19.18.
  prefs: []
  type: TYPE_NORMAL
- en: There are a total of six degrees of freedom based on three shifts and four defect
    types. Since we're considering them independent, we get *2×3=6*. A chi-squared
    table shows us that anything below 12.5916 would reflect 1 chance in 20 of the
    data being truly random. Since our value is 19.18, the data is unlikely to be
    random.
  prefs: []
  type: TYPE_NORMAL
- en: 'The cumulative distribution function for ![Computing the chi-squared value](graphics/B03652_16_35.jpg)
    shows that a value of 19.18 has a probability of the order of 0.00387: about 4
    chances in 1000 of being random. The next step is a follow-up study to discover
    the details of the various defect types and shifts. We''ll need to see which independent
    variable has the biggest correlation with defects and continue the analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of following up with this case study, we'll look at a different and
    interesting calculation.
  prefs: []
  type: TYPE_NORMAL
- en: Computing the chi-squared threshold
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The essence of the ![Computing the chi-squared threshold](graphics/B03652_16_35.jpg)
    test is a threshold value based on the number of degrees of freedom and the level
    of uncertainty we're willing to entertain in accepting or rejecting the null hypothesis.
    Conventionally, we're advised to use a threshold around 0.05 (1/20) to reject
    the null hypothesis. We'd like there to be only 1 chance in 20 that the data is
    simply random and it appears meaningful. In other words, we'd like there to be
    19 chances in 20 that the data reflects simple random variation.
  prefs: []
  type: TYPE_NORMAL
- en: The chi-squared values are usually provided in tabular form because the calculation
    involves a number of transcendental functions. In some cases, libraries will provide
    the ![Computing the chi-squared threshold](graphics/B03652_16_35.jpg) cumulative
    distribution function, allowing us to compute a value rather than look one up
    on tabulation of important values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The cumulative distribution function for a ![Computing the chi-squared threshold](graphics/B03652_16_35.jpg)
    value, *x*, and degrees of freedom, *f*, is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Computing the chi-squared threshold](graphics/B03652_16_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: It's common to the probability of being random as ![Computing the chi-squared
    threshold](graphics/B03652_16_08.jpg). That is, if *p > 0.05*, the data can be
    understood as random; the null hypothesis is true.
  prefs: []
  type: TYPE_NORMAL
- en: 'This requires two calculations: the incomplete `gamma` function, ![Computing
    the chi-squared threshold](graphics/B03652_16_09.jpg), and the complete `gamma`
    function, ![Computing the chi-squared threshold](graphics/B03652_16_10.jpg). These
    can involve some fairly complex math. We''ll cut some corners and implement two
    pretty-good approximations that are narrowly focused on just this problem. Each
    of these functions will allow us to look at functional design issues.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Both of these functions will require a factorial calculation, ![Computing the
    chi-squared threshold](graphics/B03652_16_11.jpg). We''ve seen several variations
    on the fractions theme. We''ll use the following one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This is ![Computing the chi-squared threshold](graphics/B03652_16_12.jpg):
    a product of numbers from 2 to *k* (inclusive). We''ve omitted the unit test cases.'
  prefs: []
  type: TYPE_NORMAL
- en: Computing the partial gamma value
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The partial `gamma` function has a simple series expansion. This means that
    we're going to compute a sequence of values and then do a sum on those values.
    For more information, visit [http://dlmf.nist.gov/8](http://dlmf.nist.gov/8).
  prefs: []
  type: TYPE_NORMAL
- en: '![Computing the partial gamma value](graphics/B03652_16_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This series will have a sequence of terms that—eventually—become too small
    to be relevant. The calculation ![Computing the partial gamma value](graphics/B03652_16_14.jpg)
    will yield alternating signs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Computing the partial gamma value](graphics/B03652_16_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The sequence of terms looks like this with *s=1* and *z=2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: At some point, each additional term won't have any significant impact on the
    result.
  prefs: []
  type: TYPE_NORMAL
- en: When we look back at the cumulative distribution function, ![Computing the partial
    gamma value](graphics/B03652_16_16.jpg), we can consider working with `fractions.Fraction`
    values. The degrees of freedom, *k*, will be an integer divided by 2\. The ![Computing
    the partial gamma value](graphics/B03652_16_35.jpg) value, *x*, may be either
    a `Fraction` or a `float` value; it will rarely be a simple integer value.
  prefs: []
  type: TYPE_NORMAL
- en: When evaluating the terms of ![Computing the partial gamma value](graphics/B03652_16_17.jpg),
    the value of ![Computing the partial gamma value](graphics/B03652_16_18.jpg) will
    involve integers and can be represented as a proper `Fraction` value. The value
    of ![Computing the partial gamma value](graphics/B03652_16_19.jpg) could be a
    `Fraction` or `float` value; it will lead to irrational values when ![Computing
    the partial gamma value](graphics/B03652_16_20.jpg) is not an `integer` value.
    The value of ![Computing the partial gamma value](graphics/B03652_16_20.jpg) will
    be a proper `Fraction` value, sometimes it will have the integer values, and sometimes
    it will have values that involve 1/2.
  prefs: []
  type: TYPE_NORMAL
- en: The use of `Fraction` value here—while possible—doesn't seem to be helpful because
    there will be an irrational value computed. However, when we look at the complete
    `gamma` function given here, we'll see that `Fraction` values are potentially
    helpful. In this function, they're merely incidental.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s an implementation of the previously explained series expansion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: We defined a `term()` function that will yield a series of terms. We used a
    `for` statement with an upper limit to generate only 100 terms. We could have
    used the `itertools.count()` function to generate an infinite sequence of terms.
    It seems slightly simpler to use a loop with an upper bound.
  prefs: []
  type: TYPE_NORMAL
- en: We computed the irrational ![Computing the partial gamma value](graphics/B03652_16_19.jpg)
    value and created a `Fraction` value from this value by itself. If the value for
    *z* is also a `Fraction` value and not a `float` value then, the value for `t2`
    will be a `Fraction` value. The value for `term()` function will then be a product
    of two `Fraction` objects.
  prefs: []
  type: TYPE_NORMAL
- en: We defined a `take_until()` function that takes values from an iterable, until
    a given function is true. Once the function becomes true, no more values are consumed
    from the iterable. We also defined a small threshold value, `ε`, of ![Computing
    the partial gamma value](graphics/B03652_16_21.jpg). We'll take values from the
    `term()` function until the values are less than `ε`. The sum of these values
    is an approximation to the partial `gamma` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some test cases we can use to confirm that we''re computing this properly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Computing the partial gamma value](graphics/B03652_16_22.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![Computing the partial gamma value](graphics/B03652_16_23.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![Computing the partial gamma value](graphics/B03652_16_24.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: The error function, `erf()`, is another interesting function. We won't look
    into it here because it's available in the Python math library.
  prefs: []
  type: TYPE_NORMAL
- en: Our interest is narrowly focused on the chi-squared distribution. We're not
    generally interested in the incomplete `gamma` function for other mathematical
    purposes. Because of this, we can narrow our test cases to the kinds of values
    we expect to be using. We can also limit the accuracy of the results. Most chi-squared
    tests involve three digits of precision. We've shown seven digits in the test
    data, which is more than we might properly need.
  prefs: []
  type: TYPE_NORMAL
- en: Computing the complete gamma value
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The complete `gamma` function is a bit more difficult. There are a number of
    different approximations. For more information, visit [http://dlmf.nist.gov/5](http://dlmf.nist.gov/5).
    There's a version available in the Python math library. It represents a broadly
    useful approximation that is designed for many situations.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''re not actually interested in a general implementation of the complete
    `gamma` function. We''re interested in just two special cases: integer values
    and halves. For these two special cases, we can get exact answers, and don''t
    need to rely on an approximation.'
  prefs: []
  type: TYPE_NORMAL
- en: For integer values, ![Computing the complete gamma value](graphics/B03652_16_25.jpg).
    The `gamma` function for integers can rely on the factorial function we defined
    previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'For halves, there''s a special form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Computing the complete gamma value](graphics/B03652_16_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This includes an irrational value, so we can only represent this approximately
    using `float` or `Fraction` objects.
  prefs: []
  type: TYPE_NORMAL
- en: Since the chi-squared cumulative distribution function only uses the following
    two features of the complete `gamma` function, we don't need a general approach.
    We can cheat and use the following two values, which are reasonably precise.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we use proper `Fraction` values, then we can design a function with a few
    simple cases: an `integer` value, a `Fraction` value with 1 in the denominator,
    and a `Fraction` value with 2 in the denominator. We can use the `Fraction` value
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We called the function `Gamma_Half` to emphasize that this is only appropriate
    for whole numbers and halves. For integer values, we''ll use the `fact()` function
    that was defined previously. For `Fraction` objects with a denominator of 1, we''ll
    use the same `fact()` definition: ![Computing the complete gamma value](graphics/B03652_16_25.jpg).'
  prefs: []
  type: TYPE_NORMAL
- en: For the cases where the denominator is 2, we can use the more complex "closed
    form" value. We used an explicit `Fraction()` function for the value ![Computing
    the complete gamma value](graphics/B03652_16_27.jpg). We've also provided a `Fraction`
    approximation for the irrational value ![Computing the complete gamma value](graphics/B03652_16_28.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some test cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Computing the complete gamma value](graphics/B03652_16_29.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![Computing the complete gamma value](graphics/B03652_16_30.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![Computing the complete gamma value](graphics/B03652_16_31.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![Computing the complete gamma value](graphics/B03652_16_32.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: 'These can also be shown as proper `Fraction` values. The irrational values
    lead to large, hard-to-read fractions. We can use something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: This provides a value where the denominator has been limited to be in the range
    of 1 to 2 million; this provides pleasant-looking six-digit numbers that we can
    use for unit test purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Computing the odds of a distribution being random
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have the incomplete `gamma` function, `gamma`, and complete `gamma`
    function, `Gamma_Half`, we can compute the ![Computing the odds of a distribution
    being random](graphics/B03652_16_35.jpg) `CDF` values. The `CDF` value shows us
    the odds of a given ![Computing the odds of a distribution being random](graphics/B03652_16_35.jpg)
    value being random or having some possible correlation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The function itself is quite small:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: We included some `docstring` comments to clarify the parameters. We created
    proper `Fraction` objects from the degrees of freedom and the chi-squared value,
    *x*. When converting a `float` value to a `Fraction` object, we'll create a very
    large fractional result with a large number of entirely irrelevant digits.
  prefs: []
  type: TYPE_NORMAL
- en: We can use `Fraction(x/2).limit_denominator(1000)` to limit the size of the
    `x/2` `Fraction` method to a respectably small number of digits. This will compute
    a correct `CDF` value, but won't lead to gargantuan fractions with dozens of digits.
  prefs: []
  type: TYPE_NORMAL
- en: Here are some sample data called from a table of ![Computing the odds of a distribution
    being random](graphics/B03652_16_35.jpg). Visit [http://en.wikipedia.org/wiki/Chi-squared_distribution](http://en.wikipedia.org/wiki/Chi-squared_distribution)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'To compute the correct `CDF` values execute the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Given ![Computing the odds of a distribution being random](graphics/B03652_16_35.jpg)
    and a number of degrees of freedom, our `CDF` function produces the same results
    as a widely used table of values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s an entire row from a ![Computing the odds of a distribution being random](graphics/B03652_16_35.jpg)
    table, computed with a simple generator expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The expected values are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: We have some tiny discrepancies in the third decimal place.
  prefs: []
  type: TYPE_NORMAL
- en: What we can do with this is get a probability from a ![Computing the odds of
    a distribution being random](graphics/B03652_16_35.jpg) value. From our example
    shown previously, the 0.05 probability for six degrees of freedom has a ![Computing
    the odds of a distribution being random](graphics/B03652_16_35.jpg) value 12.5916
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The actual value we got for ![Computing the odds of a distribution being random](graphics/B03652_16_35.jpg)
    in the example was 19.18\. Here''s the probability that this value is random:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: This probability is 3/775, with the denominator limited to 1000\. Those are
    not good odds of the data being random.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at three optimization techniques. The first technique
    involves finding the right algorithm and data structure. This has more impact
    on performance than any other single design or programming decision. Using the
    right algorithm can easily reduce runtimes from minutes to fractions of a second.
    Changing a poorly used sequence to a properly used mapping, for example, may reduce
    run time by a factor of 200.
  prefs: []
  type: TYPE_NORMAL
- en: We should generally optimize all of our recursions to be loops. This will be
    faster in Python and it won't be stopped by the call stack limit that Python imposes.
    There are many examples of how recursions are flattened into loops in other chapters,
    primarily, [Chapter 6](ch06.html "Chapter 6. Recursions and Reductions"), *Recursions
    and Reductions*. Additionally, we may be able to improve performance in two other
    ways. First, we can apply memoization to cache results. For numeric calculations,
    this can have a large impact; for collections, the impact may be less. Secondly,
    replacing large materialized data objects with iterables may also improve performance
    by reducing the amount of memory management required.
  prefs: []
  type: TYPE_NORMAL
- en: In the case study presented in this chapter, we looked at the advantage of using
    Python for exploratory data analysis—the initial data acquisition including a
    little bit of parsing and filtering. In some cases, a significant amount of effort
    is required to normalize data from various sources. This is a task at which Python
    excels.
  prefs: []
  type: TYPE_NORMAL
- en: 'The calculation of a ![Summary](graphics/B03652_16_35.jpg) value involved three
    `sum()` functions: two intermediate generator expressions, and a final generator
    expression to create a dictionary with expected values. A final `sum()` function
    created the statistic. In under a dozen expressions, we created a sophisticated
    analysis of data that will help us accept or reject the null hypothesis.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We also evaluated some complex statistical functions: the incomplete and the
    complete `gamma` function. The incomplete `gamma` function involves a potentially
    infinite series; we truncated this and summed the values. The complete `gamma`
    function has some potential complexity, but it doesn''t happen to apply in our
    case.'
  prefs: []
  type: TYPE_NORMAL
- en: Using a functional approach, we can write succinct and expressive programs that
    accomplish a great deal of processing. Python isn't a properly functional programming
    language. For example, we're required to use some imperative programming techniques.
    This limitation forces away from purely functional recursions. We gain some performance
    advantage, since we're forced to optimize tail recursions into explicit loops.
  prefs: []
  type: TYPE_NORMAL
- en: We also saw numerous advantages of adopting Python's hybrid style of functional
    programming. In particular, the use of Python's higher order functions and generator
    expressions give us a number of ways to write high performance programs that are
    often quite clear and simple.
  prefs: []
  type: TYPE_NORMAL
