- en: Saving Data in the Correct Format
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we were focusing on processing and loading data. We
    learned about transformations, actions, joining, shuffling, and other aspects
    of Spark.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn how to save data in the correct format and also
    save data in plain text format using Spark's standard API. We will also leverage
    JSON as a data format, and learn how to use standard APIs to save JSON. Spark
    has a CSV format and we will leverage that format as well. We will then learn
    more advanced schema-based formats, where support is required to import third-party
    dependencies. Following that, we will use Avro with Spark and learn how to use
    and save the data in a columnar format known as Parquet. By the end of this chapter,
    we will have also learned how to retrieve data to validate whether it is stored
    in the proper way.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Saving data in plain text format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leveraging JSON as a data format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tabular formats – CSV
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Avro with Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Columnar formats – Parquet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saving data in plain text format
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will learn how to save data in plain text format. The following
    topics will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Saving data in plain text format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading plain text data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will save our data in plain text format and investigate how to save it into
    the Spark directory. We will then load the plain text data, and then test and
    save it to check whether we can yield the same results code. This is our `SavePlainText.scala`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We will need a `FileName` variable, which, in our case, will be a folder name,
    and Spark will then create a couple of files underneath:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use `BeforeAndAfterEach` in our test case to clean our directory after
    every test, which means that the path should be deleted recursively. The whole
    path is deleted after the test, as it is required to rerun the tests without a
    failure. We need to comment the following code out for the first run to investigate
    the structure of the saved text file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then create an RDD of two transactions, `UserTransaction("a", 100)` and
    `UserTransaction("b", 200)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then `coalesce` our data to one partition. `coalesce()` is a very important
    aspect. If we want to save our data in a single file, we need to `coalesce` it
    into one, but there is an important implication of doing so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'If we `coalesce` it to a single file, then only one executor can save the data
    to our system. This means that saving the data will be very slow and, also, there
    will be a risk of being out of memory because all data will be sent to one executor.
    Generally, in the production environment, we save it as many partitions, based
    on the executors available, or even multiplied by its own factor. So, if we have
    16 executors, then we can save it to `64`. But this results in `64` files. For
    test purposes, we will save it to one file, as shown in the preceding code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we''ll load the data. We only need to pass the filename to the `TextFile`
    method and it will return `fromFile`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We then assert our data, which will yield `theSameElementsAS List`, `UserTransaction(a,100)`,
    and `UserTransaction(b,200)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The important thing to note is that for a list of strings, Spark doesn't know
    the schema of our data because we are saving it in plain text.
  prefs: []
  type: TYPE_NORMAL
- en: This is one of the points to note when it comes to saving plain text, because
    loading the data is not easy, since we need to manually map every string to `UserTransaction`.
    So, we will have to parse every record manually, but, for test purposes, we will
    treat our transaction as strings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s start the test and see the structure of the folder that was created:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8d1da1f2-4d80-4e73-ac6d-ac20b973bf51.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding screenshot, we can see that our test passed and that we get
    `transactions.txt`. Inside the folder, we have four files. The first one is `._SUCCESS.crc`,
    which means that the save succeeded. Next, we have `.part-00000.crc`, to control
    and validate that everything worked properly, which means that the save was proper.
    We then have `_SUCCESS` and `part-00000`, where both files have checksum, but
    `part-00000` has all the data as well. Then, we also have `UserTransaction(a,100)`
    and `UserTransaction(b,200)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a829d808-2760-4b34-bdf4-6e4b5a64fc13.png)'
  prefs: []
  type: TYPE_IMG
- en: In the next section, we will learn what will happen if we increment the number
    of partitions.
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging JSON as a data format
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will leverage JSON as a data format and save our data in
    JSON. The following topics will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Saving data in JSON format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading JSON data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This data is human-readable and gives us more meaning than simple plain text
    because it carries some schema information, such as a field name. We will then
    learn how to save data in JSON format and load our JSON data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will first create a DataFrame of `UserTransaction("a", 100)` and `UserTransaction("b",
    200)`, and use `.toDF()` to save the DataFrame API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then issue `coalesce()` and, this time, we will take the value as `2`,
    and we will have two resulting files. We will then issue the `write.format` method
    and, for the same, we need to specify a format, for which we will use the `json`
    format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'If we use the unsupported format, we will get an exception. Let''s test this
    by entering the source as `not`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We will get exceptions such as ''`This format is not expected`'', ''`Failed
    to find data source: not`'', and ''`There is no such data source`'':'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6c3f7d35-803b-470a-bea8-be3b9f91940a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In our original JSON code, we will specify the format and we need to save it
    to `FileName`. If we want to read, we need to specify it as `read` mode and also
    add a path to the folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'On this occasion, let''s comment out `afterEach()` to investigate the produced
    JSON:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s start the test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code output, we can see that our test passed and that the DataFrame
    includes all the meaningful data.
  prefs: []
  type: TYPE_NORMAL
- en: From the output, we can see that DataFrame has all the schema required. It has
    `amount` and `userId`, which is very useful.
  prefs: []
  type: TYPE_NORMAL
- en: The `transactions.json` folder has two parts—one part is `r-00000`, and the
    other part is `r-00001`, because we issued two partitions. If we save the data
    in a production system with 100 partitions, we will end up with 100 part files
    and, furthermore, every part file will have a CRC checksum file.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the first file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have a JSON file with schema and, hence, we have a `userID` field and
    `amount` field.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, we have the second file with the second record with `userID`
    and `amount` as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The advantage of this is that Spark is able to infer the data from the schema
    and is loaded in a formatted DataFrame with proper naming and types. The disadvantage,
    however, is that every record has some additional overhead. Every record needs
    to have a string in it and, in each string, if we have a file that has millions
    of files and we are not compressing it, there will be substantial overhead, which
    is not ideal.
  prefs: []
  type: TYPE_NORMAL
- en: JSON is human-readable but, on the other hand, it consumes a lot of resources,
    just like the CPU for compressing, reading, and writing, and also the disk and
    memory for the overhead. Apart from JSON, there are better formats that we will
    cover in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will look at the tabular format, where we will cover
    a CSV file that is often used to import to Microsoft Excel or Google spreadsheet.
    This is also a very useful format for data scientists, but only while using smaller
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Tabular formats – CSV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will be covering text data, but in a tabular format—CSV.
    The following topics will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Saving data in CSV format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading CSV data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saving CSV files is even more involved than JSON and plain text because we need
    to specify whether we want to retain headers of our data in our CSV file.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will create a DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will use the `write` format CSV. We also need to specify that we don''t
    want to include the `header` option in it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then perform a test to verify whether the condition is `true` or `false`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Also, we don't need to add any additional dependency to support CSV, as required
    in the previous versions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will then specify the `read` mode, which should be similar to the `write`
    mode, and we need to specify whether we have a `header` or not:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s start the test and check the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code output, we can see that the data is loaded, but we lost
    our schema. `c0` and `c1` are the aliases for column 0 (`c0`) and column 1 (`c1`)
    that were created by Spark.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, if we are specifying that the `header` should retain that information,
    let''s specify the `header` at the `write` and also at the `read`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We will specify that the `header` should retain our information. In the following
    output, we can see that the information regarding the schema was perceived throughout
    the read and write operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see what happens if we `write` with the `header` and `read` without
    it. Our test should fail, as demonstrated in the following code screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0b41f63f-a78b-462a-b058-f27cde819782.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding screenshot, we can see that our test failed because we don't
    have a schema as we were reading without headers. The first record, which was
    a `header`, was treated as the column value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try a different situation, where we are writing without `header` and
    reading with `header`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Our test will fail again because this time, we treated our first record as the
    header record.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s set both the read and write operations with `header` and test our code
    after removing the comment we added previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The CSV and JSON files will have schema, but with less overhead. Therefore,
    it could be even better than JSON.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll see how we can use a schema-based format as a whole
    with Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Using Avro with Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have looked at text-based files. We worked with plain text, JSON,
    and CSV. JSON and CSV are better than plain text because they carry some schema
    information.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we''ll be looking at an advanced schema, known as Avro. The
    following topics will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Saving data in Avro format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading Avro data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avro has a schema and data embedded within it. This is a binary format and is
    not human-readable. We will learn how to save data in Avro format, load it, and
    then test it.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will create our user transaction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then do a `coalesce` and write an Avro:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: While using CSV, we specified the format like CSV, and, when we specified JSON,
    this, too, was a format. But in Avro, we have a method. This method is not a standard
    Spark method; it is from a third-party library. To have Avro support, we need
    to access `build.sbt` and add `spark-avro` support from `com.databricks`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then need to import the proper method. We will import `com.databricks.spark.avro._` to
    give us the implicit function that is extending the Spark DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: We are actually using an Avro method and we can see that `implicit class` takes
    a `DataFrameWriter` class, and writes our data in Spark format.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `coalesce` code we used previously, we can use `write`, specify the
    format, and execute a `com.databricks.spark.avro` class. `avro` is a shortcut
    to not write `com.databricks.spark.avro` as a whole string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: In short, there is no need to specify the format; just apply the implicit `avro` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s comment out the code and remove Avro to check how it saves:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: If we open the `transactions.avro` folder, we have two parts—`part-r-00000`
    and `part-r-00001`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first part will have binary data. It consists of a number of binary records
    and some human-readable data, which is our schema:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ea76f872-a5d0-4c5d-a5eb-440c5d453832.png)'
  prefs: []
  type: TYPE_IMG
- en: We have two fields—`user ID`, which is a type string or null, and `name`: `amount`,
    which is an integer. Being a primitive type, JVM cannot have null values. The
    important thing to note is that, in production systems, we have to save really
    large datasets, and there will be thousands of records. The schema is always in
    the first line of every file. If we check the second part as well, we will see
    that there is exactly the same schema and then the binary data.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, we have only one or more lines if you have a complex schema, but still,
    it is a very low amount of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that in the resulting dataset, we have `userID` and `amount`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code block, we can see that the schema was portrayed in the
    file. Although it is a binary file, we can extract it.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will be looking at the columnar format—Parquet.
  prefs: []
  type: TYPE_NORMAL
- en: Columnar formats – Parquet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we''ll be looking at the second schema-based format, Parquet.
    The following topics will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Saving data in Parquet format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading Parquet data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is a columnar format, as the data is stored column-wise and not row-wise,
    as we saw in the JSON, CSV, plain text, and Avro files.
  prefs: []
  type: TYPE_NORMAL
- en: This is a very interesting and important format for big data processing and
    for making the process faster. In this section, we will focus on adding Parquet
    support to Spark, saving the data into the filesystem, reloading it again, and
    then testing. Parquet is similar to Avro as it gives you a `parquet` method but
    this time, it is a slightly different implementation.
  prefs: []
  type: TYPE_NORMAL
- en: In the `build.sbt` file, for the Avro format, we need to add an external dependency,
    but for Parquet, we already have that dependency within Spark. So, Parquet is
    the way to go for Spark because it is inside the standard package.
  prefs: []
  type: TYPE_NORMAL
- en: Let's have a look at the logic that's used in the `SaveParquet.scala` file for
    saving and loading Parquet files.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we coalesce the two partitions, specify the format, and then specify
    that we want to save `parquet`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The `read` method also implements exactly the same method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s begin this test but, before that, we will comment out the following
    code withing our `SaveParquet.scala` file to see the structure of the files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: A new `transactions.parquet` folder gets created and we have two parts inside
    it—`part-r-00000` and `part-r-00001`. This time, however, the format is entirely
    binary and there is some metadata embedded with it.
  prefs: []
  type: TYPE_NORMAL
- en: We have the metadata embedded and also the `amount` and `userID` fields, which
    are of the `string` type. The part `r-00000` is exactly the same and has the schema
    embedded. Hence, Parquet is also a schema-based format. When we read the data,
    we can see that we have the `userID` and `amount` columns available.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to save data in plain text format. We noticed
    that schema information is lost when we do not load the data properly. We then
    learned how to leverage JSON as a data format and saw that JSON retains the schema,
    but it has a lot of overhead because the schema is for every record. We then learned
    about CSV and saw that Spark has embedded support for it. The disadvantage of
    this approach, however, is that the schema is not about the specific types of
    records, and tabs need to be inferred implicitly. Toward the end of this chapter,
    we covered Avro and Parquet, which have columnar formats that are also embedded
    with Spark.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll be working with Spark's key/value API.
  prefs: []
  type: TYPE_NORMAL
