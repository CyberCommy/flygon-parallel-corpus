- en: '*Chapter 9*: High-Performance C++'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we switch focus from the optimal use of the hardware resources
    to the optimal application of a particular programming language. While everything
    we have learned so far can be applied, usually quite straightforwardly, to any
    program in any language, this chapter deals with C++ features and idiosyncrasies.
    You will learn which features of the C++ language are likely to cause performance
    problems and how to avoid them.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Efficiency and overhead of the C++ language
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning to notice likely inefficiencies in the use of C++ language constructs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoiding inefficient C++ code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing memory access and conditional operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Again, you will need a C++ compiler and a micro-benchmarking tool, such as the
    **Google Benchmark** library we used in the previous chapter (found at [https://github.com/google/benchmark](https://github.com/google/benchmark)).
  prefs: []
  type: TYPE_NORMAL
- en: The code accompanying this chapter can be found at [https://github.com/PacktPublishing/The-Art-of-Writing-Efficient-Programs/tree/master/Chapter09](https://github.com/PacktPublishing/The-Art-of-Writing-Efficient-Programs/tree/master/Chapter09).
  prefs: []
  type: TYPE_NORMAL
- en: 'You will also need a way to examine the assembly code generated by the compiler:
    many development environments have an option to display assembly; GCC and Clang
    can write out the assembly instead of the object code; debuggers and other tools
    can generate assembly from the object code (disassemble it). It''s a matter of
    personal preference which tool you use.'
  prefs: []
  type: TYPE_NORMAL
- en: What is the efficiency of a programming language?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Programmers often talk about a language being efficient or otherwise. C++, in
    particular, has been developed with the explicit goal of efficiency and, at the
    same time, has a reputation in some circles of being inefficient. How can this
    be?
  prefs: []
  type: TYPE_NORMAL
- en: '*Efficiency* can mean different things in different contexts or to different
    people. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'C++ design follows the principle of **zero overhead**: with a handful of exceptions,
    you don’t pay any runtime cost for any feature you do not use just because it
    is present in the language. In this sense, it is as efficient as a language can
    be.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You obviously have to pay something for the language features you do use, at
    least if they translate into some runtime work. C++ is very good about not requiring
    any runtime code for doing work that can be done during compilation (although
    the implementations of the compilers and the standard libraries vary in their
    efficiency). An efficient language does not add any overhead to the code that
    must be generated to carry out the requested work, and again, C++ is quite good
    here, with one major caveat we will discuss next.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If the preceding is true, how did C++ earn the label of *inefficient* from
    those who hold this opinion? Now we come to yet another perspective on efficiency:
    how easy is it to write efficient code in this language? Or, how easy is it to
    do something that seems natural but, in fact, is a very inefficient way of solving
    the problem? A closely related problem is the one we alluded to in the last paragraph:
    C++ is very efficient in doing exactly what you asked it to do. But it is not
    always easy to express exactly what you want in the language, and, again, the
    natural way of writing the code sometimes imposes additional requirements or constraints
    that the programmer did not want and may not be aware of. These constraints have
    runtime costs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'From the point of view of the language designer, the last problem is not a
    language inefficiency: you asked the machine to do X and Y, it costs time to do
    X and Y, we’re not doing anything beyond what you asked us to do. But from the
    point of view of the programmer, this is an inefficient language if the programmer
    only wanted to do X but didn’t care about Y.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is the goal of this chapter to help you write code that clearly expresses
    what you want the machine to do. The purpose is two-fold: you may think that your
    primary *audience* is the compiler: by precisely describing what you want and
    what the compiler is free to change, you give the compiler the freedom to generate
    more efficient code. But the same can be said about the readers of your program:
    they can only infer what you expressed in the code, not what you intended to express.
    Is it safe to optimize your code if it changes certain aspects of its behavior?
    Was this behavior intentional or an accident of the implementation that can be
    altered? Once again, we are reminded that programming is primarily a way of communicating
    with our peers, and only then with the machines.'
  prefs: []
  type: TYPE_NORMAL
- en: We will start with the simpler inefficiencies that seem easy to avoid, but they
    crop up even in the code of programmers who have mastered other aspects of the
    language.
  prefs: []
  type: TYPE_NORMAL
- en: Unnecessary copying
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Unnecessary copying of objects is probably *C++ inefficiency #1*. The main
    reason is that it’s easy to do and hard to notice. Consider the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: How many copies of the vector `v` are made in this program? The answer depends
    on the details of the functions `make_v()` and `do_work()` as well as the compiler
    optimizations. This tiny example covers several language subtleties that we will
    now discuss.
  prefs: []
  type: TYPE_NORMAL
- en: Copying and argument passing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We are going to start with the second function, `do_work()`. What matters here
    is the declaration: if the function takes the argument by reference, `const` or
    not, then no copies are made.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'If the function uses pass-by-value, then a copy must be made:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Copying a vector is an expensive operation if the vector is large: all the
    data in the vector must be copied. This is one expensive function call. If the
    work itself does not require a copy of the vector, then it’s also extremely inefficient.
    For example, if all we need is to compute the sum (or some other function) of
    all the elements in the vector, we do not need a copy. While it may seem undesirable,
    at first glance, that the call itself does not tell us whether the copy is made,
    it is how it should be. The decision to make the copy belongs to the implementer
    of the function and can be made only after considering the requirements and the
    choice of the algorithm. For the previously mentioned problem of accumulating
    the sum of all elements, the correct decision is clearly to pass the vector by
    (`const`) reference as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Using pass-by-value, in this case, is such a blatant inefficiency that it may
    be considered a bug, but it happens more often than you’d think. In particular,
    it happens in template code where the author considered only small, lightweight
    data types, but the code ends up being used wider than expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, if we need to create a copy of the arguments as a part of
    fulfilling the requirements on the function, using parameter passing is as good
    of a way as any:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Here we need to apply a so-called clamping loop to the data before processing
    it further. Assuming we read the clamped values many times, calling `std::min()`
    for every access may be less efficient than creating a cached copy of the result.
    We could make an explicit copy as well, and it may be slightly more efficient,
    but this kind of optimization should not be left to speculation; it can be definitively
    answered only by a benchmark.
  prefs: []
  type: TYPE_NORMAL
- en: 'C++11 introduced **move semantics** as a partial answer to unnecessary copying.
    In our case, we observe that if the function argument is an r-value, we can use
    it any way we want, including altering it (the caller has no way of accessing
    the object after the call completes). The usual way to take advantage of the move
    semantics is to overload the function with an r-value reference version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'However, if the object itself is move-enabled, our simple pass-by-value version
    shines in the new light. Refer to the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The first call to `do_work()` uses an l-value argument, so a local copy is made
    inside the function (the argument is passed by value!). The second call uses an
    r-value or an unnamed temporary. Since the vector has a move constructor, the
    function argument is moved (not copied!) into its parameter, and moving vectors
    is very fast. Now with a single implementation of the function, without any overloads,
    we can handle both the r-value and the l-value arguments efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Now we have seen the two extreme examples. In the first case, a copy of the
    argument was not needed, and creating one was pure inefficiency. In the second
    case, making a copy was a reasonable implementation. Not every situation falls
    into one of these extremes, as we are about to see.
  prefs: []
  type: TYPE_NORMAL
- en: Copying as an implementation technique
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There is also the middle ground where the chosen implementation needs a copy
    of the argument, but the implementation itself is not optimal. As an example,
    consider the following function that needs to print the vector in the sorted order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: For a vector of integers, this is probably the best way. We sort the container
    itself and print it in order. Since we are not supposed to modify the original
    container, we need a copy, and, again, there is nothing wrong with exploiting
    the compiler to make one.
  prefs: []
  type: TYPE_NORMAL
- en: 'But what if the elements of the vector were not integers but some large objects?
    In this case, copying the vector takes a lot of memory, and sorting it takes a
    lot of time spent copying large objects. In this case, a better implementation
    may be to create and sort a vector of pointers without moving the original objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we have learned by now to never guess about performance, the intuition
    needs to be confirmed by a benchmark. Since sorting an already sorted vector does
    not require any copying, we want a fresh, unsorted vector for every iteration
    of the benchmark, like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Of course, we should disable the actual printing since we are not interested
    in benchmarking the I/O. On the other hand, we should benchmark copying the vector
    without sorting, just so we know what portion of the measured time is spent in
    setting up the test.
  prefs: []
  type: TYPE_NORMAL
- en: 'The benchmark confirms that, for integers, it is faster to copy the entire
    vector and sort a copy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – Benchmark of sorting a vector of integers, copying versus pointer
    indirection'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.1_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.1 – Benchmark of sorting a vector of integers, copying versus pointer
    indirection
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that if the vector is small and all the data fits in the low-level cache,
    the processing is very fast either way, and there is little speed difference.
    If the objects are large and expensive to copy, the indirection becomes relatively
    more efficient:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – Benchmark of sorting a vector of large objects, copying versus
    pointer indirection'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.2_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.2 – Benchmark of sorting a vector of large objects, copying versus
    pointer indirection
  prefs: []
  type: TYPE_NORMAL
- en: There is yet another special case when copying objects is necessary for the
    implementation; we will consider it next.
  prefs: []
  type: TYPE_NORMAL
- en: Copying to store data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In C++, we can encounter another particular case of data copying. It happens
    most often in class constructors where the object must store a copy of the data,
    so a long-term copy with a lifetime exceeding that of the constructor call must
    be created. Consider this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Here the intent is to make a copy. The inefficiency would be to make multiple
    intermediate copies or make an unnecessary copy. The standard way to accomplish
    this is to take the object by `const` reference and make a copy inside the class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'If the argument of the constructor is an l-value, this is as efficient as it
    can be. However, if the argument is an r-value (a temporary), we would prefer
    to move it into the class and make no copies at all. This requires an overload
    for the constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The downside is the need to code two constructors, but it gets worse if the
    constructor takes several arguments, and each one needs to be copied or moved.
    Following this pattern, we would need 6 constructor overloads to handle 3 parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The alternative is to pass all parameters by value and *move* from the parameter,
    check the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: It is very important to remember that the parameter `v` is now an x-value (an
    object in a moved-from state), and it should not be used in the body of the constructor.
    If the argument is an l-value, a copy is made to construct the parameter `v`,
    then moved into the class. If the argument is an r-value, it is moved into the
    parameter `v` and again moved into the class. This pattern works great if the
    objects are cheap to move. However, if the objects are expensive to move or have
    no move constructors at all (so they are copied instead), we end up doing two
    copies instead of one.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have focused on the problem of getting data into functions and objects.
    But copying can also occur when we need to return the results. The considerations
    there are completely different and need to be examined separately.
  prefs: []
  type: TYPE_NORMAL
- en: Copying of return values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our example at the very beginning of this section included both kinds of copying.
    In particular, this line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'It implies that the resulting vector `v` is created from another vector, the
    one returned by the function `make_v`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'In theory, more than one copy can be made here: the local variable `vtmp` is
    copied into the (unnamed) return value of the function `make_v`, which is, in
    turn, copied into the final result `v`. In practice, this is not going to happen.
    First of all, the unnamed temporary return value of `make_v` is moved, not copied,
    into `v`. But, most likely, even this is not going to happen. If you try this
    code with your own class instead of `std::vector`, you will see that neither a
    copy nor a move constructor is used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This program prints something like the following (on most compilers, a certain
    level of optimization must be turned on):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3 – The output of the program returning an object by value'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.3_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.3 – The output of the program returning an object by value
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, only one object was constructed and destroyed. This is the
    result of the compiler optimization. The specific optimization that is used here
    is known as `ctmp`, the unnamed temporary return value, and the final result `c`
    – are all of the same type. Furthermore, it is impossible for any code we write
    to observe any two of these variables at the same time. Therefore, without changing
    any observable behavior, the compiler can use the same memory location for all
    three variables. Before calling the function, the compiler needs to allocate the
    memory where the eventual result `c` will be constructed. The address of this
    memory is passed into the function by the compiler, where it is used to construct
    the local variable `ctmp` at the same location. As a result, when the function
    `makeC` ends, there is nothing to return at all: the result is already where it
    should be. This is the RVO in a nutshell.'
  prefs: []
  type: TYPE_NORMAL
- en: While RVO seems simple, it has several subtleties.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, remember that this is an optimization. This means that the compiler
    usually does not have to do it (if yours doesn’t, you need a better compiler).
    However, it is a very special kind of optimization. In general, the compiler can
    do whatever it wants to your program as long as it does not change the observable
    behavior. The observable behavior includes input and output and accessing volatile
    memory. This optimization, however, has resulted in an observable change: the
    expected output of the copy constructor and the matching destructor is nowhere
    to be seen. Indeed, this is one exception from the otherwise ironclad rule: *the
    compiler is allowed to eliminate calls to copy or move constructors and the corresponding
    destructors even if these functions have side effects that include observable
    behavior*. This exception is not limited to RVO. The implication is that, in general,
    you cannot count on copy and move constructors to be called just because you wrote
    some code that appears to do a copy. This is known as **copy elision** (or **move
    elision**, for move constructors).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, remember (again) that this is an optimization. The code must compile
    before it can be optimized. If your object does not have any copy or move constructors,
    this code will not compile, and we will never get to the optimization step that
    is going to remove all calls to these constructors. This is easy to see if we
    delete all copy and move constructors in our example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The compilation will now fail. The exact error message depends on the compiler
    and the C++ standard level; in C++17, it is going to look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4 – Compilation output of Clang using C++17 or C++20'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.4_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.4 – Compilation output of Clang using C++17 or C++20
  prefs: []
  type: TYPE_NORMAL
- en: 'There is one special case where our program would compile even with deleted
    copy and move operations. Let us make a slight change to the `makeC` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Nothing changes in C++11 or C++14; however, in C++17 and above, this code compiles
    fine. Note the slight difference from the previous version: the returned object
    used to be an l-value, it had a name. Now it’s an r-value, an unnamed temporary.
    This makes all the difference: while the **named RVO** (**NRVO**) is still an
    optimization, the unnamed RVO is mandatory since C++17 and is no longer considered
    to be a copy elision. Instead, the standard says that no copy or move is requested
    in the first place.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, you may wonder if the function must be inlined in order for the compiler
    to know where the return value is while it compiles the function itself. With
    a simple test, you can convince yourself that this is not so: even if the function
    `makeC` is in a separate compilation unit, RVO still takes place. The compiler,
    therefore, must send the address of the result to the function at the call point.
    You can do something similar yourself if you do not return the result from the
    function at all but instead pass the reference to the result as an additional
    argument. Of course, that object has to be constructed first, while the compiler-generated
    optimization does not need an extra constructor call.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You may find a recommendation to not rely on RVO but to enforce the move of
    the return value instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The argument goes that if RVO does not happen, your program will take the performance
    penalty of the copying operation, while the move operation is cheap anyway. However,
    this argument is wrong. To understand why, look carefully at the error message
    in *Figure 9.4*: the compiler complains that the move constructor is deleted even
    though `ctmp` is an l-value and should be copied. This is not a compiler bug but
    reflects the behavior required by the standard: in the context where the return-value
    optimization is possible, but the compiler decides not to do it, the compiler
    must first try to find a `move` constructor to return the result. If the `move`
    constructor is not found, the second lookup is performed; this time, the compiler
    is looking for a copy constructor. In both cases, the compiler is really performing
    overload resolution since there can be many copy or `move` constructors. Thus,
    there is no reason to write an explicit move: the compiler will do one for us.
    But what is the harm, then? The harm is that using the explicit move disables
    RVO; you have asked for a move, so you are going to get one. While a move may
    require very little work, RVO is no work at all, and no work is always faster
    than some work.'
  prefs: []
  type: TYPE_NORMAL
- en: 'What happens if we delete the `move` constructor but not the copy constructor?
    The compilation still fails in the case where it was failing with both constructors
    deleted. This is, again, a subtle point of the language: declaring a deleted member
    function is not the same as not declaring any. If the compiler performs the overload
    resolution for a `move` constructor, it will find one, even if this constructor
    is deleted. The compilation fails because the overload resolution selected a deleted
    function as the best (or the only) overload. If you want to force the use of the
    copy constructor (in the name of science, of course), you have to not declare
    any `move` constructors at all.'
  prefs: []
  type: TYPE_NORMAL
- en: 'By now, you must see the danger of accidentally copying an object and ruining
    your program’s performance hiding behind every dark corner of your code. What
    can you do to avoid unintentional copying? We will have some suggestions in a
    moment, but first, let us return to one approach that we already used briefly:
    the use of pointers.'
  prefs: []
  type: TYPE_NORMAL
- en: Using pointers to avoid copying
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One way to avoid copying objects when passing them around is to pass pointers
    instead. This is easiest if we don’t have to manage the object’s lifetime. If
    a function needs access to an object but does not need to delete it, passing the
    object by reference or by a raw pointer is the best way (the reference, in this
    context, is really just a pointer that cannot be null).
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, we can return an object from a function using a pointer, but this
    needs more care. First of all, the object must be allocated on the heap. You must
    never return pointers or references to local variables. Refer to the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Second, the caller is now responsible for deleting the object, so every caller
    of your function must know how the object was constructed (operator `new` is not
    the only way to construct objects, just the most common one). The best solution
    here is to return a smart pointer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that such a factory function should return unique pointers even if the
    caller may use shared pointers to manage the object’s lifetime: it is easy and
    cheap to move from the unique pointer to the shared one.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Speaking of shared pointers, they are often used to pass around objects whose
    lifetime is managed by smart pointers. Unless the intent is to pass the ownership
    of the object as well, this is again an example of unnecessary and inefficient
    copying. Copying shared pointers is not cheap. So, what do we do if we have an
    object managed by a shared pointer and a function that needs to operate on this
    object without taking ownership of it? We use raw pointers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The declarations of the functions `do_work1()` and `do_work2()` tell us about
    the programmer’s intent: both functions operate the object without deleting it.
    The first function modifies the object; the second does not. Both functions expect
    to be called without the object and will handle this special case (otherwise,
    the arguments would be passed by reference).'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, you can create containers of raw pointers as long as the lifetime
    of the objects is managed elsewhere. If you want the container to manage the lifetime
    of its elements but do not want to store the objects in the container, a container
    of unique pointers will do the job.
  prefs: []
  type: TYPE_NORMAL
- en: Now it is time for some general guidelines that will help you avoid unnecessary
    copying and the inefficiencies it can cause.
  prefs: []
  type: TYPE_NORMAL
- en: How to avoid unnecessary copying
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Perhaps the most important thing you can do to reduce accidental, unintentional
    copying is to ensure that all your data types are movable if moving can be implemented
    cheaper than copying. If you have container libraries or other reusable code,
    make sure it is move-enabled as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next suggestion is somewhat hamfisted, but it can save you a lot of debugging
    time: if you have types that are expensive to copy, make them non-copyable to
    begin with. Declare the copy and assignment operations as deleted. If the classes
    support a fast move, provide move operations instead. This will, of course, prevent
    any copying, intentional or not. Hopefully, intentional copying is rare, and you
    can implement a special member function like `clone()` that will create a copy
    of the object. At least this way, all the copying is explicit and visible in your
    code. If the class is neither copyable nor movable, you will not be able to use
    it with STL containers; however, a container of unique pointers is a fine alternative.'
  prefs: []
  type: TYPE_NORMAL
- en: When passing parameters to functions, use references or pointers whenever possible.
    If the function needs to make a copy of the argument, consider passing by value
    and moving from the parameter instead. Remember that this works well only for
    move-enabled types, and see the first guideline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Everything we said about passing function arguments can be applied to temporary
    local variables as well (after all, function parameters are basically temporary
    local variables in the function scope). These should be references unless you
    need a copy. This does not apply to the built-in types like integers or pointers:
    they are cheaper to copy than to access indirectly. In template code, you don’t
    have the luxury of knowing whether the type is large or small, so use references
    and rely on compiler optimizations to avoid unnecessary indirect access to built-in
    types.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When returning values from functions, your first preference should be to rely
    on RVO and copy elision. Only when you find that the compiler does not perform
    this optimization and that it matters in your particular case should you consider
    alternatives. These alternatives are: using functions with output arguments and
    using factory functions that construct the results in dynamically allocated memory
    and return owning smart pointers such as `std::unique_ptr`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, review your algorithms and the implementation with an eye out for
    unnecessary copying: remember that ill-intentioned copying is just as bad for
    the performance as unintentional copying.'
  prefs: []
  type: TYPE_NORMAL
- en: We are done with the first bane of efficiency in C++ programs, the gratuitous
    copying of objects. The close second is poor memory management.
  prefs: []
  type: TYPE_NORMAL
- en: Inefficient memory management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The subject of memory management in C++ can merit a book all of its own. There
    are dozens if not hundreds of papers dedicated just to the issue of the STL allocators.
    In this chapter, we will focus on several problems that tend to affect performance
    the most. Some have simple solutions; for others, we will describe the issue and
    outline the possible solution approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two types of memory-related problems that you may run into in the
    context of performance. The first one is using too much memory: your program either
    runs out of memory or doesn’t meet the memory use requirements. The second problem
    occurs when your program becomes memory-bound: its performance is limited by the
    speed of memory access. Often, in these cases, the runtime of the program is directly
    related to how much memory it uses, and reducing the memory use also makes the
    program run faster.'
  prefs: []
  type: TYPE_NORMAL
- en: The material presented in this section is helpful mostly for programmers who
    deal with memory-bound programs or programs that allocate memory frequently and/or
    in large quantities. We begin with the performance impact of the memory allocations
    themselves.
  prefs: []
  type: TYPE_NORMAL
- en: Unnecessary memory allocations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the most common performance problems related to memory use is that of
    unnecessary memory allocation. Here is a very common problem, described in C++-like
    pseudocode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: A well-written program would use an RAII class to manage deallocations, but
    we wanted to make allocations and deallocations explicit for clarity. The allocations
    are usually concealed inside objects that manage their own memory, such as STL
    containers. It is not uncommon for such a program to spend most of its time in
    memory allocation and deallocation functions (such as `malloc()` and `free()`).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see the effect on performance on a very simple benchmark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Here the *work* is represented by initializing a character string, and the
    `random_number()` function returns random integer values (it could be just `rand()`,
    but the benchmark is *cleaner* if we precompute and store random numbers to avoid
    benchmarking the random number generator). You may also need to trick the compiler
    into not optimizing away the results: if the usual `benchmark::DoNotOptimize()`
    does not suffice, you may have to insert a print statement with the condition
    that never happens (but the compiler does not know it) like `rand() < 0`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The numbers we get from the benchmark are meaningless by themselves: we need
    to compare them with something. In our case, the baseline is easy to figure out:
    we must do the same work but none of the allocations. This can be accomplished
    by moving the allocation and the deallocation out of the loop since we know the
    maximum memory size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The performance difference you will observe in such a benchmark depends greatly
    on the operating system and the system libraries, but you’re likely to see something
    like this (we used strings of random sizes up to 1 KB):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5 – Performance impact of the allocation-deallocation pattern'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.5_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.5 – Performance impact of the allocation-deallocation pattern
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that the memory allocations in a micro-benchmark are typically
    more efficient than in the context of a large program where the memory allocation
    pattern is much more complex, so the real-world effect of frequent allocations
    and deallocations is likely to be even larger. Even in our small benchmark, the
    implementation that allocates memory every time runs at 40% of the speed of the
    version that allocates the maximum possible amount of memory just once.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, when the maximum amount of the memory we need during the computation
    is known in advance, preallocating it and reusing it from one iteration to the
    next is an easy solution. This solution generalizes to many containers as well:
    for a vector or deque, we can reserve the memory before the start of the iterations
    and take advantage of the fact that resizing the container does not shrink its
    capacity.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution is only slightly more complex when we do not know the maximum
    memory size in advance. This situation can be handled with a grow-only buffer.
    Here is a simple buffer that can be grown but never shrinks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, this code is useful for demonstration and exploration. In a real program,
    you are likely to use STL containers or your own library classes, but they all
    should have the capability to increase the memory capacity. We can compare the
    performance of this grow-only buffer with that of the fixed-size preallocated
    buffer by trivially modifying our benchmark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, in a real program, you are likely to get better results with a smarter
    memory growth strategy (grow by somewhat more than requested, so you don’t have
    to grow memory as often – most STL containers employ some form of this strategy).
    But, for our demonstration, we want to keep things as simple as possible. On the
    same machine, the results of the benchmark are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.6 – Performance of a grow-only buffer (compare with Figure 9.5)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.6_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.6 – Performance of a grow-only buffer (compare with Figure 9.5)
  prefs: []
  type: TYPE_NORMAL
- en: The grow-only buffer is slower than a fixed-size buffer but much faster than
    allocating and deallocating memory every time. Again, a better growth policy would
    make this buffer even faster, close to the speed of the fixed-size buffer.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is not the entire story: the importance of good memory management is even
    greater in multi-threaded programs because the calls to the system memory allocator
    do not scale well and may involve a global lock. Running our benchmark on the
    same machine using 8 threads produces these results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.7 – Performance impact of the allocation-deallocation pattern in
    a multi-threaded program'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.7_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.7 – Performance impact of the allocation-deallocation pattern in a
    multi-threaded program
  prefs: []
  type: TYPE_NORMAL
- en: Here, the penalty for frequent allocations is even greater (the grow-only buffer
    shows the cost of the remaining allocations as well and would really benefit from
    a smarter growth policy).
  prefs: []
  type: TYPE_NORMAL
- en: 'The bottom line is: interact with the OS as little as possible. If you have
    a loop that needs to allocate and deallocate memory on each iteration, allocate
    once before the loop instead. If the allocations are of the same size, or you
    know the maximum allocation size upfront, make one allocation of this size and
    hold it (of course, if you use several buffers or containers, you should not try
    to shoehorn them into a single allocation, but preallocate each one). If you do
    not know the maximum size, use a data structure that can grow, but do not shrink
    or release the memory until the work is done.'
  prefs: []
  type: TYPE_NORMAL
- en: The recommendation to avoid interacting with the OS is particularly important
    in multi-threaded programs, and we will now make some more general comments on
    the use of memory in concurrent programs.
  prefs: []
  type: TYPE_NORMAL
- en: Memory management in concurrent programs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The memory allocator provided by the operating system is a solution that balances
    many requirements: on a given machine, there is only one OS but many different
    programs with their own unique needs and memory use patterns. The developers tried
    very hard to make it not fail miserably in any reasonable use case; the flip side
    is that it’s rarely the best possible solution for any use case, too. Often, it’s
    good enough, especially if you follow the recommendation of requesting memory
    frequently.'
  prefs: []
  type: TYPE_NORMAL
- en: Memory allocation becomes more inefficient in concurrent programs. The primary
    reason is that any memory allocator has to maintain a fairly complex internal
    data structure to track allocated and free memory. In high-performance allocators,
    the memory is subdivided into multiple arenas to group allocations of similar
    size together. This increases performance at the cost of complexity. The result
    is that this management of the internal data has to be protected by a lock if
    multiple threads are allocating and deallocating memory at once. This is a global
    lock, one for the entire program, and it can limit the scaling of the entire program
    if the allocator is called often.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most common solution to this problem is to use an allocator with thread-local
    caches, such as the popular `malloc()` replacement library TCMalloc. These allocators
    reserve some amount of memory for each thread: when a thread needs to allocate
    memory, it is taken from the thread-local memory arena first. This does not need
    a lock since only one thread interacts with that arena. Only if the arena is empty
    does the allocator have to take the lock and allocate from the memory shared between
    all threads. Similarly, when a thread deallocates the memory, it is added to the
    thread-specific arena, again without any locking.'
  prefs: []
  type: TYPE_NORMAL
- en: The thread-local caches are not without their share of problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, they tend to use more memory overall: if one thread frees a lot
    of memory and another thread allocates a lot of memory, the recently freed memory
    does not become available to the other thread (it’s local to the thread that released
    it). So more memory is allocated while unused memory is available for other threads.
    To limit this memory waste, the allocators typically do not allow the per-thread
    arena to grow above some predefined limit. Once the limit is reached, the thread-local
    memory is returned to the main arena shared between all threads (this operation
    requires a lock).'
  prefs: []
  type: TYPE_NORMAL
- en: The second problem is that these allocators work well if each allocation is
    owned by one thread, that is, the same thread allocates and deallocates memory
    at each address. If one thread allocates some memory, but another thread has to
    deallocate it, this *cross-thread* deallocation is difficult because the memory
    must be transferred from the thread-local arena of one thread to that of the other
    (or to the shared arena). A simple benchmark shows that the performance of cross-thread
    deallocation with the standard allocators like `malloc()` or TCMalloc is at least
    an order of magnitude worse than that of thread-owned memory. This is likely to
    be true with any allocator that utilizes thread-local caches, and so memory transfers
    between threads should be avoided whenever possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we were talking about transferring memory from one thread to another
    for the purpose of deallocating it. What about simply using memory that was allocated
    by another thread? The performance of such memory access depends greatly on the
    hardware capabilities. For a simple system with few CPUs, this is likely a non-issue.
    But larger systems have multiple memory banks, and the connection between the
    CPU and the memory is not symmetric: each memory bank is closer to one CPU. This
    is known as the **non-uniform memory architecture** (**NUMA**). The performance
    impact of NUMA varies widely from *doesn’t matter* to *twice as fast*. There are
    ways to tune the performance of the NUMA memory system as well as making the program
    memory management sensitive to NUMA details, but beware that you are likely tuning
    the performance to a particular machine: there is very little that can be said
    about the performance of NUMA systems in general.'
  prefs: []
  type: TYPE_NORMAL
- en: We are now returning to the issue of using memory more efficiently since it
    is universally helpful for performance in concurrent and serial programs alike.
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding memory fragmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One issue that plagues many programs is an inefficient interaction with the
    memory allocation system. Let us say that the program needs to allocate 1 KB of
    memory. This chunk of memory is carved out from some larger memory arena, marked
    as used by the allocator, and the address is returned to the caller. More memory
    allocations follow, so the memory after our 1 KB chunk is now used too. Then the
    program returns the first allocation and immediately asks for 2 KB of memory.
    There is a 1 KB free chunk, but it’s not large enough to service this new request.
    There may be another 1 KB chunk somewhere else, but as long as the two chunks
    are not right next to each other, they are not useful for the purpose of the 2
    KB allocation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.8 – Memory fragmentation: 2 KB of free memory exists but is not
    useful for a single 2 KB allocation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.8_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.8 – Memory fragmentation: 2 KB of free memory exists but is not useful
    for a single 2 KB allocation'
  prefs: []
  type: TYPE_NORMAL
- en: This situation is known as `malloc()`, but for programs that churn through memory
    quickly, more extreme measures may be required.
  prefs: []
  type: TYPE_NORMAL
- en: 'One such measure is a block allocator. The idea is that all memory is allocated
    in blocks of fixed size, such as 64 KB. You should not allocate single blocks
    of this size from the OS one at a time but instead allocate larger chunks of fixed
    size (say, 8 MB) and subdivide them into the smaller blocks (64 KB in our example).
    The memory allocator that handles these requests is the primary allocator in your
    program, the one that interacts directly with `malloc()`. Because it allocates
    blocks of just one size, it can be very simple, and we can focus on the most efficient
    implementation (thread-local cache for concurrent programs, low latency for real-time
    systems, and so on). Of course, you do not want to deal with these 64 KB blocks
    everywhere in your code. That is the job of secondary allocators, as shown in
    the following *Figure 9.9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.9 – Fixed-size block allocation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.9_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.9 – Fixed-size block allocation
  prefs: []
  type: TYPE_NORMAL
- en: 'You can have an allocator that further subdivides the 64 KB blocks into smaller
    allocations. Particularly efficient is a uniform allocator (an allocator for just
    one size): for example, if you want to allocate memory for single 64-bit integers,
    you can do so without any memory overhead (by comparison, `malloc()` usually requires
    at least 16 bytes of overhead per allocation). You can also have containers that
    allocate memory in 64 KB blocks and use it to store the elements. You will not
    be using vectors since they require a single large, contiguous allocation. The
    array-like container you want here is the deque that allocates memory in fixed-size
    blocks. You can, of course, have nodal containers as well. You can use the STL
    containers if the STL allocator interface is sufficient for your needs; otherwise,
    you may have to write your own container library.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The key advantage of the fixed-size block allocation is that it does not suffer
    from fragmentation: all allocations from `malloc()` are of the same size, so are
    all allocations from the primary allocator. Any time a memory block is returned
    to the allocator, it can be reused to satisfy the next request for memory. Refer
    to the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.10 – Memory reuse in fixed-size allocators'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.10_B16229.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.10 – Memory reuse in fixed-size allocators
  prefs: []
  type: TYPE_NORMAL
- en: 'That first-in-first-out property is also an advantage: the last 64 KB memory
    block is likely to be from the most recently used memory and is still hot in the
    cache. Reusing this block immediately improves memory reference locality and,
    therefore, makes more efficient use of the cache. The allocator manages the blocks
    returned to it as a simple free list (*Figure 9.10*). These free lists can be
    maintained per thread to avoid locking, although they will likely need periodic
    rebalancing to avoid the situation where one thread has accumulated many free
    blocks while another thread is allocating new memory.'
  prefs: []
  type: TYPE_NORMAL
- en: Of course, the allocators that subdivide our 64 KB blocks into smaller sizes
    are still susceptible to fragmentation unless they are also uniform (fixed-size)
    allocators. It is, however, easier to write a self-defragmenting allocator if
    it has to deal with a small memory range (one block) and few different sizes.
  prefs: []
  type: TYPE_NORMAL
- en: It is likely that the entire program is affected by the decision to use block
    memory allocation. For example, allocating large numbers of small data structures
    such that each one uses a fraction of the 64 KB block and leaves the rest unused
    becomes prohibitively expensive. On the other hand, a data structure that itself
    is a collection of smaller data structures (a container) such that it packs many
    smaller objects into one block becomes easier to write. One can even write compressed
    containers that compress each block for keeping the data long-term, then decompress
    them one block at a time for access.
  prefs: []
  type: TYPE_NORMAL
- en: The block size itself is not set in stone, either. Some applications will be
    more efficient with smaller blocks where less memory is wasted if a block is left
    partially unused. Others can benefit from larger blocks that require fewer allocations.
  prefs: []
  type: TYPE_NORMAL
- en: The literature on application-specific allocators is extensive. For example,
    slab allocators are a generalization of the block allocators we have just seen;
    they manage multiple allocation sizes efficiently. There are many other types
    of custom memory allocators, and most of them can be used in a C++ program. Using
    an allocator that is well-suited for a specific application often brings dramatic
    performance improvements, usually at the cost of severely restricting the programmer’s
    freedom in the implementation of the data structures.
  prefs: []
  type: TYPE_NORMAL
- en: The next common reason for inefficiency is more subtle and much harder to deal
    with.
  prefs: []
  type: TYPE_NORMAL
- en: Optimization of conditional execution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After the unnecessary computations and inefficient use of memory, the next *easiest*
    way to write inefficient code that fails to utilize a large fraction of available
    computing resources is probably code that does not pipeline well. We have seen
    the importance of CPU pipelining in [*Chapter 3*](B16229_03_Epub_AM.xhtml#_idTextAnchor047),
    *CPU Architecture, Resources, and Performance Implications*. We have also learned
    there that the worst disruptor of pipelining is usually a conditional operation,
    especially the one that the hardware branch predictor fails to guess.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, optimizing conditional code for better pipelining is one of
    the hardest C++ optimizations. It should be undertaken only if the profiler shows
    poor branch prediction. Note, however, that the number of mispredicted branches
    does not have to be large to be considered “poor”: a good program will typically
    have less than 0.1% of mispredicted branches. The misprediction rate of 1% is
    quite large. It is also quite difficult to predict the effect of source code optimizations
    without examining the compiler output (the machine code).'
  prefs: []
  type: TYPE_NORMAL
- en: 'If the profiler shows a badly predicted conditional operation, the next step
    is to determine which condition is being mispredicted. We have already seen some
    examples in [*Chapter 3*](B16229_03_Epub_AM.xhtml#_idTextAnchor047), *CPU Architecture,
    Resources, and Performance Implications*. For example, this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'may yield one or more badly predicted branches even when the overall result
    is predictable. This has to do with the definition of the Boolean logic in C++:
    the operators `||` and `&&` are *short-circuited*: the expression is evaluated
    left to right until the result becomes known. For example, if `a[i]` is `true`,
    the code must not access array elements `b[i]` and `c[i]`. Sometimes, this is
    necessary: the logic of the implementation may be such that these elements don’t
    exist. But often, the Boolean expressions introduce unnecessary branches for no
    reason. The preceding `if()` statement requires 3 conditional operations. On the
    other hand, this statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: is equivalent to the last one if the values `a`, `b`, and `c` are non-negative
    but require a single conditional operation. Again, this is not the kind of optimization
    you should be doing preemptively unless you have measurements that confirm the
    need for it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is another example. Consider this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'It is very inefficient if the value of `b` is unpredictable. Much better performance
    is just a simple change away:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'This improvement can be confirmed with a simple benchmark of the original,
    conditional, implementation versus the branchless one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the branchless implementation is almost 3 times faster.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important not to go overboard with this type of optimization. It must
    always be driven by the measurements, for several reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: The branch predictors are quite complex, and our intuition about what they can
    and cannot handle is almost always wrong.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The compiler optimizations can often change the code significantly, so, without
    measuring or examining the machine code, even our expectations of the existence
    of a branch can be wrong.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even if the branch is mispredicted, the performance impact can vary, so it is
    impossible to be sure without the measurement.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, it is almost never useful to manually optimize this very common
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'It looks like conditional code, and if the sign of `x` is random, the prediction
    is impossible. However, it is very likely that the profiler will not show a large
    number of mispredicted branches here. The reason is that most compilers will not
    implement this line using a conditional jump. On x86, some compilers will use
    the CMOVE instruction, which does a *conditional move*: it moves the value from
    one of two source registers to the destination, depending on the condition. The
    *conditional* nature of this instruction is benign: remember that the problem
    with conditional code is that the CPU does not know in advance which instruction
    to execute next. With a conditional move implementation, the sequence of instructions
    is perfectly linear, and their order is predetermined, so there is nothing to
    guess.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another common example that is unlikely to benefit from a branchless optimization
    is a conditional function call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'A branchless implementation is possible using an array of function pointers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'If the functions were originally inlined, replacing them with an indirect function
    call is a performance killer. If they weren’t, this change likely does almost
    nothing: jumping to another function whose address is not known during compilation
    has an effect very similar to a mispredicted branch, so this code causes the CPU
    to flush the pipeline either way.'
  prefs: []
  type: TYPE_NORMAL
- en: The bottom line is, optimizing for branch prediction is a very advanced step.
    The results can be a spectacular improvement or a spectacular failure (or just
    some wasted time), so it is important to be guided by performance measurements
    at every step.
  prefs: []
  type: TYPE_NORMAL
- en: We have now learned a lot about many kinds of potential inefficiencies in C++
    programs and ways to improve them. We conclude with some overall guidelines for
    optimizing your code.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we have covered the first of the two large areas of C++ efficiency
    from the language standpoint: avoiding inefficient language constructs, which
    boils down to not doing unnecessary work. Many optimization techniques we have
    studied dovetail with the material we studied earlier, such as the efficiency
    of accessing memory and avoiding false sharing in concurrent programs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The big dilemma every programmer faces is how much work should be invested
    upfront into writing efficient code and what should be left to incremental optimization.
    Let us begin by saying that high performance begins at the design stage: designing
    the architecture and the interfaces that do not lock in poor performance and inefficient
    implementations is the most important effort in developing high-performance software.'
  prefs: []
  type: TYPE_NORMAL
- en: Beyond that, the distinction should be made between **premature optimization**
    and **unnecessary pessimization**. Creating temporary variables to avoid aliasing
    is premature unless you have performance measurement data showing that the function
    you are optimizing contributes greatly to the overall execution time (or unless
    it improves readability, which is a different matter). Passing large vectors by
    value until the profiler tells you to change it is just making your code slower
    for no reason, so it should be avoided from the start.
  prefs: []
  type: TYPE_NORMAL
- en: 'The line between the two is not always clear, so you must weigh several factors.
    You must consider the impact of the change on the program: does it make the code
    harder to read, more complex, or more difficult to test? Generally, you don’t
    want to risk making more bugs for the sake of performance unless the measurements
    tell you that you have to. On the other hand, sometimes more readable or more
    straightforward code is also more efficient code, then the optimization cannot
    be considered premature.'
  prefs: []
  type: TYPE_NORMAL
- en: The second major area of C++ efficiency has to do with helping the compiler
    generate more efficient code. We will cover this in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When is passing even large objects by value acceptable?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When using resource-owning smart pointers, how should we invoke functions that
    operate on the objects?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the return value optimization, and where is it used?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why does inefficient memory management affect not just memory consumption but
    also runtime?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the A-B-A problem?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
