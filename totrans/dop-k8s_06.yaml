- en: Monitoring and Logging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Monitoring and logging are a crucial part of a site''s reliability. We''ve
    learned how to leverage various controllers to take care of our application, and
    about utilizing service together with Ingress to serve our web applications. Next,
    in this chapter, we''ll learn how to keep track of our application by means of
    the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting status snapshot of a container
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring in Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converging metrics from Kubernetes by Prometheus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concepts of logging in Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logging with Fluentd and Elasticsearch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inspecting a container
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whenever our application behaves abnormally, we will definitely want to know
    what happened, using all means, such as checking logs, resource usage, processes
    watchdog, or even getting into the running host directly to dig problems out.
    In Kubernetes, we have `kubectl get` and `kubectl describe` that can query deployment
    states, which will help us determine if an application has crashed or works as
    desired.
  prefs: []
  type: TYPE_NORMAL
- en: 'Further, if we want to know what is going on from the outputs of an application,
    we also have `kubectl logs` that redirects a container''s `stdout` to our Terminal.
    For CPU and memory usage stats, there''s also a top-like command we can employ,
    `kubectl top`. `kubectl top node`, which gives an overview of the resource usages
    of nodes, and `kubectl top pod <POD_NAME>` which displays per-pod usage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: To use `kubectl top`, you'll need Heapster deployed in your cluster. We'll discuss
    this later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'What if we leave something such as logs inside a container and they are not
    sent out anywhere? We know there''s a `docker exec` execute command inside a running
    container, but it''s unlikely that we have access to nodes every time. Fortunately,
    `kubectl` allows us to do the same thing with the `kubectl exec` command. Its
    usage is similar to the Docker one. For example, we can run a shell inside the
    container in a pod like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: It's pretty much the same as logging onto a host by SSH, and it enables us to
    troubleshoot with tools we are familiar with, as we've done in non-container worlds.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes dashboard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In addition to the command-line utility, there is a dashboard that aggregates
    almost every information we have just discussed on a decent web-UI:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00094.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'It''s in fact a general purpose graphical user interface of a Kubernetes cluster,
    as it also allows us to create, edit, and delete resources. Deploying it is quite
    easy; all we need to do is apply a template:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This template is for the Kubernetes cluster with **RBAC** (**role-based access
    control**) enabled. Check the dashboard's project repository ([https://github.com/kubernetes/dashboard](https://github.com/kubernetes/dashboard))
    if you need other deployment options. Regarding RBAC, we'll talk about this in
    [Chapter 8](part0188.html#5J99O0-6c8359cae3d4492eb9973d94ec3e4f1e), *Cluster Administration*.
    Many managed Kubernetes services, such as Google Container Engine, pre-deployed
    the dashboard in the cluster so that we don't need to install it on our own. To
    determine whether the dashboard exists in our cluster or not, use `kubectl cluster-info`.
  prefs: []
  type: TYPE_NORMAL
- en: We'll see kubernetes-dashboard is running at ... if it's installed. The service
    for the dashboard deployed with the default template or provisioned by cloud providers
    is usually ClusterIP. In order to access it, we'll need to establish a proxy between
    our terminal and Kubernetes' API server with `kubectl proxy.` Once the proxy is
    up, we are then able to access the dashboard at `http://localhost:8001/ui`. The
    port `8001` is the default port of `kubectl proxy`.
  prefs: []
  type: TYPE_NORMAL
- en: As with `kubectl top`, you'll need Heapster deployed in your cluster to see
    the CPU and memory stats.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring in Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since we now know how to examine our applications in Kubernetes, it's quite
    natural that we should have a mechanism to do so constantly to detect any incident
    at the first occurrence. To put it another way, we need a monitoring system. A
    monitoring system collects metrics from various sources, stores and analyzes data
    received, and then responds to exceptions. In a classical setup of application
    monitoring, we would gather metrics from, at the very least, three different layers
    of our infrastructure to ensure our service's availability as well as quality.
  prefs: []
  type: TYPE_NORMAL
- en: Application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The data we''re concerned with at this level involves the internal states of
    an application, which can help us determine what''s going on inside our service.
    For example, the following screenshot is from Elasticsearch Marvel ([https://www.elastic.co/guide/en/marvel/current/introduction.html](https://www.elastic.co/guide/en/marvel/current/introduction.html)),
    called **Monitoring** from version 5 onward), which is a monitoring solution for
    an Elasticsearch cluster. It brings together the information about our cluster,
    particularly Elasticsearch specific metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00095.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In addition, we would leverage profiling tools in conjunction with tracing tools
    to instrument our program, which augments dimensions that enables us to inspect
    our service in a finer granularity. Especially nowadays, an application might
    be composed of dozens of services in a distributed way. Without utilizing tracing
    tools, such as OpenTracing ([http://opentracing.io](http://opentracing.io)) implementations,
    identifying performance culprits can be extremely difficult.
  prefs: []
  type: TYPE_NORMAL
- en: Host
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Collecting tasks at the host level is usually performed by agents provided by
    the monitoring framework. The agent extracts and sends out comprehensive metrics
    about a host such as loads, disks, connections, or stats of processes that assist
    in determining a host's health.
  prefs: []
  type: TYPE_NORMAL
- en: External resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Aside from the aforementioned two components, we also need to check dependent
    components' statuses. For instance, say we have an application that consumes a
    queue and executes corresponding tasks; we should also take care about the metrics,
    such as the queue length and the consuming rate. If the consuming rate is low
    and the queue length keeps growing, our application is supposedly hitting trouble.
  prefs: []
  type: TYPE_NORMAL
- en: These principles also apply to containers on Kubernetes, as running a container
    on a host is almost identical to running a process. Nonetheless, due to the fact
    that there is a subtle distinction between the way containers on Kubernetes and
    on traditional hosts utilize resources, we still need to take the differences
    into consideration when employing a monitoring strategy. For instance, containers
    of an application on Kubernetes would spread across multiple hosts, and also would
    not always be on the same hosts. It would be grueling to produce a consistent
    recording of one application if we are still adopting the host-centric monitoring
    approach. Therefore, rather than observing resource usages at the host level only,
    we should pile a container layer to our monitoring stack. Moreover, since Kubernetes
    is, in reality, the infrastructure to our applications, we absolutely should take
    it into account as well.
  prefs: []
  type: TYPE_NORMAL
- en: Container
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As mentioned, metrics collected at the container level and what we get at the
    host level are pretty much the same thing, particularly the usage of system resources.
    Notwithstanding the seeming redundancy, it''s the very key which facilitates us
    to resolve difficulties on monitoring moving containers. The idea is quite simple:
    what we need to do is attach logical information to metrics, such as pod labels
    or their controller name. In this way, metrics coming out from containers across
    distinct hosts could be meaningfully grouped. Consider the following diagram;
    say we want to know how many bytes transmitted (**tx**) on **App 2**, we could
    sum up **tx** metrics over the **App 2** label and it yields **20 MB:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00096.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Another difference is that metrics on CPU throttling are reported at container
    level only. If performance issues are encountered at a certain application but
    the CPU resource on the host is spare, we can check if it's throttled with the
    associated metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes is responsible for managing, scheduling, and orchestrating our applications.
    Accordingly, once an application has crashed, Kubernetes is certainly one of the
    first places we would want to look. In particular, when the crash happens after
    rolling out a new deployment, the state of associated objects would be reflected
    instantly on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'To sum up, components that should be monitored are illustrated in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00097.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Getting monitoring essentials for Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For every layer of the monitoring stack, we can always find a counterpart collector.
    For instance, at the application level, we can dump metrics manually; at the host
    level, we would install a metrics collector on every box; as for Kubernetes, there
    are APIs for exporting the metrics that we are interested in, and, at the very
    least, we have `kubectl` at hand.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to the container level collector, what options do we have? Perhaps
    installing the host metrics collector inside the image of our application does
    the job, but we'll soon realize that it could make our container way too clumsy
    in terms of size as well as resource utilizations. Fortunately, there's already
    a solution for such needs, namely cAdvisor ([https://github.com/google/cadvisor](https://github.com/google/cadvisor)),
    the answer to the container level metrics collector. Briefly speaking, cAdvisor
    aggregates the resource usages and performance statistics of every running container
    on a machine. Notice that the deployment of cAdvisor is one per host instead of
    one per container, which is more reasonable for containerized applications. In
    Kubernetes, we don't even care about deploying cAdvisor, as it has already been
    embedded into kubelet.
  prefs: []
  type: TYPE_NORMAL
- en: cAdvisor is accessible via port `4194` on every node. Prior to Kubernetes 1.7,
    the data gathered by cAdvisor was able to be collected via the kubelet port (`10250`/`10255`)
    as well. To access cAdvisor, we can access the instance port `4194` or through
    `kubectl proxy` at `http://localhost:8001/api/v1/nodes/<nodename>:4194/proxy/`
    or access `http://<node-ip>:4194/` directly.
  prefs: []
  type: TYPE_NORMAL
- en: The following screenshot is from the cAdvisor Web UI. You will see a similar
    page once connected. For viewing the metrics that cAdvisor grabbed, visit the
    endpoint, `/metrics`.
  prefs: []
  type: TYPE_NORMAL
- en: '>![](../images/00098.jpeg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another important component in the monitoring pipeline is Heapster ([https://github.com/kubernetes/heapster](https://github.com/kubernetes/heapster)).
    It retrieves monitoring statistics from every node, specifically kubelet on nodes
    processing, and writes to external sinks afterward. It also exposes aggregated
    metrics via the REST API. The function of Heapster sounds rather redundant with
    cAdvisor, but they play different roles in the monitoring pipeline in practice.
    Heapster gathers cluster-wide statistics; cAdvisor is a host-wide component. That
    is to say, Heapster empowers a Kubernetes cluster with the basic monitoring ability.
    The following diagram illustrates how it interacts with other components in a
    cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00099.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As a matter of fact, it's unnecessary to install Heapster if your monitoring
    framework offers a similar tool that also scrapes metrics from kubelet. However,
    since it's a default monitoring component in Kubernetes' ecosystem, many tools
    rely on it, such as `kubectl top` and the Kubernetes dashboard mentioned earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before deploying Heapster, check if the monitoring tool you''re using is supported
    as a Heapster sink in this document: [https://github.com/kubernetes/heapster/blob/master/docs/sink-configuration.md](https://github.com/kubernetes/heapster/blob/master/docs/sink-configuration.md).'
  prefs: []
  type: TYPE_NORMAL
- en: 'If not, we can just have a standalone setup and make the dashboard and `kubectl
    top` work by applying this template:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Remember to apply this template if RBAC is enabled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: After Heapster is installed, the `kubectl top` command and the Kubernetes dashboard
    should display resource usages properly.
  prefs: []
  type: TYPE_NORMAL
- en: 'While cAdvisor and Heapster focus on physical metrics, we also want the logical
    states of objects being displayed on our monitoring dashboard. kube-state-metrics
    ([https://github.com/kubernetes/kube-state-metrics](https://github.com/kubernetes/kube-state-metrics))
    is the very piece that completes our monitoring stack. It watches Kubernetes masters
    and transforms the object statues we see from `kubectl get` or `kubectl describe`
    to metrics in Prometheus format ([https://prometheus.io/docs/instrumenting/exposition_formats/](https://prometheus.io/docs/instrumenting/exposition_formats/)).
    As long as the monitoring system supports this format, we can scrape the states
    into the metrics storage and be alerted on events such as unexplainable restart
    counts. To install kube-state-metrics, first download the templates inside the
    `kubernetes` folder under the project repository([https://github.com/kubernetes/kube-state-metrics/tree/master/kubernetes](https://github.com/kubernetes/kube-state-metrics/tree/master/kubernetes)),
    and then apply them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Afterwards, we can view the states inside a cluster in the metrics on its service
    endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '`http://kube-state-metrics.kube-system:8080/metrics`'
  prefs: []
  type: TYPE_NORMAL
- en: Hands-on monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we've learned lots of principles to fabricate an impervious monitoring
    system in Kubernetes toward a robust service, and it's time to implement a pragmatic
    one. Because the vast majority of Kubernetes components expose their instrumented
    metrics on a conventional path in Prometheus format, we are free to use any monitoring
    tool with which we are acquainted as long as the tool understands the format.
    In this section, we'll set up an example with an open-source project, Prometheus
    ([https://prometheus.io](https://prometheus.io)), which is a platform-independent
    monitoring tool. Its popularity in Kubernetes' ecosystem is for not only its powerfulness
    but also for its being backed by the **Cloud Native Computing Foundation** ([https://www.cncf.io/](https://www.cncf.io/)),
    who also sponsors the Kubernetes project.
  prefs: []
  type: TYPE_NORMAL
- en: Meeting Prometheus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Prometheus framework comprises several components, as illustrated in the
    following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00100.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As with all other monitoring frameworks, Prometheus relies on agents scraping
    out statistics from the components of our system, and those agents are the exporters
    at the left of the diagram. Besides this, Prometheus adopts the pull model on
    metrics collecting, which is to say that it's not receiving metrics passively,
    but actively pulls data back from the metrics' endpoints on exporters. If an application
    exposes a metric's endpoint, Prometheus is able to scrape that data as well. The
    default storage backend is an embedded LevelDB, and can be switched to other remote
    storages such as InfluxDB or Graphite. Prometheus is also responsible for sending
    alerts according to pre-configured rules to **Alert manager**. **Alert manager**
    handles alarm sending tasks. It groups alarms received and dispatches them to
    tools that actually send messages, such as email, Slack, PagerDuty, and so on.
    In addition to alerts, we would also like to visualize collected metrics for getting
    a quick overview of our system, and Grafana is what comes in handy here.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Prometheus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The templates we''ve prepared for this chapter can be found here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DevOps-with-Kubernetes/examples/tree/master/chapter6](https://github.com/DevOps-with-Kubernetes/examples/tree/master/chapter6)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Under 6-1_prometheus are manifests for this section, including a Prometheus
    deployment, exporters, and related resources. They''ll be settled at a dedicated
    namespace, `monitoring`, except components required to work in `kube-system` namespaces.
    Please review them carefully, and now let''s create resources in the following
    order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Usages of resources are confined to a relatively low level at provided setups.
    If you''d like to use them in a more formal way, adjusting parameters according
    to actual requirements is recommended. After the Prometheus server is up, we can
    connect to its Web-UI at port `9090` by `kubectl port-forward`. We can use NodePort
    or Ingress to connect to the UI if modifying its service (`prometheus/prom-svc.yml`)
    accordingly. The first page we will see when entering the UI is Prometheus'' expression
    browser, where we build queries and visualize metrics. Under the default settings,
    Prometheus will collect metrics from itself. All valid scraping targets can be
    found at path `/targets`. To speak to Prometheus, we have to gain some understanding
    of its language: **PromQL**.'
  prefs: []
  type: TYPE_NORMAL
- en: Working with PromQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'PromQL has three data types: instant vector, range vector, and scalar. An instant
    vector is a time series of data sampled; a range vector is a set of time series
    containing data within a certain time range; a scalar is a numeric floating value.
    Metrics stored inside Prometheus are identified with a metric name and labels,
    and we can find the name of any collected metric with the drop-down list next
    to the Execute button on the expression browser. If we query Prometheus with metric
    names, say `http_requests_total`, we''ll get lots of results as instant vectors
    match the name but with different labels. Likewise, we can also query a particular
    set of labels only with `{}` syntax. For example, the query `{code="400",method="get"}`
    means that we want any metric that has the label `code`, `method` equals to `400`,
    and `get` respectively. Combining names and labels in a query is also valid, such
    as `http_requests_total{code="400",method="get"}`. PromQL grants us the detective
    ability to inspect our applications or systems from all kinds of clues so long
    as related metrics are collected.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the basic queries just mentioned, there''s so much more to PromQL,
    such as querying labels with regex and logical operators, joining and aggregating
    metrics with functions, and even performing operations between different metrics.
    For instance, the following expression gives us the total memory consumed by a
    `kube-dns` deployment in the `kube-system` namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: More detailed documents can be found at Prometheus' official site ([https://prometheus.io/docs/querying/basics/](https://prometheus.io/docs/querying/basics/)),
    and it certainly should help you to unleash the power of Prometheus.
  prefs: []
  type: TYPE_NORMAL
- en: Discovering targets in Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since Prometheus only pulls metrics from endpoints it knows, we have to explicitly
    tell it where we''d like to collect data from. Under the path `/config` is the
    page that lists current configured targets to pull. By default, there would be
    one job that collects the current metrics about Prometheus itself, and it''s in
    the conventional scraping path, `/metrics`. We would see a very long text page
    if connecting to the endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This is just the Prometheus metrics format we've mentioned several times. Next
    time when we see any page like this, we will know it's a metrics endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: The default job to scrape Prometheus is configured as a static target. However,
    with the fact that containers in Kubernetes are created and destroyed dynamically,
    it's really troublesome to find out the exact address of a container, let alone
    set it on Prometheus. In some cases, we may utilize service DNS as a static metrics
    target, but this still cannot solve all cases. Fortunately, Prometheus helps us
    overcome the problem with its ability to discover services inside Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'To be more specific, it''s able to query Kubernetes about the information of
    running services, and adds or deletes them to the target configuration accordingly.
    Four kinds of discovery mechanisms are currently supported:'
  prefs: []
  type: TYPE_NORMAL
- en: The **node** discovery mode creates one target per node, and the target port
    would be kubelet's port by default.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **service** discovery mode creates a target for every `service` object,
    and all defined ports in a service would become a scraping target.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **pod** discovery mode works in a similar way to the service discovery role,
    that is, it creates targets per pod and for each pod it exposes all the defined
    container ports. If there is no port defined in a pod's template, it would still
    create a scraping target with its address only.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **endpoints** mode discovers `endpoint` objects created by a service. For
    example, if a service is backed by three pods with two ports each, then we'll
    have six scraping targets. In addition, for a pod, not only ports that expose
    to a service but also other declared container ports would be discovered.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram illustrates four discovery mechanisms: the left ones
    are the resources in Kubernetes, and those in the right list are targets created
    in Prometheus:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00101.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Generally speaking, not all exposed ports are served as a metrics endpoint,
    so we certainly don''t want Prometheus to grab everything in our cluster but collect
    marked resources only. To achieve this, Prometheus utilizes annotations on resource
    manifests to distinguish which targets are to be grabbed. The annotation format
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**On pod**: If a pod is created by a pod controller, remember to set Prometheus
    annotations in the pod spec rather than in the pod controller:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prometheus.io/scrape`: `true` indicates that this pod should be pulled.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prometheus.io/path`: Set this annotation to the path that exposes metrics;
    it only needs to be set if the target pod is using a path other than `/metrics`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prometheus.io/port`: If the defined port is different from the actual metrics
    port, override it with this annotation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**On service**: Since endpoints are mostly not created manually, endpoint discovery
    uses the annotations inherited from a service. That is to say, annotations on
    services effect service and endpoint discovery modes simultaneously. As such,
    we''d use `prometheus.io/scrape: ''true''` to denote endpoints created by a service
    that are to be scraped, and use `prometheus.io/probe: ''true''` to tag a service
    with metrics. Moreover, `prometheus.io/scheme` designates whether `http` or `https`
    is used. Other than that, the path and port annotations also work here.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following template snippet indicates Prometheus'' endpoint discovery role,
    but the service discovery role to create targets on pods is selected at: `9100/prom`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The template `prom-config-k8s.yml` under our example repository contains the
    configuration to discover Kubernetes resources for Prometheus. Apply it with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Because it''s a ConfigMap, it takes seconds to become consistent. Afterwards,
    reload Prometheus by sending a `SIGHUP` to the process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The provided template is based on this example from Prometheus'' official repository;
    you can find out more usages here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/prometheus/prometheus/blob/master/documentation/examples/prometheus-kubernetes.yml](https://github.com/prometheus/prometheus/blob/master/documentation/examples/prometheus-kubernetes.yml)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, the document page describes in detail how the Prometheus configuration
    works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://prometheus.io/docs/operating/configuration/](https://prometheus.io/docs/operating/configuration/)'
  prefs: []
  type: TYPE_NORMAL
- en: Gathering data from Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The steps for implementing the five monitoring layers discussed earlier in
    Prometheus are quite clear now: installing exporters, annotating them with appropriate
    tags, and then collecting them on auto-discovered endpoints.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The host layer monitoring in Prometheus is done by the node exporter ([https://github.com/prometheus/node_exporter](https://github.com/prometheus/node_exporter)).
    Its Kubernetes manifest can be found under the examples for this chapter, and
    it contains one DaemonSet with a scrape annotation. Install it with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Its corresponding configuration will be created by a pod discovery role.
  prefs: []
  type: TYPE_NORMAL
- en: The container layer collector should be cAdvisor, and it has already been installed
    in kubelet. Consequently, discovering it with the node mode is the only thing
    what we need to do.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes monitoring is done by kube-state-metrics, which was also introduced
    previously. One even better thing is that it comes with Prometheus annotations,
    and this means that we don't need to do anything additional to configure it.
  prefs: []
  type: TYPE_NORMAL
- en: Up to this point, we've already set up a strong monitoring stack based on Prometheus.
    With respect to the application and external resources monitoring, there are extensive
    exporters in the Prometheus ecosystem to support monitoring various components
    inside our system. For instance, if we need statistics of our MySQL database,
    we could just install MySQL Server Exporter ([https://github.com/prometheus/mysqld_exporter](https://github.com/prometheus/mysqld_exporter)),
    which offers comprehensive and useful metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to those metrics already described, there are some other useful
    metrics from Kubernetes components that play a significant part in a variety of
    aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Kubernetes API server**: The API server exposes its state at `/metrics`,
    and this target is enabled by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kube-controller-manager**: This component exposes metrics on port `10252`,
    but it''s invisible on some managed Kubernetes services such as **Google Container
    Engine** (**GKE**). If you''re on a self-hosted cluster, applying "`kubernetes/self/kube-controller-manager-metrics-svc.yml`"
    creates endpoints for Prometheus.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kube-scheduler**: It uses port `10251`, and it''s not visible on clusters
    by GKE as well. "`kubernetes/self/kube-scheduler-metrics-svc.yml`" is the template
    for creating a target to Prometheus.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kube-dns**: There are two containers in a kube-dns pod, `dnsmasq` and `sky-dns`,
    and their metrics ports are `10054` and `10055` respectively. The corresponding
    template is `kubernetes/self/ kube-dns-metrics-svc.yml`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**etcd**: The etcd cluster also has a Prometheus metrics endpoint on port `4001`.
    If your etcd cluster is self-hosted and managed by Kubernetes, you can take "`kubernetes/self/etcd-server.yml`"
    as a reference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Nginx ingress controller**: The nginx controller publishes metrics at port
    `10254`. But the metrics contain only limited information. To get data such as
    connection counts by host or by path, you''ll need to activate the `vts` module
    in the controller to enhance the metrics collected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seeing metrics with Grafana
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The expression browser has a built-in graph panel that enables us to see the
    visualized metrics, but it's not designed to serve as a visualization dashboard
    for daily routines. Grafana is the best option for Prometheus. We've discussed
    how to set up Grafana in [Chapter 4](part0103.html#3279U0-6c8359cae3d4492eb9973d94ec3e4f1e),
    *Working with Storage and Resources*, and we also provide templates in the repository
    for this chapter; both options do the job.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see Prometheus metrics in Grafana, we have to add a data source first. The
    following configurations are required to connect to our Prometheus server:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Type: "Prometheus"'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Url: `http://prometheus-svc.monitoring:9090`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Access: proxy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once it''s connected, we can import a dashboard to see something in action.
    On Grafana''s sharing page ([https://grafana.com/dashboards?dataSource=prometheus](https://grafana.com/dashboards?dataSource=prometheus))
    are rich off-the-shelf dashboards. The following screenshot is from the dashboard
    `#1621`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00102.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Because the graphs are drawn by data from Prometheus, we are capable of plotting
    any data with which we are concerned as long as we master PromQL.
  prefs: []
  type: TYPE_NORMAL
- en: Logging events
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Monitoring with quantitative time series of a system status enables us to briskly
    dig out which components in our system failed, but it's still inadequate to diagnose
    with the root cause under syndromes. As a result, a logging system that gathers,
    persists, and searches logs is certainly helpful for uncovering the reason why
    something went wrong by means of correlating events with the anomalies detected.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, there are two main components in a logging system: the logging
    agent and the logging backend. The former is an abstract layer to a program. It
    gathers, transforms, and dispatches logs to the logging backend. A logging backend
    warehouses all logs received. As with monitoring, the most challenging part of
    building a logging system for Kubernetes is ascertaining how to gather logs from
    containers to a centralized logging backend. Typically, there are three ways to
    send out logs to a program:'
  prefs: []
  type: TYPE_NORMAL
- en: Dumping everything to `stdout`/`stderr`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing `log` files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sending logs to a logging agent or logging the backend directly; programs in
    Kubernetes are also able to emit logs in the same manner so long as we understand
    how log streams flow in Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Patterns of aggregating logs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For programs that log to a logging agent or backend directly, whether they are
    inside Kubernetes or not doesn't matter on the whole, as they technically don't
    output logs through Kubernetes. As for other cases, we'd use the following two
    patterns to centralize logs.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting logs with a logging agent per node
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We know messages we retrieved via `kubectl logs` are streams redirected from
    `stdout`/`stderr` of a container, but it's obviously not a good idea to collect
    logs with `kubectl logs`. Actually, `kubectl logs` gets logs from kubelet, and
    kubelet aggregates logs to the host path, `/var/log/containers/`, from the container
    engine underneath.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, setting up logging agents on every node and configuring them to
    tail and forward `log` files under the path are just what we need for converging
    standard streams of running containers, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00103.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In practice, we''d also configure a logging agent to tail logs from the system
    and Kubernetes, components under `/var/log` on masters and nodes such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kube-proxy.log`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-apiserver.log`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-scheduler.log`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-controller-manager.log`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`etcd.log`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aside from `stdout`/`stderr`, if logs of an application are stored as files
    in the container and persisted via the `hostPath` volume, a node logging agent
    is capable of passing them to a node likewise. However, for each exported `log`
    file, we have to customize their corresponding configurations in the logging agent
    so that they can be dispatched correctly. Moreover, we also need to name `log`
    files properly to prevent any collision and take care of log rotation on our own,
    which makes it an unscalable and unmanageable logging mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: Running a sidecar container to forward logs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sometimes it''s just difficult to modify our application to write logs to standard
    streams rather than `log` files, and we wouldn''t want to face the troubles brought
    about by logging to `hostPath` volumes. In such a situation, we could run a Sidecar
    container to deal with logging within only one pod. In other words, each application
    pod would have two containers sharing the same `emptyDir` volume so that the Sidecar
    container can follow logs from the application container and send them outside
    their pod, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00104.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Although we don't need to worry about management of `log` files anymore, chores
    such as configuring logging agents for each pod and attaching metadata from Kubernetes
    to log entries still takes extra effort. Another choice is leveraging the Sidecar
    container to outputting logs to standard streams instead of running a dedicated
    logging agent like the following pod; the application container unremittingly
    writes messages to `/var/log/myapp.log`, and the Sidecar tails `myapp.log` in
    the shared volume.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can see the written log with `kubectl logs`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Ingesting Kubernetes events
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The event messages we saw at the output of `kubectl describe` contain valuable
    information and complement the metrics gathered by kube-state-metrics, which allows
    us to know what exactly happened to our pods or nodes. Consequently, it should
    be part of our logging essentials together with system and application logs. In
    order to achieve this, we'll need something to watch Kubernetes API servers and
    aggregate events into a logging sink. And there is eventer that does what we need
    to events.
  prefs: []
  type: TYPE_NORMAL
- en: Eventer is part of Heapster, and it currently supports Elasticsearch, InfluxDB,
    Riemann, and Google Cloud Logging as its sink. Eventer can also output to `stdout`
    directly in case the logging system we're using is not supported.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deployment of eventer is similar to deploying Heapster, except for the container
    startup commands, as they are packed in the same image. The flags and options
    for each sink type can be found here: ([https://github.com/kubernetes/heapster/blob/master/docs/sink-configuration.md](https://github.com/kubernetes/heapster/blob/master/docs/sink-configuration.md)).'
  prefs: []
  type: TYPE_NORMAL
- en: Example templates we provided for this chapter also include eventer, and it's
    configured to work with Elasticsearch. We'll describe it in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Logging with Fluentd and Elasticsearch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thus far we've discussed various conditions on the logging we may encounter
    in the real world, and it's time to roll up our sleeves to fabricate a logging
    system with what we have learned.
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of a logging system and a monitoring system are pretty much
    the same in some ways--collectors, storages, and the user-interface. The corresponding
    components we're going to set up are Fluentd/eventer, Elasticsearch, and Kibana,
    respectively. Templates for this section can be found under `6-3_efk`, and they'd
    be deployed to the namespace `monitoring` from the previous part.
  prefs: []
  type: TYPE_NORMAL
- en: 'Elasticsearch is a powerful text search and analysis engine, which makes it
    an ideal choice for persisting, processing, and analyzing logs from everything
    running in our cluster. The Elasticsearch template for this chapter uses a very
    simple setup to demonstrate the concept. If you''d like to deploy an Elasticsearch
    cluster for production use, leveraging the StatefulSet controller and tuning Elasticsearch
    with the proper configuration, as we discussed in [Chapter 4](part0103.html#3279U0-6c8359cae3d4492eb9973d94ec3e4f1e),
    *Working with Storage and Resources,* is recommended. Let''s deploy Elasticsearch
    with the following template ([https://github.com/DevOps-with-Kubernetes/examples/tree/master/chapter6/6-3_efk/](https://github.com/DevOps-with-Kubernetes/examples/tree/master/chapter6/6-3_efk/)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Elasticsearch is ready if there's a response from `es-logging-svc:9200`.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is setting up a node logging agent. As we'd run it on every node,
    we definitely want it as light as possible in terms of resource usages of a node,
    hence Fluentd ([www.fluentd.org](http://www.fluentd.org)) is opted for. Fluentd
    features in lower memory footprints, which makes it a competent logging agent
    for our needs. Furthermore, since the logging requirement in the containerized
    environment is very focused, there is a sibling project, Fluent Bit (`fluentbit.io`),
    which aims to minimize the resource usages by trimming out functions that wouldn't
    be used for its target scenario. In our example, we would use the Fluentd image
    for Kubernetes ([https://github.com/fluent/fluentd-kubernetes-daemonset](https://github.com/fluent/fluentd-kubernetes-daemonset))
    to conduct the first logging pattern we mentioned previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'The image is already configured to forward container logs under `/var/log/containers`
    and logs of certain system components under `/var/log`. We are absolutely able
    to further customize its logging configuration if need be. Two templates are provided
    here: `fluentd-sa.yml` is the RBAC configuration for the Fluentd DaemonSet, `fluentd-ds.yml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Another must-have logging component is eventer. Here we prepared two templates
    for different conditions. If you''re on a managed Kubernetes service where Heapster
    is already deployed, the template for a standalone eventer, `eventer-only.yml`,
    is used in this case. Otherwise, consider the template of running Heapster in
    combination with eventer in the same pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: To see logs emitted to Elasticsearch, we can invoke the search API of Elasticsearch,
    but there's a better option, namely Kibana, a web interface that allows us to
    play with Elasticsearch. The template for Kibana is `elasticsearch/kibana-logging.yml`
    under [https://github.com/DevOps-with-Kubernetes/examples/tree/master/chapter6/6-3_efk/](https://github.com/DevOps-with-Kubernetes/examples/tree/master/chapter6/6-3_efk/).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Kibana in our example is listening to port `5601`. After exposing the service
    out of your cluster and connecting to it with any browser, you can start to search
    logs from Kubernetes. The index name of the logs sent out by eventer is `heapster-*`,
    and it's `logstash-*` for logs forwarded by Fluentd. The following screenshot
    shows what a log entry looks like in Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: The entry is from our earlier example, `myapp`, and we can find that the entry
    is already tagged with handy metadata on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00105.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Extracting metrics from logs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The monitoring and logging system we built around our application on top of
    Kubernetes is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../images/00106.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The logging part and the monitoring part look like two independent tracks, but
    the value of the logs is much more than a collection of short texts. They are
    structured data and emitted with timestamps as usual; as such, the idea to transform
    logs into time-series data is promising. However, although Prometheus is extremely
    good at processing time-series data, it cannot ingest texts without any transformation.
  prefs: []
  type: TYPE_NORMAL
- en: 'An access log entry from HTTPD looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '`10.1.8.10 - - [07/Jul/2017:16:47:12 0000] "GET /ping HTTP/1.1" 200 68`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It consists of the request IP address, time, method, handler, and so on. If
    we demarcate log segments by their meanings, counted sections can then be regarded
    as a metric sample like this: `"10.1.8.10": 1, "GET": 1, "/ping": 1, "200": 1`.'
  prefs: []
  type: TYPE_NORMAL
- en: Tools such as mtail ([https://github.com/google/mtail](https://github.com/google/mtail))
    and Grok Exporter ([https://github.com/fstab/grok_exporter](https://github.com/fstab/grok_exporter))
    count log entries and organize those numbers to metrics so that we can further
    process them in Prometheus.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the start of this chapter, we described how to get the status of running
    containers quickly by means of built-in functions such as `kubectl`. Then we expanded
    the discussion to concepts and principles of monitoring, including why it is necessary
    to do monitoring, what to monitor, and how to monitor. Afterwards, we built a
    monitoring system with Prometheus as the core, and set up exporters to collecting
    metrics from Kubernetes. The fundamentals of Prometheus were also introduced so
    that we can leverage metrics to gain more understanding of our cluster as well
    as the applications running inside. On the logging part, we mentioned common patterns
    of logging and how to deal with them in Kubernetes, and deployed an EFK stack
    to converge logs. The system we built in this chapter facilitates the reliability
    of our service. Next, we are advancing to set up a pipeline to deliver our product
    continuously in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
