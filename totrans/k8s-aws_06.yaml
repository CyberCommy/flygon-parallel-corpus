- en: Planning for Production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes provides an excellent platform for developers to rapidly build highly
    flexible distributed applications. By running our applications on Kubernetes,
    we have a number of tools at our disposal to simplify their operation, and for
    making them more reliable, resilient to errors, and, ultimately, highly available.
  prefs: []
  type: TYPE_NORMAL
- en: In order for us to depend on some of the guarantees and behaviors that our applications
    can inherit from Kubernetes, it is important that we understand how Kubernetes
    behaves, and some of the factors that have an impact on a production system.
  prefs: []
  type: TYPE_NORMAL
- en: It is important as a cluster administrator that you have an understanding of
    the requirements of the applications you are running, and of the users of those
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: Having an awareness of the way that Kubernetes behaves in production is key,
    so it is invaluable to gain some practical experience of running your applications
    on Kubernetes before you start to serve mission-critical traffic. For example,
    when GitHub migrated their main application to Kubernetes, they started by moving
    traffic for internal users to their new Kubernetes-based infrastructure, before
    switching their main production traffic.
  prefs: []
  type: TYPE_NORMAL
- en: '"The load from internal users helped us find problems, fix bugs, and start
    getting comfortable with Kubernetes in production. During this period, we worked
    to increase our confidence by simulating procedures we anticipated performing
    in the future, writing runbooks, and performing failure tests."—Jesse Newland
    ([https://githubengineering.com/kubernetes-at-github/](https://githubengineering.com/kubernetes-at-github/))'
  prefs: []
  type: TYPE_NORMAL
- en: While I can cover some of the things that you are likely to encounter when using
    Kubernetes on AWS in production, it is important to understand that every application
    and organization is unique in surprising ways. You should think of Kubernetes
    as a toolkit that will enable you to build a powerful and flexible environment
    for your organization. Kubernetes isn't a magic bullet that removes the need for
    operational expertise; it's a tool that assists you in managing your applications.
  prefs: []
  type: TYPE_NORMAL
- en: The design process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The design process is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/f50ea2e6-83e1-4fe8-8dbb-d53c03a585c6.png)'
  prefs: []
  type: TYPE_IMG
- en: When you think about preparing to use Kubernetes to manage your production infrastructure,
    you shouldn't think about Kubernetes as your end goal. It is a foundation for
    building a platform on which to run systems.
  prefs: []
  type: TYPE_NORMAL
- en: When you think about building a platform to meet the needs of the different
    people in your organization, it becomes much simpler to define the requirements
    you will place on Kubernetes. When trying to plan for a production environment,
    you need to understand the requirements that your organization has. Clearly, the
    technical requirements of the software you want to manage is important. But it
    is also key to understanding the operational process that your organization needs
    to support.
  prefs: []
  type: TYPE_NORMAL
- en: Adopting Kubernetes offers a lot of benefits to organizations that have complex
    requirements for the software that they run. Unfortunately, this complexity can
    also lead to challenges in safely adopting Kubernetes in a successful way.
  prefs: []
  type: TYPE_NORMAL
- en: Initial planning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You should consider where you will focus your efforts for your initial roll
    out. You should look for an application that will both deliver valuable results
    quickly, as well as having a lower risk profile. If we think about the example
    at GitHub, they initially focused their efforts on building an infrastructure
    for internal users to quickly test changes to their software. By focusing on a
    review or staging infrastructure, they found an application for Kubernetes that
    would both provide value quickly to developers in their organization, as well
    as an area that had low risks to their business as it was only accessed by internal
    users.
  prefs: []
  type: TYPE_NORMAL
- en: Applications like these that have a combination of immediate usefulness and
    a lower impact of downtime are very useful. They allow your organization to gain
    valuable operational experience using Kubernetes, as well as to drive out bugs
    and other issues well before you attempt to handle production workloads.
  prefs: []
  type: TYPE_NORMAL
- en: When getting started with Kubernetes, it can be tempting to choose the simplest
    application that your organization operates and start building processes and tooling
    around this. However, this can be a mistake because it might lead to you making
    assumptions about how your applications should be operated, that might make it
    much harder to later apply the same processes and configuration to more complex
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: If you choose to start building your platform to support a simple application
    that doesn't require any backend services, such as a database, you might miss
    a number of things you need to consider as part of your deployment process. For
    example, applications that are backed by a database often need to run migration
    scripts to update the schema when a new version of an application is deployed.
    If you start by designing a deployment process to meet the needs of a very simple
    application, you might not surface these requirements until later. Remember, it
    will always be much simpler to deploy a simple application that only needs a subset
    of the features that your platform provides, than a more complex application that
    needs facilities you didn't consider when designing it.
  prefs: []
  type: TYPE_NORMAL
- en: If you choose to focus your efforts on a single application for your initial
    adoption of Kubernetes, make sure that you choose an application that is representative
    of your organization's needs. It can be tempting to start using Kubernetes for
    a greenfield project, as you can take application development decisions with the
    platform in mind. But remember that a new application may well be significantly
    simpler than an application that has been in use for a longer time. In the example
    from GitHub, the application they chose to deploy first was their largest application
    operated by their organization providing many core services.
  prefs: []
  type: TYPE_NORMAL
- en: If your organization has an application that requires a lot of operational time
    and effort every time it is deployed, it could be that this would be a good choice
    for an initial adoption of Kubernetes. Applications like these will be well known
    for their needs by your development and operational teams, and they will immediately
    be able to start to utilize Kubernetes to address the issues that previously cost
    time and effort.
  prefs: []
  type: TYPE_NORMAL
- en: Planning for success
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a few things that you should try to avoid in order to deliver successfully
    on a project to adopt Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: One trap that can be all too easy to fall into is to change too much too quickly.
    If you are taking the decision to adopt containerization and Kubernetes, it can
    be very tempting to adopt a lot of new processes and tools alongside this. This
    can slow down your progress quite significantly, because what started as a project
    to run your applications in containers can quickly grow to encompass many other
    tools and processes that your organization would like to adopt.
  prefs: []
  type: TYPE_NORMAL
- en: You should aim to avoid scope creep and try to change as little as possible
    in order to deliver your initial adoption of Kubernetes as quickly as possible.
    It is important to not try to deliver too many of the promises of containerization
    in one go, as they will hold your adoption back, and may indeed lead to failure
    of your whole project.
  prefs: []
  type: TYPE_NORMAL
- en: Try to consider the environment you are currently deploying your applications
    to and aim to replicate its facilities at first, later adding additional functionality.
    Many of the tools and procedures that we discuss in the rest of this book might
    indeed be optional for your Kubernetes cluster, items that you can add at a later
    date to provide additional valuable services, but not to be viewed as blockers
    to adoption.
  prefs: []
  type: TYPE_NORMAL
- en: If you have the opportunity to reduce the scope of the infrastructure your Kubernetes
    deployment provides at the time of your additional roll out, you should consider
    doing so. It reduces the scope of new tools and processes that your organization
    needs to understand. And it will give you the opportunity to focus on that topic
    in greater detail at a later time, with reference to the operational experience
    you will have gained running your applications on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Consider log management as an example of this—if your current procedure is to
    log into servers with SSH and tail log files, you can provide the same functionality
    to operators of your Kubernetes cluster with the `kubectl logs` command. Implementing
    a solution to aggregate and search logs generated by your cluster might be desirable,
    but shouldn't necessarily be a blocker to using Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: If you currently deploy your applications onto servers running a Linux distribution
    that is readily available as a container image, you should stick with that distribution,
    rather than looking for alternatives at this stage, as your developers and operational
    staff will already be knowledgeable about how it works, and you won't have to
    invest time fixing incompatibilities. Learning to operate your applications on
    Kubernetes should be your focus, rather than learning how to configure a new operating
    system distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Planning for a successful roll out
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It can be tempting to shake up the processes and responsibilities in your organization.
    But trying to do this as part of adopting a new tool like Kubernetes can be risky.
    For example, if in your organization you have an operations team responsible for
    deploying and monitoring your applications, the point at which you adopt Kubernetes
    is not the correct time to hand this responsibility to someone else, such as your
    development team, or to attempt to automate a manual process.
  prefs: []
  type: TYPE_NORMAL
- en: This can be frustrating because, often, adoption of Kubernetes comes as part
    of wider plans to improve the processes and tooling that your organization uses.
    You should wait to successfully establish the use and operation of Kubernetes
    first. This will put you in a much better position to introduce new tools and
    processes once you have a stable foundation to build upon. You should view the
    adoption of Kubernetes as building a foundation that will be flexible enough to
    implement whatever changes to tools and processes you want to make in the future.
  prefs: []
  type: TYPE_NORMAL
- en: You will discover that implementing new tools, services, and processes becomes
    much simpler once your application infrastructure is running on Kubernetes. Once
    you have a Kubernetes cluster at your disposal, you will discover that the barriers
    to trying out a new tool are significantly reduced. Instead of spending lots of
    time planning and provisioning, you can quickly evaluate and try out a new tool
    just by submitting a new configuration to your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Discovering requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The designing requirements are shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/5a21d4ce-edfe-4d5e-9fa1-f901d1f353fc.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Availability**, **capacity**, and **performance** are key properties that
    we should consider when preparing for production. When gathering the functional
    requirements for your cluster, it can help to categorize which requirements imply
    some consideration of these properties.'
  prefs: []
  type: TYPE_NORMAL
- en: It is important to understand that it might not be possible to optimize for
    all three properties without making some trade-offs. For example, for applications
    that depend on very high network performance, AWS provides a tool called a cluster
    placement group. This ensures that the best network performance is available by
    provisioning the EC2 VMs in such a way that fast network interconnections are
    available between them (presumably by locating them in close proximity within
    an AWS data center). By provisioning instances this way, the highest network throughputs
    (above 5 GB) and lowest latencies can be achieved between machines within the
    cluster placement group. For some applications that require these levels of performance,
    this might be a worthwhile optimization.
  prefs: []
  type: TYPE_NORMAL
- en: However, since EC2 instances within a cluster placement group cannot span multiple
    availability zones, reliability of such a setup might be lower, since underlying
    power or connectivity issues could conceivably affect all the instances in a particular
    zone, especially if they are deployed in order to maximize interconnect speed.
    If your application doesn't have a requirement on such high-performance networking,
    it would indeed be unwise to trade reliability for greater performance.
  prefs: []
  type: TYPE_NORMAL
- en: Overarching these properties is a very important property for a production system—**observability**.
    Observability really describes the ability for cluster operators to understand
    what is happening to your applications. Without being able to understand if your
    applications are performing and behaving as they should, you cannot evaluate,
    improve, and evolve the design of the system. When designing your cluster, this
    is the important feedback loop that allows you to maintain and improve your cluster
    based on operational experience. If you don't consider observability when planning
    your cluster, it can be much harder to debug issues with the cluster itself and
    with your applications.
  prefs: []
  type: TYPE_NORMAL
- en: When we are discussing application requirements at a planning stage, it can
    be hard to understand what the requirements of your applications will be. Having
    good observability into the performance of your cluster, the underlying hardware,
    and the applications running on top of it, lets you make pragmatic decisions and
    be flexible enough to make changes to support your applications as you discover
    more about how they behave under production workloads and as their functionality
    is developed over time.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, perhaps the most important property to consider is security. Leaving
    the security of your cluster to the end of the planning process is a mistake.
    Remember that although security alone won't lead to the success of your project,
    failure to secure your cluster could lead to catastrophic consequences.
  prefs: []
  type: TYPE_NORMAL
- en: Recent studies and disclosures have shown that unsecured Kubernetes clusters
    have already become an attractive target to those who would exploit your computing
    power for the likes of cryptocurrency mining and other nefarious purposes, not
    to mention the potential of access to your cluster being used to access sensitive
    data held within your organization.
  prefs: []
  type: TYPE_NORMAL
- en: Security should be considered and monitored throughout the life cycle of your
    cluster; indeed, you should try and understand the security implications of each
    and every other requirement. You will need to consider how members of your organization
    need to interact with Kubernetes, as well as having a plan to ensure that you
    secure the configuration and software of your cluster and the applications that
    you run on it.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections of this chapter, we will introduce some ideas to help
    you understand the considerations that you might need to take with regard to these
    properties. Hopefully, this chapter should give you enough understanding to discover
    what your requirements are, and to begin planning your cluster for production.
    For the specific knowledge you will need to implement your plan, keep reading;
    the second half of this book is almost entirely focused on the practical knowledge
    you will need to implement your plans.
  prefs: []
  type: TYPE_NORMAL
- en: Availability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The availability is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ace2923e-7d64-418e-90af-66ec4b2d60bd.png)'
  prefs: []
  type: TYPE_IMG
- en: One of the most important things to think about when planning a production system
    is availability. It is almost always the case that we run software in order to
    provide our users with a service. If, for whatever reason, our software is not
    available to meet the requests our users put on it, then, often, we fail to meet
    their expectations. Depending on the service that your organization provides,
    unavailability could cause your users to be unhappy, inconvenienced, or even suffer
    losses or harm. Part of making an adequate plan for any production system is understanding
    how downtime or errors might affect your users.
  prefs: []
  type: TYPE_NORMAL
- en: Your definition of availability can depend on the sorts of workload that your
    cluster is running and your business requirements. A key part in planning a Kubernetes
    cluster is to understand the requirements that the users have for the services
    you are running.
  prefs: []
  type: TYPE_NORMAL
- en: Consider, for example, a batch job that emails a business report to your users
    once a day. So long as you can ensure that it runs at least once a day, at roughly
    the correct time, you can consider it 100% available, whereas a web server that
    can be accessed by your users at any time of the day or night needs to be available
    and error-free whenever your users need to access it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The CEO of your organization will be happy when they arrive at work at 9 a.m.
    with a report in their inbox ready to read. They won''t care that the task failed
    to run at midnight and was retried a few minutes later successfully. However,
    if the application server hosting the web mail application that they use to read
    email is unavailable even for a moment during the day, they may be interrupted
    and inconvenienced:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/c9aafa0a-5cfd-48aa-9ec5-0f6d085f4e19.png)A simple formula to calculate
    the availability of a service'
  prefs: []
  type: TYPE_NORMAL
- en: Generally, systems engineers consider availability of a given service to be
    the percentage of requests that were successful out of the total requests made
    to/of the service.
  prefs: []
  type: TYPE_NORMAL
- en: We can consider a batch job that fails even several times to be available. The
    request we are making of the system (that the report is emailed to the correct
    person once a day) only requires the job to be completed successfully at least
    once. If we handle failures gracefully by retrying, there is no impact on our
    users.
  prefs: []
  type: TYPE_NORMAL
- en: The exact number that you should plan for your systems to meet is, of course,
    largely a question of the needs of your users, and the priorities of your organization.
    It is worth bearing in mind, however, that systems designed for higher availability
    are almost invariably more complex and require more resources than a similar system
    where periods of downtime are acceptable. As a service approaches 100% availability,
    the cost and complexity of achieving the additional reliability increases exponentially.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you don''t already know them, it is reasonable to initiate a discussion
    about availability requirements within your organization. You should do this in
    order to set targets and understand the best ways to run your software with Kubernetes.
    Here are some questions that you should try to answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Do you know how your users are accessing your service?* For example, if your
    users are using mobile devices then connection to the internet is likely to be
    more unreliable anyway, masking the uptime (or otherwise) of your service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are migrating your service to Kubernetes, *do you know how reliable it
    currently is?*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Are you able to put a monetary value on unavailability?* For example, e-commerce
    or ad-tech organizations will know the exact amount that will be lost for a period
    of downtime.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*What levels of unavailability are your users prepared to accept?* *Do you
    have competitors?*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You might have noticed that all of these questions are about your users and
    your organization; there is no solid technical answer to any of them, but you
    need to be able to answer them to understand the requirements on the system you
    are building.
  prefs: []
  type: TYPE_NORMAL
- en: In order to provide a highly available service accessed on a network, such as
    a web server, we need to ensure that the service is available to respond to requests
    whenever required. Since we cannot ensure that the underlying machines our service
    is running upon are 100% reliable, we need to run multiple instances of our software
    and route traffic only to those instances that are able to respond to requests.
  prefs: []
  type: TYPE_NORMAL
- en: The semantics of this batch job imply that (within reason) we are not too concerned
    about the amount of time the job takes to execute, whereas the time a web server
    takes to respond is quite significant. There have been many studies that show
    even sub-second delays added to the length of time a web page takes to load have
    a significant and measurable impact on users. So, even if we are able to hide
    failures (for example, by retrying failed requests), we have much lower leeway,
    and indeed we might even consider high priority requests to have failed if they
    take longer than a particular threshold.
  prefs: []
  type: TYPE_NORMAL
- en: One reason you might choose to run your applications on Kubernetes is because
    you have heard about its self-healing properties. Kubernetes will manage our applications
    and will take action when required to ensure that our applications continue to
    run in the way that we have requested of Kubernetes. This is a helpful effect
    of Kubernetes declarative approach to configuration.
  prefs: []
  type: TYPE_NORMAL
- en: With Kubernetes, we would ask for a certain number of replicas of a service
    to be running on the cluster. The control plane is able to take action to ensure
    that this condition continues to be true even if something occurs to affect the
    running application, whether it is due a node failing or instances of an application
    being killed periodically due to a memory leak.
  prefs: []
  type: TYPE_NORMAL
- en: Contrast this to an imperative deployment procedure that relies on the operator
    to choose a particular underlying machine (or set of machines) to run an application
    on. If a machine fails, or even if an instance of the application misbehaves,
    then manual intervention is required. We want to provide our users with the services
    that they need without interruption.
  prefs: []
  type: TYPE_NORMAL
- en: For always on or latency sensitive applications such as webservers, Kubernetes
    provides mechanisms for us to run multiple replicas of our applications and to
    test the health of our services so that failing instances can be removed from
    the services or even restarted.
  prefs: []
  type: TYPE_NORMAL
- en: 'For batch jobs, Kubernetes will retry failed jobs, and will reschedule them
    to other nodes if the underlying node fails. These semantics of restarting and
    rescheduling failed applications rely on the Kubernetes control plane to function.
    Once a pod is running on a particular node, it will continue to run until the
    following happens:'
  prefs: []
  type: TYPE_NORMAL
- en: It exits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is killed by the kubelet for using too much memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The API server requests for it to be killed (perhaps to rebalance the cluster
    or to make way for a pod with a higher priority)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This means that the control plane itself can become temporarily unavailable
    without affecting the applications running on the cluster. But no pods that have
    failed, or that were running on a node that has failed, will be rescheduled until
    the control plane is available again. Clearly, you also need the API server to
    be available in order to interact with it, so the needs of your organization to
    push a new configuration to the cluster (for example, to deploy a new version
    of an application) should also be considered.
  prefs: []
  type: TYPE_NORMAL
- en: We will discuss some strategies and tools that you might use to provide a highly
    available control plane in [Chapter 7](4e7f067a-f87f-4a36-be63-eb3bde02ba81.xhtml), *A
    Production-Ready Cluster*.
  prefs: []
  type: TYPE_NORMAL
- en: Capacity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The capacity is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/c53ff269-a214-41be-9afa-eeec3728a8a6.png)'
  prefs: []
  type: TYPE_IMG
- en: Running a system such as Kubernetes means that you can respond to additional
    demand for your services literally within the time it takes for your applications
    to start up. This process can even become automated with tools such as the **Horizontal
    Pod Autoscaler** (which we we will discuss in [Chapter 8](c9c63001-5c21-4240-ac1e-c2be4659d8a0.xhtml),
    *Sorry My App Ate the Cluster*).
  prefs: []
  type: TYPE_NORMAL
- en: 'When we couple this flexibility with the ability for us to launch new EC2 instances
    at will, capacity planning is much less involved than it might have been in the
    past. Kubernetes and AWS allow us to build applications that only consume the
    amount of resources that they need to be using at any given time. Rather than
    anticipating demand for our application and pre-committing to use resources, we
    can react to the usage requirements of our applications. Kubernetes finally allows
    us to deliver one of the promises of cloud computing: the promise that we will
    only pay for the resources that we use.'
  prefs: []
  type: TYPE_NORMAL
- en: There are a few considerations that you should take into account in order to
    make the most efficient use of the resources that you pay to use on AWS.
  prefs: []
  type: TYPE_NORMAL
- en: EC2 instance types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When preparing to launch a Kubernetes cluster, you will probably be drawn into
    thinking about the type and size of the instances that will make up your cluster.
    The instances that you choose can have a big impact on the utilization, performance,
    and cost of operating a Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: When Kubernetes schedules your pods to the worker nodes in a cluster, it considers
    the resource requests and limits that are part of a pod definition.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, your pod specification will request a number of CPUs (or fractions
    thereof) and a quantity of memory. On AWS, Kubernetes uses AWS's vCPU as its unit
    of measure. A vCPU (on most instance types) is a single CPU (hyper) thread rather
    than a CPU core. If you request a fractional number of CPUs then Kubernetes allocates
    your pod a share of a vCPU. Memory is requested in bytes.
  prefs: []
  type: TYPE_NORMAL
- en: EC2 instances come in several different types that offer different ratios of
    CPU to memory.
  prefs: []
  type: TYPE_NORMAL
- en: EC2 instance types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The EC2 instance types is shown in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Category** | **Type** | **CPU to memory ratio: vCPU:GiB** | **Notes** |'
  prefs: []
  type: TYPE_TB
- en: '| Burstable | T3 | 1 CPU : 2 GiB | Provides 5-40% CPU baseline + burstable
    extra use. |'
  prefs: []
  type: TYPE_TB
- en: '| CPU optimized | C5 | 1 CPU : 2 GiB |  |'
  prefs: []
  type: TYPE_TB
- en: '| General purpose | M5 | 1 CPU : 4 GiB |  |'
  prefs: []
  type: TYPE_TB
- en: '| Memory optimized | R5 | 1 CPU : 8 GiB |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | X1 | 1 CPU : 15GiB |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | X1e | 1 CPU : 30GiB |  |'
  prefs: []
  type: TYPE_TB
- en: '| You should only consider the following instance types if you need the additional
    extra resources they provide (GPUs and/or local storage): |'
  prefs: []
  type: TYPE_TB
- en: '| GPU | P3 | 1 CPU : 7.6GiB | 1 GPU : 8 CPU (NVIDIA Tesla V100) |'
  prefs: []
  type: TYPE_TB
- en: '|  | P2 | 1 CPU : 4GiB | i. 1 GPU : 4 CPU (NVIDIA K80) |'
  prefs: []
  type: TYPE_TB
- en: '| Storage | H1 | 1 CPU : 4GiB | 2TB HDD : 8 CPU |'
  prefs: []
  type: TYPE_TB
- en: '|  | D2 | 1 CPU : 7.6GiB | 3TB HDD : 2 CPU |'
  prefs: []
  type: TYPE_TB
- en: '|  | I3 | 1 CPU : 7.6GiB | 475GiB SSD : 2 CPU |'
  prefs: []
  type: TYPE_TB
- en: When preparing a cluster, we should think about the instance types and size
    of instances that make up our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: When Kubernetes schedules our pods to the nodes in our cluster, it is of course
    aiming to pack as many containers as it can onto the cluster. This can be thwarted,
    however, if the ratio of CPU to memory requests in the majority of our pods is
    significantly different from the underlying nodes.
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider a scenario where we deploy pods that request 1 CPU and
    2 GiB of memory to our cluster. If our cluster were made up of `m5.xlarge` instances
    (4 vCPU and 16 GiB memory), each of our nodes would be able to run four pods.
    Once these four pods are running on this node, no more pods would be able to be
    scheduled to the node, but half the memory would be unused, effectively stranded.
  prefs: []
  type: TYPE_NORMAL
- en: If your workloads are quite homogeneous, of course it is quite simple to work
    out what instance type will offer the best ratio of CPU to memory to your applications.
    However, most clusters run a whole number of applications, each requiring different
    amounts of memory and CPU (and perhaps even other resources too).
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 8](c9c63001-5c21-4240-ac1e-c2be4659d8a0.xhtml), *Sorry My App Ate
    the Cluster*, we discuss using the cluster autoscaler to automatically add and
    remove instances from AWS autoscaling groups in order to size your cluster to
    match the requirements of your cluster at any given time. We also discuss how
    you can use the cluster autoscaler to scale clusters with multiple different instance
    types, in order to combat the problem of matching the ratio of CPU to memory in
    clusters where the size and shape of the workloads that are run is quite dynamic
    and can change from time to time.
  prefs: []
  type: TYPE_NORMAL
- en: Breadth versus depth
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Amazon offers many instance sizes for each family; for example, the m5 and c5
    families have six different instance sizes available, and each step up offers
    twice the resources. So, the largest instances have 48 times more resources than
    the smallest. *How should we choose what size instances to build our cluster with?*
  prefs: []
  type: TYPE_NORMAL
- en: The size of your instances limits the largest pod you can run on your cluster.
    The instance needs 10-20% larger than your largest pod to account for the overhead
    of system services, such as logging or monitoring tools, Docker, and Kubernetes
    itself.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Smaller instances will allow you to scale your cluster in smaller increments,
    increasing utilization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fewer (larger) instances may be simpler to manage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Larger instances may use a lower proportion of resources for cluster-level tasks,
    such as log shipping, and metrics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to use monitoring or logging tools, such as Datadog, Sysdig, NewRelic,
    and so on, where pricing is based on a per instance model, fewer larger instances
    may be more cost effective.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Larger instances can provide more disk and networking bandwidth, but if you
    are running more processes per instance this may not offer any advantage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Larger instance sizes are less likely to suffer from noisy neighbor issues at
    the hypervisor level.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Larger instances often imply more colocation of your pods. This is usually advantageous
    when the aim is to increase utilization, but can sometimes cause unexpected patterns
    of resource limitations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The key components of your cluster that impact performance are shown in the
    following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/7d5d9899-71d1-4f75-b644-c548bd23330a.png)'
  prefs: []
  type: TYPE_IMG
- en: Disk performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If some of your applications depend on disk performance, understanding the performance
    characteristics of EBS volumes attached to your instances can become very useful.
  prefs: []
  type: TYPE_NORMAL
- en: All of the current generation of EC2 instances relies on EBS storage. EBS storage
    is effectively a shared network attached storage, so performance can be affected
    by a number of factors.
  prefs: []
  type: TYPE_NORMAL
- en: If your cluster is running on the most recent generation of EC2 instances, you
    will be using EBS optimization. This means that dedicated bandwidth is available
    for I/O operations on your EBS volumes, effectively eliminating contention between
    EBS and other network activity.
  prefs: []
  type: TYPE_NORMAL
- en: The total maximum bandwidth available to EBS volumes is determined by the size
    of the EC2 instance. In a system where you are running multiple containers, potentially
    with one or more EBS volumes attached to each, you should have an awareness of
    this upper limit that applies to the aggregate of all volumes in use on an instance.
  prefs: []
  type: TYPE_NORMAL
- en: If you are planning to run workloads that expect to do large amounts of disk
    I/O, you may need to consider the total I/O available to the instance.
  prefs: []
  type: TYPE_NORMAL
- en: EBS provides four volume types based on two basic technologies. `gp2` and `io2`
    volumes are based on SSD, or solid-state drive, technology, while st1 and sc1
    volumes are based upon HDD, or hard disk drive technology.
  prefs: []
  type: TYPE_NORMAL
- en: This variety of disks is useful to us because, broadly, we can divide the workloads
    that your applications might deliver into two groups. Firstly, those that will
    need to make rapid random reads and/or writes to the filesystem. Workloads that
    fall into this category include databases, web servers, and boot volumes. With
    these workloads, the limiting factor for performance is usually **I/O operations
    per second** (**IOPS**). Secondly, there are workloads that need to make sequential
    reads from the disk as fast as possible. This includes applications such as Map
    Reduce, Log Management, and datastores, such as Kafka or Casandra, that have been
    specifically optimized to make sequential reads and writes as much as possible.
  prefs: []
  type: TYPE_NORMAL
- en: There are hard upper limits at the instance level to the maximum performance
    you can achieve with EBS volumes. The maximum IOPS available to all EBS volumes
    attached to a single instance is 64,000 on the largest instance size available
    on c5 and m5 instances. The smallest c5 and m5 instances only provide 1,600 IOPS.
    It is worth bearing these limits in mind, either if you want to run workloads
    requiring the higher levels of disk performance on smaller EC2 instance types
    or are using multiple EBS volumes on the larger instance types.
  prefs: []
  type: TYPE_NORMAL
- en: gp2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`gp2` EBS volumes should be your first port of call for most general-purpose
    applications. They provide **solid-state drive **(**SSD**) performance at a modest
    price point. Performance on `gp2` volumes is based on a credit system. The volumes
    provide a baseline performance, and also accrue credits over time that allow performance
    bust up to 3,000 IOPS when required until the accrued credits are exhausted.'
  prefs: []
  type: TYPE_NORMAL
- en: When a `gp2` volume is created, it automatically receives a credit balance that
    allows it to burst up to 3,000 IOPS for 30 minutes. This is very useful when a
    volume is used as a boot volume or can provide better performance where data needs
    to be copied rapidly to the volume as part of a bootstrapping procedure.
  prefs: []
  type: TYPE_NORMAL
- en: The rate at which burst credits accrue and the baseline performance of a `gp2`
    volume is proportional to the volume size. Volumes smaller than 33 GiB will always
    have a minimum baseline performance of 100 IOPS. Volumes larger than 1 TB have
    a baseline performance greater than 3,000 IOPS, so you won't need to consider
    burst credits. The maximum performance available to a single `gp2` volume is 10,000
    IOPS for volumes of 3.3 TB (and larger).
  prefs: []
  type: TYPE_NORMAL
- en: If you have a workload that requires more performance from a `gp2` volume, a
    quick fix can be to use a larger volume (even if your application does not require
    the storage it provides).
  prefs: []
  type: TYPE_NORMAL
- en: You can calculate what the maximum throughput volume will support by multiplying
    the IOPS by the block size (256 KiB). However, `gp2` volumes limit the total throughput
    to 160 MiB/s so volumes larger than 214 GiB will only provide 160 MiB/s.
  prefs: []
  type: TYPE_NORMAL
- en: Having the facility to monitor metrics as they relate to disk usage can be invaluable
    for understanding how disk performance is affecting your applications, and to
    identify if and where you are hitting performance limits.
  prefs: []
  type: TYPE_NORMAL
- en: io2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For applications where reliable performance is mission critical and `gp2` volumes
    simply cannot provide enough IOPS, `io2` volumes (otherwise known as provisioned
    IOPS volumes) are available. Where the instance they are attached to can support
    them, `io2` volumes can be provisioned to provide a maximum of 32,000 IOPS. When
    an `io2` instance is created, the IOPS required are specified upfront (we will
    discuss how to do this with Kubernetes in [Chapter 9](c2a3f846-f6cc-4fd1-af93-d325afeeffb6.xhtml),
    *Storing State*). The maximum IOPS that can be provisioned for a single volume
    are dependent on the size of the volume, with the ratio between IOPS and GiB of
    storage being `50:1`. Thus, in order to provision the maximum IOPS, you need to
    request a volume of at least 640 GiB.
  prefs: []
  type: TYPE_NORMAL
- en: For situations where the required number of IOPS is less than `gp2` volumes
    will support (10,000) and where the required throughput is less that 160 MiB/s,
    `gp2` volumes supporting similar performance characteristics will typically be
    less than half the price of an `io2` volume. Unless you know you have a need for
    the enhanced performance characteristics of `io2` volumes, it makes sense to stick
    to `gp2` volumes for most general-purpose use.
  prefs: []
  type: TYPE_NORMAL
- en: st1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For applications that have be optimized for sequential reads, where the primary
    performance metric to be concerned about is throughput, it might be surprising
    given the current dominance of SSDs to note that the best performance is still
    provided by spinning magnetic disks.
  prefs: []
  type: TYPE_NORMAL
- en: '**st1** (and **sc1**) volumes are the newest types of EBS volumes available
    on AWS. They have been designed to offer high throughput for workloads, such as
    Map Reduce, log processing, data warehousing, and streaming workloads, such as
    Kafka. st1 volumes offer throughputs of up to 500 MiB/s at less than half the
    cost of gp2 instances. The downside is that they support much lower IOPS and so
    offer much worse performance for random or small writes. The IOPS calculations
    that you might make for SSD are slightly different because the block size is much
    larger (1 MB versus 256 KB). So, making a small write will take just as long as
    writing a full 1 MB block (if written sequentially).'
  prefs: []
  type: TYPE_NORMAL
- en: Where your workload is correctly optimized to take advantage of the performance
    characteristics of st1 volumes, it is well worth considering their use because
    the cost is roughly half that of gp2 volumes.
  prefs: []
  type: TYPE_NORMAL
- en: Just like gp2 volumes, st1 uses a bust bucket model for performance. However,
    the accumulated credits allow the throughput to burst above the baseline performance.
    The baseline performance and rate at which credits accumulate are proportional
    to volume size. With the maximum burst performance being 500 MiB/s for volumes
    larger than 2 TiB and the maximum baseline performance being 500 MiB/s for volumes
    larger than 12.5 TiB, for volumes this size (or larger) there is no need to consider
    burst characteristics since performance is constant.
  prefs: []
  type: TYPE_NORMAL
- en: sc1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`sc1` volumes offer the lowest cost block storage available on AWS. They provide
    a similar performance profile to `st1` volumes, but roughly half as fast, and
    for half the cost. You might consider them for applications where you need to
    store and retrieve data from the filesystem, but access is more infrequent, or
    performance is not so important to you.'
  prefs: []
  type: TYPE_NORMAL
- en: '`sc1` volumes could be considered as an alternative to archival or blob storage
    systems, such as `s3`, as the cost is broadly similar, but with the advantage
    that no special libraries or tools are needed to read and write data from them
    and, of course, with much lower latencies before the data can be read and used.'
  prefs: []
  type: TYPE_NORMAL
- en: In use cases like Kafka, or log management, you might consider using `sc1` volumes
    for older data that you still need to keep in online storage, so it is available
    for immediate use, but where it is accessed less often so you want to optimize
    the cost of storage.
  prefs: []
  type: TYPE_NORMAL
- en: Networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When running distributed systems, network performance can be a key factor on
    the overall observable performance of an application.
  prefs: []
  type: TYPE_NORMAL
- en: Architectural patterns that encourage building applications where communication
    between different components is primarily through the network (for example, SOA
    and microservices) lead to applications where intra-cluster networking can be
    a performance bottleneck. Clustered datastores also can place high demands on
    intra-cluster networking, especially during write operations and when rebalancing
    a cluster during scaling or maintenance operations.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, network performance is also a factor to consider when running services
    that are exposed to the internet or other wide-area networks.
  prefs: []
  type: TYPE_NORMAL
- en: The latest generation of EC2 instance types benefit from a networking interface
    that AWS describes as enhanced networking. To benefit from this, you need to be
    running a relatively recent instance type (M5, C5, or R4) and have a special network
    driver installed for Amazon's Elastic Network Adapter. Luckily, if you are using
    an official AMI of any of the main Linux distributions, this should already be
    done for you.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can check that you have the correct drivers installed with the `modinfo`
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'If the drivers for the **Elastic Network Interface** are not installed, you
    will see something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The performance boost from enhanced networking doesn't cost anything extra to
    use, so it is something you should check is configured correctly whenever preparing
    for production. The only instances in common use that do not support enhanced
    networking are the t2 burstable instance types.
  prefs: []
  type: TYPE_NORMAL
- en: The network performance of EC2 instances is proportional to the instance size,
    with only the largest instance sizes of each instance types capable of the headline
    network throughputs of 10 or 20 GBps. Even when using the largest EC2 instance
    sizes, the headline network throughputs are only achievable when communicating
    to other instances within a cluster placement group.
  prefs: []
  type: TYPE_NORMAL
- en: 'A cluster placement group can be used to request that Amazon starts each of
    the instances you require together in a particular area in their data centers
    so the fastest speeds (and lowest latency) are available. To improve network performance,
    we can adjust two variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Increasing instance size**: This makes faster networking available to the
    instance, and also increases collocation so making localhost network calls between
    services more likely.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adding your instance to a cluster placement group**: This ensures that your
    instances are hosted physically nearby, improving network performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before taking decisions like this, you need to know that the network is really
    your performance bottleneck, because all of these choices make your cluster more
    at risk from underlying failures in AWS's infrastructure. So, unless you already
    know that your particular application will make particular demands on cluster
    networking, you shouldn't try to optimize for greater performance.
  prefs: []
  type: TYPE_NORMAL
- en: Security
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Some of the key areas that impact security are show in this diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/989337d6-4468-42f6-9a96-069308556c62.png)'
  prefs: []
  type: TYPE_IMG
- en: Securing the configuration and software that forms the infrastructure of your
    cluster is of vital importance, especially if you plan to expose the services
    you run on it to the internet.
  prefs: []
  type: TYPE_NORMAL
- en: You should consider that if you expose services to the public internet that
    have well known software vulnerabilities or configuration errors, it may only
    be a matter of hours before your services are detected by automated tools being
    used to scan for vulnerable systems.
  prefs: []
  type: TYPE_NORMAL
- en: It is important that you treat the security of your cluster as a moving target.
    This means that you, or a tool that you use, need to be aware of new software
    vulnerabilities and configuration vulnerabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Vulnerabilities with the Kubernetes software, and with the underlying operating
    system software of your hosts, will be updated and patched by the Kubernetes community
    and your operating system vendor, simply requiring the operator to have a procedure
    to apply updates as they become available.
  prefs: []
  type: TYPE_NORMAL
- en: More critical is the configuration of your environment, as the responsibility
    for validating its security and correctness falls on your shoulders alone. As
    well as taking the time to validate and test the configuration, you should treat
    the security of your configuration as a moving target. You should ensure that
    you take the time to review changes and advice in the Kubernetes change log as
    you make updates.
  prefs: []
  type: TYPE_NORMAL
- en: Always be updating
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A new minor version of Kubernetes is released approximately every three months.
    And the project can release patch-level updates to each released minor versions
    as often as once a week. The patch level updates will typically include fixes
    for more major bugs, and fixes for security issues. The Kubernetes community currently
    supports three minor versions at any one time, ending regular patch-level updates
    of the oldest supported version as each new minor version is released. This means
    that when you plan and build a cluster, you need to plan for two kinds of maintenance
    to the Kubernetes software:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Patch-level updates**: Up to several times a month:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These should maintain very close compatibility and mostly be trivial to perform.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They should be simple to perform with very little (or no) downtime.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Minor version upgrades**: Every 3 to 9 months:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You might need to make minor changes to the configuration of your cluster, when
    upgrading between minor versions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes does maintain good backwards compatibility, and has a strategy of
    deprecating config options before they are removed or changed. Just remember to
    take note of deprecation warnings in the change log and log output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are using third-party applications (or have written your own tools) that
    depend on beta or alpha APIs, you might need to update those tools before upgrading
    the cluster. Tools that only use the stable APIs should continue to work between
    minor version updates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You might need to think about the following:'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: A testing environment where you can apply updates to the Kubernetes software
    to validate any changes before you release them to your production environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Procedures or tools that will allow you to roll back any version upgrades, if
    you detect any errors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring that allows you to determine that your cluster is functioning as
    expected.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The procedures that you use to update the software on the machines that make
    up your cluster really depend on the tools that you are using.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are two main strategies that you might take—upgrading in place, and an
    immutable image-based update strategy.
  prefs: []
  type: TYPE_NORMAL
- en: In-place updates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several tools that allow you to upgrade the underlying operating system
    on the nodes of your cluster. Tools such as `unattended-upgrades` for Debian-based
    systems or `yum-cron` for Red Hat-based systems allow you to install updated packages
    on your nodes without any operator input.
  prefs: []
  type: TYPE_NORMAL
- en: This, of course, can be somewhat risky in a production environment if a particular
    update causes the system to fail.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, if you are managing a system with automatic updates, you would use
    the package manager to pin essential components, such as Kubernetes and **etcd**,
    to a particular version, and then handle upgrading these components in a more
    controlled way, perhaps with a configuration management tool, such as Puppet,
    Chef, or Ansible.
  prefs: []
  type: TYPE_NORMAL
- en: When upgrading packages like this in an automated way, a reboot of the system
    is required when certain components are updated. Tools such as the **KUbernetes
    REboot Daemon** (**Kured**), ([https://github.com/weaveworks/kured](https://github.com/weaveworks/kured))
    can watch for a signal that a particular node requires a reboot and orchestrate
    rebooting nodes across the cluster in order to maintain uptime of the services
    running on the cluster. This is achieved by first signaling the Kubernetes Scheduler
    to re-schedule workloads to other nodes and then triggering a reboot.
  prefs: []
  type: TYPE_NORMAL
- en: There is also a new breed of operating systems, such as CoreOS' Container Linux
    or Google's Container-Optimized OS, that take a slightly different approach to
    updates. These new container-optimized Linux distributions don't provide a traditional
    package manager at all, instead requiring you to run everything not in the base
    system (like Kubernetes) as a container.
  prefs: []
  type: TYPE_NORMAL
- en: These systems handle updates of the base operating system much more like the
    firmware update systems found in consumer electronics. The base root filesystem
    in these operating systems is read-only and mounted from one of two special partitions.
    This allows the system to download a new operating system image to the unused
    partition in the background. When the system is ready to be upgraded, it is rebooted
    and the new image from the second partition is mounted as the root filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: This has the advantage that if an upgrade fails or causes the system to become
    unstable, it is simple to roll back to the last version; indeed, this process
    can even become automated.
  prefs: []
  type: TYPE_NORMAL
- en: If you are using Container Linux, you can use the **Container Linux Update Operator**
    to orchestrate reboots due to OS updates ([https://github.com/coreos/container-linux-update-operator](https://github.com/coreos/container-linux-update-operator)).
    Using this tool, you can ensure that the workloads on your hosts are rescheduled
    before they are rebooted.
  prefs: []
  type: TYPE_NORMAL
- en: Immutable images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whilst there are tools to help manage upgrading your hosts in place, there are
    some advantages to be had from embracing a strategy using immutable images.
  prefs: []
  type: TYPE_NORMAL
- en: Once you are managing the applications that run on your infrastructure with
    Kubernetes, the software that needs to be installed on your node becomes standardized.
    This means that it becomes much simpler to manage updating the configuration of
    your hosts as immutable images.
  prefs: []
  type: TYPE_NORMAL
- en: This could be attractive, as it allows you to manage building and deploying
    your node software in a similar way to building application containers with Docker.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, if you take this approach, you will want to use a tool that simplifies
    building images in the AMI format and making them available for other tools to
    start new EC2 instances to replace those launched with a previous image. One such
    tool is packer.
  prefs: []
  type: TYPE_NORMAL
- en: Network security
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When running Kubernetes on AWS, there are four different layers you will need
    to configure in order to correctly secure the traffic on your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Infra-node networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order for traffic to pass between pods and services running on your cluster,
    you will need to configure the AWS group(s) applied to your nodes to allow this
    traffic. If you are using an overlay network, this typically means allowing traffic
    on a particular port, as all communication is encapsulated to pass over a single
    port (typically as UDP packets). For example, the flannel overlay network is typically
    configured to communicate through UPD on port `7890`.
  prefs: []
  type: TYPE_NORMAL
- en: When using a native VPC networking solution, such as `amazon-vpc-cni-k8s`, it
    is typically necessary to allow all traffic to pass between the nodes. The `amazon-vpc-cni-k8s`
    plugin associates multiple pod IP addresses with a single Elastic Network Interface,
    so it is not typically possible to manage infra-node networking in a more granular
    way using security groups.
  prefs: []
  type: TYPE_NORMAL
- en: Node-master networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In normal operations, the kubelet running on your nodes needs to connect to
    the Kubernetes API to discover the definitions of the pods it is expected to be
    running.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, this means allowing TCP connections to be made on port `443` from
    your worker nodes to your control plane security group.
  prefs: []
  type: TYPE_NORMAL
- en: The control plane connects to the kubelet on an API exposed on port `10250`. 
    This is needed for the `logs` and `exec` functionality.
  prefs: []
  type: TYPE_NORMAL
- en: External networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Correctly understanding what traffic from outside your cluster is allowed to
    access your nodes is a critical part of keeping your cluster secure.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, several researchers have discovered a significant number of otherwise
    secured clusters that allow unlimited access to the Kubernetes dashboard, and
    thus the cluster itself, to anyone accessing them on the internet.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, in these cases, the cluster administrator had failed to properly
    configure the dashboard to authenticate users. But had they thought carefully
    about the services that were exposed to the wider internet, these breaches may
    have been avoided. Only exposing sensitive services like this to specific IP addresses
    or to users accessing your VPC through a VPN would have provided an additional
    layer of security.
  prefs: []
  type: TYPE_NORMAL
- en: When you do want to expose a service (or an ingress controller) to the wider
    internet, the Kubernetes Load Balancer service type will configure appropriate
    security groups for you (as well as provisioning an **Elastic Load Balancer**
    (**ELB**)).
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes infra-pod networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Out of the box, Kubernetes doesn't provide any facilities for controlling the
    network access between pods running on your cluster. Any pod running on the cluster
    can connect to any other pod or service.
  prefs: []
  type: TYPE_NORMAL
- en: This might be reasonable for smaller deployments of fully-trusted applications.
    If you want to provide policies to restrict the connectivity between different
    applications running on your cluster, you will need to deploy a network plugin
    that will enforce Kubernetes networking policies, such as Calico, Romana, or WeaveNet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Whist there is a large choice of network plugins that can be used to support
    the enforcement of the Kubernetes Network policy, if you have chosen to make use
    of AWS-supported native VPC networking, it is recommended to use Calico, as this
    configuration is supported by AWS. AWS provide example configuration to deploy
    Calico alongside the `amazon-vpc-cni-k8s` plugin in their GitHub repository: [https://github.com/aws/amazon-vpc-cni-k8s](https://github.com/aws/amazon-vpc-cni-k8s).'
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes API provides the `NetworkPolicy` resource to provide policies
    to control the ingress and egress of traffic from pods. Each `NetworkPolicy` targets
    the pods that it will affect with a label selector and namespace. As the default
    is for pods to have no networking isolation, it can be useful if you wish to be
    strict to provide a default `NetworkPolicy` that will block traffic for pods that
    haven't yet been provided with a specific network policy.
  prefs: []
  type: TYPE_NORMAL
- en: Check the Kubernetes documentation for some examples of default network policies
    to allow and deny all traffic by default at [https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-policies](https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-policies).
  prefs: []
  type: TYPE_NORMAL
- en: IAM roles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes ships with some deep integrations with AWS. This means that Kubernetes
    can perform such tasks as provisioning EBS volumes and attaching them to the EC2
    instances in your cluster, setting up ELB, and configuring security groups on
    your behalf.
  prefs: []
  type: TYPE_NORMAL
- en: In order for the Kubernetes to have the access it requires to perform these
    actions, you need to provide IAM credentials to allow the control plane and the
    nodes the required amount of access.
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, the most convenient way to do this is to attach an instance profile
    associated with a relevant IAM role to grant the Kubernetes processes running
    on the instance the required permissions. You saw an example of this in [Chapter
    3](4e7f067a-f87f-4a36-be63-eb3bde02ba81.xhtml), *Reach for the Cloud*, when we
    launched a small cluster using `kubeadm`. When planning for a production cluster,
    there are a few more considerations you should plan for:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Are you running multiple clusters?* *Do you need cluster resources to be isolated?*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Do the applications running on your cluster also need to access resources
    within AWS that require authentication?*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Do the nodes in your cluster need to authenticate with the Kubernetes API
    using the AWS IAM Authenticator?* This will also apply if you are using Amazon
    EKS.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are running multiple clusters in your AWS account (for example, for production
    and staging or development environments), it is worth thinking about how you can
    tailor your IAM roles to prevent clusters from interfering with one another's
    resources.
  prefs: []
  type: TYPE_NORMAL
- en: In theory, a cluster shouldn't interfere with the resources created by another,
    but you might value the extra security that separate IAM roles for each environment
    can provide. Not sharing IAM roles between production and development or staging
    environments is good practice and can prevent configuration errors (or even bugs
    in Kubernetes) in one environment causing harm to resources associated with another
    cluster. Most resources that Kubernetes interacts with are tagged with a `kubernetes.io/cluster/<cluster
    name>` tag. With some of these resources, IAM offers the ability to restrict certain
    actions to resources matching that tag. Restricting delete actions in this way
    is one way to reduce the potential for harm.
  prefs: []
  type: TYPE_NORMAL
- en: When the applications running on your cluster need to access AWS resources,
    there are a number of ways to provide credentials to the AWS client libraries
    in order to authenticate correctly. You could supply credentials to your applications
    with secrets mounted as config files or as environment variables. But one of the
    most convenient ways to provide IAM credentials is to associate IAM roles to your
    pods using the same mechanism as instance profiles.
  prefs: []
  type: TYPE_NORMAL
- en: Tools such as `kube2iam` or `kiam` intercept calls made by the AWS client library
    to the metadata service and provide tokens depending on an annotation set on the
    pod. This allows IAM roles to be assigned as part of your normal deployment process.
  prefs: []
  type: TYPE_NORMAL
- en: '**kiam** ([https://github.com/uswitch/kiam](https://github.com/uswitch/kiam))
    and **kube2iam** ([https://github.com/jtblin/kube2iam](https://github.com/jtblin/kube2iam))
    are two similar projects designed to provide IAM credentials to Kubernetes pods.
    Both projects run as an agent on each node, adding network routes to route traffic
    destined for the AWS metadata service. kiam additionally runs a central server
    component that is responsible for requesting tokens from the AWS API and maintains
    a cache of the credentials required for all running pods. This approach is noted
    to be more reliable in production clusters and reduces IAM the permissions required
    by the node agents.'
  prefs: []
  type: TYPE_NORMAL
- en: Another advantage of using one of these tools is that it prevents the applications
    running on the cluster from using the permissions assigned to the underlying instance,
    reducing the risk that an application could errant or maliciously access resources
    providing control plane services.
  prefs: []
  type: TYPE_NORMAL
- en: Validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When setting up a cluster, there are many different choices you might make to
    configure your cluster. It is important that you have some way to quickly validate
    that your cluster will operate correctly.
  prefs: []
  type: TYPE_NORMAL
- en: This is a problem that the Kubernetes community has solved in order to certify
    that different Kubernetes distributions are *conformant*. To gain a seal of approval
    that a particular Kubernetes distribution is conformant, it is necessary for a
    set of integration tests to be run against a cluster. These tests are useful for
    a vendor supplying a pre-packaged installation of Kubernetes to prove that their
    distribution functions correctly. It is also very useful for cluster operators
    to use to quickly validate that configuration changes of software updates leave
    your cluster in an operable state.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes conformance testing is based on a number of specially automated tests
    from the Kubernetes code base. These tests are run against testing clusters as
    part of the end-to-end validation of the Kubernetes code base, and must pass before
    every change to the code base is merged in.
  prefs: []
  type: TYPE_NORMAL
- en: It certainly is possible to download the Kubernetes code base (and set up a
    Golang development environment) and configure it to run the conformance test directly.
    However, there is a tool called **Sonobuoy** that can automate this process for
    you.
  prefs: []
  type: TYPE_NORMAL
- en: Sonobuoy makes it simple to run a set of Kubernetes conformance tests on your
    clusters in a simple and standardized manner. The simplest way to get started
    with Sonobuoy is to use the hosted browser-based service at [https://scanner.heptio.com/](https://scanner.heptio.com/).
    This service gives you a manifest to submit to your cluster and then displays
    the test results once the tests have finished running. If you want to run everything
    on your own cluster, you can install a command-line tool that will let you run
    tests and collect the results yourself by following the instructions at [https://github.com/heptio/sonobuoy](https://github.com/heptio/sonobuoy).
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes conformance testing is important because it exercises a wide range
    of Kubernetes features, giving you early warning of any misconfiguration before
    you have even deployed applications to your cluster that might exercise those
    features. It can be very helpful when making changes to the configuration of your
    cluster to have a warning if your changes might affect the functionality of the
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Whilst Kubernetes conformance tests focus on testing the functionality of your
    cluster, security benchmarking checks your cluster's configuration against known
    unsafe configuration settings, ensuring that your cluster is configured to meet
    current security best practices.
  prefs: []
  type: TYPE_NORMAL
- en: The **Centre for Internet Security** publishes step-by-step checklists that
    you can manually follow to test your cluster against security best practices.
  prefs: []
  type: TYPE_NORMAL
- en: You can download a copy of these benchmarks for free at [https://www.cisecurity.org/benchmark/kubernetes/](https://www.cisecurity.org/benchmark/kubernetes/).
  prefs: []
  type: TYPE_NORMAL
- en: It can be useful to read and follow the advice in these checklists whist building
    your cluster, as it will help you to understand the reasons for a particular configuration
    value.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have set up your cluster it can be useful to automatically validate
    your configuration as you update and make changes, to avoid your configuration
    accidentally drifting away from a secure configuration.
  prefs: []
  type: TYPE_NORMAL
- en: '`kube-bench` is a tool that provides an automated way to run the CIS benchmarks
    against your cluster: [https://github.com/aquasecurity/kube-bench](https://github.com/aquasecurity/kube-bench).'
  prefs: []
  type: TYPE_NORMAL
- en: You might find it useful to also write your own integration tests that check
    that you can successfully deploy and operate some of your own applications. Tests
    like these can act as an important sanity check when rapidly developing the configuration
    for your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: There are many tools that could be used to perform tests like these. I would
    recommend whatever test automation tool that the engineers in your organization
    are already comfortable with. You could use a tool specially designed for running
    automated tests, such as cucumber, but a simple shell script that deploys an application
    to your cluster and then checks that it is accessible is a great start too.
  prefs: []
  type: TYPE_NORMAL
- en: Observability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Observability is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/e1f09642-520c-4579-809e-3ac616965ea2.png)'
  prefs: []
  type: TYPE_IMG
- en: Being able to monitor and debug a cluster is one of the most important points
    to bear in mind when designing a cluster for production. Luckily, there are a
    number of solutions for managing logs and metrics that have very good support
    for Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Logging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whenever you want to know what your applications are doing, the first thing
    most operators will think to do is to look at the logs generated by the application.
  prefs: []
  type: TYPE_NORMAL
- en: Logs are simple to understand, and they don't require any special tools to produce,
    as your application probably already supports some sort of the logging already.
  prefs: []
  type: TYPE_NORMAL
- en: Out of the box, Kubernetes allows you to view and tail the logs that your application
    is writing to standard out and standard errors. Using the `kubectl logs` command
    should be familiar to you if you have used the `docker logs` command on your own
    machine or on a server.
  prefs: []
  type: TYPE_NORMAL
- en: It is more convenient than logging into each node to view the logs generated
    by a particular container. As well as viewing the logs from a particular pod,
    `kubectl logs` can show the logs from all the pods matching a particular label
    expression.
  prefs: []
  type: TYPE_NORMAL
- en: If you need to search the logs generated by your application for a particular
    event, or if you need to see the logs generated at a particular time in the past,
    then you need to consider deploying a solution to aggregate and manage your logs.
  prefs: []
  type: TYPE_NORMAL
- en: The most widely used tool to implement this function is **Fluentd**. Fluentd
    is a very flexible tool that can be used to collect logs from a wide variety of
    sources and then push them to one or more destinations. If your organization already
    maintains or uses a third-party tool to aggregate application logs, you will almost
    certainly find a way to configure Fluentd to store the application logs from applications
    running on Kubernetes in your chosen tool. Members of the Fluentd team, and the
    wider community, maintain over 800 different plugins that support many different
    inputs, outputs, and filtering options.
  prefs: []
  type: TYPE_NORMAL
- en: 'As Fluentd is built upon the Ruby programming language, its plugins are distributed
    using the Rubygems package system. By convention, all Fluentd plugins have a name
    beginning with **fluent-plugin**, and all currently available plugins are listed
    here: [https://www.fluentd.org/plugins/all](https://www.fluentd.org/plugins/all). Because
    some of these plugins are maintained by the wider community, it is worth making
    some initial tests of a plugin you plan to use. The quality of plugins can be
    variable, depending on the stage of development a particular plugin is in and
    how often it is maintained. You can install and manage Fluentd plugins using the
    `gem install` command or control the exact versions of Fluentd plugins using the
    **bundler** tool. You can read more about installing plugins in your Fluentd installation
    here: [https://docs.fluentd.org/v1.0/articles/plugin-management](https://docs.fluentd.org/v1.0/articles/plugin-management).'
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Looking at the log output of your application can be useful if you know there
    is an issue with an application and want to debug the cause. But it is much harder
    if you don't know where in your system a problem is occurring, or if you simply
    want to assess the health of a system.
  prefs: []
  type: TYPE_NORMAL
- en: Your logs are very flexible because your applications can write any information
    in an unstructured way to a logging endpoint. This in a large system can become
    quite overwhelming, and the amount of effort required to filter and analyze this
    output can become complex.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring or metrics collection takes a different approach. By defining measurements
    that reflect the performance and operation of your system, of Kubernetes, and
    your infrastructure, you can much more quickly answer questions about the health
    and performance of your system.
  prefs: []
  type: TYPE_NORMAL
- en: Collected metrics are also one of the most useful sources for automated alerting
    systems. They can warn members of your organization about abnormal behavior in
    your applications or infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of commercial and open source tools that can be used to collect
    metrics and create alerts. The decision you take will most likely be influenced
    by your organization and the requirements you have.
  prefs: []
  type: TYPE_NORMAL
- en: As I have already said, trying to introduce too many new tools or processes
    to your organization at once can risk your success. In many cases, many monitoring
    tools already support integration with Kubernetes. If this is the case, it may
    be prudent to consider continuing to use the existing tools your organization
    is used to.
  prefs: []
  type: TYPE_NORMAL
- en: Whichever tools you choose to record metrics from your applications and from
    the cluster and the underlying infrastructure, you should think carefully about
    how to make it simple for members of your organization who are responsible for
    developing and deploying applications to surface their metrics. As part of planning
    your cluster, try writing the documentation for the procedure to expose metrics
    that should be followed by a developer deploying a new application to your cluster.
    You should aim to keep this process as simple as possible. If you need to automate
    steps of the process and provide default configuration values, you should do so
    in order to make the process simple. If the process of exporting new metrics from
    your applications is complex or requires lots of manual steps, then it becomes
    less likely that your organization's applications will expose them.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the process is simple and friction-free, it becomes much simpler to instill
    a culture of monitoring by default. If, for example, you choose to use Prometheus,
    you might document the process like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '`*` expose an endpoint `/metrics` on port `9102`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Add the annotation `"prometheus.io/scrape": true` to your pod'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this example, by configuring Prometheus with sensible defaults, exposing
    metrics from a pod becomes quick and simple for a developer. It is possible to
    expose more complex configuration for the way that Prometheus will scrape metrics,
    but by using well-known default values, it makes the setup process simpler and
    makes it simpler to include a standard Prometheus library in the application.
    Whatever system you choose to use to collect metrics, try to follow these principles
    wherever possible.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting metrics directly from application pods and the infrastructure provides
    deep and rich information about how your application is behaving. This information
    is very useful when you need to know specifics about the application and can be
    very useful for pre-empting issues. For example, metrics about disk usage could
    be used to provide alerts that warn operators about a state that could lead to
    an application failure if not addressed.
  prefs: []
  type: TYPE_NORMAL
- en: Blackbox monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whilst application-specific metrics provide deep insight that is useful for
    root cause analysis and pre-emptive alerting, Blackbox monitoring takes the opposite
    approach. By treating the application as a sealed entity, and exercising user-facing
    endpoints, you can surface the symptoms of a badly performing application. Blackbox
    monitoring can be implemented by using a tool such as the Prometheus Blackbox
    exporter. But another common pattern is to use a commercial service. The main
    advantage of this is that they typically allow you to probe applications from
    a number of locations, perhaps globally, truly exercising the full stack of infrastructure
    between your users and your applications.
  prefs: []
  type: TYPE_NORMAL
- en: Alerting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recording metrics about the state of the systems you are running on Kubernetes
    is the first stage of making your systems simple to observe. Once you have collected
    your metrics there are several ways to make the data you collect simple to act
    upon.
  prefs: []
  type: TYPE_NORMAL
- en: Most metrics collection tools offer some way to build graphs and dashboards
    for the metrics that are important to different members of your organization.
    For example, many users of Prometheus use Grafana to build dashboards to expose
    important metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Whilst a dashboard is a good way to get an idea of how a particular system or
    business process is performing, there are aspects of your system that need a more
    proactive approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'Any metrics-gathering system worth its salt will offer a way to emit alerts
    to members of your organization. However, when you gather metrics and whatever
    system you use to send alerts to your team, there are a few principles you should
    consider:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Alerts should be actionable**: When promoting a metric from a graph or gauge
    on a dashboard to an alert, make sure you only send alerts for states that need
    immediate human intervention, not merely warnings or information. Warnings or
    informational alerts belong on your dashboards, not on your pager.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Alerts should be used sparingly**: Alerts interrupt people from whatever
    they are doing at that moment: working, resting, or, worst of all, sleeping. If
    a person receives too many alerts they can be a cause of stress, and quickly become
    less effective as alert fatigue sets in and they lose their attention-grabbing
    power. When designing an alerting mechanism, you should make provision to record
    how often members of your organization are interrupted by your alerting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alerts should be directed—you should think about who should be responsible for
    a particular alert and direct it appropriately. Alerts can be directed to a number
    of systems, such as bug trackers, emails, chat systems, and even pager applications.
    It is important that the person who receives the most mission-critical alerts
    from your organization is in a position to take ownership and manage a response.
    Less important alerts might be assigned to a team or group in a bug tracking tool.
    If your organization makes use of a chat system, such as Slack, HipChat, or IRC,
    you might want to direct alerts for a particular application to a channel or room
    used by the team that develops or is responsible for the operation of that application.
    Just remember to ensure that volumes are kept to an acceptable level or your alerts
    will quickly come to be ignored by the people who need to know about them.
  prefs: []
  type: TYPE_NORMAL
- en: Tracing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tracing is the youngest member of the observability family and is thus often
    the last one an organization will choose to implement. The idea of a tracing system
    is to measure the time a single request takes to pass through your applications.
  prefs: []
  type: TYPE_NORMAL
- en: This might not expose any more interesting information than well-configured
    metrics for a monolithic application. But for larger-scale systems with a distributed,
    or *microservices* architecture, where a single request can pass through tens
    or even hundreds of separate processes, tracing can help to pinpoint exactly when
    and where performance issues are occurring.
  prefs: []
  type: TYPE_NORMAL
- en: When implementing a system to collect tracing information from your applications,
    you have a number of options.
  prefs: []
  type: TYPE_NORMAL
- en: AWS's built-in solution for tracing includes X-Ray ships with support for Java,
    Go Node.js, Python, Ruby, and .NET applications. For these technologies, adding
    distributed tracing to your applications is simply a question of adding a library
    to your applications and configuring it correctly. [https://aws.amazon.com/xray/](https://aws.amazon.com/xray/).
  prefs: []
  type: TYPE_NORMAL
- en: Competing with AWS's solution is a number of tools that are designed to work
    together under the OpenTracing banner.
  prefs: []
  type: TYPE_NORMAL
- en: OpenTracing provides client libraries for nine languages that are compatible
    with nine different open source and commercial tools designed to collect trace
    data. Because of the open nature of OpenTracing, several application frameworks,
    and infrastructure components, are choosing to add support for its trace format.
    You can find out more about OpenTracing at [http://opentracing.io](http://opentracing.io).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter has, hopefully, given you an idea of the myriad different options
    and decisions you can make when deciding to run Kubernetes in a production environment.
    Don't let the depth and breadth of the options and choices to make put you off,
    as Kubernetes is remarkably easy to get started with, especially on AWS.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will move to the practical work of getting a cluster
    set up and ready for work. We won't be able to cover all of the options, let alone
    all of the add-ons and additional tools that the community around Kubernetes has
    produced, but we will provide a stable starting point from which you can begin
    to implement your own plans.
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully, this chapter will serve as a guide for you and your team to discuss
    and plan a cluster that meets the needs of your organization. You can then start
    to implement the features and functionality that you may have identified while
    reading this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'If there is one single thing to remember when launching your own cluster: keep
    it simple, silly. Kubernetes makes it simple to add new tools to your arsenal
    whenever you need, so don''t over-complicate or over-engineer too fast. Start
    with the simplest setup that can possibly work, even if you think you need to
    add complexity later; often, you will discover that the simple solution works
    just fine.'
  prefs: []
  type: TYPE_NORMAL
- en: Take advantage of the fact that Kubernetes itself will allow you to rapidly
    evolve your infrastructure, and start small and add features and tools to your
    system when you need them, and not before!
  prefs: []
  type: TYPE_NORMAL
