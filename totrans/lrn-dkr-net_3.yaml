- en: Chapter 3. Building Your First Docker Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter describes practical examples of Docker networking, spanning multiple
    containers over multiple hosts. We will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Pipework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiple containers over multiple hosts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Towards scaling networks – introducing Open vSwitch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Networking with overlay networks – Flannel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparison of Docker networking options
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to Pipework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pipework lets you connect together containers in arbitrarily complex scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: In practical terms, it creates a legacy Linux bridge, adds a new interface to
    the container, and then attaches the interface to that bridge; containers get
    a network segment on which to communicate with each other.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple containers over a single host
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Pipework is a shell script and installing it is simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The following figure shows container communication using Pipework:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multiple containers over a single host](../images/00019.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'First, create two containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s use Pipework to connect them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This command creates a bridge, `brpipe`, on the host machine. It adds an `eth1`
    interface to the container `c1` with the IP address `192.168.1.1` and attaches
    the interface to the bridge as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This command will not create bridge `brpipe` as it already exists. It will
    add an `eth1` interface to the container `c2` and connect it to the bridge as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Now the containers are connected and will be able to ping each other as they
    are on the same subnet, `192.168.1.0/24`. Pipework provides the advantage of adding
    static IP addresses to the containers.
  prefs: []
  type: TYPE_NORMAL
- en: Weave your containers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Weave creates a virtual network that can connect Docker containers across multiple
    hosts as if they are all connected to a single switch. The Weave router itself
    runs as a Docker container and can encrypt routed traffic for transmission over
    the Internet. Services provided by application containers on the Weave network
    can be made accessible to the outside world, regardless of where those containers
    are running.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following code to install Weave:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The following figure shows multihost communication using Weave:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Weave your containers](../images/00020.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'On `$HOST1`, we run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we repeat similar steps on `$HOST2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In the container started on `$HOST1`, the following output is generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see the Weave network interface, `ethwe`, using the `ifconfig` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, in the container started on `$HOST2`, the following output is generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: So there we have it—two containers on separate hosts happily talking to each
    other.
  prefs: []
  type: TYPE_NORMAL
- en: Open vSwitch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker uses the Linux bridge `docker0` by default. However, there are cases
    where **Open vSwitch** (**OVS**) might be required instead of a Linux bridge.
    A single Linux bridge can only handle 1024 ports – this limits the scalability
    of Docker as we can only create 1024 containers, each with a single network interface.
  prefs: []
  type: TYPE_NORMAL
- en: Single host OVS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will now install OVS on a single host, create two containers, and connect
    them to an OVS bridge.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use this command to install OVS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Install the `ovs-docker` utility with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram shows the single-host OVS:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Single host OVS](../images/00021.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Creating an OVS bridge
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here, we will be adding a new OVS bridge and configuring it so that we can
    get the containers connected on a different network, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Add a port from the OVS bridge to the Docker container using the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create two Ubuntu Docker containers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Connect the container to the OVS bridge:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Test the connection between the two containers connected via an OVS bridge
    using the `ping` command. First, find out their IP addresses:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we know the IP addresses of `container1` and `container2`, we can
    ping them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Multiple host OVS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's see how to connect Docker containers on multiple hosts using OVS.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider our setup as shown in the following diagram, which contains
    two hosts, **Host 1** and **Host 2**, running Ubuntu 14.04:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multiple host OVS](../images/00022.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Install Docker and Open vSwitch on both the hosts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Install the `ovs-docker` utility:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: By default, Docker chooses a random network to run its containers in. It creates
    a bridge, `docker0`, and assigns an IP address (`172.17.42.1`) to it. So, both
    **Host 1** and **Host 2** `docker0` bridge IP addresses are the same, due to which
    it is difficult for containers in both the hosts to communicate. To overcome this,
    let's assign static IP addresses to the network, that is, `192.168.10.0/24`.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how to change the default Docker subnet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the following commands on Host 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the `br0` OVS bridge:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the tunnel to the other host and attach it to the:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the `br0` bridge to the `docker0` bridge:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following commands on Host 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the `br0` OVS bridge:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the tunnel to the other host and attach it to the:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the `br0` bridge to the `docker0` bridge:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The `docker0` bridge is attached to another bridge, `br0`. This time, it's an
    OVS bridge. This means that all traffic between the containers is routed through
    `br0` too.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we need to connect together the networks from both the hosts in
    which the containers are running. A GRE tunnel is used for this purpose. This
    tunnel is attached to the `br0` OVS bridge and, as a result, to `docker0` too.
  prefs: []
  type: TYPE_NORMAL
- en: After executing the preceding commands on both hosts, you should be able to
    ping the `docker0` bridge addresses from both hosts.
  prefs: []
  type: TYPE_NORMAL
- en: 'On Host 1, the following output is generated on using the `ping` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'On Host 2, the following output is generated on using the `ping` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Let's see how to create containers on both the hosts.
  prefs: []
  type: TYPE_NORMAL
- en: 'On Host 1, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'On Host 2, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Now we can ping `container2` from `container1`. In this way, we connect Docker
    containers on multiple hosts using Open vSwitch.
  prefs: []
  type: TYPE_NORMAL
- en: Networking with overlay networks – Flannel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Flannel is the virtual network layer that provides the subnet to each host
    for use with Docker containers. It is packaged with CoreOS but can be configured
    on other Linux OSes as well. Flannel creates the overlay by actually connecting
    itself to Docker bridge, to which containers are attached, as shown in the following
    figure. To setup Flannel, two host machines or VMs are required, which can be
    CoreOS or, more preferably, Linux OS, as shown in this figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Networking with overlay networks – Flannel](../images/00023.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The Flannel code can be cloned from GitHub and built locally, if required,
    on a different flavor of Linux OS, as shown here. It comes preinstalled in CoreOS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'CoreOS machines can be easily configured using Vagrant and VirtualBox, as per
    the tutorial mentioned in the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://coreos.com/os/docs/latest/booting-on-vagrant.html](https://coreos.com/os/docs/latest/booting-on-vagrant.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'After the machines are created and logged in to, we will find a Flannel bridge
    automatically created using the `etcd` configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The Flannel environment can be checked by viewing `subnet.env`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The Docker daemon requires to be restarted with the following commands in order
    to get the networking re-instantiated with the subnet from the Flannel bridge:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The Flannel environment for the second host can also be checked by viewing
    `subnet.env`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'A different subnet is allocated to the second host. The Docker service can
    also be restarted in this host by pointing to the Flannel bridge:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Docker containers can be created in their respective hosts, and they can be
    tested using the `ping` command in order to check the Flannel overlay network
    connectivity.
  prefs: []
  type: TYPE_NORMAL
- en: 'For Host 1, use the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'For Host 2, use the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Thus, in the preceding example, we can see the complexity that Flannel reduces
    by running the `flanneld` agent on each host, which is responsible for allocating
    a subnet lease out of preconfigured address space. Flannel internally uses `etcd`
    to store the network configuration and other details, such as host IP and allocated
    subnets. The forwarding of packets is achieved using the backend strategy.
  prefs: []
  type: TYPE_NORMAL
- en: Flannel also aims to resolve the problem of Kubernetes deployment on cloud providers
    other than GCE, where a Flannel overlay mesh network can ease the issue of assigning
    a unique IP address to each pod by creating a subnet for each server.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learnt how Docker containers communicate across multiple
    hosts using different networking options such as Weave, OVS, and Flannel. Pipework
    uses the legacy Linux bridge, Weave creates a virtual network, OVS uses GRE tunneling
    technology, and Flannel provides a separate subnet to each host in order to connect
    containers to multiple hosts. Some of the implementations, such as Pipework, are
    legacy and will become obsolete over a period of time, while others are designed
    to be used in the context of specific OSes, such as Flannel with CoreOS.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows a basic comparison of Docker networking options:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Summary](../images/00024.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the next chapter, we will discuss how Docker containers are networked when
    using frameworks such as Kubernetes, Docker Swarm, and Mesosphere.
  prefs: []
  type: TYPE_NORMAL
