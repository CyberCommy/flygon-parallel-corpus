- en: Building a Clustering Model with Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last few chapters, we covered supervised learning methods, where the
    training data is labeled with the true outcome that we would like to predict (for
    example, a rating for recommendations and class assignment for classification
    or a real target variable in the case of regression).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will consider the case where we do not have labeled data available.
    This is called **unsupervised learning**, as the model is not supervised with
    the true target label. The unsupervised case is very common in practice, since
    obtaining labeled training data can be very difficult or expensive in many real-world
    scenarios (for example, having humans label training data with class labels for
    classification). However, we would still like to learn some underlying structure
    in the data and use these to make predictions.
  prefs: []
  type: TYPE_NORMAL
- en: This is where unsupervised learning approaches can be useful. Unsupervised learning
    models are also often combined with supervised models; for example, applying unsupervised
    techniques to create new input features for supervised models.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering models are, in many ways, the unsupervised equivalent of classification
    models. With classification, we tried to learn a model that would predict which
    class a given training example belonged to. The model was essentially a mapping
    from a set of features to the class.
  prefs: []
  type: TYPE_NORMAL
- en: In clustering, we would like to segment the data such that each training example
    is assigned to a segment called a **cluster**. The clusters act much like classes,
    except that the true class assignments are unknown.
  prefs: []
  type: TYPE_NORMAL
- en: 'Clustering models have many use cases that are the same as classification;
    these include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Segmenting users or customers into different groups based on behavior characteristics
    and metadata
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grouping content on a website or products in a retail business
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding clusters of similar genes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Segmenting communities in ecology
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating image segments for use in image analysis applications such as object
    detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this chapter, we will:'
  prefs: []
  type: TYPE_NORMAL
- en: Briefly explore a few types of clustering models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extract features from the data specifically using the output of one model as
    input features for our clustering model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train a clustering model and use it to make predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply performance-evaluation and parameter-selection techniques to select the
    optimal number of clusters to use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types of clustering models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many different forms of clustering models available, ranging from
    simple to extremely complex ones. The Spark MLlib currently provides k-means clustering,
    which is among the simplest approaches available. However, it is often very effective,
    and its simplicity means it is relatively easy to understand and is scalable.
  prefs: []
  type: TYPE_NORMAL
- en: k-means clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: k-means attempts to partition a set of data points into *K* distinct clusters
    (whereÂ *K* is an input parameter for the model).
  prefs: []
  type: TYPE_NORMAL
- en: More formally, k-means tries to find clusters so as to minimize the sum of squared
    errors (or distances) within each cluster. This objective function is known as
    the **within cluster sum of squared errors** (**WCSS**).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_08_001.png)'
  prefs: []
  type: TYPE_IMG
- en: It is the sum, over each cluster, of the squared errors between each point and
    the cluster center.
  prefs: []
  type: TYPE_NORMAL
- en: 'Starting with a set of *K* initial cluster centers (which are computed as the
    mean vector for all data points in the cluster), the standard method for K-means
    iterates between two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Assign each data point to the cluster that minimizes the WCSS. The sum of squares
    is equivalent to the squared Euclidean distance; therefore, this equates to assigning
    each point to the **closest** cluster center as measured by the Euclidean distance
    metric.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the new cluster centers based on the cluster assignments from the first
    step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The algorithm proceeds until either a maximum number of iterations has been
    reached or convergence has been achieved. **Convergence** means that the cluster
    assignments no longer change during the first step; therefore, the value of the
    WCSS objective function does not change either.
  prefs: []
  type: TYPE_NORMAL
- en: For more details, refer to Spark's documentation on clustering at [http://spark.apache.org/docs/latest/mllib-clustering.html](http://spark.apache.org/docs/latest/mllib-clustering.html)
    or refer to [http://en.wikipedia.org/wiki/K-means_clustering](http://en.wikipedia.org/wiki/K-means_clustering).
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate the basics of K-means, we will use the simple dataset we showed
    in our multiclass classification example in [Chapter 6](7bd5bfd3-6301-49dc-ba28-5d6553b57e01.xhtml),
    *Building a Classification Model with Spark*. Recall that we have five classes,
    which are shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_08_002.png)'
  prefs: []
  type: TYPE_IMG
- en: Multiclass dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'However, assume that we don''t actually know the true classes. If we use k-means
    with five clusters, then after the first step, the model''s cluster assignments
    might look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_08_003.png)'
  prefs: []
  type: TYPE_IMG
- en: Cluster assignments after the first K-means iteration
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that k-means has already picked out the centers of each cluster
    fairly well. After the next iteration, the assignments might look like those shown
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_08_004.png)'
  prefs: []
  type: TYPE_IMG
- en: Cluster assignments after the second K-means iteration
  prefs: []
  type: TYPE_NORMAL
- en: 'Things are starting to stabilize, but the overall cluster assignments are broadly
    the same as they were after the first iteration. Once the model has converged,
    the final assignments could look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_08_005.png)'
  prefs: []
  type: TYPE_IMG
- en: Final cluster assignments for K-means
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the model has done a decent job of separating the five clusters.
    The leftmost three are fairly accurate (with a few incorrect points). However,
    the two clusters in the bottom-right corner are less accurate.
  prefs: []
  type: TYPE_NORMAL
- en: 'This illustrates:'
  prefs: []
  type: TYPE_NORMAL
- en: The iterative nature of K-means
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model's dependency on the method of initially selecting clusters' centers
    (here, we will use a random approach)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That the final cluster assignments can be very good for well-separated data
    but can be poor for data that is more difficult
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Initialization methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The standard initialization method for k-means, usually simply referred to as
    the random method, starts by randomly assigning each data point to a cluster before
    proceeding with the first update step.
  prefs: []
  type: TYPE_NORMAL
- en: Spark ML provides a parallel variant for this initialization method, called
    **K-means ++**, which is the default initialization method used.
  prefs: []
  type: TYPE_NORMAL
- en: See [http://en.wikipedia.org/wiki/K-means_clustering#Initialization_methods](http://en.wikipedia.org/wiki/K-means_clustering)
    and [http://en.wikipedia.org/wiki/K-means%2B%2B](http://en.wikipedia.org/wiki/K-means%2B%2B)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'The results of using K-means++ are shown here. Note that this time, the difficult
    lower-right points have been mostly correctly clustered:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_08_006.png)'
  prefs: []
  type: TYPE_IMG
- en: Final cluster assignments for K-means++
  prefs: []
  type: TYPE_NORMAL
- en: There are many other variants of K-means; they focus on initialization methods
    or the core model. One of the more common variants is **fuzzy K-means**. This
    model does not assign each point to one cluster as K-means does (a so-called hard
    assignment). Instead, it is a soft version of K-means, where each point can belong
    to many clusters, and is represented by the relative membership to each cluster.
    So, for *K* clusters, each point is represented as a K-dimensional membership
    vector, with each entry in this vector indicating the membership proportion in
    each cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Mixture models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **mixture model** is essentially an extension of the idea behind fuzzy K-means;
    however, it makes an assumption that there is an underlying probability distribution
    that generates the data. For example, we might assume that the data points are
    drawn from a set of K-independent Gaussian (normal) probability distributions.
    The cluster assignments are also soft, so each point is represented by *K* membership
    weights in each of the *K* underlying probability distributions.
  prefs: []
  type: TYPE_NORMAL
- en: See [http://en.wikipedia.org/wiki/Mixture_model](http://en.wikipedia.org/wiki/Mixture_model)
    for further details and for a mathematical treatment of mixture models.
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Hierarchical clustering** is a structured clustering approach that results
    in a multilevel hierarchy of clusters, where each cluster might contain many subclusters
    (or child clusters). Each child cluster is, thus, linked to the parent cluster.
    This form of clustering is often also called **tree clustering**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Agglomerative clustering is a bottom-up approach where:'
  prefs: []
  type: TYPE_NORMAL
- en: Each data point begins in its own cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The similarity (or distance) between each pair of clusters is evaluated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pair of clusters that are most similar are found; this pair is then merged
    to form a new cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The process is repeated until only one top-level cluster remains
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Divisive clustering** is a top-down approach that works in reverse, starting
    with one cluster and at each stage, splitting a cluster into two, until all data
    points are allocated to their own bottom-level cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Top-down clustering** is more complex than bottom-up clustering since a second,
    flat clustering algorithm is needed as a "subroutine''''. Top-down clustering
    has the advantage of being more efficient if we do not generate a complete hierarchy
    to individual document leaves.'
  prefs: []
  type: TYPE_NORMAL
- en: You can find more information at [http://en.wikipedia.org/wiki/Hierarchical_clustering](http://en.wikipedia.org/wiki/Hierarchical_clustering).
  prefs: []
  type: TYPE_NORMAL
- en: Extracting the right features from your data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like most of the machine learning models we have encountered so far, k-means
    clustering requires numerical vectors as input. The same feature extraction and
    transformation approaches that we have seen for classification and regression
    are applicable for clustering.
  prefs: []
  type: TYPE_NORMAL
- en: As k-means, like least squares regression, uses a squared error function as
    the optimization objective, it tends to be impacted by outliers and features with
    large variance.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering could be leveraged to detect outliers as they can cause a lot of
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: As for regression and classification cases, input data can be normalized and
    standardized to overcome this, which might improve accuracy. In some cases, however,
    it might be desirable not to standardize data, if, for example, the objective
    is to find segmentations according to certain specific features.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting features from the MovieLens dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use the ALS algorithm to get numerical features for users and items
    (movies) in this case before we can use the clustering algorithm on the data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First we load the data `u.data` into a DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we split it into a 80:20 ratio to get the training and test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We instantiate the `ALS` class, set the maximum iterations at `5`, and the
    regularization parameter at `0.01`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we create a model, followed by calculating predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This is followed by calculating `userFactors` and `itemFactors`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We convert them into libsvmformat and persist them in a file. Notice that we
    are persisting all the features as well two features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of `movie_lens_items_libsvm` will look like the following listing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we persist the top two features (with maximum variation) and persist
    them in a file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of `movie_lens_items_xy` will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we calculate the libsvm format forÂ `userFactors`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of `movie_lens_users_libsvm` will look like the following listing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we extract the top two features and persist them in a file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of `movie_lens_user_xy` will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We will need *xy* features to do clustering for two features so that we can
    create a two-dimensional plot.
  prefs: []
  type: TYPE_NORMAL
- en: K-means - training a clustering model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training for K-means in Spark ML takes an approach similar to the other models
    -- we pass a DataFrame that contains our training data to the fit method of the
    `KMeans` object.
  prefs: []
  type: TYPE_NORMAL
- en: Here we use the libsvm data format.
  prefs: []
  type: TYPE_NORMAL
- en: Training a clustering model on the MovieLens dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will train a model for both the movie and user factors that we generated
    by running our recommendation model.
  prefs: []
  type: TYPE_NORMAL
- en: We need to pass in the number of clusters *K* and the maximum number of iterations
    for the algorithm to run. Model training might run for less than the maximum number
    of iterations if the change in the objective function from one iteration to the
    next is less than the tolerance level (the default for this tolerance is 0.0001).
  prefs: []
  type: TYPE_NORMAL
- en: Spark ML's k-means provides random and K-means || initialization, with the default
    being K-means ||. As both of these initialization methods are based on random
    selection to some extent, each model training run will return a different result.
  prefs: []
  type: TYPE_NORMAL
- en: K-means does not generally converge to a global optimum model, so performing
    multiple training runs and selecting the most optimal model from these runs is
    a common practice. MLlib's training methods expose an option to complete multiple
    model training runs. The best training run, as measured by the evaluation of the
    loss function, is selected as the final model.
  prefs: []
  type: TYPE_NORMAL
- en: 'First we create a `SparkSession` instance and use it to load theÂ `movie_lens_users_libsvm`
    data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we create a model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we train a K-means model using a dataset of user vectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '**K-means**: Making predictions using a clustering model.'
  prefs: []
  type: TYPE_NORMAL
- en: Using the trained K-means model is straightforward and similar to the other
    models we have encountered so far, such as classification and regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can make predictions for multiple inputs by passing a DataFrame to the transform
    method of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting output is a cluster assignment for each data point in the prediction
    column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Due to random initialization, the cluster assignments might change from one
    run of the model to another, so your results might differ from those shown earlier.
    The cluster IDs themselves have no inherent meaning; they are simply arbitrarily
    labeled, starting from 0.
  prefs: []
  type: TYPE_NORMAL
- en: K-means - interpreting cluster predictions on the MovieLens dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have covered how to make predictions for a set of input vectors, but how
    do we evaluate how good the predictions are? We will cover performance metrics
    a little later; however, here, we will see how to manually inspect and interpret
    the cluster assignments made by our k-means model.
  prefs: []
  type: TYPE_NORMAL
- en: While unsupervised techniques have the advantage that they do not require us
    to provide labeled data for training, the disadvantage is that, often, the results
    need to be manually interpreted. Often, we would like to further examine the clusters
    that are found and possibly try to interpret them and assign some sort of labeling
    or categorization to them.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we can examine the clustering of movies we have found to try to
    see whether there is some meaningful interpretation of each cluster, such as a
    common genre or theme among the movies in the cluster. There are many approaches
    we can use, but we will start by taking a few movies in each cluster that are
    closest to the center of the cluster. These movies, we assume, would be the ones
    that are least likely to be marginal in terms of their cluster assignment, and
    so, they should be among the most representative of the movies in the cluster.
    By examining these sets of movies, we can see what attributes are shared by the
    movies in each cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting the movie clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will try to list the movie associated with each cluster by joining the dataset
    with the movie name with the prediction output dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Interpreting the movie clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we review the code where we get the predictions for each label
    and save them in a text file and draw a two-dimensional scatter plot.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will create two scatter plots, one for users and the other for items (which
    is movies in this case):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/image_08_007.png)'
  prefs: []
  type: TYPE_IMG
- en: K-means clustering with user data
  prefs: []
  type: TYPE_NORMAL
- en: The preceding figure shows K-means clusters for user data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_08_008.png)'
  prefs: []
  type: TYPE_IMG
- en: K-means clustering plot with item data
  prefs: []
  type: TYPE_NORMAL
- en: The preceding figure shows K-means clusters for item data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_08_009.png)'
  prefs: []
  type: TYPE_IMG
- en: K-means plotting effect of number of clusters
  prefs: []
  type: TYPE_NORMAL
- en: The preceding figure shows K-means clusters for user data with two features
    and one iteration.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_08_010.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding figure shows K-means clusters for user data with two features
    and 10 iterations. Notice how the cluster boundaries are moving.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_08_011.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding figure shows K-means clusters for user data with two features
    and 10 iterations. Notice how cluster boundaries are moving.
  prefs: []
  type: TYPE_NORMAL
- en: K-means - evaluating the performance of clustering models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With models such as regression, classification, and recommendation engines,
    there are many evaluation metrics that can be applied to clustering models to
    analyze their performance and the goodness of the clustering of the data points.
    Clustering evaluation is generally divided into either internal or external evaluation.
    Internal evaluation refers to the case where the same data used to train the model
    is used for evaluation. External evaluation refers to using data external to the
    training data for evaluation purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Internal evaluation metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Common internal evaluation metrics include the WCSS we covered earlier (which
    is exactly the k-means objective function), the Davies-Bouldin Index, the Dunn
    Index, and the silhouette coefficient. All these measures tend to reward clusters
    where elements within a cluster are relatively close together, while elements
    in different clusters are relatively far away from each other.
  prefs: []
  type: TYPE_NORMAL
- en: The Wikipedia page on clustering evaluation at [http://en.wikipedia.org/wiki/Cluster_analysis#Internal_evaluation](http://en.wikipedia.org/wiki/Cluster_analysis)
    has more details.
  prefs: []
  type: TYPE_NORMAL
- en: External evaluation metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since clustering can be thought of as unsupervised classification, if we have
    some form of labeled (or partially labeled) data available, we could use these
    labels to evaluate a clustering model. We can make predictions of clusters (that
    is, the class labels) using the model and evaluate the predictions against the
    true labels using metrics similar to some that we saw for classification evaluation
    (that is, based on true positive and negative and false positive and negative
    rates).
  prefs: []
  type: TYPE_NORMAL
- en: These include the Rand measure, F-measure, Jaccard index, and others.
  prefs: []
  type: TYPE_NORMAL
- en: See [http://en.wikipedia.org/wiki/Cluster_analysis#External_evaluation](http://en.wikipedia.org/wiki/Cluster_analysis)
    for more information on external evaluation for clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Computing performance metrics on the MovieLens dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Spark ML provides a convenient `computeCost` function to compute the WSSS objective
    function given a DataFrame. We will compute this metric for the following item
    and user training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This should output the result similar to the following one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The best way to measure effectiveness of WSSSE is to plot against iterations
    as shown in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Effect of iterations on WSSSE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let us find out the effect of iterations on WSSSE for the MovieLens dataset.
    We will calculate WSSSE for various values of iterations and plot the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code listing is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us plot these numbers to get a better idea:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_08_012.png)'
  prefs: []
  type: TYPE_IMG
- en: Users WSSSE versus iterations
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_08_013.png)'
  prefs: []
  type: TYPE_IMG
- en: Items WSSSE versus iterations
  prefs: []
  type: TYPE_NORMAL
- en: Bisecting KMeans
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is a variation of generic KMeans.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reference: [http://www.siam.org/meetings/sdm01/pdf/sdm01_05.pdf](http://www.siam.org/meetings/sdm01/pdf/sdm01_05.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps of the algorithm are:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize by randomly selecting a point, sayÂ ![](img/B05184_08_x11.png) then
    compute the centroid *w* of *M* and compute:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*![](img/B05184_08_x2.png)*'
  prefs: []
  type: TYPE_NORMAL
- en: The **centroid** is the center of the cluster. A centroid is a vector containing
    one number for each variable, where each number is the mean of a variable for
    the observations in that cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Divide *M =[x1, x2, ... xn]*Â into two, sub-clusters *M[L]* and *M[R]*, according
    to the following rule:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B05184_08_x3.png)'
  prefs: []
  type: TYPE_IMG
- en: Compute the centroids of *M[L]* and *M[R]*, *w[L]* and *w[R]*, as in step 2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: IfÂ *w[L] = c[L]* and *w[R] = c[R]*, stop.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Otherwise, let c[L]= w[L] c[R] = w[R] , go to step 2.
  prefs: []
  type: TYPE_NORMAL
- en: Bisecting K-means - training a clustering model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Training for bisecting K-means in Spark ML involves taking an approach similar
    to the other models -- we pass a DataFrame that contains our training data to
    the fit method of the `KMeans` object. Note that here we use the libsvm data format:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instantiate the cluster object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the command `show(3)` is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the `BisectingKMeans` object and set the parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Show movies by cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us print the movies segregated by cluster number:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us calculate the WSSSE:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we run the predictions for the items:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Source code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_08/scala/2.0.0/src/main/scala/org/sparksamples/kmeans/BisectingKMeans.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_08/scala/2.0.0/src/main/scala/org/sparksamples/kmeans/BisectingKMeans.scala)'
  prefs: []
  type: TYPE_NORMAL
- en: Plot the user and item clusters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As a next step, let us take two features and plot the user and item clusters
    with their respective clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/image_08_014.png)'
  prefs: []
  type: TYPE_IMG
- en: Plotting of MovieLens user data with clusters
  prefs: []
  type: TYPE_NORMAL
- en: The preceding plot shows whatÂ user clusters look like for two features.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_08_015.png)'
  prefs: []
  type: TYPE_IMG
- en: Plotting of MovieLens item (movies) data with clusters
  prefs: []
  type: TYPE_NORMAL
- en: The preceding plot shows whatÂ item clusters look like for two features.
  prefs: []
  type: TYPE_NORMAL
- en: WSSSE and iterations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we evaluate the effect or iterations on WSSSE for bisecting
    the K-means algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The source code is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/image_08_016.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Plot: WSSSE versus iterations for users'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_08_017.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Plot: WSSSE versus iterations for items in the case of bisecting K-means'
  prefs: []
  type: TYPE_NORMAL
- en: It is clear that the algorithm reaches optimal WSSSE by 20 iterations for both
    users and items.
  prefs: []
  type: TYPE_NORMAL
- en: Gaussian Mixture Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A mixture model is a probabilistic model of a sub-population within a population.
    These models are used to make statistical inferences about a sub-population, given
    the observations of pooled populations.
  prefs: []
  type: TYPE_NORMAL
- en: A **Gaussian Mixture Model** (**GMM**) is a mixture model represented as a weighted
    sum of Gaussian component densities. Its model coefficients are estimated from
    training data using the iterative **Expectation-Maximization** (**EM**) algorithm
    or **Maximum A Posteriori** (**MAP**) estimation from a trained model.
  prefs: []
  type: TYPE_NORMAL
- en: The `spark.ml` implementation uses the EM algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'It has the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**k**: Number of desired clusters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**convergenceTol**: Maximum change in log-likelihood at which one considers
    convergence achieved'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**maxIterations**: Maximum number of iterations to perform without reaching
    convergence'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**initialModel**: Optional starting point from which to start the EM algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (if this parameter is omitted, a random starting point will be constructed from
    the data)
  prefs: []
  type: TYPE_NORMAL
- en: Clustering using GMM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will create clusters for both users and items (movies in this case) to get
    a better understanding of how the algorithm groups users and items.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the `libsvm` file for users.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a Gaussian Mixture instance. The instance has the following parameters
    which can be configured:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'In our case, we will be setting only the number of Gaussian distributions and
    seed number:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a user model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The full code listing is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Plotting the user and item data with GMM clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we look at how GMM-based cluster boundaries move as the number
    of iterations increase:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_08_018.png)'
  prefs: []
  type: TYPE_IMG
- en: Plot of MovieLens user data clusters assigned by GMM
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_08_019.png)'
  prefs: []
  type: TYPE_IMG
- en: Plot of MovieLens item data clusters assigned by GMM
  prefs: []
  type: TYPE_NORMAL
- en: GMM - effect of iterations on cluster boundaries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let us look at how cluster boundaries change as the number of iterations increase
    for GMM:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_08_020.png)'
  prefs: []
  type: TYPE_IMG
- en: Cluster plot for GMM for user data with one iteration
  prefs: []
  type: TYPE_NORMAL
- en: The preceding figure plots GMM clusters for user data with one iteration.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_08_021.png)'
  prefs: []
  type: TYPE_IMG
- en: Cluster plot for GMM for user data with 10 iterations
  prefs: []
  type: TYPE_NORMAL
- en: The preceding figure plots GMM clusters for user data with 10 iterations.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_08_022.png)'
  prefs: []
  type: TYPE_IMG
- en: Cluster plot for GMM for user data with 20 iterations
  prefs: []
  type: TYPE_NORMAL
- en: The preceding figure plots GMM clusters for user data with 20 iterations.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored a new class of model that learns structures from
    unlabeled data -- unsupervised learning. We worked through the required input
    data and feature extraction, and saw how to use the output of one model (a recommendation
    model in our example) as the input to another model (our k-means clustering model).
    Finally, we evaluated the performance of the clustering model, using both manual
    interpretation of the cluster assignments and using mathematical performance metrics.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will cover another type of unsupervised learning used
    to reduce our data down to its most important features or components -- dimensionality
    reduction models.
  prefs: []
  type: TYPE_NORMAL
