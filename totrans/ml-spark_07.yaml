- en: Building a Regression Model with Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will build on what we covered in [Chapter 6](7bd5bfd3-6301-49dc-ba28-5d6553b57e01.xhtml),
    *Building a Classification Model with Spark*. While classification models deal
    with outcomes that represent discrete classes, regression models are concerned
    with target variables that can take any real value. The underlying principle is
    very similar--we wish to find a model that maps input features to predicted target
    variables. Like classification, regression is also a form of supervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Regression models can be used to predict just about any variable of interest.
    A few examples include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Predicting stock returns and other economic variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting loss amounts for loan defaults (this can be combined with a classification
    model that predicts the probability of default, while the regression model predicts
    the amount in the case of a default)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recommendations (the Alternating Least Squares factorization model from [Chapter
    5](d3bf76a8-26be-4db7-8310-b936d220407e.xhtml), *Building a Recommendation Engine
    with Spark*, uses linear regression in each iteration)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting **customer lifetime value** (**CLTV**) in a retail, mobile, or other
    business, based on user behavior and spending patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the different sections of this chapter, we will do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduce the various types of regression models available in ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explore feature extraction and target variable transformation for regression
    models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train a number of regression models using ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See how to make predictions using the trained models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Investigate the impact on performance of various parameter settings for regression
    using cross-validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types of regression models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The core idea of linear models (or generalized linear models) is that we model
    the predicted outcome of interest (often called the target or dependent variable)
    as a function of a simple linear predictor applied to the input variables (also
    referred to as features or independent variables).
  prefs: []
  type: TYPE_NORMAL
- en: '*y = f(w^Tx)*'
  prefs: []
  type: TYPE_NORMAL
- en: Here, *y* is the target variable, *w* is the vector of parameters (known as
    the weight vector), and *x* is the vector of input features.
  prefs: []
  type: TYPE_NORMAL
- en: '*w^Tx* is the linear predictor (or vector dot product) of the weight vector
    *w* and feature vector *x*. To this linear predictor, we applied a function *f*
    (called the link function).'
  prefs: []
  type: TYPE_NORMAL
- en: Linear models can, in fact, be used for both classification and regression simply
    by changing the link function. Standard linear regression uses an identity link
    (that is, *y = w^Tx* directly), while binary classification uses alternative link
    functions as discussed here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark''s ML library offers different regression models, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generalized linear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistical regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forest regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient boosted trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Survival regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Isotonic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ridge regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression models define the relationship between a dependent variable and one
    or more independent variables. It builds the best model that fits the values of
    independent variables or features.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression unlike classification models such as support vector machines
    and logistic regression is used for predicting the value of a dependent variable
    with generalized value rather than predicting the exact class label.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression models are essentially the same as their classification counterparts,
    the only difference is that linear regression models use a different loss function,
    related link function, and decision function. Spark ML provides a standard least
    squares regression model (although other types of generalized linear models for
    regression are planned).
  prefs: []
  type: TYPE_NORMAL
- en: Least squares regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You might recall from [Chapter 6](7bd5bfd3-6301-49dc-ba28-5d6553b57e01.xhtml),
    *Building a Classification Model with Spark*, that there are a variety of loss
    functions that can be applied to generalized linear models. The loss function
    used for least squares is the squared loss, which is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*½ (w^Tx - y)²*'
  prefs: []
  type: TYPE_NORMAL
- en: Here, as for the classification setting, *y* is the target variable (this time,
    real valued), *w* is the weight vector, and *x* is the feature vector.
  prefs: []
  type: TYPE_NORMAL
- en: The related link function is the identity link, and the decision function is
    also the identity function, as generally, no thresholding is applied in regression.
    So, the model's prediction is simply *y = w^Tx*.
  prefs: []
  type: TYPE_NORMAL
- en: The standard least squares regression in ML library does not use regularization.
    Regularization is used to solve the problem of overfitting. Looking at the squared
    loss function, we can see that the loss applied to incorrectly predicted points
    will be magnified since the loss is squared. This means that least squares regression
    is susceptible to outliers in the dataset, and also to over-fitting. Generally,
    as for classification, we should apply some level of regularization in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression with L2 regularization is commonly referred to as ridge regression,
    while applying L1 regularization is called the lasso.
  prefs: []
  type: TYPE_NORMAL
- en: When the dataset is small, or the number of examples is fewer, the tendency
    of the model to over fit is very high, therefore, it is highly recommended to
    use regularizers like L1, L2, or elastic net.
  prefs: []
  type: TYPE_NORMAL
- en: See the section on linear least squares in the Spark MLlib documentation at
    [http://spark.apache.org/docs/latest/mllib-linear-methods.html#linear-least-squares-lasso-and-ridge-regression](http://spark.apache.org/docs/latest/mllib-linear-methods.html#linear-least-squares-lasso-and-ridge-regression)
    for further information.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees for regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Just like using linear models for regression tasks involves changing the loss
    function used, using decision trees for regression involves changing the measure
    of the node impurity used. The impurity metric is called **variance**, and is
    defined in the same way as the squared loss for least squares linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: See the *MLlib - Decision Tree* section in the Spark documentation at [http://spark.apache.org/docs/latest/mllib-decision-tree.html](http://spark.apache.org/docs/latest/mllib-decision-tree.html)
    for further details on the decision tree algorithm and impurity measure for regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will plot a simple example of a regression problem with only one input
    variable shown on the *x* axis and the target variable on the *y* axis. The linear
    model prediction function is shown by a red-dashed line, while the decision tree
    prediction function is shown by a green-dashed line. We can see that the decision
    tree allows a more complex, nonlinear model to be fitted to the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_07_001.png)'
  prefs: []
  type: TYPE_IMG
- en: Evaluating the performance of regression models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We saw in [Chapter 6](7bd5bfd3-6301-49dc-ba28-5d6553b57e01.xhtml), *Building
    a Classification Model with Spark*, that evaluation methods for classification
    models typically focus on measurements related to predicted class memberships
    relative to the actual class memberships. These are binary outcomes (either the
    predicted class is correct or incorrect), and it is less important whether the
    model just barely predicted correctly or not; what we care most about is the number
    of correct and incorrect predictions.
  prefs: []
  type: TYPE_NORMAL
- en: When dealing with regression models, it is very unlikely that our model will
    precisely predict the target variable, because the target variable can take on
    any real value. However, we would naturally like to understand how far away our
    predicted values are from the true values, so will we utilize a metric that takes
    into account the overall deviation.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the standard evaluation metrics used to measure the performance of regression
    models include the **Mean Squared Error** (**MSE**) and **Root Mean Squared Error**
    (**RMSE**), the **Mean Absolute Error** (**MAE**), the R-squared coefficient,
    and many others.
  prefs: []
  type: TYPE_NORMAL
- en: Mean Squared Error and Root Mean Squared Error
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'MSE is the average of the squared error that is used as the loss function for
    least squares regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_07_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: It is the sum, over all the data points, of the square of the difference between
    the predicted and actual target variables, divided by the number of data points.
  prefs: []
  type: TYPE_NORMAL
- en: RMSE is the square root of MSE. MSE is measured in units that are the square
    of the target variable, while RMSE is measured in the same units as the target
    variable. Due to its formulation, MSE, just like the squared loss function that
    it derives from, effectively penalizes larger errors more severely.
  prefs: []
  type: TYPE_NORMAL
- en: In order to evaluate our predictions based on the mean of an error metric, we
    will first make predictions for each input feature vector in an RDD of `LabeledPoint`
    instances by computing the error for each record using a function that takes the
    prediction and true target value as inputs. This will return a `[Double]` RDD
    that contains the error values. We can then find the average using the mean method
    of RDDs that contain double values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s define our squared error function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Mean Absolute Error
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'MAE is the average of the absolute differences between the predicted and actual
    targets and is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_07_003.png)'
  prefs: []
  type: TYPE_IMG
- en: MAE is similar in principle to MSE, but it does not punish large deviations
    as much.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our function to compute MAE is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Root Mean Squared Log Error
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This measurement is not as widely used as MSE and MAE, but it is used as the
    metric for the Kaggle competition that uses the bike-sharing dataset. It is, effectively,
    the RMSE of the log-transformed predicted and target values. This measurement
    is useful when there is a wide range in the target variable, and you do not necessarily
    want to penalize large errors when the predicted and target values are themselves
    high. It is also effective when you care about percentage errors rather than the
    absolute value of errors.
  prefs: []
  type: TYPE_NORMAL
- en: The Kaggle competition evaluation page can be found at [https://www.kaggle.com/c/bike-sharing-demand/details/evaluation](https://www.kaggle.com/c/bike-sharing-demand/details/evaluation).
  prefs: []
  type: TYPE_NORMAL
- en: 'The function to compute RMSLE is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The R-squared coefficient
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The R-squared coefficient, also known as the coefficient of determination, is
    a measure of how well a model fits a dataset. It is commonly used in statistics.
    It measures the degree of variation in the target variable; this is explained
    by the variation in the input features. An R-squared coefficient generally takes
    a value between 0 and 1, where 1 equates to a perfect fit of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting the right features from your data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the underlying models for regression are the same as those for the classification
    case, we can use the same approach to create input features. The only practical
    difference is that the target is now a real-valued variable as opposed to a categorical
    one. The `LabeledPoint` class in ML library already takes this into account, as
    the `label` field is of the `Double` type, so it can handle both cases.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting features from the bike sharing dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To illustrate the concepts in this chapter, we will be using the bike sharing
    dataset. This dataset contains hourly records of the number of bicycle rentals
    in the capital bike sharing system. It also contains variables related to date,
    time, weather, seasonal, and holiday information.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset is available at [http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset](http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset).
  prefs: []
  type: TYPE_NORMAL
- en: Click on the Data Folder link, and then download the `Bike-Sharing-Dataset.zip`
    file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The bike sharing data was enriched with weather and seasonal data by Hadi Fanaee-T
    at the University of Porto and used in the following paper:'
  prefs: []
  type: TYPE_NORMAL
- en: Fanaee-T, Hadi and Gama Joao, Event labeling combining ensemble detectors and
    background knowledge, *Progress in Artificial Intelligence*, pp. 1-15, Springer
    Berlin Heidelberg, 2013.
  prefs: []
  type: TYPE_NORMAL
- en: The paper is available at [http://link.springer.com/article/10.1007%2Fs13748-013-0040-3](http://link.springer.com/article/10.1007%2Fs13748-013-0040-3).
  prefs: []
  type: TYPE_NORMAL
- en: Once you have downloaded the `Bike-Sharing-Dataset.zip` file, unzip it. This
    will create a directory called `Bike-Sharing-Dataset`, which contains the `day.csv`,
    `hour.csv`, and the `Readme.txt` files.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Readme.txt` file contains information on the dataset, including the variable
    names and descriptions. Take a look at the file, and you will see that we have
    the following variables available:'
  prefs: []
  type: TYPE_NORMAL
- en: '`instant`: This is the record ID'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dteday`: This is the raw date'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`season`: This refers to the different seasons such as spring, summer, winter,
    and fall'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`yr`: This is the year (2011 or 2012)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mnth`: This is the month of the year'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hr`: This is the hour of the day'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`holiday`: This shows whether the day was a holiday or not'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weekday`: This is the day of the week'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`workingday`: This refers to whether the day was a working day or not'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weathersit`: This is a categorical variable that describes the weather at
    a particular time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`temp`: This is the normalized temperature'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`atemp`: This is the normalized apparent temperature'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hum`: This is the normalized humidity'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`windspeed`: This is the normalized wind speed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cnt`: This is the target variable, that is, the count of bike rentals for
    that hour'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will work with the hourly data contained in `hour.csv`. If you look at the
    first line of the dataset, you will see that it contains the column names as header.
    The following code snippet prints the header and the top 20 records:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code snippet should output the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/image_07_004.png)'
  prefs: []
  type: TYPE_IMG
- en: We will be using Scala to demonstrate the examples in this chapter. The source
    code for the chapter can be found at the location [https://github.com/ml-resources/spark-ml/tree/branch-ed2/Chapter_07](https://github.com/ml-resources/spark-ml/tree/branch-ed2/Chapter_07).
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll start as usual by loading the dataset and inspecting it; from the previous
    dataframe, get the record count as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This should output the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: So, we have `17,379` hourly records in our dataset. We have inspected the column
    names already. We will ignore the record ID and raw date columns. We will also
    ignore the `casual` and `registered` count target variables, and focus on the
    overall count variable, `cnt` (which is the sum of the other two counts). We are
    left with 12 variables. The first eight are categorical, while the last four are
    normalized real-valued variables.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This last bit of code should output the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'All the columns are casted to double; the following snippet shows how this
    is done:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code should output the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The bike sharing dataset is categorical in nature, and needs to be processed
    using **Vector Assembler** and **Vector Indexer** as described next:'
  prefs: []
  type: TYPE_NORMAL
- en: Vector Assembler is a transformer that combines a list of columns into a single
    vector column. It combines raw features into a feature vector in order to train
    ML models like linear regression and decision trees.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vector Indexer indexes categorical features, passed from Vector Assembler in
    this case. It automatically decides which features are categorical, and converts
    actual values to category indices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In our case, all the columns in df2 except `label` are converted by `VectorAssembler`
    into `rawFeatures`.
  prefs: []
  type: TYPE_NORMAL
- en: Given an input column of type `Vector` and a `param` called `maxCategories`,
    it decides which features should be categorical based on distinct values, where
    features with at most `maxCategories` are declared categorical.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The complete code-listing is available at [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_07/scala/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/regression/bikesharing/BikeSharingExecutor.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_07/scala/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/regression/bikesharing/BikeSharingExecutor.scala).
  prefs: []
  type: TYPE_NORMAL
- en: Training and using regression models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training for regression models follows the same procedure as for classification
    models. We simply pass the training data to the relevant train method.
  prefs: []
  type: TYPE_NORMAL
- en: BikeSharingExecutor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `BikeSharingExecutor` object can be used to choose and run the respective
    regression model, for example, to run `LinearRegression` and execute the linear
    regression pipeline, set the program argument as `LR_<type>`, where `type` is
    the data format; for other commands, refer to the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The code-listing can be found at this link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_07/scala/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/regression/bikesharing/BikeSharingExecutor.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_07/scala/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/regression/bikesharing/BikeSharingExecutor.scala)'
  prefs: []
  type: TYPE_NORMAL
- en: Training a regression model on the bike sharing dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Linear regression is the most commonly used algorithm. At the core of the regression
    analysis is the task of fitting a single line through a data plot. Linear equation
    is described by *y = c + b*x*, where *y* = estimated dependent, *c* = constant,
    *b* = regression coefficients, and *x* = independent variable.
  prefs: []
  type: TYPE_NORMAL
- en: Let's train the bike sharing dataset by splitting it into 80% training and 20%
    testing, use `LinearRegression` with the regression evaluator from Spark to build
    the model, and get evaluation metrics around the test data. The `linearRegressionWithVectorFormat`
    method uses categorical data, whereas `linearRegressionWithSVMFormat` uses the
    `libsvm` format of the `Bike-sharing` dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This preceding code should show the following output. Please note that residual
    stands for the expression Residuals: (label-predicted value)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The code-listing can be found at [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_07/scala/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/regression/bikesharing/LinearRegressionPipeline.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_07/scala/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/regression/bikesharing/LinearRegressionPipeline.scala).
  prefs: []
  type: TYPE_NORMAL
- en: Generalized linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear regression follows a Gaussian distribution, whereas, **generalized linear
    models** (**GLMs**) are specifications of linear models where the response variable
    `Y` follows some distribution from the exponential family of distributions.
  prefs: []
  type: TYPE_NORMAL
- en: Let's train the bike sharing dataset by splitting it into 80 % training and
    20% testing, use `GeneralizedLinearRegression` with regression evaluator from
    Spark to build the model, and get evaluation metrics around the test data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This should output the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: Standard error of estimated coefficients and intercept.
  prefs: []
  type: TYPE_NORMAL
- en: If `[GeneralizedLinearRegression.fitIntercept]` is set to true, then the last
    element returned corresponds to the intercept.
  prefs: []
  type: TYPE_NORMAL
- en: 'Coefficient Standard Errors in the previous code are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'T-statistic of estimated coefficients and intercept is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The two-sided p-value of estimated coefficients and intercept is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The dispersion is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The dispersion of the fitted model is taken as 1.0 for the "binomial" and "poisson"
    families, and otherwise estimated by the residual Pearson's Chi-Squared statistic
    (which is defined as the sum of the squares of the Pearson residuals) divided
    by the residual degrees of freedom.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Null deviance output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Residual degree of freedom is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: In logistic regression analysis, deviance is used in lieu of the sum of squares
    calculations. Deviance is analogous to the sum of squares calculations in linear
    regression, and is a measure of the lack of fit to the data in a logistic regression
    model. When a "saturated" model is available (a model with a theoretically perfect
    fit), deviance is calculated by comparing a given model with the saturated model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deviance: `3.886235458383082E8`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Reference: [https://en.wikipedia.org/wiki/Logistic_regression](https://en.wikipedia.org/wiki/Logistic_regression)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Degrees of freedom**:'
  prefs: []
  type: TYPE_NORMAL
- en: The concept of degrees of freedom is central to the principle of estimating
    statistics of populations from samples of them. "Degrees of freedom" is commonly
    abbreviated to df.
  prefs: []
  type: TYPE_NORMAL
- en: 'Think of df as a mathematical restriction that needs to be put in place when
    estimating one statistic from an estimate of another. The preceding code will
    give the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The **Akaike information criterion** (**AIC**) is a measure of the relative
    quality of statistical models for a given set of data. Given a collection of models
    for the data, AIC estimates the quality of each model relative to each of the
    other models. Hence, AIC provides a means for model selection.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reference : [https://en.wikipedia.org/wiki/Akaike_information_criterion](https://en.wikipedia.org/wiki/Akaike_information_criterion)'
  prefs: []
  type: TYPE_NORMAL
- en: 'AIC for the fitted model output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The complete code listing is available at this link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_07/scala/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/regression/bikesharing/GeneralizedLinearRegressionPipeline.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_07/scala/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/regression/bikesharing/GeneralizedLinearRegressionPipeline.scala)'
  prefs: []
  type: TYPE_NORMAL
- en: Decision tree regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The decision tree model is a powerful, non-probabilistic technique, which can
    capture more complex nonlinear patterns and feature interactions. They have been
    shown to perform well on many tasks, are relatively easy to understand and interpret,
    can handle categorical and numerical features, and do not require input data to
    be scaled or standardized. They are well-suited to be included in ensemble methods
    (for example, ensembles of decision tree models, which are called decision forests).
  prefs: []
  type: TYPE_NORMAL
- en: The decision tree algorithm is a top-down approach, which begins at a root node
    (or feature), and then selects a feature at each step that gives the best split
    of the dataset, as measured by the information gain of this split. The information
    gain is computed from the node impurity (which is the extent to which the labels
    at the node are similar, or homogenous) minus the weighted sum of the impurities
    for the two child nodes that would be created by the split.
  prefs: []
  type: TYPE_NORMAL
- en: Let's train the bike sharing dataset by splitting it into 80 % training and
    20% testing, use `DecisionTreeRegression` with regression evaluator from Spark
    to build the model, and get evaluation metrics around the test data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This should output the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Please refer to the previous section (Generalized Linear Regression) to learn
    how to interpret the results.
  prefs: []
  type: TYPE_NORMAL
- en: The code listing is available at [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_07/scala/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/regression/bikesharing/DecisionTreeRegressionPipeline.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_07/scala/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/regression/bikesharing/DecisionTreeRegressionPipeline.scala).
  prefs: []
  type: TYPE_NORMAL
- en: Ensembles of trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The ensemble method is a machine learning algorithm, which creates a model
    composed of a set of other base models. Spark machine learning supports two major
    ensemble algorithms: `RandomForest` and `GradientBoostedTrees`.'
  prefs: []
  type: TYPE_NORMAL
- en: Random forest regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Random forests are known as ensembles of decision trees formed by combining
    many decision trees. Like decision trees, random forests can handle categorical
    features, support multiclass, and don't require feature scaling.
  prefs: []
  type: TYPE_NORMAL
- en: Let's train the bike sharing dataset by splitting it into 80 % training and
    20% testing, use `RandomForestRegressor` with Regression Evaluator from Spark
    to build the model, and get evaluation metrics around the test data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This should output the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code uses various features and their values to create a decision
    tree.
  prefs: []
  type: TYPE_NORMAL
- en: The code listing can be found at [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_07/scala/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/regression/bikesharing/RandomForestRegressionPipeline.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_07/scala/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/regression/bikesharing/RandomForestRegressionPipeline.scala).
  prefs: []
  type: TYPE_NORMAL
- en: Gradient boosted tree regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gradient boosted trees are ensembles of decision trees. Gradient boosted trees
    iteratively train decision trees to minimize the loss function. Gradient boosted
    trees handle categorical features, support multiclass, and don't require feature
    scaling.
  prefs: []
  type: TYPE_NORMAL
- en: Spark ML implements gradient boosted trees using the existing decision tree
    implementation. It supports both classification and regression.
  prefs: []
  type: TYPE_NORMAL
- en: Let's train the bike sharing dataset by splitting it into 80% training and 20%
    testing, use GBTRegressor with regression evaluator from Spark to build the model,
    and get evaluation metrics around the test data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This should output the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The code listing is available at [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_07/scala/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/regression/bikesharing/GradientBoostedTreeRegressorPipeline.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_07/scala/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/regression/bikesharing/GradientBoostedTreeRegressorPipeline.scala).
  prefs: []
  type: TYPE_NORMAL
- en: Improving model performance and tuning parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 6](7bd5bfd3-6301-49dc-ba28-5d6553b57e01.xhtml), *Building a Classification
    Model with Spark*, we showed how feature transformation and selection can make
    a large difference to the performance of a model. In this chapter, we will focus
    on another type of transformation that can be applied to a dataset: transforming
    the target variable itself.'
  prefs: []
  type: TYPE_NORMAL
- en: Transforming the target variable
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recall that many machine learning models, including linear models, make assumptions
    regarding the distribution of the input data as well as target variables. In particular,
    linear regression assumes a normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: In many real-world cases, the distributional assumptions of linear regression
    do not hold. In this case, for example, we know that the number of bike rentals
    can never be negative. This alone should indicate that the assumption of normality
    might be problematic. To get a better idea of the target distribution, it is often
    a good idea to plot a histogram of the target values.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now create a plot of the target variable distribution in the following
    piece of code:'
  prefs: []
  type: TYPE_NORMAL
- en: Scala
  prefs: []
  type: TYPE_NORMAL
- en: The code for plotting raw data can be found at [https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_07/scala/1.6.2/scala-spark-app/src/main/scala/org/sparksamples/PlotRawData.scala](https://github.com/ml-resources/spark-ml/blob/branch-ed2/Chapter_07/scala/1.6.2/scala-spark-app/src/main/scala/org/sparksamples/PlotRawData.scala).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot for the preceding output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_07_005.png)'
  prefs: []
  type: TYPE_IMG
- en: One way in which we might deal with this situation is by applying a transformation
    to the target variable such that we take the logarithm of the target value instead
    of the raw value. This is often referred to as log-transforming the target variable
    (this transformation can also be applied to feature values).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will apply a log transformation to the following target variable, and plot
    a histogram of the log-transformed values using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: Scala
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot for the preceding output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_07_006.png)'
  prefs: []
  type: TYPE_IMG
- en: A second type of transformation that is useful in the case of target values
    that do not take on negative values, and, in addition, might take on a very wide
    range of values, is to take the square root of the variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will apply the square root transform in the following code, once more plotting
    the resulting target variable distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: From the plots of the log and square root transformations, we can see that both
    result in a more even distribution relative to the raw values. While they are
    still not normally distributed, they are a lot closer to a normal distribution
    when compared to the original target variable.
  prefs: []
  type: TYPE_NORMAL
- en: Impact of training on log-transformed targets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, does applying these transformations have any impact on model performance?
    Let's evaluate the various metrics we used previously on the log-transformed data
    as an example.
  prefs: []
  type: TYPE_NORMAL
- en: We will do this first for the linear model by applying the log function to the
    `label` field of each `LabeledPoint` RDD. Here, we will only transform the target
    variable, and we will not apply any transformations to the features.
  prefs: []
  type: TYPE_NORMAL
- en: We will then train a model on this transformed data, and form the RDD of predicted
    versus true values.
  prefs: []
  type: TYPE_NORMAL
- en: Note that now that we have transformed the target variable, the predictions
    of the model will be on the log scale, as will the target values of the transformed
    dataset. Therefore, in order to use our model and evaluate its performance, we
    must first transform the log data back into the original scale by taking the exponent
    of both the predicted and true values using the `numpy exp` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we will compute the MSE, MAE, and RMSLE metrics for the model:'
  prefs: []
  type: TYPE_NORMAL
- en: Scala
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code will be similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The code listing is available at this link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/ml-resources/spark-ml/tree/branch-ed2/Chapter_07/scala/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/regression/
    linearregression/LinearRegressionWithLog.scala](https://github.com/ml-resources/spark-ml/tree/branch-ed2/Chapter_07/scala/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/regression/%20linearregression/LinearRegressionWithLog.scala)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/ml-resources/spark-ml/tree/branch-ed2/Chapter_07/scala/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/regression/
    linearregression/LinearRegression.scala](https://github.com/ml-resources/spark-ml/tree/branch-ed2/Chapter_07/scala/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/regression/%20linearregression/LinearRegression.scala)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we compare these preceding results to the results on the raw target variable,
    we see that all three values became worse.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Tuning model parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far in this chapter, we have illustrated the concepts of model training and
    evaluation for MLlib's regression models by training and testing on the same dataset.
    We will now use a cross-validation approach similar to what we used previously
    to evaluate the effect of different parameter settings on the performance of our
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Creating training and testing sets to evaluate parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step is to create a test and training set for cross-validation purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Scala, the split is easier to implement, and the `randomSplit` function
    is available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Splitting data for Decision tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The final step is to apply the same approach to the features extracted for the
    decision tree model.
  prefs: []
  type: TYPE_NORMAL
- en: Scala
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The impact of parameter settings for linear models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have prepared our training and test sets, we are ready to investigate
    the impact of the different parameter settings on model performance. We will first
    carry out this evaluation for the linear model. We will create a convenience function
    to evaluate the relevant performance metric by training the model on the training
    set, and evaluating it on the test set for different parameter settings.
  prefs: []
  type: TYPE_NORMAL
- en: We will use the RMSLE evaluation metric, as it is the one used in the Kaggle
    competition with this dataset, and this allows us to compare our model results
    against the competition leaderboard to see how we perform.
  prefs: []
  type: TYPE_NORMAL
- en: 'The evaluation function is defined here:'
  prefs: []
  type: TYPE_NORMAL
- en: Scala
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Note that in the following sections, you might get slightly different results
    due to some random initialization for SGD. However, your results will be comparable.
  prefs: []
  type: TYPE_NORMAL
- en: Iterations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we saw when evaluating our classification models, we generally expect that
    a model trained with SGD will achieve better performance as the number of iterations
    increases, although the increase in performance will slow down as the number of
    iterations goes above some minimum number. Note that here, we will set the step
    size to 0.01 to better illustrate the impact at higher iteration numbers.
  prefs: []
  type: TYPE_NORMAL
- en: 'We implemented the same in Scala with different values of iterations, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'For the scala implementation, we use JfreeChart''s scala version. Implementation
    reaches minimum RMSLE at 20 iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot for the preceding output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_07_007.png)'
  prefs: []
  type: TYPE_IMG
- en: Step size
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will perform a similar analysis for step size in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: Scala
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Output for the previous code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot for the preceding output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_07_008.png)'
  prefs: []
  type: TYPE_IMG
- en: Now we can see why we avoided using the default step size when training the
    linear model originally. The default is set to *1.0*, which, in this case, results
    in a `nan` output for the RMSLE metric. This typically means that the SGD model
    has converged to a very poor local minimum in the error function that it is optimizing.
    This can happen when the step size is relatively large, as it is easier for the
    optimization algorithm to overshoot good solutions.
  prefs: []
  type: TYPE_NORMAL
- en: We can also see that for low step sizes and a relatively low number of iterations
    (we used 10 here), the model performance is slightly poorer. However, in the preceding
    *Iterations* section, we saw that for the lower step-size setting, a higher number
    of iterations will generally converge to a better solution.
  prefs: []
  type: TYPE_NORMAL
- en: Generally speaking, setting step size and number of iterations involves a trade-off.
    A lower step size means that convergence is slower, but slightly more assured.
    However, it requires a higher number of iterations, which is more costly in terms
    of computation and time, particularly, at a very large scale.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the best parameter settings can be an intensive process, which involves
    training a model on many combinations of parameter settings and selecting the
    best outcome. Each instance of model training involves a number of iterations,
    so this process can be very expensive and time consuming when performed on very
    large datasets. Model initialization also has an impact on the results, both on
    reaching global minima, or reaching a sub-optimal local minima in the gradient
    descent graph.
  prefs: []
  type: TYPE_NORMAL
- en: L2 regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 6](7bd5bfd3-6301-49dc-ba28-5d6553b57e01.xhtml), *Building a Classification
    Model with Spark*, we saw that regularization has the effect of penalizing model
    complexity in the form of an additional loss term that is a function of the model
    weight vector. L2 regularization penalizes the L2-norm of the weight vector, while
    L1 regularization penalizes the L1-norm.
  prefs: []
  type: TYPE_NORMAL
- en: We expect training set performance to deteriorate with increasing regularization,
    as the model cannot fit the dataset well. However, we would also expect some amount
    of regularization that will result in optimal generalization performance as evidenced
    by the best performance on the test set.
  prefs: []
  type: TYPE_NORMAL
- en: L1 regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can apply the same approach for differing levels of L1 regularization, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, the results are more clearly seen when plotted in the following graph.
    We see that there is a much more subtle decline in RMSLE, and it takes a very
    high value to cause a jump back up. Here, the level of L1 regularization required
    is much higher than that for the L2 form; however, the overall performance is
    poorer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Using L1 regularization can encourage sparse weight vectors. Does this hold
    true in this case? We can find out by examining the number of entries in the weight
    vector that are zero, with increasing levels of regularization.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: We can see from the results that, as we might expect, the number of zero feature
    weights in the model weight vector increases as greater levels of L1 regularization
    are applied.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Intercept
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The final parameter option for the linear model is whether to use an intercept
    or not. An intercept is a constant term that is added to the weight vector, and
    effectively accounts for the mean value of the target variable. If the data is
    already centered or normalized, an intercept is not necessary; however, it often
    does not hurt to use one in any case.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will evaluate the effect of adding an intercept term to the model here:'
  prefs: []
  type: TYPE_NORMAL
- en: Scala
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot for the preceding output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_07_009.png)'
  prefs: []
  type: TYPE_IMG
- en: As can be seen in the preceding image, the RMSLE value for intercept=true is
    slightly higher as compared to intercept=false.
  prefs: []
  type: TYPE_NORMAL
- en: The impact of parameter settings for the decision tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Decision trees provide two main parameters: maximum tree depth and maximum
    number of bins. We will now perform the same evaluation of the effect of parameter
    settings for the decision tree model. Our starting point is to create an evaluation
    function for the model, similar to the one used for the linear regression earlier.
    This function is provided here:'
  prefs: []
  type: TYPE_NORMAL
- en: Scala
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Tree depth
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We would generally expect performance to increase with more complex trees (that
    is, trees of greater depth). Having a lower tree depth acts as a form of regularization,
    and it might be the case that as with L2 or L1 regularization in linear models,
    there is a tree depth that is optimal with respect to the test set performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we will try to increase the depth of trees to see what impact they have
    on the test set RMSLE, keeping the number of bins at the default level of `32`:'
  prefs: []
  type: TYPE_NORMAL
- en: Scala
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot for the preceding output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_07_010.png)'
  prefs: []
  type: TYPE_IMG
- en: Maximum bins
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, we will perform our evaluation on the impact of setting the number
    of bins for the decision tree. As with the tree depth, a larger number of bins
    should allow the model to become more complex, and might help performance with
    larger feature dimensions. After a certain point, it is unlikely that it will
    help any more, and might, in fact, hinder performance on the test set due to over-fitting.
  prefs: []
  type: TYPE_NORMAL
- en: Scala
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot for the preceding output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_07_011.png)'
  prefs: []
  type: TYPE_IMG
- en: The impact of parameter settings for the Gradient Boosted Trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gradient boosted trees have two main parameter iterations and maximum depth.
    We are going to make variations in these and see the effects.
  prefs: []
  type: TYPE_NORMAL
- en: Iterations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scala
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot for the preceding output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_07_012.png)'
  prefs: []
  type: TYPE_IMG
- en: MaxBins
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next we look at how changing the maximum number of bins affects the RMSLE values.
  prefs: []
  type: TYPE_NORMAL
- en: Scala
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us look at the sample implementation in Scala. We will calculate the RMSLE
    value for maximum number of bins: `10`, `16`, `32`, and `64`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot for the preceding output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_07_013.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you saw how to use ML Library's linear model, decision tree,
    gradient boosted trees, Ridge Regression, and the isotonic regression functionality
    in Scala within the context of regression models. We explored categorical feature
    extraction, and the impact of applying transformations to the target variable
    in a regression problem. Finally, we implemented various performance-evaluation
    metrics, and used them to implement a cross-validation exercise that explores
    the impact of the various parameter settings available in both linear models and
    decision trees on test set model performance.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will cover a different approach to machine learning,
    that is, unsupervised learning, specifically in clustering models.
  prefs: []
  type: TYPE_NORMAL
