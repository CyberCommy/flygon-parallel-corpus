- en: 'Chapter 3. Testing 1, 2, 3: Basic Unit Testing'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, we began learning about testing Django applications
    by writing some doctests for the `Survey` model. In the process, we experienced
    some of the advantages and disadvantages of doctests. When discussing some of
    the disadvantages, unit tests were mentioned as an alternative test approach that
    avoids some doctest pitfalls. In this chapter, we will start to learn about unit
    tests in detail. Specifically, we will:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Re-implement the `Survey` doctests as unit tests
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assess how the equivalent unit test version compares to the doctests in terms
    of ease of implementation and susceptibility to the doctest caveats discussed
    in the previous chapter
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Begin learning some of the additional capabilities of unit tests as we extend
    the existing tests to cover additional functions
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unit tests for the Survey save override method
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recall in the previous chapter that we ultimately implemented four individual
    tests of the `Survey` save override function:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: A straightforward test of the added capability, which verifies that if `closes`
    is not specified when a `Survey` is created, it is auto-set to a week after `opens`
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A test that verifies that this auto-set operation is not performed if `closes`
    is explicitly specified during creation
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A test that verifies that `closes` is only auto-set if its value is missing
    during initial creation, not while saving an existing instance
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A test that verifies that the `save` override function does not introduce an
    unexpected exception in the error case where neither `opens` nor `closes` is specified
    during creation
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To implement these as unit tests instead of doctests, create a `TestCase` within
    the `suvery/tests.py` file, replacing the sample `SimpleTest`. Within the new
    `TestCase` class, define each individual test as a separate test method in that
    `TestCase`, like so:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This is more difficult to implement than the doctest version, isn't it? It is
    not possible to use a direct cut-and-paste from a shell session, and there is
    a fair amount of code overhead—code that does not appear anywhere in the shell
    session—that needs to be added. We can still use cut-and-paste from our shell
    session as a starting point, but we must edit the code after pasting it, in order
    to turn the pasted code into a proper unit test. Though not difficult, this can
    be tedious.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 'Most of the extra work consists of choosing names for the individual test methods,
    minor editing of cut-and-pasted code to refer to class variables such as `t` and
    `sd` correctly, and creating the appropriate test assertions to verify the expected
    result. The first of these requires the most brainpower (choosing good names can
    be hard), the second is trivial, and the third is fairly mechanical. For example,
    in a shell session where we had:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In the unit test, we instead have an `assertEqual`:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Expected exceptions are similar, but use `assertRaises`. For example, where
    in a shell session we had:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In the unit test, this is:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note we do not actually call the `create` routine in our unit test code, but
    rather leave that up to the code within `assertRaises`. The first parameter passed
    to `assertRaises` is the expected exception, followed by the callable expected
    to raise the exception, followed by any parameters that need to be passed to the
    callable when calling it.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Pros of the unit test version
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'What do we get from this additional work? Right off the bat, we get a little
    more feedback from the test runner, when running at the highest verbosity level.
    For the doctest version, the output of `manage.py test survey -v2` was simply:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'For the unit test version, we get individual results reported for each test
    method:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'If we take a little more effort and provide single-line docstrings for our
    test methods, we can get even more descriptive results from the test runner. For
    example, if we add docstrings like so:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The test runner output for this test will then be:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This additional descriptive detail may not be that important when all tests
    pass, but when they fail, it can be very helpful as a clue to what the test is
    trying to accomplish.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这种额外的描述性细节在所有测试通过时可能并不那么重要，但当测试失败时，它可能非常有助于作为测试试图实现的线索。
- en: 'For example, let''s assume we have broken the `save` override method by neglecting
    to add seven days to `opens`, so that if `closes` is not specified, it is auto-set
    to the same value as `opens`. With the doctest version of the test, the failure
    would be reported as:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们已经破坏了`save`覆盖方法，忽略了向`opens`添加七天，因此如果未指定`closes`，它将自动设置为与`opens`相同的值。使用测试的doctest版本，失败将被报告为：
- en: '[PRE9]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'That doesn''t give much information on what has gone wrong, and you really
    have to go read the full test code to see what is even being tested. The same
    failure reported by the unit test is a bit more descriptive, as the `FAIL` header
    includes the test docstring, so we immediately know the problem has something
    to do with `closes` being auto-set:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这并没有提供有关出了什么问题的详细信息，您真的必须阅读完整的测试代码才能看到正在测试什么。与单元测试报告的相同失败更具描述性，因为`FAIL`标题包括测试文档字符串，因此我们立即知道问题与`closes`的自动设置有关：
- en: '[PRE10]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can take this one step further and make the error message a bit friendlier
    by specifying our own error message on the call to `assertEqual`:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以进一步迈出一步，通过在调用`assertEqual`时指定自己的错误消息，使错误消息更友好：
- en: '[PRE11]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The reported failure would then be:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然后报告的失败将是：
- en: '[PRE12]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In this case, the custom error message may not be much more useful than the
    default one, since what the `save` override is supposed to do here is quite simple.
    However, such custom error messages can be valuable for more complicated test
    assertions to help explain what is being tested, and the "why" behind the expected
    result.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，自定义错误消息可能并不比默认消息更有用，因为这里`save`覆盖应该做的事情非常简单。然而，对于更复杂的测试断言，这样的自定义错误消息可能是有价值的，以帮助解释正在测试的内容以及预期结果背后的“为什么”。
- en: 'Another benefit of unit tests is that they allow for more selective test execution
    than doctests. On the `manage.py test` command line, one or more unit tests to
    be executed can be identified by `TestCase` name. You can even specify that only
    particular methods in a `TestCase` should be run. For example:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 单元测试的另一个好处是，它们允许比doctests更有选择性地执行测试。在`manage.py test`命令行上，可以通过`TestCase`名称标识要执行的一个或多个单元测试。甚至可以指定只运行`TestCase`中的特定方法。例如：
- en: '[PRE13]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Here we are indicating that we only want to run the `testClosesAutoset` test
    method in the `SurveySaveTest` unit test found in the `survey` application. Being
    able to run just a single method or a single test case is a very convenient time
    saver when developing tests.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们指示只想在`survey`应用程序中找到的`SurveySaveTest`单元测试中运行`testClosesAutoset`测试方法。在开发测试时，能够仅运行单个方法或单个测试用例是非常方便的时间节省器。
- en: Cons of the unit test version
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单元测试版本的缺点
- en: 'Has anything been lost by switching to unit tests? A bit. First, there is the
    ease of implementation that has already been mentioned: unit tests require more
    work to implement than doctests. Though generally not difficult work, it can be
    tedious. It is also work where errors can be made, resulting in a need to debug
    the test code. This increased implementation burden can serve to discourage writing
    comprehensive tests.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 切换到单元测试是否有所损失？有一点。首先，已经提到的实施便利性：单元测试需要比doctests更多的工作来实施。虽然通常不是困难的工作，但可能会很乏味。这也是可能出现错误的工作，导致需要调试测试代码。这种增加的实施负担可能会阻止编写全面的测试。
- en: We've also lost the nice property of having tests right there with the code.
    This was mentioned in the previous chapter as one negative effect of moving some
    doctests out of docstrings and into the `__test__` dictionary in `tests.py`. The
    effect is worse with unit tests since all unit tests are usually kept in files
    separate from the code being tested. Thus there are usually no tests to be seen
    right near the code, which again may discourage writing tests. With unit tests,
    unless a methodology such as a test-driven development is employed, the "out of
    sight, out of mind" effect may easily result in test-writing becoming an afterthought.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还失去了将测试与代码放在一起的好处。在上一章中提到，这是将一些doctests从文档字符串移出并放入`tests.py`中的`__test__`字典的一个负面影响。由于单元测试通常保存在与被测试的代码分开的文件中，因此通常看不到靠近代码的测试，这可能会阻止编写测试。使用单元测试时，除非采用测试驱动开发等方法，否则“视而不见”效应很容易导致编写测试成为事后想法。
- en: Finally, we've lost the built-in documentation of the doctest version. This
    is more than just the potential for automatically-generated documentation from
    docstrings. Doctests are often more readable than unit tests, where extraneous
    code that is just test overhead can obscure what the test is intending to test.
    Note though, that using unit tests does not imply that you have to throw away
    doctests; it is perfectly fine to use both kinds of tests in your application.
    Each has their strengths, thus for many projects it is probably best to have a
    good mixture of unit tests and doctests rather than relying on a single type for
    all testing.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们失去了doctest版本的内置文档。这不仅仅是来自文档字符串的自动生成文档的潜力。Doctests通常比单元测试更易读，其中只是测试开销的多余代码可能会掩盖测试的意图。但请注意，使用单元测试并不意味着您必须放弃doctests；在应用程序中同时使用这两种测试是完全可以的。每种测试都有其优势，因此对于许多项目来说，最好是在所有测试中使用一种类型，而不是依赖单一类型。
- en: Revisiting the doctest caveats
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重新审视doctest的注意事项
- en: In the previous chapter, we developed a list of things to watch out for when
    writing doctests. When discussing these, unit tests were sometimes mentioned as
    an alternative that did not suffer from the same problems. But are unit tests
    really immune to these problems, or do they just make the problems easier to avoid
    or address? In this section, we revisit the doctest caveats and consider how susceptible
    unit tests are to the same or similar issues.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们列出了编写文档测试时需要注意的事项。在讨论这些事项时，有时会提到单元测试作为一个不会遇到相同问题的替代方法。但是单元测试是否真的免疫于这些问题，还是只是使问题更容易避免或解决？在本节中，我们重新审视文档测试的警告，并考虑单元测试对相同或类似问题的敏感程度。
- en: Environmental dependence
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 环境依赖
- en: 'The first doctest caveat discussed was environmental dependence: relying on
    the implementation details of code other than the code actually being tested.
    Though this type of dependence can happen with unit tests, it is less likely to
    occur. This is because a very common way for this type of dependence to creep
    into doctests is due to reliance on the printed representation of objects, as
    they are displayed in a Python shell session. Unit tests are far removed from
    the Python shell. It requires some coding effort to get an object''s printed representation
    in a unit test, thus it is rare for this form of environmental dependence to creep
    into a unit test.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论的第一个文档测试警告是环境依赖：依赖于实际被测试的代码以外的代码的实现细节。尽管单元测试也可能出现这种依赖，但发生的可能性较小。这是因为这种依赖的非常常见的方式是依赖于对象的打印表示，因为它们在Python
    shell会话中显示。单元测试与Python shell相去甚远。在单元测试中需要一些编码工作才能获得对象的打印表示，因此这种形式的环境依赖很少会出现在单元测试中。
- en: One common form of environmental dependence mentioned in [Chapter 2](ch02.html
    "Chapter 2. Does This Code Work? Doctests in Depth") that also afflicts unit tests
    involves file pathnames. Unit tests, just as doctests, need to take care that
    differences in file pathname conventions across operating systems do not cause
    bogus test failures when a test is run on an operating system different from the
    one where it was originally written. Thus, though unit tests are less prone to
    the problem of environmental dependence, they are not entirely immune.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[第2章](ch02.html "第2章。深入了解文档测试")中提到的一种常见的环境依赖形式也影响到了单元测试，涉及文件路径名。单元测试和文档测试一样，需要注意跨操作系统的文件路径名约定差异，以防在不同于最初编写测试的操作系统上运行测试时导致虚假的测试失败。因此，尽管单元测试不太容易出现环境依赖问题，但它们并非完全免疫。'
- en: Database dependence
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据库依赖
- en: Database dependence is a specific form of environmental dependence that is particularly
    common for Django applications to encounter. In the doctests, we saw that the
    initial implementation of the tests was dependent on the specifics of the message
    that accompanied an `IntegrityError`. In order to make the doctests pass on multiple
    different databases, we needed to modify the initial tests to ignore the details
    of this message.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库依赖是Django应用程序特别常见的一种环境依赖形式。在文档测试中，我们看到测试的初始实现依赖于伴随`IntegrityError`的消息的具体内容。为了使文档测试在多个不同的数据库上通过，我们需要修改初始测试以忽略此消息的细节。
- en: 'We do not have this same problem with the unit test version. The `assertRaises`
    used to check for an expected exception already does not consider the exception
    message detail. For example:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在单元测试版本中没有这个问题。用于检查预期异常的`assertRaises`已经不考虑异常消息的细节。例如：
- en: '[PRE14]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: There are no message specifics included there, so we don't need to do anything
    to ignore differences in messages from different database implementations.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 那里没有包含具体的消息，所以我们不需要做任何事情来忽略来自不同数据库实现的消息差异。
- en: 'In addition, unit tests make it easier to deal with even more wide-reaching
    differences than message details. It was noted in the previous chapter that for
    some configurations of MySQL, ignoring the message detail is not enough to allow
    all the tests to pass. The test that has a problem here is the one that ensures
    `closes` is only auto-set during initial model creation. The unit test version
    of this test is:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，单元测试使处理比消息细节更广泛的差异变得更容易。在上一章中指出，对于MySQL的某些配置，忽略消息细节不足以使所有测试通过。在这里出现问题的测试是确保`closes`仅在初始模型创建期间自动设置的测试。这个测试的单元测试版本是：
- en: '[PRE15]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This test fails if it is run on a MySQL server that is running in non-strict
    mode. In this mode, MySQL does not raise an `IntegrityError` on an attempt to
    update a row to contain a `NULL` value in a column declared to be `NOT NULL`.
    Rather, the value is set to an implicit default value, and a warning is issued.
    Thus, we see a test error when we run this test on a MySQL server configured to
    run in non-strict mode:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在运行在非严格模式下的MySQL服务器上运行此测试，则此测试将失败。在此模式下，MySQL在尝试将行更新为包含在声明为“NOT NULL”的列中包含“NULL”值时不会引发`IntegrityError`。相反，该值将设置为隐式默认值，并发出警告。因此，当我们在配置为在非严格模式下运行的MySQL服务器上运行此测试时，我们会看到测试错误：
- en: '[PRE16]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Here we see that the warning issued by MySQL causes a simple `Exception` to
    be raised, not an `IntegrityError`, so the test reports an error.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到MySQL发出的警告导致引发了一个简单的`Exception`，而不是`IntegrityError`，因此测试报告了一个错误。
- en: 'There is also an additional wrinkle to consider here: This behavior of raising
    an `Exception` when MySQL issues a warning is dependent on the Django `DEBUG`
    setting. MySQL warnings are turned into raised `Exceptions` only when `DEBUG`
    is `True` (as it was for the previously run test). If we set `DEBUG` to `False`
    in `settings.py`, we see yet a different form of test failure:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这里还有一个额外的问题需要考虑：当MySQL发出警告时引发`Exception`的行为取决于Django的`DEBUG`设置。只有在`DEBUG`为`True`时（就像先前运行的测试一样），MySQL警告才会转换为引发的`Exception`。如果我们在`settings.py`中将`DEBUG`设置为`False`，我们会看到另一种形式的测试失败：
- en: '[PRE17]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In this case, MySQL allowed the save, and since `DEBUG` was not turned on Django
    did not transform the warning issued by MySQL into an `Exception`, so the save
    simply worked.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we may seriously question whether it is even worth the effort
    to get this test to run properly in all these different situations, given the
    wildly divergent observed behaviors. Perhaps we should just require that if the
    code is run on MySQL, the server must be configured to run in strict mode. Then
    the test would be fine as it is, since the previous failures would both signal
    a server configuration problem. However, let's assume we do need to support running
    on MySQL, yet we cannot impose any particular configuration requirement on MySQL,
    and we still need to verify whether our code is behaving properly for this test.
    How do we do that?
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Note what we are attempting to verify in this test is that our code does not
    auto-set `closes` to some value during save if it has been reset to `None` after
    initial creation. At first, it seemed that this was easily done by just checking
    for an `IntegrityError` on an attempted save. However, we've found a database
    configuration where we don't get an `IntegrityError`. Also, depending on the `DEBUG`
    setting, we may not get any error reported at all, even if our code behaves properly
    and leaves `closes` set to `None` during an attempted save. Can we write the test
    so that it reports the proper result—that is, whether our code behaves properly—in
    all these situations?
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 'The answer is yes, so long as we can determine in our test code what database
    is in use, how it is configured, and what the `DEBUG` setting is. Then all we
    need to do is change the expected results based on the environment the test is
    running in. In fact, we can test for all these things with a bit of work:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The test code starts by assuming that we are running on a database that is operating
    in strict mode, and set the local variable `strict` to `True`. We also assume
    `DEBUG` is `False` and set a local variable to reflect that. Then, if the database
    in use is MySQL (determined by checking the value of `settings.DATABASE_ENGINE`),
    we need to perform some further checking to see how it is configured. Consulting
    the MySQL documentation shows that the way to do this is to `SELECT` the session's
    `sql_mode` variable. If the returned value contains the string `STRICT`, then
    MySQL is operating in strict mode, otherwise it is not. We issue this query and
    obtain the result using Django's support for sending raw SQL to the database.
    If we determine that MySQL is not configured to run in strict mode, we update
    our local variable `strict` to be `False`.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: If we get to the point where we set strict to `False`, that is also when the
    `DEBUG` value in settings becomes important, since it is in this case that MySQL
    will issue a warning instead of raising an `IntegrityError` for the case we are
    testing here. If `DEBUG` is `True` in the settings file, then warnings from MySQL
    will be turned into `Exceptions` by Django's MySQL backend. This is done by the
    backend using Python's `warnings` module. When the backend is loaded, if `DEBUG`
    is `True`, then a `warnings.filterwarnings` call is issued to force all database
    warnings to be turned into `Exceptions`.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, at some point after the database backend is loaded and before
    our test code runs, the test runner will change the in-memory settings so that
    `DEBUG` is set to `False`. This is done so that the behavior of test code matches
    as closely as possible what will happen in production. However, it means that
    we cannot just test the value of `settings.DEBUG` during the test to see if `DEBUG`
    was `True` when the database backend was loaded. Rather, we have to re-load the
    settings module and check the value in the newly loaded version. We do this using
    the `import_module` function of `django.utils.importlib` (this is a function from
    Python 2.7 that was backported to be used by Django 1.1).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we know what to look for when we run our test code. If we have determined
    that we are running a database operating in strict mode, we assert that attempting
    to save our model instance with `closes` set to `None` should raise an `IntegrityError`.
    Else, if we are running in non-strict mode, but `DEBUG` is `True` in the settings
    file, then the attempted save should result in an `Exception` being raised. Otherwise
    the save should work, and we test the correct behavior of our code by ensuring
    that `closes` is still set to `None` even after the model instance has been saved.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: All of that may seem like rather a lot of trouble to go through for a pretty
    minor test, but it illustrates how unit tests can be written to accommodate significant
    differences in expected behavior in different environments. Doing the same for
    the doctest version is not so straightforward. Thus, while unit tests clearly
    do not eliminate the problem of dealing with database dependence in the tests,
    they make it easier to write tests that account for such differences.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Test interdependence
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next doctest caveat encountered in the last chapter was test interdependence.
    When the doctests were run on PostgreSQL, an error was encountered in the test
    following the first one that intentionally triggered a database error, since that
    error caused the database connection to enter a state where it would accept no
    further commands, except ones that terminated the transaction. The fix for that
    was to remember to "clean up" after the intentionally triggered error by including
    a transaction rollback after any test step that causes such an error.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Django unit tests do not suffer from this problem. The Django test case class,
    `django.test.TestCase`, ensures that the database is reset to a clean state before
    each test method is called. Thus, even though the `testClosesReset` method ends
    by attempting a model save that triggers an `IntegrityError`, no error is seen
    by the next test method that runs, because the database connection is reset in
    the interim by the `django.test.TestCase` code. It is not just this error situation
    that is cleaned up, either. Any database rows that are added, deleted, or modified
    by a test case method are reset to their original states before the next method
    is run. (Note that on most databases, the test runner can use a transaction rollback
    call to accomplish this very efficiently.) Thus Django unit test methods are fully
    isolated from any database changes that may have been performed by tests that
    ran before them.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Unicode
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The final doctest caveat discussed in the previous chapter concerned using Unicode
    literals within doctests. These were observed to not work properly, due to underlying
    open issues in Python related to Unicode docstrings and Unicode literals within
    docstrings.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: 'Unit tests do not have this problem. A straightforward unit test for the behavior
    of the `Survey` model `__unicode__` method works:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Note that it is necessary to add the encoding declaration to the top of `survey/tests.py`,
    just as we did in the previous chapter for `survey/models.py`, but it is not necessary
    to do any manual decoding of bytestring literals to construct Unicode objects
    as needed to be done in the doctest version. We just need to set our variables
    as we normally would, create the `Survey` instance, and assert that the result
    of calling `unicode` on that instance produces the string we expect. Thus testing
    with non-ASCII data is much more straightforward when using unit tests than it
    is with doctests.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Providing data for unit tests
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Besides not suffering from some of the disadvantages of doctests, unit tests
    provide some additional useful features for Django applications. One of these
    features is the ability to load the database with test data prior to the test
    run. There are a few different ways this can be done; each is discussed in detail
    in the following sections.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Providing data in test fixtures
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first way to provide test data for unit tests is to load them from files,
    called fixtures. We will cover this method by first developing an example test
    that can benefit from pre-loaded test data, then showing how to create a fixture
    file, and finally describing how to ensure that the fixture file is loaded as
    part of the test.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Example test that needs test data
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before jumping into the details of how to provide a test with pre-loaded data,
    it would help to have an example of a test that could use this feature. So far
    our simple tests have gotten by pretty easily by just creating the data they need
    as they go along. However, as we begin to test more advanced functions, we quickly
    run into cases were it would become burdensome for the test itself to have to
    create all of the data needed for a good test.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider the `Question` model:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: (Note that we have added a `__unicode__` method to this model. This will come
    in handy later in the chapter when we begin to use the admin interface to create
    some survey application data.)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that the allowed answers for a given `Question` instance are stored
    in a separate model, `Answer`, which is linked to `Question` using a `ForeignKey`:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This `Answer` model also tracks how many times each answer has been chosen,
    in its `votes` field. (We have not added a `__unicode__` method to this model
    yet, since given the way we will configure admin later in the chapter, it is not
    yet needed.)
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Now, when analyzing survey results, one of the things we will want to know about
    a given `Question` is which of its `Answers` was chosen most often. That is, one
    of the functions that a `Question` model will need to support is one which returns
    the "winning answer" for that `Question`. If we think about this a bit, we realize
    there may not be a single winning answer. There could be a tie with multiple answers
    getting the same number of votes. So, this winning answer method should be flexible
    enough to return more than one answer. Similarly, if there were no responses to
    the question, it would be better to return no winning answers than the whole set
    of allowed answers, none of which were ever chosen. Since this method (let's call
    it `winning_answers`) may return zero, one, or more results, it's probably best
    for consistency's sake for it to always return something like a list.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 'Before even starting to implement this function, then, we have a sense of the
    different situations it will need to handle, and what sort of test data will be
    useful to have in place when developing the function itself and tests for it.
    A good test of this routine will require at least three different questions, each
    with a set of answers:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: One question that has a clear winner among the answers, that is one answer with
    more votes than all of the others, so that `winning_answers` returns a single
    answer
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One question that has a tie among the answers, so that `winning_answers` returns
    multiple answers
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One question that gets no responses at all, so that `winning_answers` returns
    no answers
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, we should test with a `Question` that has no answers linked to
    it. This is an edge case, certainly, but we should ensure that the `winning_answers`
    function operates properly even when it seems that the data hasn't been fully
    set up for analysis of which answer was most popular. So, really there should
    be four questions in the test data, three with a set of answers and one with no
    answers.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Using the admin application to create test data
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Creating four questions, three with several answers, in a shell session or
    even a program is pretty tedious, so let''s use the Django admin application instead.
    Back in the first chapter we included `django.contrib.admin` in `INSTALLED_APPS`,
    so it is already loaded. Also, when we ran `manage.py syncdb`, the tables needed
    for admin were created. However, we still need to un-comment the admin-related
    lines in our `urls.py` file. When we do that `urls.py` should look like this:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Finally, we need to provide some admin definitions for our survey application
    models, and register them with the admin application so that we can edit our models
    in the admin. Thus, we need to create a `survey/admin.py` file that looks something
    like this:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Here we have mostly used the admin defaults for everything, except that we have
    defined and specified some admin inline classes to make it easier to edit multiple
    things on a single page. The way we have set up the inlines here allows us to
    edit `Questions` on the same page as the `Survey` they belong to, and similarly
    edit `Answers` on the same page as the `Questions` they are associated with. We've
    also specified that we want four extra empty `Questions` when they appear inline.
    The default for this value is three, but we know we want to set up four questions
    and we might as well set things up so we can add all four at one time.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can start the development server by running `python manage.py runserver`
    in a command prompt, and access the admin application by navigating to `http://localhost:8000/admin/`
    from a browser on the same machine. After logging in as the superuser we created
    back in the first chapter, we''ll be shown the admin main page. From there, we
    can click on the link to add a `Survey`. The **Add survey** page will let us create
    a survey with our four `Questions`:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '![Using the admin application to create test data](img/7566_03_01.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
- en: 'Here we''ve assigned our `Question` instances `question` values that are not
    so much questions as indications of what we are going to use each one to test.
    Notice this page also reflects a slight change made to the `Survey` model: `blank=True`
    has been added to the `closes` field specification. Without this change, admin
    would require a value to be specified here for `closes`. With this change, the
    admin application allows the field to be left blank, so that the automatic assignment
    done by the save override method can be used.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have saved this survey, we can navigate to the change page for the
    first question, **Clear Winner**, and add some answers:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '![Using the admin application to create test data](img/7566_03_02.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
- en: 'Thus, we set up the **Clear Winner** question to have one answer (**Max Votes**)
    that has more votes than all of the other answers. Similarly, we can set up the
    **2-Way Tie** question to have two answers that have the same number of votes:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '![Using the admin application to create test data](img/7566_03_03.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
- en: 'And finally, we set up the answers for **No Responses** so that we can test
    the situation where none of the answers to a `Question` have received any votes:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '![Using the admin application to create test data](img/7566_03_04.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
- en: We do not need to do anything further with the **No Answers** question since
    that one is going to be used to test the case where the answer set for the question
    is empty, as it is when it is first created.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Writing the function itself
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have our database set up with test data, we can experiment in the
    shell with the best way to implement the `winning_answers` function. As a result,
    we might come up with something like:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The method starts by initializing a local variable `rv` (return value) to an
    empty list. Then, it uses the aggregation `Max` function to retrieve the maximum
    value for `votes` that exists in the set of `Answer` instances associated with
    this `Question` instance. That one line of code does several things in order to
    come up with the answer, so it may bear some more explanation. To see how it works,
    take a look at what each piece in turn returns in a shell session:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Here we see that applying the aggregate function `Max` to the `votes` field
    of the `answer_set` associated with a given `Question` returns a dictionary containing
    a single key-value pair. We''re only interested in the value, so we retrieve just
    the values from the dictionary using `.values()`:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'However, `values()` returns a list and we want the single item in the list,
    so we retrieve it by requesting the item at index zero in the list:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Next the code tests for whether `max_votes` exists and if it is greater than
    zero (at least one answer was chosen at least once). If so, `rv` is reset to be
    the set of answers filtered down to only those that have that maximum number of
    votes.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 'But when would `max_votes` not exist, since it was just set in the previous
    line? This can happen in the edge case where there are no answers linked to a
    question. In that case, the aggregate `Max` function is going to return `None`
    for the maximum votes value, not zero:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Thus in this edge case, `max_votes` may be set to `None`, so it's best to test
    for that and avoid trying to compare `None` to `0`. While that comparison will
    actually work and return what seems like a sensible answer (`None` is not greater
    than `0`) in Python 2.x, the attempted comparison will return a `TypeError` beginning
    with Python 3.0\. It's wise to avoid such comparisons now so as to limit problems
    if and when the code needs to be ported to run under Python 3.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the function returns `rv`, at this point hopefully set to the correct
    value. (Yes, there's a bug in this function. It's more entertaining to write tests
    that catch bugs now and then.)
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Writing a test that uses the test data
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we have an implementation of `winning_answers`, and data to test it
    with, we can start writing our test for the `winning_answers` method. We might
    start by adding the following test to `tests.py`, testing the case where there
    is a clear winner among the answers:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The test starts by retrieving the `Question` that has its `question` value set
    to `'Clear Winner'`. Then, it calls `winning_answers` on that `Question` instance
    to retrieve the query set of answers for the question that received the most number
    of votes. Since this question is supposed to have a single winner, the test asserts
    that there is one element in the returned query set. It then does some further
    checking by retrieving the winning answer itself and verifying that its answer
    value is `'Max Votes'`. If all that succeeds, we can be pretty sure that `winning_answers`
    returns the correct result for the case where there is a single "winner" among
    the answers.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Extracting the test data from the database
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, how do we run that test against the test data we loaded via the admin application
    into our database? When we run the tests, they are not going to use our production
    database, but rather create and use an initially empty test database. This is
    where fixtures come in. Fixtures are just files containing data that can be loaded
    into the database.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'The first task, then, is to extract the test data that we loaded into our production
    database into a fixture file. We can do this by using the `manage.py dumpdata`
    command:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Beyond the `dumpdata` command itself, the various things specified there are:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '`survey`: This limits the dumped data to the survey application. By default,
    `dumpdata` will output data for all installed applications, but the winning answers
    test does not need data from any application other than survey, so we can limit
    the fixture file to contain only data from the survey application.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--indent 4`: This makes the data output easier to read and edit. By default,
    `dumpdata` will output the data all on a single line, which is difficult to deal
    with if you ever need to examine or edit the result. Specifying `indent 4` makes
    `dumpdata` format the data on multiple lines, with four-space indentation making
    the hierarchy of structures clear. (You can specify whatever number you like for
    the indent value, it does not have to be `4`.)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`>test_winning_answers.json`: This redirects the output from the command to
    a file. The default output format for `dumpdata` is JSON, so we use `.json` as
    the file extension so that when the fixture is loaded its format will be interpreted
    correctly.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When `dumpdata` completes, we will have a `test_winning_answers.json` file,
    which contains a serialized version of our test data. Besides loading it as part
    of our test (which will be covered next), what might we do with this or any fixture
    file?
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we can load fixtures using the `manage.py loaddata` command. Thus `dumpdata`
    and `loaddata` together provide a way to move data from one database to another.
    Second, we might have or write programs that process the serialized data in some
    way: it can sometimes be easier to perform analysis on data contained in a flat
    file instead of a database. Finally, the `manage.py testserver` command supports
    loading fixtures (specified on the command line) into a test database and then
    running the development server. This can come in handy in situations where you''d
    like to experiment with how a real server behaves given this test data, instead
    of being limited to the results of the tests written to use the data.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Getting the test data loaded during the test run
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Returning to our task at hand: how do we get this fixture we just created loaded
    when running the tests? An easy way to do this is to rename it to `initial_data.json`
    and place it in a `fixtures` subdirectory of our survey application directory.
    If we do that and run the tests, we will see that the fixture file is loaded,
    and our test for the clear winner case runs successfully:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'However, that is not really the right way to get this particular fixture data
    loaded. Initial data fixtures are meant for constant application data that should
    always be there as part of the application, and this data does not fall into that
    category. Rather, it is specific to this particular test, and needs to be loaded
    only for this test. To do that, place it in the `survey/fixtures` directory with
    the original name, `test_winning_answers.json`. Then, update the test case code
    to specify that this fixture should be loaded for this test by including the file
    name in a `fixtures` class attribute of the test case:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Note that `manage.py test`, at least as of Django 1.1, does not provide as much
    feedback for the loading of test fixtures specified this way as it does for loading
    initial data fixtures. In the previous test output, where the fixture was loaded
    as initial data, there are messages about the initial data fixture being loaded
    and 13 objects being installed. There are no messages like that when the fixture
    is loaded as part of the `TestCase`.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore there is no error indication if you make a mistake and specify
    the wrong filename in your `TestCase fixtures` value. For example, if you mistakenly
    leave the ending `s` off of `test_winning_answers`, the only indication of the
    problem will be that the test case fails:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Possibly the diagnostics provided for this error case may be improved in the
    future, but in the meantime it's best to keep in mind that mysterious errors such
    as that `DoesNotExist` above are likely due to the proper test fixture not being
    loaded rather than some error in the test code or the code being tested.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we''ve got the test fixture loaded and the first test method working
    properly, we can add the tests for the three other cases: the one where there
    is a two-way tie among the answers, the one where no responses were received to
    a question, and the one where no answers are linked to a question. These can be
    written to be very similar to the existing method that tests the clear winner
    case:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Differences are in the names of the `Questions` retrieved from the database,
    and how the specific results are tested. In the case of the `2-Way Tie`, the test
    verifies that `winning_answers` returns two answers, and that both have `answer`
    values that start with `'Max Votes'`. In the case of no responses, and no answers,
    all the tests have to do is verify that there are no items in the query set returned
    by `winning_answers`.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'If we now run the tests, we will find the bug that was mentioned earlier, since
    our last two tests fail:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The problem here is that `winning_answers` is inconsistent in what it returns:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The return value `rv` is initialized to a list in the first line of the function,
    but then when it is set in the case where there are answers that received votes,
    it is set to be the return value from a `filter` call, which returns a `QuerySet`,
    not a list. The test methods, since they use `count()` with no arguments on the
    return value of `winning_answers`, are expecting a `QuerySet`.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'Which is more appropriate for `winning_answers` to return: a list or a `QuerySet`?
    Probably a `QuerySet`. The caller may only be interested in the count of answers
    in the set and not the specific answers, so it may not be necessary to retrieve
    the actual answers from the database. If `winning_answers` consistently returns
    a list, it would have to force the answers to be read from the database in order
    to put them in a list. Thus, it''s probably more efficient to always return a
    `QuerySet` and let the caller''s requirements dictate what ultimately needs to
    be read from the database. (Given the small number of items we''d expect to be
    in this set, there is probably little to no efficiency to be gained here, but
    it is still a good habit to get into in order to consider such things when designing
    interfaces.)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 'A way to fix `winning_answers` to always return a `QuerySet` is to use the
    `none()` method applied to the `answer_set`, which will return an empty `QuerySet`:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: After making this change, the complete `QuestionWinningAnswersTest TestCase`
    runs successfully.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Creating data during test set up
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While test fixtures are very convenient, they are sometimes not the right tool
    for the job. Specifically, since the fixture files contain fixed, hard-coded values
    for all model data, fixtures are sometimes not flexible enough for all tests.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, let''s return to the `Survey` model and consider some methods
    we are likely to want it to support. Recall that a survey has both, an `opens`
    and a `closes` date, so at any point in time a particular `Survey` instance may
    be considered "completed", "active", or "upcoming", depending on where the current
    date falls in relation to the survey''s `opens` and `closes` dates. It will be
    useful to have easy access to these different categories of surveys. The typical
    way to support this in Django is to create a special model `Manager` for `Survey`
    that implements methods to return appropriately-filtered query sets. Such a `Manager`
    might look like this:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'This manager implements three methods:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '`completed`: This returns a `QuerySet` of `Survey` filtered down to only those
    with `closes` values earlier than today. These are surveys that are closed to
    any more responses.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`active`: This returns a `QuerySet` of `Survey` filtered down to only those
    with `opens` values earlier or equal to today, and `closes` later than or equal
    to today. These are surveys that are open to receiving responses.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`upcoming`: This returns a `QuerySet` of `Survey` filtered down to only those
    with `opens` values later than today. These are surveys that are not yet open
    to responses.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To make this custom manager the default for the `Survey` model, assign an instance
    of it to the value of the `Survey objects` attribute:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Why might we have difficulty testing these methods using fixture data? The problem
    arises due to the fact that the methods rely on the moving target of today's date.
    It's not actually a problem for testing `completed`, as we can set up test data
    for surveys with `closes` dates in the past, and those `closes` dates will continue
    to be in the past no matter how much further forward in time we travel.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: It is, however, a problem for `active` and `upcoming`, since eventually, even
    if we choose `closes` (and, for `upcoming`, `opens`) dates far in the future,
    today's date will (barring universal catastrophe) at some point catch up with
    those far-future dates. When that happens, the tests will start to fail. Now,
    we may expect that there is no way our software will still be running in that
    far-future time. (Or we may simply hope that we are no longer responsible for
    maintaining it then.) But that's not really a good approach. It would be much
    better to use a technique that doesn't result in time-bombs in the tests.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'If we don''t want to use a test fixture file with hard-coded dates to test
    these routines, what is the alternative? What we can do instead is much like what
    we were doing earlier: create the data dynamically in the test case. As noted
    earlier, this might be somewhat tedious, but note we do not have to re-create
    the data for each test method. Unit tests provide a hook method, `setUp`, which
    we can use to implement any common pre-test initialization. The test machinery
    will ensure that our `setUp` routine is run prior to each of our test methods.
    Thus `setUp` is a good place to put code that dynamically creates fixture-like
    data for our tests.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: 'In a test for the custom `Survey` manager, then, we might have a `setUp` routine
    that looks like this:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'This method creates three `Surveys`: one that opened and closed yesterday,
    one that opens and closes today, and one that opens and closes tomorrow. Before
    it creates these, it deletes all `Survey` objects that are in the database. Thus,
    each test method in the `SurveyManagerTest` can rely on there being exactly three
    `Surveys` in the database, one in each of the three states.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Why does the test first delete all `Survey` objects? There should not be any
    `Surveys` in the database yet, right? That call is there just in case at some
    future point, the survey application acquires an initial data fixture that includes
    one or more `Surveys`. If such a fixture existed, it would be loaded during test
    initialization, and would break these tests that rely on there being exactly three
    `Surveys` in the database. Thus, it is safest for `setUp` here to ensure that
    the only `Surveys` in the database are the ones it creates.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'A test for the `Survey` manager `completed` function might then be:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The test first asserts that on entry there is one completed `Survey` in the
    database. It then verifies that the one `Survey` returned by the `completed` function
    is in fact that actual survey it expects to be completed, that is the one with
    title set to `"Yesterday"`. The test then goes a step further and modifies that
    completed `Survey` so that its `closes` date no longer qualifies it as completed,
    and saves that change to the database. When that has been done, the test asserts
    that there are now zero completed `Surveys` in the database.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'Testing with that routine verifies that the test works, so a similar test for
    active surveys might be written as:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: This is very much like the test for `completed`. It asserts that there is one
    active `Survey` on entry, retrieves the active `Survey` and verifies that it is
    the one expected to be active, modifies it so that it no longer qualifies as active
    (by making it qualify as closed), saves the modification, and finally verifies
    that `active` then returns that there are no active `Surveys`.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, a test for upcoming surveys might be:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: But won't all those tests interfere with each other? For example, the test for
    `completed` makes the `"Yesterday"` survey appear to be active, and the test for
    `active` makes the `"Today"` survey appear to be closed. It seems that whichever
    one runs first is going to make a change that will interfere with the correct
    operation of the other test.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, though, the tests don''t interfere with each other, because the database
    is reset and the test case `setUp` method is re-run before each test method is
    run. So `setUp` is not run once per `TestCase`, but rather once per test method
    within the `TestCase`. Running the tests shows that all of these tests pass, even
    though each updates the database in a way that would interfere with the others,
    if the changes it made were seen by the others:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: There is a companion method to `setUp`, called `tearDown` that can be used to
    perform any cleaning up after test methods. In this case it isn't necessary, since
    the default Django operation of resetting the database between test method executions
    takes care of un-doing the database changes made by the test methods. The `tearDown`
    routine is useful for cleaning up any non-database changes (such as temporary
    file creation, for example) that may be done by the tests.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have now covered the basics of unit testing Django applications. In this
    chapter, we:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Converted the previously-written doctests for the `Survey` model to unit tests,
    which allowed us to directly compare the pros and cons of each test approach
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Revisited the doctest caveats from the previous chapter and examined to what
    extent (if any) unit tests are susceptible to the same issues
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Began to learn some of the additional features available with unit tests; in
    particular, features related to loading test data
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next chapter, we will start investigating even more advanced features
    that are available to Django unit tests.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
