- en: Implementations, Applications, and Tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning about algorithms without any real-life application remains a purely
    academic pursuit. In this chapter, we will explore the data structures and algorithms
    that are shaping our world.
  prefs: []
  type: TYPE_NORMAL
- en: One of the golden nuggets of this age is the abundance of data. Emails, phone
    numbers, text documents, and images contain large amounts of data. In this data,
    there is valuable information that makes the data more important. But to extract
    this information from the raw data, we have to use data structures, processes,
    and algorithms that specialize in this task.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning employs a significant number of algorithms to analyze and predict
    the occurrence of certain variables. Analyzing data on a purely numerical basis
    still leaves much of the latent information buried in the raw data. Presenting
    data visually thus enables one to understand and gain valuable insights too.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you should be able to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Prune and present data accurately
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use both supervised and unsupervised learning algorithms for the purposes of
    prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visually represent data in order to gain more insight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to proceed with this chapter, you will need to install the following
    packages. These packages will be used to pre-process and visually represent the
    data being processed. Some of the packages also contain a well-written implementation
    of the algorithms that will operate on our data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Preferably, these modules should be installed using `pip`. So, firstly, we
    need to install pip for Python 3 using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sudo apt-get update`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sudo apt-get install python3-pip`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Furthermore, the following commands are to be run to install the `numpy`, `scikit-learn`,
    `matplotlib`, `pandas`, and `textblob` packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If you are using the old version of Python (that is, Python 2), the packages
    can be installed using the same commands by replacing `pip3` with `pip`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You are also required to install the `nltk` and `punkt` packages, which provide
    inbuilt text processing functions. To install them, open the Python Terminal and
    run the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'These packages may require other platform-specific modules to be installed
    first. Take note and install all dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**NumPy**: A library with functions to operate on n-dimensional arrays and
    matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scikit-learn**: A highly advanced module for machine learning. It contains
    an implementation of many algorithms for classification, regression, and clustering,
    among others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Matplotlib**: This is a plotting library that makes use of NumPy to graph
    a good variety of charts, including line plots, histograms, scatter plots, and
    even 3D graphs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pandas**: This library deals with data manipulation and analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The GitHub link is as follows: [https://github.com/PacktPublishing/Hands-On-Data-Structures-and-Algorithms-with-Python-3.x-Second-Edition/tree/master/Chapter14](https://github.com/PacktPublishing/Hands-On-Data-Structures-and-Algorithms-with-Python-3.x-Second-Edition/tree/master/Chapter14).
  prefs: []
  type: TYPE_NORMAL
- en: Data preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To analyze the data, first of all, we have to preprocess the data to remove
    the noise and convert it in to an appropriate format so that it can be further
    analyzed. A collection of data from the real world is mostly full of noise, which
    makes it difficult to apply any algorithm directly. The raw data collected is
    plagued by a lot of issues so we need to adopt ways to sanitize the data to make
    it suitable for use in further studies.
  prefs: []
  type: TYPE_NORMAL
- en: Processing raw data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data collected may also be inconsistent with other records collected over
    time. The existence of duplicate entries and incomplete records warrant that we
    treat the data in such a way as to bring out hidden and useful information.
  prefs: []
  type: TYPE_NORMAL
- en: To clean the data, we totally discard irrelevant and noisy data. Data with missing
    parts or attributes can be replaced with sensible estimates. Also, where the raw
    data suffers from inconsistency, detecting and correcting that becomes necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Let's explore how we can use `NumPy` and `pandas` for data preprocessing techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Missing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The performance of the machine learning algorithm deteriorates if the data
    has missing values. Just because a dataset has missing fields or attributes does
    not mean it is not useful. Several methods can be used to fill in the missing
    values. Some of these methods are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Using a global constant to fill in the missing values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the mean or median value in the dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supplying the data manually.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the attribute mean or median to fill in the missing values. The choice
    is based on the context and sensitivity of what the data is going to be used for.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Take, for instance, the following data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the data elements `data[1][0]` and `data[1][1]` have values of `np.NAN`,
    representing the fact that they have no value. If the `np.NAN` values are not
    desired in a given dataset, they can be set to a constant figure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s set data elements with the value `np.NAN` to `0.1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The new state of the data becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'To apply the mean values instead, we do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The mean value for each column is calculated and inserted into those data areas
    with the `np.NAN` value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: For the first column, column `0`, the mean value was obtained by `(4 + 94)/2`.
    The resulting `49.0` is then stored at `data[1][0]`. A similar operation is carried
    out for columns `1` and `2`.
  prefs: []
  type: TYPE_NORMAL
- en: Feature scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The columns in a data frame are known as its features. The rows are known as
    records or observations. The performance of the machine learning algorithm decreases
    if one attribute has values in a higher range compared to other attributes' values.
    Thus, it is often required to scale or normalize the attribute values in a common
    range.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider an example, the following data matrix. This data will be referenced
    in subsections so please do take note:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Feature one, with data of `58`, `10`, and `20`, has its values lying between
    `10` and `58`. For feature two, the data lies between `1` and `200`. Inconsistent
    results will be produced if we supply this data to any machine learning algorithm.
    Ideally, we will need to scale the data to a certain range in order to get consistent
    results.
  prefs: []
  type: TYPE_NORMAL
- en: Once again, closer inspection reveals that each feature (or column) lies around
    different mean values. Therefore, what we want to do is to align the features
    around similar means.
  prefs: []
  type: TYPE_NORMAL
- en: One benefit of feature scaling is that it boosts the learning parts of machine
    learning. The `scikit` module has a considerable number of scaling algorithms
    that we shall apply to our data.
  prefs: []
  type: TYPE_NORMAL
- en: Min-max scalar form of normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The min-max scalar form of normalization uses the mean and standard deviation
    to box all the data into a range lying between certain min and max values. Generally,
    the range is set between `0` and `1`; although other ranges may be applied, the
    `0` to `1` range remains the default:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'An instance of the `MinMaxScaler` class is created with the range `(0,1)` and
    passed to the `scaled_values` variables. The `fit` function is called to make
    the necessary calculations that are used internally to change the dataset. The
    `transform` function affects the actual operation on the dataset, returning the
    value to `results`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We can see from the preceding output that all the data is normalized and lies
    between `0` and `1`. This kind of output can now be supplied to a machine learning
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Standard scalar
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The mean values for the respective features in our initial dataset or table
    are 29.3, 92, and 38\. To make all the data have a similar mean, that is, a zero
    mean and a unit variance across the data, we can apply the standard scalar algorithm,
    shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '`data` is passed to the `fit` method of the object returned from instantiating
    the `StandardScaler` class. The `transform` method acts on the data elements in
    the data and returns the output to the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Examining the results, we observe that all our features are now evenly distributed.
  prefs: []
  type: TYPE_NORMAL
- en: Binarizing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To binarize a given feature set, we can make use of a threshold. If any value
    within a given dataset is greater than the threshold, the value is replaced by
    `1`, and if the value is less than the threshold, it is replaced with `0`. Consider
    the following code snippet, where we take 50 as the threshold to binarize the
    original data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'An instance of `Binarizer` is created with the argument `50.0`. `50.0` is the
    threshold that will be used in the binarizing algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: All values in the data that are less than 50 will have a value of `0`, and hold
    a value of `1` otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: Learning about machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning is a subfield of artificial intelligence. Machine learning
    is basically an algorithm that can learn from the example data and can provide
    predictions based on that. Machine learning models learn the patterns from the
    data examples and use those learned patterns to make predictions for unseen data. For
    example, we feed many examples of spam and ham email messages to develop a machine
    learning model that can learn the patterns in emails and can classify new emails
    as spam or ham.
  prefs: []
  type: TYPE_NORMAL
- en: Types of machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are three broad categories of machine learning, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Supervised learning**: Here, an algorithm is fed a set of inputs and their
    corresponding outputs. The algorithm then has to figure out what the output will
    be for an unseen input. Supervised learning algorithms try to learn the patterns
    in the input features and target output in such a way that the learned model can
    predict the output for the new unseen data. Classification and regression are
    two kinds of problem that are solved using a supervised learning approach, in
    which the machine learning algorithm learns from the given data and labels. Classification
    is a process that classifies the given unseen data into one of the predefined
    sets of classes, given a set of input features and labels associated with them.
    Regression is very similar to classification, with one exception—in this, we have
    continuous target values instead of a fixed pre-defined set of classes (nominal
    or categorical attribute), and we predict the value in a continuous response for
    new unseen data.  Examples of such algorithms include naive bayes, support vector
    machines, k-nearest neighbors, linear regression, neural networks, and decision
    tree algorithms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unsupervised learning**: Without using the relationship that exists between
    a set of input and output variables, the unsupervised learning algorithm uses
    only the input to learn the patterns and clusters within the data. Unsupervised
    algorithms are used to learn the patterns in the given input data without labels
    associated with them. Clustering problems are one of the most popular types of
    problems that are solved using an unsupervised learning approach. In this, the
    data points are grouped together to form groups or clusters on the basis of the
    similarities among the features. Examples of such algorithms include k-means clustering,
    agglomerative clustering, and hierarchical clustering.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reinforcement learning**: The computer in this kind of learning method dynamically
    interacts with its environment in such a way as to improve its performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The hello classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's take a simple example to understand how machine learning works; we begin
    with a `hello world` example of a text classifier. This is meant to be a gentle
    introduction to machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: This example will predict whether the given text carries a negative or positive
    connotation. Before this can be done, we need to train our algorithm (model) with
    some data.
  prefs: []
  type: TYPE_NORMAL
- en: The naive bayes model is suited for text classification purposes. Algorithms
    based on the naive bayes models are generally fast and produce accurate results.
    It is based on the assumption that features are independent of each other. To
    accurately predict the occurrence of rainfall, three conditions need to be considered.
    These are wind speed, temperature, and the amount of humidity in the air. In reality,
    these factors do have an influence on each other to determine the likelihood of
    rainfall. But the abstraction in naive bayes is to assume that these features
    are unrelated in any way and thus independently contribute to the chances of rainfall.
    Naive bayes is useful in predicting the class of an unknown dataset, as we will
    see soon.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, back to our hello classifier. After we have trained our model, its prediction
    will fall into either the positive or negative categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: First, we will import the `NaiveBayesClassifier` class from the `textblob` package.
    This classifier is very easy to work with and is based on the bayes theorem.
  prefs: []
  type: TYPE_NORMAL
- en: The `train` variable consists of tuples that each hold the actual training data.
    Each tuple contains the sentence and the group it is associated with.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, to train our model, we will instantiate a `NaiveBayesClassifier` object
    by passing train to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The updated naive bayesian model `cl` will predict the category that an unknown
    sentence belongs to. Up to this point, our model has known of only two categories
    that a phrase can belong to, `neg` and `pos`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code runs tests using our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of our tests is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the algorithm has had some degree of success in classifying
    the input phrases into their categories correctly.
  prefs: []
  type: TYPE_NORMAL
- en: This contrived example is overly simplistic, but it does show the promise that
    if given the right amount of data and a suitable algorithm or model, it is possible
    for a machine to carry out tasks without any human help.
  prefs: []
  type: TYPE_NORMAL
- en: In our next example, we will use the `scikit` module to predict the category
    that a phrase may belong to.
  prefs: []
  type: TYPE_NORMAL
- en: A supervised learning example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's consider an example of the text classification problem, which can be solved
    using a supervised learning approach. The text classification problem is to classify
    a new document into one of the pre-defined sets of categories of documents when
    we have a set of documents related to a fixed number of categories. As with supervised
    learning, we need to first train the model in order to accurately predict the
    category of an unknown document.
  prefs: []
  type: TYPE_NORMAL
- en: Gathering data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `scikit` module comes with sample data that we can use for training the
    machine learning model. In this example, we will use the newsgroups documents,
    which have 20 categories of documents. To load those documents, we will use the
    following lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take only four categories of documents for training the model. After
    we have trained our model, the results of the prediction will belong to one of
    the following categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The total number of records we are going to use as training data is obtained
    by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Machine learning algorithms do not work on textual attributes directly, so
    the names of the categories that each document belongs to are denoted as numbers
    (for example, `alt.atheism` is denoted as `0`) using the following code line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The categories have integer values that we can map back to the categories themselves
    with `print(training_data.target_names[0])`.
  prefs: []
  type: TYPE_NORMAL
- en: Here, `0` is a numerical random index picked from `set(training_data.target)`.
  prefs: []
  type: TYPE_NORMAL
- en: Now that the training data has been obtained, we must feed the data to a machine
    learning algorithm. The bag of words model is an approach to convert the text
    document into a feature vector in order to turn the text into a form on which
    the learning algorithm or model can be applied. Furthermore, those feature vectors
    will be used for training the machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Bag of words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Bag of words is a model that is used for representing text data in such a way
    that it does not take into consideration the order of words but rather uses word
    counts. Let''s consider an example to understand how the bag of words method is
    used to represent text. Look at the following two sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Bag of words enables us to split the text into numerical feature vectors represented
    by a matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'To reduce our two sentences using the bag of words model, we need to obtain
    a unique list of all the words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This set will become our columns in the matrix, called the features in machine
    learning terminology. The rows in the matrix will represent the documents that
    are being used for training. The intersection of a row and column will store the
    number of times that word occurs in the document. Using our two sentences as examples,
    we obtain the following matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **as** | **fit** | **a** | **fiddle** | **you** | **like** | **it** |'
  prefs: []
  type: TYPE_TB
- en: '| **Sentence 1** | 2 | 1 | 1 | 1 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **Sentence 2** | 1 | 0 | 0 | 0 | 1 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: The preceding data has many features that are generally not important for text
    classification. The stop words can be removed to make sure only relevant data
    is analyzed. Stop words include is, am, are, was, and so on. Since the bag of
    words model does not include grammar in its analysis, the stop words can safely
    be dropped.
  prefs: []
  type: TYPE_NORMAL
- en: 'To generate the values that go into the columns of our matrix, we have to tokenize
    our training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '`training_matrix` has a dimension of (2,257 x 35,788) for the four categories
    of data we used in this example. This means that 2,257 corresponds to the total
    number of documents while 35,788 corresponds to the number of columns, which is
    the total number of features that make up the unique set of words in all documents.'
  prefs: []
  type: TYPE_NORMAL
- en: We instantiate the `CountVectorizer` class and pass `training_data.data` to
    the `fit_transform` method of the `count_vect` object. The result is stored in
    `training_matrix`. `training_matrix` holds all the unique words and their respective
    frequencies.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, frequency counts do not perform well for a text-classification problem;
    instead of using frequency count, we may use the **term frequency-inverse document
    frequency** (**TF-IDF**) weighting method for representing the features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, will import `TfidfTransformer`, which helps to assign the weights of
    each feature in our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '`tfidf_data[1:4].todense()` only shows a truncated list of a three rows by
    35,788 columns matrix. The values seen are the TF-IDF; it is a better representation
    method compared to using a frequency count.'
  prefs: []
  type: TYPE_NORMAL
- en: Once we have extracted features and represented them in a tabular format, we
    can apply a machine learning algorithm for training. There are many supervising
    learning algorithms; let's look at an example of the naive bayes algorithm to
    train a text classifier model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The naive bayes algorithm is a simple classification algorithm that is based
    on the bayes theorem. It is a probability-based learning algorithm that constructs
    a model by using the term frequency of a feature/word/term to compute the probability
    of belonging. The naive bayes algorithm classifies a given document into one of
    the predefined categories where there is the maximum probability of observing
    the words of the new document in that category. The naive bayes algorithm works
    as follows—initially, all training documents are processed to extract the vocabulary
    of all the words that appear in the text, then it counts their frequencies among
    the different target classes to obtain their probabilities. Next, a new document
    is classified in the category, which has the maximum probability of belonging
    to that particular class. The naive bayes classifier is based on the assumption
    that the probability of word occurrence is independent of position within the
    text. Multinomial naive bayes can be implemented using the `MultinomialNB` function
    of the `scikit` library, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '`MultinomialNB` is a variant of the naive bayes model. We pass the rationalized
    data matrix, `tfidf_data`, and categories, `training_data.target`, to its `fit`
    method.'
  prefs: []
  type: TYPE_NORMAL
- en: Prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To test how the trained model works to predict the category of an unknown document,
    let''s consider some example test data to evaluate the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The `test_data` list is passed to the `count_vect.transform` function to obtain
    the vectorized form of the test data. To obtain the TF-IDF representation of the
    test dataset, we call the `transform` method of the `matrix_transformer` object.
    When we pass new test data to the machine learning model, we have to process the
    data in the same way as we did in preparing the training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To predict which category the docs may belong to, we use the `predict` function
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The loop can be used to iterate over the prediction, showing the categories
    they are predicted to belong to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'When the loop has run to completion, the phrase, together with the category
    that it may belong to, is displayed. A sample output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: All that we have seen up to this point is a prime example of supervised learning.
    We started by loading documents whose categories were already known. These documents
    were then fed into the machine learning algorithm most suited for text processing,
    based on the naive bayes theorem. A set of test documents was supplied to the
    model and the category was predicted.
  prefs: []
  type: TYPE_NORMAL
- en: To explore an example of an unsupervised learning algorithm, we will discuss
    the k-means algorithm for clustering some data.
  prefs: []
  type: TYPE_NORMAL
- en: An unsupervised learning example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unsupervised learning algorithms are able to discover inherent patterns in the
    data that may exist and can cluster them in groups in such a way that the data
    points in one cluster are very similar and data points from two different clusters
    are highly dissimilar in nature. An example of these algorithms is the k-means
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: K-means algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The k-means algorithm uses the mean points in a given dataset to cluster and
    discover groups within the dataset. The `K` is the number of clusters that we
    want and are hoping to discover. After the k-means algorithm has generated the
    groupings/clusters, we can pass unknown data to this model to predict which cluster
    the new data should belong to.
  prefs: []
  type: TYPE_NORMAL
- en: Note that in this kind of algorithm, only the raw uncategorized data is fed
    to the algorithm without any labels associated with the data. It is up to the
    algorithm to find out if the data has inherent groups within it.
  prefs: []
  type: TYPE_NORMAL
- en: The k-means algorithm iteratively assigns the data points to the clusters based
    on the similarities among the features provided. K-means clustering groups the
    data points in k clusters/groups using the mean point. It works as follows. Firstly,
    we create k non-empty sets, and we compute the distance between the data point
    and the cluster center. Next, we assign the data point to the cluster that has
    the minimum distance and is closest. Next, we recalculate the cluster point and
    we iteratively follow the same process until all the data is clustered.
  prefs: []
  type: TYPE_NORMAL
- en: To understand how this algorithm works, let's examine `100` data points consisting
    of x and y values (assuming two attributes). We will feed these values to the
    learning algorithm and expect that the algorithm will cluster the data into two
    sets. We will color the two sets so that the clusters are visible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a sample data of 100 records of *x* and *y* pairs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: First, we create 100 records with `-2 * np.random.rand(100, 2)`. In each of
    the records, we will use the data in it to represent *x* and *y* values that will
    eventually be plotted.
  prefs: []
  type: TYPE_NORMAL
- en: The last 50 numbers in `original_set` will be replaced by `1+2*np.random.rand(50,
    2)`. In effect, what we have done is to create two subsets of data, where one
    set has numbers in the negative while the other set has numbers in the positive.
    It is now the responsibility of the algorithm to discover these segments appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: 'We instantiate the `KMeans` algorithm class and pass it `n_clusters=2`. That
    makes the algorithm cluster all its data into two groups. In the k-means algorithm,
    the number of clusters has to be known in advance. The implementation of the k-means
    algorithm using the `scikit` library is as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The dataset is passed to the `fit` function of `kmean`, `kmean.fit(original_set)`.
    The clusters generated by the algorithm will revolve around a certain mean point.
    The points that define these two mean points are obtained by `kmean.cluster_centers_`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The mean points when printed appear as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Each data point in `original_set` will belong to a cluster after our k-means
    algorithm has finished its training. The k-mean algorithm represents the two clusters
    it discovers as ones and zeros. If we had asked the algorithm to cluster the data
    into four, the internal representation of these clusters would have been 0, 1,
    2, and 3\. To print out the various clusters that each dataset belongs to, we
    do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'There are `100` ones and zeros. Each shows the cluster that each data point
    falls under. By using `matplotlib.pyplot`, we can chart the points of each group
    and color it appropriately to show the clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '`index = kmean.labels_ == i` is a nifty way by which we select all points that
    correspond to group `i`. When `i=0`, all points belonging to group zero are returned
    to the variable index. It''s the same for `index =1, 2`, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: '`plt.plot(original_set[index,0], original_set[index,1], ''o'')` then plots
    these data points using `o` as the character for drawing each point.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will plot the centroids or mean values around which the clusters have
    formed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, we show the whole graph with the two means illustrated by red star
    using the code snippet  `plt.show()`  as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/9130b340-90ee-4cee-b5ff-ca93c2b01e27.png)'
  prefs: []
  type: TYPE_IMG
- en: The algorithm discovers two distinct clusters in our sample data.
  prefs: []
  type: TYPE_NORMAL
- en: Prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the two clusters that we have obtained, we can predict the group that a
    new set of data might belong to.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s predict which group the points `[[-1.4, -1.4]]` and `[[2.5, 2.5]]` will
    belong to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Here, two test samples are assigned to two different clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Data visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The numerical analysis is sometimes not that easy to understand. In this section,
    we show you some methods to visualize the data and results. Images present a quick
    way to analyze data. Differences in size and length are quick markers in an image,
    upon which conclusions can be drawn. In this section, we will take a tour of the
    different ways to represent data. Besides the graphs listed here, there is more
    that can be achieved when dealing with data.
  prefs: []
  type: TYPE_NORMAL
- en: Bar chart
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To chart the values 25, 5, 150, and 100 into a bar graph, we will store the
    values in an array and pass it to the `bar` function. The bars in the graph represent
    the magnitude along the *y*-axis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '`x_values` stores an array of values generated by `range(len(data))`. Also,
    `x_values` will determine the points on the *x*-axis where the bars will be drawn.
    The first bar will be drawn on the *x*-axis where *x* is zero. The second bar
    with data 5 will be drawn on the *x*-axis where *x* is 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/5c2618e0-70aa-41eb-aa32-ff94e8b3f8c6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The width of each bar can be changed by modifying the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'This should produce the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/9cd8e72f-79e7-4c7e-9cfc-e80a375153f0.png)'
  prefs: []
  type: TYPE_IMG
- en: However, this is not visually appealing because there is no space between the
    bars anymore, which makes it look clumsy. Each bar now occupies one unit on the
    *x*-axis.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple bar charts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In trying to visualize data, stacking a number of bars enables one to further
    understand how one piece of data or variable varies compared to another:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The `y` values for the first batch of data are `[8., 57., 22., 10.]`. The second
    batch is `[16., 7., 32., 40.]`. When the bars are plotted, 8 and 16 will occupy
    the same `x` position, side by side.
  prefs: []
  type: TYPE_NORMAL
- en: '`x_values = np.arange(4)` generates the array with values `[0, 1, 2, 3]`. The
    first set of bars are drawn first at position `x_values + 0.30`. Thus, the first
    x values will be plotted at `0.00, 1.00, 2.00 and 3.00`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second batch of `x_values` will be plotted at `0.30, 1.30, 2.30` and `3.30`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/d0624b77-b67d-470b-96fb-22a604eafad6.png)'
  prefs: []
  type: TYPE_IMG
- en: Box plot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The box plot is used to visualize the median value and low and high ranges of
    a distribution. It is also referred to as a box and whisker plot.
  prefs: []
  type: TYPE_NORMAL
- en: Let's chart a simple box plot.
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin by generating `50` numbers from a normal distribution. These are then
    passed to `plt.boxplot(data)` to be charted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram is what is produced:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/05876ad6-cc46-4141-bfa8-bb3b119b6c0e.png)'
  prefs: []
  type: TYPE_IMG
- en: A few comments on the preceding diagram—the features of the box plot include
    a box spanning the interquartile range, which measures the dispersion; the outer
    fringes of the data are denoted by the whiskers attached to the central box; the
    red line represents the median.
  prefs: []
  type: TYPE_NORMAL
- en: The box plot is useful to easily identify the outliers in a dataset, as well
    as determining in which direction a dataset may be skewed.
  prefs: []
  type: TYPE_NORMAL
- en: Pie chart
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The pie chart interprets and visually represents the data as if to fit into
    a circle. The individual data points are expressed as sectors of a circle that
    add up to 360 degrees. This chart is good for displaying categorical data and
    summaries too:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The sectors in the graph are labeled with the strings in the labels array:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/17ea1d44-5589-427c-8c5c-0d37c801d374.png)'
  prefs: []
  type: TYPE_IMG
- en: Bubble chart
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another variant of the scatter plot is the bubble chart. In a scatter plot,
    we only plot the `x` and `y` points of the data. Bubble charts add another dimension
    by illustrating the size of the points. This third dimension may represent sizes
    of markets or even profits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: With the `n` variable, we specify the number of randomly generated `x` and `y`
    values. This same number is used to determine the random colors for our `x` and
    `y` coordinates. Random bubble sizes are determined by `area = np.pi * (60 * np.random.rand(n))**2`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows this bubble chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/9b77b68c-6fed-44e9-93f5-c9f23357954a.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have explored how data and algorithms come together to aid
    machine learning. Making sense of huge amounts of data is made possible by first
    pruning our data through data cleaning techniques and scaling and normalization
    processes. Feeding this data to specialized learning algorithms, we are able to
    predict the categories of unseen data based on the patterns learned by the algorithm
    from the data. We also discussed the basics of machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: We explained supervised and unsupervised machine learning algorithms in detail
    with the naive bayes and k-means clustering algorithms. We also provided the implementation
    of these algorithms using the `scikit-learn` Python-based machine learning library. Finally,
    some important visualization techniques were discussed, as charting and plotting
    the condensed data helps you to better understand and make insightful discoveries.
  prefs: []
  type: TYPE_NORMAL
- en: I hope you had a good experience with this book and that it helps in your future
    endeavors with data structures and Python 3.7!
  prefs: []
  type: TYPE_NORMAL
