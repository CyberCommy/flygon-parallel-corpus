- en: Chapter 2. Does This Code Work? Doctests in Depth
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the first chapter, we learned how to run the sample tests created by `manage.py
    startapp`. Although we used a Django utility to run the tests, there was nothing
    specific to Django about the sample tests themselves. In this chapter, we will
    start getting into details of how to write tests for a Django application. We
    will:'
  prefs: []
  type: TYPE_NORMAL
- en: Begin writing the market research project created in the first chapter by developing
    some basic models that will be used by the project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experiment with adding doctests to one of the models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Begin to learn the kinds of tests that are useful, and the kinds that just add
    clutter to the code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discover some of the advantages and disadvantages of doctests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'While the previous chapter mentioned both doctests and unit tests, the focus
    for this chapter will be on doctests exclusively. Developing unit tests for Django
    applications will be the focus of [Chapter 3](ch03.html "Chapter 3. Testing 1,
    2, 3: Basic Unit Testing"), *Testing 1, 2, 3: Basic Unit Testing* and [Chapter
    4](ch04.html "Chapter 4. Getting Fancier: Django Unit Test Extensions"), *Getting
    Fancier: Django Unit Test Extensions*.'
  prefs: []
  type: TYPE_NORMAL
- en: The Survey application models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A common place to start development of a new Django application is with the
    models: the basic building blocks of data that are going to be manipulated and
    stored by the application. A cornerstone model for our example market research
    `survey` application will be the `Survey` model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A `Survey` is going to be similar to the Django tutorial `Poll` model, except
    that:'
  prefs: []
  type: TYPE_NORMAL
- en: Where the tutorial `Poll` only contains one question, a `Survey` will have multiple
    questions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `Survey` will have a title for reference purposes. For the tutorial `Poll`,
    a single question could be used for this.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `Survey` will only be open for responses for a limited (and variable, depending
    on the `Survey` instance) time. While the `Poll` model has a `pub_date` field,
    it is not used for anything other than ordering `Polls` on the index page. Thus,
    `Survey` will need two date fields where `Poll` has only one, and the `Survey`
    date fields will be used more than the `Poll pub_date` field is used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Given just these few simple requirements for `Survey`, we can start developing
    a Django model for it. Specifically, we can capture those requirements in code
    by adding the following to the auto-generated `models.py` file for our `survey`
    application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that since a `Survey` may have several questions, it does not have a question
    field. Instead there is a separate model, `Question`, to hold questions along
    with the Survey instance they are related to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The final model we need (at least to start with) is one to hold the possible
    answers to each question, and to track how many times each answer is chosen by
    a survey respondent. This model, `Answer`, is much like the tutorial `Choice`
    model, except it is related to a `Question`, not a `Poll`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Testing the Survey model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are at all like me, at this point you might want to start verifying that
    what you've got so far is correct. True, there is not much code yet, but particularly
    when just starting out on a project I like to make sure, early and often, that
    what I've got so far is valid. So, how do we start testing at this point? First,
    we can verify that we've got no syntax errors by running `manage.py syncdb`, which
    will also let us start experimenting with these models in a Python shell. Let's
    do that. Since this is the first time we've run `syncdb` for this project, we'll
    get messages about creating tables for the other applications listed in `INSTALLED_APPS`,
    and we'll be asked if we want to create a superuser, which we may as well go ahead
    and do also.
  prefs: []
  type: TYPE_NORMAL
- en: Testing Survey model creation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, what might we do with these models to test them in a Python shell? Really,
    not much beyond creating each, perhaps verifying that if we don''t specify one
    of the fields we get an error, or the correct default value is assigned, and verifying
    whether we can traverse the relationships between the models. If we focus first
    on the `Survey` model and what we might do in order to test the creation of it,
    a Python shell session for that might look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Here we started by importing our `Survey` model and the Python `datetime` module,
    then created a variable `t` to hold a title string and a variable `d` to hold
    a date value, and used those values to create a `Survey` instance. No error was
    reported, so that looks good.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we then wanted to verify whether we''d get an error if we tried to create
    a `Survey` with no close date, we would proceed with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Here all we did differently with the `Survey` instance creation attempt was
    specify `None` for the `closes` value instead of passing in our date variable
    `d`. The result was an error ending in a message reporting an `IntegrityError`,
    since the closes column of the survey table cannot be null. This confirms our
    expectation of what should happen, so all is good so far. We could then perform
    similar tests for the other fields, and see identical tracebacks reporting an
    `IntegrityError` for the other columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we wanted to, we could then make these tests a permanent part of our model
    definition by cutting-and-pasting them from our shell session directly in our
    `survey/models.py` file, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'You probably noticed that the results shown are not a direct cut-and-paste
    from the shell session. Differences include:'
  prefs: []
  type: TYPE_NORMAL
- en: The `import datetime` was moved out of the doctest and made part of the code
    in the `models.py` file. This wasn't strictly necessary—it would have worked fine
    as part of the doctest, but it is not necessary in the doctest if the import is
    in the main code. As the code in `models.py` will likely need to use `datetime`
    functions later on, putting the import in the main code now reduces duplication
    and clutter later, when the main code needs the import.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The call stack portion of the tracebacks, that is everything except the first
    and last lines, were removed and replaced with lines containing three dots. This
    too was not strictly necessary and was done simply to remove clutter and highlight
    the important bits of the result. The doctest runner ignores the contents of the
    call stack (if present in the expected output) when deciding on test success or
    failure. So you can leave a call stack in the test if it has some explanatory
    value. However, for the most part, it is best to remove call stacks since they
    produce a lot of clutter without providing much in the way of useful information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If we now run `manage.py test survey -v2`, the tail end of the output will
    be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We've still got our sample tests in `tests.py` running, and now we can also
    see our `survey.models.Survey` doctest listed as being run, and passing.
  prefs: []
  type: TYPE_NORMAL
- en: Is that test useful?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'But wait; is that test we just added useful? What is it actually testing? Nothing
    really, beyond verifying that basic Django functions work as advertised. It tests
    whether we can create an instance of a model we''ve defined, and that the fields
    we specified as required in the model definition are in fact required in the associated
    database table. It seems that this test is testing the underlying Django code
    more than our application. Testing Django itself is not necessary in our application:
    Django has its own test suite we can run if we want to test it (though it is pretty
    safe to assume basic functions work correctly in any released version of Django).'
  prefs: []
  type: TYPE_NORMAL
- en: It could be argued that this test validates that the correct and intended options
    have been specified for each field in the model, and so it is a test of the application
    and not just the underlying Django functions. However, testing things that are
    obvious by inspection (to anyone with a basic knowledge of Django) strikes me
    as going a bit overboard. This is not a test I would generally include in a project
    I was writing.
  prefs: []
  type: TYPE_NORMAL
- en: 'That is not to say I would not try out things like this in a Python shell during
    development: I would, and I do. But not everything experimented with in the shell
    during development needs to become a permanent test in the application. The kinds
    of tests you want to include in the application are those that exercise behavior
    that is unique to the application. So let''s start developing some survey application
    code and experiment with testing it in the Python shell. When we have the code
    working, we can assess what tests from the shell session are useful to keep.'
  prefs: []
  type: TYPE_NORMAL
- en: Developing a custom Survey save method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To begin writing some application-specific code, consider that for the Survey
    model we may want to allow for the `closes` field to assume a default value of
    a week after `opens`, if `closes` is not specified when the model instance is
    created. We cannot use the Django model field default option for this, as the
    value we want to assign is dependent on another field in the model. Therefore,
    we would typically do this by overriding the model''s save method. A first attempt
    at implementing this might be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'That is, in the case where `save` is called and the model instance does not
    have a primary key assigned yet (and so this is the first save to the database),
    and `closes` has not been specified, we assign `closes` a value that is a week
    later than `opens` before calling the superclass `save` method. We could then
    test if this works properly by experimenting in a Python shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This is very similar to our earlier tests except we chose a specific date to
    assign to `opens` rather than using today's date, and after creating the `Survey`
    instance without specifying a value for `closes`, we checked the value that was
    assigned to it. The value displayed is a week later than `opens`, so that looks
    good.
  prefs: []
  type: TYPE_NORMAL
- en: Note the choice of an `opens` date where the week-later value would be in the
    next month and year was deliberate. Testing boundary values is always a good idea
    and a good habit to get into, even when (as here) there is nothing in the code
    we are writing that is responsible for getting the answer right for the boundary
    case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next we might want to make sure that if we do specify a value for `closes`,
    it is honored and not overridden by a week-later default date:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'All looks good there, `opens` and `closes` are displayed as having the same
    value, as we specified on the `create` call. We can also verify that if we reset
    `closes` to `None` after the model has already been saved, and try to save again,
    we''ll get an error. Resetting `closes` to `None` on an existing model instance
    would be an error in the code that does that. So what we are testing here is that
    our `save` method override does not hide that error by quietly re-assigning a
    value to `closes`. In our shell session, we proceed like so and see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, that looks good since it is the result we expect. Finally, since we
    have inserted some of our own code into the basic model save processing, we should
    verify that we have not broken anything for the other expected failure cases where
    no `title` or no `opens` field is specified on `create`. If we do that, we will
    see that the case of no `title` specified works correctly (we get the expected
    `IntegrityError` on the database title column), but if neither `opens` nor `closes`
    is specified we get an unexpected error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we have traded a reasonably clear error message reporting that we have
    left a required value unspecified for a rather more obscure message complaining
    about unsupported operand types—that''s not good. The problem is we did not check
    if `opens` had a value before attempting to use it in our `save` method override.
    In order to get the correct (clearer) error for this case, our `save` method should
    be modified to look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'That is, we should not attempt to set `closes` if `opens` has not been specified.
    Rather, in this case we forward the `save` call directly to the superclass and
    let the normal error path report the problem. Then, when we try to create a `Survey`
    without specifying an `opens` or `closes` value, we will see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: That is much better, since the reported error directly indicates what the problem
    is.
  prefs: []
  type: TYPE_NORMAL
- en: Deciding what to test
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At this point we are reasonably certain our `save` override is working the
    way we intended. Of all the tests we ran in the Python shell for verification
    purposes, which ones make sense to include in the code permanently? The answer
    to that question involves a judgment call, and reasonable people may have different
    answers. Personally, I would tend to include:'
  prefs: []
  type: TYPE_NORMAL
- en: All tests involving the parameter(s) directly affected by the code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any tests that I ran across while doing initial testing of the code that did
    not work in the original version of the code I had written
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, my `save` override function, including doctests with comments to explain
    them, might look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Some pros and cons of doctests so far
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even with the experience of just this one example method we have studied, we
    can begin to see some of the pros and cons of doctests. Clearly, it is easy to
    re-use work done in Python shell sessions (work that is likely already being done
    as part of coding) for permanent test purposes. This makes it both more likely
    that tests will be written for the code, and that the tests themselves will not
    need to be debugged. Those are two nice advantages of doctests.
  prefs: []
  type: TYPE_NORMAL
- en: A third is that doctests provide unambiguous documentation of how the code is
    expected to behave. Prose descriptions can be fuzzy while code examples in the
    form of tests are impossible to misinterpret. Furthermore, the fact that the tests
    are part of the docstrings makes them accessible to all Python tools that use
    docstrings to auto-generate help and documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Including tests here helps to make this documentation complete. For example,
    the behavior after resetting `closes` to `None` is one where the intended behavior
    might not be obvious—an equally valid design would have been to say that in this
    case `closes` would be reset to a week-later date during `save`. This sort of
    detail can easily be forgotten when writing documentation. Thus having the intended
    behavior spelled out in a doctest is helpful, as it is then automatically documented.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, this tests-doubling-as-documentation feature also has a down side:
    some of the testing you may want to include may not really be appropriate as documentation,
    and you may wind up with an overwhelming amount of documentation for rather simple
    code. Consider the `save` override case we developed. It has four lines of code
    and a more than 30 line docstring. That ratio may be appropriate for some complicated
    functions with many parameters, or parameters that interact in non-obvious ways,
    but nearly ten times as much documentation as code seems excessive for this straightforward
    method.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider the individual tests in `save`, focusing on their usefulness
    as documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: The first test, which shows creating a `Survey` with `title` and `opens` but
    no `closes`, and verifies that the correct value is assigned to `closes` after
    creation, is an example of what the `save` override allows a caller to do. This
    is the specific call pattern enabled by the added code, and is therefore useful
    as documentation, even though it largely duplicates the prose description.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second test, which shows that `closes` is honored if specified, is not particularly
    useful as documentation. Any programmer would expect that if `closes` is specified,
    it should be honored. This behavior may be good to test, but is not necessary
    to document.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third test, which illustrates the expected behavior of `save` after resetting
    `closes` to `None` on an existing `Survey` instance, is useful as documentation,
    for the previously-mentioned reasons.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fourth and final test illustrates that the added code will not cause an
    unexpected exception to be generated in the error case where neither `opens` nor
    `closes` is specified. This is another example of something that is good to test,
    but not necessary to document, as the right behavior is obvious.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Having half of our docstring classified as not useful for documentation purposes
    is not good. People tend to stop reading when they encounter obvious, redundant,
    or unhelpful information. We can address this problem without giving up some of
    the advantages of doctests by moving such tests from the docstring method into
    our `tests.py` file. If we take this approach, we might change the `__test__`
    dictionary in `tests.py` to look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Here we changed the key for the test from the generic `doctest` to `survey_save`,
    so that the reported test name in any test output will give a hint as to what
    is being tested. Then we just moved the "non-documentation" tests (along with
    some of the variable setup code that now needs to be in both places) from our
    `save` override docstring into the key value here, adding a general comment at
    the top noting what the tests are for.
  prefs: []
  type: TYPE_NORMAL
- en: 'What remains in the docstring for the `save` method itself are the tests that
    do have some value as documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: That is certainly a much more manageable docstring for the function, and is
    no longer likely to overwhelm someone typing `help(Survey.save)` in a Python shell.
  prefs: []
  type: TYPE_NORMAL
- en: This approach, though, does also have its down side. The tests for the code
    are no longer all in one place, making it hard to know or easily determine how
    completely the code is tested. Anyone who ran across the test in `tests.py`, without
    knowing there were additional tests in the method's docstring, might well wonder
    why only these two edge cases were tested and why a straightforward test of the
    basic function added was omitted.
  prefs: []
  type: TYPE_NORMAL
- en: Also, when adding tests, it may not be clear (especially to programmers new
    to the project) where exactly the new tests should go. So even if a project starts
    out with a nice clean split of "tests that make for good documentation" in the
    docstring tests and "tests that are necessary but not good documentation" in the
    `tests.py` file, this distinction may easily become blurred over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Test choice and placement thus involves a tradeoff. There is not necessarily
    a "right" answer for every project. Adopting a consistent approach, though, is
    best. When choosing that approach, each project team should take into account
    the answers to questions such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '**What is the expected audience for auto-generated docstring-based documentation?**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If other documentation exists (or is being written) that is expected to be the
    main source for "consumers" of the code, then it may not be a problem to have
    doctests that do not serve the documentation function very well.
  prefs: []
  type: TYPE_NORMAL
- en: '**How many people will likely be working on the code?**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If it is a relatively small and constant number, it may not be much of an issue
    to get everyone to remember about tests split between two places. For a larger
    project or if there is high developer turnover, educating developers about this
    sort of split may become more of an issue and it may be harder to maintain consistent
    code.
  prefs: []
  type: TYPE_NORMAL
- en: Additional doctest caveats
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Doctests have some additional disadvantages that we haven't necessarily run
    into or noticed yet. Some of these are just things we need to watch out for if
    we want to make sure our doctests will work properly in a wide variety of environments
    and as code surrounding our code changes. Others are more serious issues that
    are most easily solved by switching to unit tests instead of doctests for at least
    the affected tests. In this section, we will list many of the additional doctest
    issues to watch out for, and give guidance on what to do to avoid or overcome
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Beware of environmental dependence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is very easy for doctests to be unintentionally dependent on implementation
    details of code other than the code that is actually being tested. We have some
    of this already in the `save` override tests, though we have not tripped over
    it yet. The dependence we have is actually a very specific form of environmental
    dependence—database dependence. As database dependence is a fairly big issue on
    its own, it will be discussed in detail in the next section. However, we'll first
    cover some other minor environmental dependencies we might easily run into and
    see how to avoid including them in our tests.
  prefs: []
  type: TYPE_NORMAL
- en: 'An extremely common form of environmental dependence that creeps into doctests
    is relying on the printed representation of objects. For example, a `__unicode__`
    method is a common method to be implemented in a model class first. It was omitted
    from the earlier `Survey` model discussion since it wasn''t necessary at that
    time, but in reality we probably would have implemented `__unicode__` before the
    `save` override. A first pass at a `__unicode__` method for `Survey` may have
    looked something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we have decided that the printed representation of a `Survey` instance
    will consist of the title value followed by a parenthesized note about when this
    survey opens and closes. Given that method definition, our shell session for testing
    the proper setting of `closes` when it is not specified during creation may have
    looked something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: That is, instead of specifically checking the value assigned to `closes`, we
    may have just displayed the printed representation of the created instance, since
    it includes the `closes` value. When experimenting in a shell session, it's natural
    to perform checking this way rather than interrogating the attribute in question
    directly. For one thing, it's shorter (`s` is a good bit easier to type than `s.closes`).
    In addition, it often displays more information than the specific piece we may
    be testing, which is helpful when we are experimenting.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if we had cut and pasted directly from that shell session into our
    `save` override doctest, we would have made that doctest dependent on the implementation
    details of `__unicode__`. We might subsequently decide we didn''t want to include
    all of that information in the printable representation of a `Survey`, or even
    just that it would look better if the "o" in "Opens" was not capitalized. So we
    make a minor change to the `__unicode__` method implementation and suddenly a
    doctest for an unrelated method begins to fail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Thus when creating doctests from shell sessions, it's good to carefully consider
    whether the session relied on implementation details of any code other than that
    specifically being tested, and if so make adjustments to remove the dependence.
    In this case, using `s.closes` to test what value has been assigned to `closes`
    removes the dependence on how the `Survey` model `__unicode__` method happens
    to be implemented.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many other cases of environmental dependence that may arise in doctests,
    including:'
  prefs: []
  type: TYPE_NORMAL
- en: Any test that relies on the printed representation of a file path can run afoul
    of the fact that on Unix-based operating systems path components are separated
    by a forward slash where Windows uses a backslash. If you need to include doctests
    that rely on file path values, it may be necessary to use a utility function to
    normalize file path representations across different operating systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any test that relies on dictionary keys being printed in a specific order can
    run afoul of the fact that this order may be different for different operating
    systems or Python implementations. Thus to make such tests robust across different
    platforms, it may be necessary to specifically interrogate dictionary key values
    instead of simply printing the entire dictionary contents, or use a utility function
    that applies a consistent order to the keys for the printed representation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There is nothing particularly specific to Django about these kinds of environmental
    dependence issues that often arise in doctests. There is, however, one type of
    environmental dependence that is particularly likely to arise in a Django application:
    database dependence. This issue is discussed next.'
  prefs: []
  type: TYPE_NORMAL
- en: Beware of database dependence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Django **object-relational manager** (**ORM**) goes through considerable
    trouble to shield application code from differences in the underlying databases.
    However, it is not feasible for Django to make all of the different supported
    databases look exactly the same under all circumstances. Thus it is possible to
    observe database-specific differences at the application level. These differences
    may then easily find their way into doctests, making the tests dependent on a
    specific database backend in order to pass.
  prefs: []
  type: TYPE_NORMAL
- en: 'This sort of dependence is already present in the `save` override tests developed
    earlier in this chapter. Because SQLite is the easiest database to use (since
    it requires no installation or configuration), so far the example code and tests
    have been developed using a setting of `DATABASE_ENGINE = ''sqlite3''` in `settings.py`.
    If we switch to using MySQL (`DATABASE_ENGINE = ''mysql''`) for the database instead,
    and attempt to run our `survey` application tests, we will see failures. There
    are two failures, but we will first focus only on the last one in the test output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'What''s the problem here? For the `save` call in the doctest in `tests.py`
    where no value for `opens` was specified, an `IntegrityError` was expected, and
    an `IntegrityError` was produced, but the details of the `IntegrityError` message
    are different. The SQLite database returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'MySQL says the same thing somewhat differently:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: There are two simple ways to fix this. One is to use the doctest directive `IGNORE_EXCEPTION_DETAIL`
    on the failing test. With this option, the doctest runner will only consider the
    type of exception (in this case, `IntegrityError`) when determining whether the
    expected result matches the actual result. So differences in the exact exception
    messages produced by the different databases will not cause the test to fail.
  prefs: []
  type: TYPE_NORMAL
- en: 'Doctest directives are specified for individual tests by placing them as comments
    on the line containing the test. The comment starts with `doctest:` and is followed
    by one or more directive names preceded either by `+` to turn the option on or
    `–` to turn the option off. So in this case, we would change the failing test
    line in `tests.py` to be (note that though this line wraps to a second line on
    this page, it needs to be kept on a single line in the test):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The other way to fix this is to replace the detailed message portion of the
    expected output in the test with three dots, which is an ellipsis marker. That
    is, change the test to be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This is an alternate way to tell the doctest runner to ignore the specifics
    of the exception message. It relies on the doctest option `ELLIPSIS` being enabled
    for the doctest run. While this option is not enabled by default by Python, it
    is enabled by the doctest runner that Django uses, so you do not need to do anything
    in your test code to enable use of ellipsis markers in expected output. Also note
    that `ELLIPSIS` is not specific to exception message details; it's a more general
    method that lets you indicate portions of doctest output that may differ from
    run to run without resulting in test failure.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you read the Python documentation for `ELLIPSIS`, you may notice that it
    was introduced in Python 2.4\. You may expect, then, if you are running Python
    2.3 (which is still supported by Django 1.1), that you would not be able to use
    the ellipsis marker technique in your Django application's doctests. However,
    Django 1.0 and 1.1 ship with a customized doctest runner that is used when you
    run your application's doctests. This customized runner is based on the doctest
    module that is shipped with Python 2.4\. Thus you can use doctest options, such
    as `ELLIPSIS`, from Python 2.4 even if you are running an earlier Python version.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note, though, the flip side of Django using its own customized doctest runner:
    if you are running a more recent Python version than 2.4, you cannot use doctest
    options added later than 2.4 in your application''s doctests. For example, Python
    added the `SKIP` option in Python 2.5\. Until Django updates the version of its
    customized doctest module, you will not be able to use this new option in your
    Django application doctests.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that there were two test failures and we only looked at the output from
    one (the other most likely scrolled off the screen too quickly to read). Given
    what the one failure we examined was, though, we might expect the other one would
    be the same, since we have a very similar test for an `IntegrityError` in the
    doctest in `models.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This will certainly also need to be fixed to ignore the exception detail, so
    we may as well do both at the same time and perhaps correct both test failures.
    And in fact, when we run the tests again after changing both expected `IntegrityErrors`
    to include an ellipsis marker instead of a specific error message, the tests all
    pass.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that for some configurations of MySQL, this second test failure will not
    be corrected by ignoring the exception details. Specifically, if the MySQL server
    is configured to run in "non-strict" mode, attempting to update a row to contain
    a `NULL` value in a column declared as `NOT NULL` does not raise an error. Rather,
    the value is set to the implicit default value for the column's type and a warning
    is issued.
  prefs: []
  type: TYPE_NORMAL
- en: Most likely if you are using MySQL, you will want to configure it to run in
    "strict mode". However, if for some reason you cannot, and you need to have a
    test like this in your application, and you need the test to pass on multiple
    databases, you would have to account for that difference in database behavior
    in your test. It can be done, but it is much more easily done in a unit test than
    a doctest, so we will not cover how to fix the doctest for this case.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have gotten our tests to pass on two different database backends,
    we may think we are set and would likely get a clean test run on all databases
    that Django supports. We'd be wrong, as we will discover when we attempt to run
    these same tests using PostgreSQL as the database. The database difference we
    encounter with PostgreSQL highlights the next item to beware of when writing doctests,
    and is covered in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Beware of test interdependence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We get a very curious result if we now try running our tests using PostgreSQL
    as the database (specify `DATABASE_ENGINE = ''postgresql_psycopg2''` in `settings.py`).
    From the tail end of the output of `manage.py test survey -v2`, we see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The sample unit test we still have in `tests.py` runs and passes, then the
    doctest from `models.py` also passes, but the doctest we added to `tests.py` fails.
    The failure details are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '****This time we need to examine the reported errors in order as the second
    error is resulting from the first. Such chaining of errors is common, so it is
    good to keep in mind that while it may be tempting to start by looking at the
    last failure, since it is the easiest one to see at the end of the test run, that
    may not be the most productive route. If it isn''t immediately obvious what is
    causing the last failure, it''s usually best to start at the beginning and figure
    out what is causing the first failure. The reason for subsequent failures may
    then become obvious. For reference, the beginning of the test that is failing
    is:****'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '****Thus, based on the test output, the very first attempt to access the database—that
    is the attempt to create a `Survey` instance—in this test results in an error:****'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '****Then the next line of the test also results in an error as it uses the
    variable `s` that was supposed to be assigned in the previous line. However, that
    line did not complete execution, so the variable `s` is not defined when the test
    attempts to use it. So the second error makes sense given the first, but why did
    the first database access in this test result in an error?****'
  prefs: []
  type: TYPE_NORMAL
- en: '****In order to understand the explanation for that, we have to look back at
    the test that ran immediately preceding this one. We can see from the test output
    that the test immediately preceding this one was the doctest in `models.py`. The
    end of that test is:****'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '****The last thing that test did was something that was expected to raise a
    database error. A side-effect of this, on PostgreSQL, is that the database connection
    enters a state where the only commands it will allow are ones that end the transaction
    block. So this test ended leaving the database connection in a broken state, and
    it was still broken when the next doctest began running, causing the next doctest
    to fail as soon as it attempted any database access.****'
  prefs: []
  type: TYPE_NORMAL
- en: '****This problem illustrates that there is no database isolation between doctests.
    What one doctest does to the database can be observed by subsequent ones that
    run. This includes problems such as the one seen here, in addition to creation,
    updates, or deletion of rows in the database tables. This particular problem can
    be solved by adding a call to rollback the current transaction following the code
    that deliberately caused a database error:****'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '****This will allow the tests to pass on PostgreSQL and will be harmless on
    the other database backends. Thus one way to deal with no database isolation in
    doctests is to code them so that they clean up after themselves. That may be an
    acceptable approach for problems such as this one, but if a test has added, modified,
    or deleted objects in the database, it may be difficult to put everything back
    the way it was originally at the end.****'
  prefs: []
  type: TYPE_NORMAL
- en: '****A second approach is to reset the database to a known state on entry to
    every doctest. Django does not do this for you, but you can do it manually by
    calling the management command to synchronize the database. I would not recommend
    this approach in general because it becomes extremely time-consuming as your application
    grows.****'
  prefs: []
  type: TYPE_NORMAL
- en: '****A third approach is to make doctests reasonably tolerant of database state,
    so that they will be likely to run properly regardless of whether other tests
    may or may not have run before them. Techniques to use here include:****'
  prefs: []
  type: TYPE_NORMAL
- en: '****Create all objects needed by the test in the test itself. That is, do not
    rely on the existence of objects created by any previously-run tests since that
    test may change, or be removed, or the order in which tests run may change at
    some time.****'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '****When creating objects, guard against collisions with similar objects that
    may be created by other tests. For example, if a test needs to create a `User`
    instance with the `is_superuser` field set to `True` in order to test certain
    behavior for users that have that attribute, it might seem natural to give the
    `User` instance a `username` of "superuser". However, if two doctests did that,
    then whichever one was unlucky enough to run second would encounter an error because
    the `username` field of the `User` model is declared to be unique, so the second
    attempt to create a `User` with this `username` would fail. Thus it is best to
    use values for unique fields in shared models that are unlikely to have been used
    by other tests.****'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '****All of these approaches and techniques have their disadvantages. For this
    particular issue, unit tests are a much better solution, as they automatically
    provide database isolation without incurring a performance cost to reset the database
    (so long as you run them on a database that supports transactions). Thus if you
    start encountering a lot of test interdependence issues with doctests, I''d strongly
    suggest considering unit tests as a solution instead of relying on any of the
    approaches listed here.****'
  prefs: []
  type: TYPE_NORMAL
- en: '****Beware of Unicode****'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '****The final issue we will cover in doctest caveats is Unicode. If you have
    done much work with Django (or even just Python) using data from languages with
    character sets broader than English, you''ve likely run into `UnicodeDecodeError`
    or `UnicodeEncodeError` once or twice. As a result, you may have gotten into the
    habit of routinely including some non-ASCII characters in your tests to ensure
    that everything is going to work properly for all languages, not just English.
    That''s a good habit, but unfortunately testing with Unicode values in doctests
    has some unexpected glitches that need to be overcome.****'
  prefs: []
  type: TYPE_NORMAL
- en: '****The previously mentioned `__unicode__` method of `Survey` would be a likely
    place we would want to test for proper behavior in the face of non-ASCII characters.
    A first pass at a test for this might be:****'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '****This test is similar to many of the save override tests in that it first
    creates a `Survey` instance. The significant parameter in this case is the title,
    which is specified as a Unicode literal string and contains non-ASCII characters.
    After the `Survey` instance is created, a call is made to print it in order to
    verify that the non-ASCII characters are displayed correctly in the printed representation
    of the instance, and that no Unicode exceptions are raised.****'
  prefs: []
  type: TYPE_NORMAL
- en: '****How well does this test work? Not so well. Attempting to run the survey
    tests after adding that code will result in an error:****'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '****This one is easy to fix; we simply forgot to declare the encoding for our
    Python source file. To do that, we need to add a comment line to the top of the
    file specifying the encoding used by the file. Let''s assume we are using UTF-8
    encoding, so we should add the following as the first line of our `models.py`
    file:****'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '****Now will the new test work? Not yet, we still get a failure:****'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '****This one is a bit puzzling. Though we specified the title as a Unicode
    literal string `u''¿Como está usted?''` in our test, it is apparently coming back
    as **Â¿Como estÃ¡ usted?** when printed. Data corruption like this is a telltale
    sign that the wrong encoding has been used at some point to transform a bytestring
    into a Unicode string. In fact the specific nature of the corruption here, where
    each non-ASCII character in the original string has been replaced by two (or more)
    characters in the corrupted version, is the characteristic of a string which is
    actually encoded in UTF-8 being interpreted as if it were encoded in ISO-8859-1
    (also called Latin-1). But how could that happen here, as we specified UTF-8 as
    our Python file encoding declaration? Why would this string be interpreted using
    any other encoding?****'
  prefs: []
  type: TYPE_NORMAL
- en: '****At this point, we might go and carefully read the web page referenced in
    the first error message we got, and learn that the encoding declaration we have
    added only has an effect on how Unicode literal strings are constructed by the
    Python interpreter from the source file. We may then notice that though our title
    is a Unicode literal string, the doctest it is contained in is not. So perhaps
    this odd result is because we neglected to make the docstring containing the doctest
    a Unicode literal. Our next version of the test, then, might be to specify the
    whole docstring as a Unicode literal.****'
  prefs: []
  type: TYPE_NORMAL
- en: '****Unfortunately this too would be unsuccessful, due to problems with Unicode
    literal docstrings. First the doctest runner cannot correctly compare expected
    output (now Unicode, since the docstring itself is a Unicode literal) with actual
    output that is a bytestring containing non-ASCII characters. Such a bytestring
    must be converted to Unicode in order to perform the comparison. Python will automatically
    perform this conversion when necessary, but the problem is that it does not know
    the actual encoding of the bytestring it is converting. Thus it assumes ASCII,
    and fails to perform the conversion if the bytestring contains any non-ASCII characters.****'
  prefs: []
  type: TYPE_NORMAL
- en: '****This failure in conversion will lead to an assumed failure of the comparison
    involving the bytestring, which in turn will lead to the test being reported as
    failing. Even if the expected and received outputs were identical, if only the
    right encoding were assumed for the bytestring, there is no way to get the proper
    encoding to be used, so the test will fail. For the `Survey` model `__unicode__`
    doctest, this problem will cause the test to fail when attempting to compare the
    actual output of `print s` (which will be a UTF-8 encoded bytestring) to the expected
    output.****'
  prefs: []
  type: TYPE_NORMAL
- en: '****A second problem with Unicode literal docstrings involves reporting of
    output that contains non-ASCII characters, such as this failure that will occur
    with the `Survey` model `__unicode__` doctest. The doctest runner will attempt
    to display a message showing the expected and received outputs. However, it will
    run into the same problem as encountered during the comparison when it attempts
    to combine the expected and received outputs into a single message for display.
    Thus instead of generating a message that would at least reveal where the test
    is running into trouble, the doctest runner itself generates a `UnicodeDecodeError`.****'
  prefs: []
  type: TYPE_NORMAL
- en: '****There is an open Python issue in Python''s bug tracker that reports these
    problems: [http://bugs.python.org/issue1293741](http://bugs.python.org/issue1293741).
    Until it is fixed, it is probably best to avoid using Unicode literal docstrings
    for doctests.****'
  prefs: []
  type: TYPE_NORMAL
- en: '****Is there any way, then, to include some testing of non-ASCII data in doctests?
    Yes, it is possible. The key to making such tests work is to avoid using Unicode
    literals within the docstring. Instead, explicitly decode strings to Unicode objects.
    For example:****'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '****That is, replace the Unicode literal title string with a bytestring that
    is explicitly decoded using UTF-8 to create a Unicode string.****'
  prefs: []
  type: TYPE_NORMAL
- en: '****Does that work? Running `manage.py test survey -v2` now, we see the following
    at the tail end of the output:****'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '****Success! It is possible, then, to correctly test with non-ASCII data in
    doctests. Some care must simply be taken to avoid running into existing problems
    related to using Unicode literal docstrings or embedding Unicode literal strings
    within a doctest.****'
  prefs: []
  type: TYPE_NORMAL
- en: '****# Summary'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our exploration of doctests for Django applications is now complete. In this
    chapter, we:'
  prefs: []
  type: TYPE_NORMAL
- en: Began to develop some models for our Django `survey` application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experimented with adding doctests to one of these models—the `Survey` model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learned what sorts of doctests are useful and which simply add clutter to the
    code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experienced some of the advantages of doctests, namely the easy re-use of Python
    shell session work and convenient use of doctests as documentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ran afoul of many of the disadvantages of doctests, and learned how to avoid
    or overcome them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the next chapter, we will begin to explore unit tests. While unit tests
    may not offer some of the easy re-use features of doctests, they also do not suffer
    from many of the disadvantages of doctests. Furthermore, the overall unit test
    framework allows Django to provide convenient support specifically useful for
    web applications, which will be covered in detail in [Chapter 4](ch04.html "Chapter 4. Getting
    Fancier: Django Unit Test Extensions").****'
  prefs: []
  type: TYPE_NORMAL
