- en: Advanced Machine Learning Best Practices
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级机器学习最佳实践
- en: '"Hyperparameter optimization or model selection is the problem of choosing
    a set of hyperparameters [when defined as?] for a learning algorithm, usually
    with the goal of optimizing a measure of the algorithm''s performance on an independent
    dataset."'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: “超参数优化或模型选择是选择学习算法的一组超参数[何时定义为？]的问题，通常目标是优化算法在独立数据集上的性能度量。”
- en: '- Machine learning model tuning quote'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '- 机器学习模型调整报价'
- en: In this chapter, we will provide some theoretical and practical aspects of some
    advanced topics of machine learning (ML) with Spark. We will see how to tune machine
    learning models for better and optimized performance using grid search, cross-validation
    and hyperparameter tuning. In the later section, we will cover how to develop
    a scalable recommendation system using the ALS, which is an example of a model-based
    recommendation algorithm. Finally, a topic modeling application as a text clustering
    technique will be demonstrated.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将提供一些关于使用Spark进行机器学习（ML）的一些高级主题的理论和实践方面。我们将看到如何使用网格搜索、交叉验证和超参数调整来调整机器学习模型，以获得更好和优化的性能。在后面的部分，我们将介绍如何使用ALS开发可扩展的推荐系统，这是一个基于模型的推荐算法的示例。最后，将演示一种文本聚类技术作为主题建模应用。
- en: 'In a nutshell, we will cover the following topics in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，本章中我们将涵盖以下主题：
- en: Machine learning best practice
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习最佳实践
- en: Hyperparameter tuning of ML models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ML模型的超参数调整
- en: Topic modeling using latent dirichlet allocation (LDA)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用潜在狄利克雷分配（LDA）进行主题建模
- en: A recommendation system using collaborative filtering
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用协同过滤的推荐系统
- en: Machine learning best practices
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习最佳实践
- en: 'Sometimes, it is recommended to consider the error rate rather than only the
    accuracy. For example, suppose an ML system with 99% accuracy and 50% errors is
    worse than the one with 90% accuracy but 25% errors. So far, we have discussed
    the following machine learning topics:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，建议考虑错误率而不仅仅是准确性。例如，假设一个ML系统的准确率为99%，错误率为50%，比一个准确率为90%，错误率为25%的系统更差。到目前为止，我们已经讨论了以下机器学习主题：
- en: '**Regression**: This is for predicting values that are linearly separable'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归**：用于预测线性可分离的值'
- en: '**Anomaly detection**: This is for finding unusual data points often done using
    a clustering algorithm'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**异常检测**：用于发现异常数据点，通常使用聚类算法进行'
- en: '**Clustering**: This is for discovering the hidden structure in the dataset
    for clustering homogeneous data points'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类**：用于发现数据集中同质数据点的隐藏结构'
- en: '**Binary classification**: This is for predicting two categories'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**二元分类**：用于预测两个类别'
- en: '**Multi-class classification**: This is for predicting three or more categories'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多类分类**：用于预测三个或更多类别'
- en: Well, we have also seen that there are some good algorithms for these tasks.
    However, choosing the right algorithm for your problem type is a tricky task for
    achieving higher and outstanding accuracy in your ML algorithms. For this, we
    need to adopt some good practices through the stages, that is, from data collection,
    feature engineering, model building, evaluating, tuning, and deployment. Considering
    these, in this section, we will provide some practical recommendation while developing
    your ML application using Spark.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，我们也看到了一些适合这些任务的好算法。然而，选择适合您问题类型的正确算法是实现ML算法更高和更出色准确性的棘手任务。为此，我们需要通过从数据收集、特征工程、模型构建、评估、调整和部署的阶段采用一些良好的实践。考虑到这些，在本节中，我们将在使用Spark开发ML应用程序时提供一些建议。
- en: Beware of overfitting and underfitting
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意过拟合和欠拟合
- en: A straight line cutting across a curving scatter plot would be a good example
    of under-fitting, as we can see in the diagram here. However, if the line fits
    the data too well, there evolves an opposite problem called **overfitting**. When
    we say a model overfits a dataset, we mean it may have a low error rate for the
    training data, but it does not generalize well to the overall population in the
    data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一条直线穿过一个弯曲的散点图将是欠拟合的一个很好的例子，正如我们在这里的图表中所看到的。然而，如果线条过于贴合数据，就会出现一个相反的问题，称为**过拟合**。当我们说一个模型过拟合了数据集，我们的意思是它可能在训练数据上有低错误率，但在整体数据中不能很好地泛化。
- en: '![](img/00148.jpeg)**Figure 1**: Overfitting-underfitting trade-off (source:
    The book, "Deep Learning" by Adam Gibson, Josh Patterson)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00148.jpeg)**图1**：过拟合-欠拟合权衡（来源：亚当吉布森，乔什帕特森的书《深度学习》）'
- en: 'More technically, if you evaluate your model on the training data instead of
    test or validated data, you probably won''t be able to articulate whether your
    model is overfitting or not. The common symptoms are as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，如果您在训练数据上评估模型而不是测试或验证数据，您可能无法确定您的模型是否过拟合。常见的症状如下：
- en: Predictive accuracy of the data used for training can be over accurate (that
    is, sometimes even 100%).
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于训练的数据的预测准确性可能过于准确（即有时甚至达到100%）。
- en: The model might show better performance compared to the random prediction for
    new data.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与随机预测相比，模型可能在新数据上表现更好。
- en: We like to fit a dataset to a distribution because if the dataset is reasonably
    close to the distribution, we can make assumptions based on the theoretical distribution
    of how we operate with the data. Consequently, the normal distribution in the
    data allows us to assume that sampling distributions of statistics are normally
    distributed under specified conditions. The normal distribution is defined by
    its mean and standard deviation and has generally the same shape across all variations.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们喜欢将数据集拟合到分布中，因为如果数据集与分布相当接近，我们可以基于理论分布对我们如何处理数据进行假设。因此，数据中的正态分布使我们能够假设在指定条件下统计的抽样分布是正态分布的。正态分布由其均值和标准差定义，并且在所有变化中通常具有相同的形状。
- en: '![](img/00012.jpeg)**Figure 2**: Normal distribution in the data helps overcoming
    both the over-fitting and underfitting (source: The book, "Deep Learning" by Adam
    Gibson, Josh Patterson)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '**图2**：数据中的正态分布有助于克服过度拟合和拟合不足（来源：Adam Gibson、Josh Patterson的《深度学习》一书）'
- en: 'Sometimes, the ML model itself becomes underfit for a particular tuning or
    data point, which means the model become too simplistic. Our recommendation (like
    that of others, we believe) is as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，ML模型本身对特定调整或数据点拟合不足，这意味着模型变得过于简单。我们的建议（我们相信其他人也是如此）如下：
- en: Splitting the dataset into two sets to detect overfitting situations--the first
    one is for training and model selection called the training set, and the second
    one is the test set for evaluating the model started in place of the ML workflow
    section.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据集分为两组以检测过度拟合情况——第一组用于训练和模型选择的训练集，第二组是用于评估模型的测试集，开始替代ML工作流程部分。
- en: Alternatively, you also could avoid the overfitting by consuming simpler models
    (for example, linear classifiers in preference to Gaussian kernel SVM) or by swelling
    the regularization parameters of your ML model (if available).
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或者，您还可以通过使用更简单的模型（例如，线性分类器而不是高斯核SVM）或增加ML模型的正则化参数（如果可用）来避免过度拟合。
- en: Tune the model with a correct data value of parameters to avoid both the overfitting
    as well as underfitting.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整模型的正确数据值参数，以避免过度拟合和拟合不足。
- en: 'Thus, solving underfitting is the priority, but most of the machines learning
    practitioners suggest spending more time and effort attempting not to overfit
    the line to the data. Also, many machine learning practitioners, on the other
    hand, have recommended splitting the large-scale dataset into three sets: a training
    set (50%), validation set (25%), and test set (25%). They also suggested to build
    the model using the training set and to calculate the prediction errors using
    the validation set. The test set was recommended to be used to assess the generalization
    error of the final model. If the amount of labeled data available on the other
    hand during the supervised learning is smaller, it is not recommended to split
    the datasets. In that case, use cross validation. More specifically, divide the
    dataset into 10 parts of (roughly) equal size; after that, for each of these 10
    parts, train the classifier iteratively and use the 10th part to test the model.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，解决拟合不足是首要任务，但大多数机器学习从业者建议花更多时间和精力尝试不要过度拟合数据。另一方面，许多机器学习从业者建议将大规模数据集分为三组：训练集（50%）、验证集（25%）和测试集（25%）。他们还建议使用训练集构建模型，并使用验证集计算预测误差。测试集被推荐用于评估最终模型的泛化误差。然而，在监督学习期间，如果可用的标记数据量较小，则不建议拆分数据集。在这种情况下，使用交叉验证。更具体地说，将数据集分为大致相等的10个部分；然后，对这10个部分中的每一个，迭代训练分类器，并使用第10个部分来测试模型。
- en: Stay tuned with Spark MLlib and Spark ML
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 请继续关注Spark MLlib和Spark ML
- en: The first step of the pipeline designing is to create the building blocks (as
    a directed or undirected graph consisting of nodes and edges) and make a linking
    between those blocks. Nevertheless, as a data scientist, you should be focused
    on scaling and optimizing nodes (primitives) too so that you are able to scale
    up your application for handling large-scale datasets in the later stage to make
    your ML pipeline performing consistently. The pipeline process will also help
    you to make your model adaptive for new datasets. However, some of these primitives
    might be explicitly defined to particular domains and data types (for example,
    text, image, and video, audio, and spatiotemporal).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 管道设计的第一步是创建构件块（作为由节点和边组成的有向或无向图），并在这些块之间建立联系。然而，作为一名数据科学家，您还应该专注于扩展和优化节点（原语），以便在后期处理大规模数据集时能够扩展应用程序，使您的ML管道能够持续执行。管道过程还将帮助您使模型适应新数据集。然而，其中一些原语可能会明确定义为特定领域和数据类型（例如文本、图像和视频、音频和时空）。
- en: Beyond these types of data, the primitives should also be working for general
    purpose domain statistics or mathematics. The casting of your ML model in terms
    of these primitives will make your workflow more transparent, interpretable, accessible,
    and explainable.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些类型的数据之外，原语还应该适用于通用领域统计或数学。将您的ML模型转换为这些原语将使您的工作流程更加透明、可解释、可访问和可解释。
- en: A recent example would be the ML-matrix, which is a distributed matrix library
    that can be used on top of Spark. Refer to the JIRA issue at [https://issues.apache.org/jira/browse/SPARK-3434](https://issues.apache.org/jira/browse/SPARK-3434).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的一个例子是ML-matrix，它是一个可以在Spark之上使用的分布式矩阵库。请参阅[JIRA问题](https://issues.apache.org/jira/browse/SPARK-3434)。
- en: '![](img/00109.jpeg)**Figure 3:** Stay tune and interoperate ML and MLlib'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**图3**：保持关注并相互操作ML和MLlib'
- en: As we already stated in the previous section, as a developer, you can seamlessly
    combine the implementation techniques in Spark MLlib along with the algorithms
    developed in Spark ML, Spark SQL, GraphX, and Spark Streaming as hybrid or interoperable
    ML applications on top of RDD, DataFrame, and datasets as shown in *Figure 3*.
    Therefore, the recommendation here is to stay in tune or synchronized with the
    latest technologies around you for the betterment of your ML application.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前一节中已经提到的，作为开发人员，您可以无缝地将Spark MLlib中的实现技术与Spark ML、Spark SQL、GraphX和Spark
    Streaming中开发的算法结合起来，作为RDD、DataFrame和数据集的混合或可互操作的ML应用程序，如*图3*所示。因此，这里的建议是与您周围的最新技术保持同步，以改善您的ML应用程序。
- en: Choosing the right algorithm for your application
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为您的应用程序选择正确的算法
- en: '"What machine learning algorithm should I use?" is a very frequently asked
    question for the naive machine learning practitioners but the answer is always
    *it depends*. More elaborately:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: “我应该使用什么机器学习算法？”是一个非常常见的问题，但答案总是“这取决于”。更详细地说：
- en: It depends on the volume, quality, complexity, and the nature of the data you
    have to be tested/used
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这取决于你要测试/使用的数据的数量、质量、复杂性和性质
- en: It depends on external environments and parameters like your computing system's
    configuration or underlying infrastructures
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这取决于外部环境和参数，比如你的计算系统配置或基础设施
- en: It depends on what you want to do with the answer
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这取决于你想要用答案做什么
- en: It depends on how the mathematical and statistical formulation of the algorithm
    was translated into machine instructions for the computer
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这取决于算法的数学和统计公式如何被转化为计算机的机器指令
- en: It depends on how much time do you have
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这取决于你有多少时间
- en: 'The reality is, even the most experienced data scientists or data engineers
    can''t give a straight recommendation about which ML algorithm will perform the
    best before trying them all together. Most of the statements of agreement/disagreement
    begins with "It depends...hmm..." Habitually, you might wonder if there are cheat
    sheets of machine learning algorithms, and if so, how should you use that cheat
    sheet? Several data scientists have said that the only sure way to find the very
    best algorithm is to try all of them; therefore, there is no shortcut dude! Let''s
    make it clearer; suppose you do have a set of data and you want to do some clustering.
    Technically, this could be either a classification or regression problem that
    you want o apply on your dataset if your data is labeled. However, if you have
    a unlabeled dataset, it is the clustering technique that you will be using. Now,
    the concerns that evolve in your mind are as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，即使是最有经验的数据科学家或数据工程师在尝试所有算法之前也无法直接推荐哪种机器学习算法会表现最好。大多数同意/不同意的陈述都以“这取决于...嗯...”开始。习惯上，你可能会想知道是否有机器学习算法的备忘单，如果有的话，你应该如何使用？一些数据科学家表示，找到最佳算法的唯一方法是尝试所有算法；因此，没有捷径！让我们更清楚地说明一下；假设你有一组数据，你想做一些聚类。从技术上讲，如果你的数据有标签，这可能是一个分类或回归问题。然而，如果你有一个无标签的数据集，你将使用聚类技术。现在，你脑海中出现的问题如下：
- en: Which factors should I consider before choosing an appropriate algorithm? Or
    should I just choose an algorithm randomly?
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在选择适当的算法之前，我应该考虑哪些因素？还是应该随机选择一个算法？
- en: How do I choose any data pre-processing algorithm or tools that can be applied
    to my data?
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我如何选择适用于我的数据的任何数据预处理算法或工具？
- en: What sort of feature engineering techniques should I be using to extract the
    useful features?
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我应该使用什么样的特征工程技术来提取有用的特征？
- en: What factors can improve the performance of my ML model?
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么因素可以提高我的机器学习模型的性能？
- en: How can I adopt my ML application for new data types?
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我如何适应新的数据类型？
- en: Can I scale up my ML application for large-scale datasets? And so on.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我能否扩展我的机器学习应用以处理大规模数据集？等等。
- en: In this section, we will try to answer these questions with our little machine
    learning knowledge.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将尝试用我们有限的机器学习知识来回答这些问题。
- en: Considerations when choosing an algorithm
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择算法时的考虑因素
- en: 'The recommendation or suggestions we provide here are for the novice data scientist
    who is just learning machine learning. These will use useful to expert the data
    scientists too, who is trying to choose an optimal algorithm to start with Spark
    ML APIs. Don''t worry, we will guide you to the direction! We also recommend going
    with the following algorithmic properties when choosing an algorithm:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里提供的建议或建议是给那些刚开始学习机器学习的新手数据科学家。这些对于试图选择一个最佳算法来开始使用Spark ML API的专家数据科学家也会有用。不用担心，我们会指导你的方向！我们还建议在选择算法时考虑以下算法属性：
- en: '**Accuracy**: Whether getting the best score is the goal or an approximate
    solution (*good enough*) in terms of precision, recall, f1 score or AUC and so
    on, while trading off overfitting.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确性**：是否达到最佳分数是目标，还是在精确度、召回率、f1分数或AUC等方面进行权衡，得到一个近似解（足够好），同时避免过拟合。'
- en: '**Training time**: The amount of time available to train the model (including
    the model building, evaluation, and tanning time).'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练时间**：训练模型的可用时间（包括模型构建、评估和训练时间）。'
- en: '**Linearity**: An aspect of model complexity in terms of how the problem is
    modeled. Since most of the non-linear models are often more complex to understand
    and tune.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性度**：模型复杂性的一个方面，涉及问题建模的方式。由于大多数非线性模型通常更复杂，难以理解和调整。'
- en: '**Number of parameters**'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**参数数量**'
- en: '**Number of features**: The problem of having more attributes than instances,
    the *p>>n* problem. This often requires specialized handling or specialized techniques
    using dimensionality reduction or better feature engineering approach.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征数量**：拥有的属性比实例多的问题，即*p>>n*问题。这通常需要专门处理或使用降维或更好的特征工程方法。'
- en: Accuracy
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准确性
- en: 'Getting the most accurate results from your ML application isn''t always indispensable.
    Depending on what you want to use it for, sometimes an approximation is adequate
    enough. If the situation is something like this, you may be able to reduce the
    processing time drastically by incorporating the better-estimated methods. When
    you become familiar of the workflow with the Spark machine learning APIs, you
    will enjoy the advantage of having more approximation methods, because these approximation
    methods will tend to avoid the overfitting problem of your ML model automatically.
    Now, suppose you have two binary classification algorithms that perform as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 从你的机器学习应用中获得最准确的结果并非总是必不可少的。根据你想要使用它的情况，有时近似解就足够了。如果情况是这样的，你可以通过采用更好的估计方法大大减少处理时间。当你熟悉了Spark机器学习API的工作流程后，你将享受到更多的近似方法的优势，因为这些近似方法将自动避免你的机器学习模型的过拟合问题。现在，假设你有两个二元分类算法的表现如下：
- en: '| **Classifier** | **Precision** | **Recall** |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| **分类器** | **精确度** | **召回率** |'
- en: '| X | 96% | 89% |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| X | 96% | 89% |'
- en: '| Y | 99% | 84% |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| Y | 99% | 84% |'
- en: 'Here, none of the classifiers is obviously superior, so it doesn''t immediately
    guide you toward picking the optimal one. F1-score which is the harmonic mean
    of precision and recall helps you. Let''s calculate it and place it in the table:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '| **Classifier** | **Precision** | **Recall** | **F1 score** |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
- en: '| X | 96% | 89% | 92.36% |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
- en: '| Y | 99% | 84% | 90.885% |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
- en: Therefore, having an F1-score helps make a decision for selecting from a large
    number of classifiers. It gives a clear preference ranking among all of them,
    and therefore a clear direction for progress--that is classifier **X**.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Training time
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training time is often closely related to the model training and the accuracy.
    In addition, often you will discover that some of the algorithms are elusive to
    the number of data points compared to others. However, when your time is inadequate
    but the training set is large with a lot of features, you can choose the simplest
    one. In this case, you might have to compromise with the accuracy. But it will
    fulfill your minimum requirements at least.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Linearity
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many machine learning algorithms developed recently that make use
    of linearity (also available in the Spark MLlib and Spark ML). For example, linear
    classification algorithms undertake that classes can be separated by plotting
    a differentiating straight line or using higher-dimensional equivalents. Linear
    regression algorithms, on the other hand, assume that data trends simply follow
    a straight line. This assumption is not naive for some machine learning problems;
    however, there might be some other cases where the accuracy will be down. Despite
    their hazards, linear algorithms are very popular with data engineers and data
    scientists as the first line of the outbreak. Moreover, these algorithms also
    tend to be simple and fast, to train your models during the whole process.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Inspect your data when choosing an algorithm
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You will find many machine learning datasets available at the UC Irvine Machine
    Learning Repository. The following data properties should also be prioritized:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Number of parameters
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of features
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Size of the training dataset
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of parameters
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Parameters or data properties are the handholds for a data scientist when setting
    up an algorithm. They are numbers that affect the algorithm's performance, such
    as error tolerance or a number of iterations, or options between variants of how
    the algorithm acts. The training time and accuracy of the algorithm can sometimes
    be quite sensitive making it difficult get the right settings. Typically, algorithms
    with a large number of parameters require more trial and error to find an optimal
    combination.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite the fact that this is a great way to span the parameter space, the
    model building or train time increases exponentially with the increased number
    of parameters. This is a dilemma as well as a time-performance trade-off. The
    positive sides are:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Having many parameters characteristically indicates the greater flexibility
    of the ML algorithms
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your ML application achieves much better accuracy
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How large is your training set?
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If your training set is smaller, high bias with low variance classifiers, such
    as Naive Bayes have an advantage over low bias with high variance classifiers
    (also can be used for regression) such as the **k-nearest neighbors algorithm**
    (**kNN**).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '**Bias, Variance, and the kNN model:** In reality, *increasing k* will *decrease
    the variance,* but *increase the bias*. On the other hand, *decreasing k* will
    *increase variance* and *decrease bias*. As *k* increases, this variability is
    reduced. But if we increase *k* too much, then we no longer follow the true boundary
    line and we observe high bias. This is the nature of the Bias-Variance Trade-off.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 'We have seen the over and underfitting issue already. Now, you can assume that
    dealing with bias and variance is like dealing with over- and underfitting. Bias
    is reduced and variance is increased in relation to model complexity. As more
    and more parameters are added to a model, the complexity of the model rises and
    variance becomes our primary concern while bias steadily falls. In other words,
    bias has a negative first-order derivative in response to model complexity, while
    variance has a positive slope. Refer to the following figure for a better understanding:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了过拟合和欠拟合的问题。现在，可以假设处理偏差和方差就像处理过拟合和欠拟合一样。随着模型复杂性的增加，偏差减小，方差增加。随着模型中添加更多参数，模型的复杂性增加，方差成为我们关注的主要问题，而偏差稳步下降。换句话说，偏差对模型复杂性的响应具有负的一阶导数，而方差具有正的斜率。请参考以下图表以更好地理解：
- en: '![](img/00077.jpeg)**Figure 4:** Bias and variance contributing to total error'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00077.jpeg)**图4：** 偏差和方差对总误差的影响'
- en: Therefore, the latter will overfit. But low bias with high variance classifiers,
    on the other hand, starts to win out as your training set grows linearly or exponentially,
    since they have lower asymptotic errors. High bias classifiers aren't powerful
    enough to provide accurate models.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，后者会过拟合。但是低偏差高方差的分类器，在训练集线性或指数增长时，开始获胜，因为它们具有更低的渐近误差。高偏差分类器不足以提供准确的模型。
- en: Number of features
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征数
- en: For certain types of experimental dataset, the number of extracted features
    can be very large compared to the number of data points itself. This is often
    the case with genomics, biomedical, or textual data. A large number of features
    can swamp down some learning algorithms, making training time ridiculously higher.
    **Support Vector Machines** (**SVMs**) are particularly well suited in this case
    for its high accuracy, nice theoretical guarantees regarding overfitting, and
    with an appropriate kernel.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些类型的实验数据集，提取的特征数量可能与数据点本身的数量相比非常大。这在基因组学、生物医学或文本数据中经常发生。大量的特征可能会淹没一些学习算法，使训练时间变得非常长。**支持向量机**（**SVM**）特别适用于这种情况，因为它具有高准确性，对过拟合有良好的理论保证，并且具有适当的核函数。
- en: '**The SVM and the Kernel**: The task is to find a set of weight and bias such
    that the margin can maximize the function:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**支持向量机和核函数：** 任务是找到一组权重和偏差，使间隔最大化函数：'
- en: y = w*¥(x) +b,
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: y = w*¥(x) +b,
- en: 'Where *w* is the weight, *¥* is the feature vector, and *b* is the bias. Now
    if *y> 0*, then we classify datum to class *1*, else to class *0*, whereas, the
    feature vector *¥(x)* makes the data linearly separable. However, using the kernel
    makes the calculation process faster and easier, especially when the feature vector
    *¥* consisting of very high dimensional data. Let''s see a concrete example. Suppose
    we have the following value of *x* and *y*: *x = (x1, x2, x3)* and *y = (y1, y2,
    y3)*, then for the function *f(x) = (x1x1, x1x2, x1x3, x2x1, x2x2, x2x3, x3x1,
    x3x2, x3x3)*, the kernel is *K(x, y ) = (<x, y>)²*. Following the above, if *x*
    *= (1, 2, 3)* and *y = (4, 5, 6)*, then we have the following values:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*w*是权重，*¥*是特征向量，*b*是偏差。现在如果*y> 0*，那么我们将数据分类到类*1*，否则到类*0*，而特征向量*¥(x)*使数据线性可分。然而，使用核函数可以使计算过程更快、更容易，特别是当特征向量*¥*包含非常高维的数据时。让我们看一个具体的例子。假设我们有以下值*x*和*y*：*x
    = (x1, x2, x3)*和*y = (y1, y2, y3)*，那么对于函数*f(x) = (x1x1, x1x2, x1x3, x2x1, x2x2,
    x2x3, x3x1, x3x2, x3x3)*，核函数是*K(x, y ) = (<x, y>)²*。根据上述，如果*x* *= (1, 2, 3)*和*y
    = (4, 5, 6)*，那么我们有以下值：
- en: f(x) = (1, 2, 3, 2, 4, 6, 3, 6, 9)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: f(x) = (1, 2, 3, 2, 4, 6, 3, 6, 9)
- en: f(y) = (16, 20, 24, 20, 25, 30, 24, 30, 36)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: f(y) = (16, 20, 24, 20, 25, 30, 24, 30, 36)
- en: <f(x), f(y)> = 16 + 40 + 72 + 40 + 100+ 180 + 72 + 180 + 324 = 1024
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: <f(x), f(y)> = 16 + 40 + 72 + 40 + 100+ 180 + 72 + 180 + 324 = 1024
- en: This is a simple linear algebra that maps a 3-dimensional space to a 9 dimensional.
    On the other hand, the kernel is a similarity measure used for SVMs. Therefore,
    choosing an appropriate kernel value based on the prior knowledge of invariances
    is suggested. The choice of the kernel and kernel and regularization parameters
    can be automated by optimizing a cross-validation based model selection.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单的线性代数，将一个3维空间映射到一个9维空间。另一方面，核函数是用于支持向量机的相似性度量。因此，建议根据对不变性的先验知识选择适当的核值。核和正则化参数的选择可以通过优化基于交叉验证的模型选择来自动化。
- en: Nevertheless, an automated choice of kernels and kernel parameters is a tricky
    issue, as it is very easy to overfit the model selection criterion. This might
    result in a worse model than you started with. Now, if we use the kernel function
    *K(x, y), this gives the same value but with much simpler calculation -i.e. (4
    + 10 + 18) ^2 = 32^2 = 1024*.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，自动选择核和核参数是一个棘手的问题，因为很容易过度拟合模型选择标准。这可能导致比开始时更糟糕的模型。现在，如果我们使用核函数*K(x, y)*，这将给出相同的值，但计算更简单
    - 即(4 + 10 + 18) ^2 = 32^2 = 1024。
- en: Hyperparameter tuning of ML models
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习模型的超参数调整
- en: 'Tuning an algorithm is simply a process that one goes through in order to enable
    the algorithm to perform optimally in terms of runtime and memory usage. In Bayesian
    statistics, a hyperparameter is a parameter of a prior distribution. In terms
    of machine learning, the term hyperparameter refers to those parameters that cannot
    be directly learned from the regular training process. Hyperparameters are usually
    fixed before the actual training process begins. This is done by setting different
    values for those hyperparameters, training different models, and deciding which
    ones work best by testing them. Here are some typical examples of such parameters:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 调整算法只是一个过程，通过这个过程，使算法在运行时间和内存使用方面表现最佳。在贝叶斯统计中，超参数是先验分布的参数。在机器学习方面，超参数指的是那些不能直接从常规训练过程中学习到的参数。超参数通常在实际训练过程开始之前固定。这是通过为这些超参数设置不同的值，训练不同的模型，并通过测试来决定哪些模型效果最好来完成的。以下是一些典型的超参数示例：
- en: Number of leaves, bins, or depth of a tree
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 叶子节点数、箱数或树的深度
- en: Number of iterations
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迭代次数
- en: Number of latent factors in a matrix factorization
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵分解中的潜在因子数量
- en: Learning rate
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率
- en: Number of hidden layers in a deep neural network
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度神经网络中的隐藏层数量
- en: Number of clusters in a k-means clustering and so on.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k均值聚类中的簇数量等。
- en: In this section, we will discuss how to perform hyperparameter tuning using
    the cross-validation technique and grid searching.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论如何使用交叉验证技术和网格搜索进行超参数调整。
- en: Hyperparameter tuning
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数调整
- en: 'Hyperparameter tuning is a technique for choosing the right combination of
    hyperparameters based on the performance of presented data. It is one of the fundamental
    requirements to obtain meaningful and accurate results from machine learning algorithms
    in practice. The following figure shows the model tuning process, consideration,
    and workflow:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数调整是一种根据呈现数据的性能选择正确的超参数组合的技术。这是从实践中获得机器学习算法的有意义和准确结果的基本要求之一。下图显示了模型调整过程、考虑因素和工作流程：
- en: '![](img/00343.jpeg)**Figure 5**: The model tuning process, consideration, and
    workflow'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00343.jpeg)**图5**：模型调整过程、考虑因素和工作流程'
- en: 'For example, suppose we have two hyperparameters to tune for a pipeline presented
    in *Figure 17* from [Chapter 11](part0343.html#A73GU1-21aec46d8593429cacea59dbdcd64e1c),
    *Learning Machine Learning - Spark MLlib and Spark ML*, a Spark ML pipeline model
    using a logistic regression estimator (dash lines only happen during pipeline
    fitting). We can see that we have put three candidate values for each. Therefore,
    there would be nine combinations in total. However, only four are shown in the
    diagram, namely, Tokenizer, HashingTF, Transformer, and Logistic Regression (LR).
    Now, we want to find the one that will eventually lead to the model with the best
    evaluation result. The fitted model consists of the Tokenizer, the HashingTF feature
    extractor, and the fitted logistic regression model:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有两个要为管道调整的超参数，该管道在[第11章](part0343.html#A73GU1-21aec46d8593429cacea59dbdcd64e1c)中的*图17*中呈现，使用逻辑回归估计器的Spark
    ML管道模型（虚线只会在管道拟合期间出现）。我们可以看到我们为每个参数放置了三个候选值。因此，总共会有九种组合。但是，在图中只显示了四种，即Tokenizer、HashingTF、Transformer和Logistic
    Regression（LR）。现在，我们要找到最终会导致具有最佳评估结果的模型。拟合的模型包括Tokenizer、HashingTF特征提取器和拟合的逻辑回归模型：
- en: If you recall *Figure 17* from [Chapter 11](part0343.html#A73GU1-21aec46d8593429cacea59dbdcd64e1c),
    *Learning Machine Learning - Spark MLlib and Spark ML*, the dashed line, however,
    happens only during the pipeline fitting. As mentioned earlier, the fitted pipeline
    model is a Transformer. The Transformer can be used for prediction, model validation,
    and model inspection. In addition, we also argued that one ill-fated distinguishing
    characteristic of the ML algorithms is that typically they have many hyperparameters
    that need to be tuned for better performance. For example, the degree of regularizations
    in these hyperparameters is distinctive from the model parameters optimized by
    the Spark MLlib.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您回忆起[第11章](part0343.html#A73GU1-21aec46d8593429cacea59dbdcd64e1c)中的*图17*，*学习机器学习
    - Spark MLlib和Spark ML*，虚线只会在管道拟合期间出现。正如前面提到的，拟合的管道模型是一个Transformer。Transformer可用于预测、模型验证和模型检查。此外，我们还认为ML算法的一个不幸的特点是，它们通常有许多需要调整以获得更好性能的超参数。例如，这些超参数中的正则化程度与Spark
    MLlib优化的模型参数有所不同。
- en: As a consequence, it is really hard to guess or measure the best combination
    of hyperparameters without expert knowledge of the data and the algorithm to use.
    Since the complex dataset is based on the ML problem type, the size of the pipeline
    and the number of hyperparameters may grow exponentially (or linearly); the hyperparameter
    tuning becomes cumbersome even for an ML expert, not to mention that the result
    of the tuning parameters may become unreliable.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果没有对数据和要使用的算法的专业知识，很难猜测或衡量最佳超参数组合。由于复杂数据集基于ML问题类型，管道的大小和超参数的数量可能会呈指数级增长（或线性增长）；即使对于ML专家来说，超参数调整也会变得繁琐，更不用说调整参数的结果可能会变得不可靠。
- en: 'According to Spark API documentation, a unique and uniform API is used for
    specifying Spark ML estimators and Transformers. A `ParamMap` is a set of (parameter,
    value) pairs with a Param as a named parameter with self-contained documentation
    provided by Spark. Technically, there are two ways for passing the parameters
    to an algorithm as specified in the following options:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Spark API文档，用于指定Spark ML估计器和Transformer的是一个独特且统一的API。`ParamMap`是一组(参数，值)对，其中Param是由Spark提供的具有自包含文档的命名参数。从技术上讲，有两种方法可以将参数传递给算法，如下所示：
- en: '**Setting parameters**: If an LR is an instance of Logistic Regression (that
    is, Estimator), you can call the `setMaxIter()` method as follows: `LR.setMaxIter(5)`.
    It essentially fits the model pointing the regression instance as follows: `LR.fit()`.
    In this particular example, there would be at most five iterations.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**设置参数**：如果LR是逻辑回归的实例（即估计器），则可以调用`setMaxIter()`方法，如下所示：`LR.setMaxIter(5)`。它基本上将模型拟合到回归实例，如下所示：`LR.fit()`。在这个特定的例子中，最多会有五次迭代。'
- en: '**The second option**: This one involves passing a `ParamMaps` to `fit()` or
    `transform()` (refer *Figure 5* for details). In this circumstance, any parameters
    will be overridden by the `ParamMaps` previously specified via setter methods
    in the ML application-specific codes or algorithms.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第二个选项**：这涉及将`ParamMaps`传递给`fit()`或`transform()`（有关详细信息，请参见*图5*）。在这种情况下，任何参数都将被先前通过ML应用程序特定代码或算法中的setter方法指定的`ParamMaps`覆盖。'
- en: Grid search parameter tuning
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网格搜索参数调整
- en: 'Suppose you selected your hyperparameters after necessary feature engineering.
    In this regard, a full grid search of the space of hyperparameters and features
    is computationally too intensive. Therefore, you need to perform a fold of the
    K-fold cross-validation instead of a full-grid search:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您在必要的特征工程之后选择了您的超参数。在这方面，对超参数和特征空间进行完整的网格搜索计算量太大。因此，您需要执行K折交叉验证的折叠，而不是进行完整的网格搜索：
- en: Tune the required hyperparameters using cross validation on the training set
    of the fold, using all the available features
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在折叠的训练集上使用交叉验证来调整所需的超参数，使用所有可用的特征
- en: Select the required features using those hyperparameters
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用这些超参数选择所需的特征
- en: Repeat the computation for each fold in K
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对K中的每个折叠重复计算
- en: The final model is constructed on all the data using the N most prevalent features
    that were selected from each fold of CV
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终模型是使用从每个CV折叠中选择的N个最常见特征构建的所有数据
- en: The interesting thing is that the hyperparameters would also be tuned again
    using all the data in a cross-validation loop. Would there be a large downside
    from this method as compared to a full-grid search? In essence, I am doing a line
    search in each dimension of free parameters (finding the best value in one dimension,
    holding that constant, then finding the best in the next dimension), rather than
    every single combination of parameter settings. The most important downside for
    searching along single parameters instead of optimizing them all together, is
    that you ignore interactions.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，超参数也将在交叉验证循环中再次进行调整。与完整的网格搜索相比，这种方法会有很大的不利因素吗？实质上，我在每个自由参数的维度上进行线性搜索（找到一个维度中的最佳值，将其保持恒定，然后找到下一个维度中的最佳值），而不是每个参数设置的所有组合。沿着单个参数搜索而不是一起优化它们的最重要的不利因素是，您忽略了相互作用。
- en: It is quite common that, for instance, more than one parameter influences model
    complexity. In that case, you need to look at their interaction in order to successfully
    optimize the hyperparameters. Depending on how large your dataset is and how many
    models you compare, optimization strategies that return the maximum observed performance
    may run into trouble (this is true for both grid search and your strategy).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，很常见的是，不止一个参数影响模型复杂性。在这种情况下，您需要查看它们的相互作用，以成功地优化超参数。根据您的数据集有多大以及您比较了多少个模型，返回最大观察性能的优化策略可能会遇到麻烦（这对网格搜索和您的策略都是如此）。
- en: 'The reason is that searching through a large number of performance estimates
    for the maximum skims the variance of the performance estimate: you may just end
    up with a model and training/test split combination that accidentally happens
    to look good. Even worse, you may get several perfect-looking combinations, and
    the optimization then cannot know which model to choose and thus becomes unstable.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 原因是在大量性能估计中寻找最大值会削弱性能估计的方差：您可能最终只得到一个模型和训练/测试分割组合，碰巧看起来不错。更糟糕的是，您可能会得到几个看起来完美的组合，然后优化无法知道选择哪个模型，因此变得不稳定。
- en: Cross-validation
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交叉验证
- en: Cross-validation (also called the **rotation estimation** (**RE**)) is a model
    validation technique for assessing the quality of the statistical analysis and
    results. The target is to make the model generalize toward an independent test
    set. One of the perfect uses of the cross-validation technique is making a prediction
    from a machine learning model. It will help if you want to estimate how a predictive
    model will perform accurately in practice when you deploy it as an ML application.
    During the cross-validation process, a model is usually trained with a dataset
    of a known type. Conversely, it is tested using a dataset of unknown type.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证（也称为**旋转估计**（**RE**））是一种模型验证技术，用于评估统计分析和结果的质量。目标是使模型向独立测试集泛化。交叉验证技术的一个完美用途是从机器学习模型中进行预测。如果您想要估计在实践中部署为ML应用时预测模型的准确性，这将有所帮助。在交叉验证过程中，模型通常是使用已知类型的数据集进行训练的。相反，它是使用未知类型的数据集进行测试的。
- en: 'In this regard, cross-validation helps to describe a dataset to test the model
    in the training phase using the validation set. There are two types of cross-validation
    that can be typed as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在这方面，交叉验证有助于描述数据集，以便在训练阶段使用验证集测试模型。有两种类型的交叉验证，可以如下分类：
- en: '**Exhaustive cross-validation**: This includes leave-p-out cross-validation
    and leave-one-out cross-validation.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**穷举交叉验证**：这包括留p-out交叉验证和留一出交叉验证。'
- en: '**Non-exhaustive cross-validation**: This includes K-fold cross-validation
    and repeated random sub-sampling cross-validation.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非穷尽交叉验证**：这包括K折交叉验证和重复随机子采样交叉验证。'
- en: 'In most of the cases, the researcher/data scientist/data engineer uses 10-fold
    cross-validation instead of testing on a validation set. This is the most widely
    used cross-validation technique across the use cases and problem type as explained
    by the following figure:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，研究人员/数据科学家/数据工程师使用10折交叉验证，而不是在验证集上进行测试。这是最广泛使用的交叉验证技术，如下图所示：
- en: '![](img/00372.gif)**Figure 6:** Cross-validation basically splits your complete
    available training data into a number of folds. This parameter can be specified.
    Then the whole pipeline is run once for every fold and one machine learning model
    is trained for each fold. Finally, the different machine learning models obtained
    are joined by a voting scheme for classifiers or by averaging for regression'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00372.gif)**图6：**交叉验证基本上将您的完整可用训练数据分成多个折叠。可以指定此参数。然后，整个流程对每个折叠运行一次，并为每个折叠训练一个机器学习模型。最后，通过分类器的投票方案或回归的平均值将获得的不同机器学习模型结合起来'
- en: 'Moreover, to reduce the variability, multiple iterations of cross-validation
    are performed using different partitions; finally, the validation results are
    averaged over the rounds. The following figure shows an example of hyperparameter
    tuning using the logistic regression:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00046.jpeg)**Figure 7:** An example of hyperparameter tuning using
    the logistic regression'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: 'Using cross-validation instead of conventional validation has two main advantages
    outlined as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, if there is not enough data available to partition across the separate
    training and test sets, there's the chance of losing significant modeling or testing
    capability.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secondly, the K-fold cross-validation estimator has a lower variance than a
    single hold-out set estimator. This low variance limits the variability and is
    again very important if the amount of available data is limited.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In these circumstances, a fair way to properly estimate the model prediction
    and related performance are to use cross-validation as a powerful general technique
    for model selection and validation. If we need to perform manual features and
    a parameter selection for the model tuning, after that, we can perform a model
    evaluation with a 10-fold cross-validation on the entire dataset. What would be
    the best strategy? We would suggest you go for the strategy that provides an optimistic
    score as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Divide the dataset into training, say 80%, and testing 20% or whatever you chose
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the K-fold cross-validation on the training set to tune your model
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Repeat the CV until you find your model optimized and therefore tuned.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, use your model to predict on the testing set to get an estimate of out
    of model errors.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Credit risk analysis – An example of hyperparameter tuning
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will show a practical example of machine learning hyperparameter
    tuning in terms of grid searching and cross-validation technique. More specifically,
    at first, we will develop a credit risk pipeline that is commonly used in financial
    institutions such as banks and credit unions. Later on, we will look at how to
    improve the prediction accuracy by hyperparameter tuning. Before diving into the
    example, let's take a quick overview of what credit risk analysis is and why it
    is important?
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: What is credit risk analysis? Why is it important?
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When an applicant applies for loans and a bank receives that application, based
    on the applicant''s profile, the bank has to make a decision whether to approve
    the loan application or not. In this regard, there are two types of risk associated
    with the bank''s decision on the loan application:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '**The applicant is a good credit risk**: That means the client or applicant
    is more likely to repay the loan. Then, if the loan is not approved, the bank
    can potentially suffer the loss of business.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The applicant is a bad credit risk**: That means that the client or applicant
    is most likely not to repay the loan. In that case, approving the loan to the
    client will result in financial loss to the bank.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The institution says that the second one is riskier than that of the first one,
    as the bank has a higher chance of not getting reimbursed the borrowed amount.
    Therefore, most banks or credit unions evaluate the risks associated with lending
    money to a client, applicant, or customer. In business analytics, minimizing the
    risk tends to maximize the profit to the bank itself.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: In other words, maximizing the profit and minimizing the loss from a financial
    perspective is important. Often, the bank makes a decision about approving a loan
    application based on different factors and parameters of an applicant, such as
    demographic and socio-economic conditions regarding their loan application.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: The dataset exploration
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The German credit dataset was downloaded from the UCI Machine Learning Repository
    at [https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/](https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/).
    Although a detailed description of the dataset is available in the link, we provide
    some brief insights here in **Table 3**. The data contains credit-related data
    on 21 variables and the classification of whether an applicant is considered a
    good or a bad credit risk for 1000 loan applicants (that is, binary classification
    problem).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table shows details about each variable that was considered before
    making the dataset available online:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '| **Entry** | **Variable** | **Explanation** |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
- en: '| 1 | creditability | Capable of repaying: has value either 1.0 or 0.0 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
- en: '| 2 | balance | Current balance |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
- en: '| 3 | duration | Duration of the loan being applied for |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
- en: '| 4 | history | Is there any bad loan history? |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
- en: '| 5 | purpose | Purpose of the loan |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
- en: '| 6 | amount | Amount being applied for |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
- en: '| 7 | savings | Monthly saving |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
- en: '| 8 | employment | Employment status |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
- en: '| 9 | instPercent | Interest percent |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
- en: '| 10 | sexMarried | Sex and marriage status |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
- en: '| 11 | guarantors | Are there any guarantors? |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
- en: '| 12 | residenceDuration | Duration of residence at the current address |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
- en: '| 13 | assets | Net assets |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
- en: '| 14 | age | Age of the applicant |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
- en: '| 15 | concCredit | Concurrent credit |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
- en: '| 16 | apartment | Residential status |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
- en: '| 17 | credits | Current credits |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
- en: '| 18 | occupation | Occupation |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
- en: '| 19 | dependents | Number of dependents |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
- en: '| 20 | hasPhone | If the applicant uses a phone |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
- en: '| 21 | foreign | If the applicant is a foreigner |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
- en: Note that, although *Table 3* describes the variables with an associated header,
    there is no associated header in the dataset. In *Table 3*, we have shown the
    variable, position, and associated significance of each variable.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Step-by-step example with Spark ML
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here, we will provide a step-by-step example of credit risk prediction using
    the Random Forest classifier. The steps include from data ingestion, some statistical
    analysis, training set preparation, and finally model evaluation:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1.** Load and parse the dataset into RDD:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'For the preceding line, the `parseRDD()` method is used to split the entry
    with `,` and then converted all of them as `Double` value (that is, numeric).
    This method goes as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'On the other hand, the `parseCredit()` method is used to parse the dataset
    based on the `Credit` case class:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The `Credit` case class goes as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Step 2\. Prepare the DataFrame for the ML pipeline** - Get the DataFrame
    for the ML pipeline'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Save them as a temporary view for making the query easier:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let''s take a snap of the DataFrame:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The preceding `show()` method prints the credit DataFrame:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00124.gif)**Figure 8:** A snap of the credit dataset'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 3\. Observing related statistics** - First, let''s see some aggregate
    values:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let''s see the statistics about the balance:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, let''s see the creditability per average balance:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output of the three lines:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00030.gif)**Figure 9:** Some statistics of the dataset'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 4\. Feature vectors and labels creation** - As you can see, the credibility
    column is the response column, and, for the result, we need to create the feature
    vector without considering this column. Now, let''s create the feature column
    as follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let''s assemble all the features of these selected columns using `VectorAssembler()`
    API as follows:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now let''s see what the feature vectors look like:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The preceding line shows the features created by the VectorAssembler transformer:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00360.gif)**Figure 10:** Generating features for ML models using VectorAssembler'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s create a new column as a label from the old response column creditability
    using `StringIndexer` as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The preceding line shows the features and labels created by the `VectorAssembler`
    transformer:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00274.gif)**Figure 11:** Corresponding labels and feature for ML models
    using VectorAssembler'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 5.** Prepare the training and test set:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**Step 6\. Train the random forest model** - At first, instantiate the model:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'For an explanation of the preceding parameters, refer to the random forest
    algorithm section in this chapter. Now, let''s train the model using the training
    set:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '**Step 7.** Compute the raw prediction for the test set:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Let''s see the top 20 rows of this DataFrame:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The preceding line shows the DataFrame containing the label, raw prediction,
    probablity, and actual prediciton:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00040.gif)**Figure 12:** The DataFrame containing raw and actual prediction
    for test set'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Now after seeing the prediction from the last column, a bank can make a decision
    about the applications for which the application should be accepted.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 8\. Model evaluation before tuning** - Instantiate the binary evaluator:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Calculate the accuracy of the prediction for the test set as follows:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The accuracy before pipeline fitting: `0.751921784149243`'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 'This time, the accuracy is 75%, which is not that good. Let''s compute other
    important performance metrics for the binary classifier like **area under receiver
    operating characteristic** (**AUROC**) and **area under precision recall curve**
    (**AUPRC**):'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The `printlnMetric()` method goes as follows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Finally, let''s compute a few more performance metrics using the `RegressionMetrics
    ()` API for the random forest model we used during training:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, let''s see how our model is:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We get the following output:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Not that bad! However, not satisfactory either, right? Let's tune the model
    using grid search and cross-validation techniques.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 9\. Model tuning using grid search and cross-validation** - First, let''s
    use the `ParamGridBuilder` API to construct a grid of parameters to search over
    the param grid consisting of 20 to 70 trees with `maxBins` between 25 and 30,
    `maxDepth` between 5 and 10, and impurity as entropy and gini:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Let''s train the cross-validator model using the training set as follows:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Compute the raw prediction for the test set as follows:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '**Step 10\. Evaluation of the model after tuning** - Let''s see the accuracy:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We get the following output:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now, it''s above 83%. Good improvement indeed! Let''s see the two other metrics
    computing AUROC and AUPRC:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We get the following output:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now based on the `RegressionMetrics` API, compute the other metrics:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We get the following output:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '**Step 11\. Finding the best cross-validated model** - Finally, let''s find
    the best cross-validated model information:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We get the following output:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: A recommendation system with Spark
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A recommender system tries to predict potential items a user might be interested
    in based on a history from other users. Model-based collaborative filtering is
    commonly used in many companies such as Netflix. It is to be noted that Netflix
    is an American entertainment company founded by Reed Hastings and Marc Randolph
    on August 29, 1997, in Scotts Valley, California. It specializes in and provides
    streaming media and video-on-demand online and DVD by mail. In 2013, Netflix expanded
    into film and television production, as well as online distribution. As of 2017,
    the company has its headquarters in Los Gatos, California (source Wikipedia).
    Netflix is a recommender system for a real-time movie recommendation. In this
    section, we will see a complete example of how it works toward recommending movies
    for new users.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: Model-based recommendation with Spark
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The implementation in Spark MLlib supports model-based collaborative filtering.
    In the model based collaborative filtering technique, users and products are described
    by a small set of factors, also called the **latent factors** (**LFs**). From
    the following figure, you can get some idea of a different recommender system.
    *Figure 13* justifies why are going to use model-based collaborative filtering
    for the movie recommendation example:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00284.gif)**Figure 13**: A comparative view of a different recommendation
    system'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: 'The LFs are then used for predicting the missing entries. Spark API provides
    the implementation of the alternating least squares (also known as the ALS widely)
    algorithm, which is used to learn these latent factors by considering six parameters,
    including:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '*numBlocks*: This is the number of blocks used to parallelize computation (set
    to -1 to auto-configure).'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*rank*: This is the number of latent factors in the model.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*iterations*: This is the number of iterations of ALS to run. ALS typically
    converges to a reasonable solution in 20 iterations or less.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*lambda*: This specifies the regularization parameter in ALS.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*implicitPrefs*: This specifies whether to use the *explicit feedback* ALS
    variant or one adapted for *implicit feedback* data.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*alpha*: This is a parameter applicable to the implicit feedback variant of
    ALS that governs the *baseline* confidence in preference observations.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note that to construct an ALS instance with default parameters; you can set
    the value based on your requirements. The default values are as follows: `numBlocks:
    -1`, `rank: 10`, `iterations: 10`, `lambda: 0.01`, `implicitPrefs: false`, and
    `alpha: 1.0`.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Data exploration
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The movie and the corresponding rating dataset were downloaded from the MovieLens
    Website ([https://movielens.org](https://movielens.org)). According to the data
    description on the MovieLens Website, all the ratings are described in the `ratings.csv`
    file. Each row of this file followed by the header represents one rating for one
    movie by one user.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: 'The CSV dataset has the following columns: **userId**, **movieId**, **rating**,
    and **timestamp**, as shown in *Figure 14*. The rows are ordered first by the
    **userId**, and within the user, by **movieId**. Ratings are made on a five-star
    scale, with half-star increments (0.5 stars up to 5.0 stars). The timestamps represent
    the seconds since midnight Coordinated Universal Time (UTC) on January 1, 1970,
    where we have 105,339 ratings from the 668 users on 10,325 movies:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00244.gif)**Figure 14:** A snap of the ratings dataset'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, the movie information is contained in the `movies.csv` file.
    Each row apart from the header information represents one movie containing the
    columns: movieId, title, and genres (see *Figure 14*). Movie titles are either
    created or inserted manually or imported from the website of the movie database
    at [https://www.themoviedb.org/](https://www.themoviedb.org/). The release year,
    however, is shown in the bracket. Since movie titles are inserted manually, some
    errors or inconsistencies may exist in these titles. Readers are, therefore, recommended
    to check the IMDb database ([https://www.ibdb.com/](https://www.ibdb.com/)) to
    make sure if there are no inconsistencies or incorrect titles with their corresponding
    release year.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: 'Genres are a separated list, and are selected from the following genre categories:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: Action, Adventure, Animation, Children's, Comedy, Crime
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentary, Drama, Fantasy, Film-Noir, Horror, Musical
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mystery, Romance, Sci-Fi, Thriller, Western, War
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00356.gif)**Figure 15**: The title and genres for the top 20 movies'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: Movie recommendation using ALS
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this subsection, we will show you how to recommend the movie for other users
    through a step-by-step example from data collection to movie recommendation.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1\. Load, parse and explore the movie and rating Dataset** - Here is
    the code illustrated:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'This code segment should return you the DataFrame of the ratings. On the other
    hand, the following code segment shows you the DataFrame of movies:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '**Step 2\. Register both DataFrames as temp tables to make querying easier**
    - To register both Datasets, we can use the following code:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: This will help to make the in-memory querying faster by creating a temporary
    view as a table in min-memory. The lifetime of the temporary table using the `createOrReplaceTempView
    ()` method is tied to the `[[SparkSession]]` that was used to create this DataFrame.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 3\. Explore and query for related statistics** - Let''s check the ratings-related
    statistics. Just use the following code lines:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: You should find 105,339 ratings from 668 users on 10,325 movies. Now, let's
    get the maximum and minimum ratings along with the count of users who have rated
    a movie. However, you need to perform a SQL query on the ratings table we just
    created in-memory in the previous step. Making a query here is simple, and it
    is similar to making a query from a MySQL database or RDBMS. However, if you are
    not familiar with SQL-based queries, you are recommended to look at the SQL query
    specification to find out how to perform a selection using `SELECT` from a particular
    table, how to perform the ordering using `ORDER`, and how to perform a joining
    operation using the `JOIN` keyword.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, if you know the SQL query, you should get a new dataset by using a complex
    SQL query as follows:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We get the following output:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00073.gif)**Figure 16:** max, min ratings along with the count of users
    who have rated a movie'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: 'To get an insight, we need to know more about the users and their ratings.
    Now, let''s find the top most active users and how many times they rated a movie:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '![](img/00262.jpeg)**Figure 17:** top 10 most active users and how many times
    they rated a movie'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at a particular user, and find the movies that, say user
    668, rated higher than 4:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '![](img/00035.gif)**Figure 18:** movies that user 668 rated higher than 4'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 4\. Prepare training and test rating data and see the counts** - The
    following code splits ratings RDD into training data RDD (75%) and test data RDD
    (25%). Seed here is optional but required for the reproducibility purpose:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: You should find that there are 78,792 ratings in the training and 26,547 ratings
    in the test
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: DataFrame.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 5\. Prepare the data for building the recommendation model using ALS**
    - The ALS algorithm takes the RDD of `Rating` for the training purpose. The following
    code illustrates for building the recommendation model using APIs:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The `ratingsRDD` is an RDD of ratings that contains the `userId`, `movieId`,
    and corresponding ratings from the training dataset that we prepared in the previous
    step. On the other hand, a test RDD is also required for evaluating the model.
    The following `testRDD` also contains the same information coming from the test
    DataFrame we prepared in the previous step:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '**Step 6\. Build an ALS user product matrix** - Build an ALS user matrix model
    based on the `ratingsRDD` by specifying the maximal iteration, a number of blocks,
    alpha, rank, lambda, seed, and `implicitPrefs`. Essentially, this technique predicts
    missing ratings for specific users for specific movies based on ratings for those
    movies from other users who did similar ratings for other movies:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Finally, we iterated the model for learning 15 times. With this setting, we
    got good prediction accuracy. Readers are suggested to apply the hyperparameter
    tuning to get to know the optimum values for these parameters. Furthermore, set
    the number of blocks for both user blocks and product blocks to parallelize the
    computation into a pass -1 for an auto-configured number of blocks. The value
    is -1.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 7\. Making predictions** - Let''s get the top six movie predictions
    for user 668\. The following source code can be used to make the predictions:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The preceding code segment produces the following output containing the rating
    prediction with `UserID`, `MovieID`, and corresponding `Rating` for that movie:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00376.gif)**Figure 19**: top six movie predictions for user 668'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 8\. Evaluating the model** - In order to verify the quality of the models,
    **Root Mean Squared Error** (**RMSE**) is used to measure the differences between
    values predicted by a model and the values actually observed. By default, the
    smaller the calculated error, the better the model. In order to test the quality
    of the model, the test data is used (which was split above in step 4). According
    to many machine learning practitioners, the RMSE is a good measure of accuracy,
    but only to compare forecasting errors of different models for a particular variable
    and not between variables, as it is scale-dependent. The following line of code
    calculates the RMSE value for the model that was trained using the training set:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'It is to be noted that the `computeRmse()` is a UDF, that goes as follows:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: The preceding method computes the RMSE to evaluate the model. Less the RMSE,
    the better the model and its prediction capability.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: 'For the earlier setting, we got the following output:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: The performance of the preceding model could be increased further we believe.
    Interested readers should refer to this URL for more on tuning the ML-based ALS
    models [https://spark.apache.org/docs/preview/ml-collaborative-filtering.html](https://spark.apache.org/docs/preview/ml-collaborative-filtering.html).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: The topic modeling technique is widely used in the task of mining text from
    a large collection of documents. These topics can then be used to summarize and
    organize documents that include the topic terms and their relative weights. In
    the next section, we will show an example of topic modeling using the **latent
    dirichlet allocation** (**LDA**) algorithm.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: Topic modelling - A best practice for text clustering
  id: totrans-334
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The topic modeling technique is widely used in the task of mining text from
    a large collection of documents. These topics can then be used to summarize and
    organize documents that include the topic terms and their relative weights. The
    dataset that will be used for this example is just in plain text, however, in
    an unstructured format. Now the challenging part is finding useful patterns about
    the data using LDA called topic modeling.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: How does LDA work?
  id: totrans-336
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LDA is a topic model which infers topics from a collection of text documents.
    LDA can be thought of as a clustering algorithm where topics correspond to cluster
    centers, and documents correspond to examples (rows) in a dataset. Topics and
    documents both exist in a feature space, where feature vectors are vectors of
    word counts (bag of words). Instead of estimating a clustering using a traditional
    distance, LDA uses a function based on a statistical model of how text documents
    are generated.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: 'LDA supports different inference algorithms via `setOptimizer` function. `EMLDAOptimizer`
    learns clustering using expectation-maximization on the likelihood function and
    yields comprehensive results, while `OnlineLDAOptimizer` uses iterative mini-batch
    sampling for online variational inference and is generally memory friendly. LDA
    takes in a collection of documents as vectors of word counts and the following
    parameters (set using the builder pattern):'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: '`k`: Number of topics (that is, cluster centers).'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`optimizer`: Optimizer to use for learning the LDA model, either `EMLDAOptimizer`
    or `OnlineLDAOptimizer`.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`docConcentration`: Dirichlet parameter for prior over documents'' distributions
    over topics. Larger values encourage smoother inferred distributions.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`topicConcentration`: Dirichlet parameter for prior over topics'' distributions
    over terms (words). Larger values encourage smoother inferred distributions.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxIterations`: Limit on the number of iterations.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`checkpointInterval`: If using checkpointing (set in the Spark configuration),
    this parameter specifies the frequency with which checkpoints will be created.
    If `maxIterations` is large, using checkpointing can help reduce shuffle file
    sizes on disk and help with failure recovery.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Particularly, we would like to discuss the topics people talk about most from
    the large collection of texts. Since the release of Spark 1.3, MLlib supports
    the LDA, which is one of the most successfully used topic modeling techniques
    in the area of text mining and **Natural Language Processing** (**NLP**). Moreover,
    LDA is also the first MLlib algorithm to adopt Spark GraphX.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: To get more information about how the theory behind the LDA works, please refer
    to David M. Blei, Andrew Y. Ng and Michael I. Jordan, Latent, Dirichlet Allocation,
    *Journal of Machine Learning Research 3* (2003) 993-1022.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows the topic distribution from randomly generated tweet
    text:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00165.jpeg)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
- en: '**Figure 20**: The topic distribution and how it looks like'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will look at an example of topic modeling using the LDA
    algorithm of Spark MLlib with unstructured raw tweets datasets. Note that here
    we have used LDA, which is one of the most popular topic modeling algorithms commonly
    used for text mining. We could use more robust topic modeling algorithms such
    as **Probabilistic Latent Sentiment Analysis** (**pLSA**), **pachinko allocation
    model** (**PAM**), or **hierarchical dirichlet process** (**HDP**) algorithms.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: However, pLSA has the overfitting problem. On the other hand, both HDP and PAM
    are more complex topic modeling algorithms used for complex text mining such as
    mining topics from high dimensional text data or documents of unstructured text.
    Moreover, to this date, Spark has implemented only one topic modeling algorithm,
    that is LDA. Therefore, we have to use LDA reasonably.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: Topic modeling with Spark MLlib
  id: totrans-352
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this subsection, we represent a semi-automated technique of topic modeling
    using Spark. Using other options as defaults, we train LDA on the dataset downloaded
    from the GitHub URL at [https://github.com/minghui/Twitter-LDA/tree/master/data/Data4Model/test](https://github.com/minghui/Twitter-LDA/tree/master/data/Data4Model/test).
    The following steps show the topic modeling from data reading to printing the
    topics, along with their term-weights. Here''s the short workflow of the topic
    modeling pipeline:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The actual computation on topic modeling is done in the `LDAforTM` class. The
    `Params` is a case class, which is used for loading the parameters to train the
    LDA model. Finally, we train the LDA model using the parameters setting via the
    `Params` class. Now, we will explain each step broadly with step-by-step source
    code:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1\. Creating a Spark session** - Let''s create a Spark session by defining
    number of computing core, SQL warehouse, and application name as follows:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '**Step 2\. Creating vocabulary, tokens count to train LDA after text pre-processing**
    - At first, load the documents, and prepare them for LDA as follows:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The pre-process method is used to process the raw texts. At first, let''s read
    the whole texts using the `wholeTextFiles()` method as follows:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'In the preceding code, paths are the path of the text files. Then, we need
    to prepare a morphological RDD from the raw text based on the lemma texts as follows:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Here the `getLemmaText()` method from the `helperForLDA` class supplies the
    lemma texts after filtering the special characters such as `("""[! @ # $ % ^ &
    * ( ) _ + - − , " '' ; : . ` ? --]` as regular expressions using the `filterSpaecialChatacters()`
    method.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: 'It is to be noted that the `Morphology()` class computes the base form of English
    words, by removing just inflections (not derivational morphology). That is, it
    only does noun plurals, pronoun case, and verb endings, and not things like comparative
    adjectives or derived nominals. This comes from the Stanford NLP group. To use
    this, you should have the following import in the main class file: `edu.stanford.nlp.process.Morphology`.
    In the `pom.xml` file, you will have to include the following entries as dependencies:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The method goes as follows:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The `filterSpecialCharacters()` goes as follows:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: '`def filterSpecialCharacters(document: String) = document.replaceAll("""[!
    @ # $ % ^ & * ( ) _ + - − , " '' ; : . ` ? --]""", " ")`. Once we have the RDD
    with special characters removed in hand, we can create a DataFrame for building
    the text analytics pipeline:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'So, the DataFrame consist of only of the documents tag. A snapshot of the DataFrame
    is as follows:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00332.gif)**Figure 21**: Raw texts'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: 'Now if you examine the preceding DataFrame carefully, you will see that we
    still need to tokenize the items. Moreover, there are stop words in a DataFrame
    such as this, so we need to remove them as well. At first, let''s tokenize them
    using the `RegexTokenizer` API as follows:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Now, let''s remove all the stop words as follows:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Furthermore, we also need to apply count victories to find only the important
    features from the tokens. This will help make the pipeline chained at the pipeline
    stage. Let''s do it as follows:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Now, create the pipeline by chaining the transformers (`tokenizer`, `stopWordsRemover`,
    and `countVectorizer`) as follows:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Let''s fit and transform the pipeline towards the vocabulary and number of
    tokens:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Finally, return the vocabulary and token count pairs as follows:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Now, let''s see the statistics of the training data:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'We get the following output:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '****Step 4\. Instantiate the LDA model before training****'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '**Step 5: Set the NLP optimizer**'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: 'For better and optimized results from the LDA model, we need to set the optimizer
    for the LDA model. Here we use the `EMLDAOPtimizer` optimizer. You can also use
    the `OnlineLDAOptimizer()` optimizer. However, you need to add (1.0/actualCorpusSize)
    to `MiniBatchFraction` to be more robust on tiny datasets. The whole operation
    goes as follows. First, instantiate the `EMLDAOptimizer` as follows:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Now set the optimizer using the `setOptimizer()` method from the LDA API as
    follows:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'The `Params` case class is used to define the parameters to training the LDA
    model. This goes as follows:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'For a better result you can set these parameters in a naive way. Alternatively,
    you should go with the cross-validation for even better performance. Now if you
    want to checkpoint the current parameters, use the following line of codes:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '**Step 6.** Training the LDA model:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: For the texts we have, the LDA model took 6.309715286 sec to train. Note that
    these timing codes are optional. Here we provide them for reference purposes,
    only to get an idea of the training time.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 7\. Measuring the likelihood of the data** - Now, of to get some more
    statistics about the data such as maximum likelihood or log-likelihood, we can
    use the following code:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'The preceding code calculates the average log likelihood if the LDA model is
    an instance of the distributed version of the LDA model. We get the following
    output:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: The likelihood is used after data are available to describe a function of a
    parameter (or parameter vector) for a given outcome. This helps especially for
    estimating a parameter from a set of statistics. For more information on the likelihood
    measurement, interested readers should refer to [https://en.wikipedia.org/wiki/Likelihood_function](https://en.wikipedia.org/wiki/Likelihood_function).
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 8\. Prepare the topics of interests** - Prepare the top five topics
    with each topic having 10 terms. Include the terms and their corresponding weights.'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '**Step 9\. Topic modeling** - Print the top ten topics, showing the top-weighted
    terms for each topic. Also, include the total weight in each topic as follows:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Now, let''s see the output of our LDA model toward topics modeling:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'From the preceding output, we can see that the topic of the input documents
    is topic 5 having the most weight of `0.31363611105890865`. This topic discusses
    the terms love, long, shore, shower, ring, bring, bear and so on. Now, for a better
    understanding of the flow, here''s the complete source code:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: Scalability of LDA
  id: totrans-417
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The previous example shows how to perform topic modeling using the LDA algorithm
    as a standalone application. The parallelization of LDA is not straightforward,
    and there have been many research papers proposing different strategies. The key
    obstacle in this regard is that all methods involve a large amount of communication.
    According to the blog on the Databricks website ([https://databricks.com/blog/2015/03/25/topic-modeling-with-lda-mllib-meets-graphx.html](https://databricks.com/blog/2015/03/25/topic-modeling-with-lda-mllib-meets-graphx.html)),
    here are the statistics of the dataset and related training and test sets that
    were used during the experimentation:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: 'Training set size: 4.6 million documents'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vocabulary size: 1.1 million terms'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Training set size: 1.1 billion tokens (~239 words/document)'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 100 topics
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 16-worker EC2 cluster, for example, M4.large or M3.medium depending upon budget
    and requirements
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the preceding setting, the timing result was 176 secs/iteration on average
    over 10 iterations. From these statistics, it is clear that LDA is quite scalable
    for a very large number of the corpus as well.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-425
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we provided theoretical and practical aspects of some advanced
    topics of machine learning with Spark. We also provided some recommendations about
    the best practice in machine learning. Following that, we have seen how to tune
    machine learning models for better and optimized performance using grid search,
    cross-validation, and hyperparameter tuning. In the later section, we have seen
    how to develop a scalable recommendation system using the ALS, which is an example
    of a model-based recommendation system using a model-based collaborative filtering
    approach. Finally, we have seen how to develop a topic modeling application as
    a text clustering technique.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: For additional aspects and topics on machine learning best practice, interested
    readers can refer to the book titled *Large Scale Machine Learning with Spark*
    at [https://www.packtpub.com/big-data-and-business-intelligence/large-scale-machine-learning-spark.](https://www.packtpub.com/big-data-and-business-intelligence/large-scale-machine-learning-spark)
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will enter into more advanced use of Spark. Although
    we have discussed and provided a comparative analysis on binary and multiclass
    classification, we will get to know more about other multinomial classification
    algorithms with Spark such as Naive Bayes, decision trees, and the One-vs-Rest
    classifier.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
