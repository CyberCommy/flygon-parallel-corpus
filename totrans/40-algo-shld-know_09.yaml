- en: Traditional Supervised Learning Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will focus on supervised machine learning algorithms, which
    are one of the most important types of modern algorithms. The distinguishing characteristic
    of a supervised machine learning algorithm is the use of labeled data to train
    a model. In this book, supervised machine learning algorithms are divided into
    two chapters. In this chapter, we will present all the traditional supervised
    machine learning algorithms, excluding neural networks. The next chapter is all
    about implementing supervised machine learning algorithms using neural networks.
    The truth is that with so much ongoing development in this field, neural networks
    are a comprehensive topic that deserves a separate chapter in this book.
  prefs: []
  type: TYPE_NORMAL
- en: So, this chapter is the first of two parts about supervised machine learning
    algorithms. First, we will introduce the fundamental concepts of supervised machine
    learning. Next, we will present two types of supervised machine models—classifiers
    and regressors. In order to demonstrate the abilities of classifiers, we will
    first present a real-world problem as a challenge. Then, we will present six different
    classification algorithms that are used to solve the problem. Then, we will focus
    on regression algorithms, first by presenting a similar problem to be solved for
    the regressors. Next, we will present three regression algorithms and use them
    to solve the problem. Finally, we will compare the results to help us summarize
    the concepts presented in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The overall objective of this chapter is for you to understand the different
    types of supervised machine learning techniques and know what the best supervised
    machine learning techniques are for certain classes of problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following concepts are discussed in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding supervised machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding classification algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The methods for evaluating the performance of classifiers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding regression algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The methods for evaluating the performance of regression algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start by looking at the basic concepts behind supervised machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding supervised machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning focuses on using data-driven approaches to create autonomous
    systems that can help us to make decisions with or without human supervision.
    In order to create these autonomous systems, machine learning uses a group of
    algorithms and methodologies to discover and formulate repeatable patterns in
    data. One of the most popular and powerful methodologies used in machine learning
    is the supervised machine learning approach. In supervised machine learning, an
    algorithm is given a set of inputs, called **features**, and their corresponding
    outputs, called **target** **variables**. Using a given dataset, a supervised
    machine learning algorithm is used to train a model that captures the complex
    relationship between the features and target variables represented by a mathematical
    formula. This trained model is the basic vehicle that is used for predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Predictions are made by generating the target variable of an unfamiliar set
    of features through the trained model.
  prefs: []
  type: TYPE_NORMAL
- en: The ability to learn from existing data in supervised learning is similar to
    the ability of the human brain to learn from experience. This learning ability
    in supervised learning uses one of the attributes of the human brain and is a
    fundamental way of opening the gates to bring decision-making power and intelligence
    to machines.
  prefs: []
  type: TYPE_NORMAL
- en: Let's consider an example where we want to use supervised machine learning techniques
    to train a model that can categorize a set of emails into legitimate ones (called
    **legit**) and unwanted ones (called **spam**). First of all, in order to get
    started, we need examples from the past so that the machine can learn what sort
    of content of emails should be classified as spam. This content-based learning
    task for text data is a complex process and is achieved through one of the supervised
    machine learning algorithms. Some examples of supervised machine learning algorithms
    that can be used to train the model in this example include decision trees and
    naive Bayes classifiers, which we will discuss later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Formulating supervised machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before going deeper into the details of supervised machine learning algorithms,
    let''s define some of the basic supervised machine learning terminologies:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Terminology** | **Explanation** |'
  prefs: []
  type: TYPE_TB
- en: '| Target variable | The target variable is the variable that we want our model
    to predict. There can be only one target variable in a supervised machine learning
    model. |'
  prefs: []
  type: TYPE_TB
- en: '| Label | If the target variable we want to predict is a category variable,
    it is called a label. |'
  prefs: []
  type: TYPE_TB
- en: '| Features | The set of input variables used to predict the label is called
    the features. |'
  prefs: []
  type: TYPE_TB
- en: '| Feature engineering | Transforming features to prepare them for the chosen
    supervised machine learning algorithm is called feature engineering. |'
  prefs: []
  type: TYPE_TB
- en: '| Feature vector | Before providing an input to a supervised machine learning
    algorithm, all the features are combined in a data structure called a feature
    vector. |'
  prefs: []
  type: TYPE_TB
- en: '| Historical data | The data from the past that is used to formulate the relationship
    between the target variable and the features is called historical data. Historical
    data comes with examples. |'
  prefs: []
  type: TYPE_TB
- en: '| Training/testing data | Historical data with examples is divided into two
    parts—a larger dataset called the training data and a smaller dataset called the
    testing data. |'
  prefs: []
  type: TYPE_TB
- en: '| Model | A mathematical formulation of the patterns that best capture the
    relationship between the target variable and the features. |'
  prefs: []
  type: TYPE_TB
- en: '| Training | Creating a model using training data. |'
  prefs: []
  type: TYPE_TB
- en: '| Testing | Evaluating the quality of the trained model using testing data.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Prediction | Using a model to predict the target variable. |'
  prefs: []
  type: TYPE_TB
- en: A trained supervised machine learning model is capable of making predictions
    by estimating the target variable based on the features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s introduce the notation that we will be using in this chapter to discuss
    the machine learning techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Variable** | **Meaning** |'
  prefs: []
  type: TYPE_TB
- en: '| *y* | Actual label |'
  prefs: []
  type: TYPE_TB
- en: '| *ý* | Predicted label |'
  prefs: []
  type: TYPE_TB
- en: '| *d* | Total number of examples |'
  prefs: []
  type: TYPE_TB
- en: '| *b* | Number of training examples |'
  prefs: []
  type: TYPE_TB
- en: '| *c* | Number of testing examples |'
  prefs: []
  type: TYPE_TB
- en: Now, let's see how some of these terminologies are formulated practically.
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed, a feature vector is defined as a data structure that has all
    the features stored in it.
  prefs: []
  type: TYPE_NORMAL
- en: If the number of features is *n* and the number of training examples is *b*,
    then `X_train` represents the training feature vector. Each example is a row in
    the feature vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the training dataset, the feature vector is represented by `X_train`. If
    there are *b* examples in the training dataset, then `X_train` will have *b* rows.
    If there are *n* variables in the training dataset, then it will have *n* columns.
    So, the training dataset will have a dimension of *n* x *b*, as represented in
    the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/83067a86-5ff3-4854-a16f-5415111cc00d.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, let's assume that there are *b* training examples and *c* testing examples.
    A particular training example is represented by (*X*, *y*).
  prefs: []
  type: TYPE_NORMAL
- en: We use superscript to indicate which training example is which within the training
    set.
  prefs: []
  type: TYPE_NORMAL
- en: So, our labeled dataset is represented by D = {X^((1)),y^((1))), (X^((2)),y^((2))),
    ..... , (X^((d)),y^((d)))}.
  prefs: []
  type: TYPE_NORMAL
- en: We divide that into two parts—D[train]and D[test].
  prefs: []
  type: TYPE_NORMAL
- en: So, our training set can be represented by D[train]  = {X^((1)),y^((1))), (X^((2)),y^((2))),
    ..... , (X^((b)),y^((b)))}.
  prefs: []
  type: TYPE_NORMAL
- en: The objective of training a model is that for any *i*^(th) example in the training
    set, the predicted value of the target value should be as close to the actual
    value in the examples as possible. In other words, ![](assets/191e0803-78b6-4df4-b1a4-54ac88b95d3f.png).
  prefs: []
  type: TYPE_NORMAL
- en: So, our testing set can be represented by D[test] = {X^((1)),y^((1))), (X^((2)),y^((2))),
    ..... , (X^((c)),y^((c)))}.
  prefs: []
  type: TYPE_NORMAL
- en: 'The values of the target variable are represented by a vector, *Y*:'
  prefs: []
  type: TYPE_NORMAL
- en: Y ={ y^((1)), y^((2)), ....., y^((m))}
  prefs: []
  type: TYPE_NORMAL
- en: Understanding enabling conditions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Supervised machine learning is based on the ability of an algorithm to train
    a model using examples. A supervised machine learning algorithm needs certain
    enabling conditions to be met in order to perform. These enabling conditions are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Enough examples**:Supervised machine learning algorithms need enough examples
    to train a model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Patterns in historical data**: The examples used to train a model need to
    have patterns in it. The likelihood of the occurrence of our event of interest
    should be dependent on a combination of patterns, trends, and events. Without
    these, we are dealing with random data that cannot be used to train a model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Valid assumptions**:When we train a supervised machine learning model using
    examples, we expect that the assumptions that apply to the examples will also
    be valid in the future. Let''s look at an actual example. If we want to train
    a machine learning model for the government that can predict the likelihood of
    whether a visa will be granted to a student, the understanding is that the laws
    and policies will not change when the model is used for predictions. If new policies
    or laws are enforced after training the model, the model may need to be retrained
    to incorporate this new information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Differentiating between classifiers and regressors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a machine learning model, the target variable can be a category variable
    or a continuous variable. The type of target variable determines what type of
    supervised machine learning model we have. Fundamentally, we have two types of
    supervised machine learning models:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Classifiers**: If the target variable is a category variable, the machine
    learning model is called a classifier. Classifiers can be used to answer the following
    type of business questions:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is this abnormal tissue growth a malignant tumor?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on the current weather conditions, will it rain tomorrow?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on the profile of a particular applicant, should their mortgage application
    be approved?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regressors**: If the target variable is a continuous variable, we train a
    regressor. Regressors can be used to answer the following types of business questions:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on the current weather condition, how much will it rain tomorrow?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What will the price of a particular home be with given characteristics?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look at both classifiers and regressors in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding classification algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In supervised machine learning, if the target variable is a category variable,
    the model is categorized as a classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: The target variable is called a  **label**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The historical data is called  **labeled data**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The production data, which the label needs to be predicted for, is called  **unlabeled
    data**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to accurately label unlabeled data using a trained model is the
    real power of classification algorithms. Classifiers predict labels for unlabeled
    data to answer a particular business question.
  prefs: []
  type: TYPE_NORMAL
- en: Before we present the details of classification algorithms, let's first present
    a business problem that we will use as a challenge for classifiers. We will then
    use six different algorithms to answer the same challenge, which will help us
    compare their methodology, approach, and performance.
  prefs: []
  type: TYPE_NORMAL
- en: Presenting the classifiers challenge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will first present a common problem, which we will use as a challenge to
    test six different classification algorithms. This common problem is referred
    to as the classifier challenge in this chapter. Using all the six classifiers
    to solve the same problem will help us in two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: All the input variables need to be processed and assembled as a complex data
    structure, called a feature vector. Using the same feature vector helps us avoid
    repeating data preparation for all six algorithms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can compare the performance of various algorithms as we are using the same
    feature vector for input.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The classifiers challenge is about predicting the likelihood of a person making
    a purchase. In the retail industry, one of the things that can help maximize sales
    is better understanding the behavior of the customers. This can be done by analyzing
    the patterns found in historical data. Let's state the problem, first.
  prefs: []
  type: TYPE_NORMAL
- en: The problem statement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given the historical data, can we train a binary classifier that can predict
    whether a particular user will eventually buy a product based on their profile?
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s explore the historical labeled data set available to solve this
    problem:'
  prefs: []
  type: TYPE_NORMAL
- en: x € ℜ^b, y € {0,1}
  prefs: []
  type: TYPE_NORMAL
- en: For a particular example, when *y* = 1, we call it a positive class and when
    *y* = 0, we call it a negative class.
  prefs: []
  type: TYPE_NORMAL
- en: Although the level of the positive and negative class can be chosen arbitrarily,
    it is good practice to define the positive class as the event of interest. If
    we are trying to flag the fraudulent transaction for a bank, then the positive
    class (that is, *y* = 1 ) should be the fraudulent transaction, not the other
    way around.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s look at the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The actual label, denoted by *y*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The predicted label, denoted by *y`*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that for our classifiers challenge, the actual value of the label found
    in examples is represented by *y*. If, in our example, someone has purchased an
    item, we say *y* =1\. The predicted values are represented by *y`*. The input
    feature vector, *x*, has a dimension of 4\. We want to determine what the probability
    is that a user will make a purchase, given a particular input.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we want to determine the probability that *y* = 1 is, given a particular
    value of feature vector *x*. Mathematically, we can represent this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/64b062ae-9b4e-4cb4-b2f0-608d0ba569e0.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, let's look at how we can process and assemble different input variables
    in the feature vector, *x*. The methodology to assemble different parts of *x*
    using the processing pipeline is discussed in more detail in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering using a data processing pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Preparing data for a chosen machine learning algorithm is called **feature engineering**
    and is a crucial part of the machine learning life cycle. Feature engineering
    is done in different stages or phases. The multi-stage processing code used to
    process data is collectively known as a **data pipeline**. Making a data pipeline
    using standard processing steps, wherever possible, makes it reusable and decreases
    the effort needed to train the models. By using more well-tested software modules,
    the quality of the code is also enhanced.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see design a reusable processing pipeline for the classifiers challenge.
    As mentioned, we will prepare data once and then use it for all the classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: Importing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The historical data for this problem is stored in a file called `dataset` in
    `.csv` format. We will use the `pd.read_csv` function from pandas to import the
    data as a data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Feature selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The process of selecting features that are relevant to the context of the problem
    that we want to solve is called **feature selection**. It is an essential part
    of feature engineering.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the file is imported, we drop the `User ID` column, which is used to identify
    a person and should be excluded when training a model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s preview the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The dataset looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/b0a76952-07bb-4025-9614-1e7e6d683bf0.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, let's look at how we can further process the input dataset.
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Many machine learning algorithms require all the features to be continuous
    variables. It means that if some of the features are category variables, we need
    to find a strategy to convert them into continuous variables. One-hot encoding
    is one of the most effective ways of performing this transformation. For this
    particular problem, the only category variable we have is `Gender`. Let''s convert
    that into a continuous variable using one-hot encoding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Once it''s converted, let''s look at the dataset again:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/6a3f7425-0e6d-444f-914c-7c12b15ada54.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice that in order to convert a variable from a category variable into a continuous
    variable, one-hot encoding has converted `Gender` into two separate columns—`Male`
    and `Female`.
  prefs: []
  type: TYPE_NORMAL
- en: Specifying the features and label
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s specify the features and labels. We will use `y` through this book to
    represent the label and `X` to represent the feature set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '`X` represents the feature vector and contains all the input variables that
    we need to use to train the model.'
  prefs: []
  type: TYPE_NORMAL
- en: Dividing the dataset into testing and training portions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s divide the training dataset into 25% testing and 75% training portions
    using `sklearn.model_selection import train_test_split`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This has created the following four data structures:'
  prefs: []
  type: TYPE_NORMAL
- en: '`X_train`: A data structure containing the features of the training data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`X_test`: A data structure containing the features of the training test'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`y_train`: A vector containing the values of the label in the training dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`y_test`: A vector containing the values of the label in the testing dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling the features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For many machine learning algorithms, it''s good practice to scale the variables
    from `0` to `1`. This is also called **feature normalization**. Let''s apply the
    scaling transformation to achieve this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: After we scale the data, it is ready to be used as input to the different classifiers
    that we will present in the subsequent sections.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the classifiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once the model is trained, we need to evaluate its performance. To do that,
    we will use the following process:'
  prefs: []
  type: TYPE_NORMAL
- en: We will divide the labeling dataset into two parts—a training partition and
    a testing partition. We will use the testing partition to evaluate the trained
    model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will use the features of our testing partition to generate labels for each
    row. This is our set of predicted labels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will compare the set of predicted labels with the actual labels to evaluate
    the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unless we are trying to solve something quite trivial, there will be some misclassifications
    when we evaluate the model. How we interpret these misclassifications to determine
    the quality of the model depends on which performance metrics we choose to use.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have both the set of actual labels and the predicted labels, a bunch
    of performance metrics can be used to evaluate the models. The best metric to
    quantify the model will depend on the requirements of the business problem that
    we want to solve, as well as the characteristics of the training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Confusion matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A confusion matrix is used to summarize the results of the evaluation of a
    classifier. The confusion matrix for a binary classifier looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ca9d79e4-5e30-4bf9-b81f-d0aee047b206.png)If the label of the classifier
    we are training has two levels, it is called a **binary classifier**. The first
    critical use case of supervised machine learning—specifically, a binary classifier—was
    during the First World War to differentiate between an aircraft and flying birds.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The classification can be divided into the following four categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**True positives** (**TP**): The positive classifications that were correctly
    classified'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True Negatives** (**TN**): The negative classifications that were correctly
    classified'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Positives** (**FP**): The positive classifications  that were actually
    negative'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Negatives** (**FN**): The negative classifications that were actually
    positive'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's see how we can use these four categories to create various performance
    metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Performance metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Performance metrics are used to quantify the performance of the trained models.
    Based on this, let''s define the following four metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Metric** | **Formula** |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy | ![](assets/6732024e-647e-40b8-8965-e0e10573db8d.png) |'
  prefs: []
  type: TYPE_TB
- en: '| Recall | ![](assets/52d52b33-c954-4bc6-ac31-325849e8a6e7.png) |'
  prefs: []
  type: TYPE_TB
- en: '| Precision | ![](assets/985538cb-41f1-4fcf-93a4-8da6a3fecf13.png) |'
  prefs: []
  type: TYPE_TB
- en: '| F1 score | ![](assets/e50cf3cc-6542-4e86-82dc-39e65afcba92.png) |'
  prefs: []
  type: TYPE_TB
- en: Accuracy is the proportion of correction classifications among all predictions.
    While calculating accuracy, we do not differentiate between TP and TN. Evaluating
    a model through accuracy is straightforward, but in certain situations, it will
    not work.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the situations where we need more than accuracy to quantify
    the performance of a model. One of these situations is when we use a model to
    predict a rare event, such as in the following examples:'
  prefs: []
  type: TYPE_NORMAL
- en: A model to predict the fraudulent transactions in a banks transactional database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A model to predict the likelihood of mechanical failure of an engine part of
    an aircraft
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In both of these examples, we are trying to predict a rare event. Two additional
    measures become more important than accuracy in these situations—recall and precision.
    Let''s look at them one by one:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Recall**: This calculates the hit rate. In the first of the preceding examples,
    it is the proportion of fraudulent documents successfully flagged by the model
    out of all the fraudulent documents. If, in our testing dataset, we had 1 million
    transactions, out of which 100 were known to be fraudulent, the model was able
    to identify 78 of them. In this case, the recall value would be 78/100.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Precision**: The precision measures how many of the transactions flagged
    by the model were actually bad. Instead of focusing on the bad transactions that
    the model failed to flag, we want to determine how precise the bad bins flagged
    by the model really is.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that the F1 score brings both the recall and precision together. If a model
    has perfect scores for both precision and recall, then its F1 score will be perfect.
    A high F1 score means that we have trained a high-quality model that has high
    recall and precision.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding overfitting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If a machine learning model performs great in a development environment but
    degrades noticeably in a production environment, we say the model is overfitted.
    This means the trained model too closely follows the training dataset. It is an
    indication there are too many details in the rules created by the model. The trade-off
    between model variance and bias best captures the idea. Let's look at these concepts
    one by one.
  prefs: []
  type: TYPE_NORMAL
- en: Bias
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Any machine learning model is trained based on certain assumptions. In general,
    these assumptions are the simplistic approximations of some real-world phenomena.
    These assumptions simplify the actual relationships between features and their
    characteristics and make a model easier to train. More assumptions means more
    bias. So, while training a model, more simplistic assumptions = high bias, and
    realistic assumptions that are more representative of actual phenomena = low bias.
  prefs: []
  type: TYPE_NORMAL
- en: In linear regression, the non-linearity of the features is ignored and they
    are approximated as linear variables. So, linear regression models are inherently
    vulnerable to exhibiting high bias.
  prefs: []
  type: TYPE_NORMAL
- en: Variance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Variance quantifies how accurately a model estimates the target variable if
    a different dataset is used to train the model. It quantifies whether the mathematical
    formulation of our model is a good generalization of the underlying patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Specific overfitted rules based on specific scenarios and situations = high
    variance, and rules that are generalized and applicable to a variety of scenarios
    and situations = low variance.
  prefs: []
  type: TYPE_NORMAL
- en: Our goal in machine learning is to train models that exhibit low bias and low
    variance. Achieving this goal is not always easy and usually keeps data scientists
    awake at night.
  prefs: []
  type: TYPE_NORMAL
- en: Bias-variance trade-off
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When training a particular machine learning model, it is tricky to decide the
    right level of generalization for the rules that comprise a trained model. The
    struggle to come up with the right level of generalization is captured by the
    bias-variance trade-off.
  prefs: []
  type: TYPE_NORMAL
- en: Note that more simplistic assumptions = more generalization = low variance =
    high variance.
  prefs: []
  type: TYPE_NORMAL
- en: This trade-off between bias and variance is determined by the choice of algorithm,
    the characteristics of the data, and various hyperparameters. It is important
    to achieve the right compromise between the bias and variance based on the requirements
    of the specific problem you are trying to solve.
  prefs: []
  type: TYPE_NORMAL
- en: Specifying the phases of classifiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once the labeled data is prepared, the development of the classifiers involves
    training, evaluation, and deployment. These three phases of implementing a classifier
    are shown in the **CRISP-DM**  (**Cross-Industry Standard Process for Data Mining**)
    life cycle in the following diagram (the CRISP-DM life cycle was explained in
    more detail in [Chapter 5](051e9b32-f15f-4e88-a63a-ae3c14696492.xhtml)*, Graph
    Algorithms*)
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/0d74d428-cad1-4af4-bf70-f6eecb3aba00.png)'
  prefs: []
  type: TYPE_IMG
- en: In the first two phases of implementing a classifier—the testing and training
    phases—we use labeled data. The labeled data is divided into two partitions—a
    larger partition called the training data and a smaller partition called the testing
    data. A random sampling technique is used to divide the input labeled data into
    training and testing partitions to make sure that both partitions contain consistent
    patterns. Note that, as the preceding diagram shows, first, there is a training
    phase, where training data is used to train a model. Once the training phase is
    over, the trained model is evaluated using the testing data. Different performance
    matrices are used to quantify the performance of the trained model. Once the model
    is evaluated, we have the model deployment phase, where the trained model is deployed
    and used for inference to solve real-world problems by labeling unlabeled data.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's look at some classification algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will look at the following classification algorithms in the subsequent sections:'
  prefs: []
  type: TYPE_NORMAL
- en: The decision tree algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The XGBoost algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The random forest algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The logistic regression algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Support Vector Machine** (**SVM**) algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The naive Bayes algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start with the decision tree algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Decision tree classification algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A decision tree is based on the recursive partitioning approach (divide and
    conquer), which generates a set of rules that can be used to predict a label.
    It starts with a root node and splits into multiple branches. Internal nodes represent
    a test on a certain attribute and the result of the test is represented by a branch
    to the next level. The decision tree ends in leaf nodes that contain the decisions.
    The process stops when partitioning no longer improves the outcome.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the decision tree classification algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The distinguishing feature of decision tree classification is the generation
    of the human-interpretable hierarchy of rules that are used to predict the label
    at runtime. The algorithm is recursive in nature. Creating this hierarchy of rules
    involves the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Find the most important feature**:  Out of all of the features, the algorithm
    identifies the feature that best differentiates between the data points in the
    training dataset with respect to the label. The calculation is based on metrics
    such as information gain or Gini impurity.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Bifurcate**: Using the most identified important feature, the algorithm creates
    a criterion that is used to divide the training dataset into two branches:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data points that pass the criterion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data points that fail the criterion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Check for leaf nodes**:  If any resultant branch mostly contains labels of
    one class, the branch is made final, resulting in a leaf node.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Check the stopping conditions and repeat**: If the provided stopping conditions
    are not met, then the algorithm will go back to *step 1* for the next iteration.
    Otherwise, the model is marked as trained and each node of the resultant decision
    tree at the lowest level is labeled as a leaf node. The stopping condition can
    be as simple as defining the number of iterations, or the default stopping condition
    can be used, where the algorithm stops as soon it reaches a certain homogeneity
    level for each of the leaf nodes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The decision tree algorithm can be explained by the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/53f88b67-009e-48f6-9389-b487fe7c0388.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, the root contains a bunch of circles and crosses.
    The algorithm creates a criterion that tries to separate the circles from the
    crosses. At each level, the decision tree creates partitions of the data, which
    are expected to be more and more homogeneous from level 1 upward. A perfect classifier
    has leaf nodes that only contain circles or crosses. Training perfect classifiers
    is usually difficult due to the inherent randomness of the training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Using the decision tree classification algorithm for the classifiers challenge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s use the decision tree classification algorithm for the common problem
    that we previously defined to predict whether a customer ends up purchasing a
    product:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To do, first, let''s instantiate the decision tree classification algorithm
    and train a model using the training portion of the data that we prepared for
    our classifiers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s use our trained model to predict the labels for the testing portion
    of our labeled data. Let''s generate a confusion matrix that can summarize the
    performance of our trained model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/7588760f-69c4-4470-ade7-85255d5e67f7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s calculate the `accuracy`, `recall`, and `precision` values for
    the created classifier by using the decision tree classification algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the preceding code will produce the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/a7a7e80c-e144-4e7c-8c1b-98cc1cec5fef.png)'
  prefs: []
  type: TYPE_IMG
- en: The performance measures help us compare different training modeling techniques
    with each other.
  prefs: []
  type: TYPE_NORMAL
- en: The strengths and weaknesses of decision tree classifiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, let's look at the strengths and weaknesses of using the decision
    tree classification algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Strengths
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the strengths of decision tree classifiers:'
  prefs: []
  type: TYPE_NORMAL
- en: The rules of the models created by using a decision tree algorithm are interpretable
    by humans. Models such as this are called **whitebox models**. Whitebox models
    are a requirement whenever transparency is needed to trace the details and reasons
    for decisions that are made by the model. This transparency is essential in applications
    where we want to prevent bias and protect vulnerable communities. For example,
    a whitebox model is generally a requirement for critical use cases in government
    and insurance industries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision tree classifiers are designed to extract information from discrete
    problem space. This means that most of the features are category variables, so
    using a decision tree to train the model is a good choice.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weaknesses
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the weaknesses of decision tree classifiers:'
  prefs: []
  type: TYPE_NORMAL
- en: If the tree generated by the decision tree classifier goes too deep, the rules
    capture too many details, resulting in an overfitted model. While using a decision
    tree algorithm, we need to be aware that decision trees are vulnerable to overfitting
    and so we need to prune the tree, whenever necessary, to prevent this.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A weakness of decision tree classifiers is their inability to capture non-linear
    relationships in the rules that they create.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use cases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, let's look at the use cases that the decision tree algorithm
    is used for.
  prefs: []
  type: TYPE_NORMAL
- en: Classifying records
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Decision trees classifiers can be used to classify data points, such as in
    the following examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mortgage applications**: To train a binary classifier to determine whether
    an applicant is likely to default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Customer segmentation**: To categorize customers into high-worth, medium-worth,
    and low-worth customers so that marketing strategies can be customized for each
    category.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Medical diagnosis**: To train a classifier that can categorize a benign or
    malignant growth.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Treatment-effectiveness analysis**: To train a classifier that can flag patients
    that have reacted positively to a particular treatment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The decision tree classification algorithm selects a small subset of features
    to create rules for. That feature selection can be used to select the features
    for another machine learning algorithm when you have a large number of features.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the ensemble methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An ensemble is a method, in machine learning, of creating more than one slightly
    different model using different parameters and then combining them into an aggregate
    model. In order to create effective ensembles, we need to find what our aggregation
    criterion is to generate the resultant model. Let's look at some ensemble algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing gradient boosting with the XGBoost algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: XGBoost was created in 2014 and is based on gradient-boosting principles. It
    has become one of the most popular ensemble classification algorithms. It generates
    a bunch of interrelated trees and uses gradient descent to minimize the residual
    error. This makes it a perfect fit for distributed infrastructures, such as Apache
    Spark, or for cloud computing, such as Google Cloud or  **Amazon Web Services**(**AWS**)**.**
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now see how we can implement gradient boosting with the XGBoost algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will instantiate the XGBClassfier classifier and train the model
    using the training portion of the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/27a8fe49-3854-4c12-84f2-e71c0f64e430.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, we will generate predictions based on the newly trained model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The produces the following output :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/3d7d4afa-c3ae-4ae3-a2d2-7ab69b67dcdd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, we will quantify the performance of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/beaac243-597f-4f5d-b1ec-022f89f01584.png)'
  prefs: []
  type: TYPE_IMG
- en: Next, let's look at the random forest algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Using the random forest algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Random forest is a type of ensemble method that works by combining several decision
    trees to decrease both the bias and the variance.
  prefs: []
  type: TYPE_NORMAL
- en: Training a random forest algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In training, this algorithm takes *N* samples from the training data and creates
    *m* subsets of our overall data. These subsets are created by randomly selecting
    some of the rows and columns of the input data. The algorithm builds *m* independent
    decision trees. These classification trees are represented by `C[1]` to `C[m]`.
  prefs: []
  type: TYPE_NORMAL
- en: Using random forest for predictions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once the model is trained, it can be used to label new data. Each of the individual
    trees generates a label. The final prediction is determined by voting these individual
    predictions, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/89d3a5b6-24cc-4fbd-b922-90865ca6b739.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that in the preceding diagram, *m* trees are trained, which is represented
    by `C[1]` to `C[m]`. That is Trees = {C[1],..,C[m]}
  prefs: []
  type: TYPE_NORMAL
- en: 'Each of the trees generates a prediction that is represented by a set:'
  prefs: []
  type: TYPE_NORMAL
- en: Individual predictions = P= {P[1],..., P[m]}
  prefs: []
  type: TYPE_NORMAL
- en: 'The final prediction is represented by `P[f]`. It is determined by the majority
    of the individual predictions. The `mode` function can be used to find the majority
    decision (`mode` is the number that repeats most often and is in the majority).
    The individual prediction and the final prediction are linked, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: P[f] = mode (P)
  prefs: []
  type: TYPE_NORMAL
- en: Differentiating the random forest algorithm from ensemble boosting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Each of the trees generated by the random forest algorithm is totally independent
    of each other. It is not aware of any of the details of the other trees in the
    ensemble. This differentiates it from other techniques, such as ensemble boosting.
  prefs: []
  type: TYPE_NORMAL
- en: Using the random forest algorithm for the classifiers challenge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's instantiate the random forest algorithm and use it to train our model
    using the training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two key hyperparameters that we''ll be looking at here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`n_estimators`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_depth`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `n_estimators` hyperparameter controls how many individual decision trees
    are built and the `max_depth` hyperparameter controls how deep each of these individual
    decision trees can go.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, in other words, a decision tree can keep splitting and splitting until
    it has a node that represents every given example in the training set. By setting
    `max_depth`, we constrain how many levels of splits it can make. This controls
    the complexity of the model and determines how closely it fits the training data.
    If we refer to the following output, `n_estimators` controls the width of the
    random forest model and `max_depth` controls the depth of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/d4ee0995-7c13-4508-bf5c-5fa23929a1be.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once the random forest model is trained, let''s use it for predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Which gives the output as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/374d8a18-c24f-44c3-a475-e19c54bc4329.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s quantify how good our model is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We will observe the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ac3d4ce9-a599-4064-a93f-4742874af7b4.png)'
  prefs: []
  type: TYPE_IMG
- en: Next, let's look into logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logistic regression is a classification algorithm used for binary classification.
    It uses a logistic function to formulate the interaction between the input features
    and the target variable. It is one of the simplest classification techniques that
    is used to model a binary dependent variable.
  prefs: []
  type: TYPE_NORMAL
- en: Assumptions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Logistic regression assumes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The training dataset does not have a missing value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The label is a binary category variable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The label is ordinal—in other words, a categorical variable with ordered values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All features or input variables are independent of each other.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Establishing the relationship
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For logistic regression, the predicted value is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/57472f0a-fed2-46a3-8f85-82a58abe001c.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's suppose that ![](assets/57ba1e44-a466-4007-99ec-5e7c03a7d299.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'So now:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/5a9b351d-1683-44cf-8324-c8897ee20680.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding relationship can be graphically shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/c4c1e51c-a610-4fb4-8c21-ba38fed82d20.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that if *z* is large, σ (*z*) will equal `1`. If *z* is very small or a
    large negative number, σ (z) will equal `0`. So, the objective of logistic regression
    is to find the correct values for *w* and *j*.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression is named after the function that is used to formulate it,
    called the **logistic** or **sigmoid function**.
  prefs: []
  type: TYPE_NORMAL
- en: The loss and cost functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `loss` function defines how we want to quantify an error for a particular
    example in our training data. The `cost` function defines how we want to minimize
    an error in our entire training dataset. So, the `loss` function is used for one
    of the examples in the training dataset and the `cost` function is used for the
    overall cost that quantifies the overall deviation of the actual and predicted
    values. It is dependent on the choice of *w* and *h*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `loss` function used in logistic regression is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Loss (ý^((i)), y^((i))) = - (y^((i))log ý^((i))+(1-y^((i)) ) log (1-ý^((i)))*'
  prefs: []
  type: TYPE_NORMAL
- en: Note that when  *y^((i))  = 1, Loss(ý^((i)), y^((i))**) = - logý^((i))*.Minimizing
    the loss will result in a large value of ý^((i)) . Being a sigmoid function, the
    maximum value will be `1`.
  prefs: []
  type: TYPE_NORMAL
- en: If *y^((i)) = 0, Loss (ý^((i)), y^((i))) = - log (1-ý^((i))**)*. Minimizing
    the loss will result in *ý^((i))* being as small as possible, which is `0`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The cost function of logistic regression is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/41a6378a-1b02-4912-8fcd-0279b7e7fc45.png)'
  prefs: []
  type: TYPE_IMG
- en: When to use logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logistic regression works great for binary classifiers. Logistic regression
    doesn't do very well when the data is huge but the quality of the data is not
    great. It can capture relationships that aren't too complex. While it doesn't
    usually generate the greatest performance, it does set a very nice benchmark to
    start.
  prefs: []
  type: TYPE_NORMAL
- en: Using the logistic regression algorithm for the classifiers challenge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will see how we can use the logistic regression algorithm
    for the classifiers challenge:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s instantiate a logistic regression model and train it using the
    training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s predict the values of the `test` data and create a confusion matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output upon running the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/e6d66de7-957c-43d0-83a6-bc2b2f334b6d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s look at the performance metrics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output upon running the preceding code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/31737825-b84d-423e-8416-c6885f175eb1.png)'
  prefs: []
  type: TYPE_IMG
- en: Next, let's look at **SVM**.
  prefs: []
  type: TYPE_NORMAL
- en: The SVM algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s look at SVM. SVM is a classifier that finds an optimal hyperplane
    that maximizes the margin between two classes. In SVMs, our optimization objective
    is to maximize the margin. The margin is defined as the distance between the separating
    hyperplane (the decision boundary) and the training samples that are closest to
    this hyperplane, called the **support vectors***.* So, let''s start with a very
    basic example with only two dimensions, *X1* and *X2*. We want a line to separate
    the circles from the crosses. This is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/9830448e-e681-400a-9017-57888b5df6da.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We have drawn two lines and both perfectly separate the crosses from the circles.
    However, there has to be an optimal line, or decision boundary, that gives us
    the best chance to correctly classify most of the additional examples. A reasonable
    choice may be a line that is evenly spaced between these two classes to give a
    little bit of a buffer for each class, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/710cb98c-c09e-42bb-a173-1e392b0f8d5b.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, let's see how we can use SVM to train a classifier for our challenge.
  prefs: []
  type: TYPE_NORMAL
- en: Using the SVM algorithm for the classifiers challenge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, let''s instantiate the SVM classifier and then use the training portion
    of the labeled data to train it. The `kernel` hyperparameter determines the type
    of transformation that is applied to the input data in order to make it linearly
    separable.:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Once trained, let''s generate some predictions and look at the confusion matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Observe the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/9e0d3b4b-2ba9-48ee-8999-692c32d334b5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s look at the various performance metrics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the preceding code, we get the following values as our output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/64c827cb-bf53-453d-96f1-cb1d34c01db9.png)'
  prefs: []
  type: TYPE_IMG
- en: Understanding the naive Bayes algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Based on probability theory, naive Bayes is one of the simplest classification
    algorithms. If used properly, it can come up with accurate predictions. The Naive
    Bayes Algorithm is s0-named for two reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: It is based on a naive assumption that there is independence between the features
    and the input variable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is based on Bayes, theorem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This algorithm tries to classify instances based on the probabilities of the
    preceding attributes/instances, assuming complete attribute independence.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three types of events:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Independent** events do not affect the probability of another event occurring
    (for example, receiving an email offering you free entry to a tech event *and*
    a re-organization occurring in your company).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dependent** events affect the probability of another event occurring; that
    is, they are linked in some way (for example, the probability of you getting to
    a conference on time could be affected by an airline staff strike or flights that
    may not run on time).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mutually exclusive** events cannot occur simultaneously (for example, the
    probability of rolling a three and a six on a single dice roll is 0—these two
    outcomes are mutually exclusive).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayes, theorem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Bayes, theorem is used to calculate the conditional probability between two
    independent events, *A* and *B*. The probability of events *A* and *B* happening  is
    represented by P(*A*) and P(*B*). The conditional probability is represented by
    P(*B*|*A*), which is the conditional probability that event *B* will happen given
    that event *A* has occurred:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/c3857335-8ad7-48cb-bc9d-d0a1a132aebf.png)'
  prefs: []
  type: TYPE_IMG
- en: Calculating probabilities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Naive Bayes is based on probability fundamentals. The probability of a single
    event occurring (the observational probability) is calculated by taking the number
    of times the event occurred and dividing it by the total number of processes that
    could have led to that event. For example, a call center receives over 100 support
    calls per day, 50 times over the course of a month. You want to know the probability
    that a call is responded to in under 3 minutes based on the previous times it
    was responded to. If the call center manages to match this time record on 27 occasions,
    then the observational probability of 100 calls being answered in under 3 minutes
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(100 support calls in under 3 mins) = (27 / 50) = 0.54 (54%)*'
  prefs: []
  type: TYPE_NORMAL
- en: 100 calls can be responded to in under 3 minutes in about half the time, based
    on records of the 50 times it occurred in the past.
  prefs: []
  type: TYPE_NORMAL
- en: Multiplication rules for AND events
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To calculate the probability of two or more events occurring simultaneously,
    consider whether events are independent or dependent. If they are independent,
    the simple multiplication rule is used:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(outcome 1 AND outcome 2) = P(outcome 1) * P(outcome 2)*'
  prefs: []
  type: TYPE_NORMAL
- en: For example, to calculate the probability of receiving an email with free entry
    to a tech event  *and* re-organization occurring in your workplace, this simple
    multiplication rule would be used. The two events are independent as the occurrence
    of one does not affect the chance of the other occurring
  prefs: []
  type: TYPE_NORMAL
- en: 'If receiving the tech event email has a probability of 31% and the probability
    of staff re-organization is 82%, then the probability of both occurring is calculated
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: P(email AND re-organization) = P(email) * P(re-organization) = (0.31) * (0.82)
    = 0.2542 (25%)
  prefs: []
  type: TYPE_NORMAL
- en: The general multiplication rule
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If two or more events are dependent, the general multiplication rule is used.
    This formula is actually valid in both cases of independent and dependent events:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(outcome 1 AND outcome 2)=P(outcome 1)*P(outcome 2 | outcome 1)*'
  prefs: []
  type: TYPE_NORMAL
- en: Note that `P(outcome 2 | outcome 1)` refers to the conditional probability of
    `outcome 2` occurring given `outcome 1` has already occurred. The formula incorporates
    the dependence between the events. If the events are independent, then the conditional
    probability is irrelevant as one outcome does not influence the chance of the
    other occurring, and `P(outcome 2 | outcome 1)` is simply `P(outcome 2)`. Note
    that the formula in this case just becomes the simple multiplication rule.
  prefs: []
  type: TYPE_NORMAL
- en: Addition rules for OR events
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When calculating the probability of either one event or the other occurring
    (mutually exclusive), the following simple addition rule is used:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(outcome 1 OR outcome 2) = P(outcome 1) + P(outcome 2)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, what is the probability of rolling a 6 or a 3? To answer this
    question, first, note that both outcomes cannot occur simultaneously. The probability
    of rolling a 6 is (1 / 6) and the same can be said for rolling a 3:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(6 OR 3) = (1 / 6) + (1 / 6) = 0.33 (33%)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'If the events are not mutually exclusive and can occur simultaneously, use
    the following general addition formula, which is always valid in both cases of
    mutual exclusiveness and of non-mutual exclusiveness:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(outcome 1 OR outcome 2) = P(outcome 1) + P(outcome 2) P(outcome 1 AND outcome
    2)*'
  prefs: []
  type: TYPE_NORMAL
- en: Using the naive Bayes algorithm for the classifiers challenge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s use the naive Bayes algorithm to solve the classifiers challenge:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the `GaussianNB()` function and use it to train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s use the trained model to predict the results. We will use it to
    predict the labels for our test partition, which is `X_test`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s print the confusion matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/61ad873a-a502-41be-8d3b-433d21072c96.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s print the performance matrices to quantify the quality of our trained
    model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Which gives the output as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/7cc3ecd6-99b2-426d-9e47-d434e60e3d13.png)'
  prefs: []
  type: TYPE_IMG
- en: For classification algorithms, the winner is...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at the performance metrics of the various algorithms we have presented.
    This is summarized in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Algorithm** | **Accuracy** | **Recall** | **Precision** |'
  prefs: []
  type: TYPE_TB
- en: '| Decision tree | 0.94 | 0.93 | 0.88 |'
  prefs: []
  type: TYPE_TB
- en: '| XGBoost | 0.93 | 0.90 | 0.87 |'
  prefs: []
  type: TYPE_TB
- en: '| Random forest | 0.93 | 0.90 | 0.87 |'
  prefs: []
  type: TYPE_TB
- en: '| Logistic regression | 0.91 | 0.81 | 0.89 |'
  prefs: []
  type: TYPE_TB
- en: '| SVM | 0.89 | 0.71 | 0.92 |'
  prefs: []
  type: TYPE_TB
- en: '| Naive Bayes | 0.92 | 0.81 | 0.92 |'
  prefs: []
  type: TYPE_TB
- en: Looking at the preceding table, we can observe that the decision tree classifier
    performs the best in terms of accuracy and recall. If we are looking for precision,
    then there is a tie between SVM and naive Bayes, so either one will work for us.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding regression algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The supervised machine learning model uses one of the regression algorithms
    if the target variable is a continuous variable. In this case, the machine learning
    model is called a regressor.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will present various algorithms that can be used to train
    a supervised machine learning regression model—or simply, regressors. Before we
    go into the details of the algorithms, let's first create a challenge for these
    algorithms to test their performance, abilities, and effectiveness on.
  prefs: []
  type: TYPE_NORMAL
- en: Presenting the regressors challenge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Similar to the approach that we used with the classification algorithms, we
    will first present a problem to be solved as a challenge for all regression algorithms.
    We will call this common problem as the regressors challenge. Then, we will use
    three different regression algorithms to address the challenge. This approach
    of using a common challenge for different regression algorithms has two benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: We can prepare the data once and used the prepared data on all three regression
    algorithms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can compare the performance of three regression algorithms in a meaningful
    way as we will use them to solve the same problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look at the problem statement of the challenge.
  prefs: []
  type: TYPE_NORMAL
- en: The problem statement of the regressors challenge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Predicting the mileage of different vehicles is important these days. An efficient
    vehicle is good for the environment and is also cost-effective. The mileage can
    be estimated from the power of the engine and the characteristics of the vehicle.
    Let's create a challenge for regressors to train a model that can predict the
    **Miles Per Gallon** (**MPG**) of a vehicle based on its characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at the historical dataset that we will use to train the regressors.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the historical dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the features of the historical dataset data that we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Name** | **Type** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `NAME` | Category | Identifies a particular vehicle |'
  prefs: []
  type: TYPE_TB
- en: '| `CYLINDERS` | Continuous | The number of cylinders (between 4 and 8) |'
  prefs: []
  type: TYPE_TB
- en: '| `DISPLACEMENT` | Continuous | The displacement of the engine in cubic.inches
    |'
  prefs: []
  type: TYPE_TB
- en: '| `HORSEPOWER` | Continuous | The horsepower of the engine |'
  prefs: []
  type: TYPE_TB
- en: '| `ACCELERATION` | Continuous | The time it takes to accelerate from 0 to 60
    mph (in seconds) |'
  prefs: []
  type: TYPE_TB
- en: The target variable for this problem is a continuous variable, `MPG`, that specifies
    the mpg for each of the vehicles.
  prefs: []
  type: TYPE_NORMAL
- en: Let's first design the data processing pipeline for this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering using a data processing pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how we can design a reusable processing pipeline to address the
    regressors challenge. As mentioned, we will prepare the data once and then use
    it in all the regression algorithms. Let''s follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by importing the dataset, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now preview the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This is how the dataset will look:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/c62f1a01-274e-4d93-b005-cd688ca4d630.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s proceed on to feature selection. Let''s drop the `NAME` column
    as it is only an identifier that is needed for cars. Columns that are used to
    identify the rows in our dataset are not relevant for training the model. Let''s
    drop this column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s convert all of the input variables and impute all the null values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Imputation improves the quality of the data and prepares it to be used to train
    the model. Now, let''s see the final step:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s divide the data into testing and training partitions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This has created the following four data structures:'
  prefs: []
  type: TYPE_NORMAL
- en: '`X_train`: A data structure containing the features of the training data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`X_test`: A data structure containing the features of the training test'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`y_train`: A vector containing the values of the label in the training dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`y_test`: A vector containing the values of the label in the testing dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's use the prepared data on three different regressors so that we can
    compare their performance.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Of all the supervised machine learning techniques, the linear regression algorithm
    is the easiest one to understand. We will first look at simple linear regression
    and then we will expand the concept to multiple linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: Simple linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In its simplest form, linear regression formulates the relationship between
    a single continuous independent variable and a single continuous independent variable.
    A (simple) regression is used to show the extent that changes in a dependent variable
    (shown on the *y*-axis) can be attributed to changes in an explanatory variable
    (shown on the *x*-axis). It can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/a1ca60f3-867b-410e-b01f-f8f00c083695.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This formula can be explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*y* is the dependent variable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*X* is the independent variable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](assets/4dc02d52-61ac-4dc5-ade3-3868a299cd92.png) is the slope that indicates
    how much the line rises for each increase in *X*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*α* is the intercept that indicates the value of *y* when *X* = 0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some examples of relationships between a single continuous dependent variable
    and a single continuous independent variable are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A person's weight and their calories intake
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The price of a house and its area in square feet in a particular neighborhood
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The humidity in the air and the likelihood of rain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For linear regression, both the input (independent) variable and the target
    (dependent) variable must be numeric. The best relationship is found by minimizing
    the sum of the squares of the vertical distances of each point from a line drawn
    through all the points. It is assumed that the relationship is linear between
    the predictor variable and the target variable. For example, the more money invested
    in research and development, the higher the sales.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at a specific example. Let''s try to formulate the relationship
    between marketing expenditures and sales for a particular product. They are found
    to be directly relational to each other. The marketing expenditures and sales
    are drawn on a two-dimensional graph and are shown as blue diamonds. The relationship
    can best be approximated by drawing a straight line, as in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/4308ca96-fec7-4de9-96f0-f700a67e5f77.png)'
  prefs: []
  type: TYPE_IMG
- en: Once the linear line is drawn, we can see the mathematical relationship between
    the marketing expenditure and sales.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the regressors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The linear line that we drew is an approximation of the relationship between
    the dependent and independent variables. Even the best line will have some deviation
    from the actual values, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/71d38f55-1568-4a86-ba02-4681a035c983.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A typical way of quantifying the performance of linear regression models is
    by using **Root Mean Square Error** (**RMSE**). This calculates the standard deviation
    of the errors made by the trained model mathematically. For a certain example
    in the training dataset, the `loss` function is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Loss (ý^((i)), y^((i))) = 1/2(ý^((i)-) y^((i)))²
  prefs: []
  type: TYPE_NORMAL
- en: 'This leads to the following `cost` function, which minimizes the loss of all
    of the examples in the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/25cd8116-2626-4182-a7c5-fb22681dd6b4.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's try to interpret RMSE. If RMSE is $50 for our example model that predicts
    the price of a product, this means that around 68.2% of the predictions will fall
    within $50 of the true value (that is, *α*). It also means that 95% of the predictions
    will fall within $100 (that is, 2*α*) of the actual value. Finally, 99.7% of the
    predictions will fall within $150 of the actual value.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The fact is that most real-world analyses have more than one independent variable.  Multiple
    regression is an extension of simple linear regression. The key difference is
    that there are additional beta coefficients for the additional predictor variables.
    When training a model, the goal is to find the beta coefficients that minimize
    the errors of the linear equation. Let's try to mathematically formulate the relationship
    between the dependent variable and the set of independent variables (features).
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to a simple linear equation, the dependent variable, *y*, is quantified
    as the sum of an intercept term plus the product of the *β* coefficients multiplied
    by the *x* value for each of the *i* features:'
  prefs: []
  type: TYPE_NORMAL
- en: y = α + β  [1]  x  [1]  + β  [2]  x 2 +...+ β  [i]  x  [i]  + ε
  prefs: []
  type: TYPE_NORMAL
- en: The error is represented by *ε* and indicates that the predictions are not perfect.
  prefs: []
  type: TYPE_NORMAL
- en: The *β* coefficients allow each feature to have a separate estimated effect
    on the value of *y* because of  *y* changes by an amount of *β  [i]*  for each
    unit increase in *x*[*i*.] Moreover, the intercept (*α*) indicates the expected
    value of *y* when the independent variables are all 0.
  prefs: []
  type: TYPE_NORMAL
- en: Note that all the variables in the preceding equation can be represented by
    a bunch of vectors. The target and predictor variables are now vectors with a
    row and the regression coefficients, *β*, and errors, *ε*, are also vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Using the linear regression algorithm for the regressors challenge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s train the model using the training portion of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by importing the linear regression package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, let''s instantiate the linear regression model and train it using the
    training dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s predict the results using the test portion of the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The output generated by running the preceding code will generate the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/10b80cc4-475a-4fc7-9c53-ab9cad02c6e7.png)'
  prefs: []
  type: TYPE_IMG
- en: As discussed in the preceding section, RMSE is the standard deviation of the
    error. It indicates that 68.2% of predictions will fall within `4.36` of the value
    of the target variable.
  prefs: []
  type: TYPE_NORMAL
- en: When is linear regression used?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Linear regression is used to solve many real-world problems, including the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Sales forecasting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting optimum product prices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantifying the causal relationship between an event and the response, such
    as in clinical drug trials, engineering safety tests, or marketing research
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying patterns that can be used to forecast future behavior, given known
    criteria—for example, predicting insurance claims, natural disaster damage, election
    results, and crime rates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The weaknesses of linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The weaknesses of linear regression are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It only works with numerical features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Categorical data needs to be preprocessed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It does not cope well with missing data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It makes assumptions about the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The regression tree algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The regression tree algorithm is similar to the classification tree algorithm,
    except the target variable is a continuous variable, not a category variable.
  prefs: []
  type: TYPE_NORMAL
- en: Using the regression tree algorithm for the regressors challenge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will see how a regression tree algorithm can be used for
    the regressors challenge:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we train the model using a regression tree algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/2e89f7e5-0fd7-4b78-82ee-d60ba4e2b15d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once the regression tree model is trained, we use the trained model to predict
    the values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we calculate RMSE to quantify the performance of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/c5716e34-aa3b-4b39-b5b2-88b8ab4419c0.png)'
  prefs: []
  type: TYPE_IMG
- en: The gradient boost regression algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's now look at the gradient boost regression algorithm. It uses an ensemble
    of decision trees in an effort to better formulate the underlying patterns in
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Using gradient boost regression algorithm for the regressors challenge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will see how we can use the gradient boost regression algorithm
    for the regressors challenge:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we train the model using the gradient boost regression algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/c59155c9-6344-4272-b33e-826d814ed78f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once the gradient regression algorithm model is trained, we use it to predict
    the values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we calculate RMSE to quantify the performance of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this will give us the output value, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/bd5c2be4-d453-4b08-90be-dbe55b9e1dc9.png)'
  prefs: []
  type: TYPE_IMG
- en: For regression algorithms, the winner is...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at the performance of the three regression algorithms that we used
    on the same data and exactly the same use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Algorithm** | **RMSE** |'
  prefs: []
  type: TYPE_TB
- en: '| Linear regression | 4.36214129677179 |'
  prefs: []
  type: TYPE_TB
- en: '| Regression tree | 5.2771702288377 |'
  prefs: []
  type: TYPE_TB
- en: '| Gradient boost regression | 4.034836373089085 |'
  prefs: []
  type: TYPE_TB
- en: Looking at the performance of all the regression algorithms, it is obvious that
    the performance of gradient boost regression is the best as it has the lowest
    RMSE. This is followed by linear regression. The regression tree algorithm performed
    the worst for this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Practical example – how to predict the weather
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's see how we can use the concepts developed in this chapter to predict the
    weather. Let's assume that we want to predict whether it will rain tomorrow based
    on the data collected over a year for a particular city.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data available to train this model is in the CSV file called `weather.csv`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s import the data as a pandas data frame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at the columns of the data frame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/cc8c37ff-f4b0-42ab-8a02-2d6901bbd040.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, let''s look at the header of the first 13 columns of the `weather.csv`
    data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/5c5f2025-72f7-413f-b7a2-42b3fcfa249b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s look at the last 10 columns of the `weather.csv` data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/825cb777-bb57-4caf-ab05-15d8ab625241.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s use `x` to represent the input features. We will drop the `Date` field
    for the feature list as it is not useful in the context of predictions. We will
    also drop the `RainTomorrow` label:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s use `y` to represent the label:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s divide the data into `train_test_split`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'As the label is a binary variable, we are training a classifier. So, logistic
    regression will be a good choice here. First, let''s instantiate the logistic
    regression model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can use `train_x` and `test_x` to train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the model is trained, let''s use it for predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s find the accuracy of our trained model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/baa59ee9-e705-4a2c-9824-b92d37b0b333.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, this binary classifier can be used to predict whether it will rain tomorrow.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we started by looking at the basics of supervised machine learning.
    Then, we looked at various classification algorithms  in more detail. Next, we
    looked at different methods to evaluate the performance of classifiers and studied
    various regression algorithms. We also looked at the different methods that can
    be used to evaluate the performance of the algorithms that we studied.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at neural networks and deep learning algorithms.
    We will look at the methods used to train a neural network and we will also look
    at the various tools and frameworks available for evaluating and deploying a neural
    network.
  prefs: []
  type: TYPE_NORMAL
