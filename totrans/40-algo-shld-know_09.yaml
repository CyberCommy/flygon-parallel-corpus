- en: Traditional Supervised Learning Algorithms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 传统监督学习算法
- en: In this chapter, we will focus on supervised machine learning algorithms, which
    are one of the most important types of modern algorithms. The distinguishing characteristic
    of a supervised machine learning algorithm is the use of labeled data to train
    a model. In this book, supervised machine learning algorithms are divided into
    two chapters. In this chapter, we will present all the traditional supervised
    machine learning algorithms, excluding neural networks. The next chapter is all
    about implementing supervised machine learning algorithms using neural networks.
    The truth is that with so much ongoing development in this field, neural networks
    are a comprehensive topic that deserves a separate chapter in this book.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将重点介绍监督式机器学习算法，这是现代算法中最重要的类型之一。监督式机器学习算法的显著特征是使用带标签的数据来训练模型。在本书中，监督式机器学习算法分为两章。在本章中，我们将介绍所有传统的监督式机器学习算法，不包括神经网络。下一章将全面介绍使用神经网络实现监督式机器学习算法。事实上，在这一领域有如此多的持续发展，神经网络是一个值得在本书中单独章节讨论的综合性主题。
- en: So, this chapter is the first of two parts about supervised machine learning
    algorithms. First, we will introduce the fundamental concepts of supervised machine
    learning. Next, we will present two types of supervised machine models—classifiers
    and regressors. In order to demonstrate the abilities of classifiers, we will
    first present a real-world problem as a challenge. Then, we will present six different
    classification algorithms that are used to solve the problem. Then, we will focus
    on regression algorithms, first by presenting a similar problem to be solved for
    the regressors. Next, we will present three regression algorithms and use them
    to solve the problem. Finally, we will compare the results to help us summarize
    the concepts presented in this chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这一章是关于监督式机器学习算法的两个部分中的第一部分。首先，我们将介绍监督式机器学习的基本概念。接下来，我们将介绍两种监督式机器模型——分类器和回归器。为了展示分类器的能力，我们将首先提出一个真实世界的问题作为挑战。然后，我们将介绍六种不同的分类算法，用于解决这个问题。然后，我们将专注于回归算法，首先提出一个类似的问题，以便为回归器解决问题。接下来，我们将介绍三种回归算法，并使用它们来解决问题。最后，我们将比较结果，以帮助我们总结本章介绍的概念。
- en: The overall objective of this chapter is for you to understand the different
    types of supervised machine learning techniques and know what the best supervised
    machine learning techniques are for certain classes of problems.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的总体目标是让您了解不同类型的监督式机器学习技术，并了解对于某些类别的问题，最佳的监督式机器学习技术是什么。
- en: 'The following concepts are discussed in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了以下概念：
- en: Understanding supervised machine learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解监督式机器学习
- en: Understanding classification algorithms
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解分类算法
- en: The methods for evaluating the performance of classifiers
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估分类器性能的方法
- en: Understanding regression algorithms
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解回归算法
- en: The methods for evaluating the performance of regression algorithms
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估回归算法性能的方法
- en: Let's start by looking at the basic concepts behind supervised machine learning.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从理解监督式机器学习背后的基本概念开始。
- en: Understanding supervised machine learning
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解监督式机器学习
- en: Machine learning focuses on using data-driven approaches to create autonomous
    systems that can help us to make decisions with or without human supervision.
    In order to create these autonomous systems, machine learning uses a group of
    algorithms and methodologies to discover and formulate repeatable patterns in
    data. One of the most popular and powerful methodologies used in machine learning
    is the supervised machine learning approach. In supervised machine learning, an
    algorithm is given a set of inputs, called **features**, and their corresponding
    outputs, called **target** **variables**. Using a given dataset, a supervised
    machine learning algorithm is used to train a model that captures the complex
    relationship between the features and target variables represented by a mathematical
    formula. This trained model is the basic vehicle that is used for predictions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习专注于使用数据驱动的方法来创建可以帮助我们做出决策的自主系统，无论是否有人类监督。为了创建这些自主系统，机器学习使用一组算法和方法来发现和制定数据中可重复的模式。在机器学习中最流行和强大的方法之一是监督式机器学习方法。在监督式机器学习中，算法被给定一组输入，称为**特征**，以及它们对应的输出，称为**目标**
    **变量**。使用给定的数据集，监督式机器学习算法用于训练一个捕捉特征和目标变量之间复杂关系的模型，该关系由数学公式表示。这个训练好的模型是用于预测的基本工具。
- en: Predictions are made by generating the target variable of an unfamiliar set
    of features through the trained model.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 通过训练模型，通过生成未知特征集的目标变量来进行预测。
- en: The ability to learn from existing data in supervised learning is similar to
    the ability of the human brain to learn from experience. This learning ability
    in supervised learning uses one of the attributes of the human brain and is a
    fundamental way of opening the gates to bring decision-making power and intelligence
    to machines.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中从现有数据中学习的能力类似于人脑从经验中学习的能力。监督学习中的这种学习能力使用了人脑的一个属性，是将决策能力和智能引入机器的基本途径。
- en: Let's consider an example where we want to use supervised machine learning techniques
    to train a model that can categorize a set of emails into legitimate ones (called
    **legit**) and unwanted ones (called **spam**). First of all, in order to get
    started, we need examples from the past so that the machine can learn what sort
    of content of emails should be classified as spam. This content-based learning
    task for text data is a complex process and is achieved through one of the supervised
    machine learning algorithms. Some examples of supervised machine learning algorithms
    that can be used to train the model in this example include decision trees and
    naive Bayes classifiers, which we will discuss later in this chapter.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Formulating supervised machine learning
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before going deeper into the details of supervised machine learning algorithms,
    let''s define some of the basic supervised machine learning terminologies:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '| **Terminology** | **Explanation** |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
- en: '| Target variable | The target variable is the variable that we want our model
    to predict. There can be only one target variable in a supervised machine learning
    model. |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
- en: '| Label | If the target variable we want to predict is a category variable,
    it is called a label. |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
- en: '| Features | The set of input variables used to predict the label is called
    the features. |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
- en: '| Feature engineering | Transforming features to prepare them for the chosen
    supervised machine learning algorithm is called feature engineering. |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
- en: '| Feature vector | Before providing an input to a supervised machine learning
    algorithm, all the features are combined in a data structure called a feature
    vector. |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
- en: '| Historical data | The data from the past that is used to formulate the relationship
    between the target variable and the features is called historical data. Historical
    data comes with examples. |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
- en: '| Training/testing data | Historical data with examples is divided into two
    parts—a larger dataset called the training data and a smaller dataset called the
    testing data. |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
- en: '| Model | A mathematical formulation of the patterns that best capture the
    relationship between the target variable and the features. |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
- en: '| Training | Creating a model using training data. |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
- en: '| Testing | Evaluating the quality of the trained model using testing data.
    |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
- en: '| Prediction | Using a model to predict the target variable. |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
- en: A trained supervised machine learning model is capable of making predictions
    by estimating the target variable based on the features.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s introduce the notation that we will be using in this chapter to discuss
    the machine learning techniques:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '| **Variable** | **Meaning** |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
- en: '| *y* | Actual label |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
- en: '| *ý* | Predicted label |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
- en: '| *d* | Total number of examples |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
- en: '| *b* | Number of training examples |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
- en: '| *c* | Number of testing examples |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
- en: Now, let's see how some of these terminologies are formulated practically.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed, a feature vector is defined as a data structure that has all
    the features stored in it.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: If the number of features is *n* and the number of training examples is *b*,
    then `X_train` represents the training feature vector. Each example is a row in
    the feature vector.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 'For the training dataset, the feature vector is represented by `X_train`. If
    there are *b* examples in the training dataset, then `X_train` will have *b* rows.
    If there are *n* variables in the training dataset, then it will have *n* columns.
    So, the training dataset will have a dimension of *n* x *b*, as represented in
    the following diagram:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/83067a86-5ff3-4854-a16f-5415111cc00d.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
- en: Now, let's assume that there are *b* training examples and *c* testing examples.
    A particular training example is represented by (*X*, *y*).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: We use superscript to indicate which training example is which within the training
    set.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: So, our labeled dataset is represented by D = {X^((1)),y^((1))), (X^((2)),y^((2))),
    ..... , (X^((d)),y^((d)))}.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: We divide that into two parts—D[train]and D[test].
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: So, our training set can be represented by D[train]  = {X^((1)),y^((1))), (X^((2)),y^((2))),
    ..... , (X^((b)),y^((b)))}.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: The objective of training a model is that for any *i*^(th) example in the training
    set, the predicted value of the target value should be as close to the actual
    value in the examples as possible. In other words, ![](assets/191e0803-78b6-4df4-b1a4-54ac88b95d3f.png).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: So, our testing set can be represented by D[test] = {X^((1)),y^((1))), (X^((2)),y^((2))),
    ..... , (X^((c)),y^((c)))}.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: 'The values of the target variable are represented by a vector, *Y*:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Y ={ y^((1)), y^((2)), ....., y^((m))}
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Understanding enabling conditions
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Supervised machine learning is based on the ability of an algorithm to train
    a model using examples. A supervised machine learning algorithm needs certain
    enabling conditions to be met in order to perform. These enabling conditions are
    as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '**Enough examples**:Supervised machine learning algorithms need enough examples
    to train a model.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Patterns in historical data**: The examples used to train a model need to
    have patterns in it. The likelihood of the occurrence of our event of interest
    should be dependent on a combination of patterns, trends, and events. Without
    these, we are dealing with random data that cannot be used to train a model.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Valid assumptions**:When we train a supervised machine learning model using
    examples, we expect that the assumptions that apply to the examples will also
    be valid in the future. Let''s look at an actual example. If we want to train
    a machine learning model for the government that can predict the likelihood of
    whether a visa will be granted to a student, the understanding is that the laws
    and policies will not change when the model is used for predictions. If new policies
    or laws are enforced after training the model, the model may need to be retrained
    to incorporate this new information.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Differentiating between classifiers and regressors
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a machine learning model, the target variable can be a category variable
    or a continuous variable. The type of target variable determines what type of
    supervised machine learning model we have. Fundamentally, we have two types of
    supervised machine learning models:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '**Classifiers**: If the target variable is a category variable, the machine
    learning model is called a classifier. Classifiers can be used to answer the following
    type of business questions:'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is this abnormal tissue growth a malignant tumor?
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on the current weather conditions, will it rain tomorrow?
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on the profile of a particular applicant, should their mortgage application
    be approved?
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regressors**: If the target variable is a continuous variable, we train a
    regressor. Regressors can be used to answer the following types of business questions:'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on the current weather condition, how much will it rain tomorrow?
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What will the price of a particular home be with given characteristics?
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look at both classifiers and regressors in more detail.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Understanding classification algorithms
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In supervised machine learning, if the target variable is a category variable,
    the model is categorized as a classifier:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: The target variable is called a  **label**.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The historical data is called  **labeled data**.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The production data, which the label needs to be predicted for, is called  **unlabeled
    data**.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to accurately label unlabeled data using a trained model is the
    real power of classification algorithms. Classifiers predict labels for unlabeled
    data to answer a particular business question.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Before we present the details of classification algorithms, let's first present
    a business problem that we will use as a challenge for classifiers. We will then
    use six different algorithms to answer the same challenge, which will help us
    compare their methodology, approach, and performance.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Presenting the classifiers challenge
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will first present a common problem, which we will use as a challenge to
    test six different classification algorithms. This common problem is referred
    to as the classifier challenge in this chapter. Using all the six classifiers
    to solve the same problem will help us in two ways:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先提出一个常见的问题，我们将使用它作为测试六种不同分类算法的挑战。这个常见的问题在本章中被称为分类器挑战。使用所有六种分类器来解决同一个问题将帮助我们以两种方式：
- en: All the input variables need to be processed and assembled as a complex data
    structure, called a feature vector. Using the same feature vector helps us avoid
    repeating data preparation for all six algorithms.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有输入变量都需要被处理和组装成一个复杂的数据结构，称为特征向量。使用相同的特征向量可以帮助我们避免为所有六个算法重复数据准备。
- en: We can compare the performance of various algorithms as we are using the same
    feature vector for input.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以通过使用相同的特征向量作为输入来比较各种算法的性能。
- en: The classifiers challenge is about predicting the likelihood of a person making
    a purchase. In the retail industry, one of the things that can help maximize sales
    is better understanding the behavior of the customers. This can be done by analyzing
    the patterns found in historical data. Let's state the problem, first.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器挑战是关于预测一个人购买的可能性。在零售行业，可以帮助最大化销售的一件事是更好地了解客户的行为。这可以通过分析历史数据中发现的模式来实现。让我们先阐述问题。
- en: The problem statement
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题陈述
- en: Given the historical data, can we train a binary classifier that can predict
    whether a particular user will eventually buy a product based on their profile?
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 根据历史数据，我们能否训练一个二元分类器，可以预测特定用户最终是否会购买产品？
- en: 'First, let''s explore the historical labeled data set available to solve this
    problem:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们探索可用于解决这个问题的历史标记数据集：
- en: x € ℜ^b, y € {0,1}
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: x € ℜ^b, y € {0,1}
- en: For a particular example, when *y* = 1, we call it a positive class and when
    *y* = 0, we call it a negative class.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 对于特定示例，当*y* = 1时，我们称之为正类，当*y* = 0时，我们称之为负类。
- en: Although the level of the positive and negative class can be chosen arbitrarily,
    it is good practice to define the positive class as the event of interest. If
    we are trying to flag the fraudulent transaction for a bank, then the positive
    class (that is, *y* = 1 ) should be the fraudulent transaction, not the other
    way around.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管正类和负类的级别可以任意选择，但定义正类为感兴趣的事件是一个好的做法。如果我们试图为银行标记欺诈交易，那么正类（即*y* = 1）应该是欺诈交易，而不是相反。
- en: 'Now, let''s look at the following:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下以下内容：
- en: The actual label, denoted by *y*
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实际标签，用*y*表示
- en: The predicted label, denoted by *y`*
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测的标签，用*y`*表示
- en: Note that for our classifiers challenge, the actual value of the label found
    in examples is represented by *y*. If, in our example, someone has purchased an
    item, we say *y* =1\. The predicted values are represented by *y`*. The input
    feature vector, *x*, has a dimension of 4\. We want to determine what the probability
    is that a user will make a purchase, given a particular input.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，对于我们的分类器挑战，示例中找到的标签的实际值由*y*表示。如果在我们的示例中，有人购买了一个物品，我们说*y* = 1。预测值由*y`*表示。输入特征向量*x*的维度为4。我们想确定用户在给定特定输入时购买的概率是多少。
- en: 'So, we want to determine the probability that *y* = 1 is, given a particular
    value of feature vector *x*. Mathematically, we can represent this as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们希望确定在给定特征向量*x*的特定值时*y* = 1的概率。从数学上讲，我们可以表示如下：
- en: '![](assets/64b062ae-9b4e-4cb4-b2f0-608d0ba569e0.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/64b062ae-9b4e-4cb4-b2f0-608d0ba569e0.png)'
- en: Now, let's look at how we can process and assemble different input variables
    in the feature vector, *x*. The methodology to assemble different parts of *x*
    using the processing pipeline is discussed in more detail in the following section.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何处理和组装特征向量*x*中的不同输入变量。在下一节中，将更详细地讨论使用处理管道组装*x*的不同部分的方法。
- en: Feature engineering using a data processing pipeline
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用数据处理管道进行特征工程
- en: Preparing data for a chosen machine learning algorithm is called **feature engineering**
    and is a crucial part of the machine learning life cycle. Feature engineering
    is done in different stages or phases. The multi-stage processing code used to
    process data is collectively known as a **data pipeline**. Making a data pipeline
    using standard processing steps, wherever possible, makes it reusable and decreases
    the effort needed to train the models. By using more well-tested software modules,
    the quality of the code is also enhanced.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 为了选择一个特定的机器学习算法的数据准备被称为**特征工程**，它是机器学习生命周期的一个关键部分。特征工程在不同的阶段或阶段进行。用于处理数据的多阶段处理代码被统称为**数据管道**。在可能的情况下使用标准处理步骤制作数据管道，使其可重用并减少训练模型所需的工作量。通过使用更多经过测试的软件模块，代码的质量也得到了提高。
- en: Let's see design a reusable processing pipeline for the classifiers challenge.
    As mentioned, we will prepare data once and then use it for all the classifiers.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为分类器挑战设计一个可重用的处理管道。如前所述，我们将准备数据一次，然后将其用于所有分类器。
- en: Importing data
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入数据
- en: 'The historical data for this problem is stored in a file called `dataset` in
    `.csv` format. We will use the `pd.read_csv` function from pandas to import the
    data as a data frame:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题的历史数据存储在一个名为`dataset`的文件中，格式为`.csv`。我们将使用pandas的`pd.read_csv`函数将数据导入为数据框：
- en: '[PRE0]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Feature selection
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征选择
- en: The process of selecting features that are relevant to the context of the problem
    that we want to solve is called **feature selection**. It is an essential part
    of feature engineering.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 选择与我们想要解决的问题相关的特征的过程称为**特征选择**。这是特征工程的一个重要部分。
- en: 'Once the file is imported, we drop the `User ID` column, which is used to identify
    a person and should be excluded when training a model:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦文件被导入，我们删除`User ID`列，该列用于识别一个人，并且在训练模型时应该被排除：
- en: '[PRE1]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now let''s preview the dataset:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们预览数据集：
- en: '[PRE2]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The dataset looks like this:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集如下：
- en: '![](assets/b0a76952-07bb-4025-9614-1e7e6d683bf0.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/b0a76952-07bb-4025-9614-1e7e6d683bf0.png)'
- en: Now, let's look at how we can further process the input dataset.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encoding
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Many machine learning algorithms require all the features to be continuous
    variables. It means that if some of the features are category variables, we need
    to find a strategy to convert them into continuous variables. One-hot encoding
    is one of the most effective ways of performing this transformation. For this
    particular problem, the only category variable we have is `Gender`. Let''s convert
    that into a continuous variable using one-hot encoding:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Once it''s converted, let''s look at the dataset again:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/6a3f7425-0e6d-444f-914c-7c12b15ada54.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
- en: Notice that in order to convert a variable from a category variable into a continuous
    variable, one-hot encoding has converted `Gender` into two separate columns—`Male`
    and `Female`.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Specifying the features and label
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s specify the features and labels. We will use `y` through this book to
    represent the label and `X` to represent the feature set:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`X` represents the feature vector and contains all the input variables that
    we need to use to train the model.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Dividing the dataset into testing and training portions
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s divide the training dataset into 25% testing and 75% training portions
    using `sklearn.model_selection import train_test_split`:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This has created the following four data structures:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '`X_train`: A data structure containing the features of the training data'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`X_test`: A data structure containing the features of the training test'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`y_train`: A vector containing the values of the label in the training dataset'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`y_test`: A vector containing the values of the label in the testing dataset'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling the features
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For many machine learning algorithms, it''s good practice to scale the variables
    from `0` to `1`. This is also called **feature normalization**. Let''s apply the
    scaling transformation to achieve this:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: After we scale the data, it is ready to be used as input to the different classifiers
    that we will present in the subsequent sections.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the classifiers
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once the model is trained, we need to evaluate its performance. To do that,
    we will use the following process:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: We will divide the labeling dataset into two parts—a training partition and
    a testing partition. We will use the testing partition to evaluate the trained
    model.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will use the features of our testing partition to generate labels for each
    row. This is our set of predicted labels.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will compare the set of predicted labels with the actual labels to evaluate
    the model.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unless we are trying to solve something quite trivial, there will be some misclassifications
    when we evaluate the model. How we interpret these misclassifications to determine
    the quality of the model depends on which performance metrics we choose to use.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Once we have both the set of actual labels and the predicted labels, a bunch
    of performance metrics can be used to evaluate the models. The best metric to
    quantify the model will depend on the requirements of the business problem that
    we want to solve, as well as the characteristics of the training dataset.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Confusion matrix
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A confusion matrix is used to summarize the results of the evaluation of a
    classifier. The confusion matrix for a binary classifier looks as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ca9d79e4-5e30-4bf9-b81f-d0aee047b206.png)If the label of the classifier
    we are training has two levels, it is called a **binary classifier**. The first
    critical use case of supervised machine learning—specifically, a binary classifier—was
    during the First World War to differentiate between an aircraft and flying birds.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: 'The classification can be divided into the following four categories:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '**True positives** (**TP**): The positive classifications that were correctly
    classified'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True Negatives** (**TN**): The negative classifications that were correctly
    classified'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Positives** (**FP**): The positive classifications  that were actually
    negative'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Negatives** (**FN**): The negative classifications that were actually
    positive'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阴性**（**FN**）：实际上是积极的负面分类'
- en: Let's see how we can use these four categories to create various performance
    metrics.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用这四个类别来创建各种性能指标。
- en: Performance metrics
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能指标
- en: 'Performance metrics are used to quantify the performance of the trained models.
    Based on this, let''s define the following four metrics:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 性能指标用于量化训练模型的性能。基于此，让我们定义以下四个指标：
- en: '| **Metric** | **Formula** |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| **指标** | **公式** |'
- en: '| Accuracy | ![](assets/6732024e-647e-40b8-8965-e0e10573db8d.png) |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | ![](assets/6732024e-647e-40b8-8965-e0e10573db8d.png) |'
- en: '| Recall | ![](assets/52d52b33-c954-4bc6-ac31-325849e8a6e7.png) |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 召回率 | ![](assets/52d52b33-c954-4bc6-ac31-325849e8a6e7.png) |'
- en: '| Precision | ![](assets/985538cb-41f1-4fcf-93a4-8da6a3fecf13.png) |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 精度 | ![](assets/985538cb-41f1-4fcf-93a4-8da6a3fecf13.png) |'
- en: '| F1 score | ![](assets/e50cf3cc-6542-4e86-82dc-39e65afcba92.png) |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| F1分数 | ![](assets/e50cf3cc-6542-4e86-82dc-39e65afcba92.png) |'
- en: Accuracy is the proportion of correction classifications among all predictions.
    While calculating accuracy, we do not differentiate between TP and TN. Evaluating
    a model through accuracy is straightforward, but in certain situations, it will
    not work.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率是所有预测中正确分类的比例。在计算准确率时，我们不区分TP和TN。通过准确率评估模型是直接的，但在某些情况下，它不起作用。
- en: 'Let''s look at the situations where we need more than accuracy to quantify
    the performance of a model. One of these situations is when we use a model to
    predict a rare event, such as in the following examples:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们需要更多的东西来量化模型的性能的情况。其中之一是当我们使用模型来预测罕见事件时，比如以下的例子：
- en: A model to predict the fraudulent transactions in a banks transactional database
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于预测银行交易数据库中欺诈交易的模型
- en: A model to predict the likelihood of mechanical failure of an engine part of
    an aircraft
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于预测飞机发动机零部件机械故障可能性的模型
- en: 'In both of these examples, we are trying to predict a rare event. Two additional
    measures become more important than accuracy in these situations—recall and precision.
    Let''s look at them one by one:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两个例子中，我们试图预测罕见事件。在这种情况下，比准确率更重要的是召回率和精度。让我们逐个来看：
- en: '**Recall**: This calculates the hit rate. In the first of the preceding examples,
    it is the proportion of fraudulent documents successfully flagged by the model
    out of all the fraudulent documents. If, in our testing dataset, we had 1 million
    transactions, out of which 100 were known to be fraudulent, the model was able
    to identify 78 of them. In this case, the recall value would be 78/100.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**召回率**：这计算了命中率。在前面的例子中，它是模型成功标记的欺诈文件占所有欺诈文件的比例。如果在我们的测试数据集中有100万笔交易，其中有100笔被确认为欺诈交易，模型能够识别出78笔。在这种情况下，召回率值将是78/100。'
- en: '**Precision**: The precision measures how many of the transactions flagged
    by the model were actually bad. Instead of focusing on the bad transactions that
    the model failed to flag, we want to determine how precise the bad bins flagged
    by the model really is.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精度**：精度衡量了模型标记的交易中实际上是坏的交易有多少。我们不是专注于模型未能标记的坏交易，而是想确定模型标记的坏交易有多精确。'
- en: Note that the F1 score brings both the recall and precision together. If a model
    has perfect scores for both precision and recall, then its F1 score will be perfect.
    A high F1 score means that we have trained a high-quality model that has high
    recall and precision.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，F1分数将召回率和精度结合在一起。如果一个模型的精度和召回率都是完美的，那么它的F1分数将是完美的。高F1分数意味着我们训练了一个高质量的模型，具有高召回率和精度。
- en: Understanding overfitting
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解过拟合
- en: If a machine learning model performs great in a development environment but
    degrades noticeably in a production environment, we say the model is overfitted.
    This means the trained model too closely follows the training dataset. It is an
    indication there are too many details in the rules created by the model. The trade-off
    between model variance and bias best captures the idea. Let's look at these concepts
    one by one.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个机器学习模型在开发环境中表现出色，但在生产环境中明显下降，我们说这个模型是过拟合的。这意味着训练模型过于密切地遵循训练数据集。这表明模型创建的规则中有太多细节。模型方差和偏差之间的权衡最能捕捉到这个概念。让我们逐个来看这些概念。
- en: Bias
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 偏差
- en: Any machine learning model is trained based on certain assumptions. In general,
    these assumptions are the simplistic approximations of some real-world phenomena.
    These assumptions simplify the actual relationships between features and their
    characteristics and make a model easier to train. More assumptions means more
    bias. So, while training a model, more simplistic assumptions = high bias, and
    realistic assumptions that are more representative of actual phenomena = low bias.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 任何机器学习模型都是基于某些假设进行训练的。一般来说，这些假设是对一些真实世界现象的简化近似。这些假设简化了特征和特征特性之间的实际关系，并使模型更容易训练。更多的假设意味着更多的偏差。因此，在训练模型时，更简化的假设=高偏差，更符合实际现象的现实假设=低偏差。
- en: In linear regression, the non-linearity of the features is ignored and they
    are approximated as linear variables. So, linear regression models are inherently
    vulnerable to exhibiting high bias.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性回归中，忽略了特征的非线性，并将它们近似为线性变量。因此，线性回归模型天生容易表现出高偏差。
- en: Variance
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方差
- en: Variance quantifies how accurately a model estimates the target variable if
    a different dataset is used to train the model. It quantifies whether the mathematical
    formulation of our model is a good generalization of the underlying patterns.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 方差量化了模型在使用不同数据集训练时对目标变量的估计准确性。它量化了我们的模型的数学公式是否是底层模式的良好概括。
- en: Specific overfitted rules based on specific scenarios and situations = high
    variance, and rules that are generalized and applicable to a variety of scenarios
    and situations = low variance.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 基于特定情景和情况的特定过拟合规则=高方差，而基于广泛情景和情况的泛化规则=低方差。
- en: Our goal in machine learning is to train models that exhibit low bias and low
    variance. Achieving this goal is not always easy and usually keeps data scientists
    awake at night.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Bias-variance trade-off
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When training a particular machine learning model, it is tricky to decide the
    right level of generalization for the rules that comprise a trained model. The
    struggle to come up with the right level of generalization is captured by the
    bias-variance trade-off.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Note that more simplistic assumptions = more generalization = low variance =
    high variance.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: This trade-off between bias and variance is determined by the choice of algorithm,
    the characteristics of the data, and various hyperparameters. It is important
    to achieve the right compromise between the bias and variance based on the requirements
    of the specific problem you are trying to solve.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Specifying the phases of classifiers
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once the labeled data is prepared, the development of the classifiers involves
    training, evaluation, and deployment. These three phases of implementing a classifier
    are shown in the **CRISP-DM**  (**Cross-Industry Standard Process for Data Mining**)
    life cycle in the following diagram (the CRISP-DM life cycle was explained in
    more detail in [Chapter 5](051e9b32-f15f-4e88-a63a-ae3c14696492.xhtml)*, Graph
    Algorithms*)
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/0d74d428-cad1-4af4-bf70-f6eecb3aba00.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
- en: In the first two phases of implementing a classifier—the testing and training
    phases—we use labeled data. The labeled data is divided into two partitions—a
    larger partition called the training data and a smaller partition called the testing
    data. A random sampling technique is used to divide the input labeled data into
    training and testing partitions to make sure that both partitions contain consistent
    patterns. Note that, as the preceding diagram shows, first, there is a training
    phase, where training data is used to train a model. Once the training phase is
    over, the trained model is evaluated using the testing data. Different performance
    matrices are used to quantify the performance of the trained model. Once the model
    is evaluated, we have the model deployment phase, where the trained model is deployed
    and used for inference to solve real-world problems by labeling unlabeled data.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's look at some classification algorithms.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'We will look at the following classification algorithms in the subsequent sections:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: The decision tree algorithm
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The XGBoost algorithm
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The random forest algorithm
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The logistic regression algorithm
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Support Vector Machine** (**SVM**) algorithm
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The naive Bayes algorithm
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start with the decision tree algorithm.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Decision tree classification algorithm
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A decision tree is based on the recursive partitioning approach (divide and
    conquer), which generates a set of rules that can be used to predict a label.
    It starts with a root node and splits into multiple branches. Internal nodes represent
    a test on a certain attribute and the result of the test is represented by a branch
    to the next level. The decision tree ends in leaf nodes that contain the decisions.
    The process stops when partitioning no longer improves the outcome.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the decision tree classification algorithm
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The distinguishing feature of decision tree classification is the generation
    of the human-interpretable hierarchy of rules that are used to predict the label
    at runtime. The algorithm is recursive in nature. Creating this hierarchy of rules
    involves the following steps:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '**Find the most important feature**:  Out of all of the features, the algorithm
    identifies the feature that best differentiates between the data points in the
    training dataset with respect to the label. The calculation is based on metrics
    such as information gain or Gini impurity.'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Bifurcate**: Using the most identified important feature, the algorithm creates
    a criterion that is used to divide the training dataset into two branches:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data points that pass the criterion
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data points that fail the criterion
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Check for leaf nodes**:  If any resultant branch mostly contains labels of
    one class, the branch is made final, resulting in a leaf node.'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Check the stopping conditions and repeat**: If the provided stopping conditions
    are not met, then the algorithm will go back to *step 1* for the next iteration.
    Otherwise, the model is marked as trained and each node of the resultant decision
    tree at the lowest level is labeled as a leaf node. The stopping condition can
    be as simple as defining the number of iterations, or the default stopping condition
    can be used, where the algorithm stops as soon it reaches a certain homogeneity
    level for each of the leaf nodes.'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The decision tree algorithm can be explained by the following diagram:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/53f88b67-009e-48f6-9389-b487fe7c0388.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, the root contains a bunch of circles and crosses.
    The algorithm creates a criterion that tries to separate the circles from the
    crosses. At each level, the decision tree creates partitions of the data, which
    are expected to be more and more homogeneous from level 1 upward. A perfect classifier
    has leaf nodes that only contain circles or crosses. Training perfect classifiers
    is usually difficult due to the inherent randomness of the training dataset.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Using the decision tree classification algorithm for the classifiers challenge
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s use the decision tree classification algorithm for the common problem
    that we previously defined to predict whether a customer ends up purchasing a
    product:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'To do, first, let''s instantiate the decision tree classification algorithm
    and train a model using the training portion of the data that we prepared for
    our classifiers:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, let''s use our trained model to predict the labels for the testing portion
    of our labeled data. Let''s generate a confusion matrix that can summarize the
    performance of our trained model:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This gives the following output:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/7588760f-69c4-4470-ade7-85255d5e67f7.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s calculate the `accuracy`, `recall`, and `precision` values for
    the created classifier by using the decision tree classification algorithm:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Running the preceding code will produce the following output:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/a7a7e80c-e144-4e7c-8c1b-98cc1cec5fef.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
- en: The performance measures help us compare different training modeling techniques
    with each other.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: The strengths and weaknesses of decision tree classifiers
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, let's look at the strengths and weaknesses of using the decision
    tree classification algorithm.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Strengths
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the strengths of decision tree classifiers:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: The rules of the models created by using a decision tree algorithm are interpretable
    by humans. Models such as this are called **whitebox models**. Whitebox models
    are a requirement whenever transparency is needed to trace the details and reasons
    for decisions that are made by the model. This transparency is essential in applications
    where we want to prevent bias and protect vulnerable communities. For example,
    a whitebox model is generally a requirement for critical use cases in government
    and insurance industries.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision tree classifiers are designed to extract information from discrete
    problem space. This means that most of the features are category variables, so
    using a decision tree to train the model is a good choice.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weaknesses
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the weaknesses of decision tree classifiers:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: If the tree generated by the decision tree classifier goes too deep, the rules
    capture too many details, resulting in an overfitted model. While using a decision
    tree algorithm, we need to be aware that decision trees are vulnerable to overfitting
    and so we need to prune the tree, whenever necessary, to prevent this.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A weakness of decision tree classifiers is their inability to capture non-linear
    relationships in the rules that they create.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use cases
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, let's look at the use cases that the decision tree algorithm
    is used for.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Classifying records
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Decision trees classifiers can be used to classify data points, such as in
    the following examples:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '**Mortgage applications**: To train a binary classifier to determine whether
    an applicant is likely to default.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Customer segmentation**: To categorize customers into high-worth, medium-worth,
    and low-worth customers so that marketing strategies can be customized for each
    category.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Medical diagnosis**: To train a classifier that can categorize a benign or
    malignant growth.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Treatment-effectiveness analysis**: To train a classifier that can flag patients
    that have reacted positively to a particular treatment.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature selection
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The decision tree classification algorithm selects a small subset of features
    to create rules for. That feature selection can be used to select the features
    for another machine learning algorithm when you have a large number of features.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the ensemble methods
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An ensemble is a method, in machine learning, of creating more than one slightly
    different model using different parameters and then combining them into an aggregate
    model. In order to create effective ensembles, we need to find what our aggregation
    criterion is to generate the resultant model. Let's look at some ensemble algorithms.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Implementing gradient boosting with the XGBoost algorithm
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: XGBoost was created in 2014 and is based on gradient-boosting principles. It
    has become one of the most popular ensemble classification algorithms. It generates
    a bunch of interrelated trees and uses gradient descent to minimize the residual
    error. This makes it a perfect fit for distributed infrastructures, such as Apache
    Spark, or for cloud computing, such as Google Cloud or  **Amazon Web Services**(**AWS**)**.**
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now see how we can implement gradient boosting with the XGBoost algorithm:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will instantiate the XGBClassfier classifier and train the model
    using the training portion of the data:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/27a8fe49-3854-4c12-84f2-e71c0f64e430.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
- en: 'Then, we will generate predictions based on the newly trained model:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The produces the following output :'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/3d7d4afa-c3ae-4ae3-a2d2-7ab69b67dcdd.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
- en: 'Finally, we will quantify the performance of the model:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This gives us the following output:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/beaac243-597f-4f5d-b1ec-022f89f01584.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
- en: Next, let's look at the random forest algorithm.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Using the random forest algorithm
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Random forest is a type of ensemble method that works by combining several decision
    trees to decrease both the bias and the variance.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Training a random forest algorithm
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In training, this algorithm takes *N* samples from the training data and creates
    *m* subsets of our overall data. These subsets are created by randomly selecting
    some of the rows and columns of the input data. The algorithm builds *m* independent
    decision trees. These classification trees are represented by `C[1]` to `C[m]`.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Using random forest for predictions
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once the model is trained, it can be used to label new data. Each of the individual
    trees generates a label. The final prediction is determined by voting these individual
    predictions, as shown:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/89d3a5b6-24cc-4fbd-b922-90865ca6b739.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
- en: Note that in the preceding diagram, *m* trees are trained, which is represented
    by `C[1]` to `C[m]`. That is Trees = {C[1],..,C[m]}
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'Each of the trees generates a prediction that is represented by a set:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Individual predictions = P= {P[1],..., P[m]}
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: 'The final prediction is represented by `P[f]`. It is determined by the majority
    of the individual predictions. The `mode` function can be used to find the majority
    decision (`mode` is the number that repeats most often and is in the majority).
    The individual prediction and the final prediction are linked, as follows:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: P[f] = mode (P)
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Differentiating the random forest algorithm from ensemble boosting
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Each of the trees generated by the random forest algorithm is totally independent
    of each other. It is not aware of any of the details of the other trees in the
    ensemble. This differentiates it from other techniques, such as ensemble boosting.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Using the random forest algorithm for the classifiers challenge
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's instantiate the random forest algorithm and use it to train our model
    using the training data.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two key hyperparameters that we''ll be looking at here:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '`n_estimators`'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_depth`'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `n_estimators` hyperparameter controls how many individual decision trees
    are built and the `max_depth` hyperparameter controls how deep each of these individual
    decision trees can go.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'So, in other words, a decision tree can keep splitting and splitting until
    it has a node that represents every given example in the training set. By setting
    `max_depth`, we constrain how many levels of splits it can make. This controls
    the complexity of the model and determines how closely it fits the training data.
    If we refer to the following output, `n_estimators` controls the width of the
    random forest model and `max_depth` controls the depth of the model:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/d4ee0995-7c13-4508-bf5c-5fa23929a1be.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
- en: 'Once the random forest model is trained, let''s use it for predictions:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Which gives the output as:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/374d8a18-c24f-44c3-a475-e19c54bc4329.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s quantify how good our model is:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We will observe the following output:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/ac3d4ce9-a599-4064-a93f-4742874af7b4.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
- en: Next, let's look into logistic regression.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression
  id: totrans-279
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logistic regression is a classification algorithm used for binary classification.
    It uses a logistic function to formulate the interaction between the input features
    and the target variable. It is one of the simplest classification techniques that
    is used to model a binary dependent variable.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: Assumptions
  id: totrans-281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Logistic regression assumes the following:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: The training dataset does not have a missing value.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The label is a binary category variable.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The label is ordinal—in other words, a categorical variable with ordered values.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All features or input variables are independent of each other.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Establishing the relationship
  id: totrans-287
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For logistic regression, the predicted value is calculated as follows:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/57472f0a-fed2-46a3-8f85-82a58abe001c.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
- en: Let's suppose that ![](assets/57ba1e44-a466-4007-99ec-5e7c03a7d299.png).
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: 'So now:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/5a9b351d-1683-44cf-8324-c8897ee20680.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
- en: 'The preceding relationship can be graphically shown as follows:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/c4c1e51c-a610-4fb4-8c21-ba38fed82d20.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
- en: Note that if *z* is large, σ (*z*) will equal `1`. If *z* is very small or a
    large negative number, σ (z) will equal `0`. So, the objective of logistic regression
    is to find the correct values for *w* and *j*.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression is named after the function that is used to formulate it,
    called the **logistic** or **sigmoid function**.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: The loss and cost functions
  id: totrans-297
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `loss` function defines how we want to quantify an error for a particular
    example in our training data. The `cost` function defines how we want to minimize
    an error in our entire training dataset. So, the `loss` function is used for one
    of the examples in the training dataset and the `cost` function is used for the
    overall cost that quantifies the overall deviation of the actual and predicted
    values. It is dependent on the choice of *w* and *h*.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: 'The `loss` function used in logistic regression is as follows:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '*Loss (ý^((i)), y^((i))) = - (y^((i))log ý^((i))+(1-y^((i)) ) log (1-ý^((i)))*'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: Note that when  *y^((i))  = 1, Loss(ý^((i)), y^((i))**) = - logý^((i))*.Minimizing
    the loss will result in a large value of ý^((i)) . Being a sigmoid function, the
    maximum value will be `1`.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: If *y^((i)) = 0, Loss (ý^((i)), y^((i))) = - log (1-ý^((i))**)*. Minimizing
    the loss will result in *ý^((i))* being as small as possible, which is `0`.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: 'The cost function of logistic regression is as follows:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/41a6378a-1b02-4912-8fcd-0279b7e7fc45.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
- en: When to use logistic regression
  id: totrans-305
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logistic regression works great for binary classifiers. Logistic regression
    doesn't do very well when the data is huge but the quality of the data is not
    great. It can capture relationships that aren't too complex. While it doesn't
    usually generate the greatest performance, it does set a very nice benchmark to
    start.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: Using the logistic regression algorithm for the classifiers challenge
  id: totrans-307
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will see how we can use the logistic regression algorithm
    for the classifiers challenge:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s instantiate a logistic regression model and train it using the
    training data:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let''s predict the values of the `test` data and create a confusion matrix:'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We get the following output upon running the preceding code:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/e6d66de7-957c-43d0-83a6-bc2b2f334b6d.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s look at the performance metrics:'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We get the following output upon running the preceding code:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/31737825-b84d-423e-8416-c6885f175eb1.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
- en: Next, let's look at **SVM**.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: The SVM algorithm
  id: totrans-320
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s look at SVM. SVM is a classifier that finds an optimal hyperplane
    that maximizes the margin between two classes. In SVMs, our optimization objective
    is to maximize the margin. The margin is defined as the distance between the separating
    hyperplane (the decision boundary) and the training samples that are closest to
    this hyperplane, called the **support vectors***.* So, let''s start with a very
    basic example with only two dimensions, *X1* and *X2*. We want a line to separate
    the circles from the crosses. This is shown in the following diagram:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/9830448e-e681-400a-9017-57888b5df6da.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
- en: 'We have drawn two lines and both perfectly separate the crosses from the circles.
    However, there has to be an optimal line, or decision boundary, that gives us
    the best chance to correctly classify most of the additional examples. A reasonable
    choice may be a line that is evenly spaced between these two classes to give a
    little bit of a buffer for each class, as shown:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/710cb98c-c09e-42bb-a173-1e392b0f8d5b.png)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
- en: Now, let's see how we can use SVM to train a classifier for our challenge.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: Using the SVM algorithm for the classifiers challenge
  id: totrans-326
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, let''s instantiate the SVM classifier and then use the training portion
    of the labeled data to train it. The `kernel` hyperparameter determines the type
    of transformation that is applied to the input data in order to make it linearly
    separable.:'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Once trained, let''s generate some predictions and look at the confusion matrix:'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Observe the following output:'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/9e0d3b4b-2ba9-48ee-8999-692c32d334b5.png)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s look at the various performance metrics:'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'After running the preceding code, we get the following values as our output:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/64c827cb-bf53-453d-96f1-cb1d34c01db9.png)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
- en: Understanding the naive Bayes algorithm
  id: totrans-337
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Based on probability theory, naive Bayes is one of the simplest classification
    algorithms. If used properly, it can come up with accurate predictions. The Naive
    Bayes Algorithm is s0-named for two reasons:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: It is based on a naive assumption that there is independence between the features
    and the input variable.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is based on Bayes, theorem.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This algorithm tries to classify instances based on the probabilities of the
    preceding attributes/instances, assuming complete attribute independence.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three types of events:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: '**Independent** events do not affect the probability of another event occurring
    (for example, receiving an email offering you free entry to a tech event *and*
    a re-organization occurring in your company).'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dependent** events affect the probability of another event occurring; that
    is, they are linked in some way (for example, the probability of you getting to
    a conference on time could be affected by an airline staff strike or flights that
    may not run on time).'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mutually exclusive** events cannot occur simultaneously (for example, the
    probability of rolling a three and a six on a single dice roll is 0—these two
    outcomes are mutually exclusive).'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayes, theorem
  id: totrans-346
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Bayes, theorem is used to calculate the conditional probability between two
    independent events, *A* and *B*. The probability of events *A* and *B* happening  is
    represented by P(*A*) and P(*B*). The conditional probability is represented by
    P(*B*|*A*), which is the conditional probability that event *B* will happen given
    that event *A* has occurred:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/c3857335-8ad7-48cb-bc9d-d0a1a132aebf.png)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
- en: Calculating probabilities
  id: totrans-349
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Naive Bayes is based on probability fundamentals. The probability of a single
    event occurring (the observational probability) is calculated by taking the number
    of times the event occurred and dividing it by the total number of processes that
    could have led to that event. For example, a call center receives over 100 support
    calls per day, 50 times over the course of a month. You want to know the probability
    that a call is responded to in under 3 minutes based on the previous times it
    was responded to. If the call center manages to match this time record on 27 occasions,
    then the observational probability of 100 calls being answered in under 3 minutes
    is as follows:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: '*P(100 support calls in under 3 mins) = (27 / 50) = 0.54 (54%)*'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: 100 calls can be responded to in under 3 minutes in about half the time, based
    on records of the 50 times it occurred in the past.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: Multiplication rules for AND events
  id: totrans-353
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To calculate the probability of two or more events occurring simultaneously,
    consider whether events are independent or dependent. If they are independent,
    the simple multiplication rule is used:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: '*P(outcome 1 AND outcome 2) = P(outcome 1) * P(outcome 2)*'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: For example, to calculate the probability of receiving an email with free entry
    to a tech event  *and* re-organization occurring in your workplace, this simple
    multiplication rule would be used. The two events are independent as the occurrence
    of one does not affect the chance of the other occurring
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: 'If receiving the tech event email has a probability of 31% and the probability
    of staff re-organization is 82%, then the probability of both occurring is calculated
    as follows:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: P(email AND re-organization) = P(email) * P(re-organization) = (0.31) * (0.82)
    = 0.2542 (25%)
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: The general multiplication rule
  id: totrans-359
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If two or more events are dependent, the general multiplication rule is used.
    This formula is actually valid in both cases of independent and dependent events:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: '*P(outcome 1 AND outcome 2)=P(outcome 1)*P(outcome 2 | outcome 1)*'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: Note that `P(outcome 2 | outcome 1)` refers to the conditional probability of
    `outcome 2` occurring given `outcome 1` has already occurred. The formula incorporates
    the dependence between the events. If the events are independent, then the conditional
    probability is irrelevant as one outcome does not influence the chance of the
    other occurring, and `P(outcome 2 | outcome 1)` is simply `P(outcome 2)`. Note
    that the formula in this case just becomes the simple multiplication rule.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: Addition rules for OR events
  id: totrans-363
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When calculating the probability of either one event or the other occurring
    (mutually exclusive), the following simple addition rule is used:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: '*P(outcome 1 OR outcome 2) = P(outcome 1) + P(outcome 2)*'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, what is the probability of rolling a 6 or a 3? To answer this
    question, first, note that both outcomes cannot occur simultaneously. The probability
    of rolling a 6 is (1 / 6) and the same can be said for rolling a 3:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: '*P(6 OR 3) = (1 / 6) + (1 / 6) = 0.33 (33%)*'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: 'If the events are not mutually exclusive and can occur simultaneously, use
    the following general addition formula, which is always valid in both cases of
    mutual exclusiveness and of non-mutual exclusiveness:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: '*P(outcome 1 OR outcome 2) = P(outcome 1) + P(outcome 2) P(outcome 1 AND outcome
    2)*'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: Using the naive Bayes algorithm for the classifiers challenge
  id: totrans-370
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s use the naive Bayes algorithm to solve the classifiers challenge:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the `GaussianNB()` function and use it to train the model:'
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, let''s use the trained model to predict the results. We will use it to
    predict the labels for our test partition, which is `X_test`:'
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, let''s print the confusion matrix:'
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/61ad873a-a502-41be-8d3b-433d21072c96.png)'
  id: totrans-377
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s print the performance matrices to quantify the quality of our trained
    model:'
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Which gives the output as:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/7cc3ecd6-99b2-426d-9e47-d434e60e3d13.png)'
  id: totrans-381
  prefs: []
  type: TYPE_IMG
- en: For classification algorithms, the winner is...
  id: totrans-382
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at the performance metrics of the various algorithms we have presented.
    This is summarized in the following table:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: '| **Algorithm** | **Accuracy** | **Recall** | **Precision** |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
- en: '| Decision tree | 0.94 | 0.93 | 0.88 |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
- en: '| XGBoost | 0.93 | 0.90 | 0.87 |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
- en: '| Random forest | 0.93 | 0.90 | 0.87 |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
- en: '| Logistic regression | 0.91 | 0.81 | 0.89 |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
- en: '| SVM | 0.89 | 0.71 | 0.92 |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
- en: '| Naive Bayes | 0.92 | 0.81 | 0.92 |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
- en: Looking at the preceding table, we can observe that the decision tree classifier
    performs the best in terms of accuracy and recall. If we are looking for precision,
    then there is a tie between SVM and naive Bayes, so either one will work for us.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: Understanding regression algorithms
  id: totrans-392
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The supervised machine learning model uses one of the regression algorithms
    if the target variable is a continuous variable. In this case, the machine learning
    model is called a regressor.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will present various algorithms that can be used to train
    a supervised machine learning regression model—or simply, regressors. Before we
    go into the details of the algorithms, let's first create a challenge for these
    algorithms to test their performance, abilities, and effectiveness on.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: Presenting the regressors challenge
  id: totrans-395
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Similar to the approach that we used with the classification algorithms, we
    will first present a problem to be solved as a challenge for all regression algorithms.
    We will call this common problem as the regressors challenge. Then, we will use
    three different regression algorithms to address the challenge. This approach
    of using a common challenge for different regression algorithms has two benefits:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: We can prepare the data once and used the prepared data on all three regression
    algorithms.
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can compare the performance of three regression algorithms in a meaningful
    way as we will use them to solve the same problem.
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look at the problem statement of the challenge.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: The problem statement of the regressors challenge
  id: totrans-400
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Predicting the mileage of different vehicles is important these days. An efficient
    vehicle is good for the environment and is also cost-effective. The mileage can
    be estimated from the power of the engine and the characteristics of the vehicle.
    Let's create a challenge for regressors to train a model that can predict the
    **Miles Per Gallon** (**MPG**) of a vehicle based on its characteristics.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at the historical dataset that we will use to train the regressors.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the historical dataset
  id: totrans-403
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the features of the historical dataset data that we have:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: '| **Name** | **Type** | **Description** |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
- en: '| `NAME` | Category | Identifies a particular vehicle |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
- en: '| `CYLINDERS` | Continuous | The number of cylinders (between 4 and 8) |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
- en: '| `DISPLACEMENT` | Continuous | The displacement of the engine in cubic.inches
    |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
- en: '| `HORSEPOWER` | Continuous | The horsepower of the engine |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
- en: '| `ACCELERATION` | Continuous | The time it takes to accelerate from 0 to 60
    mph (in seconds) |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
- en: The target variable for this problem is a continuous variable, `MPG`, that specifies
    the mpg for each of the vehicles.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: Let's first design the data processing pipeline for this problem.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering using a data processing pipeline
  id: totrans-413
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how we can design a reusable processing pipeline to address the
    regressors challenge. As mentioned, we will prepare the data once and then use
    it in all the regression algorithms. Let''s follow these steps:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by importing the dataset, as follows:'
  id: totrans-415
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let''s now preview the dataset:'
  id: totrans-417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This is how the dataset will look:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/c62f1a01-274e-4d93-b005-cd688ca4d630.png)'
  id: totrans-420
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s proceed on to feature selection. Let''s drop the `NAME` column
    as it is only an identifier that is needed for cars. Columns that are used to
    identify the rows in our dataset are not relevant for training the model. Let''s
    drop this column:'
  id: totrans-421
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Let''s convert all of the input variables and impute all the null values:'
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Imputation improves the quality of the data and prepares it to be used to train
    the model. Now, let''s see the final step:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s divide the data into testing and training partitions:'
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-427
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This has created the following four data structures:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: '`X_train`: A data structure containing the features of the training data'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`X_test`: A data structure containing the features of the training test'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`y_train`: A vector containing the values of the label in the training dataset'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`y_test`: A vector containing the values of the label in the testing dataset'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's use the prepared data on three different regressors so that we can
    compare their performance.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression
  id: totrans-434
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Of all the supervised machine learning techniques, the linear regression algorithm
    is the easiest one to understand. We will first look at simple linear regression
    and then we will expand the concept to multiple linear regression.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: Simple linear regression
  id: totrans-436
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In its simplest form, linear regression formulates the relationship between
    a single continuous independent variable and a single continuous independent variable.
    A (simple) regression is used to show the extent that changes in a dependent variable
    (shown on the *y*-axis) can be attributed to changes in an explanatory variable
    (shown on the *x*-axis). It can be represented as follows:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/a1ca60f3-867b-410e-b01f-f8f00c083695.png)'
  id: totrans-438
  prefs: []
  type: TYPE_IMG
- en: 'This formula can be explained as follows:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: '*y* is the dependent variable.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*X* is the independent variable.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](assets/4dc02d52-61ac-4dc5-ade3-3868a299cd92.png) is the slope that indicates
    how much the line rises for each increase in *X*.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*α* is the intercept that indicates the value of *y* when *X* = 0.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some examples of relationships between a single continuous dependent variable
    and a single continuous independent variable are as follows:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: A person's weight and their calories intake
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The price of a house and its area in square feet in a particular neighborhood
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The humidity in the air and the likelihood of rain
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For linear regression, both the input (independent) variable and the target
    (dependent) variable must be numeric. The best relationship is found by minimizing
    the sum of the squares of the vertical distances of each point from a line drawn
    through all the points. It is assumed that the relationship is linear between
    the predictor variable and the target variable. For example, the more money invested
    in research and development, the higher the sales.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at a specific example. Let''s try to formulate the relationship
    between marketing expenditures and sales for a particular product. They are found
    to be directly relational to each other. The marketing expenditures and sales
    are drawn on a two-dimensional graph and are shown as blue diamonds. The relationship
    can best be approximated by drawing a straight line, as in the following graph:'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/4308ca96-fec7-4de9-96f0-f700a67e5f77.png)'
  id: totrans-450
  prefs: []
  type: TYPE_IMG
- en: Once the linear line is drawn, we can see the mathematical relationship between
    the marketing expenditure and sales.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the regressors
  id: totrans-452
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The linear line that we drew is an approximation of the relationship between
    the dependent and independent variables. Even the best line will have some deviation
    from the actual values, as shown:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/71d38f55-1568-4a86-ba02-4681a035c983.png)'
  id: totrans-454
  prefs: []
  type: TYPE_IMG
- en: 'A typical way of quantifying the performance of linear regression models is
    by using **Root Mean Square Error** (**RMSE**). This calculates the standard deviation
    of the errors made by the trained model mathematically. For a certain example
    in the training dataset, the `loss` function is calculated as follows:'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: Loss (ý^((i)), y^((i))) = 1/2(ý^((i)-) y^((i)))²
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: 'This leads to the following `cost` function, which minimizes the loss of all
    of the examples in the training set:'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/25cd8116-2626-4182-a7c5-fb22681dd6b4.png)'
  id: totrans-458
  prefs: []
  type: TYPE_IMG
- en: Let's try to interpret RMSE. If RMSE is $50 for our example model that predicts
    the price of a product, this means that around 68.2% of the predictions will fall
    within $50 of the true value (that is, *α*). It also means that 95% of the predictions
    will fall within $100 (that is, 2*α*) of the actual value. Finally, 99.7% of the
    predictions will fall within $150 of the actual value.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: Multiple regression
  id: totrans-460
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The fact is that most real-world analyses have more than one independent variable.  Multiple
    regression is an extension of simple linear regression. The key difference is
    that there are additional beta coefficients for the additional predictor variables.
    When training a model, the goal is to find the beta coefficients that minimize
    the errors of the linear equation. Let's try to mathematically formulate the relationship
    between the dependent variable and the set of independent variables (features).
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to a simple linear equation, the dependent variable, *y*, is quantified
    as the sum of an intercept term plus the product of the *β* coefficients multiplied
    by the *x* value for each of the *i* features:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: y = α + β  [1]  x  [1]  + β  [2]  x 2 +...+ β  [i]  x  [i]  + ε
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: The error is represented by *ε* and indicates that the predictions are not perfect.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: The *β* coefficients allow each feature to have a separate estimated effect
    on the value of *y* because of  *y* changes by an amount of *β  [i]*  for each
    unit increase in *x*[*i*.] Moreover, the intercept (*α*) indicates the expected
    value of *y* when the independent variables are all 0.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
- en: Note that all the variables in the preceding equation can be represented by
    a bunch of vectors. The target and predictor variables are now vectors with a
    row and the regression coefficients, *β*, and errors, *ε*, are also vectors.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
- en: Using the linear regression algorithm for the regressors challenge
  id: totrans-467
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s train the model using the training portion of the dataset:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by importing the linear regression package:'
  id: totrans-469
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-470
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Then, let''s instantiate the linear regression model and train it using the
    training dataset:'
  id: totrans-471
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-472
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now, let''s predict the results using the test portion of the dataset:'
  id: totrans-473
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-474
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The output generated by running the preceding code will generate the following:'
  id: totrans-475
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/10b80cc4-475a-4fc7-9c53-ab9cad02c6e7.png)'
  id: totrans-476
  prefs: []
  type: TYPE_IMG
- en: As discussed in the preceding section, RMSE is the standard deviation of the
    error. It indicates that 68.2% of predictions will fall within `4.36` of the value
    of the target variable.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: When is linear regression used?
  id: totrans-478
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Linear regression is used to solve many real-world problems, including the
    following:'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: Sales forecasting
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting optimum product prices
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantifying the causal relationship between an event and the response, such
    as in clinical drug trials, engineering safety tests, or marketing research
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying patterns that can be used to forecast future behavior, given known
    criteria—for example, predicting insurance claims, natural disaster damage, election
    results, and crime rates
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The weaknesses of linear regression
  id: totrans-484
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The weaknesses of linear regression are as follows:'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: It only works with numerical features.
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Categorical data needs to be preprocessed.
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It does not cope well with missing data.
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It makes assumptions about the data.
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The regression tree algorithm
  id: totrans-490
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The regression tree algorithm is similar to the classification tree algorithm,
    except the target variable is a continuous variable, not a category variable.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
- en: Using the regression tree algorithm for the regressors challenge
  id: totrans-492
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will see how a regression tree algorithm can be used for
    the regressors challenge:'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we train the model using a regression tree algorithm:'
  id: totrans-494
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/2e89f7e5-0fd7-4b78-82ee-d60ba4e2b15d.png)'
  id: totrans-495
  prefs: []
  type: TYPE_IMG
- en: 'Once the regression tree model is trained, we use the trained model to predict
    the values:'
  id: totrans-496
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-497
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Then, we calculate RMSE to quantify the performance of the model:'
  id: totrans-498
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-499
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We get the following output:'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/c5716e34-aa3b-4b39-b5b2-88b8ab4419c0.png)'
  id: totrans-501
  prefs: []
  type: TYPE_IMG
- en: The gradient boost regression algorithm
  id: totrans-502
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's now look at the gradient boost regression algorithm. It uses an ensemble
    of decision trees in an effort to better formulate the underlying patterns in
    data.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
- en: Using gradient boost regression algorithm for the regressors challenge
  id: totrans-504
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will see how we can use the gradient boost regression algorithm
    for the regressors challenge:'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we train the model using the gradient boost regression algorithm:'
  id: totrans-506
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/c59155c9-6344-4272-b33e-826d814ed78f.png)'
  id: totrans-507
  prefs: []
  type: TYPE_IMG
- en: 'Once the gradient regression algorithm model is trained, we use it to predict
    the values:'
  id: totrans-508
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-509
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Finally, we calculate RMSE to quantify the performance of the model:'
  id: totrans-510
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-511
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Running this will give us the output value, as follows:'
  id: totrans-512
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/bd5c2be4-d453-4b08-90be-dbe55b9e1dc9.png)'
  id: totrans-513
  prefs: []
  type: TYPE_IMG
- en: For regression algorithms, the winner is...
  id: totrans-514
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at the performance of the three regression algorithms that we used
    on the same data and exactly the same use case:'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
- en: '| **Algorithm** | **RMSE** |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
- en: '| Linear regression | 4.36214129677179 |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
- en: '| Regression tree | 5.2771702288377 |'
  id: totrans-518
  prefs: []
  type: TYPE_TB
- en: '| Gradient boost regression | 4.034836373089085 |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
- en: Looking at the performance of all the regression algorithms, it is obvious that
    the performance of gradient boost regression is the best as it has the lowest
    RMSE. This is followed by linear regression. The regression tree algorithm performed
    the worst for this problem.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
- en: Practical example – how to predict the weather
  id: totrans-521
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's see how we can use the concepts developed in this chapter to predict the
    weather. Let's assume that we want to predict whether it will rain tomorrow based
    on the data collected over a year for a particular city.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
- en: 'The data available to train this model is in the CSV file called `weather.csv`:'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s import the data as a pandas data frame:'
  id: totrans-524
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-525
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Let''s look at the columns of the data frame:'
  id: totrans-526
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/cc8c37ff-f4b0-42ab-8a02-2d6901bbd040.png)'
  id: totrans-527
  prefs: []
  type: TYPE_IMG
- en: 'Next, let''s look at the header of the first 13 columns of the `weather.csv`
    data:'
  id: totrans-528
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/5c5f2025-72f7-413f-b7a2-42b3fcfa249b.png)'
  id: totrans-529
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s look at the last 10 columns of the `weather.csv` data:'
  id: totrans-530
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/825cb777-bb57-4caf-ab05-15d8ab625241.png)'
  id: totrans-531
  prefs: []
  type: TYPE_IMG
- en: 'Let''s use `x` to represent the input features. We will drop the `Date` field
    for the feature list as it is not useful in the context of predictions. We will
    also drop the `RainTomorrow` label:'
  id: totrans-532
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-533
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Let''s use `y` to represent the label:'
  id: totrans-534
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-535
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now, let''s divide the data into `train_test_split`:'
  id: totrans-536
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-537
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'As the label is a binary variable, we are training a classifier. So, logistic
    regression will be a good choice here. First, let''s instantiate the logistic
    regression model:'
  id: totrans-538
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-539
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Now, we can use `train_x` and `test_x` to train the model:'
  id: totrans-540
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-541
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Once the model is trained, let''s use it for predictions:'
  id: totrans-542
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-543
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now, let''s find the accuracy of our trained model:'
  id: totrans-544
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](assets/baa59ee9-e705-4a2c-9824-b92d37b0b333.png)'
  id: totrans-545
  prefs: []
  type: TYPE_IMG
- en: Now, this binary classifier can be used to predict whether it will rain tomorrow.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-547
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we started by looking at the basics of supervised machine learning.
    Then, we looked at various classification algorithms  in more detail. Next, we
    looked at different methods to evaluate the performance of classifiers and studied
    various regression algorithms. We also looked at the different methods that can
    be used to evaluate the performance of the algorithms that we studied.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at neural networks and deep learning algorithms.
    We will look at the methods used to train a neural network and we will also look
    at the various tools and frameworks available for evaluating and deploying a neural
    network.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
