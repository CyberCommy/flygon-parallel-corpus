- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ML4T Workflow – From Model to Strategy Backtesting
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, it's time to **integrate the various building blocks** of the **machine
    learning for trading** (**ML4T**) workflow that we have so far discussed separately.
    The goal of this chapter is to present an end-to-end perspective of the process
    of designing, simulating, and evaluating a trading strategy driven by an ML algorithm.
    To this end, we will demonstrate in more detail how to backtest an ML-driven strategy
    in a historical market context using the Python libraries backtrader and Zipline.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'The **ultimate objective of the ML4T workflow** is to gather evidence from
    historical data. This helps us decide whether to deploy a candidate strategy in
    a live market and put financial resources at risk. This process builds on the
    skills you developed in the previous chapters because it relies on your ability
    to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Work with a diverse set of data sources to engineer informative factors
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Design ML models that generate predictive signals to inform your trading strategy
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimize the resulting portfolio from a risk-return perspective
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A realistic simulation of your strategy also needs to faithfully represent how
    security markets operate and how trades are executed. Therefore, the institutional
    details of exchanges, such as which order types are available and how prices are
    determined, also matter when you design a backtest or evaluate whether a backtesting
    engine includes the requisite features for accurate performance measurements.
    Finally, there are several methodological aspects that require attention to avoid
    biased results and false discoveries that will lead to poor investment decisions.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 'More specifically, after working through this chapter, you will be able to:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Plan and implement end-to-end strategy backtesting
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand and avoid critical pitfalls when implementing backtests
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discuss the advantages and disadvantages of vectorized versus event-driven backtesting
    engines
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify and evaluate the key components of an event-driven backtester
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Design and execute the ML4T workflow using data sources at both minute and daily
    frequencies, with ML models trained separately or as part of the backtest
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use Zipline and backtrader
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find the code samples for this chapter and links to additional resources
    in the corresponding directory of the GitHub repository. The notebooks include
    color versions of the images.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: How to backtest an ML-driven strategy
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a nutshell, the ML4T workflow, illustrated in *Figure 8.1*, is about backtesting
    a trading strategy that leverages machine learning to generate trading signals,
    select and size positions, or optimize the execution of trades. It involves the
    following steps, with a specific investment universe and horizon in mind:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Source and prepare market, fundamental, and alternative data
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Engineer predictive alpha factors and features
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Design, tune, and evaluate ML models to generate trading signals
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decide on trades based on these signals, for example, by applying rules
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Size individual positions in the portfolio context
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Simulate the resulting trades triggered using historical market data
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate how the resulting positions would have performed
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B15439_08_01.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.1: The ML4T workflow'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: When we discussed the ML process in *Chapter 6*, *The Machine Learning Process*,
    we emphasized that the model's learning should generalize well to new applications.
    In other words, the predictions of an ML model trained on a given set of data
    should perform equally well when provided new input data. Similarly, the (relative)
    **backtest performance of a strategy should be indicative of future market performance**.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Before we take a look at how backtesting engines run historical simulations,
    we need to review several methodological challenges. Failing to properly address
    them will render results unreliable and lead to poor decisions about the strategy's
    live implementation.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Backtesting pitfalls and how to avoid them
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Backtesting simulates an algorithmic strategy based on historical data, with
    the goal of producing performance results that generalize to new market conditions.
    In addition to the generic uncertainty around predictions in the context of ever-changing
    markets, several implementation aspects can bias the results and increase the
    risk of mistaking in-sample performance for patterns that will hold out-of-sample.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 回测模拟基于历史数据的算法策略，旨在产生适用于新市场条件的绩效结果。除了在不断变化的市场环境中对预测的一般不确定性外，几个实施方面可能会使结果产生偏差，并增加误将样本内绩效误认为将在样本外持续的风险。
- en: These aspects are under our control and include the selection and preparation
    of data, unrealistic assumptions about the trading environment, and the flawed
    application and interpretation of statistical tests. The risks of false backtest
    discoveries multiply with increasing computing power, bigger datasets, and more
    complex algorithms that facilitate the misidentification of apparent signals in
    a noisy sample.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方面在我们的控制范围内，包括数据的选择和准备、对交易环境的不切实际假设以及对统计测试的错误应用和解释。随着计算能力的增加、数据集的扩大以及更复杂的算法，使得在嘈杂的样本中误识别明显信号的风险增加，错误的回测发现也会增加。
- en: In this section, we will outline the most serious and common methodological
    mistakes. Please refer to the literature on multiple testing for further detail,
    in particular, a series of articles by Marcos Lopez de Prado collected in *Advances
    in Financial Machine Learning (2018)*. We will also introduce the deflated **Sharpe
    ratio** (**SR**), which illustrates how to adjust metrics that result from repeated
    trials when using the same set of financial data for your analysis.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将概述最严重和常见的方法论错误。请参考有关多重检验的文献以获取更多细节，特别是Marcos Lopez de Prado在《金融机器学习进展（2018）》中收集的一系列文章。我们还将介绍通货紧缩的夏普比率（SR），以说明如何调整使用相同一组金融数据进行分析时产生的重复试验的指标。
- en: Getting the data right
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取正确的数据
- en: Data issues that undermine the validity of a backtest include **look-ahead bias**,
    **survivorship bias**, **outlier control**, as well as the **selection of the
    sample period**. We will address each of these in turn.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 破坏回测有效性的数据问题包括**前瞻偏差**、**生存偏差**、**异常值控制**以及**样本期的选择**。我们将依次解决这些问题。
- en: Look-ahead bias – use only point-in-time data
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 前瞻偏差-仅使用时点数据
- en: At the heart of an algorithmic strategy are trading rules that trigger actions
    based on data. Look-ahead bias emerges when we develop or evaluate trading rules
    **using historical information before it was known or available**. The resulting
    performance measures will be misleading and not representative of the future when
    data availability differs during live strategy execution.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 算法策略的核心是基于数据触发操作的交易规则。当我们使用**先前已知或可用的历史信息**开发或评估交易规则时，就会出现前瞻偏差。由此产生的绩效指标将是误导性的，不代表未来，当实时策略执行期间数据可用性不同时。
- en: A common cause of this bias is the failure to account for corrections or restatements
    of reported financials after their initial publication. Stock splits or reverse
    splits can also generate look-ahead bias. For example, when computing the earnings
    yield, **earnings-per-share** (**EPS**) data is usually reported on a quarterly
    basis, whereas market prices are available at a much higher frequency. Therefore,
    adjusted EPS and price data need to be synchronized, taking into account when
    the available data was, in fact, released to market participants.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这种偏差的常见原因是未能考虑在初始发布后对报告的财务数据进行更正或重新说明。股票拆分或反向拆分也可能产生前瞻偏差。例如，当计算收益率时，**每股收益**（EPS）数据通常以季度为基础报告，而市场价格则以更高的频率可用。因此，需要同步调整的EPS和价格数据，考虑到可用数据实际上是何时发布给市场参与者的。
- en: The **solution** involves the careful validation of the timestamps of all data
    that enters a backtest. We need to guarantee that conclusions are based only on
    point-in-time data that does not inadvertently include information from the future.
    High-quality data providers ensure that these criteria are met. When point-in-time
    data is not available, we need to make (conservative) assumptions about the lag
    in reporting.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决方案**涉及仔细验证进入回测的所有数据的时间戳。我们需要确保结论仅基于时点数据，不会无意中包含未来信息。高质量的数据提供商确保满足这些标准。当时点数据不可用时，我们需要对报告滞后进行（保守的）假设。'
- en: Survivorship bias – track your historical universe
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生存偏差-跟踪您的历史宇宙
- en: Survivorship bias arises when the backtest data contains only securities that
    are currently active while **omitting assets that have disappeared** over time,
    due to, for example, bankruptcy, delisting, or acquisition. Securities that are
    no longer part of the investment universe often did not perform well, and failing
    to include these cases positively skew the backtest result.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 生存偏差是指回测数据仅包含当前活跃的证券，**而忽略了随时间消失的资产**，例如破产、退市或收购。不再属于投资宇宙的证券通常表现不佳，而未能包括这些情况会使回测结果产生正向偏差。
- en: The **solution**, naturally, is to verify that datasets include all securities
    available over time, as opposed to only those that are still available when running
    the test. In a way, this is another way of ensuring the data is truly point-in-time.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决方案**自然是验证数据集是否包含随时间可用的所有证券，而不仅仅是在测试运行时仍然可用的证券。在某种程度上，这是确保数据真正是时点数据的另一种方式。'
- en: Outlier control – do not exclude realistic extremes
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 异常值控制-不要排除现实极端情况
- en: Data preparation typically includes some treatment of outliers such as winsorizing,
    or clipping, extreme values. The challenge is to **identify outliers that are
    truly not representative** of the period under analysis, as opposed to any extreme
    values that are an integral part of the market environment at that time. Many
    market models assume normally distributed data when extreme values are observed
    more frequently, as suggested by fat-tailed distributions.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备通常包括对异常值的处理，例如winsorizing或剪切极端值。挑战在于**识别那些真正不代表分析期的异常值**，而不是那些在那个时候是市场环境的一个组成部分的极端值。许多市场模型假设当频繁观察到极端值时，数据呈正态分布，如厚尾分布所示。
- en: The **solution** involves a careful analysis of outliers with respect to the
    probability of extreme values occurring and adjusting the strategy parameters
    to this reality.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决方案**涉及对异常值进行仔细分析，以确定极端值发生的概率，并根据这一现实调整策略参数。'
- en: Sample period – try to represent relevant future scenarios
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 样本期间 - 尝试代表相关的未来情景
- en: A backtest will not yield representative results that generalize to the future
    if the sample data does not **reflect the current (and likely future) environment**.
    A poorly chosen sample data might lack relevant market regime aspects, for example,
    in terms of volatility or volumes, fail to include enough data points, or contain
    too many or too few extreme historical events.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果样本数据不**反映当前（和可能的未来）环境**，回测将无法产生代表性的结果。选择不当的样本数据可能缺乏相关的市场制度方面，例如波动性或成交量，未能包含足够的数据点，或包含太多或太少的极端历史事件。
- en: The **solution** involves using sample periods that include important market
    phenomena or generating synthetic data that reflects the relevant market characteristics.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案涉及使用包括重要市场现象的样本期间或生成反映相关市场特征的合成数据。
- en: Getting the simulation right
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正确进行模拟
- en: 'Practical issues related to the implementation of the historical simulation
    include:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 实施历史模拟涉及的实际问题包括：
- en: Failure to **mark to market** to accurately reflect market prices and account
    for drawdowns
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未能**按市场价值**准确反映市场价格并考虑回撤
- en: '**Unrealistic assumptions** about the availability, cost, or market impact
    of trades'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对可用性、成本或交易市场影响的**不切实际假设**
- en: Incorrect **timing of signals and trade execution**
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 错误的**信号时机和交易执行**
- en: Let's see how to identify and address each of these issues.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何识别和解决这些问题。
- en: Mark-to-market performance – track risks over time
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 按市场价值表现 - 随时间跟踪风险
- en: A strategy needs to **meet investment objectives and constraints at all times**.
    If it performs well over the course of the backtest but leads to unacceptable
    losses or volatility over time, this will (obviously) not be practical. Portfolio
    managers need to track and report the value of their positions, called mark to
    market, on a regular basis and possibly in real time.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 策略需要**始终满足投资目标和约束条件**。如果它在回测过程中表现良好，但随着时间的推移导致无法接受的损失或波动，这显然是不切实际的。投资组合经理需要定期追踪和报告其头寸的价值，称为按市场价值计算，并可能是实时的。
- en: The solution involves plotting performance over time or calculating (rolling)
    risk metrics, such as the **value at risk** (**VaR**) or the Sortino ratio.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案涉及随时间绘制绩效或计算（滚动）风险指标，如**风险价值**（VaR）或Sortino比率。
- en: Transaction costs – assume a realistic trading environment
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 交易成本 - 假设一个真实的交易环境
- en: Markets do not permit the execution of all trades at all times or at the targeted
    price. A backtest that assumes **trades that may not actually be available** or
    would have occurred at less favorable terms will produce biased results.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 市场不允许在任何时候以任何价格执行所有交易。假设**实际上可能不可用的交易**或以不利的条件发生的交易的回测将产生偏见的结果。
- en: Practical shortcomings include a strategy that assumes short sales when there
    may be no counterparty, or one that underestimates the market impact of trades
    (slippage) that are large or deal in less liquid assets, or the costs that arise
    due to broker fees.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 实际缺陷包括一种假设在没有交易对手时进行空头交易的策略，或者低估了交易的市场影响（滑点），这些交易规模较大或交易较少流动性资产，或者由于经纪费而产生的成本。
- en: The **solution** includes a limitation to a liquid universe and/or realistic
    parameter assumptions for trading and slippage costs. This also safeguards against
    misleading conclusions from unstable factor signals that decay fast and produce
    a high portfolio turnover.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决方案**包括对流动性范围的限制和/或对交易和滑点成本的现实参数假设。这也可以防止不稳定因子信号产生高投资组合周转率的误导性结论。'
- en: Timing of decisions – properly sequence signals and trades
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 决策的时机 - 正确地顺序信号和交易
- en: Similar to look-ahead bias, the simulation could make **unrealistic assumptions
    about when it receives and trades on signals**. For instance, signals may be computed
    from close prices when trades are only available at the next open, with possibly
    quite different prices. When we evaluate performance using the close price, the
    backtest results will not represent realistic future outcomes.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于前瞻性偏差，模拟可能会对其何时接收和交易信号**做出不切实际的假设**。例如，当交易仅在下一个开盘时才可用时，可能会从收盘价格计算信号，而这时的价格可能会有很大的不同。当我们使用收盘价格评估绩效时，回测结果将无法代表现实的未来结果。
- en: The **solution** involves careful orchestration of the sequence of signal arrival,
    trade execution, and performance evaluation.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决方案**涉及对信号到达、交易执行和绩效评估的顺序进行仔细协调。'
- en: Getting the statistics right
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正确进行统计
- en: The most prominent challenge when backtesting validity, including published
    results, is the discovery of spurious patterns due to multiple testing. Selecting
    a strategy based on the tests of different candidates on the same data will bias
    the choice. This is because a positive outcome is more likely caused by the stochastic
    nature of the performance measure itself. In other words, the strategy overfits
    the test sample, producing deceptively positive results that are unlikely to generalize
    to future data that's encountered during live trading.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 回测有效性的最突出挑战，包括已发表的结果，是由于多次测试而发现虚假模式。基于对相同数据的不同候选人的测试选择策略将偏向于选择。这是因为积极的结果更可能是由于绩效度量本身的随机性。换句话说，策略过度拟合了测试样本，产生了具有欺骗性的积极结果，不太可能推广到实际交易中遇到的未来数据。
- en: Hence, backtest performance is only informative if the number of trials is reported
    to allow for an assessment of the risk of selection bias. This is rarely the case
    in practical or academic research, inviting doubts about the validity of many
    published claims.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，只有在报告试验次数以允许评估选择偏差风险的情况下，回测性能才具有信息价值。在实际或学术研究中很少出现这种情况，这引发了对许多已发表声明有效性的怀疑。
- en: Furthermore, the risk of backtest overfitting does not only arise from running
    numerous tests but also affects strategies designed based on prior knowledge of
    what works and doesn't. Since the risks include the knowledge of backtests run
    by others on the same data, backtest-overfitting is very hard to avoid in practice.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，回测过度拟合的风险不仅来自进行大量测试，而且还影响基于先前知识的工作和不工作的策略。由于风险包括他人在相同数据上运行的回测知识，回测过度拟合在实践中很难避免。
- en: Proposed **solutions** include prioritizing tests that can be justified using
    investment or economic theory, rather than arbitrary data-mining efforts. It also
    implies testing in a variety of contexts and scenarios, including possibly on
    synthetic data.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 提出的**解决方案**包括优先考虑可以用投资或经济理论来证明的测试，而不是任意的数据挖掘努力。它还意味着在各种情境和场景中进行测试，包括可能在合成数据上进行测试。
- en: The minimum backtest length and the deflated SR
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最小回测长度和通货紧缩SR
- en: Marcos Lopez de Prado ([http://www.quantresearch.info/](http://www.quantresearch.info/))
    has published extensively on the risks of backtesting and how to detect or avoid
    it. This includes an online simulator of backtest-overfitting ([http://datagrid.lbl.gov/backtest/](http://datagrid.lbl.gov/backtest/),
    *Bailey, et al. 2015*).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Marcos Lopez de Prado（[http://www.quantresearch.info/](http://www.quantresearch.info/)）在回测风险以及如何检测或避免它方面发表了大量文章。这包括一个回测过度拟合的在线模拟器（[http://datagrid.lbl.gov/backtest/](http://datagrid.lbl.gov/backtest/)，*Bailey等，2015年*）。
- en: Another result includes an estimate of the minimum length of the backtest period
    that an investor should require to avoid selecting a strategy that achieves a
    certain SR for a given number of in-sample trials, but has an expected out-of-sample
    SR of zero. The result implies that, for example, 2 years of daily backtesting
    data does not support conclusions about more than seven strategies. 5 years of
    data expands this number to 45 strategy variations. See *Bailey, Borwein, and
    Prado (2016)* for implementation details.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个结果包括对投资者应要求的回测期最小长度的估计，以避免选择一种在样本内试验次数给定的情况下实现一定SR的策略，但在样本外预期SR为零的策略。该结果意味着，例如，2年的每日回测数据不支持对超过七种策略的结论。5年的数据将这一数字扩大到45种策略变体。有关实施细节，请参阅*Bailey，Borwein和Prado（2016年）*。
- en: '*Bailey and Prado (2014)* also derived a deflated SR to compute the probability
    that the SR is statistically significant while controlling for the inflationary
    effect of multiple testing, non-normal returns, and shorter sample lengths. (See
    the `multiple_testing` subdirectory for the Python implementation of `deflated_sharpe_ratio.py`
    and references for the derivation of the related formulas.)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*Bailey和Prado（2014年）*还推导出了一个通货紧缩SR，用于计算SR在控制多次测试、非正态收益和较短样本长度的通货膨胀效应的情况下是否具有统计显著性的概率。（有关`deflated_sharpe_ratio.py`的Python实现以及相关公式的推导，请参见`multiple_testing`子目录。）'
- en: Optimal stopping for backtests
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 回测的最佳停止点
- en: In addition to limiting backtests to strategies that can be justified on theoretical
    grounds as opposed to mere data-mining exercises, an important question is when
    to stop running additional tests.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 除了将回测限制在可以在理论上证明的策略而不是仅仅是数据挖掘练习上，一个重要的问题是何时停止运行额外的测试。
- en: One way to answer this question relies on the solution to the **secretary problem**
    from the optimal stopping theory. This problem assumes we are selecting an applicant
    based on interview results and need to decide whether to hold an additional interview
    or choose the most recent candidate. In this context, the optimal rule is to always
    reject the first *n*/*e* candidates and then select the first candidate that surpasses
    all the previous options. Using this rule results in a 1/*e* probability of selecting
    the best candidate, irrespective of the size *n* of the candidate pool.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 回答这个问题的一种方法依赖于最优停止理论中的**秘书问题**的解决方案。这个问题假设我们根据面试结果选择申请人，并需要决定是否进行额外的面试或选择最近的候选人。在这种情况下，最佳规则是始终拒绝前*n*/*e*个候选人，然后选择超过所有先前选项的第一个候选人。使用这个规则会导致以1/*e*的概率选择最佳候选人，而不管候选人池的大小*n*如何。
- en: 'Translating this rule directly to the backtest context produces the following
    **recommendation**: test a random sample of 1/*e* (roughly 37 percent) of reasonable
    strategies and record their performance. Then, continue with the tests until a
    strategy outperforms those tested before. This rule applies to tests of several
    alternatives, with the goal of choosing a near-best as soon as possible while
    minimizing the risk of a false positive. See the resources listed on GitHub for
    additional information.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 将这个规则直接转化为回测环境产生以下**建议**：测试一个随机样本的1/*e*（大约37%）合理策略，并记录它们的表现。然后，继续测试，直到一种策略胜过之前测试过的策略。这个规则适用于测试几种替代方案，目标是尽快选择一个接近最佳的策略，同时最小化虚假阳性的风险。有关更多信息，请参阅GitHub上列出的资源。
- en: How a backtesting engine works
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Put simply, a backtesting engine iterates over historical prices (and other
    data), passes the current values to your algorithm, receives orders in return,
    and keeps track of the resulting positions and their value.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: In practice, there are numerous requirements for creating a realistic and robust
    simulation of the ML4T workflow that was depicted in *Figure 8.1* at the beginning
    of this chapter. The difference between vectorized and event-driven approaches
    illustrates how the faithful reproduction of the actual trading environment adds
    significant complexity.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Vectorized versus event-driven backtesting
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A vectorized backtest is the most basic way to evaluate a strategy. It simply
    multiplies a signal vector that represents the target position size with a vector
    of returns for the investment horizon to compute the period performance.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Let's illustrate the vectorized approach using the daily return predictions
    that we created using ridge regression in the previous chapter. Using a few simple
    technical factors, we predicted the returns for the next day for the 100 stocks
    with the highest recent dollar trading volume (see *Chapter 7*, *Linear Models
    – From Risk Factors to Return Forecasts*, for details).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll transform the predictions into signals for a very simple strategy: on
    any given trading day, we will go long on the 10 highest positive predictions
    and go short on the lowest 10 negative predictions. If there are fewer positive
    or negative predictions, we''ll hold fewer long or short positions. The notebook
    `vectorized_backtest` contains the following code example, and the script `data.py`
    creates the input data stored in `backtest.h5`.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we load the data for our strategy, as well as S&P 500 prices (which
    we convert into daily returns) to benchmark the performance:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The data combines daily return predictions and OHLCV market data for 253 distinct
    stocks over the 2014-17 period, with 100 equities for each day. Now, we can compute
    the daily forward returns and convert these and the predictions into wide format,
    with one ticker per column:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The next step is to select positive and negative predictions, rank them in
    descending and ascending fashion, and create long and short signals using an integer
    mask that identifies the top 10 on each side with identifies the predictions outside
    the top 10 with a one, and a zero:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can then multiply the binary DataFrames with the forward returns (using
    their negative inverse for the shorts) to get the daily performance of each position,
    assuming equal-sized investments. The daily average of these returns corresponds
    to the performance of equal-weighted long and short portfolios, and the sum reflects
    the overall return of a market-neutral long-short strategy:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: When we compare the results, as shown in *Figure 8.2*, our strategy performed
    well compared to the S&P 500 for the first 2 years of the period – that is, until
    the benchmark catches up and our strategy underperforms during 2017.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 'The strategy returns are also less volatile with a standard deviation of 0.002
    compared to 0.008 for the S&P 500; the correlation is low and negative at -0.093:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_08_02.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.2: Vectorized backtest results'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 'While this approach permits a quick back-of-the-envelope evaluation, it misses
    important features of a robust, realistic, and user-friendly backtest engine;
    for example:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: We need to manually align the timestamps of predictions and returns (using pandas' built-in
    capabilities) and do not have any safeguards against inadvertent look-ahead bias.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no explicit position sizing and representation of the trading process
    that accounts for costs and other market realities, or an accounting system that
    tracks positions and their performance.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is also no performance measurement other than what we compute after the
    fact, and risk management rules like stop-loss are difficult to simulate.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That's where event-driven backtesting comes in. An event-driven backtesting
    engine explicitly simulates the time dimension of the trading environment and
    imposes significantly more structure on the simulation. This includes the use
    of historical calendars that define when trades can be made and when quotes are
    available. The enforcement of timestamps also helps to avoid look-ahead bias and
    other implementation errors mentioned in the previous section (but there is no
    guarantee).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是事件驱动的反向测试的用武之地。事件驱动的反向测试引擎明确模拟了交易环境的时间维度，并对模拟施加了更多结构。这包括使用历史日历来定义何时可以进行交易以及何时可以获得报价。时间戳的强制执行还有助于避免前瞻性偏见和前一节中提到的其他实施错误（但不能保证）。
- en: Generally, event-driven systems aim to capture the actions and constraints encountered
    by a strategy more closely and, ideally, can readily be converted into a live
    trading engine that submits actual orders.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，事件驱动系统旨在更紧密地捕捉策略遇到的行动和限制，并且理想情况下可以轻松转换为提交实际订单的实时交易引擎。
- en: Key implementation aspects
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关键实施方面
- en: The requirements for a realistic simulation may be met by a **single platform**
    that supports all steps of the process in an end-to-end fashion, or by **multiple
    tools** that each specialize in different aspects.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 实现真实模拟的要求可以通过**支持整个过程的单一平台**以端到端方式实现，也可以通过**专门化不同方面的多个工具**来实现。
- en: For instance, you could handle the design and testing of ML models that generate
    signals using generic ML libraries like scikit-learn, or others that we will encounter
    in this book, and feed the model outputs into a separate backtesting engine. Alternatively,
    you could run the entire ML4T workflow end-to-end on a single platform like Quantopian
    or QuantConnect.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您可以使用通用的ML库（如scikit-learn）处理ML模型的设计和测试，或者使用本书中将遇到的其他库，并将模型输出馈送到单独的反向测试引擎。另一种方法是在Quantopian或QuantConnect等单一平台上端到端地运行整个ML4T工作流程。
- en: The following sections highlight key items and implementation details that need
    to be addressed to put this process into action.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分突出了需要解决的关键问题和实施细节，以将该流程付诸实施。
- en: Data ingestion – format, frequency, and timing
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据摄入-格式、频率和时间
- en: The first step in the process concerns the sources of data. Traditionally, algorithmic
    trading strategies focused on market data, namely the OHLCV price and volume data
    that we discussed in *Chapter 2*, *Market and Fundamental Data – Sources and Techniques*.
    Today, data sources are more diverse and raise the question of how many different
    **storage formats and data types** to support, and whether to use a proprietary
    or custom format or rely on third-party or open source formats.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程的第一步涉及数据来源。传统上，算法交易策略侧重于市场数据，即我们在*第2章*中讨论的OHLCV价格和成交量数据。今天，数据来源更加多样化，引发了如何支持多少不同的**存储格式和数据类型**以及是使用专有或自定义格式，还是依赖第三方或开源格式的问题。
- en: Another aspect is the **frequency of data sources** that can be used and whether
    sources at different frequencies can be combined. Common options in increasing
    order of computational complexity and memory and storage requirements include
    daily, minute, and tick frequency. Intermediate frequencies are also possible.
    Algorithmic strategies tend to perform better at higher frequencies, even though
    quantamental investors are gaining ground, as discussed in *Chapter 1*, *Machine
    Learning for Trading – From Idea to Execution*. Regardless, institutional investors
    will certainly require tick frequency.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个方面是可以使用的**数据源频率**，以及不同频率的数据源是否可以合并。按照计算复杂性和内存存储需求递增的常见选项包括每日、分钟和tick频率。也可以使用中间频率。尽管量化基金投资者正在崛起，但算法策略在更高频率下往往表现更好，如*第1章*中所讨论的《交易的机器学习-从构想到执行》。无论如何，机构投资者肯定需要tick频率。
- en: Finally, data ingestion should also address **point-in-time constraints** to
    avoid look-ahead bias, as outlined in the previous section. The use of trading
    calendars helps limit data to legitimate dates and times; adjustments to reflect
    corporate actions like stock splits and dividends or restatements that impact
    prices revealed at specific times need to be made prior to ingestion.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，数据摄入还应解决**时间点限制**，以避免前瞻性偏见，如前一节所述。使用交易日历有助于限制数据在合法日期和时间内；需要在摄入之前进行调整，以反映影响特定时间点价格的公司行为，如股票拆分和股利或重述。
- en: Factor engineering – built-in factors versus libraries
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 因子工程-内置因子与库
- en: To facilitate the engineering of alpha factors for use in ML models, many backtesting
    engines include computational tools suitable for numerous standard transformations
    like moving averages and various technical indicators. A key advantage of **built-in
    factor engineering** is the easy conversion of the backtesting pipeline into a
    live trading engine that applies the same computations to the input data.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于为ML模型工程化alpha因子，许多反向测试引擎包括适用于许多标准转换的计算工具，如移动平均线和各种技术指标。**内置因子工程**的一个关键优势是将反向测试管道轻松转换为应用相同计算于输入数据的实时交易引擎。
- en: The **numerical Python libraries** (pandas, NumPy, TA-Lib) presented in *Chapter
    4*, *Financial Feature Engineering – How to Research Alpha Factors*, are an alternative
    to **pre-compute factors**. This can be efficient when the goal is to reuse factors
    in various backtests that amortize the computational cost.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*第4章*中介绍的**数值Python库**（pandas、NumPy、TA-Lib）是**预先计算因子**的替代方案。当目标是在各种反向测试中重复使用因子以摊销计算成本时，这可能是有效的。'
- en: ML models, predictions, and signals
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ML模型、预测和信号
- en: As mentioned earlier, the ML workflow discussed in *Chapter 6*, *The Machine
    Learning Process*, can be embedded in an end-to-end platform that integrates the
    model design and evaluation part into the backtesting process. While convenient,
    this is also costly because model training becomes part of the backtest when the
    goal is perhaps to fine-tune trading rules.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Similar to factor engineering, you can decouple these aspects and design, train,
    and evaluate ML models using generic libraries for this purpose, and also provide
    the relevant predictions as inputs to the backtester. We will mostly use this
    approach in this book because it makes the exposition more concise and less repetitive.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Trading rules and execution
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A realistic strategy simulation requires a faithful representation of the trading
    environment. This includes access to relevant exchanges, the availability of the
    various order types discussed in *Chapter 2*, *Market and Fundamental Data – Sources
    and Techniques*, and the accounting for transaction costs. Costs include broker
    commissions, bid-ask spreads, and slippage, giving us the difference between the
    target execution price and the price that's eventually obtained. It is also important
    to ensure trades execute with delays that reflect liquidity and operating hours.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Performance evaluation
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finally, a backtesting platform needs to facilitate performance evaluation.
    It can provide standard metrics derived from its accounting of transactions, or
    provide an output of the metrics that can be used with a library like **pyfolio**
    that's suitable for this purpose.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: In the next two sections, we will explore two of the most popular backtesting
    libraries, namely backtrader and Zipline.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: backtrader – a flexible tool for local backtests
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**backtrader** is a popular, flexible, and user-friendly Python library for
    local backtests with great documentation, developed since 2015 by Daniel Rodriguez.
    In addition to a large and active community of individual traders, there are several
    banks and trading houses that use backtrader to prototype and test new strategies
    before porting them to a production-ready platform using, for example, Java. You
    can also use backtrader for live trading with several brokers of your choice (see
    the backtrader documentation and *Chapter 23*, *Conclusions and Next Steps*).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: We'll first summarize the key concepts of backtrader to clarify the big picture
    of the backtesting workflow on this platform, and then demonstrate its usage for
    a strategy driven by ML predictions.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Key concepts of backtrader's Cerebro architecture
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: backtrader's **Cerebro** (Spanish for "brain") architecture represents the key
    components of the backtesting workflow as (extensible) Python objects. These objects
    interact to facilitate processing input data and the computation of factors, formulate
    and execute a strategy, receive and execute orders, and track and measure performance.
    A Cerebro instance orchestrates the overall process from collecting inputs, executing
    the backtest bar by bar, and providing results.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: The library uses conventions for these interactions that allow you to omit some
    detail and streamline the backtesting setup. I highly recommend browsing the documentation
    to dive deeper if you plan on using backtrader to develop your own strategies.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 8.3* outlines the key elements in the Cerebro architecture, and the
    following subsections summarize their most important functionalities:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_08_03.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.3: The backtrader Cerebro architecture'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Data feeds, lines, and indicators
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data feeds are the raw material for a strategy and contain information about
    individual securities, such as OHLCV market data with a timestamp for each observation,
    but you can customize the available fields. backtrader can ingest data from various
    sources, including CSV files and pandas DataFrames, and from online sources like
    Yahoo Finance. There are also extensions you can use to connect to online trading
    platforms like Interactive Brokers to ingest live data and execute transactions.
    The compatibility with DataFrame objects implies that you can load data from accessible
    by pandas, ranging from databases to HDF5 files. (See the demonstration in the
    *How to use backtrader in practice* section; also, see the *I/O* section of the
    pandas documentation.)
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Once loaded, we add the data feeds to a Cerebro instance, which, in turn, makes
    it available to one or more strategies in the order received. Your strategy's
    trading logic can access each data feed by name (for example, the ticker) or sequence
    number and retrieve the current and past values of any field of the data feed.
    Each field is called a **line**.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: backtrader comes with over 130 common technical **indicators** that allow you
    to compute new values from lines or other indicators for each data feed to drive
    your strategy. You can also use standard Python **operations** to derive new values.
    Usage is fairly straightforward and well explained in the documentation.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: From data and signals to trades – strategy
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **Strategy** object contains your trading logic that places orders based
    on data feed information that the Cerebro instance presents at every bar during
    backtest execution. You can easily test variations by configuring a Strategy to
    accept arbitrary parameters that you define when adding an instance of your Strategy
    to your Cerebro.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: For every bar of a backtest, the Cerebro instance calls either the `.prenext()`
    or `.next()` method of your Strategy instance. The role of `.prenext()` is to
    address bars that do not yet have complete data for all feeds, for example, before
    there are enough periods to compute an indicator like a built-in moving average
    or if there is otherwise missing data. The default is to do nothing, but you can
    add trading logic of your choice or call `next()` if your main Strategy is designed
    to handle missing values (see the *How to use backtrader in practice* section).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: You can also use backtrader without defining an explicit Strategy and instead
    use a simplified Signals interface. The Strategy API gives you more control and
    flexibility, though; see the backtrader documentation for details on how to use
    the Signals API.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'A Strategy outputs orders: let''s see how backtrader handles these next.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Commissions instead of commission schemes
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once your Strategy has evaluated current and past data points at each bar, it
    needs to decide which orders to place. backtrader lets you create several standard
    **order** types that Cerebro passes to a Broker instance for execution and provides
    a notification of the result at each bar.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use the Strategy methods `buy()` and `sell()` to place market, close,
    and limit orders, as well as stop and stop-limit orders. Execution works as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '**Market order**: Fills at the next open bar'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Close order**: Fills at the next close bar'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Limit order**: Executes only if a price threshold is met (for example, only
    buy up to a certain price) during an (optional) period of validity'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stop order**: Becomes a market order if the price reaches a given threshold'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stop limit order**: Becomes a limit order once the stop is triggered'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In practice, stop orders differ from limit orders because they cannot be seen
    by the market prior to the price trigger. backtrader also provides target orders
    that compute the required size, taking into account the current position to achieve
    a certain portfolio allocation in terms of the number of shares, the value of
    the position, or the percentage of portfolio value. Furthermore, there are **bracket
    orders** that combine, for a long order, a buy with two limit sell orders that
    activate as the buy executes. Should one of the sell orders fill or cancel, the
    other sell order also cancels.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: The **Broker** handles order execution, tracks the portfolio, cash value, and
    notifications and implements transaction costs like commission and slippage. The
    Broker may reject trades if there is not enough cash; it can be important to sequence
    buys and sells to ensure liquidity. backtrader also has a `cheat_on_open` feature
    that permits looking ahead to the next bar, to avoid rejected trades due to adverse
    price moves by the next bar. This feature will, of course, bias your results.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: In addition to **commission schemes** like a fixed or percentage amount of the
    absolute transaction value, you can implement your own logic, as demonstrated
    later, for a flat fee per share.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Making it all happen – Cerebro
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Cerebro control system synchronizes the data feeds based on the bars represented
    by their timestamp, and runs the trading logic and broker actions on an event-by-event
    basis accordingly. backtrader does not impose any restrictions on the frequency
    or the trading calendar and can use multiple time frames in parallel.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: It also vectorizes the calculation for indicators if it can preload source data.
    There are several options you can use to optimize operations from a memory perspective
    (see the Cerebro documentation for details).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: How to use backtrader in practice
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are going to demonstrate backtrader using the daily return predictions from
    the ridge regression from *Chapter 7*, *Linear Models – From Risk Factors to Return
    Forecasts*, as we did for the vectorized backtest earlier in this chapter. We
    will create the Cerebro instance, load the data, formulate and add the Strategy,
    run the backtest, and review the results.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: The notebook `backtesting_with_backtrader` contains the following code examples
    and some additional details.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: How to load price and other data
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We need to ensure that we have price information for all the dates on which
    we would like to buy or sell stocks, not only for the days with predictions. To
    load data from a pandas DataFrame, we subclass backtrader''s `PandasData` class
    to define the fields that we will provide:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We then instantiate a `Cerebro` class and use the `SignalData` class to add
    one data feed for each ticker in our dataset that we load from HDF5:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now, we are ready to define our Strategy.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: How to formulate the trading logic
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our `MLStrategy` subclasses backtrader''s `Strategy` class and defines parameters
    that we can use to modify its behavior. We also create a log file to create a
    record of the transactions:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The core of the strategy resides in the `.next()` method. We go long/short
    on the `n_position` stocks with the highest positive/lowest negative forecast,
    as long as there are at least `min_positions` positions. We always sell any existing
    positions that do not appear in the new long and short lists and use `order_target_percent`
    to build equal-weights positions in the new targets (log statements are omitted
    to save some space):'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now, we need to configure our `Cerebro` instance and add our `Strategy`.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: How to configure the Cerebro instance
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We use a custom commission scheme that assumes we pay a fixed amount of $0.02
    per share that we buy or sell:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then, we define our starting cash amount and configure the broker accordingly:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, all that''s missing is adding the `MLStrategy` to our `Cerebro` instance,
    providing parameters for the desired number of positions and the minimum number
    of long/shorts. We''ll also add a pyfolio analyzer so we can view the performance
    tearsheets we presented in *Chapter 5*, *Portfolio Optimization and Performance
    Evaluation*:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The backtest uses 869 trading days and takes around 45 seconds to run. The following
    figure shows the cumulative return and the evolution of the portfolio value, as
    well as the daily value of long and short positions.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Performance looks somewhat similar to the preceding vectorized test, with outperformance
    relative to the S&P 500 benchmark during the first half and poor performance thereafter.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 'The `backtesting_with_backtrader` notebook contains the complete pyfolio results:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_08_04.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.4: backtrader results'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: backtrader summary and next steps
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: backtrader is a very straightforward yet flexible and performant backtesting
    engine for local backtesting. You can load any dataset at the frequency you desire
    from a broad range of sources due to pandas compatibility. `Strategy` lets you
    define arbitrary trading logic; you just need to ensure you access the distinct
    data feeds as needed. It also integrates well with pyfolio for quick yet comprehensive
    performance evaluation.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: In the demonstration, we applied our trading logic to predictions from a pre-trained
    model. We can also train a model during backtesting because we can access data
    prior to the current bar. Often, however, it is more efficient to decouple model
    training from strategy selection and avoid duplicating model training.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: One of the reasons for backtrader's popularity is the ability to use it for
    live trading with a broker of your choosing. The community is very lively, and
    code to connect to brokers or additional data sources, including for cryptocurrencies,
    is readily available online.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Zipline – scalable backtesting by Quantopian
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The backtesting engine Zipline powers Quantopian's online research, backtesting,
    and live (paper) trading platform. As a hedge fund, Quantopian aims to identify
    robust algorithms that outperform, subject to its risk management criteria. To
    this end, they use competitions to select the best strategies and allocate capital
    to share profits with the winners.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Quantopian first released Zipline in 2012 as version 0.5, and the latest version,
    1.3, dates from July 2018\. Zipline works well with its sister libraries Alphalens,
    pyfolio, and empyrical that we introduced in *Chapter 4,* *Financial Feature Engineering
    – How to Research Alpha Factors* and *Chapter 5,* *Portfolio Optimization and
    Performance Evaluation*, and integrates well with NumPy, pandas, and numeric libraries,
    but may not always support the latest version.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Zipline is designed to operate at the scale of thousands of securities, and
    each can be associated with a large number of indicators. It imposes more structure
    on the backtesting process than backtrader to ensure data quality by eliminating
    look-ahead bias, for example, and optimize computational efficiency while executing
    a backtest. We'll take a look at the key concepts and elements of the architecture,
    shown in *Figure 8.5,* before we demonstrate how to use Zipline to backtest ML-driven
    models on the data of your choice.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Calendars and the Pipeline for robust simulations
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Key features that contribute to the goals of scalability and reliability are
    data bundles that store OHLCV market data with on-the-fly adjustments for splits
    and dividends, trading calendars that reflect operating hours of exchanges around
    the world, and the powerful Pipeline API (see the following diagram). We will
    discuss their usage in the following sections to complement the brief Zipline
    introduction we gave in earlier chapters:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_08_05.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.5: The Zipline architecture'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Bundles – point-in-time data with on-the-fly adjustments
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The principal data store is a **bundle** that resides on disk in compressed,
    columnar bcolz format for efficient retrieval, combined with metadata stored in
    an SQLite database. Bundles are designed to contain only OHLCV data and are limited
    to daily and minute frequency. A great feature is that bundles store split and
    dividend information, and Zipline computes **point-in-time adjustments**, depending
    on the time period you pick for your backtest.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: Zipline relies on the **TradingCalendar** library (also maintained by Quantopian)
    for operational details on exchanges around the world, such as time zone, market
    open and closing times, or holidays. Data sources have domains (for now, these
    are countries) and need to conform to the assigned exchange calendar. Quantopian
    is actively developing support for international securities, and these features
    may evolve.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: After installation, the command `zipline ingest -b bundle` lets you install
    the Quandl Wiki dataset (daily frequency) right away. The result ends up in the
    `.zipline` directory, which, by default, resides in your home folder. In addition,
    you can design your own bundles, as we'll see.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: In addition to bundles, you can provide OHCLV data to an algorithm as a pandas
    DataFrame or Panel. (Panel is recently deprecated, but Zipline is a few pandas
    versions behind.) However, bundles are more convenient and efficient.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'A shortcoming of bundles is that they do not let you store data other than
    price and volume information. However, two alternatives let you accomplish this:
    the `fetch_csv()` function downloads DataFrames from a URL and was designed for
    other Quandl data sources, for example, fundamentals. Zipline reasonably expects
    the data to refer to the same securities for which you have provided OHCLV data
    and aligns the bars accordingly. It''s very easy to patch the library to load
    a local CSV or HDF5 using pandas, and the GitHub repository provides some guidance
    on how to do so.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: In addition, `DataFrameLoader` and `BlazeLoader` permit you to feed additional
    attributes to a Pipeline (see the `DataFrameLoader` demo later in this chapter).
    `BlazeLoader` can interface with numerous sources, including databases. However,
    since the Pipeline API is limited to daily data, `fetch_csv()` will be critical
    to adding features at a minute frequency, as we will do in later chapters.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: The Algorithm API – backtests on a schedule
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `TradingAlgorithm` class implements the Zipline Algorithm API and operates
    on `BarData` that has been aligned with a given trading calendar. After the initial
    setup, the backtest runs for a specified period and executes its trading logic
    as specific events occur. These events are driven by the daily or minutely trading
    frequency, but you can also schedule arbitrary functions to evaluate signals,
    place orders, and rebalance your portfolio, or log information about the ongoing
    simulation.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: You can execute an algorithm from the command line, in a Jupyter Notebook, or
    by using the `run_algorithm()` method of the underlying `TradingAlgorithm` class.
    The algorithm requires an `initialize()` method that is called once when the simulation
    starts. It keeps state through a context dictionary and receives actionable information
    through a data variable containing point-in-time current and historical data.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: You can add properties to the context dictionary, which is available to all
    other `TradingAlgorithm` methods, or register pipelines that perform more complex
    data processing, such as computing alpha factors and filtering securities accordingly.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm execution occurs through optional methods that are either scheduled
    automatically by Zipline or at user-defined intervals. The method `before_trading_start()`
    is called daily before the market opens and primarily serves to identify a set
    of securities the algorithm may trade during the day. The method `handle_data()`
    is called at the given trading frequency, for example, every minute.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Upon completion, the algorithm returns a DataFrame containing portfolio performance
    metrics if there were any trades, as well as user-defined metrics. As demonstrated
    in *Chapter 5,* *Portfolio Optimization and Performance Evaluation*, the output
    is compatible with pyfolio so that you can quickly create performance tearsheets.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Known issues
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Zipline currently requires the presence of Treasury curves and the S&P 500 returns
    for benchmarking ([https://github.com/quantopian/zipline/issues/2480](https://github.com/quantopian/zipline/issues/2480)).
    The latter relies on the IEX API, which now requires registration to obtain a
    key. It is easy to patch Zipline to circumvent this and download data from the
    Federal Reserve, for instance. The GitHub repository describes how to go about
    this. Alternatively, you can move the SPY returns provided in `zipline/resources/market_data/SPY_benchmark.csv`
    to your `.zipline` folder, which usually lives in your home directory, unless
    you changed its location.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Live trading ([https://github.com/zipline-live/zipline](https://github.com/zipline-live/zipline))
    your own systems is only available with Interactive Brokers and is not fully supported
    by Quantopian.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Ingesting your own bundles with minute data
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use the NASDAQ100 2013-17 sample provided by AlgoSeek that we introduced
    in *Chapter 2,* *Market and Fundamental Data – Sources and Techniques**,* to demonstrate
    how to write your own custom bundle. There are four steps:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Divide your OHCLV data into one file per ticker and store metadata and split
    and dividend adjustments.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a script to pass the result to an `ingest()` function, which, in turn,
    takes care of writing the bundle to bcolz and SQLite format.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Register the bundle in an extension.py script that lives in your `.zipline`
    directory in your home folder, and symlink the data sources.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For AlgoSeek data, we also provide a custom TradingCalendar because it includes
    trading activity outside NYSE market hours.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The directory `custom_bundles` contains the code examples for this section.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Getting your data ready to be bundled
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In *Chapter 2, Market and Fundamental Data – Sources and Techniques*, we parsed
    the daily files containing the AlgoSeek NASDAQ 100 OHLCV data to obtain a time
    series for each ticker. We will use this result because Zipline also stores each
    security individually.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we obtain equity metadata using the pandas DataReader `get_nasdaq_symbols()`
    function. Finally, since the Quandl Wiki data covers the NASDAQ 100 tickers for
    the relevant period, we extract the split and dividend adjustments from that bundle's
    SQLite database.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: The result is an HDF5 store containing price and volume data on some 135 tickers,
    as well as the corresponding meta and adjustment data. The script `algoseek_preprocessing.py`
    illustrates this process.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Writing your custom bundle ingest function
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Zipline documentation outlines the required parameters for an `ingest()`
    function, which kicks off the I/O process, but does not provide a lot of practical
    detail. The script `algoseek_1min_trades.py` shows how to get this part to work
    for minute data.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: There is a `load_equities()` function that provides the metadata, a `ticker_generator()`
    function that feeds symbols to a `data_generator()`, which, in turn, loads and
    format each symbol's market data, and an `algoseek_to_bundle()` function, which
    integrates all the pieces and returns the desired `ingest()` function.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Time zone alignment matters because Zipline translates all data series to UTC;
    we add US/Eastern time zone information to the OHCLV data and convert it to UTC.
    To facilitate execution, we create symlinks for this script and the `algoseek.h5`
    data in the `custom_data` folder in the `.zipline` directory, which we'll add
    to the `PATH` in the next step so Zipline can find this information.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Registering your bundle
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we can run `zipline ingest -b algoseek`, we need to register our custom
    bundle so Zipline knows what we are talking about. To this end, we'll add the
    following lines to an `extension.py` script in the `.zipline` file, which you
    may have to create first, alongside some inputs and settings (see the `extension.py`
    example).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 'The registration itself is fairly straightforward but highlights a few important
    details. First, Zipline needs to be able to import the `algoseek_to_bundle()`
    function, so its location needs to be on the search path, for example, by using
    `sys.path.append()`. Second, we reference a custom calendar that we will create
    and register in the next step. Third, we need to inform Zipline that our trading
    days are longer than the default 6 and a half hours of NYSE days to avoid misalignments:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Creating and registering a custom TradingCalendar
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As mentioned previously, Quantopian also provides a TradingCalendar library
    to support trading around the world. The package contains numerous examples, and
    it is fairly straightforward to subclass one of the examples. Based on the NYSE
    calendar, we only need to override the open/close times and change the name:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We put the definition into `extension.py` and add the following registration:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: And now, we can refer to this trading calendar to ensure a backtest includes
    off-market hour activity.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: The Pipeline API – backtesting an ML signal
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Pipeline API facilitates the definition and computation of alpha factors
    for a cross-section of securities from historical data. Pipeline significantly
    improves efficiency because it optimizes computations over the entire backtest
    period, rather than tackling each event separately. In other words, it continues
    to follow an event-driven architecture but vectorizes the computation of factors
    where possible.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: A pipeline uses factors, filters, and classifiers classes to define computations
    that produce columns in a table with point-in-time values for a set of securities.
    Factors take one or more input arrays of historical bar data and produce one or
    more outputs for each security. There are numerous built-in factors, and you can
    also design your own `CustomFactor` computations.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram depicts how loading the data using `DataFrameLoader`,
    computing the predictive `MLSignal` using the Pipeline API, and various scheduled
    activities integrate with the overall trading algorithm that''s executed via the
    `run_algorithm()` function. We''ll go over the details and the corresponding code
    in this section:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_08_06.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.6: ML signal backtest using Zipline''s Pipeline API'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: You need to register your pipeline with the `initialize()` method and execute
    it at each time step or on a custom schedule. Zipline provides numerous built-in
    computations, such as moving averages or Bollinger Bands, that can be used to
    quickly compute standard factors, but it also allows for the creation of custom
    factors, as we will illustrate next.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Most importantly, the Pipeline API renders alpha factor research modular because
    it separates the alpha factor computation from the remainder of the algorithm,
    including the placement and execution of trade orders and the bookkeeping of portfolio
    holdings, values, and so on.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: We'll now illustrate how to load the lasso model daily return predictions, together
    with price data for our universe, into a pipeline and use a `CustomFactor` to
    select the top and bottom 10 predictions as long and short positions, respectively.
    The notebook `backtesting_with_zipline` contains the following code examples.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Our goal is to combine the daily return predictions with the OHCLV data from
    our Quandl bundle, and then to go long on up to 10 equities with the highest predicted
    returns and short on those with the lowest predicted returns, requiring at least
    five stocks on either side, similar to the backtrader example above.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Enabling the DataFrameLoader for our Pipeline
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, we load our predictions for the 2015-17 period and extract the Zipline
    IDs for the ~250 stocks in our universe during this period using the `bundle.asset_finder.lookup_symbols()`
    method, as shown in the following code:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To make the predictions available to the Pipeline API, we need to define a
    `Column` with a suitable data type for a `DataSet` with an appropriate `domain`,
    like so:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'While the bundle''s OHLCV data can rely on the built-in `USEquityPricingLoader`,
    we need to define our own `DataFrameLoader`, as follows:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In fact, we need to slightly modify the Zipline library's source code to bypass
    the assumption that we will only load price data. To this end, we add a `custom_loader`
    parameter to the `run_algorithm` method and ensure that this loader is used when
    the pipeline needs one of SignalData's `Column` instances.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Creating a pipeline with a custom ML factor
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our pipeline is going to have two Boolean columns that identify the assets
    we would like to trade as long and short positions. To get there, we first define
    a `CustomFactor` called `MLSignal` that just receives the current return predictions.
    The motivation is to allow us to use some of the convenient `Factor` methods designed
    to rank and filter securities:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, we can set up our actual pipeline by instantiating `CustomFactor`, which
    requires no arguments other than the defaults provided. We combine its `top()`
    and `bottom()` methods with a filter to select the highest positive and lowest
    negative predictions:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The next step is to initialize our algorithm by defining a few context variables,
    setting transaction cost parameters, performing schedule rebalancing and logging,
    and attaching our pipeline:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Every day before the market opens, we run our pipeline to obtain the latest
    predictions:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'After the market opens, we place orders for our long and short targets and
    close all other positions:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, we are ready to execute our backtest and pass the results to pyfolio:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '*Figure 8.7* shows the plots for the strategy''s cumulative returns (left panel)
    and the rolling Sharpe ratio, which are comparable to the previous backtrader
    example.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 'The backtest only takes around half the time, though:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_08_07.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.7: Zipline backtest results'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: The notebook `backtesting_with_zipline` contains the full pyfolio tearsheet
    with additional metrics and plots.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: How to train a model during the backtest
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can also integrate the model training into our backtest. You can find the
    code for the following end-to-end example of our ML4T workflow in the `ml4t_with_zipline`
    notebook:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_08_08.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.8: Flowchart of Zipline backtest with model training'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: The goal is to roughly replicate the ridge regression daily return predictions
    we used earlier and generated in *Chapter 7,* *Linear Models – From Risk Factors
    to Return Forecasts*. We will, however, use a few additional pipeline factors
    to illustrate their usage. The principal new element is a `CustomFactor` that
    receives features and returns them as inputs to train a model and produce predictions.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the features – how to define pipeline factors
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To create a **pipeline factor**, we need one or more input variables, a `window_length`
    that indicates the number of most recent data points for each input and security,
    and the computation we want to conduct.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: 'A linear price trend that we estimate using linear regression (see *Chapter
    7,* *Linear Models – From Risk Factors to Return Forecasts*) works as follows:
    we use the 252 latest close prices to compute the regression coefficient on a
    linear time trend:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We will use 10 custom and built-in factors as features for our model to capture
    risk factors like momentum and volatility (see notebook `ml4t_with_zipline` for
    details). Next, we'll come up with a `CustomFactor` that trains our model.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: How to design a custom ML factor
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our `CustomFactor`, called `ML`, will have `StandardScaler` and a **stochastic
    gradient descent** (**SGD**) implementation of ridge regression as instance attributes,
    and we will train the model 3 days a week:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The `compute` method generates predictions (addressing potential missing values),
    but first checks if the model should be trained:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The `_train_model` method is the centerpiece of the puzzle. It shifts the returns
    and aligns the resulting forward returns with the factor features, removing missing
    values in the process. It scales the remaining data points and trains the linear
    `SGDRegressor`:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The `make_ml_pipeline()` function preprocesses and combines the outcome, feature,
    and model parts into a pipeline with a column for predictions:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Tracking model performance during a backtest
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We obtain new predictions using the `before_trading_start()` function, which
    runs every morning before the market opens:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '`evaluate_predictions` does exactly this: it tracks the past predictions of
    our model and evaluates them once returns for the relevant time horizon materialize
    (in our example, the next day):'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We also record the evaluation on a daily basis so we can review it after the
    backtest:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_08_09.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.9: Model out-of-sample performance'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: 'The following plots summarize the backtest performance in terms of the cumulative
    returns and the rolling SR. The results have improved relative to the previous
    example (due to a different feature set), yet the model still underperforms the
    benchmark since mid-2016:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15439_08_10.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.10: Zipline backtest performance with model training'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: Please see the notebook for additional details on how we define a universe,
    run the backtest, and rebalance and analyze the results using pyfolio.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: Instead of how to use
  id: totrans-300
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The notebook `ml4t_quantopian` contains an example of how to backtest a strategy
    that uses a simple ML model in the Quantopian research environment. The key benefit
    of using Zipline in the Quantopian cloud is access to many additional datasets,
    including fundamental and alternative data. See the notebook for more details
    on the various factors that we can derive in this context.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we took a much closer look at how backtesting works, what challenges
    there are, and how to manage them. We demonstrated how to use the two popular
    backtesting libraries, backtrader and Zipline.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: Most importantly, however, we walked through the end-to-end process of designing
    and testing an ML model, showed you how to implement trading logic that acts on
    the signals provided by the model's predictions, and saw how to conduct and evaluate
    backtests. Now, we are ready to continue exploring a much broader and more sophisticated
    array of ML models than the linear regressions we started with.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will cover how to incorporate the time dimension into our models.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
