- en: Programming with Libraries and Other Languages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers other GPU programming methods—programming with GPU accelerated
    libraries and other languages. Programming using the GPU accelerated libraries
    enables us to develop applications with the optimized kernels. Also, we can develop
    the CUDA software using other programming languages, which are aware of CUDA acceleration.
    Both ways improve programmability and productivity. Also, we don't have to spend
    our time optimizing the common operations, which are already optimized.
  prefs: []
  type: TYPE_NORMAL
- en: The CUDA Toolkit provides many GPU accelerated libraries in linear algebra,
    image and signal processing, and random processing. They are cuBLAS (Basic Linear
    Algebra Subroutines), cuFFT (Fast Fourier Transform), cuRAND (Random Number Generation), NPP
    (image and signal processing), cuSPARSE (Sparse Linear Algebra), nvGRAPH (Graph
    Analysis), cuSolver (LAPACK in GPU), Thrust (STL in CUDA), and so on. We also
    can write GPU accelerated programs with the OpenCV library. We will cover some
    of these libraries in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We also can use GPU accelerations using R, MATLAB, Octave, and Python. Nowadays,
    Python integration is popular and powerful, as GPU can accelerate many machine
    learning and data science tasks. We also cover these languages as an entry-level.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Linear algebra operation using cuBLAS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mixed-precision operation using cuBLAS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: cuRAND for parallel random number generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: cuFFT for Fast Fourier Transformation in GPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NPP for image and signal processing with GPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing GPU accelerated code in OpenCV
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing Python code that works with CUDA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NVBLAS for zero coding acceleration in Octave and R
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CUDA acceleration in MATLAB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear algebra operation using cuBLAS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The cuBLAS library is a GPU-optimized, standard implementation of **Basic Linear
    Algebra Subroutines** (**BLAS**). Using its APIs, the programmers can write GPU-optimized,
    compute-intensive code to a single GPU or multiple GPUs. There are three levels
    in cuBLAS. Level-1 performs the vector-vector operation, level-2 does the matrix-vector
    operation, and level-3 does the matrix-matrix operation.
  prefs: []
  type: TYPE_NORMAL
- en: Covering each level is out of the scope of this book. We are just focusing on
    how to use cuBLAS APIs and extend its performance for multiple GPUs. To be specific,
    this receipt will cover a **Single Precision Floating Matrix Multiplication** (**SGEMM**)
    operation—a level-3 operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The cuBLAS library is a part of CUDA Toolkit, so you can use cuBLAS without
    extra installation. Also, you can use the `cc` or `cpp` file extensions, rather
    than `.cu`, because you do not need to use CUDA-specific built-in keywords such
    as `__global__` or `threadIdx`. This following code snippet shows the basic application
    of the cuBLAS function (`cubalsSgemm`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, cuBLAS APIs work with the `cublasHandle_t` type handle.
  prefs: []
  type: TYPE_NORMAL
- en: cuBLAS SGEMM operation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The GEMM operation can be denoted by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e837544a-a0b8-4716-9c9d-0943d86de399.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where *alpha* and *beta* are scalas and *A*, *B*, and *C *are matrices in column-major
    format. This matches with the cuBLAS function interface in the following box:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Before using this GEMM function, let''s look at the details of the parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`transa` and `transb`: Instruction to the cuBLAS functions whether the matrices
    *A* and *B* should be transposed or not for the operation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`m`, `n`, and `k`: The dimensional size of the matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`alpha` and `beta`: Parameters that determine how to configure the output value
    from the source.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`*A`, `*B`, and `*C`: Linear buffer for the matrix data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lda`: Leading column dimension of matrix *A*. cuBLAS aligns the matrix elements
    with this value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ldb`: Leading column dimension of matrix *B*. cuBLAS aligns the matrix elements
    with this value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To transfer the data between the host and the device, you can use cuBLAS's `cublasSetMatrix()`
    and `cublasGetMatrix()` helper functions. They are wrapper functions of `cudaMemcpy()`,
    but have the matrices' dimensional information, so that they help to enhance the
    code readability; you can simply use `cudaMemcpy()` instead, of course.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s just implement an application that has the GEMM operation using cuBLAS
    SGEMM function. We will include `cublas_v2.h` to use the updated cuBLAS API. For
    convenience, we will use the `getMatrix()` function to get a randomly generated
    matrix from the given dimension, and the `printMatrix()` function to print the
    matrix elements. The codes are implemented in the given example codes. In the
    main function, we will initialize three matrices—`A`, `B`, and `C`—from the given
    `M`, `N`, and `K`. Then we will compute `cublasSgemm()` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Compile the code with `nvcc` by linking the cuBLAS library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code snippet shows the output of the execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As described in the `cublasSgemm()` function call, matrices *A* and *B* are
    transposed matrices. We passed the original leading column size to the `cublasSgemm()`
    function as `lda`, `ldb`, and `ldc`, and we could see that the operation works
    as expected.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-GPU operation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The cuBLAS library''s cuBLAS-XT API provides cuBLAS''s level-3 operation when
    it is working on multiple GPUs. With this API, your application can use multi-GPU computing
    operation. This snippet shows the basic operation for using cuBLAS-XT:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The `cublasXtSgemm()` interface is the same as the `cublasSgemm()` function,
    so we can use multiple GPU''s computing performances at ease. For example, we
    can obtain the following result using the sample code in the repository with two
    GPUs. This performance can vary depending on your GPU and system configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d2ce6a4a-c2c3-4fe3-a141-2416f6ce45de.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The cuBLAS library provides a lot of versatile linear algebra operations. So
    you should check how your necessary function is provided in the library. Also,
    you will need an example of how to use that function. The following items are
    the links to the document and examples. So, it is recommended that you check both
    documents frequently when you need to implement an application based on cuBLAS:'
  prefs: []
  type: TYPE_NORMAL
- en: NVIDIA's cuBLAS programming guide—A reference guide: [https://docs.nvidia.com/cuda/cublas/index.html](https://docs.nvidia.com/cuda/cublas/index.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <q>Matrix computations on the GPU</q>*: CUBLAS, CUSOLVER, and MAGMA by example*
    by Andrzej Chrzȩszczyk and Jacob Anders: [https://developer.nvidia.com/sites/default/files/akamai/cuda/files/Misc/mygpu.pdf](https://developer.nvidia.com/sites/default/files/akamai/cuda/files/Misc/mygpu.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mixed-precision operation using cuBLAS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The cuBLAS library supports mixed-precision computation. This computation means
    an operation that operates with different precisions, for instance, computation
    with single and half-precision variables, or with single and characters (`INT8`).
    This technique is useful when we need to achieve a higher performance using lowered
    precision, while also obtaining a higher accuracy in the result.
  prefs: []
  type: TYPE_NORMAL
- en: 'The cuBLAS library provides `cublasGemmEx()` and `cublas{S/C}gemmEx()` to support
    GEMM operation for the mixed-precision operations. They are extensions of `cublas<t>gemm()`,
    which accepts specified data types for each *A*, *B*, and *C* matrices. The following
    table shows the precision support matrix for `cublasGemmEx()`, and other replaceable
    APIs in cuBLAS library:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Compute type | A type / B type | C type | Replaceable APIs |'
  prefs: []
  type: TYPE_TB
- en: '| `CUDA_R_16F` | `CUDA_R_16F` | `CUDA_R_16F` | `cublasHgemm()` |'
  prefs: []
  type: TYPE_TB
- en: '| `CUDA_R_32I` | `CUDA_R_8I` | `CUDA_R_32I` | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| `CUDA_R_32F` | `CUDA_R_16F` | `CUDA_R_16F` | `cublasSgemmEx()` |'
  prefs: []
  type: TYPE_TB
- en: '| `CUDA_R_8I` | `CUDA_R_32F` |'
  prefs: []
  type: TYPE_TB
- en: '| `CUDA_R_16F` | `CUDA_R_32F` |'
  prefs: []
  type: TYPE_TB
- en: '| `CUDA_R_32F` | `CUDA_R_32F` |'
  prefs: []
  type: TYPE_TB
- en: '| `CUDA_R_64F` | `CUDA_R_64F` | `CUDA_R_64F` | `cublasDgemm()` |'
  prefs: []
  type: TYPE_TB
- en: '| `CUDA_C_32F` | `CUDA_C_8I` | `CUDA_C_32F` | `cublasCgemmEx()` |'
  prefs: []
  type: TYPE_TB
- en: '| `CUDA_C_32F` | `CUDA_C_32F` |'
  prefs: []
  type: TYPE_TB
- en: '| `CUDA_C_64F` | `CUDA_C_64F` | `CUDA_C_64F` | `cublasZgemm()` |'
  prefs: []
  type: TYPE_TB
- en: You can see that `cublasGemmEx()` can cover the `cublas{S/C}gemmEx()` function's
    operation. Therefore, we will cover `cublasGemmEx()` in this section.
  prefs: []
  type: TYPE_NORMAL
- en: The last parameter of the `cublasGemmEx()` function, `cublasGemmAlgo_t`, specifies
    the algorithm for matrix-matrix multiplication. With this parameter, we can choose
    whether to use TensorCore or not. `CUBLAS_GEMM_DEFAULT` selects the GEMM algorithm
    and runs on CUDA cores. On the other hand, `CUBLAS_GEMM_DEFAULT_TENSOR_OP` selects
    algorithms that use tensor cores. If TensorCore is unavailable for the given condition,
    cuBLAS selects an algorithm that uses CUDA cores. This condition can be for the
    GPUs having no tensor cores or matrix size, which does not fit with how the TensorCore
    operates—in multiples of four (** 4*).
  prefs: []
  type: TYPE_NORMAL
- en: GEMM with mixed precision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s try the mixed-precision using the cuBLAS GEMM operation. After
    the implementation, we will cover how the matrix size can affect the operations.
    The fully implemented version is in `02_sgemm_mixed_precision/cublasGemmEx.cu`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This code uses a custom memory managing class, `CBuffer`, to ease the handling
    of mixed precisions and copy, but it can use unified memory instead. For the cuBLAS
    operation, we should include `cublas_v2.h` in our code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s implement the `main()` function. First, we will create and initialize
    `A`, `B`, and `C` matrices. The following snippet shows how to use the `CBuffer` class
    and initialize the matrices:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'To specify the precision types of `A`, `B`, and `C`, and to test the various
    precisions together, we need to specify some CUDA data type parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'For the cuBLAS operation, we should initialize `cublas_handle`, `alpha`, and
    `beta`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we copy the data to the GPU:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We then call `cublasGemmEx()` function as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'To review the matrix values, we can use `printMatrix()`, which is defined in `helper.h`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The `printMatrix()` is defined using the function overriding method to allow
    the printing of half-precision values with the same format in other data types.
    Part of the definitions is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, the code will have a GEMM operation to the given `A`, `B`, and `C` matrices.
    The following shows an example of the output when `M` is `4`, `N` is `5`, and
    `M` is `6`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s try with the other data types and see how `cublasGemmEx()` operates
    to the given matrices. The provided example also outputs the operation''s execution
    time to measure the performance:'
  prefs: []
  type: TYPE_NORMAL
- en: What should we modify if matrix *A* or matrix *B* is the transposed matrix?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is there any preferable matrix size to the operation? Compare the execution
    time by changing the size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is there any preferable matrix size for each data type? If you try the `INT8`
    precision, you will see errors. How this can be fixed? Change the size and see
    how the `INT8` operation can be supported in `cublasGemmEx()`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GEMM with TensorCore
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorCore provides the accelerated performance of tensor's dot operations.
    It supports FP16 in the Volta architecture, and `INT8` and `INT4` in the Turing
    architecture. Therefore, we should use reduced precision or mixed precision to
    use TensorCore.
  prefs: []
  type: TYPE_NORMAL
- en: Previously we used `CUBLAS_GEMM_DEFAULT` as the cuBLAS GEMM algorithm, which
    uses CUDA cores in their operation. To use TensorCore, we should use `CUBLAS_GEMM_DEFAULT_TENSOR_OP`.
    To utilize the TensorCore, each dimension of your operand matrices should be a
    multiple of 4\. That is the unit size of TensorCore's **WMMA** (short for, **Warp
    Matrix Multiply Accumulate**) operation optimization internally. For instance,
    matrix-matrix multiplication with *A* (8,192 × 8,192) and *B* (8,192 × 8,192)
    shows a much higher performance against the operation with *A* (8,192 × 8,192)
    and *B* (8,192 × 8,190). You can also confirm this operation via a profile.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following timeline is a result of a matrix multiplication using matrix
    *A* (8,192 × 8,192) and matrix *B* (8,192 × 8,190):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ceedb286-f967-4a78-b6d1-31340f23ae90.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Furthermore, this timeline image is a result of a matrix multiplication from
    matrix *A* (8,192 × 8,192) and matrix *B* (8,192 × 8,192):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3430e8a9-267e-4233-9876-ee4deb8c06af.png)'
  prefs: []
  type: TYPE_IMG
- en: Both tests use `CUBLAS_GEMM_DEFAULT_TENSOR_OP` in CUDA C/C++, but the GEMM operation
    with TensorCore is 6.7x faster than with CUDA cores. As TensorCore is available
    based on the matrix size, `nvcc` compiles the code with the special kernel functions,
    starting with `volta_s884g`. In conclusion, pad your matrices to align with 4,
    if you want to get benefits of TensorCore. This can be an overhead, but performance
    gain from TensorCore may overwhelm the overhead.
  prefs: []
  type: TYPE_NORMAL
- en: NVIDIA provides how-to programming TensorCores using the cuBLAS library in their
    development blog site ([https://devblogs.nvidia.com/programming-tensor-cores-cuda-9](https://devblogs.nvidia.com/programming-tensor-cores-cuda-9)).
    This document also introduces other available methods. But, using the cuBLAS library
    provides the fastest performance for you, as proven following a paper from Oak
    Ridge National Laboratory—*NVIDIA Tensor Core Programmability, **Performance and
    Precision* ([https://arxiv.org/pdf/1803.04014.pdf](https://arxiv.org/pdf/1803.04014.pdf)).
  prefs: []
  type: TYPE_NORMAL
- en: cuRAND for parallel random number generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many applications use the pseudo-random number for their simulation or probabilistic
    analysis. In spite of its conventional usages, a large number of random number
    generation procedure took much time. One solution is to generate random numbers
    in parallel, but each multiple thread should have different random seeds in order
    to generate random numbers independently.
  prefs: []
  type: TYPE_NORMAL
- en: The cuRAND library enables GPU to generate a number of random numbers from GPU.
    This library is available from the host or from the device code. The host API
    enables the generation of random numbers only using the host code. Therefore,
    you can use the generated data directly for other kernel functions. The device
    API enables the generation of random numbers in kernel code, so you can make CUDA
    threads that have their own randomly generated numbers during the execution.
  prefs: []
  type: TYPE_NORMAL
- en: cuRAND host API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, you need to create a new generator of the desired type, using `curandGenerator()`.
    Then, set the generator options of the desired seed and order. For example, you
    can generate the pseudo-random generator using `curandSetPseudoRandomGeneratorSeed()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, you can generate random numbers using `curandGenerate()`. There are nine
    different generation functions. For example, you can generate uniformly distributed
    floating-point values using `curandGenerateUnifrom()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The cuRAND programming guide provides descriptions of various kinds of generation
    functions: [https://docs.nvidia.com/cuda/curand/host-api-overview.html#generation-functions](https://docs.nvidia.com/cuda/curand/host-api-overview.html#generation-functions).'
  prefs: []
  type: TYPE_NORMAL
- en: 'After the use of random number generation, you can terminate the cuRAND generator
    with the destroyer function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's implement an application that generates random numbers using several
    cuRAND APIs. The fully implemented version is `03_curand/curand_host.cpp`. So,
    you can modify the code and test other functions as you need.
  prefs: []
  type: TYPE_NORMAL
- en: 'At first, we should include `curand.h` for the cuRAND host APIs and other CPP-related
    header files as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s assume that we will create a matrix that is initialized with random
    numbers. We need to implement the `printMatrix()` function in order to review
    the generated random numbers as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will allocate the required memory space as follows. Now, we will implement
    the `main()` function that initializes random numbers and prints the result using
    `printMatrix()`. First, we will initialize the cuRAND handle for the operation
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'You can change the random seed as you want. The next thing is to allocate memory
    space. To ease the evaluation of the operation, we will use a unified memory,
    because cuRAND functions will generate random numbers on GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will generate random numbers to the given memory space. We will use
    the integer memory space (`np_random`) for random number generation, and floating
    memory space (`fp_random`) for uniformly distributed random numbers, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Because we are using a unified memory, we can allow the GPU and the host to
    share the same memory address, and we can review the output values by synchronizing
    them. Finally, we can terminate the cuRAND handle and memories as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, its time to compile and run the code. Compiling the code using cuRAND
    APIs should provide `-lcurand` for the `nvcc` compiler. When `M = 3` and `N =
    5`, the outputs are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: We have covered how CUDA can generate random numbers using the host API, but,
    in some cases, it is better to design the CUDA kernels to generate random numbers.
    We call this the device API, and we can obtain random numbers from each CUDA thread.
  prefs: []
  type: TYPE_NORMAL
- en: cuRAND device API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using the device API, we can set the generator seed and generate random numbers
    on your CUDA device.
  prefs: []
  type: TYPE_NORMAL
- en: 'Firstly, we need to prepare a device memory space of `curandState_t`, in order
    to store generator seeds to provide the random seed to the CUDA threads in parallel.
    This can be done like a normal device memory allocation code as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'In your kernel code, we need to initialize random seeds using `curand_init()`.
    This function requires seed, sequence number, and offset. Then, this function
    sets up the state. For the same seed, cuFFT always generates the same state. To
    generate random values, use the `curand()` function. Like the host''s generation
    function, the device API has various generation functions. For example, uniformly
    distributed random number generation can be done like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The cuRAND library provides various generation functions for the various data
    types and stochastic distributions. To find your desired generation function,
    check the cuRAND developer guide''s device API overview. After the random number
    generation, the device states buffer should be terminated like normal memory,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will create an application that uses cuRAND device APIs. The fully
    implemented codes are `curand_device.cu`, so you can modify and test the code
    too. Firstly, we should include the `curand_kernel.h` file with other C++ required
    header files as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We will write `setup_kernel()` that initializes a random seed for each CUDA
    thread as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Write two random number generation functions: `generate_kernel()` and `generate_uniform_kernel()`.
    We will generate a 32-bit integer and a single floating point with uniformly distributed
    random numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will implement the `main()` function and initialize the device states
    buffer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, generate random numbers using `generate_kernel()`. For convenience, we
    will use unified memory for space and validate the output from the host. After
    that, we will print out the result as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'In the same way, we will create uniformly distributed random numbers using
    `generate_uniform_kernel()` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Because we are using unified memory, we can allow the GPU and the host to share
    the same memory address, and we can review the output values by synchronizing
    them. Finally, we can terminate the cuRAND handle and memories as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, its time to compile and run the code. In order to compile the code using
    cuRAND, APIs should provide `-lcurand` for the `nvcc` compiler. When `M` equals
    `3` and `N` equals `5`, the outputs are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: When you compare the output numbers from the host API and the device API, the
    generated random numbers are the same, whereas the uniform random numbers are
    not. This can be resolved if you reset the random seed ahead of the second random
    number generation.
  prefs: []
  type: TYPE_NORMAL
- en: cuRAND with mixed precision cuBLAS GEMM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Previously, we have used the C++ random number generator to initialize matrices
    for a GEMM operation. This function is handy when we want to generate random numbers
    in general. However, you may find that this function took a long time to generate
    large random numbers in the last section. In this section, we will cover how cuRAND
    API can work with the cuBLAS GEMM operations. The fully implemented version is
    the `gemm_with_curand_host.cpp` file. Let''s see how this was implemented:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently, we don''t have a low-precision random number generator in the cuRAND
    library. Also, we need to convert the half-precision numbers to float in order
    to evaluate the output. For these reasons, we need to create type conversion functions
    on GPU as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will write a random number generation function that uses the cuRAND
    host API. As we discussed before, we should convert the generated random numbers
    from float to half, when we need to use half-precision data. This function can
    be implemented as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Define some local variables that control GEMM operations in the `main()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: In this code, we determine the GEMM operation size, data type, and operation
    type.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s create input buffer arrays, and set parameters, along with the
    operation precision:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Create cuRAND and cuBLAS handles as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we should determine the operation type in order to use TensorCores:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can call the `cublasGemmEx()` function that affords FP32 and FP16
    operations as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The GEMM operation should show a similar performance when compared to the previous
    version. But, you may find that the whole application speed is enhanced, since
    the parallel random number generation on the GPU is much faster than the generation
    from the host.
  prefs: []
  type: TYPE_NORMAL
- en: The cuRAND developer guide will help you to find other random number generators,
    options, and distributions. This document is located at [https://docs.nvidia.com/pdf/CURAND_Library.pdf](https://docs.nvidia.com/pdf/CURAND_Library.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: cuFFT for Fast Fourier Transformation in GPU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The cuFFT library provides GPU accelerated operations for the **FFT** (short
    for, **Fast Fourier Transform**) algorithm. The programmers can transform real
    or complex data using GPU computing power, and apply GPU kernel operations for
    the transformed signal. Also, the supported functions are matched with the FFTW
    library, so we can migrate the host project to the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: To handle the FFT sample's dimensional information, cuFFT is required to create
    a plan handle using `cufftPlan1D()`, `cufftPlan2D()`, or `cufftPlan3D()`, accordingly.
    If sample data has a batched and stride layout, we should use `cufftPlanMany()`.
    If the sample size is greater than 4 GB, we should use `64` as a suffix to the
    plan functions to support that size. For example, `cufftPlanMany64()` supports
    larger samples on top of the `cufftPlanMany()` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The cuFFT library supports multi-GPU operations. First, you need to create
    an empty plan using `cufftCreate()`. Then, we can specify the list of GPUs that
    will carry out the operation using `cufftXtSetGPUs()`. After that, we can generate
    a plan using normal plan generation functions, which we have previously covered.
    The following table shows the plan generation function categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Basic plans | Multi-GPU plans |'
  prefs: []
  type: TYPE_TB
- en: '| Simple plan | `cufftPlan{1d,2d,3d}()` | `cufftMakePlan{1d,2d,3d}()` |'
  prefs: []
  type: TYPE_TB
- en: '| Advanced data layout | `cufftPlanMany()` | `cufftMakePlanMany()` |'
  prefs: []
  type: TYPE_TB
- en: '| FP16 operation |  `cufftXtMakePlanMany()` |'
  prefs: []
  type: TYPE_TB
- en: 'Then you can forward (FFT) and inverse (IFFT) to your sample data using the
    `cufftExec()` function. The cuFFT library provides three kinds of data transformation:
    complex-to-complex, real-to-complex, and complex-to-real. Its operation data type
    can be a float or a double:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Transform direction | Float | Double |'
  prefs: []
  type: TYPE_TB
- en: '| Complex-to-complex | `cufftExecC2C()` `cufftXtExecDescriptorC2C()` | `cufftExecZ2Z()`
    `cufftXtExecDescriptorZ2Z()` |'
  prefs: []
  type: TYPE_TB
- en: '| Real-to-complex | `cufftDExecR2C()` `cufftXtExecDescriptorR2C()` | `cufftExecD2Z()`
    `cufftXtExecDescriptorD2Z()` |'
  prefs: []
  type: TYPE_TB
- en: '| Complex-to-real | `cufftExecC2R()` `cufftXtExecDescriptorC2R()` | `cufftExecZ2D()`
    `cufftXtExecDesciptorZ2D()` |'
  prefs: []
  type: TYPE_TB
- en: '| All | `cufftXtExec()` / `cufftXtExecDesciptor()` |'
  prefs: []
  type: TYPE_TB
- en: The cuFFT operation is either *forward* or *inverse*, and the operation should
    be paired with the other direction.
  prefs: []
  type: TYPE_NORMAL
- en: The functions that transform between the real data and the complex data, such
    as `R2C` and `C2R`, have implicit directional information in their function name.
    This feature helps you to avoid having to have an additional operation in order
    to convert your data in the real domain to a complex data type. Meanwhile, you
    have to create an additional plan since each plan has transformation direction
    information.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, you have to provide the transform direction information for
    the complex-to-complex transformation, such as `C2C` and `Z2Z`. For the inversion
    operation, you don't have to create another cuFFT handle, because the plan should
    be the same data type operation.
  prefs: []
  type: TYPE_NORMAL
- en: The `cufftXtExec()` and `cufftXtExecDescriptor()` functions can perform transformation
    on any given data type, since every input data should be provided with their data
    type information when you create a cuFFT plan.
  prefs: []
  type: TYPE_NORMAL
- en: Basic usage of cuFFT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now let''s try to use cuFFT. The fully implemented version is the `04_cufft/cufft.1d.cpp`
    file. Let''s discuss how this is implemented:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Firstly, start with some header files: C++, CUDA, cuRAND, and cuFFT:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'In this FFT operation, we will have both real-to-complex and complex-to-real
    transformations. Therefore, let''s declare some custom data types, `Real` and
    `Complex`, in order to simplify the code. This can be done as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s begin with the `main()` function. For the input sample data, we
    will use unified memory in order to ease the data transfer between the host and
    the GPU. The transformed data may only be used on the GPU. Therefore, memory space
    can be allocated as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will use the cuRAND host API to initialize the input data as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'And, we should initialize the cuFFT plan for forward and inverse transforms.
    Since they have different data types we should create two plans, respectively,
    for real-to-complex and complex-to-real transformations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can have forward or inverse transforms, with the given cuFFT plans.
    In order to measure the execution time, we can embrace these operations using
    CUDA events:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can compile the code with the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The `cufft.1d` command will report its transform time for each step as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: cuFFT with mixed precision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The cuFFT library provides extended CUDA computing features, such as the FP16
    FFT operation. The full version is the `cufft.half.cpp` file. Let's discuss its
    implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this code, we should use `cufftXtMakePlanMany()` for the plan creation and
    the `cufftXtExec()` function for the transformation. `cufftXtMakePlanMany()` allows
    the passing of input and output data types if they are FP16 or FP32\. Also, we
    should create two plans for forward and inverse transformation, in order to cover
    real-to-complex and complex-to-real transformations. To an empty cuFFT plan, `cufftXtMakePlanMany()` can
    specify the sample size, the input data format and type, the batch size, and so
    on. For example, plan creations can be implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'In this implementation, we also have to consider whether to provide the input
    data in half-precision. You may use the host random function and convert them
    into half-precision data, but, this code shows you how the cuRAND host API can
    be used for this purpose, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'So, we can provide half-precision on uniformly distributed random numbers for
    FFT, and we can use `cufftXtExec()` for the forward and inverse transformations.
    Transformation performance is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: cuFFT for multi-GPU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another usage of cuFFT is having a large FFT operation using multiple GPUs.
    To do this, we have to create an empty cuFFT plan using `cufftCreate()`, and provide
    GPU numbers using `cufftXtSetGPUs()`. For example, this can be done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The total number of GPUs can vary depending on the system. Now, we can generate
    the cuFFT plan using `cufftXtMakePlanMany()` to specify the sample information.
    For instance, `cufftXtMakePlanMany()` can be called like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The cuFFT library provides `cufftXtMalloc()`, which prepares the GPU memory
    space for the target GPUs. Then, we can copy our data to the allocated memory
    using the `cufftXtMemcpy()` function. For example, this can be implemented as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can execute FFT on multi-GPUs with the `cufftXtExecDesciptor()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Using `nvidia-smi`, we can monitor the distributed memory allocation and execution
    across the GPUs. The elapsed time can be different, depending on your GPUs and
    system configuration.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to learn more about the cuFFT library and its functions, the cuFFT
    library user guide ([https://docs.nvidia.com/cuda/cufft/index.html](https://docs.nvidia.com/cuda/cufft/index.html))
    is a good reference for you.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA sample code is another good reference for learning how to use cuFFT functions.
    The sample codes are placed in the `NVIDIA_CUDA-10.x_Samples/7_CUDALibraries/CUFFT*`
    directory. You can learn how to apply filter operations using CUDA kernel code,
    and by working with cuFFT's forward/backward transformations.
  prefs: []
  type: TYPE_NORMAL
- en: NPP for image and signal processing with GPU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **NPP** (short for, **NVIDIA Performance Primitive**) library is a default
    CUDA library with a set of GPU accelerated processing functions that focus on
    imaging and video processing. While it enables flexible development in these fields,
    the developers can save their application development time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The NPP library has two functional parts: imaging-processing APIs, and signal-processing
    APIs. The image-processing APIs include tools relating to image filtering, compression/decompression,
    color transformation, resizing, color conversion, statistical operations, and
    so on. The signal-processing APIs are filtering, conversion, and so on. You can
    visit the NPP''s document ([https://docs.nvidia.com/cuda/npp](https://docs.nvidia.com/cuda/npp)),
    and see its configurations and the full list of functionalities.'
  prefs: []
  type: TYPE_NORMAL
- en: CUDA provides many NPP-based samples. In this section, we will cover the basic
    use of the NPP library and discuss its application.
  prefs: []
  type: TYPE_NORMAL
- en: Image processing with NPP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we will cover how the NPP library can ease an image-processing task.
    Before doing this, we should install the FreeImage library in order to be able
    to load and write a JPEG compressed image file easily. There are three options
    that can be used to prepare the library:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Installation from the Ubuntu archive:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Build from the source code and install:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Use the library that has already been installed with the CUDA Toolkit. An NPP
    sample code, `7_CUDALibraries/freeImageInteropNPP`, in CUDA sample code uses the
    FreeImage library. For this sample, NPP header files and library files are installed
    at `7_CUDALibrires/common/FreeImage` in the CUDA sample directory. You may use
    this if you prefer not to install other binaries into your machine.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, let''s implement the NPP-based image-processing application. The fully
    implemented code is `05_npp/imageFilter.cpp`. This file begins with the header
    files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'In this application, it has the `ImageInfo_t` structure to easily manage image
    information and data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Write the `LoadImage()` function in order to load a JPEG image. The `FreeImage`
    library supports any other image format, so you can try other images as you want.
    Then, we will fill the source image information managing structure with the loaded
    image data. The `loadImage()` function is implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, write some NPPI helper functions that provide the NPPI image size and
    the NPPI ROI size data from the image structure as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, let''s implement the NPPI-based image resizing function as follows. In
    this function, we will use `nppiResize_8u_C3R()`, which was discussed at the beginning. NPP
    APIs have naming convention rules to explicitly clarify their operation. Depending
    on their functional categories, their naming starts with `nppi` for the image
    processing, and `npps`for the signal processing. For instance, an NPP image-processing
    function, `nppiResize_8u_C3R()`, begins with the `nppi` prefix, and it resizes
    input data with an unsigned char data type in three channels to the given ROI
    (you can learn more detail about this convention in the document):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'To compare the performance with the CPU, we will use a FreeImage''s function,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s implement the `main()` function. At first, we should initialize
    the FreeImage library and load an image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will initialize the GPU memory space for the input image, as follows.
    In this procedure, we initialize the global memory space with an NPPI function
    and transfer the loaded image into the global memory using `cudaMemcpy2D()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, we will initialize the output memory space with the resized image
    size information as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we call the `ResizeGPU()` and `ResizeCPU()` functions, which we have
    implemented already. For each operation, we will use `cudaEvent` to measure the execution
    time on the GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'For verification, we will save the result to the file. To do this, we should
    create a FreeImage bitmap, and copy the resized image into the memory space. Then,
    we can save an output image, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, we can finally terminate the related resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Compile the code using `nvcc` with the linked NPP and FreeImage library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, when the scale factor is 0.5 f, the image size is reduced like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'The measured elapsed time is `0.04576 ms` using V100\. Its time can vary depending
    on the GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'For more detail on the use of NPP for image processing, visit and see the linked
    document: [http://on-demand.gputechconf.com/gtc/2014/presentations/HANDS-ON-LAB-S4793-image-processing-using-npp.pdf](http://on-demand.gputechconf.com/gtc/2014/presentations/HANDS-ON-LAB-S4793-image-processing-using-npp.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: Signal processing with NPP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: NPP also provides signal-processing features. The main difference to image-processing
    APIs is that they do not require image-shape-related information. As we continue
    to cover the basic usage of NPP functions, we will find out how we can obtain
    the sum, min/max, mean, and L2 normalized distribution value from the given arrays.
    The fully written code is `05_npp/statisticsNPP.cpp`.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s begin with the required header file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'As an input data, we will use randomly generated numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we call the statistical operation functions, we need a temporary memory
    space for their operations. We can obtain the required size using other NPP functions
    that are related to the operations, and we can create a common workspace memory
    space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s begin with the `main()` function. At first, we will begin with input
    data preparation, and getting to know the required workspace memory space. We
    will prepare two input data types, and compare their differences using NPP:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, we will allocate GPU memory space for the input/output and workspace.
    We will also transfer the input data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s do some simple statistical operations, using NPP functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'NPP also provides functions that report the differences between the two inputs
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we terminate the used memories. After that, let''s compile the code with
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, we obtain the result as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: Applications of NPP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we have covered the filtering in image processing, and statistical
    operations in signal processing. Although we have tried simple applications, we
    may find that NPP programming is much easier than kernel implementation. For this
    reason, NPP is applied to many media transcoding filters, bath image-processing
    applications, pre-processing of images in computer vision or deep learning, and
    so on.
  prefs: []
  type: TYPE_NORMAL
- en: Writing GPU accelerated code in OpenCV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The OpenCV library is quite a popular library in computer vision. It supports
    GPU programming in order to benefit performance at higher resolutions in the computer
    vision area. In this section, we will cover how to use a GPU with OpenGL.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA-enabled OpenCV installation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To start OpenCV programming with CUDA, you need to compile the OpenCV library
    with the CUDA feature enabled. Follow this to enable OpenCV in Ubuntu:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'If your system can use X window (not a server), install other packages to enable
    the GTK dialog:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'Download the source code and untar them using the following commands. This
    was tested with OpenCV, which were the latest OpenCV versions at the time of writing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s compile the downloaded source code using the following commands. You
    can put other options if you want. Its compilation takes a while:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'To confirm the installation use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'In OpenCV 4, CUDA-related functions and classes are defined in CUDA namespaces.
    For instance, you can create a CUDA global memory space using this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: Then, the device `cuda_mem` memory space can be handled like a normal CPU memory
    type (`cv::Mat`).
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a CUDA-enabled blur filter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we will implement a tiny GPU-enabled OpenCV application and compare its
    performance. Let''s begin by including the required header files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the host blur filter implementation using OpenCV:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'And this is the CUDA-enabled blur filter implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'This receipt code shows how the `bilateralFilter()` operation matches with
    the host, and how CUDA matches with the CUDA namespace. For the CUDA memory manipulation, `cv::cuda::GpuMat` is
    used for the device memory, and the device memory provides `upload()` and `download()` member
    functions, such as `cudaMemcpy()`. To measure the elapsed time, `cv::TickMeter` was
    used. Then, `main()` calls both implementations, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s compile the code. We should include the OpenCV header files and
    libraries using ``pkg-config --cflag opencv`` in your compilation option. For
    example, the compilation option can be written like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, the output result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: The execution time can be different depending on your system and the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling multi-stream processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In OpenCV, the CUDA stream is managed with `cv::cuda::Stream`. Using this,
    we can have multi-stream-based pipelining GPU operations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we know, the host memory should be a pinned memory in order to have asynchronous
    data transfer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will create multiple streams, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'And, we load the source image and initialize the GPU memory based on the loaded
    image information as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will transfer the image to the GPU, blur the image, and transfer it
    back to the host with each stream:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we have to synchronize the host and the GPU. To do this, we will use
    the `cv::Stream.waitForCompletion()` function that can synchronize for each stream
    after they finish the data transfer to the host:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'To compare the performance with the CPU, we also call `cv::bilateralFilter()`
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'Its execution time is as follows. The GPU execution time is the average of
    the measured time from the multi-stream execution loop to synchronization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to confirm the multi-stream operation, we can profile the operation.
    The following screenshot shows this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/aea5b6e4-ce11-4d84-9057-0c5466df0094.png)'
  prefs: []
  type: TYPE_IMG
- en: Profiling the operation
  prefs: []
  type: TYPE_NORMAL
- en: The first operation on the default stream is warp-up execution, and a four-multi-stream
    operation follows. Here, we can see that the GPU operations are overlapped. For
    this reason, the average execution time is shorter than for a one-stream execution.
  prefs: []
  type: TYPE_NORMAL
- en: We have only covered bilateral filtering in OpenCV. However, many OpenCV features
    support CUDA acceleration, so that you can get the benefits of GPU computing.
    Its interface is consistent with the CPU version, so you can easily migrate your
    CPU version to the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an introductory level, there are some useful materials from GTC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://on-demand.gputechconf.com/gtc/2013/webinar/opencv-gtc-express-shalini-gupta.pdf](http://on-demand.gputechconf.com/gtc/2013/webinar/opencv-gtc-express-shalini-gupta.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://on-demand.gputechconf.com/gtc/2013/webinar/gtc-express-itseez-opencv-webinar.pdf](http://on-demand.gputechconf.com/gtc/2013/webinar/gtc-express-itseez-opencv-webinar.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://developer.download.nvidia.com/GTC/PDF/1085_Fung.pdf](http://developer.download.nvidia.com/GTC/PDF/1085_Fung.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is recommended that you start with OpenCV's reference guide: [https://docs.opencv.org/4.1.1/d2/dbc/cuda_intro.html](https://docs.opencv.org/4.1.1/d2/dbc/cuda_intro.html).
  prefs: []
  type: TYPE_NORMAL
- en: Writing Python code that works with CUDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Nowadays, many people use CUDA with Python. It works not only as a glue of binaries,
    but it also enables to us write GPU accelerated code directly. As a glue language,
    Python can call the APIs from the CUDA C/C++ libraries, using `pybind11` ([https://github.com/pybind/pybind11](https://github.com/pybind/pybind11))
    or SWIG ([http://swig.org/](http://swig.org/)). However, we have to write CUDA
    C/C++ codes and integrate them into the Python application.
  prefs: []
  type: TYPE_NORMAL
- en: However, there are Python packages—Numba, CuPy, and PyCUDA—that enable GPU programming
    with Python. They provide native accelerated APIs and wrappers for CUDA kernels.
    In other words, we don't have to write C/C++ code and spend our time performing
    integration. Numba provides a vectorization and CUDA **just-in-time** (**jit**)
    compiler to accelerate its operation. It is compatible with NumPy, so you can
    accelerate your numerical computing code based on NumPy. You can also write flexible
    CUDA code in Python thanks to the jit compiler. CuPy is also NumPy compatible
    and accelerates linear algebra algorithms. It provides Pythonic programmability
    and transparent custom kernel programming, such as Numba. PyCUDA provides a CUDA
    C/C++ interface, so that you can write and use the CUDA kernel function in your
    Python code.
  prefs: []
  type: TYPE_NORMAL
- en: Numba – a high-performance Python compiler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Numba ([https://numba.pydata.org/](https://numba.pydata.org/)) translates Python
    functions for execution on the GPU without any C/C++ programming.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Numba, you can easily write vectorized functions by applying the Numba decorator
    to the target function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the decorator specifies the parameters and return data types,
    and the target specifies which architecture that code will operate. There are
    three kinds of targets:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Target | Description | Recommended data size and operation |'
  prefs: []
  type: TYPE_TB
- en: '| `cuda` | Targeting NVIDIA GPU | Larger than 1 MB, compute-intensive operation
    |'
  prefs: []
  type: TYPE_TB
- en: '| `parallel` | Optimized for multi-core CPU | Less than 1 MB, normal operation
    |'
  prefs: []
  type: TYPE_TB
- en: '| `cpu` | Optimized for single thread operation | Less than 1 KB, low compute-intensive
    operation |'
  prefs: []
  type: TYPE_TB
- en: If your function does not return a value, use `@guvectorize`, and specify the
    parameter as the vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another use of Numba is with the `@cuda.jit` decorator. This enables you to
    write CUDA-specific operations like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: 'The `cuda.grid()` keyword provides the CUDA threads index in grid-level, so
    that you can write the kernel code, such as CUDA C/C++ code, in the Python way.
    Calling the CUDA kernel function can be done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's install this package and try some examples.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Numba
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To use Numba in your Python code, you need to install the package, and configure
    the environment variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: 'You would need to put the environment variable settings at the end of `.bashrc`
    or `.zshrc`, for the ease of future use. If they are not set, Python will return
    this message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: Using Numba with the @vectorize decorator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will test the `@vectorize` decorator with a simple `saxpy` operation. This
    converts a specific function to work in parallel:'
  prefs: []
  type: TYPE_NORMAL
- en: Create `numba_saxpy.py`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import `numba`, `numpy`, and any other required packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: 'Write a `saxpy` code with the `@vectorize` decorator and target with `''cuda''`
    in order to work on a CUDA device:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: 'Write a saxpy code with the `@vecotrize` decorator and target with `''parallel''`
    to work on a multi-core processor (host):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: 'Write an operation code to call the functions with some NumPy-generated input
    data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: 'This code reports the elapsed time with the various operand sizes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: In this situation, CUDA shows slower performance than the CPU, because the operation
    is simple, but data transfer overhead is heavy.
  prefs: []
  type: TYPE_NORMAL
- en: Using Numba with the @cuda.jit decorator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We also can write sophisticated operations to work on the GPU with Numba using
    the `@cuda.jit` decorator:'
  prefs: []
  type: TYPE_NORMAL
- en: Create `numba_matmul.py`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import `numpy`, `numba`, and any other required packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: 'Write a matrix multiplication code with the `@cuda.jit` decorator:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: In this code, we use `cuda.grid(dimension_size)` to specify the CUDA thread
    index among the grid, so, we can specify the index of the CUDA threads in Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the `a` and `b` matrices as a NumPy matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: 'Copy the NumP- generated data to the device:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the `c` matrix that will be placed in the CUDA device memory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: 'Call the matrix multiplication kernel function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: 'Copy the output to the host:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: 'Compare the CUDA operation with the host:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: 'With a `@cuda.jit` decorator and the built-in `cuda.grid()` keywords, this
    sample code shows how simple it is to implement Numba into the matrix multiplication
    in Python. This code reports the operation-elapsed time on both the device and
    the host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's cover the CuPy that enables more Pythonic programming in CUDA programming.
  prefs: []
  type: TYPE_NORMAL
- en: CuPy – GPU accelerated Python matrix library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CuPy ([https://cupy.chainer.org](https://cupy.chainer.org)) enables linear algebra
    accelerations using Python and fully utilizes GPUs by using CUDA libraries. It
    is NumPy compatible and provides enjoyable Pythonic programmability.
  prefs: []
  type: TYPE_NORMAL
- en: Let's cover its installation, basic usage, and manual kernel developments.
  prefs: []
  type: TYPE_NORMAL
- en: Installing CuPy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can use `pip` to install CuPy using the following command. Then it also
    installs the `cupy` package and the CUDA dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's cover the basic usage of CuPy.
  prefs: []
  type: TYPE_NORMAL
- en: Basic usage of CuPy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can write a saxpy operation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: 'We also can use the `matmul()` function for the matrix multiplication as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: 'As we discussed earlier, CuPy is compatible with NumPy. Basically, the previous
    CuPy''s object is CuPy''s array type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: 'However, we can convert that using the `cupy.asnumpy()` function to convert
    to the NumPy array as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: 'The reverse is also available using `cupy.ascupy()` function. Therefore, we
    can do the following operation based on this compatibility:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we can easily switch the target computing process, and we can
    benefit from each platform's advantages. Now, let's cover the custom kernel implementation
    using CuPy.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing custom kernel functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'CuPy provides three types of custom kernel function: elementwise, reduction
    and raw kernels. The elementwise kernel helps with the automatic indexing for
    each element. Therefore, we can just write an element''s operation. The reduction
    kernel carries out the reduction operation, while also performing the user-defined
    operation. The raw kernel enables direct CUDA C/C++ kernel programming on Python
    codes, so that we can define any operation on it. In this section, we will not
    cover all of them. However, you can learn more from the relevant documentation—[https://docs-cupy.chainer.org/en/stable/tutorial/kernel.html](https://docs-cupy.chainer.org/en/stable/tutorial/kernel.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s discuss the user-defined elementwise kernel implementation. Here is
    an example of elementwise operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can do the elementwise operation without the explicit indexing operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, CuPy provides a highly Pythonic interface and is easy to learn.
    There are lots of internal routines, which are also compatible with NumPy—[https://docs-cupy.chainer.org/en/stable/reference/routines.html](https://docs-cupy.chainer.org/en/stable/reference/routines.html).
    In other words, we can consider using CuPy when we need accelerated computations
    in NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will cover PyCUDA, which provides direct kernel programming and implicit
    memory management wrappers.
  prefs: []
  type: TYPE_NORMAL
- en: PyCUDA – Pythonic access to CUDA API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PyCUDA ([https://documen.tician.de/pycuda/](https://documen.tician.de/pycuda/))
    enables us to write CUDA C/C++ codes in Python codes, and execute them without
    compilation. In this way, you can write CUDA C/C++ codes that are CUDA-specific
    operations. But, you have to optimize this code yourself, since PyCUDA doesn't
    optimize your kernel functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a snippet of code that was produced using PyCUDA:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see in this code, we can write the kernel code using the same Python
    code. We also can retain signs of required data transfer using `driver.In()` and
    `driver.Out()`. These indicate that PyCUDA should transfer the data before invoking
    the kernel. The data transfers automatically, and we also can transfer the data
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE131]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's install PyCUDA and try some simple examples.
  prefs: []
  type: TYPE_NORMAL
- en: Installing PyCUDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To use PyCUDA, you also need to install the package. Download the PyCUDA source
    file from the website ([https://pypi.org/project/pycuda/](https://pypi.org/project/pycuda/)).
    At the moment, version 2019.1.1 is in use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then install the dependencies as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: If you want to use Python 2, skip using Python 3 for the `configure.py` command.
    The configuration command can be different depending on your Python version.
  prefs: []
  type: TYPE_NORMAL
- en: Matrix multiplication using PyCUDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can perform matrix multiplication using PyCUDA in the following manner:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a `pycuda_matmul.py` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the required packages as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE133]'
  prefs: []
  type: TYPE_PRE
- en: 'Write a CUDA kernel function code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE134]'
  prefs: []
  type: TYPE_PRE
- en: 'Generate input/output matrices using NumPy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE135]'
  prefs: []
  type: TYPE_PRE
- en: 'Compile the kernel code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE136]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the kernel function from the compiled module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE137]'
  prefs: []
  type: TYPE_PRE
- en: 'Create device memories with the input data that is generated from the host:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE138]'
  prefs: []
  type: TYPE_PRE
- en: 'Configure the grid and block dimensions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE139]'
  prefs: []
  type: TYPE_PRE
- en: 'Prepare to get the GPU events:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE140]'
  prefs: []
  type: TYPE_PRE
- en: 'Call the kernel function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE141]'
  prefs: []
  type: TYPE_PRE
- en: 'Launch the matrix multiplication from the host, and compare this with the result
    from the device:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE142]'
  prefs: []
  type: TYPE_PRE
- en: 'This code also reports an estimated time on the device and the host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE143]'
  prefs: []
  type: TYPE_PRE
- en: While PyCUDA exposes the CUDA C/C++ kernel code, this result gives a hint that
    manual kernel optimization is required, due to the lack of performance against
    the operation that was carried out by Numba.
  prefs: []
  type: TYPE_NORMAL
- en: NVBLAS for zero coding acceleration in Octave and R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: NVBLAS is a CUDA library for the BLAS operation for other packages, such as
    Octave and R. By replacing the operations carried out OpenBLAS, the Octave or
    developers and data scientists can easily enjoy GPU performance. In this chapter,
    we will cover how to accelerate Octave and R using NVBLAS.
  prefs: []
  type: TYPE_NORMAL
- en: NVBLAS is a dynamic library on top of the cuBLAS operation. The cuBLAS library
    is a GPU implementation of linear algebra operations. It replaces BLAS libraries,
    so that we can easily accelerate any application with zero coding effort. Let's
    see how this can be done from GEMM example codes.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To use NVBLAS in Octave and R, we need to provide some working environment
    variables to NVBLAS. To do this, let''s create an `nvblas.conf` file, where the
    directory, which we will work with Octave and R code examples, can be found. The
    `nvblas.conf` file can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE144]'
  prefs: []
  type: TYPE_PRE
- en: 'In this file, we can see that NVBLAS needs to be aware of the CPU side''s BLAS
    library. We will use OpenBLAS in this session, so we need to install it with the
    following command in Ubuntu:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE145]'
  prefs: []
  type: TYPE_PRE
- en: Also, we can get a multi-GPU performance by providing multiple GPU IDs for `NVBLAS_GPU_LIST`.
    This book provides results from a GPU execution result, but try to provide multiple
    IDs if you have multiple GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use NVBLAS in Octave and R, we should set an environment—`LD_PRELOAD=libnvblas.so`—with
    your application execution:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For Octave code, execute your code as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE146]'
  prefs: []
  type: TYPE_PRE
- en: 'For the R script, execute your script as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE147]'
  prefs: []
  type: TYPE_PRE
- en: Of course, the `libnvblas.so` file should be accessible from the working directory.
    It is located in `/usr/local/cuda/lib64/`.
  prefs: []
  type: TYPE_NORMAL
- en: 'NVBLAS is compatible with the archived packages. Therefore, using Octave- and
    R-installed ones with the following commands works well with our test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE148]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's try using NVBLAS using the Octave and R languages.
  prefs: []
  type: TYPE_NORMAL
- en: Accelerating Octave's computation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we will try NVBLAS using Octave. The fully implemented code is `08_nvblas/sgemm.m`.
    This is implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE149]'
  prefs: []
  type: TYPE_PRE
- en: 'For the GPU operation, execute the Octave script using the following command,
    and compare the performance with GPU, with the NVBLAS environment library and
    CPU by default:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE150]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can launch this with the `octave sgemm.m` command. The output results
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| CPU | GPU V100 |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '`Elapsed Time [1024]: 0.011 ms, 188.909 GFlops`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Elapsed Time [2048]: 0.075 ms, 228.169 GFlops`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Elapsed Time [4096]: 0.212 ms, 647.022 GFlops`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Elapsed Time [8192]: 1.158 ms, 949.763 GFlops`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Elapsed Time [16384]: 7.292 ms, 1206.241 GFlops`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '`Elapsed Time [1024]: 0.010 ms, 208.346 GFlops`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Elapsed Time [2048]: 0.024 ms, 721.731 GFlops`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Elapsed Time [4096]: 0.094 ms, 1465.538 GFlops`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Elapsed Time [8192]: 0.582 ms, 1889.193 GFlops`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Elapsed Time [16384]: 4.472 ms, 1967.037 GFlops`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, GPU shows higher computational throughput as the matrices' size
    get larger.
  prefs: []
  type: TYPE_NORMAL
- en: Accelerating R's compuation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we will try NVBLAS for the R language, with the help of the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s write a `sgemm.R` file which carries out a dot operation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE151]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the R script using the following command and compare the performance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE152]'
  prefs: []
  type: TYPE_PRE
- en: 'The sample code operates several times, while increasing the data size. The
    following table shows the outputs of the previous commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '| CPU | GPU V100 |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '`Elapsed Time [1024]: 0.029 ms, 74.051 GFlops`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Elapsed Time [2048]: 0.110 ms, 156.181 GFlops`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Elapsed Time [4096]: 0.471 ms, 291.802 GFlops`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Elapsed Time [8192]: 2.733 ms, 402.309 GFlops`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Elapsed Time [16384]: 18.291 ms, 480.897 GFlops`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '`Elapsed Time [1024]: 0.034 ms, 63.161 GFlops`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Elapsed Time [2048]: 0.063 ms, 272.696 GFlops`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Elapsed Time [4096]: 0.286 ms, 480.556 GFlops`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Elapsed Time [8192]: 1.527 ms, 720.047 GFlops`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Elapsed Time [16384]: 9.864 ms, 891.737 GFlops`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: From the results, we can see the performance gap between the CPU and GPU. Also,
    we are able to identify that the performance gain of GPU increases when we increase
    the sample size.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are interested in R acceleration with GPU, please visit an NVIDIA development
    blog: [https://devblogs.nvidia.com/accelerate-r-applications-cuda/](https://devblogs.nvidia.com/accelerate-r-applications-cuda/)'
  prefs: []
  type: TYPE_NORMAL
- en: CUDA acceleration in MATLAB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MATLAB is a productive, high-level numerical analysis tool with various tools
    and functions. This tool supports CUDA from the early stages with their **Parallel
    Computing Toolbox**. This section will show us how to generate CUDA code using
    this tool.
  prefs: []
  type: TYPE_NORMAL
- en: To enable GPU acceleration, we need to install MATLAB with the Parallel Computing
    Toolbox. If you already have MATLAB, check if your license covers the Parallel
    Computing Toolbox. If you don't, you can try the MATLAB evaluation code. From
    MATLAB's evaluation site, you may download any kind of package, except the control
    systems. Most packages contain the Parallel Computing Toolbox, so you may try
    this. But if you are not considering using MATLAB, you may skip this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we use MATLAB code to work on GPU, you need to create a device memory
    using gpuArray. In the same way that *Numba* and *PyCUDA* send their host data
    to the device, MATLAB''s `gpuArray()` creates a device memory and transfers the
    given host data to the device:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE153]'
  prefs: []
  type: TYPE_PRE
- en: 'This session will assume that you have already installed MATLAB and the Parallel
    Computing Toolbox. In this section, we will focus on implementing the sample code,
    and compare the performances of the host and GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s write a `host.m` file, which can work on the CPU. The code is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE154]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s execute both implementations with the following commands. This
    is the command to MATLAB, and its output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE155]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, let''s write a `cuda.m` file, which works on the GPU. We just apply `gpuArray()` to
    the input matrices as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE156]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the GPU version execution code, and the execution result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE157]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, GPU shows a higher performance against the CPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'MathWorks provides plenty of examples of GPU computing with MATLAB. Please
    visit their site if you want to learn more: [https://www.mathworks.com/examples/parallel-computing/category/gpu-computing](https://www.mathworks.com/examples/parallel-computing/category/gpu-computing).'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have covered the CUDA programming methods using CUDA libraries,
    and other compatible languages. We have also covered the basic use of cuBLAS and
    its mixed-precision operation feature. Also, we explored the cuRAND, cuFFT, NPP,
    and OpenCV libraries. Thanks to these libraries, we could implement GPU applications
    with little effort, as discussed at the beginning of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We have implemented some GPU applications using other languages that are compatible
    with CUDA. Firstly, we covered several Python packages, which enable Python and
    CUDA interops. They provide Pythonic programmabilities and compatibilities with
    other Python features. Then, we covered CUDA accelerations in other scientific
    computing languages, such as Octave, R, and MATLAB.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have one more GPU programming method to cover—OpenACC. With this we
    can covert the original C/C++ and Fortran host codes to work on GPUs using directives
    such as `#pragma acc kernels`. We will cover this in the next chapter.
  prefs: []
  type: TYPE_NORMAL
