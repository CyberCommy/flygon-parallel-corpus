- en: Performance Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Performance is one of the key drivers for choosing C++ as a programming language
    for a project. The time has come to discuss how we can improve performance when
    we're structuring code in a functional style.
  prefs: []
  type: TYPE_NORMAL
- en: While performance is a huge topic that we obviously can't completely cover in
    one chapter, we will look at key ideas for improving performance, how purely functional
    languages optimize performance, and how to translate these optimizations into
    C++.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: A process for delivering performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use parallel/async to improve performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding what tail recursion is and how to activate it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to improve memory consumption when using functional constructs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Functional asynchronous code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will need a compiler that supports C++ 17\. I used GCC 7.3.0.
  prefs: []
  type: TYPE_NORMAL
- en: The code can be found on GitHub at [https:/​/​github.​com/​PacktPublishing/​Hands-​On-​Functional-Programming-​with-​Cpp](https://github.%E2%80%8Bcom/PacktPublishing/Hands-On-Functional-Programming-with-Cpp)
    in the `Chapter10` folder. It includes and uses `doctest`, which is a single-header
    open source unit testing library. You can find it on its GitHub repository at [https:/​/github.​com/​onqtam/​doctest](https://github.%E2%80%8Bcom/onqtam/doctest).
  prefs: []
  type: TYPE_NORMAL
- en: Performance optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Talking about performance optimization is like talking about pizza. Some people
    like and search for pizza with pineapple. Others only eat traditional Italian
    pizzas (or from a specific region). Some only eat vegetarian pizza, while others
    like all kinds of pizza. The point is, performance optimization is contextual
    to your code base and your product. What kind of performance are you looking at?
    What is the most valuable part of performance for your users? And what constraints
    do you need to take into account?
  prefs: []
  type: TYPE_NORMAL
- en: 'The customers I work with usually have a few performance requirements, depending
    on the topic:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Embedded products* (for example, automotive, energy, or telecommunications)
    often need to work within memory constraints. The stack and the heap are often
    small, thus limiting the number of long-lived variables. The cost of increasing
    memory can be prohibitive (one customer told us they would need more than 10 million
    euros for an extra 1 MB of memory on all of their devices). Therefore, programmers
    need to work around these limitations by avoiding unnecessary memory allocation
    whenever possible. This can include initialization, passing arguments by copy
    (especially larger structures), and avoiding specific algorithms that require
    memory consumption, among others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Engineering applications* (for example computer-aided design or CAD) need
    to use specific algorithms derived from math, physics, and engineering on very
    large datasets and return results as quickly as possible. Processing is usually
    done on modern PCs, so RAM is less of a problem; however, the CPU is. With the
    advent of multi-core CPUs, specialized GPUs that can take over part of the processing
    and cloud technologies that allow the distribution of workloads between multiple
    powerful or specialized servers, the job of developers often becomes optimizing
    for speed in a parallel and asynchronous world.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Desktop games and game engines* have their own particular concerns. The graphics
    have to look as good as possible to gracefully scale down on middle- or lower-
    end machines and avoid lag. Games usually take over the machine they run on, so
    they only need to fight for resources with the operating system and the system
    applications (such as antiviruses or firewalls). They can also assume a specific
    level of GPU, CPU, and RAM. Optimization becomes about parallelism (since multiple
    cores are expected) and about avoiding waste in order to keep a smooth experience
    throughout the gameplay.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Game servers*, however, are a different beast. Services such as Blizzard''s
    Battle.net (the one I''m using a lot as a *Starcraft II* player) are required
    to respond quickly, even under stress. The number of servers used and their power
    doesn''t really matter in the age of cloud computing; we can easily scale them
    up or down. The main concern is responding as quickly as possible to a mostly
    I/O workload.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The future is exciting*. The tendency in games is to move processing to the
    servers, thus allowing gamers to play even on lower-end machines. This will open
    up amazing opportunities for future games. (What could you do with 10 GPUs instead
    of one? What about with 100?)But will also lead to the need to optimize the game
    engine for server-side, multi-machine, parallel processing. To move away from
    gaming, the IoT industry opens up even more opportunities for embedded software
    and scalable server-side processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given all these possibilities, what can we do to deliver performance in a code
    base?
  prefs: []
  type: TYPE_NORMAL
- en: A process for delivering performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As you can see, performance optimization depends a lot on what you''re trying
    to achieve. The next steps can be quickly summarized as such:'
  prefs: []
  type: TYPE_NORMAL
- en: Define a clear goal for performance, including the metrics and how to measure
    them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define a few coding guidelines for performance. Keep them clear and tailored
    to specific parts of the code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make the code work.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Measure and improve performance where needed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Monitor and improve.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Before we look into each of these steps in more detail, it's important to understand
    one important caveat of performance optimization—there are two types of optimization.
    The first comes from clean designs and clean code. For example, by removing certain
    types of similarity from your code, you may end up reducing the size of the executable,
    thus allowing more space for data; the data may end up traveling less through
    the code, thus avoiding unnecessary copies or indirections; or, it will allow
    the compiler to understand the code better and optimize it for you. From my experience,
    refactoring code towards simple design has also often improved performance.
  prefs: []
  type: TYPE_NORMAL
- en: The second way to improve performance is by using point optimizations. These
    are very specific ways in which we can rewrite a function or a flow that allows
    the code to work faster or with less memory consumption, usually for a specific
    compiler and platform. The resulting code often looks smart but is difficult to
    understand and difficult to change.
  prefs: []
  type: TYPE_NORMAL
- en: Point optimizations have a natural conflict with writing code that's easy to
    change and maintain. This has famously led to Donald Knuth saying that *premature
    optimization is the root of all evil*. This doesn't mean that we should write
    code that's obviously slow, such as passing large collections by copy. It does
    mean, however, that we should first optimize the design for changeability, then
    measure performance, then optimize it, and only use point optimizations if absolutely
    necessary. Quirks in the platform, a specific compiler version, or libraries that
    are used may require point optimizations from time to time; keep them separate
    and use them scarcely.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look now into our process for optimizing performance.
  prefs: []
  type: TYPE_NORMAL
- en: Define a clear goal for performance, including the metrics and how to measure
    them
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we don't know where we're going, it doesn't matter in which direction we
    go—I'm paraphrasing from Alice in the Wonderland. We should, therefore, know where
    we're going. We need a list of performance metrics that fit the needs of our product.
    In addition, for each of the performance metrics, we need a range that defines
    what is a *good* value for the metric and what is an *acceptable* value. Let's
    look at a few examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you''re building an *embedded product* for a device with 4 MB of memory,
    you might look at metrics such as:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Memory consumption:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Great: 1-3 MB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Good: 3-4 MB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Device boot time:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Great: < 1s'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Good: 1-3s'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you're building a *desktop CAD application* that models the sound waves through
    a building design, other metrics are interesting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Computation time for modeling sound waves:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For a small room:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Great: < 1 min'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Good: < 5 min'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For a medium-sized room:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Great: < 2 min'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Good: < 10 min'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The numbers here are illustrative only; you'll need to find your own metrics
    for your product.
  prefs: []
  type: TYPE_NORMAL
- en: Having these metrics and the good/great ranges allows us to measure performance
    after a new feature is added and optimize accordingly. It also allows us to simply
    explain the performance of a product to stakeholders or business people.
  prefs: []
  type: TYPE_NORMAL
- en: Define a few coding guidelines for performance—keep them clear and tailored
    to specific parts of the code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you ask 50 different C++ programmers about tips to optimize performance,
    you'll soon be overwhelmed with advice. If you start investigating the advice,
    it will turn out that some of it is dated, some of it is very specific, and some
    is great.
  prefs: []
  type: TYPE_NORMAL
- en: It is, therefore, important to have coding guidelines for performance, but there's
    a caveat. C++ code bases tend to be huge because they've been developed over many
    years. If you look critically at your code base, you'll realize that only some
    parts of the code are bottlenecks for performance. To give an example, computing
    a mathematical operation 1 ms faster only makes sense if that operation will be
    called many times; if it's only called once or twice, or very seldom, there's
    no need to optimize it. In fact, the next version of the compiler or CPU will
    probably do a better job than you at optimizing it.
  prefs: []
  type: TYPE_NORMAL
- en: Because of this fact, you should understand which parts of your code are critical
    for the performance criteria that you've defined. Figure out what design fits
    that particular piece of code best; have clear guidelines, and follow them. While
    `const&` is useful everywhere, maybe you could avoid wasting the developer's time
    sorting a very small collection that's only done once.
  prefs: []
  type: TYPE_NORMAL
- en: Make the code work
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With these guidelines in mind, and with a new feature to implement, the first
    step should always be to make the code work. Also, structure it so it's easy to
    change within your constraints. Don't try to optimize for performance here; once
    again, the compiler and the CPU might be smarter than you think and do more work
    than you expect. The only way to know whether that's the case is to measure performance.
  prefs: []
  type: TYPE_NORMAL
- en: Measure and improve performance where needed
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Your code works and is structured according to your guidelines and is optimized
    for change. It's time to write down a few hypotheses about optimizing it and then
    test them.
  prefs: []
  type: TYPE_NORMAL
- en: Since you have clear metrics for performance, it's relatively easy to verify
    them. Sure, it requires the correct infrastructure and a proper measurement process.
    With these in place, you can measure where you stand against your performance
    metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Additional hypotheses should be welcome here. Something like—*if we restructure
    this code like this, I expect an improvement in the indicator X*. You can then
    move on and test your hypothesis—start a branch, change the code, build the product,
    take it through the performance metrics measurement process, and see the results.
    Sure, it's more complex than I make it sound—sometimes it may require builds with
    different compilers, with different optimization options, or statistics. All these
    are necessary if you want to make an informed decision. It's better to invest
    some time into metrics over changing the code and making it more difficult to
    understand. Otherwise, you'll end up with a technical debt on which you'll pay
    interest for a long time.
  prefs: []
  type: TYPE_NORMAL
- en: However, if you have to do point optimizations, there's no workaround. Just
    make sure to document them in as much detail as possible. Since you've tested
    your hypothesis before, you'll have a lot to write, won't you?
  prefs: []
  type: TYPE_NORMAL
- en: Monitor and improve
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started the loop by defining metrics for performance. It's time to close
    it—we need to monitor those metrics (and possibly others) and adjust our intervals
    and coding guidelines based on what we've learned. Performance optimization is
    a continuous process because the target devices evolve as well.
  prefs: []
  type: TYPE_NORMAL
- en: We've looked at a process for delivering performance, but how does this relate
    to functional programming? Which use cases make functional code structures shine,
    and which don't work so well? It's time to look deeper into our code structures.
  prefs: []
  type: TYPE_NORMAL
- en: Parallelism – taking advantage of immutability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Writing code that runs in parallel has been the source of much pain in software
    development. It seems like the problems arising from multithreaded, multi-process,
    or multi-server environments are fundamentally difficult to solve. Deadlocks,
    starvation, data races, locks, or debugging multi-threaded code are just a few
    terms that make those of us who've seen them afraid of ever meeting them again.
    However, we have to face parallel code because of multi-core CPUs, GPUs, and multiple
    servers. Can functional programming help with this?
  prefs: []
  type: TYPE_NORMAL
- en: Everyone agrees that this is one of the strong points of functional programming,
    specifically derived from immutability. If your data never changes, there are
    no locks and the synchronization is so simple that it can be generalized. If you
    just use pure functions and functional transformations (barring I/O, of course),
    you get parallelization (almost) for free.
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, the C++ 17 standard includes execution policies for the STL higher-level
    functions, allowing us to change the algorithm from sequential to parallel with
    just one parameter. Let''s check whether all numbers from a vector are greater
    than `5` in parallel. We just need to use `execution::par` as the execution policy
    for `all_of`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then measure the difference between using the sequential and the parallel
    version of the algorithm with the high-resolution timer from the `chrono` namespace,
    as seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Normally, I would now show you the difference in execution based on my experiments.
    Unfortunately, in this case, I can''t do this. At the time of writing, the only
    compilers implementing execution policies are MSVC and Intel C++, but neither
    of them met the standard. However, as shown in the following snippet, I wrote
    the code in the `parallelExecution.cpp` source file, allowing you to enable it
    by uncommenting a line when your compiler supports the standard, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The code you will be running when you do this will display the comparative
    duration for running `all_of` sequentially and in parallel, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: While I would have loved to analyze some execution data here, maybe it's for
    the best that I can't, since the most important message of this chapter is measure,
    measure, measure, and, only then, optimize. Hopefully, you'll do some measuring
    yourself when the time comes.
  prefs: []
  type: TYPE_NORMAL
- en: The C++ 17 standard supports the execution policies for many STL functions,
    including `sort`, `find`, `copy`, `transform`, and `reduce`. That is, if you're
    chaining these functions and using pure functions, you just need to pass an extra
    parameter to all calls (or `bind` the higher-level functions) to achieve parallel
    execution! I would go as far as to say that this is akin to magic for anyone who
    has tried managing threads by themselves or debugging weird synchronization issues.
    In fact, all the code we wrote for Tic-Tac-Toe and Poker Hands in the previous
    chapters can be easily switched to parallel execution, provided the compiler supports
    the full C++ 17 standard.
  prefs: []
  type: TYPE_NORMAL
- en: But how does this work? It's fairly easy for `all_of` to run in multiple threads; each
    of them executes the predicate on a specific element from the collection, a Boolean
    value is returned, and the process stops when the first predicate returns `False`.
    This is only possible if the predicate is a pure function; modifying the result
    or the vector in any way will create race conditions. The documentation specifically
    states that the programmer is responsible for keeping the predicate function pure—there
    will be no warning or compilation error. In addition to being pure, your predicate
    must not assume the order in which the elements are treated.
  prefs: []
  type: TYPE_NORMAL
- en: 'In case the parallel execution policy cannot be started (for example, due to
    a lack of resources), the execution will fall back to sequential calls. This is
    a useful thing to remember when measuring performance: if it''s much lower than
    expected, check first whether the program can execute in parallel.'
  prefs: []
  type: TYPE_NORMAL
- en: This option is useful for computation-heavy applications using multiple CPUs.
    If you're interested in its memory hit, you'll have to measure it, since it depends
    on the compiler and the standard library that you use.
  prefs: []
  type: TYPE_NORMAL
- en: Memoization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pure functions have an interesting property. For the same input values, they
    return the same outputs. This makes them equivalent to a big table of values with
    an output value corresponding to every combination of values for the input arguments.
    Sometimes, it's faster to remember parts of this table rather than doing the computation.
    This technique is called **memoization**.
  prefs: []
  type: TYPE_NORMAL
- en: Pure functional programming languages, as well as languages such as Python and
    Groovy, have ways to enable memoization on specific function calls, thus providing
    a high level of control. Unfortunately, C++ doesn't have this facility, so we'll
    have to write it ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing memoization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To start our implementation, we will need a function; ideally, computationally
    expensive. Let''s pick the `power` function. A simple implementation is just a
    wrapper over the standard `pow` function, as shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: How do we start to implement memoization? Well, at its core, memoization is
    caching. Whenever a function is called for the first time, it runs normally but
    also stores the result in combination with the input values. On subsequent calls,
    the function will search through the map to see if the value is cached and return
    it if so.
  prefs: []
  type: TYPE_NORMAL
- en: 'This means that we''ll need a cache that has, as key, the parameters, and,
    as value, the result of the computation. To group the parameters together, we
    can simply use a pair or a tuple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Therefore, the cache will be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s change our `power` function to use this cache. First, we need to look
    in the cache for a result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'If nothing is found, we compute the result and store it in the cache. If something
    is found, that''s the value we return:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'To check that this method is working fine, let''s run some tests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Everything works fine. Now let''s compare the two versions of power, with and
    without memoization in the following snippet. The following code shows how we
    can extract a more generic way to memoize functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The first observation is that we can replace the bold line with a call to the
    original power function, so let''s do that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'If we pass in the function we need to call during memoization, we obtain a
    more general solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'But wouldn''t it be nice to return a memoized function instead? We can modify
    our `memoize` function to receive a function and return a function that is memoized,
    which receives the same parameters as the initial function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This change doesn''t work initially—I''m getting a segmentation fault. The
    reason is that we are changing the cache inside the lambda. To make it work, we
    need to make the lambda mutable and capture by value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have a function that can memoize any function with two integer parameters.
    It''s easy to make it more generic with the help of a few type arguments. We need
    a type for the return value, a type for the first argument, and a type for the
    second argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We have achieved a memoization function for any function that has two arguments.
    We can do even better. C++ allows us to use templates with an unspecified number
    of type arguments—so-called **variadic templates**. Using their magic, we end
    up with an implementation for memoization that works with any function with any
    number of arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This function is helpful for caching any other function; however, there''s
    a catch. We have, until now, used the wrapped implementation of power. The following
    is an example of what it would look like if we wrote our own instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Memoizing this function will merely cache the final results. However, the function
    is recursive and the call to our `memoize` function will not memoize the intermediate
    results from the recursion. To do so, we need to tell our memoized power function
    not to call the power function but the memoized `power` function.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, there's no easy way to do this. We could pass, as an argument,
    the function to call recursively, but this would change the original function
    signature for implementation reasons. Or we could just rewrite the function to
    take advantage of memoization.
  prefs: []
  type: TYPE_NORMAL
- en: Still, we end up with quite a good solution. Let's put it to the test.
  prefs: []
  type: TYPE_NORMAL
- en: Using memoization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s use our `measureExecutionTimeForF` function to measure the time it takes
    to make various calls to our `power` function. It''s time to also think about
    the results we expect. We do cache the values of repeated calls, but this requires
    its own processing and memory on every call to the function. So, maybe it will
    help, maybe it won''t. We won''t know until we try it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This code is calling the `power` function with the same values, with the last
    call returning to the first values. It then proceeds to do the same, but after
    creating the memoized version of `power`. Finally, a sanity check—the result of
    the `power` function and the memoized `power` function are compared to ensure
    that we don't have a bug in the `memoize` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The question is—has memoization improved the time it takes to execute the last
    call from the series (exactly the same as the first call from the series)? In
    my configuration, the results are mixed, as shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, for a better view (calls without memoization are first), there
    is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Overall, the calls with memoization are better, except when we repeat the first
    call. Of course, the results vary when running the test repeatedly, but this shows
    that improving performance is not as easy as just using caching. What happens
    behind the scenes? I think that the most likely explanation is that another caching
    mechanism kicks in—CPU or otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: If anything, this proves the importance of measurements. It's not a surprise
    that CPUs and compilers already do a fair share of optimizations, and we can only
    do so much in code.
  prefs: []
  type: TYPE_NORMAL
- en: 'What if we try recursive memoization? I rewrote the `power` function to use
    memoization recursively, and it mixes caching with the recursive call. Here''s
    the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run it, the results are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, in a compressed view (calls without memoization are first),
    there is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the time for building the cache is enormous. However, it pays
    off for repeated calls but it still can't beat the CPU and compiler optimizations
    in this case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Does memoization help then? It does when we use a more complex function. Let''s
    next try computing the difference between the factorial of two numbers. We''ll
    use a naive implementation of the factorial, and we''ll try to memoize the factorial
    function first, and then the function computing the difference. For the sake of
    consistency, we''ll use the same pairs of numbers as before. Let''s look at the
    code in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'What are the results? Let''s first see the difference between the normal function,
    and the function using the memoized factorial:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s compare them side by side once again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Although the result is mixed for the other calls, there''s a ~20% improvement
    with the memoized function over the non-memoized function when hitting the cached
    value. That seems a small improvement since factorial is recursive, so, in theory,
    the memoization should help immensely. However, *we did not memoize the recursion*.
    Instead, the factorial function is still calling the non-memoized version recursively.
    We''ll come back to this later; for now, let''s check what happens when memoizing
    the `factorialDifference` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at the results side by side:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The memoized version is twice as fast as the non-memoized one on the cached
    value! This is huge! However, we pay for this improvement with a performance hit
    when we don't have the value cached. Also, something weird is going on at the
    second call; some kind of caching may interfere with our results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Can we make this better by optimizing all the recursions of the factorial function?
    Let''s see. We need to change our factorial function such that the cache applies
    to each call. In order to do this, we''ll need to call the memoized factorial
    function recursively instead of the normal factorial function, as shown in the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the difference function, which recursively memoizes both calls to the
    factorial:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'By running the initial function without memoization and the previous function
    with the same data side by side, I got the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We can look at this side by side:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the cache is building up, with a massive penalty hit for the
    first large computation; the second call involved 1024! However, the subsequent
    calls are much faster due to the cache hits.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, we can say that memoization is useful for speeding up repeated
    complex computations when enough memory is available. It may require some tweaking
    since the cache size and cache hits depend on how many calls and how many repeated
    calls are made to the function. So, don't take this for granted—measure, measure,
    measure.
  prefs: []
  type: TYPE_NORMAL
- en: Tail recursion optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recursive algorithms are very common in functional programming. In fact, many
    of our imperative loops can be rewritten as recursive algorithms using pure functions.
  prefs: []
  type: TYPE_NORMAL
- en: However, recursion is not very popular in imperative programming because it
    has a few issues. First, developers tend to have less practice with recursive
    algorithms compared to imperative loops. Second, the dreaded stack overflow—recursive
    calls are placed to the stack by default and if there are too many iterations,
    the stack overflows with an ugly error.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, compilers are smart and can fix this problem for us, while at the
    same time optimizing recursive functions. Enter tail recursion optimization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at a simple recursive function. We''ll reuse the factorial
    from the previous section, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Normally, each call would be placed on the stack, so your stack will grow with
    each call. Let''s visualize it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We can avoid the stack by rewriting the code. We notice that the recursive
    call comes at the end; we can, therefore, rewrite the function similar to the
    following pseudocode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: In a nutshell, this is what the compiler can do for us if we enable the correct
    optimization flag. Not only does this call take less memory and avoid stack overflows—but
    it is also faster.
  prefs: []
  type: TYPE_NORMAL
- en: By now, you should know not to trust anyone's claims—including mine—without
    measuring them. So, let's check this hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll need a test that measures the timing for multiple calls to the
    factorial function. I picked some values to carry out the test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Then, we need to compile this function with optimization disabled and enabled.
    The **GNU Compiler Collection** (**GCC**) flag that optimizes tail recursion is
    `-foptimize-sibling-calls`; the name refers to the fact that the flag optimizes
    both sibling calls and tail calls. I will not go into detail about what sibling
    call optimization does; let's just say that it doesn't affect our test in any
    way.
  prefs: []
  type: TYPE_NORMAL
- en: 'Time to run the two programs. First, let''s look at the raw output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the program without optimization:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the program with optimization:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see the results side by side now; the duration without optimization
    is on the left:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: It seems that the optimization really kicks in for larger values on my machine.
    Once again, this proves the importance of metrics whenever performance matters.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we'll experiment with the code in various ways and
    measure the results.
  prefs: []
  type: TYPE_NORMAL
- en: Fully optimized calls
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Out of curiosity, I''ve decided to run the same program with all of the safe
    optimization flags turned on. In GCC, this option is `-O3`. The results are staggering,
    to say the least:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s compare the results of enabling all of the optimization flags (the second
    value in the next snippet) with the results for just tail recursion optimization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: The difference is staggering, as you can see. The conclusion is that, while
    tail recursion optimization is useful, it's even better to have CPU cache hits
    and all the goodies enabled by compilers.
  prefs: []
  type: TYPE_NORMAL
- en: But we're using an `if` statement; will this work differently when we use the
    `?:` operator?
  prefs: []
  type: TYPE_NORMAL
- en: 'If vs ?:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For curiosity''s sake, I decided to re-write the code using the `?:` operator
    instead of `if` statements, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'I didn''t know what to expect, and the results were interesting. Let''s look
    at the raw output:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Without optimization flags:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'With tail recursion flag turned on:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at a comparison of the results; the duration without optimization
    comes first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The difference is very great between the two versions, which is something I
    didn't quite expect. As always, this is most likely the result of the GCC compiler,
    and you should test it on your own. However, it seems that this version is better
    for tail optimization with my compiler—an intriguing result to say the least.
  prefs: []
  type: TYPE_NORMAL
- en: Double recursion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Does tail recursion work for double recursion? We need to come up with an example
    that passes recursion from one function to another to check for this. I decided
    to write two functions, `f1` and `f2`, which recursively call each other. `f1`
    multiplies the current parameter with `f2(n - 1 )`, while `f2` adds `f1(n)` to `f1(n-1)`.
    Here''s the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s check the timing for calls to `f1` with values from `0` to `8`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s what we obtain:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Without tail call optimization:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'With call optimization:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at the results side by side; the duration of calls without tail
    optimization is on the left:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The differences are very great indeed, showing that the code is greatly optimized.
    However, remember that, for GCC, we are using the `-foptimize-sibling-calls` optimization
    flag. This flag carries out two types of optimization: tail calls and sibling
    calls. Sibling calls are calls to functions that have a return type of the same
    size and a parameter list of the same total size, thus allowing the compiler to
    treat them similarly with tail calls. It''s quite possible that, in our case,
    both optimizations are applied.'
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing execution time with asynchronous code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we have multiple threads, we can use two close techniques to optimize
    the execution time: parallel execution and asynchronous execution. We''ve seen
    how parallel execution works in a previous section; what about asynchronous calls?'
  prefs: []
  type: TYPE_NORMAL
- en: First, let's remind ourselves what asynchronous calls are. We would like to
    make a call, continue normally on the main thread, and get the result back at
    some point in the future. To me, this sounds like a perfect job for functions.
    We just need to call functions, let them execute, and talk to them again after
    a while.
  prefs: []
  type: TYPE_NORMAL
- en: Since we've talked about the future, let's talk about the `future` construct
    in C++.
  prefs: []
  type: TYPE_NORMAL
- en: Futures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've already established that it's ideal to avoid managing threads in a program,
    except when doing very specialized work, but we need parallel execution and often
    need synchronization to obtain a result from another thread. A typical example
    is a long computation that would block the main thread unless we run it in its
    own thread. How do we know when the computation is done and how can we get the
    result of the computation?
  prefs: []
  type: TYPE_NORMAL
- en: 'In 1976–1977, two concepts were proposed in computer science to simplify the
    solution to this problem—futures and promises. While these concepts are often
    used interchangeably in various technologies, in C++ they have specific meanings:'
  prefs: []
  type: TYPE_NORMAL
- en: A future can retrieve a value from a provider while taking care of synchronization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A promise stores a value for the future, offering, in addition, a synchronization
    point
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Due to its nature, a `future` object has restrictions in C++. It cannot be copied,
    only moved, and it's only valid when associated with a shared state. This means
    that we can only create a valid future object by calling `async`, `promise.get_future()` or
    `packaged_task.get_future()`.
  prefs: []
  type: TYPE_NORMAL
- en: It's also worth mentioning that promises and futures use threading libraries
    in their implementation; therefore, you may need to add a dependency to another
    library. On my system (Ubuntu 18.04, 64 bits), when compiling with g++, I had
    to add a link dependency to the `pthread` library; I expect you'll need the same
    if you're using g++ on a mingw or cygwin configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first see how we use `future` and `promise` in tandem. First, we''ll
    create a `promise` for a secret message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, let''s create a `future` and start a new thread using it. The thread
    will use a lambda that simply prints the secret message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Notice that we need to avoid copying the `future`; in this case, we use a reference
    wrapper over the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll stick with this thread for now; the next thing is to fulfill the promise,
    that is, to set a value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'In the meantime, the other thread will do some stuff and then will request
    that we keep our promise. Well, not quite; it will ask for the value of the `promise`,
    which blocks it until `join()` is called:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: As you may notice, this method sets the responsibility for computing the value
    in the main thread. What if we want it to be on the secondary thread? We just
    need to use `async`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say we''d like to check whether a number is prime. We first write a
    lambda that will check for this in a naive way, for each possible divisor from
    `2` to `x-1`, and check whether `x` is divisible by it. If it''s not divisible
    by any value, it is a prime number:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'A few helping lambdas are used. One for generating a range like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'This is then specialized for generating a range that starts with `2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, a predicate that checks whether two numbers are divisible or not:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'To run this function in a separate thread from the main one, we need to declare
    a `future` using `async`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: The second argument of `async` is the input argument for our function. Multiple
    arguments are allowed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we can do other stuff, and finally, ask for the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: The bold line of code marks the point when the main thread stops to wait for
    a result from the secondary thread.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you need more than one `future`, you can use them. In the following example,
    we''ll run `is_prime` with four different values in four different threads, as
    shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Functional asynchronous code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've seen that the simplest implementation of a thread is a lambda, but we
    can do even more. The last example, which uses multiple threads to run the same
    operation asynchronously on different values, can be turned into a functional
    high-order function.
  prefs: []
  type: TYPE_NORMAL
- en: 'But let''s start with a few simple loops. First, we will transform the input
    values and the expected results into vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we need a `for` loop to create the futures. It''s important not to call
    the `future()` constructor, because this will fail due to trying to copy the newly
    constructed `future` object into a container. Instead, add the result of `async()`
    directly into the container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we need to get the results back from the threads. Once again, we need
    to avoid copying the `future`, so we will use a reference when iterating:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see the whole test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s quite obvious that we can turn this into a few transform calls. However,
    we need to pay special attention to avoid the copying of futures. First, I created
    a lambda that helps with creating a `future`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'The first `for` loop then turns into a `transformAll` call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'The second part is trickier than expected. Our implementation of `transformAll`
    doesn''t work, so I will call `transform` inline instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'We end up with the following test, which passes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: I have to be honest with you, this was the most difficult code to implement
    correctly so far. So many things can go wrong when working with futures and it's
    not obvious why. The error messages are quite unhelpful, at least for my version
    of g++. The only way I managed to make this work was by going step by step, as
    I showed you in this section.
  prefs: []
  type: TYPE_NORMAL
- en: However, this code sample show an important fact; with the thoroughly thought
    out and tested use of futures, we can parallelize higher-order functions. It is,
    therefore, a possible solution if you need better performance, can use multiple
    cores, and can't wait for the implementation of a parallel running policy in the
    standard. If only for this, I think my efforts were useful!
  prefs: []
  type: TYPE_NORMAL
- en: Since we're talking about asynchronous calls, we could also do a quick pass
    through the world of reactive programming.
  prefs: []
  type: TYPE_NORMAL
- en: A taste of reactive programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Reactive programming** is a paradigm for writing code that focuses on processing
    data streams. Imagine having to analyze a stream of temperature values, values
    coming from sensors mounted on self-driving cars, or share values for specific
    companies. In reactive programming, we receive this continuous stream of data
    and run functions that analyze it. Since new data can arrive unpredictably on
    stream, the programming model has to be asynchronous; that is, the main thread
    is continuously waiting for new data, and, when it arrives, the processing is
    delegated to secondary streams. The results are usually collected asynchronously
    as well—either pushed to the user interface, saved in data stores, or passed to
    other data streams.'
  prefs: []
  type: TYPE_NORMAL
- en: We've seen that the main focus of functional programming is on data. Therefore,
    it shouldn't be any surprise that functional programming is a good candidate for
    processing real-time data streams. The composability of higher-order functions
    such as `map`, `reduce`, or `filter`, plus the opportunities for parallel processing,
    make the functional style of design a great solution for reactive programming.
  prefs: []
  type: TYPE_NORMAL
- en: We won't go into much detail about reactive programming. Usually, specific libraries
    or frameworks are used that facilitate the implementation of such data flow processing,
    but with the elements we have up to now, we can write a small-scale example.
  prefs: []
  type: TYPE_NORMAL
- en: We need a few things. First, a data stream; second, a main thread that receives
    data and immediately passes it to a processing pipeline; and third, a way to get
    the output.
  prefs: []
  type: TYPE_NORMAL
- en: For the goal of this example, I will simply use the standard input as an input
    stream. We will input numbers from the keyboard and check whether they are prime
    in a reactive manner, thus keeping the main thread responsive at all times. This
    means we'll use the `async` function to create a `future` for every number we
    read from the keyboard. The output will simply be written to the output stream.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the same `is_prime` function as before, but add another function
    that prints to the standard output whether the value is prime or not:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'The `main` function is an infinite cycle that reads data from the input stream
    and starts a `future` every time a new value comes in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this code with some randomly typed values results in the following
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the results are returned as soon as possible, but the program
    allows new data to be introduced at all times.
  prefs: []
  type: TYPE_NORMAL
- en: I have to mention that, in order to avoid infinite cycles every time I compile
    the code for this chapter, the reactive example can be compiled and run with `make
    reactive`. You'll have to stop it with an interrupt since it's an infinite loop.
  prefs: []
  type: TYPE_NORMAL
- en: This is a basic reactive programming example. It can obviously become more complex
    with higher volumes of data, complex pipelines, and the parallelization of each
    pipeline among others. However, we achieved our goal for this section—to give
    you a taste of reactive programming and how we can use functional constructs and
    asynchronous calls to make it work.
  prefs: []
  type: TYPE_NORMAL
- en: We've discussed a lot about optimizing execution time, looking at various ways
    that help us accomplish faster performance. It's now time to look at a situation
    where we want to reduce the memory usage of our programs.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing memory usage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The method we''ve discussed so far, for structuring code in a functional way,
    involves passing multiple times through a collection that is treated as immutable.
    As a result, this can lead to copies of the collection. Let''s look, for example,
    at a simple code sample that uses `transform` to increment all the elements of
    a vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: This implementation leads to a lot of memory allocations. First, the `manyNumbers` vector
    is copied into `transformAll`. Then, `result.push_back()` is automatically called,
    potentially resulting in memory allocation. Finally, the `result` is returned,
    but the initial `manyNumbers` vector is still allocated.
  prefs: []
  type: TYPE_NORMAL
- en: We can improve some of these problems immediately, but it's also worth discussing
    how they compare with other possible optimizations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to carry out the tests, we will need to work with large collections
    and a way to measure the memory allocation for a process. The first part is easy—just
    allocate a lot of 64-bit values (the long, long type on my compiler); enough to
    allocate 1 GB of RAM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'The second part is a bit more difficult. Fortunately, on my Ubuntu 18.04 system,
    I can watch the memory for a process in a file in `/proc/PID/status`, in which
    PID is the process identifier. With a bit of Bash magic, I can create a `makefile`
    recipe that outputs memory values taken every 0.1 s into a file, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: You'll notice the `-DNO_MOVE_ITERATOR` argument; this is a compilation directive
    that allows me to compile the same file for different goals, in order to check
    the memory footprint of multiple solutions. This means that our previous test
    is written within an `#if NO_MOVE_ITERATOR` directive.
  prefs: []
  type: TYPE_NORMAL
- en: There's only one caveat—since I used the bash `watch` command to generate the
    output, you will need to press a key after running `make memoryConsumptionNoMoveIterator`,
    as well as for every other memory logs recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this set up, let''s improve `transformAll` to use less memory, and look
    at the output. We need to use reference types and allocate memory for the result
    from the beginning, as shown in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: As expected, the result of the improvement is that the maximum allocation starts
    from 0.99 GB, but jumps to 1.96 GB, which is roughly double.
  prefs: []
  type: TYPE_NORMAL
- en: We need to put this value in context. Let's first measure what a simple `for`
    loop can do, and compare the result with the same algorithm implemented with `transform`.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring memory for a simple for loop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The solution with a `for` loop is very simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: When measuring the memory, there's no surprise—the footprint stays at 0.99 GB
    during the whole process. Can we achieve this result with `transform` as well?
    Well, there's one version of `transform` that can modify the collection in place.
    Let's put it to the test.
  prefs: []
  type: TYPE_NORMAL
- en: Measure memory for in-place transform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To use `transform` in place, we need to provide the destination iterator parameter, `source.begin()`, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: According to the documentation, this is supposed to change in the same collection;
    therefore, it shouldn't allocate any more memory. As expected, it has the same
    behavior as a simple `for` loop and the memory footprint stays at 0.99 GB for
    the whole duration of the program.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, you may notice that we''re not returning the value now to avoid a
    copy. I like transform-to-return values though, and we have another option, using
    move semantics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'To make the call compile, we need to pass in the type of the source when calling
    `transformAllInPlace`, so our test changes to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: Let's measure to see if the move semantics helps in any way. The result is as
    expected; the memory footprint stays at 0.99 GB during the whole runtime.
  prefs: []
  type: TYPE_NORMAL
- en: This leads to an interesting idea. What if we use move semantics in the call
    to `transform`?
  prefs: []
  type: TYPE_NORMAL
- en: Transform with the move iterator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can rewrite our `transform` function to use move iterators as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'In theory, what this should do is move values to the destination rather than
    copying them, thus keeping the memory footprint low. To put it to the test, we
    run the same test while recording the memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: The result is unexpected; the memory starts at 0.99 GB, rises to 1.96 GB (probably
    after the `transform` call), and then goes back to 0.99 GB (most likely, the result
    of `source.clear()`). I tried multiple variants to avoid this behavior, but couldn't
    find a solution to keep the memory footprint at 0.99 GB. This appears to be a
    problem with the implementation of move iterators; I advise you to test it on
    your compiler to find out whether it works or not.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing the solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The solutions using in-place or move semantics, while reducing the memory footprint,
    only work when the source data is not required for additional computations. If
    you plan to reuse the data for other computations, there's no way around preserving
    the initial collection. Moreover, it's unclear whether these calls can run in
    parallel; since g++ doesn't yet implement parallel execution policies I can't
    test them, so I will leave this question as an exercise for the reader.
  prefs: []
  type: TYPE_NORMAL
- en: But what do functional programming languages do in order to reduce memory footprint?
    The answer is very interesting.
  prefs: []
  type: TYPE_NORMAL
- en: Immutable data structures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Purely functional programming languages use a combination of immutable data
    structures and garbage collection. Each call to modify a data structure creates,
    what seems to be, a copy of the initial data structure, with only one element
    changed. The initial structure is not affected in any way. However, this is done
    using pointers; basically, the new data structure is the same as the initial one,
    except there is a pointer towards the changed value. When discarding the initial
    collection, the old value is no longer used and the garbage collector automatically
    removes it from memory.
  prefs: []
  type: TYPE_NORMAL
- en: This mechanism takes full advantage of immutability, allowing optimizations
    that are unavailable to C++. Moreover, the implementation is usually recursive,
    which also takes advantage of tail recursion optimization.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, it is possible to implement such data structures in C++. An example
    is a library called **immer**, which you can find on GitHub at [https://github.com/arximboldi/immer](https://github.com/arximboldi/immer).
    Immer implements a number of immutable collections. We will look at `immer::vector`;
    every time we call an operation that would normally modify the vector (such as
    `push_back`), `immer::vector` returns a new collection. Each value returned can
    be constant, since it never changes. I wrote a small test using immer 0.5.0 in
    the chapter code, showcasing the usage of `immer::vector`, which you can see in
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: I will not go into more detail regarding immutable data structures; however,
    I strongly advise you to take a look at the documentation on the *immer* website
    ([https://sinusoid.es/immer/introduction.html](https://sinusoid.es/immer/introduction.html))
    and play with the library.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ve seen that performance optimization is a complex topic. As C++ programmers,
    we are primed to require more performance from our code; the question we asked
    in this chapter was: is it possible to optimize code written in a functional style?'
  prefs: []
  type: TYPE_NORMAL
- en: The answer is—yes, if you measure and if you have a clear goal. Do we need a
    specific computation to finish more quickly? Do we need to reduce the memory footprint?
    What area of the application requires the most performance improvements? How much
    do we want to do weird point optimizations that might need rewriting with the
    next compiler, library, or platform version? These are all questions you need
    to answer before moving on to optimize your code.
  prefs: []
  type: TYPE_NORMAL
- en: However, we have seen that functional programming has a huge benefit when it
    comes to using all the cores on a computer. While we're waiting for the standard
    implementation of parallel execution for higher-order functions, we can take advantage
    of immutability by writing our own parallel algorithms. Recursion is another staple
    of functional programming and we can take advantage of tail recursion optimization
    whenever we use it.
  prefs: []
  type: TYPE_NORMAL
- en: As for memory consumption, immutable data structures implemented in third-party
    libraries, and carefully optimizing the higher-order functions we're using depending
    on their goal, can help us maintain the simplicity of the code, while the complexity
    happens in specific places in the code. Move semantics can be used when we throw
    away the source collections, but remember to check it works with parallel execution.
  prefs: []
  type: TYPE_NORMAL
- en: Above all, I hope you've learned that measuring is the most important part of
    performance optimization. After all, if you don't know where you are and where
    you need to go, how can you make the trip?
  prefs: []
  type: TYPE_NORMAL
- en: We will continue our journey with functional programming by taking advantage
    of data generators for our tests. It's time to look at property-based testing.
  prefs: []
  type: TYPE_NORMAL
