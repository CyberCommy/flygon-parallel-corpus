- en: Best Practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As with most things, it's best to avoid making mistakes rather than correcting
    them afterwards. This chapter looks at a number of common mistakes and design
    issues with multithreaded applications, and shows ways to avoid the common - and
    less common - issues.
  prefs: []
  type: TYPE_NORMAL
- en: 'Topics in this chapter include:'
  prefs: []
  type: TYPE_NORMAL
- en: Common multithreading issues, such as deadlocks and data races.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The proper use of mutexes, locks, and pitfalls.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Potential issues when using static initialization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Proper multithreading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the preceding chapters, we have seen a variety of potential issues which
    can occur when writing multithreaded code. These range from the obvious ones,
    such as two threads not being able to write to the same location at the same time,
    to the more subtle, such as incorrect usage of a mutex.
  prefs: []
  type: TYPE_NORMAL
- en: There are also many issues with elements which aren't directly part of multithreaded
    code, yet which can nevertheless cause seemingly random crashes and other frustrating
    issues. One example of this is static initialization of variables. In the following
    sections, we'll be looking at all of these issues and many more, as well as ways
    to prevent ever having to deal with them.
  prefs: []
  type: TYPE_NORMAL
- en: As with many things in life, they are interesting experiences, but you generally
    do not care to repeat them.
  prefs: []
  type: TYPE_NORMAL
- en: Wrongful expectations - deadlocks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A deadlock is described pretty succinctly by its name already. It occurs when
    two or more processes attempt to gain access to a resource which the other is
    holding, while that other thread is simultaneously waiting to gain access to a
    resource which it is holding.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example:'
  prefs: []
  type: TYPE_NORMAL
- en: Thread 1 gains access to resource A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Thread 1 and 2 both want to gain access to resource B
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Thread 2 wins and now owns B, with thread 1 still waiting on B
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Thread 2 wants to use A now, and waits for access
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Both thread 1 and 2 wait forever for a resource
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this situation, we assume that the thread will be able to gain access to
    each resource at some point, while the opposite is true, thanks to each thread
    holding on to the resource which the other thread needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Visualized, this deadlock process would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00017.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'This makes it clear that two basic rules when it comes to preventing deadlocks
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: Try to never hold more than one lock at any time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Release any held locks as soon as you can.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We saw a real-life example of this in [Chapter 4](part0076.html#28FAO0-1ab5991b318547348fc444437bdacb24),
    *Thread Synchronization and Communication*, when we looked at the dispatcher demonstration
    code. This code involves two mutexes, to safe-guard access to two data structures:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The mutexes here are the `workersMutex` and `requestsMutex` variables. We can
    clearly see how at no point do we hold onto a mutex before trying to obtain access
    to the other one. We explicitly lock the `workersMutex` at the beginning of the
    method, so that we can safely check whether the workers data structure is empty
    or not.
  prefs: []
  type: TYPE_NORMAL
- en: If it's not empty, we hand the new request to a worker. Then, as we are done
    with the workers, data structure, we release the mutex. At this point, we retain
    zero mutexes. Nothing too complex here, as we just use a single mutex.
  prefs: []
  type: TYPE_NORMAL
- en: 'The interesting thing is in the else statement, for when there is no waiting
    worker and we need to obtain the second mutex. As we enter this scope, we retain
    one mutex. We could just attempt to obtain the `requestsMutex` and assume that
    it will work, yet this may deadlock, for this simple reason:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The accompanying function to the earlier preceding function we see also uses
    these two mutexes. Worse, this function runs in a separate thread. As a result,
    when the first function holds the `workersMutex` as it tries to obtain the `requestsMutex`,
    with this second function simultaneously holding the latter, while trying to obtain
    the former, we hit a deadlock.
  prefs: []
  type: TYPE_NORMAL
- en: In the functions, as we see them here, however, both rules have been implemented
    successfully; we never hold more than one lock at a time, and we release any locks
    we hold as soon as we can. This can be seen in both else cases, where as we enter
    them, we first release any locks we do not need any more.
  prefs: []
  type: TYPE_NORMAL
- en: 'As in either case, we do not need to check respectively, the workers or requests
    data structures any more; we can release the relevant lock before we do anything
    else. This results in the following visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00018.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: It is of course possible that we may need to use data contained in two or more
    data structures or variables; data which is used by other threads simultaneously.
    It may be difficult to ensure that there is no chance of a deadlock in the resulting
    code.
  prefs: []
  type: TYPE_NORMAL
- en: Here, one may want to consider using temporary variables or similar. By locking
    the mutex, copying the relevant data, and immediately releasing the lock, there
    is no chance of deadlock with that mutex. Even if one has to write back results
    to the data structure, this can be done in a separate action.
  prefs: []
  type: TYPE_NORMAL
- en: 'This adds two more rules in preventing deadlocks:'
  prefs: []
  type: TYPE_NORMAL
- en: Try to never hold more than one lock at a time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Release any held locks as soon as you can.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Never hold a lock any longer than is absolutely necessary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When holding multiple locks, mind their order.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Being careless - data races
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A data race, also known as a race condition, occurs when two or more threads
    attempt to write to the same shared memory simultaneously. As a result, the state
    of the shared memory during and at the end of the sequence of instructions executed
    by each thread is by definition, non-deterministic.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we saw in [Chapter 6](part0129.html#3R0OI0-1ab5991b318547348fc444437bdacb24),
    *Debugging Multithreaded Code*, data races are reported quite often by tools used
    to debug multi-threaded applications. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The code which generated the preceding warning was the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Consider this code in the `Worker` instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We also have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `running` is a Boolean variable that is being set to `false` (writing
    to it from one thread), signaling the worker thread that it should terminate its
    waiting loop, where reading the Boolean variable is done from a different process,
    the main thread versus the worker thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00019.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This particular example's warning was due to a Boolean variable being simultaneously
    written and read. Naturally, the reason why this specific situation is safe has
    to do with atomics, as explained in detail in [Chapter 8](part0169.html#515F20-1ab5991b318547348fc444437bdacb24),
    *Atomic Operations - Working with the Hardware*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reason why even an operation like this is potentially risky is because
    the reading operation may occur while the variable is still in the process of
    being updated. In the case of, for example, a 32-bit integer, depending on the
    hardware architecture, updating this variable might be done in one operation,
    or multiple. In the latter case, the reading operation might read an intermediate
    value with unpredictable results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00020.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'A more comical situation occurs when multiple threads write to a standard with
    out using, for example, `cout`. As this stream is not thread-safe, the resulting
    output stream will contain bits and pieces of the input streams, from whenever
    either of the threads got a chance to write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00021.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The basic rules to prevent data races thus are:'
  prefs: []
  type: TYPE_NORMAL
- en: Never write to an unlocked, non-atomic, shared resource
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Never read from an unlocked, non-atomic, shared resource
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This essentially means that any write or read has to be thread-safe. If one
    writes to shared memory, no other thread should be able to write to it at the
    same time. Similarly, when we read from a shared resource, we need to ensure that,
    at most, only other threads are also reading the shared resource.
  prefs: []
  type: TYPE_NORMAL
- en: This level of mutual exclusion is naturally accomplished by mutexes as we have
    seen in the preceding chapters, with a refinement offered in read-write locks,
    which allows for simultaneous readers while having writes as fully mutually exclusive
    events.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, there are also gotchas with mutexes, as we will see in the following
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Mutexes aren't magic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mutexes form the basis of practically all forms of mutual exclusion APIs. At
    their core, they seem extremely simple, only one thread can own a mutex, with
    other threads neatly waiting in a queue until they can obtain the lock on the
    mutex.
  prefs: []
  type: TYPE_NORMAL
- en: 'One might even picture this process as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00022.gif)'
  prefs: []
  type: TYPE_IMG
- en: The reality is of course less pretty, mostly owing to the practical limitations
    imposed on us by the hardware. One obvious limitation is that synchronization
    primitives aren't free. Even though they are implemented in the hardware, it takes
    multiple calls to make them work.
  prefs: []
  type: TYPE_NORMAL
- en: The two most common ways to implement mutexes in the hardware is to use either
    the **test-and-set** (**TAS**) or **compare-and-swap** (**CAS**) CPU features.
  prefs: []
  type: TYPE_NORMAL
- en: Test-and-set is usually implemented as two assembly-level instructions, which
    are executed autonomously, meaning that they cannot be interrupted. The first
    instruction tests whether a certain memory area is set to a 1 or zero. The second
    instruction is executed only when the value is a zero (`false`). This means that
    the mutex was not locked yet. The second instruction thus sets the memory area
    to a 1, locking the mutex.
  prefs: []
  type: TYPE_NORMAL
- en: 'In pseudo-code, this would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Compare-and-swap is a lesser used variation on this, which performs a comparison
    operation on a memory location and a given value, only replacing the contents
    of that memory location if the first two match:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In either case, one would have to actively repeat either function until a positive
    value is returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Here, a simple while loop is used to constantly poll the memory area (marked
    as volatile to prevent possibly problematic compiler optimizations). Generally,
    an algorithm is used for this which slowly reduces the rate at which it is being
    polled. This is to reduce the amount of pressure on the processor and memory systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'This makes it clear that the use of a mutex is not free, but that each thread
    which waits for a mutex lock actively uses resources. As a result, the general
    rules here are:'
  prefs: []
  type: TYPE_NORMAL
- en: Ensure that threads wait for mutexes and similar locks as briefly as possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use condition variables or timers for longer waiting periods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Locks are fancy mutexes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we saw earlier in the section on mutexes, there are some issues to keep in
    mind when using mutexes. Naturally these also apply when using locks and other
    mechanisms based on mutexes, even if some of these issues are smoothed over by
    these APIs.
  prefs: []
  type: TYPE_NORMAL
- en: One of the things one may get confused about when first using multithreading
    APIs is what the actual difference is between the different synchronization types.
    As we covered earlier in this chapter, mutexes underlie virtually all synchronization
    mechanisms, merely differing in the way that they use mutexes to implement the
    provided functionality.
  prefs: []
  type: TYPE_NORMAL
- en: The important thing here is that they are not distinct synchronization mechanisms,
    but merely specializations of the basic mutex type. Whether one would use a regular
    mutex, a read/write lock, a semaphore - or even something as esoteric as a reentrant
    (recursive) mutex or lock - depends fully on the particular problem which one
    is trying to solve.
  prefs: []
  type: TYPE_NORMAL
- en: For the scheduler, we first encountered in [Chapter 4](part0076.html#28FAO0-1ab5991b318547348fc444437bdacb24),
    *Thread Aynchronization and Communication*, we used regular mutexes to protect
    the data structures containing the queued worker threads and requests. Since any
    access of either data structure would likely not only involve reading actions,
    but also the manipulation of the structure, it would not make sense there to use
    read/write locks. Similarly, recursive locks would not serve any purpose over
    the humble mutex.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each synchronization problem, one therefore has to ask the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Which requirements do I have?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which synchronization mechanism best fits these requirements?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It's therefore attractive to go for a complex type, but generally it's best
    to stick with the simpler type which fulfills all the requirements. When it comes
    to debugging one's implementation, precious time can be saved over a fancier implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Threads versus the future
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recently it has become popular to advise against the use of threads, instead
    advocating the use of other asynchronous processing mechanisms, such as `promise`.
    The reasons behind this are that the use of threads and the synchronization involved
    is complex and error-prone. Often one just wants to run a task in parallel and
    not concern oneself with how the result is obtained.
  prefs: []
  type: TYPE_NORMAL
- en: For simple tasks which would run only briefly, this can certainly make sense.
    The main advantage of a thread-based implementation will always be that one can
    fully customize its behavior. With a `promise`, one sends in a task to run and
    at the end, one gets the result out of a `future` instance. This is convenient
    for simple tasks, but obviously does not cover a lot of situations.
  prefs: []
  type: TYPE_NORMAL
- en: The best approach here is to first learn threads and synchronization mechanisms
    well, along with their limitations. Only after that does it really make sense
    to consider whether one wishes to use a promise, `packaged_task`, or a full-blown
    thread.
  prefs: []
  type: TYPE_NORMAL
- en: Another major consideration with these fancier, future-based APIs is that they
    are heavily template-based, which can make the debugging and troubleshooting of
    any issues which may occur significantly less easy than when using the more straightforward
    and low-level APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Static order of initialization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Static variables are variables which are declared only once, essentially existing
    in a global scope, though potentially only shared between instances of a particular
    class. It''s also possible to have classes which are completely static:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: As we can see here, static variables along with static functions seem like a
    very simple, yet powerful concept. While at its core this is true, there's a major
    issue which will catch the unwary when it comes to static variables and the initialization
    of classes. This is in the form of initialization order.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine what happens if we wish to use the preceding class from another class''
    static initialization, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: While this may seem like it would work fine, adding the first string to the
    class' map structure with the integer as key means there is a very good chance
    that this code will crash. The reason for this is simple, there is no guarantee
    that `Foo::string` is initialized at the point when we call `Foo::init()`. Trying
    to use an uninitialized map structure will thus lead to an exception.
  prefs: []
  type: TYPE_NORMAL
- en: In short, the initialization order of static variables is basically random,
    leading to non-deterministic behavior if this is not taken into account.
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution to this problem is fairly simple. Basically, the goal is to make
    the initialization of more complex static variables explicit instead of implicit
    like in the preceding example. For this we modify the Foo class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Starting at the top, we see that we no longer define the static map directly.
    Instead, we have a private function with the same name. This function's implementation
    is found at the bottom of this sample code. In it, we have a static pointer to
    a map structure with the familiar map definition.
  prefs: []
  type: TYPE_NORMAL
- en: When this function is called, a new map is created when there's no instance
    yet, due to it being a static variable. In the modified `init()` function, we
    see that we call the `strings()` function to obtain a reference to this instance.
    This is the explicit initialization part, as calling the function will always
    ensure that the map structure is initialized before we use it, solving the earlier
    problem we had.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also see a small optimization here: the `stringsStatic` variable we create
    is also static, meaning that we will only ever call the `strings()` function once.
    This makes repeated function calls unnecessary and regains the speed we would
    have had with the previous simple--but unstable--implementation.'
  prefs: []
  type: TYPE_NORMAL
- en: The essential rule with static variable initialization is thus, always use explicit
    initialization for non-trivial static variables.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at a number of good practices and rules to keep in
    mind when writing multithreaded code, along with some general advice. At this
    point, one should be able to avoid some of the bigger pitfalls and major sources
    of confusion when writing such code.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will be looking at how to use the underlying hardware
    to our advantage with atomic operations, along with the `<atomics>` header that
    was also introduced with C++11.
  prefs: []
  type: TYPE_NORMAL
