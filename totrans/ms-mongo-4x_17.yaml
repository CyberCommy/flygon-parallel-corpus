- en: Sharding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sharding is the ability to horizontally scale out our database by partitioning
    our datasets across different servers—the shards. It has been a feature of MongoDB
    since version 1.6 was released in August, 2010\. Foursquare and Bitly are two
    of MongoDB's most famous early customers, and have used the sharding feature from
    its inception all the way to its general release.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: How to design a sharding cluster and how to make the single most important decision
    concerning its use—choosing the shard key
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different sharding techniques and how to monitor and administrate sharded clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `mongos` router and how it is used to route our queries across different
    shards
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How we can recover from errors in our shard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why do we use sharding?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In database systems and computing systems in general, we have two ways to improve
    performance. The first one is to simply replace our servers with more powerful
    ones, keeping the same network topology and systems architecture. This is called
    **vertical scaling**.
  prefs: []
  type: TYPE_NORMAL
- en: An advantage of vertical scaling is that it is simple, from an operational standpoint,
    especially with cloud providers such as Amazon making it a matter of a few clicks
    to replace an **m2.medium** with an **m2.extralarge** server instance. Another
    advantage is that we don't need to make any code changes, and so there is little
    to no risk of something going catastrophically wrong.
  prefs: []
  type: TYPE_NORMAL
- en: The main disadvantage of vertical scaling is that there is a limit to it; we
    can only get servers that are as powerful as those that our cloud provider can
    give to us.
  prefs: []
  type: TYPE_NORMAL
- en: A related disadvantage is that getting more powerful servers generally comes
    with an increase in cost that is not linear but exponential. So, even if our cloud
    provider offers more powerful instances, we will hit the cost effectiveness barrier
    before we hit the limit of our department's credit card.
  prefs: []
  type: TYPE_NORMAL
- en: The second way to improve performance is by using the same servers with the
    same capacity and increase their number. This is called **horizontal scaling**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Horizontal scaling offers the advantage of theoretically being able to scale
    exponentially while remaining practical enough for real-world applications. The
    main disadvantage is that it can be operationally more complex and requires code
    changes and the careful design of the system upfront. Horizontal scaling is also
    more complex when it comes to the system because it requires communication between
    the different servers over network links that are not as reliable as inter-process
    communication on a single server. The following diagram shows the difference between
    horizontal and vertical scaling:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/18bb77a8-2036-44b5-b85b-16f97f38f403.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To understand scaling, it''s important to understand the limitations of single-server
    systems. A server is typically bound by one or more of the following characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**CPU**: A CPU-bound system is one that is limited by our CPU''s speed. A task
    such as the multiplication of matrices that can fit in RAM will be CPU bound because
    there is a specific number of steps that have to be performed in the CPU without
    any disk or memory access needed for the task to complete. In this case, CPU usage
    is the metric that we need to keep track of.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**I/O**: Input-output bound systems are similarly limited by the speed of our
    storage system (HDD or SSD). A task such as reading large files from a disk to
    load into memory will be I/O bound as there is little to do in terms of CPU processing;
    the great majority of the time is spent reading the files from the disk. The important
    metrics to keep track of are all the metrics related to disk access, the reads
    per second, and the writes per second, compared to the practical limit of our
    storage system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory and cache**: Memory-bound and cache-bound systems are restricted by
    the amount of available RAM memory and/or the cache size that we have assigned
    to them. A task that multiplies matrices larger than our RAM size will be memory
    bound, as it will need to page in/out data from the disk to perform the multiplication.
    The important metric to keep track of is the memory used. This may be misleading
    in MongoDB MMAPv1, as the storage engine will allocate as much memory as possible
    through the filesystem cache.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the WiredTiger storage engine, on the other hand, if we don't allocate enough
    memory for the core MongoDB process, out-of-memory errors may kill it, and this
    is something that we want to avoid at all costs.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring memory usage has to be done both directly through the operating system
    and indirectly by keeping a track of page in/out data. An increasing memory paging
    number is often an indication that we are running short of memory and the operating
    system is using virtual address space to keep up.
  prefs: []
  type: TYPE_NORMAL
- en: MongoDB, being a database system, is generally memory and I/O bound. Investing
    in SSD and more memory for our nodes is almost always a good investment. Most
    systems are a combination of one or more of the preceding limitations. Once we
    add more memory, our system may become CPU bound, as complex operations are almost
    always a combination of CPU, I/O, and memory usage.
  prefs: []
  type: TYPE_NORMAL
- en: MongoDB's sharding is simple enough to set up and operate, and this has contributed
    to its huge success over the years as it provides the advantages of horizontal
    scaling without requiring a large commitment of engineering and operations resources.
  prefs: []
  type: TYPE_NORMAL
- en: That being said, it's really important to get sharding right from the beginning,
    as it is extremely difficult from an operational standpoint to change the configuration
    once it has been set up. Sharding should not be an afterthought, but rather a
    key architectural design decision from an early point in the design process.
  prefs: []
  type: TYPE_NORMAL
- en: Architectural overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A sharded cluster is comprised of the following elements:'
  prefs: []
  type: TYPE_NORMAL
- en: Two or more shards. Each shard must be a replica set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One or more query routers (`mongos`). A `mongos` provides an interface between
    our application and the database.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A replica set of config servers. Config servers store metadata and configuration
    settings for the entire cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The relationships between these elements is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0b87bede-ff79-4863-a498-c5c3cf5b86b1.png)'
  prefs: []
  type: TYPE_IMG
- en: As of MongoDB 3.6, shards must be implemented as replica sets.
  prefs: []
  type: TYPE_NORMAL
- en: Development, continuous deployment, and staging environments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In preproduction environments, it may be overkill to use the full set of servers.
    For efficiency reasons, we may opt to use a more simplified architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest possible configuration that we can deploy for sharding is the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: One `mongos` router
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One sharded replica set with one MongoDB server and two arbiters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One replica set of config servers with one MongoDB server and two arbiters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This should be strictly used for development and testing as this architecture
    defies most of the advantages that a replica set provides, such as high availability,
    scalability, and replication of data.
  prefs: []
  type: TYPE_NORMAL
- en: Staging is strongly recommended to mirror our production environment in terms
    of its servers, configuration, and (if possible) dataset requirements too, in
    order to avoid surprises at deployment time.
  prefs: []
  type: TYPE_NORMAL
- en: Planning ahead with sharding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we will see in the next sections, sharding is complicated and expensive,
    operation wise. It is important to plan ahead and make sure that we start the
    sharding process long before we hit our system's limits.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some rough guidelines on when you need to start sharding are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: When you have a CPU utilization of less than 70% on average
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When I/O (and especially write) capacity is less than 80%
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When memory utilization is less than 70% on average
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As sharding helps with write performance, it's important to keep an eye on our
    I/O write capacity and the requirements of our application.
  prefs: []
  type: TYPE_NORMAL
- en: Don't wait until the last minute to start sharding in an already busy-up-to-the-neck
    MongoDB system, as it can have unintended consequences.
  prefs: []
  type: TYPE_NORMAL
- en: Sharding setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sharding is performed at the collection level. We can have collections that
    we don't want or need to shard for several reasons. We can leave these collections
    unsharded.
  prefs: []
  type: TYPE_NORMAL
- en: These collections will be stored in the primary shard. The primary shard is
    different for each database in MongoDB. The primary shard is automatically selected
    by MongoDB when we create a new database in a sharded environment. MongoDB will
    pick the shard that has the least data stored at the moment of creation.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to change the primary shard at any other point, we can issue the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: With this, we move the database named `mongo_books` to the shard named `UK_based`.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the shard key
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Choosing our shard key is the most important decision we need to make: once
    we shard our data and deploy our cluster, it becomes very difficult to change
    the shard key. First, we will go through the process of changing the shard key.'
  prefs: []
  type: TYPE_NORMAL
- en: Changing the shard key
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is no command or simple procedure to change the shard key in MongoDB.
    The only way to change the shard key involves backing up and restoring all of
    our data, something that may range from being extremely difficult to impossible
    in high-load production environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the steps that we need to go through in order to change the
    shard key:'
  prefs: []
  type: TYPE_NORMAL
- en: Export all data from MongoDB
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Drop the original sharded collection
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configure sharding with the new key
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Presplit the new shard key range
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Restore our data back into MongoDB
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Of these steps, step 4 is the one that needs further explanation.
  prefs: []
  type: TYPE_NORMAL
- en: MongoDB uses chunks to split data in a sharded collection. If we bootstrap a
    MongoDB sharded cluster from scratch, chunks will be calculated automatically
    by MongoDB. MongoDB will then distribute the chunks across different shards to
    ensure that there are an equal number of chunks in each shard.
  prefs: []
  type: TYPE_NORMAL
- en: The only time when we cannot really do this is when we want to load data into
    a newly sharded collection.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reasons for this are threefold:'
  prefs: []
  type: TYPE_NORMAL
- en: MongoDB creates splits only after an `insert` operation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chunk migration will copy all of the data in that chunk from one shard to another.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `floor(n/2)` chunk migrations can happen at any given time, where `n` is
    the number of shards we have. Even with three shards, this is only a `floor(1.5)=1`
    chunk migration at a time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These three limitations mean that letting MongoDB figure it out on its own will
    definitely take much longer, and may result in an eventual failure. This is why
    we want to presplit our data and give MongoDB some guidance on where our chunks should go.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example of the `mongo_books` database and the `books` collection, this
    would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `middle` command parameter will split our key space in documents that have `id` less
    than or equal to `50` and documents that have `id` greater than `50`. There is
    no need for a document to exist in our collection with `id` that is equal to `50` as
    this will only serve as the guidance value for our partitions.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we chose `50`, as we assume that our keys follow a uniform
    distribution (that is, there is the same count of keys for each value) in the
    range of values from `0` to `100`.
  prefs: []
  type: TYPE_NORMAL
- en: We should aim to create at least 20-30 chunks to grant MongoDB flexibility in
    potential migrations. We can also use `bounds` and `find` instead of `middle`
    if we want to manually define the partition key, but both parameters need data
    to exist in our collection before applying them.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the correct shard key
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After the previous section, it's now self-evident that we need to take the choice
    of our shard key into consideration as it is a decision that we have to stick
    with.
  prefs: []
  type: TYPE_NORMAL
- en: 'A great shard key has three characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: High cardinality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Low frequency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nonmonotonic changes in value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will go over the definitions of these three properties first to understand
    what they mean:'
  prefs: []
  type: TYPE_NORMAL
- en: '**High cardinality**: It means that the shard key must have as many distinct
    values as possible. A Boolean can take only the values of `true`/`false`, and
    so it is a bad shard key choice. A 64-bit long value field that can take any value
    from *−(2^63)* to *2^63−1* is a good shard key choice, in terms of cardinality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Low frequency**: It directly relates to the argument about high cardinality.
    A low-frequency shard key will have a distribution of values as close to a perfectly
    random/uniform distribution. Using the example of our 64-bit long value, it is
    of little use to us if we have a field that can take values ranging from *−(2^63)*
    to *2^63−1* if we end up observing the values of zero and one all the time. In
    fact, it is as bad as using a Boolean field, which can also take only two values. If
    we have a shard key with high-frequency values, we will end up with chunks that
    are indivisible. These chunks cannot be further divided and will grow in size, negatively affecting
    the performance of the shard that contains them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Nonmonotonically changing values**:It mean that our shard key should not
    be, for example, an integer that always increases with every new insert. If we
    choose a monotonically increasing value as our shard key, this will result in
    all writes ending up in the last of all of our shards, limiting our write performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we want to use a monotonically changing value as the shard key, we should
    consider using hash-based sharding.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will describe different sharding strategies, including
    their advantages and disadvantages.
  prefs: []
  type: TYPE_NORMAL
- en: Range-based sharding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The default and most widely used sharding strategy is range-based sharding.
    This strategy will split our collection's data into chunks, grouping documents
    with nearby values in the same shard.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our example database and collection, `mongo_books` and `books` respectively,
    we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This creates a range-based shard key on `id` with an ascending direction. The
    direction of our shard key will determine which documents will end up in the first
    shard and which ones in the subsequent ones.
  prefs: []
  type: TYPE_NORMAL
- en: This is a good strategy if we plan to have range-based queries, as these will
    be directed to the shard that holds the result set instead of having to query
    all shards.
  prefs: []
  type: TYPE_NORMAL
- en: Hash-based sharding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we don't have a shard key (or can't create one) that achieves the three goals
    mentioned previously, we can use the alternative strategy of using hash-based
    sharding. In this case, we are trading data distribution with query isolation.
  prefs: []
  type: TYPE_NORMAL
- en: Hash-based sharding will take the values of our shard key and hash them in a
    way that guarantees close to uniform distribution. This way, we can be sure that
    our data will be evenly distributed across the shards. The downside is that only
    exact match queries will get routed to the exact shard that holds the value. Any
    range query will have to go out and fetch data from all the shards.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our example database and collection (`mongo_books` and `books` respectively),
    we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Similar to the preceding example, we are now using the `id` field as our hashed
    shard key.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we use fields with float values for hash-based sharding. Then we will
    end up with collisions if the precision of our floats is more than *2^53*. These
    fields should be avoided where possible.
  prefs: []
  type: TYPE_NORMAL
- en: Coming up with our own key
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Range-based sharding does not need to be confined to a single key. In fact,
    in most cases, we would like to combine multiple keys to achieve high cardinality
    and low frequency.
  prefs: []
  type: TYPE_NORMAL
- en: A common pattern is to combine a low-cardinality first part (but still with
    a number of distinct values more than two times the number of shards that we have)
    with a high-cardinality key as its second field. This achieves both read and write
    distribution from the first part of the sharding key and then cardinality and
    read-locality from the second part.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if we don't have range queries, then we can get away with
    using hash-based sharding on a primary key, as this will exactly target the shard
    and document that we are going after.
  prefs: []
  type: TYPE_NORMAL
- en: To make things more complicated, these considerations may change depending on
    our workload. A workload that consists almost exclusively (say 99.5%) of reads
    won't care about write distribution. We can use the built-in `_id` field as our
    shard key, and this will only add 0.5% load to the last shard. Our reads will
    still be distributed across shards. Unfortunately, in most cases, this is not
    simple.
  prefs: []
  type: TYPE_NORMAL
- en: Location-based data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Because of government regulations and the desire to have our data as close to
    our users as possible, there is often a constraint and need to limit data in a
    specific data center. By placing different shards at different data centers, we
    can satisfy this requirement.
  prefs: []
  type: TYPE_NORMAL
- en: Every shard is essentially a replica set. We can connect to it as we would connect
    to a replica set for administrative and maintenance operations. We can query one
    shard's data directly, but the results will only be a subset of the full sharded
    result set.
  prefs: []
  type: TYPE_NORMAL
- en: Sharding administration and monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sharded MongoDB environments have some unique challenges and limitations compared
    to single-server or replica set deployments. In this section, we will explore
    how MongoDB balances our data across shards using chunks and how we can tweak
    them if we need to. Together, we will explore some of sharding's design limitations.
  prefs: []
  type: TYPE_NORMAL
- en: Balancing data – how to track and keep our data balanced
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the advantages of sharding in MongoDB is that it is mostly transparent
    to the application and requires minimal administration and operational effort.
  prefs: []
  type: TYPE_NORMAL
- en: One of the core tasks that MongoDB needs to perform continuously is balancing
    data between shards. No matter whether we implement range-based or hash-based
    sharding, MongoDB will need to calculate bounds for the hashed field to be able
    to figure out which shard to direct every new document insert or update toward.
    As our data grows, these bounds may need to get readjusted to avoid having a hot
    shard that ends up with the majority of our data.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the sake of this example, let''s assume that there is a data type named
    `extra_tiny_int` with integer values from [`-12`, `12`). If we enable sharding
    on this `extra_tiny_int` field, then the initial bounds of our data will be the
    whole range of values denoted by `$minKey: -12` and `$maxKey: 11`.'
  prefs: []
  type: TYPE_NORMAL
- en: After we insert some initial data, MongoDB will generate chunks and recalculate
    the bounds of each chunk to try and balance our data.
  prefs: []
  type: TYPE_NORMAL
- en: By default, the initial number of chunks created by MongoDB is *2 × number of
    shards*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case of two shards and four initial chunks, the initial bounds will
    be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chunk1: [-12..-6)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chunk2:  [-6..0)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chunk3:  [0..6)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chunk4:  [6,12)* where *''[''* is inclusive and *'')''* is not inclusive'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the preceding explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/75712448-e2de-4265-a8ce-458548db4503.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After we insert some data, our chunks will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*ShardA*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Chunk1*:* -12,-8,-7*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Chunk2*:*  -6*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*ShardB*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Chunk3*:* 0, 2      *'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Chunk4*: *7,8,9,10,11,11,11,11*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the preceding explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7b631e16-8db1-4e9d-b9d3-56585f600f75.png)'
  prefs: []
  type: TYPE_IMG
- en: In this case, we observe that c*hunk4* has more items than any other chunk.
    MongoDB will first split *chunk4* into two new chunks, attempting to keep the
    size of each chunk under a certain threshold (64 MB, by default).
  prefs: []
  type: TYPE_NORMAL
- en: Now, instead of c*hunk4*, we have *chunk4A*:*7,8,9,10* and *chunk4B*: *11,11,11,11*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the preceding explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/88357d07-47cd-4fc3-b6b3-a01a761d08f2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The new bounds of this are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*chunk4A*: *[6,11)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*chunk4B*: *[11,12)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that *chunk4B* can only hold one value. This is now an indivisible chunk—a
    chunk that cannot be broken down into smaller ones anymore—and will grow in size
    unbounded, causing potential performance issues down the line.
  prefs: []
  type: TYPE_NORMAL
- en: This clarifies why we need to use a high-cardinality field as our shard key
    and why something like a Boolean, which only has `true`/`false` values, is a bad
    choice for a shard key.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, we now have two chunks in *ShardA* and three chunks in *ShardB*.
    Let''s look at the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Number of chunks** | **Migration threshold** |'
  prefs: []
  type: TYPE_TB
- en: '| *≤19* | *2* |'
  prefs: []
  type: TYPE_TB
- en: '| *20-79* | *4* |'
  prefs: []
  type: TYPE_TB
- en: '| *≥80* | *8* |'
  prefs: []
  type: TYPE_TB
- en: We have not reached our migration threshold yet, since *3-2 = 1*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The migration threshold is calculated as the number of chunks in the shard
    with the highest count of chunks and the number of chunks in the shard with the
    lowest count of chunks, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Shard1 -> 85 chunks*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Shard2 -> 86 chunks*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Shard3 -> 92 chunks*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the preceding example, balancing will not occur until *Shard3* (or *Shard2*)
    reaches *93* chunks because the migration threshold is *8* for *≥80* chunks and
    the difference between *Shard1* and *Shard3* is still *7* chunks (*92-85*).
  prefs: []
  type: TYPE_NORMAL
- en: If we continue adding data in *chunk4A*, it will eventually be split into *chunk4A1*
    and *chunk4A2*.
  prefs: []
  type: TYPE_NORMAL
- en: Now we have four chunks in *ShardB* (*chunk3*, *chunk4A1*, *chunk4A2*, and *chunk4B*)
    and two chunks in *ShardA* (*chunk1* and *chunk2*).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the relationships of the chunks to the shards:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c609adaf-fda7-40e0-b569-ccb75e021fcc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The MongoDB balancer will now migrate one chunk from *ShardB* to *ShardA* as
    *4-2 = 2*, reaching the migration threshold for fewer than *20* chunks. The balancer
    will adjust the boundaries between the two shards in order to be able to query
    more effectively (targeted queries). The following diagram illustrates the preceding
    explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/43018e6d-96c8-4920-a51f-566b0211645a.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see from the preceding diagram, MongoDB will try to split *>64* MB
    chunks in half in terms of size. The bounds between the two resulting chunks may
    be completely uneven if our data distribution is uneven to begin with. MongoDB
    can split chunks into smaller ones, but cannot merge them automatically. We need
    to manually merge chunks, a delicate and operationally expensive procedure.
  prefs: []
  type: TYPE_NORMAL
- en: Chunk administration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of the time, we should leave chunk administration to MongoDB. We should
    manually manage chunks at the start, upon receiving the initial load of data,
    when we change our configuration from a replica set to sharding.
  prefs: []
  type: TYPE_NORMAL
- en: Moving chunks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To move a chunk manually, we need to issue the following command after connecting
    to `mongos` and the `admin` database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the preceding command, we move the chunk containing the document with
    `id: 50` (this has to be the shard key) from the `books` collection of the `mongo_books` database to
    the new shard named `shard1.packtdb.com`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also more explicitly define the bounds of the chunk that we want to
    move. Now the syntax is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Here, `minValue` and `maxValue` are the values that we get from `db.printShardingStatus()`.
  prefs: []
  type: TYPE_NORMAL
- en: In the example used previously, for *chunk2*, `minValue` would be `-6` and `maxValue`
    would be `0`.
  prefs: []
  type: TYPE_NORMAL
- en: Do not use `find` in hash-based sharding. Use `bounds` instead.
  prefs: []
  type: TYPE_NORMAL
- en: Changing the default chunk size
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To change the default chunk size, we need to connect to a `mongos` router and,
    consequently, to the `config` database.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we issue the following command to change our global `chunksize` to `16`
    MB:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The main reasoning behind changing `chunksize` comes from cases where the default
    `chunksize` of 64 MB can cause more I/O than our hardware can handle. In this
    case, defining a smaller `chunksize` will result in more frequent but less data-intensive
    migrations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Changing the default chunk size has the following drawbacks:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating more splits by defining a smaller chunk size cannot be undone automatically.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increasing the chunk size will not force any chunk migration; instead, chunks
    will grow through inserts and updates until they reach the new size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lowering the chunk size may take quite some time to complete.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automatic splitting to comply with the new chunk size if it is lower will only
    happen upon an insert or update. We may have chunks that don't get any write operations,
    and thus will not be changed in size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The chunk size can be 1 to 1024 MB.
  prefs: []
  type: TYPE_NORMAL
- en: Jumbo chunks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In rare cases, we may end up with jumbo chunks, chunks that are larger than
    the chunk size and cannot be split by MongoDB. We may also run into the same situation
    if the number of documents in our chunk exceeds the maximum document limit.
  prefs: []
  type: TYPE_NORMAL
- en: These chunks will have the `jumbo` flag enabled. Ideally, MongoDB will keep
    track of whether it can split the chunk, and, as soon as it can, it will get split;
    however, we may decide that we want to manually trigger the split before MongoDB
    does.
  prefs: []
  type: TYPE_NORMAL
- en: 'The way to do this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Connect via shell to your `mongos` router and run the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Identify the chunk that has `jumbo` in its description using the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Invoke `splitAt()` or `splitFind()` manually to split the chunk on the `books` collection of
    the `mongo_books` database at the `id` that is equal to `8` using the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The `splitAt()` function will split based on the split point we define. The
    two new splits may or may not be balanced.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, if we want to leave it to MongoDB to find where to split our
    chunk, we can use `splitFind`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `splitFind` phrase will try to find the chunk that the `id:7` query belongs
    to and automatically define the new bounds for the split chunks so that they are
    roughly balanced.
  prefs: []
  type: TYPE_NORMAL
- en: In both cases, MongoDB will try to split the chunk, and if successful, it will
    remove the `jumbo` flag from it.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the preceding operation is unsuccessful, then, and only then, should we try
    stopping the balancer first, while also verifying the output and waiting for any
    pending migrations to finish first, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This should return `false `
  prefs: []
  type: TYPE_NORMAL
- en: Wait for any `waiting…` messages to stop printing, and then find the `jumbo`
    flagged chunk in the same way as before.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then update the `chunks` collection in your `config` database of the `mongos`
    router, like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command is a regular `update()` command, with the first argument
    being the `find()` part to find out which document to update and the second argument
    being the operation to apply to it (`$unset: jumbo flag`).'
  prefs: []
  type: TYPE_NORMAL
- en: 'After all this is done, we re-enable the balancer, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we connect to the `admin` database to flush the new configuration to
    all nodes, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Always back up the `config` database before modifying any state manually.
  prefs: []
  type: TYPE_NORMAL
- en: Merging chunks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have seen previously, usually MongoDB will adjust the bounds for each
    chunk in our shard to make sure that our data is equally distributed. This may
    not work in some cases—especially when we define the chunks manually—if our data
    distribution is surprisingly unbalanced, or if we have many `delete` operations
    in our shard.
  prefs: []
  type: TYPE_NORMAL
- en: Having empty chunks will invoke unnecessary chunk migrations and give MongoDB
    a false impression of which chunk needs to be migrated. As we have explained before,
    the threshold for chunk migration is dependent on the number of chunks that each
    shard holds. Having empty chunks may or may not trigger the balancer when it's
    needed.
  prefs: []
  type: TYPE_NORMAL
- en: Chunk merging can only happen when at least one of the chunks is empty, and
    only between adjacent chunks.
  prefs: []
  type: TYPE_NORMAL
- en: 'To find empty chunks, we need to connect to the database that we want to inspect
    (in our case, `mongo_books`) and use `runCommand` with `dataSize` set as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The `dataSize` phrase follows the `database_name.collection_name` pattern, whereas
    `keyPattern` is the shard key that we have defined for this collection.
  prefs: []
  type: TYPE_NORMAL
- en: The `min` and `max` values should be calculated by the chunks that we have in
    this collection. In our case, we have entered *chunkB's* details from the example
    earlier in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the bounds of our query (which, in our case, are the bounds of *chunkB*)
    return no documents, the result will resemble the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we know that *chunkB* has no data, we can merge it with another chunk
    (in our case, this could only be *chunkA*) like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'On a success, this will return MongoDB''s default `ok` status message, as shown
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We can then verify that we only have one chunk on *ShardA* by invoking `sh.status()` again.
  prefs: []
  type: TYPE_NORMAL
- en: Adding and removing shards
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Adding a new shard to our cluster is as easy as connecting to `mongos`, connecting
    to the `admin` database, and invoking `runCommand` with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This adds a new shard from the replica set named `mongo_books_replica_set` from
    the `rs01.packtdb.com` host running on port `27017`. We also define the `maxSize`
    of data for this shard as `18000` MB (or we can set it to `0` to give it no limit)
    and the name of the new shard as `packt_mongo_shard_UK`.
  prefs: []
  type: TYPE_NORMAL
- en: This operation will take quite some time to complete as chunks will have to
    be rebalanced and migrated to the new shard.
  prefs: []
  type: TYPE_NORMAL
- en: 'Removing a shard, on the other hand, requires more involvement since we have
    to make sure that we won''t lose any data on the way. We do this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to make sure that the balancer is enabled using `sh.getBalancerState()`.
    Then, after identifying the shard we want to remove using any one of the `sh.status()`,
    `db.printShardingStatus()`, or `listShards admin` commands, we connect to the
    `admin` database and invoke `removeShard` as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should contain the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, if we invoke the same command again, we get the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The remaining document in the result contains the number of `chunks` and `dbs`
    that are still being transferred. In our case, it's `2` and `3` respectively.
  prefs: []
  type: TYPE_NORMAL
- en: All the commands need to be executed in the `admin` database.
  prefs: []
  type: TYPE_NORMAL
- en: An extra complication in removing a shard can arise if the shard we want to
    remove serves as the primary shard for one or more of the databases that it contains.
    The primary shard is allocated by MongoDB when we initiate sharding, so when we
    remove the shard, we need to manually move these databases to a new shard.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will know whether we need to perform this operation by looking at the following
    section of the result from `removeShard()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We need to drop or `movePrimary` our `mongo_books` database. The way to do this
    is to first make sure that we are connected to the `admin` database.
  prefs: []
  type: TYPE_NORMAL
- en: We need to wait for all of the chunks to finish migrating before running this
    command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Make sure that the result contains the following before proceeding:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Only after we have made sure that the chunks to be moved are down to zero can
    we safely run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This command will invoke a blocking operation, and, when it returns, it should
    have the following result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Invoking the same `removeShard()` command after we are all done should return
    the following result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Once we get to `state` as `completed` and `ok` as `1`, it is safe to remove
    our `packt_mongo_shard_UK` shard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Removing a shard is naturally more complicated than adding one. We need to allow
    some time, hope for the best, and plan for the worst when performing potentially
    destructive operations on our live cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Sharding limitations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sharding comes with great flexibility. Unfortunately, there are a few limitations
    in the way that we perform some of the operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will highlight the most important ones in the following list:'
  prefs: []
  type: TYPE_NORMAL
- en: The `group()` database command does not work. The `group()` command should not
    be used anyway; use `aggregate()` and the aggregation framework instead, or `mapreduce()`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `db.eval()` command does not work and should be disabled in most cases for
    security reasons.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `$isolated` option for updates does not work. This is a functionality that
    is missing in sharded environments. The `$isolated` option for `update()` provides
    the guarantee that, if we update multiple documents at once, other readers and
    writers will not see some of the documents updated with the new value, and the
    others will still have the old value. The way this is implemented in unsharded
    environments is by holding a global write lock and/or serializing operations to
    a single thread to make sure that every request for the documents affected by
    `update()` will not be accessed by other threads/operations. This implementation
    means that it is not performant and does not support any concurrency, which makes
    it prohibitive to allow the `$isolated` operator in a sharded environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `$snapshot` operator for queries is not supported. The `$snapshot` operator in
    the `find()` cursor prevents documents from appearing more than once in the results,
    as a result of being moved to a different location on the disk after an update.
    The `$snapshot` operator is operationally expensive and often not a hard requirement.
    The way to substitute it is by using an index for our queries on a field whose
    keys will not change for the duration of the query.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The indexes cannot cover our queries if our queries do not contain the shard
    key. Results in sharded environments will come from the disk and not exclusively
    from the index. The only exception is if we query only on the built-in `_id` field
    and return only the `_id` field, in which case, MongoDB can still cover the query
    using built-in indexes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `update()` and `remove()` operations work differently. All `update()` and
    `remove()` operations in a sharded environment must include either the `_id` of
    the documents that are to be affected or the shard key; otherwise, the `mongos`
    router will have to do a full table scan across all collections, databases, and
    shards, which would be operationally very expensive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unique indexes across shards need to contain the shard key as a prefix of the
    index. In other words, to achieve uniqueness of documents across shards, we need
    to follow the data distribution that MongoDB follows for the shards.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The shard key has to be up to 512 bytes in size. The shard key index has to
    be in ascending order on the key field that gets sharded and optionally other
    fields as well, or a hashed index on it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The shard key value in a document is also immutable. If our shard key for our
    `User` collection is `email`, then we cannot update the `email` value for any
    user after we set it.
  prefs: []
  type: TYPE_NORMAL
- en: Querying sharded data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Querying our data using a MongoDB shard is different than a single-server deployment
    or a replica set. Instead of connecting to the single server or the primary of
    the replica set, we connect to the `mongos` router that decides which shard to
    ask for our data. In this section, we will explore how the query router operates
    and use Ruby to illustrate how similar to a replica set this is for the developer
    .
  prefs: []
  type: TYPE_NORMAL
- en: The query router
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The query router, also known as the `mongos` process, acts as the interface
    and entry point to our MongoDB cluster. Applications connect to it instead of
    connecting to the underlying shards and replica sets; `mongos` executes queries,
    gathers results and passes them to our application.
  prefs: []
  type: TYPE_NORMAL
- en: The `mongos` process doesn't hold any persistent state and is typically low
    on system resources.
  prefs: []
  type: TYPE_NORMAL
- en: The `mongos` process is typically hosted in the same instance as the application
    server.
  prefs: []
  type: TYPE_NORMAL
- en: It acts as a proxy for requests. When a query comes in, `mongos` will examine
    and decide which shards need to execute the query and establish a cursor in each
    one of them.
  prefs: []
  type: TYPE_NORMAL
- en: Find
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If our query includes the shard key or a prefix of the shard key, `mongos` will
    perform a targeted operation, only querying the shards that hold the keys that
    we are looking for.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, with a composite shard key of `{_id, email, address}` on our `User `collection,
    we can have a targeted operation with any of the following queries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: These queries consist of either a prefix (as is the case with the first two)
    or the complete shard key.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, a query on `{email, address}` or `{address}` will not be
    able to target the right shards, resulting in a broadcast operation. A broadcast
    operation is any operation that doesn't include the shard key or a prefix of the
    shard key, and they result in `mongos` querying every shard and gathering results
    from them. They are also known as **scatter-and-gather operations** or **fan out
    queries**.
  prefs: []
  type: TYPE_NORMAL
- en: This behavior is a direct result of the way indexes are organized, and is similar
    to the behavior that we identified in the chapter about indexing.
  prefs: []
  type: TYPE_NORMAL
- en: Sort/limit/skip
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If we want to sort our results, we have the following two options:'
  prefs: []
  type: TYPE_NORMAL
- en: If we are using the shard key in our sort criteria, then `mongos` can determine
    the order in which it has to query the shard or shards. This results in an efficient
    and, again, targeted operation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we are not using the shard key in our sort criteria, then, as is the case
    with a query without any sort criteria, it's going to be a fan out query. To sort
    the results when we are not using the shard key, the primary shard executes a
    distributed merge sort locally before passing on the sorted result set to `mongos`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A limit on the queries is enforced on each individual shard and then again at
    the `mongos` level, as there may be results from multiple shards. A `skip` operator,
    on the other hand, cannot be passed on to individual shards, and will be applied
    by `mongos` after retrieving all the results locally.
  prefs: []
  type: TYPE_NORMAL
- en: If we combine the `skip` and `limit` operators, `mongos` will optimize the query
    by passing both values to individual shards. This is particularly useful in cases
    such as pagination. If we query without `sort` and the results are coming from
    more than one shard, `mongos` will round robin across shards for the results.
  prefs: []
  type: TYPE_NORMAL
- en: Update/remove
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In document modifier operations, such as `update` and `remove`, we have a similar
    situation to the one we saw with `find`. If we have the shard key in the `find`
    section of the modifier, then `mongos` can direct the query to the relevant shard.
  prefs: []
  type: TYPE_NORMAL
- en: If we don't have the shard key in the `find` section, then it will again be
    a fanout operation.
  prefs: []
  type: TYPE_NORMAL
- en: The `UpdateOne`, `replaceOne`, and `removeOne` operations must have the shard
    key or the `_id` value.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table sums up the operations that we can use with sharding:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Type of operation** | **Query topology** |'
  prefs: []
  type: TYPE_TB
- en: '| Insert | Must have the shard key |'
  prefs: []
  type: TYPE_TB
- en: '| Update | Can have the shard key |'
  prefs: []
  type: TYPE_TB
- en: '| Query with shard key | Targeted operation |'
  prefs: []
  type: TYPE_TB
- en: '| Query without shard key | Scatter-and-gather operation/fan out query |'
  prefs: []
  type: TYPE_TB
- en: '| Indexed, sorted query with shard key | Targeted operation |'
  prefs: []
  type: TYPE_TB
- en: '| Indexed, sorted query without shard key | Distributed sort merge |'
  prefs: []
  type: TYPE_TB
- en: Querying using Ruby
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Connecting to a sharded cluster using Ruby is no different than connecting
    to a replica set. Using the official Ruby driver, we have to configure the `client`
    object to define the set of `mongos` servers, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The `mongo-ruby-driver` will then return a `client` object, which is no different
    than connecting to a replica set from the Mongo Ruby client. We can then use the
    `client` object as we did in previous chapters, with all the caveats around how
    sharding behaves differently than a standalone server or a replica set with regards
    to querying and performance.
  prefs: []
  type: TYPE_NORMAL
- en: Performance comparison with replica sets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Developers and architects are always looking out for ways to compare performance
    between replica sets and sharded configurations.
  prefs: []
  type: TYPE_NORMAL
- en: The way MongoDB implements sharding is based on top of replica sets. Every shard
    in production should be a replica set. The main difference in performance comes
    from fan out queries. When we are querying without the shard key, MongoDB's execution
    time is limited by the worst performing replica set. In addition, when using sorting
    without the shard key, the primary server has to implement the distributed merge
    sort on the entire dataset. This means that it has to collect all data from different
    shards, merge sort them, and pass them as sorted to `mongos`. In both cases, network
    latency and limitations in bandwidth can slow down operations, which they wouldn't
    do with a replica set.
  prefs: []
  type: TYPE_NORMAL
- en: On the flip side, by having three shards, we can distribute our working set
    requirements across different nodes, thereby serving results from RAM instead
    of reaching out to the underlying storage, HDD or SSD.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, writes can be sped up significantly since we are no longer
    bound by a single node's I/O capacity, and we can have writes in as many nodes
    as there are shards. Summing up, in most cases, and especially for the cases where
    we are using the shard key, both queries and modification operations will be significantly
    sped up by sharding.
  prefs: []
  type: TYPE_NORMAL
- en: The shard key is the single most important decision in sharding, and should
    reflect and apply to our most common application use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Sharding recovery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will explore different failure types and how we can recover
    in a sharded environment. Failure in a distributed system can take multiple forms
    and shapes. In this section we will cover all the possible cases, from the simplest
    case of a stateless component like `mongos` failing to an entire shard going down.
  prefs: []
  type: TYPE_NORMAL
- en: mongos
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `mongos` process is a relatively lightweight process that holds no state.
    In the case that the process fails, we can just restart it or spin up a new process
    in a different server. It's recommended that `mongos` processes are located in
    the same server as our application, and so it makes sense to connect from our
    application using the set of `mongos` servers that we have colocated in our application
    servers to ensure a high availability of `mongos` processes.
  prefs: []
  type: TYPE_NORMAL
- en: mongod
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A `mongod` process failing in a sharded environment is no different than it
    failing in a replica set. If it is a secondary, the primary and the other secondary
    (assuming three-node replica sets) will continue as usual.
  prefs: []
  type: TYPE_NORMAL
- en: If it is a `mongod` process acting as a primary, then an election round will
    start to elect a new primary in this shard (which is really a replica set).
  prefs: []
  type: TYPE_NORMAL
- en: In both cases, we should actively monitor and try to repair the node as soon
    as possible, as our availability can be impacted.
  prefs: []
  type: TYPE_NORMAL
- en: Config server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Starting from MongoDB 3.4, config servers are also configured as a replica set.
    A config server failing is no different than a regular `mongod` process failing.
    We should monitor, log, and repair the process.
  prefs: []
  type: TYPE_NORMAL
- en: A shard goes down
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Losing an entire shard is pretty rare, and in many cases can be attributed to
    network partitioning rather than failing processes. When a shard goes down, all
    operations that would go to this shard will fail. We can (and should) implement
    fault tolerance in our application level, allowing our application to resume for
    the operations that can be completed.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing a shard key that can easily map on our operational side can also help;
    for example, if our shard key is based on location, we may lose the EU shard,
    but will still be able to write and read data regarding US-based customers through
    our US shard.
  prefs: []
  type: TYPE_NORMAL
- en: The entire cluster goes down
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we lose the entire cluster, we can't do anything other than get it back up
    and running as soon as possible. It's important to have monitoring, and to put
    a proper process in place to understand what needs to be done, when, and by whom,
    should this ever happen.
  prefs: []
  type: TYPE_NORMAL
- en: Recovering when the entire cluster goes down essentially involves restoring
    from backups and setting up new shards, which is complicated and will take time.
    Dry testing this in a staging environment is also advisable, as  is investing
    in regular backups via MongoDB Ops Manager or any other backup solution.
  prefs: []
  type: TYPE_NORMAL
- en: A member of each shard's replica set could be in a different location for disaster-recovery
    purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following sources are recommended for you to study sharding in depth:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Scaling MongoDB* by Kristina Chodorow'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*MongoDB: The Definitive Guide* by Kristina Chodorow and Michael Dirolf'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.mongodb.com/manual/sharding/](https://docs.mongodb.com/manual/sharding/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.mongodb.com/blog/post/mongodb-16-released](https://www.mongodb.com/blog/post/mongodb-16-released)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/mongodb/mongo/blob/r3.4.2-rc0/src/mongo/s/commands/cluster_shard_collection_cmd.cpp#L469](https://github.com/mongodb/mongo/blob/r3.4.2-rc0/src/mongo/s/commands/cluster_shard_collection_cmd.cpp#L469)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.mongodb.com/blog/post/sharding-pitfalls-part-iii-chunk-balancing-and](https://www.mongodb.com/blog/post/sharding-pitfalls-part-iii-chunk-balancing-and)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://plusnconsulting.com/post/mongodb-sharding-and-chunks/](http://plusnconsulting.com/post/mongodb-sharding-and-chunks/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/mongodb/mongo/wiki/Sharding-Internals](https://github.com/mongodb/mongo/wiki/Sharding-Internals)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://learnmongodbthehardway.com/schema/sharding](http://learnmongodbthehardway.com/schema/sharding)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://learnmongodbthehardway.com/schema/sharding/](http://learnmongodbthehardway.com/schema/sharding/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.infoq.com/news/2010/08/MongoDB-1.6](https://www.infoq.com/news/2010/08/MongoDB-1.6)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://www.pc-freak.net/images/horizontal-vs-vertical-scaling-vertical-and-horizontal-scaling-explained-diagram.png](http://www.pc-freak.net/images/horizontal-vs-vertical-scaling-vertical-and-horizontal-scaling-explained-diagram.png)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored sharding, one of the most interesting features
    of MongoDB. We started with an architectural overview of sharding and moved on
    to how we can design a shard and choose the right shard key.
  prefs: []
  type: TYPE_NORMAL
- en: We learned about monitoring, administration, and the limitations that come with
    sharding. We also learned about `mongos`, the MongoDB sharding router that directs
    our queries to the correct shard. Finally, we discussed recovery from common failure
    types in a MongoDB sharded environment.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter on fault tolerance and high availability will offer some useful
    tips and tricks that have not been covered in the 11 other chapters.
  prefs: []
  type: TYPE_NORMAL
