- en: Sharding
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sharding is the ability to horizontally scale out our database by partitioning
    our datasets across different servers—the shards. It has been a feature of MongoDB
    since version 1.6 was released in August, 2010\. Foursquare and Bitly are two
    of MongoDB's most famous early customers, and have used the sharding feature from
    its inception all the way to its general release.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: How to design a sharding cluster and how to make the single most important decision
    concerning its use—choosing the shard key
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different sharding techniques and how to monitor and administrate sharded clusters
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `mongos` router and how it is used to route our queries across different
    shards
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How we can recover from errors in our shard
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why do we use sharding?
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In database systems and computing systems in general, we have two ways to improve
    performance. The first one is to simply replace our servers with more powerful
    ones, keeping the same network topology and systems architecture. This is called
    **vertical scaling**.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: An advantage of vertical scaling is that it is simple, from an operational standpoint,
    especially with cloud providers such as Amazon making it a matter of a few clicks
    to replace an **m2.medium** with an **m2.extralarge** server instance. Another
    advantage is that we don't need to make any code changes, and so there is little
    to no risk of something going catastrophically wrong.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: The main disadvantage of vertical scaling is that there is a limit to it; we
    can only get servers that are as powerful as those that our cloud provider can
    give to us.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: A related disadvantage is that getting more powerful servers generally comes
    with an increase in cost that is not linear but exponential. So, even if our cloud
    provider offers more powerful instances, we will hit the cost effectiveness barrier
    before we hit the limit of our department's credit card.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: The second way to improve performance is by using the same servers with the
    same capacity and increase their number. This is called **horizontal scaling**.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'Horizontal scaling offers the advantage of theoretically being able to scale
    exponentially while remaining practical enough for real-world applications. The
    main disadvantage is that it can be operationally more complex and requires code
    changes and the careful design of the system upfront. Horizontal scaling is also
    more complex when it comes to the system because it requires communication between
    the different servers over network links that are not as reliable as inter-process
    communication on a single server. The following diagram shows the difference between
    horizontal and vertical scaling:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/18bb77a8-2036-44b5-b85b-16f97f38f403.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
- en: 'To understand scaling, it''s important to understand the limitations of single-server
    systems. A server is typically bound by one or more of the following characteristics:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '**CPU**: A CPU-bound system is one that is limited by our CPU''s speed. A task
    such as the multiplication of matrices that can fit in RAM will be CPU bound because
    there is a specific number of steps that have to be performed in the CPU without
    any disk or memory access needed for the task to complete. In this case, CPU usage
    is the metric that we need to keep track of.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**I/O**: Input-output bound systems are similarly limited by the speed of our
    storage system (HDD or SSD). A task such as reading large files from a disk to
    load into memory will be I/O bound as there is little to do in terms of CPU processing;
    the great majority of the time is spent reading the files from the disk. The important
    metrics to keep track of are all the metrics related to disk access, the reads
    per second, and the writes per second, compared to the practical limit of our
    storage system.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory and cache**: Memory-bound and cache-bound systems are restricted by
    the amount of available RAM memory and/or the cache size that we have assigned
    to them. A task that multiplies matrices larger than our RAM size will be memory
    bound, as it will need to page in/out data from the disk to perform the multiplication.
    The important metric to keep track of is the memory used. This may be misleading
    in MongoDB MMAPv1, as the storage engine will allocate as much memory as possible
    through the filesystem cache.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内存和缓存**：受内存限制和缓存限制的系统受到可用RAM内存和/或我们分配给它们的缓存大小的限制。一个将矩阵乘以大于我们RAM大小的任务将受到内存限制，因为它将需要从磁盘中分页数据来执行乘法。要跟踪的重要指标是已使用的内存。在MongoDB
    MMAPv1中，这可能会产生误导，因为存储引擎将通过文件系统缓存分配尽可能多的内存。'
- en: In the WiredTiger storage engine, on the other hand, if we don't allocate enough
    memory for the core MongoDB process, out-of-memory errors may kill it, and this
    is something that we want to avoid at all costs.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，在WiredTiger存储引擎中，如果我们没有为核心MongoDB进程分配足够的内存，内存不足的错误可能会导致其崩溃，这是我们要尽一切努力避免的。
- en: Monitoring memory usage has to be done both directly through the operating system
    and indirectly by keeping a track of page in/out data. An increasing memory paging
    number is often an indication that we are running short of memory and the operating
    system is using virtual address space to keep up.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 监控内存使用量必须通过操作系统直接进行，并间接地通过跟踪分页数据来进行。增加的内存分页数通常表明我们的内存不足，操作系统正在使用虚拟地址空间来跟上。
- en: MongoDB, being a database system, is generally memory and I/O bound. Investing
    in SSD and more memory for our nodes is almost always a good investment. Most
    systems are a combination of one or more of the preceding limitations. Once we
    add more memory, our system may become CPU bound, as complex operations are almost
    always a combination of CPU, I/O, and memory usage.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据库系统的MongoDB通常受到内存和I/O的限制。为我们的节点投资SSD和更多内存几乎总是一个不错的投资。大多数系统是前述限制的一个或多个组合。一旦我们增加了更多内存，我们的系统可能会变得CPU受限，因为复杂的操作几乎总是CPU、I/O和内存使用的组合。
- en: MongoDB's sharding is simple enough to set up and operate, and this has contributed
    to its huge success over the years as it provides the advantages of horizontal
    scaling without requiring a large commitment of engineering and operations resources.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB的分片设置和操作非常简单，这也是它多年来取得巨大成功的原因，因为它提供了横向扩展的优势，而不需要大量的工程和运营资源。
- en: That being said, it's really important to get sharding right from the beginning,
    as it is extremely difficult from an operational standpoint to change the configuration
    once it has been set up. Sharding should not be an afterthought, but rather a
    key architectural design decision from an early point in the design process.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，从一开始就正确地进行分片非常重要，因为一旦设置好了，从操作的角度来看，要更改配置是非常困难的。分片不应该是一个事后想法，而应该是设计过程早期的一个关键架构设计决策。
- en: Architectural overview
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 架构概述
- en: 'A sharded cluster is comprised of the following elements:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一个分片集群由以下元素组成：
- en: Two or more shards. Each shard must be a replica set.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个或更多分片。每个分片必须是一个副本集。
- en: One or more query routers (`mongos`). A `mongos` provides an interface between
    our application and the database.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个或多个查询路由器（`mongos`）。`mongos`提供了我们的应用程序和数据库之间的接口。
- en: A replica set of config servers. Config servers store metadata and configuration
    settings for the entire cluster.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个副本集的配置服务器。配置服务器存储整个集群的元数据和配置设置。
- en: 'The relationships between these elements is shown in the following diagram:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这些元素之间的关系如下图所示：
- en: '![](img/0b87bede-ff79-4863-a498-c5c3cf5b86b1.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0b87bede-ff79-4863-a498-c5c3cf5b86b1.png)'
- en: As of MongoDB 3.6, shards must be implemented as replica sets.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 从MongoDB 3.6开始，分片必须实现为副本集。
- en: Development, continuous deployment, and staging environments
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发、持续部署和暂存环境
- en: In preproduction environments, it may be overkill to use the full set of servers.
    For efficiency reasons, we may opt to use a more simplified architecture.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在预生产环境中，使用完整服务器集可能是过度的。出于效率原因，我们可能选择使用更简化的架构。
- en: 'The simplest possible configuration that we can deploy for sharding is the
    following:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以为分片部署的最简单配置如下：
- en: One `mongos` router
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个`mongos`路由器
- en: One sharded replica set with one MongoDB server and two arbiters
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个分片副本集，有一个MongoDB服务器和两个仲裁者
- en: One replica set of config servers with one MongoDB server and two arbiters
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个配置服务器的副本集，有一个MongoDB服务器和两个仲裁者
- en: This should be strictly used for development and testing as this architecture
    defies most of the advantages that a replica set provides, such as high availability,
    scalability, and replication of data.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这应严格用于开发和测试，因为这种架构违背了副本集提供的大多数优势，如高可用性、可扩展性和数据复制。
- en: Staging is strongly recommended to mirror our production environment in terms
    of its servers, configuration, and (if possible) dataset requirements too, in
    order to avoid surprises at deployment time.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 强烈建议在暂存环境中镜像我们的生产环境，包括服务器、配置和（如果可能）数据集要求，以避免在部署时出现意外。
- en: Planning ahead with sharding
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提前计划分片
- en: As we will see in the next sections, sharding is complicated and expensive,
    operation wise. It is important to plan ahead and make sure that we start the
    sharding process long before we hit our system's limits.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将在接下来的部分中看到的，分片在操作上是复杂且昂贵的。重要的是要提前计划，并确保我们在达到系统极限之前很久就开始分片过程。
- en: 'Some rough guidelines on when you need to start sharding are as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一些关于何时需要开始分片的大致指导原则如下：
- en: When you have a CPU utilization of less than 70% on average
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当平均CPU利用率低于70%时
- en: When I/O (and especially write) capacity is less than 80%
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当I/O（尤其是写入）容量低于80%时
- en: When memory utilization is less than 70% on average
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当平均内存利用率低于70%时
- en: As sharding helps with write performance, it's important to keep an eye on our
    I/O write capacity and the requirements of our application.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 由于分片有助于写入性能，重要的是要关注我们的I/O写入容量和应用程序的要求。
- en: Don't wait until the last minute to start sharding in an already busy-up-to-the-neck
    MongoDB system, as it can have unintended consequences.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 不要等到最后一刻才开始在已经忙碌到极致的MongoDB系统中进行分片，因为这可能会产生意想不到的后果。
- en: Sharding setup
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分片设置
- en: Sharding is performed at the collection level. We can have collections that
    we don't want or need to shard for several reasons. We can leave these collections
    unsharded.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 分片是在集合级别执行的。我们可以有一些我们不想或不需要分片的集合，有几个原因。我们可以将这些集合保持为未分片状态。
- en: These collections will be stored in the primary shard. The primary shard is
    different for each database in MongoDB. The primary shard is automatically selected
    by MongoDB when we create a new database in a sharded environment. MongoDB will
    pick the shard that has the least data stored at the moment of creation.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这些集合将存储在主分片中。在MongoDB中，每个数据库的主分片都不同。在分片环境中创建新数据库时，MongoDB会自动选择主分片。MongoDB将选择在创建时存储数据最少的分片。
- en: 'If we want to change the primary shard at any other point, we can issue the
    following command:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想在任何其他时间更改主分片，我们可以发出以下命令：
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: With this, we move the database named `mongo_books` to the shard named `UK_based`.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个，我们将名为`mongo_books`的数据库移动到名为`UK_based`的分片中。
- en: Choosing the shard key
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择分片键
- en: 'Choosing our shard key is the most important decision we need to make: once
    we shard our data and deploy our cluster, it becomes very difficult to change
    the shard key. First, we will go through the process of changing the shard key.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 选择我们的分片键是我们需要做出的最重要的决定：一旦我们分片我们的数据并部署我们的集群，更改分片键就变得非常困难。首先，我们将经历更改分片键的过程。
- en: Changing the shard key
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更改分片键
- en: There is no command or simple procedure to change the shard key in MongoDB.
    The only way to change the shard key involves backing up and restoring all of
    our data, something that may range from being extremely difficult to impossible
    in high-load production environments.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在MongoDB中，没有命令或简单的程序可以更改分片键。更改分片键的唯一方法涉及备份和恢复所有数据，这在高负载生产环境中可能从极其困难到不可能。
- en: 'The following are the steps that we need to go through in order to change the
    shard key:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们需要经历的步骤，以更改分片键：
- en: Export all data from MongoDB
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从MongoDB导出所有数据
- en: Drop the original sharded collection
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除原始的分片集合
- en: Configure sharding with the new key
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用新键配置分片
- en: Presplit the new shard key range
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预先拆分新的分片键范围
- en: Restore our data back into MongoDB
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将我们的数据恢复到MongoDB中
- en: Of these steps, step 4 is the one that needs further explanation.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些步骤中，步骤4是需要进一步解释的步骤。
- en: MongoDB uses chunks to split data in a sharded collection. If we bootstrap a
    MongoDB sharded cluster from scratch, chunks will be calculated automatically
    by MongoDB. MongoDB will then distribute the chunks across different shards to
    ensure that there are an equal number of chunks in each shard.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB使用块来分割分片集合中的数据。如果我们从头开始引导MongoDB分片集群，MongoDB将自动计算块。然后，MongoDB将这些块分布到不同的分片上，以确保每个分片中有相等数量的块。
- en: The only time when we cannot really do this is when we want to load data into
    a newly sharded collection.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一不能真正做到这一点的时候是当我们想要将数据加载到新的分片集合中。
- en: 'The reasons for this are threefold:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的原因有三个：
- en: MongoDB creates splits only after an `insert` operation.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MongoDB仅在`insert`操作后创建拆分。
- en: Chunk migration will copy all of the data in that chunk from one shard to another.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 块迁移将从一个分片复制该块中的所有数据到另一个分片。
- en: The `floor(n/2)` chunk migrations can happen at any given time, where `n` is
    the number of shards we have. Even with three shards, this is only a `floor(1.5)=1`
    chunk migration at a time.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`floor(n/2)`块迁移可以在任何时间发生，其中`n`是我们拥有的分片数量。即使有三个分片，这也只是一次`floor(1.5)=1`块迁移。'
- en: These three limitations mean that letting MongoDB figure it out on its own will
    definitely take much longer, and may result in an eventual failure. This is why
    we want to presplit our data and give MongoDB some guidance on where our chunks should go.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个限制意味着让MongoDB自行解决这个问题肯定会花费更长时间，并且可能最终导致失败。这就是为什么我们希望预先拆分我们的数据，并为MongoDB提供一些关于我们的块应该放在哪里的指导。
- en: 'In our example of the `mongo_books` database and the `books` collection, this
    would be as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，`mongo_books`数据库和`books`集合如下：
- en: '[PRE1]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `middle` command parameter will split our key space in documents that have `id` less
    than or equal to `50` and documents that have `id` greater than `50`. There is
    no need for a document to exist in our collection with `id` that is equal to `50` as
    this will only serve as the guidance value for our partitions.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`middle`命令参数将在我们的键空间中拆分文档，这些文档的`id`小于或等于`50`，以及`id`大于`50`的文档。我们的集合中没有必要存在`id`等于`50`的文档，因为这只会作为我们分区的指导值。'
- en: In this example, we chose `50`, as we assume that our keys follow a uniform
    distribution (that is, there is the same count of keys for each value) in the
    range of values from `0` to `100`.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们选择了`50`，因为我们假设我们的键在值范围从`0`到`100`中遵循均匀分布（即，每个值的键数量相同）。
- en: We should aim to create at least 20-30 chunks to grant MongoDB flexibility in
    potential migrations. We can also use `bounds` and `find` instead of `middle`
    if we want to manually define the partition key, but both parameters need data
    to exist in our collection before applying them.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该努力创建至少20-30个块，以赋予MongoDB在潜在迁移中的灵活性。如果我们想手动定义分区键，我们也可以使用`bounds`和`find`而不是`middle`，但是这两个参数在应用它们之前需要数据存在于我们的集合中。
- en: Choosing the correct shard key
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择正确的分片键
- en: After the previous section, it's now self-evident that we need to take the choice
    of our shard key into consideration as it is a decision that we have to stick
    with.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分之后，现在很明显我们需要考虑我们的分片键的选择，因为这是一个我们必须坚持的决定。
- en: 'A great shard key has three characteristics:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一个很好的分片键具有三个特点：
- en: High cardinality
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高基数
- en: Low frequency
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低频率
- en: Nonmonotonic changes in value
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 值的非单调变化
- en: 'We will go over the definitions of these three properties first to understand
    what they mean:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先介绍这三个属性的定义，以了解它们的含义：
- en: '**High cardinality**: It means that the shard key must have as many distinct
    values as possible. A Boolean can take only the values of `true`/`false`, and
    so it is a bad shard key choice. A 64-bit long value field that can take any value
    from *−(2^63)* to *2^63−1* is a good shard key choice, in terms of cardinality.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高基数**：这意味着分片键必须具有尽可能多的不同值。布尔值只能取`true`/`false`，因此不是一个好的分片键选择。一个可以取从*−(2^63)*到*2^63−1*的任何值的64位长值字段在基数方面是一个好的选择。'
- en: '**Low frequency**: It directly relates to the argument about high cardinality.
    A low-frequency shard key will have a distribution of values as close to a perfectly
    random/uniform distribution. Using the example of our 64-bit long value, it is
    of little use to us if we have a field that can take values ranging from *−(2^63)*
    to *2^63−1* if we end up observing the values of zero and one all the time. In
    fact, it is as bad as using a Boolean field, which can also take only two values. If
    we have a shard key with high-frequency values, we will end up with chunks that
    are indivisible. These chunks cannot be further divided and will grow in size, negatively affecting
    the performance of the shard that contains them.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**低频率**：它直接关系到高基数的论点。低频率的分片键将具有接近完全随机/均匀分布的值分布。以我们64位长值的例子，如果我们一直观察到零和一这样的值，那么它对我们几乎没有用处。事实上，这和使用布尔字段一样糟糕，因为布尔字段也只能取两个值。如果我们有一个高频率值的分片键，我们最终会得到不可分割的块。这些块无法进一步分割，并且会增长，负面影响包含它们的分片的性能。'
- en: '**Nonmonotonically changing values**:It mean that our shard key should not
    be, for example, an integer that always increases with every new insert. If we
    choose a monotonically increasing value as our shard key, this will result in
    all writes ending up in the last of all of our shards, limiting our write performance.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非单调变化的值**：这意味着我们的分片键不应该是一个每次新插入都增加的整数，例如。如果我们选择一个单调递增的值作为我们的分片键，这将导致所有写入最终都进入我们所有分片中的最后一个，从而限制我们的写入性能。'
- en: If we want to use a monotonically changing value as the shard key, we should
    consider using hash-based sharding.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要使用单调变化的值作为分片键，我们应该考虑使用基于哈希的分片。
- en: In the next section, we will describe different sharding strategies, including
    their advantages and disadvantages.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将描述不同的分片策略，包括它们的优点和缺点。
- en: Range-based sharding
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于范围的分片
- en: The default and most widely used sharding strategy is range-based sharding.
    This strategy will split our collection's data into chunks, grouping documents
    with nearby values in the same shard.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 默认和最广泛使用的分片策略是基于范围的分片。这种策略将把我们集合的数据分成块，将具有相邻值的文档分组到同一个分片中。
- en: 'For our example database and collection, `mongo_books` and `books` respectively,
    we have the following:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的示例数据库和集合，分别是`mongo_books`和`books`，我们有以下内容：
- en: '[PRE2]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This creates a range-based shard key on `id` with an ascending direction. The
    direction of our shard key will determine which documents will end up in the first
    shard and which ones in the subsequent ones.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在`id`上创建一个基于范围的分片键，并且是升序的。我们的分片键的方向将决定哪些文档将最终出现在第一个分片中，哪些文档出现在随后的分片中。
- en: This is a good strategy if we plan to have range-based queries, as these will
    be directed to the shard that holds the result set instead of having to query
    all shards.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们计划进行基于范围的查询，这是一个很好的策略，因为这些查询将被定向到保存结果集的分片，而不必查询所有分片。
- en: Hash-based sharding
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于哈希的分片
- en: If we don't have a shard key (or can't create one) that achieves the three goals
    mentioned previously, we can use the alternative strategy of using hash-based
    sharding. In this case, we are trading data distribution with query isolation.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们没有一个达到前面提到的三个目标的分片键（或者无法创建一个），我们可以使用替代策略，即使用基于哈希的分片。在这种情况下，我们正在用数据分布来交换查询隔离。
- en: Hash-based sharding will take the values of our shard key and hash them in a
    way that guarantees close to uniform distribution. This way, we can be sure that
    our data will be evenly distributed across the shards. The downside is that only
    exact match queries will get routed to the exact shard that holds the value. Any
    range query will have to go out and fetch data from all the shards.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 基于哈希的分片将获取我们的分片键的值，并以一种接近均匀分布的方式进行哈希。这样，我们可以确保我们的数据将均匀分布在分片中。缺点是只有精确匹配查询将被路由到持有该值的确切分片。任何范围查询都必须从所有分片中获取数据。
- en: 'For our example database and collection (`mongo_books` and `books` respectively),
    we have the following:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的示例数据库和集合（分别是`mongo_books`和`books`），我们有以下内容：
- en: '[PRE3]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Similar to the preceding example, we are now using the `id` field as our hashed
    shard key.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 与前面的示例类似，我们现在将`id`字段作为我们的哈希分片键。
- en: Suppose we use fields with float values for hash-based sharding. Then we will
    end up with collisions if the precision of our floats is more than *2^53*. These
    fields should be avoided where possible.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们使用浮点值字段进行基于哈希的分片。如果我们的浮点数的精度超过*2^53*，那么我们将会遇到碰撞。在可能的情况下，应该避免使用这些字段。
- en: Coming up with our own key
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提出我们自己的键
- en: Range-based sharding does not need to be confined to a single key. In fact,
    in most cases, we would like to combine multiple keys to achieve high cardinality
    and low frequency.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 基于范围的分片不需要局限于单个键。事实上，在大多数情况下，我们希望结合多个键来实现高基数和低频率。
- en: A common pattern is to combine a low-cardinality first part (but still with
    a number of distinct values more than two times the number of shards that we have)
    with a high-cardinality key as its second field. This achieves both read and write
    distribution from the first part of the sharding key and then cardinality and
    read-locality from the second part.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if we don't have range queries, then we can get away with
    using hash-based sharding on a primary key, as this will exactly target the shard
    and document that we are going after.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: To make things more complicated, these considerations may change depending on
    our workload. A workload that consists almost exclusively (say 99.5%) of reads
    won't care about write distribution. We can use the built-in `_id` field as our
    shard key, and this will only add 0.5% load to the last shard. Our reads will
    still be distributed across shards. Unfortunately, in most cases, this is not
    simple.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Location-based data
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Because of government regulations and the desire to have our data as close to
    our users as possible, there is often a constraint and need to limit data in a
    specific data center. By placing different shards at different data centers, we
    can satisfy this requirement.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Every shard is essentially a replica set. We can connect to it as we would connect
    to a replica set for administrative and maintenance operations. We can query one
    shard's data directly, but the results will only be a subset of the full sharded
    result set.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Sharding administration and monitoring
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sharded MongoDB environments have some unique challenges and limitations compared
    to single-server or replica set deployments. In this section, we will explore
    how MongoDB balances our data across shards using chunks and how we can tweak
    them if we need to. Together, we will explore some of sharding's design limitations.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Balancing data – how to track and keep our data balanced
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the advantages of sharding in MongoDB is that it is mostly transparent
    to the application and requires minimal administration and operational effort.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: One of the core tasks that MongoDB needs to perform continuously is balancing
    data between shards. No matter whether we implement range-based or hash-based
    sharding, MongoDB will need to calculate bounds for the hashed field to be able
    to figure out which shard to direct every new document insert or update toward.
    As our data grows, these bounds may need to get readjusted to avoid having a hot
    shard that ends up with the majority of our data.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 'For the sake of this example, let''s assume that there is a data type named
    `extra_tiny_int` with integer values from [`-12`, `12`). If we enable sharding
    on this `extra_tiny_int` field, then the initial bounds of our data will be the
    whole range of values denoted by `$minKey: -12` and `$maxKey: 11`.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: After we insert some initial data, MongoDB will generate chunks and recalculate
    the bounds of each chunk to try and balance our data.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: By default, the initial number of chunks created by MongoDB is *2 × number of
    shards*.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case of two shards and four initial chunks, the initial bounds will
    be calculated as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '*Chunk1: [-12..-6)*'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '*Chunk2:  [-6..0)*'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '*Chunk3:  [0..6)*'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '*Chunk4:  [6,12)* where *''[''* is inclusive and *'')''* is not inclusive'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the preceding explanation:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/75712448-e2de-4265-a8ce-458548db4503.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
- en: 'After we insert some data, our chunks will look as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '*ShardA*:'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Chunk1*:* -12,-8,-7*'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Chunk2*:*  -6*'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*ShardB*:'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Chunk3*:* 0, 2      *'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Chunk4*: *7,8,9,10,11,11,11,11*'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the preceding explanation:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7b631e16-8db1-4e9d-b9d3-56585f600f75.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
- en: In this case, we observe that c*hunk4* has more items than any other chunk.
    MongoDB will first split *chunk4* into two new chunks, attempting to keep the
    size of each chunk under a certain threshold (64 MB, by default).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Now, instead of c*hunk4*, we have *chunk4A*:*7,8,9,10* and *chunk4B*: *11,11,11,11*.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the preceding explanation:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表说明了先前的解释：
- en: '![](img/88357d07-47cd-4fc3-b6b3-a01a761d08f2.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/88357d07-47cd-4fc3-b6b3-a01a761d08f2.png)'
- en: 'The new bounds of this are as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 其新边界如下：
- en: '*chunk4A*: *[6,11)*'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*chunk4A*: *[6,11)*'
- en: '*chunk4B*: *[11,12)*'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*chunk4B*: *[11,12)*'
- en: Note that *chunk4B* can only hold one value. This is now an indivisible chunk—a
    chunk that cannot be broken down into smaller ones anymore—and will grow in size
    unbounded, causing potential performance issues down the line.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，*chunk4B*只能容纳一个值。这现在是一个不可分割的分片，无法再分割成更小的分片，并且将无限增长，可能会导致性能问题。
- en: This clarifies why we need to use a high-cardinality field as our shard key
    and why something like a Boolean, which only has `true`/`false` values, is a bad
    choice for a shard key.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这解释了为什么我们需要使用高基数字段作为我们的分片键，以及为什么像布尔值这样只有`true`/`false`值的字段是分片键的不良选择。
- en: 'In our case, we now have two chunks in *ShardA* and three chunks in *ShardB*.
    Let''s look at the following table:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，*ShardA*现在有两个分片，*ShardB*有三个分片。让我们看看下表：
- en: '| **Number of chunks** | **Migration threshold** |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| **分片数量** | **迁移阈值** |'
- en: '| *≤19* | *2* |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| *≤19* | *2* |'
- en: '| *20-79* | *4* |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| *20-79* | *4* |'
- en: '| *≥80* | *8* |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| *≥80* | *8* |'
- en: We have not reached our migration threshold yet, since *3-2 = 1*.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有达到迁移阈值，因为*3-2 = 1*。
- en: 'The migration threshold is calculated as the number of chunks in the shard
    with the highest count of chunks and the number of chunks in the shard with the
    lowest count of chunks, as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移阈值是根据拥有最多分片的分片和拥有最少分片的分片数量计算得出的，如下所示：
- en: '*Shard1 -> 85 chunks*'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Shard1 -> 85 chunks*'
- en: '*Shard2 -> 86 chunks*'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Shard2 -> 86 chunks*'
- en: '*Shard3 -> 92 chunks*'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Shard3 -> 92 chunks*'
- en: In the preceding example, balancing will not occur until *Shard3* (or *Shard2*)
    reaches *93* chunks because the migration threshold is *8* for *≥80* chunks and
    the difference between *Shard1* and *Shard3* is still *7* chunks (*92-85*).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的例子中，直到*Shard3*（或*Shard2*）达到*93*个分片之前，平衡都不会发生，因为迁移阈值对于*≥80*个分片是*8*，而*Shard1*和*Shard3*之间的差距仍然是*7*个分片（*92-85*）。
- en: If we continue adding data in *chunk4A*, it will eventually be split into *chunk4A1*
    and *chunk4A2*.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们继续在*chunk4A*中添加数据，它最终将被分割成*chunk4A1*和*chunk4A2*。
- en: Now we have four chunks in *ShardB* (*chunk3*, *chunk4A1*, *chunk4A2*, and *chunk4B*)
    and two chunks in *ShardA* (*chunk1* and *chunk2*).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在*ShardB*有四个分片（*chunk3*，*chunk4A1*，*chunk4A2*和*chunk4B*），*ShardA*有两个分片（*chunk1*和*chunk2*）。
- en: 'The following diagram illustrates the relationships of the chunks to the shards:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表说明了分片与分片之间的关系：
- en: '![](img/c609adaf-fda7-40e0-b569-ccb75e021fcc.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c609adaf-fda7-40e0-b569-ccb75e021fcc.png)'
- en: 'The MongoDB balancer will now migrate one chunk from *ShardB* to *ShardA* as
    *4-2 = 2*, reaching the migration threshold for fewer than *20* chunks. The balancer
    will adjust the boundaries between the two shards in order to be able to query
    more effectively (targeted queries). The following diagram illustrates the preceding
    explanation:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB平衡器现在将从*ShardB*迁移一个分片到*ShardA*，因为*4-2 = 2*，达到了少于*20*个分片的迁移阈值。平衡器将调整两个分片之间的边界，以便能够更有效地查询（有针对性的查询）。以下图表说明了先前的解释：
- en: '![](img/43018e6d-96c8-4920-a51f-566b0211645a.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/43018e6d-96c8-4920-a51f-566b0211645a.png)'
- en: As you can see from the preceding diagram, MongoDB will try to split *>64* MB
    chunks in half in terms of size. The bounds between the two resulting chunks may
    be completely uneven if our data distribution is uneven to begin with. MongoDB
    can split chunks into smaller ones, but cannot merge them automatically. We need
    to manually merge chunks, a delicate and operationally expensive procedure.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 从上图表中可以看出，MongoDB将尝试将*>64* MB的分片一分为二。如果我们的数据分布不均匀，那么两个结果分片之间的边界可能会完全不均匀。MongoDB可以将分片分割成更小的分片，但不能自动合并它们。我们需要手动合并分片，这是一个复杂且操作成本高昂的过程。
- en: Chunk administration
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分片管理
- en: Most of the time, we should leave chunk administration to MongoDB. We should
    manually manage chunks at the start, upon receiving the initial load of data,
    when we change our configuration from a replica set to sharding.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数情况下，我们应该让MongoDB来管理分片。我们应该在开始时手动管理分片，在接收到初始数据负载时，当我们将配置从副本集更改为分片时。
- en: Moving chunks
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 移动分片
- en: 'To move a chunk manually, we need to issue the following command after connecting
    to `mongos` and the `admin` database:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 要手动移动一个分片，我们需要在连接到`mongos`和`admin`数据库后发出以下命令：
- en: '[PRE4]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Using the preceding command, we move the chunk containing the document with
    `id: 50` (this has to be the shard key) from the `books` collection of the `mongo_books` database to
    the new shard named `shard1.packtdb.com`.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '使用上述命令，我们将包含`id: 50`的文档（这必须是分片键）从`mongo_books`数据库的`books`集合移动到名为`shard1.packtdb.com`的新分片。'
- en: 'We can also more explicitly define the bounds of the chunk that we want to
    move. Now the syntax is as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以更明确地定义我们要移动的分片的边界。现在的语法如下：
- en: '[PRE5]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here, `minValue` and `maxValue` are the values that we get from `db.printShardingStatus()`.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`minValue`和`maxValue`是我们从`db.printShardingStatus()`中获取的值。
- en: In the example used previously, for *chunk2*, `minValue` would be `-6` and `maxValue`
    would be `0`.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在先前的示例中，对于*chunk2*，`minValue`将是`-6`，`maxValue`将是`0`。
- en: Do not use `find` in hash-based sharding. Use `bounds` instead.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于哈希的分片中不要使用`find`。使用`bounds`代替。
- en: Changing the default chunk size
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更改默认的分片大小
- en: To change the default chunk size, we need to connect to a `mongos` router and,
    consequently, to the `config` database.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 要更改默认的分片大小，我们需要连接到`mongos`路由器，因此连接到`config`数据库。
- en: 'Then we issue the following command to change our global `chunksize` to `16`
    MB:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们发出以下命令将我们的全局`chunksize`更改为`16` MB：
- en: '[PRE6]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The main reasoning behind changing `chunksize` comes from cases where the default
    `chunksize` of 64 MB can cause more I/O than our hardware can handle. In this
    case, defining a smaller `chunksize` will result in more frequent but less data-intensive
    migrations.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 更改`chunksize`的主要原因来自于默认的64 MB `chunksize`可能会导致比我们的硬件处理能力更多的I/O。在这种情况下，定义较小的`chunksize`将导致更频繁但数据密度较小的迁移。
- en: 'Changing the default chunk size has the following drawbacks:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 更改默认块大小有以下缺点：
- en: Creating more splits by defining a smaller chunk size cannot be undone automatically.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过定义较小的块大小创建更多的拆分无法自动撤消。
- en: Increasing the chunk size will not force any chunk migration; instead, chunks
    will grow through inserts and updates until they reach the new size.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加块大小不会强制进行任何块迁移；相反，块将通过插入和更新而增长，直到达到新的大小。
- en: Lowering the chunk size may take quite some time to complete.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降低块大小可能需要相当长的时间才能完成。
- en: Automatic splitting to comply with the new chunk size if it is lower will only
    happen upon an insert or update. We may have chunks that don't get any write operations,
    and thus will not be changed in size.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果较低的块大小需要遵守新的块大小，那么只有在插入或更新时才会自动拆分。我们可能有一些块不会进行任何写操作，因此大小不会改变。
- en: The chunk size can be 1 to 1024 MB.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 块大小可以为1到1024 MB。
- en: Jumbo chunks
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 巨型块
- en: In rare cases, we may end up with jumbo chunks, chunks that are larger than
    the chunk size and cannot be split by MongoDB. We may also run into the same situation
    if the number of documents in our chunk exceeds the maximum document limit.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在罕见情况下，我们可能会遇到巨型块，即大于块大小且无法由MongoDB拆分的块。如果我们的块中的文档数量超过最大文档限制，也可能遇到相同的情况。
- en: These chunks will have the `jumbo` flag enabled. Ideally, MongoDB will keep
    track of whether it can split the chunk, and, as soon as it can, it will get split;
    however, we may decide that we want to manually trigger the split before MongoDB
    does.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这些块将启用`jumbo`标志。理想情况下，MongoDB将跟踪它是否可以拆分块，并且一旦可以，它将被拆分；但是，我们可能决定在MongoDB之前手动触发拆分。
- en: 'The way to do this is as follows:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的方法如下：
- en: 'Connect via shell to your `mongos` router and run the following:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过shell连接到您的`mongos`路由器并运行以下命令：
- en: '[PRE7]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Identify the chunk that has `jumbo` in its description using the following
    code:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码标识具有`jumbo`的块：
- en: '[PRE8]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Invoke `splitAt()` or `splitFind()` manually to split the chunk on the `books` collection of
    the `mongo_books` database at the `id` that is equal to `8` using the following
    code:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用`splitAt()`或`splitFind()`手动在`mongo_books`数据库的`books`集合上拆分`id`等于`8`的块，使用以下代码：
- en: '[PRE9]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The `splitAt()` function will split based on the split point we define. The
    two new splits may or may not be balanced.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '`splitAt()`函数将根据我们定义的拆分点进行拆分。两个新的拆分可能平衡也可能不平衡。'
- en: 'Alternatively, if we want to leave it to MongoDB to find where to split our
    chunk, we can use `splitFind`, as follows:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果我们想让MongoDB找到拆分块的位置，我们可以使用`splitFind`，如下所示：
- en: '[PRE10]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `splitFind` phrase will try to find the chunk that the `id:7` query belongs
    to and automatically define the new bounds for the split chunks so that they are
    roughly balanced.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`splitFind`短语将尝试找到`id:7`查询所属的块，并自动定义拆分块的新边界，使它们大致平衡。'
- en: In both cases, MongoDB will try to split the chunk, and if successful, it will
    remove the `jumbo` flag from it.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，MongoDB将尝试拆分块，如果成功，它将从中删除`jumbo`标志。
- en: 'If the preceding operation is unsuccessful, then, and only then, should we try
    stopping the balancer first, while also verifying the output and waiting for any
    pending migrations to finish first, as shown in the following code:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果前面的操作不成功，那么只有在这种情况下，我们应该首先尝试停止平衡器，同时验证输出并等待任何待处理的迁移完成，如下所示：
- en: '[PRE11]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This should return `false `
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该返回`false`
- en: Wait for any `waiting…` messages to stop printing, and then find the `jumbo`
    flagged chunk in the same way as before.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 等待任何`waiting…`消息停止打印，然后以与之前相同的方式找到带有`jumbo`标志的块。
- en: 'Then update the `chunks` collection in your `config` database of the `mongos`
    router, like this:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后在`mongos`路由器的`config`数据库中更新`chunks`集合，如下所示：
- en: '[PRE12]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The preceding command is a regular `update()` command, with the first argument
    being the `find()` part to find out which document to update and the second argument
    being the operation to apply to it (`$unset: jumbo flag`).'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '前面的命令是一个常规的`update()`命令，第一个参数是`find()`部分，用于查找要更新的文档，第二个参数是要应用于它的操作（`$unset:
    jumbo flag`）。'
- en: 'After all this is done, we re-enable the balancer, as follows:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成所有这些操作后，我们重新启用平衡器，如下所示：
- en: '[PRE13]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then, we connect to the `admin` database to flush the new configuration to
    all nodes, as follows:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们连接到`admin`数据库，将新配置刷新到所有节点，如下所示：
- en: '[PRE14]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Always back up the `config` database before modifying any state manually.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在手动修改任何状态之前，始终备份`config`数据库。
- en: Merging chunks
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 合并块
- en: As we have seen previously, usually MongoDB will adjust the bounds for each
    chunk in our shard to make sure that our data is equally distributed. This may
    not work in some cases—especially when we define the chunks manually—if our data
    distribution is surprisingly unbalanced, or if we have many `delete` operations
    in our shard.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所看到的，通常情况下，MongoDB将调整每个分片的块边界，以确保我们的数据均匀分布。在某些情况下，这可能不起作用，特别是当我们手动定义块时，如果我们的数据分布出奇地不平衡，或者我们的分片中有许多`delete`操作。
- en: Having empty chunks will invoke unnecessary chunk migrations and give MongoDB
    a false impression of which chunk needs to be migrated. As we have explained before,
    the threshold for chunk migration is dependent on the number of chunks that each
    shard holds. Having empty chunks may or may not trigger the balancer when it's
    needed.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有空块将引发不必要的块迁移，并使MongoDB对需要迁移的块产生错误印象。正如我们之前解释的那样，块迁移的阈值取决于每个分片持有的块的数量。拥有空块可能会触发平衡器，也可能不会在需要时触发平衡器。
- en: Chunk merging can only happen when at least one of the chunks is empty, and
    only between adjacent chunks.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 只有当至少有一个块为空时，块合并才会发生，并且只会发生在相邻块之间。
- en: 'To find empty chunks, we need to connect to the database that we want to inspect
    (in our case, `mongo_books`) and use `runCommand` with `dataSize` set as follows:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到空块，我们需要连接到要检查的数据库（在我们的情况下是`mongo_books`），并使用`runCommand`，设置`dataSize`如下：
- en: '[PRE15]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The `dataSize` phrase follows the `database_name.collection_name` pattern, whereas
    `keyPattern` is the shard key that we have defined for this collection.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '`dataSize`短语遵循`database_name.collection_name`模式，而`keyPattern`是我们为这个集合定义的分片键。'
- en: The `min` and `max` values should be calculated by the chunks that we have in
    this collection. In our case, we have entered *chunkB's* details from the example
    earlier in this chapter.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '`min`和`max`值应该由我们在这个集合中拥有的数据块计算得出。在我们的情况下，我们已经在本章前面的示例中输入了*chunkB*的详细信息。'
- en: 'If the bounds of our query (which, in our case, are the bounds of *chunkB*)
    return no documents, the result will resemble the following:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的查询边界（在我们的情况下是*chunkB*的边界）没有返回任何文档，结果将类似于以下内容：
- en: '[PRE16]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now that we know that *chunkB* has no data, we can merge it with another chunk
    (in our case, this could only be *chunkA*) like this:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道*chunkB*没有数据，我们可以像这样将它与另一个数据块（在我们的情况下，只能是*chunkA*）合并：
- en: '[PRE17]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'On a success, this will return MongoDB''s default `ok` status message, as shown
    in the following code:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 成功后，这将返回MongoDB的默认`ok`状态消息，如下所示：
- en: '[PRE18]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We can then verify that we only have one chunk on *ShardA* by invoking `sh.status()` again.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以通过再次调用`sh.status()`来验证*ShardA*上只有一个数据块。
- en: Adding and removing shards
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 添加和移除分片
- en: 'Adding a new shard to our cluster is as easy as connecting to `mongos`, connecting
    to the `admin` database, and invoking `runCommand` with the following:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 向我们的集群添加一个新的分片就像连接到`mongos`，连接到`admin`数据库，并使用以下命令调用`runCommand`一样简单：
- en: '[PRE19]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This adds a new shard from the replica set named `mongo_books_replica_set` from
    the `rs01.packtdb.com` host running on port `27017`. We also define the `maxSize`
    of data for this shard as `18000` MB (or we can set it to `0` to give it no limit)
    and the name of the new shard as `packt_mongo_shard_UK`.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这将从`rs01.packtdb.com`主机的端口`27017`上运行的`mongo_books_replica_set`复制集中添加一个新的分片。我们还将为这个分片定义数据的`maxSize`为`18000`
    MB（或者我们可以将其设置为`0`以不设限），新分片的名称为`packt_mongo_shard_UK`。
- en: This operation will take quite some time to complete as chunks will have to
    be rebalanced and migrated to the new shard.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这个操作将需要相当长的时间来完成，因为数据块将需要重新平衡和迁移到新的分片。
- en: 'Removing a shard, on the other hand, requires more involvement since we have
    to make sure that we won''t lose any data on the way. We do this as follows:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，移除一个分片需要更多的参与，因为我们必须确保在这个过程中不会丢失任何数据。我们按照以下步骤进行：
- en: 'First, we need to make sure that the balancer is enabled using `sh.getBalancerState()`.
    Then, after identifying the shard we want to remove using any one of the `sh.status()`,
    `db.printShardingStatus()`, or `listShards admin` commands, we connect to the
    `admin` database and invoke `removeShard` as follows:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要确保负载均衡器已启用，使用`sh.getBalancerState()`。然后，在使用`sh.status()`、`db.printShardingStatus()`或`listShards
    admin`命令中任何一个来识别我们想要移除的分片后，我们连接到`admin`数据库并按以下方式调用`removeShard`：
- en: '[PRE20]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The output should contain the following:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该包含以下内容：
- en: '[PRE21]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Then, if we invoke the same command again, we get the following:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，如果我们再次调用相同的命令，我们会得到以下结果：
- en: '[PRE22]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The remaining document in the result contains the number of `chunks` and `dbs`
    that are still being transferred. In our case, it's `2` and `3` respectively.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 结果中剩下的文档包含仍在传输的`chunks`和`dbs`的数量。在我们的情况下，分别是`2`和`3`。
- en: All the commands need to be executed in the `admin` database.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 所有命令都需要在`admin`数据库中执行。
- en: An extra complication in removing a shard can arise if the shard we want to
    remove serves as the primary shard for one or more of the databases that it contains.
    The primary shard is allocated by MongoDB when we initiate sharding, so when we
    remove the shard, we need to manually move these databases to a new shard.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 移除分片时可能会出现额外的复杂情况，如果我们要移除的分片作为它包含的一个或多个数据库的主分片。主分片是在我们启用分片时由MongoDB分配的，因此当我们移除分片时，我们需要手动将这些数据库移动到一个新的分片。
- en: 'We will know whether we need to perform this operation by looking at the following
    section of the result from `removeShard()`:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过查看`removeShard()`结果中的以下部分来确定是否需要执行此操作：
- en: '[PRE23]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We need to drop or `movePrimary` our `mongo_books` database. The way to do this
    is to first make sure that we are connected to the `admin` database.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要删除或`movePrimary`我们的`mongo_books`数据库。首先要确保我们连接到`admin`数据库。
- en: We need to wait for all of the chunks to finish migrating before running this
    command.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行此命令之前，我们需要等待所有数据块完成迁移。
- en: 'Make sure that the result contains the following before proceeding:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在继续之前，请确保结果包含以下内容：
- en: '[PRE24]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Only after we have made sure that the chunks to be moved are down to zero can
    we safely run the following command:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 只有在我们确保要移动的数据块已经减少到零后，我们才能安全地运行以下命令：
- en: '[PRE25]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This command will invoke a blocking operation, and, when it returns, it should
    have the following result:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个命令将调用一个阻塞操作，当它返回时，应该有以下结果：
- en: '[PRE26]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Invoking the same `removeShard()` command after we are all done should return
    the following result:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们完成所有操作后再次调用相同的`removeShard()`命令应该返回以下结果：
- en: '[PRE27]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Once we get to `state` as `completed` and `ok` as `1`, it is safe to remove
    our `packt_mongo_shard_UK` shard.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦`state`为`completed`，`ok`为`1`，就可以安全地移除我们的`packt_mongo_shard_UK`分片。
- en: Removing a shard is naturally more complicated than adding one. We need to allow
    some time, hope for the best, and plan for the worst when performing potentially
    destructive operations on our live cluster.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 移除分片自然比添加分片更复杂。在对我们的实时集群执行潜在破坏性操作时，我们需要留出一些时间，希望一切顺利，并为最坏的情况做好准备。
- en: Sharding limitations
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分片限制
- en: Sharding comes with great flexibility. Unfortunately, there are a few limitations
    in the way that we perform some of the operations.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 分片提供了很大的灵活性。不幸的是，在执行一些操作的方式上存在一些限制。
- en: 'We will highlight the most important ones in the following list:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在以下列表中突出显示最重要的部分：
- en: The `group()` database command does not work. The `group()` command should not
    be used anyway; use `aggregate()` and the aggregation framework instead, or `mapreduce()`.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`group()`数据库命令不起作用。无论如何都不应该使用`group()`命令；而是使用`aggregate()`和聚合框架，或者`mapreduce()`。'
- en: The `db.eval()` command does not work and should be disabled in most cases for
    security reasons.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `$isolated` option for updates does not work. This is a functionality that
    is missing in sharded environments. The `$isolated` option for `update()` provides
    the guarantee that, if we update multiple documents at once, other readers and
    writers will not see some of the documents updated with the new value, and the
    others will still have the old value. The way this is implemented in unsharded
    environments is by holding a global write lock and/or serializing operations to
    a single thread to make sure that every request for the documents affected by
    `update()` will not be accessed by other threads/operations. This implementation
    means that it is not performant and does not support any concurrency, which makes
    it prohibitive to allow the `$isolated` operator in a sharded environment.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `$snapshot` operator for queries is not supported. The `$snapshot` operator in
    the `find()` cursor prevents documents from appearing more than once in the results,
    as a result of being moved to a different location on the disk after an update.
    The `$snapshot` operator is operationally expensive and often not a hard requirement.
    The way to substitute it is by using an index for our queries on a field whose
    keys will not change for the duration of the query.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The indexes cannot cover our queries if our queries do not contain the shard
    key. Results in sharded environments will come from the disk and not exclusively
    from the index. The only exception is if we query only on the built-in `_id` field
    and return only the `_id` field, in which case, MongoDB can still cover the query
    using built-in indexes.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `update()` and `remove()` operations work differently. All `update()` and
    `remove()` operations in a sharded environment must include either the `_id` of
    the documents that are to be affected or the shard key; otherwise, the `mongos`
    router will have to do a full table scan across all collections, databases, and
    shards, which would be operationally very expensive.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unique indexes across shards need to contain the shard key as a prefix of the
    index. In other words, to achieve uniqueness of documents across shards, we need
    to follow the data distribution that MongoDB follows for the shards.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The shard key has to be up to 512 bytes in size. The shard key index has to
    be in ascending order on the key field that gets sharded and optionally other
    fields as well, or a hashed index on it.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The shard key value in a document is also immutable. If our shard key for our
    `User` collection is `email`, then we cannot update the `email` value for any
    user after we set it.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Querying sharded data
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Querying our data using a MongoDB shard is different than a single-server deployment
    or a replica set. Instead of connecting to the single server or the primary of
    the replica set, we connect to the `mongos` router that decides which shard to
    ask for our data. In this section, we will explore how the query router operates
    and use Ruby to illustrate how similar to a replica set this is for the developer
    .
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: The query router
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The query router, also known as the `mongos` process, acts as the interface
    and entry point to our MongoDB cluster. Applications connect to it instead of
    connecting to the underlying shards and replica sets; `mongos` executes queries,
    gathers results and passes them to our application.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: The `mongos` process doesn't hold any persistent state and is typically low
    on system resources.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: The `mongos` process is typically hosted in the same instance as the application
    server.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: It acts as a proxy for requests. When a query comes in, `mongos` will examine
    and decide which shards need to execute the query and establish a cursor in each
    one of them.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Find
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If our query includes the shard key or a prefix of the shard key, `mongos` will
    perform a targeted operation, only querying the shards that hold the keys that
    we are looking for.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, with a composite shard key of `{_id, email, address}` on our `User `collection,
    we can have a targeted operation with any of the following queries:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: These queries consist of either a prefix (as is the case with the first two)
    or the complete shard key.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, a query on `{email, address}` or `{address}` will not be
    able to target the right shards, resulting in a broadcast operation. A broadcast
    operation is any operation that doesn't include the shard key or a prefix of the
    shard key, and they result in `mongos` querying every shard and gathering results
    from them. They are also known as **scatter-and-gather operations** or **fan out
    queries**.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: This behavior is a direct result of the way indexes are organized, and is similar
    to the behavior that we identified in the chapter about indexing.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: Sort/limit/skip
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If we want to sort our results, we have the following two options:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: If we are using the shard key in our sort criteria, then `mongos` can determine
    the order in which it has to query the shard or shards. This results in an efficient
    and, again, targeted operation.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we are not using the shard key in our sort criteria, then, as is the case
    with a query without any sort criteria, it's going to be a fan out query. To sort
    the results when we are not using the shard key, the primary shard executes a
    distributed merge sort locally before passing on the sorted result set to `mongos`.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A limit on the queries is enforced on each individual shard and then again at
    the `mongos` level, as there may be results from multiple shards. A `skip` operator,
    on the other hand, cannot be passed on to individual shards, and will be applied
    by `mongos` after retrieving all the results locally.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: If we combine the `skip` and `limit` operators, `mongos` will optimize the query
    by passing both values to individual shards. This is particularly useful in cases
    such as pagination. If we query without `sort` and the results are coming from
    more than one shard, `mongos` will round robin across shards for the results.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: Update/remove
  id: totrans-286
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In document modifier operations, such as `update` and `remove`, we have a similar
    situation to the one we saw with `find`. If we have the shard key in the `find`
    section of the modifier, then `mongos` can direct the query to the relevant shard.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: If we don't have the shard key in the `find` section, then it will again be
    a fanout operation.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: The `UpdateOne`, `replaceOne`, and `removeOne` operations must have the shard
    key or the `_id` value.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table sums up the operations that we can use with sharding:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '| **Type of operation** | **Query topology** |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
- en: '| Insert | Must have the shard key |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
- en: '| Update | Can have the shard key |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
- en: '| Query with shard key | Targeted operation |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
- en: '| Query without shard key | Scatter-and-gather operation/fan out query |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
- en: '| Indexed, sorted query with shard key | Targeted operation |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
- en: '| Indexed, sorted query without shard key | Distributed sort merge |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
- en: Querying using Ruby
  id: totrans-298
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Connecting to a sharded cluster using Ruby is no different than connecting
    to a replica set. Using the official Ruby driver, we have to configure the `client`
    object to define the set of `mongos` servers, as shown in the following code:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The `mongo-ruby-driver` will then return a `client` object, which is no different
    than connecting to a replica set from the Mongo Ruby client. We can then use the
    `client` object as we did in previous chapters, with all the caveats around how
    sharding behaves differently than a standalone server or a replica set with regards
    to querying and performance.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Performance comparison with replica sets
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Developers and architects are always looking out for ways to compare performance
    between replica sets and sharded configurations.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: The way MongoDB implements sharding is based on top of replica sets. Every shard
    in production should be a replica set. The main difference in performance comes
    from fan out queries. When we are querying without the shard key, MongoDB's execution
    time is limited by the worst performing replica set. In addition, when using sorting
    without the shard key, the primary server has to implement the distributed merge
    sort on the entire dataset. This means that it has to collect all data from different
    shards, merge sort them, and pass them as sorted to `mongos`. In both cases, network
    latency and limitations in bandwidth can slow down operations, which they wouldn't
    do with a replica set.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: On the flip side, by having three shards, we can distribute our working set
    requirements across different nodes, thereby serving results from RAM instead
    of reaching out to the underlying storage, HDD or SSD.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, writes can be sped up significantly since we are no longer
    bound by a single node's I/O capacity, and we can have writes in as many nodes
    as there are shards. Summing up, in most cases, and especially for the cases where
    we are using the shard key, both queries and modification operations will be significantly
    sped up by sharding.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: The shard key is the single most important decision in sharding, and should
    reflect and apply to our most common application use cases.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: Sharding recovery
  id: totrans-308
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will explore different failure types and how we can recover
    in a sharded environment. Failure in a distributed system can take multiple forms
    and shapes. In this section we will cover all the possible cases, from the simplest
    case of a stateless component like `mongos` failing to an entire shard going down.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: mongos
  id: totrans-310
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `mongos` process is a relatively lightweight process that holds no state.
    In the case that the process fails, we can just restart it or spin up a new process
    in a different server. It's recommended that `mongos` processes are located in
    the same server as our application, and so it makes sense to connect from our
    application using the set of `mongos` servers that we have colocated in our application
    servers to ensure a high availability of `mongos` processes.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: mongod
  id: totrans-312
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A `mongod` process failing in a sharded environment is no different than it
    failing in a replica set. If it is a secondary, the primary and the other secondary
    (assuming three-node replica sets) will continue as usual.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: If it is a `mongod` process acting as a primary, then an election round will
    start to elect a new primary in this shard (which is really a replica set).
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: In both cases, we should actively monitor and try to repair the node as soon
    as possible, as our availability can be impacted.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: Config server
  id: totrans-316
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Starting from MongoDB 3.4, config servers are also configured as a replica set.
    A config server failing is no different than a regular `mongod` process failing.
    We should monitor, log, and repair the process.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: A shard goes down
  id: totrans-318
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Losing an entire shard is pretty rare, and in many cases can be attributed to
    network partitioning rather than failing processes. When a shard goes down, all
    operations that would go to this shard will fail. We can (and should) implement
    fault tolerance in our application level, allowing our application to resume for
    the operations that can be completed.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: Choosing a shard key that can easily map on our operational side can also help;
    for example, if our shard key is based on location, we may lose the EU shard,
    but will still be able to write and read data regarding US-based customers through
    our US shard.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: The entire cluster goes down
  id: totrans-321
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we lose the entire cluster, we can't do anything other than get it back up
    and running as soon as possible. It's important to have monitoring, and to put
    a proper process in place to understand what needs to be done, when, and by whom,
    should this ever happen.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: Recovering when the entire cluster goes down essentially involves restoring
    from backups and setting up new shards, which is complicated and will take time.
    Dry testing this in a staging environment is also advisable, as  is investing
    in regular backups via MongoDB Ops Manager or any other backup solution.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: A member of each shard's replica set could be in a different location for disaster-recovery
    purposes.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-325
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following sources are recommended for you to study sharding in depth:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '*Scaling MongoDB* by Kristina Chodorow'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*MongoDB: The Definitive Guide* by Kristina Chodorow and Michael Dirolf'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.mongodb.com/manual/sharding/](https://docs.mongodb.com/manual/sharding/)'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.mongodb.com/blog/post/mongodb-16-released](https://www.mongodb.com/blog/post/mongodb-16-released)'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/mongodb/mongo/blob/r3.4.2-rc0/src/mongo/s/commands/cluster_shard_collection_cmd.cpp#L469](https://github.com/mongodb/mongo/blob/r3.4.2-rc0/src/mongo/s/commands/cluster_shard_collection_cmd.cpp#L469)'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.mongodb.com/blog/post/sharding-pitfalls-part-iii-chunk-balancing-and](https://www.mongodb.com/blog/post/sharding-pitfalls-part-iii-chunk-balancing-and)'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://plusnconsulting.com/post/mongodb-sharding-and-chunks/](http://plusnconsulting.com/post/mongodb-sharding-and-chunks/)'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/mongodb/mongo/wiki/Sharding-Internals](https://github.com/mongodb/mongo/wiki/Sharding-Internals)'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://learnmongodbthehardway.com/schema/sharding](http://learnmongodbthehardway.com/schema/sharding)'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://learnmongodbthehardway.com/schema/sharding/](http://learnmongodbthehardway.com/schema/sharding/)'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.infoq.com/news/2010/08/MongoDB-1.6](https://www.infoq.com/news/2010/08/MongoDB-1.6)'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://www.pc-freak.net/images/horizontal-vs-vertical-scaling-vertical-and-horizontal-scaling-explained-diagram.png](http://www.pc-freak.net/images/horizontal-vs-vertical-scaling-vertical-and-horizontal-scaling-explained-diagram.png)'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  id: totrans-339
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored sharding, one of the most interesting features
    of MongoDB. We started with an architectural overview of sharding and moved on
    to how we can design a shard and choose the right shard key.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: We learned about monitoring, administration, and the limitations that come with
    sharding. We also learned about `mongos`, the MongoDB sharding router that directs
    our queries to the correct shard. Finally, we discussed recovery from common failure
    types in a MongoDB sharded environment.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter on fault tolerance and high availability will offer some useful
    tips and tricks that have not been covered in the 11 other chapters.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
