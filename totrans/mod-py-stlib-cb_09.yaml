- en: Concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: ThreadPools—running tasks concurrently through a pool of threads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coroutines—interleaving the execution of code through coroutines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processes—dispatching work to multiple subprocesses
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Futures—futures represent a task that will complete in the future
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scheduled tasks—setting a task to run at a given time, or every few seconds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharing data between processes—managing variables that are accessible across
    multiple processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Concurrency is the ability to run two or more tasks in the same time span,
    whether they are parallel or not. Python provides many tools to implement concurrency
    and asynchronous behaviors: threads, coroutines, and processes. While some of
    them don''t allow real parallelism due to their design (coroutines), or due to
    a Global Interpreter Lock (threads), they are very easy to use and can be leveraged
    to perform parallel I/O operations or to interleave functions with minimum effort.
    When real parallelism is required, multiprocessing is easy enough in Python to
    be a viable solution for any kind of software.'
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will cover the most common ways to achieve concurrency in Python,
    will show you how to perform asynchronous tasks that will wait in the background
    for certain conditions, and how to share data between processes.
  prefs: []
  type: TYPE_NORMAL
- en: ThreadPools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Threads have been, historically, the most common way to achieve concurrency
    within software.
  prefs: []
  type: TYPE_NORMAL
- en: In theory, when the system allows, these threads can achieve real parallelism,
    but in Python, the **Global Interpreter Lock** (**GLI**) doesn't allow threads
    actually to leverage multicore systems, as the lock will allow a single Python
    operation to proceed at any given time.
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, threads are frequently undervalued in Python, but in fact,
    even when the GIL is involved, they can be a very convenient solution to run I/O
    operations concurrently.
  prefs: []
  type: TYPE_NORMAL
- en: While using coroutines, we would need a `run` loop and some custom code to ensure
    that the I/O operation proceeds in parallel. Using threads, we can run any kind
    of function within a thread and, if that function does some kind of I/O, such
    as reading from a socket or from a disk, the other threads will proceed in the
    meantime.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the major drawbacks of threads is the cost of spawning them. That''s
    frequently stated as one of the reasons why coroutines can be a better solution,
    but there is a way to avoid paying that cost whenever you need a thread: `ThreadPool`.'
  prefs: []
  type: TYPE_NORMAL
- en: A `ThreadPool` is a set of threads that is usually started when your application
    starts and sits there doing nothing until you actually have some work to dispatch.
    This way, when we have a task that we want to run into a separate thread, we just
    have to send it to `ThreadPool`, and `ThreadPool` will assign it to the first
    available thread out of all the threads that it owns. As those threads are already
    there and running, we don't have to pay the cost to spawn a thread every time
    we have work to do.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The steps for this recipe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To showcase how `ThreadPool` works, we will need two operations that we want
    to run concurrently. One will fetch a URL from the web, which might take some
    time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The other will just wait for a given condition to be true, looping over and
    over until it''s done:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Then we will just fire the download for `https://httpbin.org/delay/3`, which
    will take 3 seconds, and concurrently wait for the download to complete.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To do so, we will run the two tasks in a `ThreadPool` (of four threads), and
    we will wait for both of them to complete:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`ThreadPool` is made of two major components: a bunch of threads and a bunch
    of queues. When the pool is created, a few orchestration threads are started together
    with as many worker threads as you specified at pool initialization.'
  prefs: []
  type: TYPE_NORMAL
- en: The worker threads will be in charge of actually running the tasks you dispatch
    to them, while the orchestration threads will be in charge of managing the worker
    threads, doing things such as telling them to quit when the pool is closed, or
    restarting them when they crash.
  prefs: []
  type: TYPE_NORMAL
- en: If no number of worker threads is provided, `TaskPool` will just start as many
    threads as the amount of cores on your system as returned by `os.cpu_count()`.
  prefs: []
  type: TYPE_NORMAL
- en: Once the threads are started, they will just sit there waiting to consume something
    from the queue containing the work that is to be done. As soon as the queue has
    an entry, the worker thread will wake up and consume it, starting the work.
  prefs: []
  type: TYPE_NORMAL
- en: Once the work is done, the job and its result are put back into the results
    queue so that whoever was waiting for them can fetch them.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, when we created `TaskPool`, we actually started four workers that began
    waiting for anything to do from the tasks queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, once we provided work for the `TaskPool`, we actually queued up two functions
    into the tasks queue, and as soon as a worker became available, it fetched one
    of them and started running it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Meanwhile, `TaskPool` returns an `AsyncResult` object, which has two interesting
    methods: `AsyncResult.ready()`, which tells us whether the result is ready (the
    task finished), and `AsyncResult.get()`, which returns the result once it''s available.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second function we queued up was the one that would wait for a specific
    predicate to be `True`, and in this case, we provided `t1.ready`, which is the
    ready method of the previous `AsyncResult`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This means that the second task will complete once the first one completes,
    as it will wait until `t1.ready() == True`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once both of the tasks are running, we tell `pool` that we have nothing more
    to do, so that it can quit once it''s finished what it''s doing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'And we wait for `pool` to quit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This way, we will wait for both tasks to complete and then we will quit all
    the threads started by `pool`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we know that all tasks are completed (because `pool.join()` returned),
    we can grab the results and print them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: If we had more work to do, we would avoid running the `pool.close()` and `pool.join()`
    methods, so that we could send more work to `TaskPool`, which would get done as
    soon as there was a thread free.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`ThreadPool` is particularly convenient when you have multiple entries to which
    you need to apply the same operation over and over. Suppose you have a list of
    four URLs that you need to download:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Fetching them in a single thread would take a lot of time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We can test the time by running the function through the `timeit` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: If we could do so using a separate thread for each function, it would only take
    the time of the slowest one to fetch all the provided URLs, as the download would
    proceed concurrently for all of them.
  prefs: []
  type: TYPE_NORMAL
- en: '`ThreadPool` actually provides us with the `map` method that does exactly that:
    it applies a function to a list of arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The result will be a list containing the results returned by each call and
    we can easily test that this will be much faster than our original example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Coroutines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Threads are the most common way to implement concurrency in most languages and
    use cases, but they are expensive in terms of cost, and while `ThreadPool` can
    be a good solution for cases when thousands of threads are involved, it's usually
    unreasonable to involve thousands of threads. Especially when long-lived I/O is
    involved, you might easily reach thousands of operations running concurrently
    (think of the amount of concurrent HTTP requests an HTTP server might have to
    handle) and most of those tasks will be sitting doing nothing, just waiting for
    data from the network or from the disk most of the time.
  prefs: []
  type: TYPE_NORMAL
- en: In those cases, asynchronous I/O is the preferred approach. Compared to synchronous
    blocking I/O where your code is sitting there waiting for the read or write operation
    to complete, asynchronous I/O allows a task that needs data to initiate the read
    operation, switch to doing something else, and once the data is available, go
    back to what it was doing.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, the notification of available data might come in the form of
    a signal, which would interrupt the concurrently running code, but, more commonly,
    asynchronous I/O is implemented through the usage of a selector (such as `select`,
    `poll`, or `epoll`) and an event loop that will resume the function waiting for
    the data as soon as the selector is notified that the data is available.
  prefs: []
  type: TYPE_NORMAL
- en: This actually leads to interleaving functions that are able to run for a while,
    reach a point where they need some I/O, and pass control to another function that
    will give it back as soon as it needs to perform some I/O too. Functions whose
    execution can be interleaved by suspending and resuming them are called **coroutines**,
    as they run cooperatively.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Python, coroutines are implemented through the `async def` syntax and are
    executed through an `asyncio` event loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we might write a function that runs two coroutines that count
    down from a given number of seconds, printing their progress. That would easily
    allow us to see that the two coroutines are running concurrently, as we would
    see output from one interleaved with output from the other:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Once an event loop is created and we run `main` within it, we will see the
    two functions running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the execution has completed, we can close the event loop as we won''t
    need it anymore:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The core of our coroutines world is the **event loop**. It''s not possible
    to run coroutines (or, at least, it gets very complicated) without an event loop,
    so the first thing our code does is create an event loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we ask the event loop to wait until a provided coroutine is completed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The `main` coroutine only starts two `countdown` coroutines and waits for their
    completion. That''s done by using `await` and, in that, the `asyncio.wait` function
    is in charge of waiting for a bunch of coroutines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '`await` is important here, because we are talking about coroutines, so unless
    they are explicitly awaited, our code would immediately move forward, and thus,
    even though we called `asyncio.wait`, we would not be waiting.'
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we are waiting for the two countdowns to complete. The first countdown
    will start from `2` and will be identified by the character `A`, while the second
    countdown will start from `3` and will be identified by `B`.
  prefs: []
  type: TYPE_NORMAL
- en: The `countdown` function by itself is very simple. It's just a function that
    loops forever and prints how much there is left to wait.
  prefs: []
  type: TYPE_NORMAL
- en: 'Between each loop it waits one second, so that it waits the expected number
    of seconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: You might be wondering why we are using `asyncio.sleep` instead of `time.sleep`,
    and the reason is that, when working with coroutines, you must ensure that every
    other function that will block is a coroutine too. That way, you know that while
    your function is blocked, you would let the other coroutines move forward.
  prefs: []
  type: TYPE_NORMAL
- en: By using `asyncio.sleep`, we let the event loop move the other `countdown` function
    forward while the first one is waiting and, thus, we properly interleave the execution
    of the two functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be verified by checking the output. When `asyncio.sleep` is used,
    the output will be interleaved between the two functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'When `time.sleep` is used, the first coroutine will have to complete fully
    before the second one can move forward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: So, a general rule when working with coroutines is that whenever you are going
    to call something that will block, make sure that it's a coroutine too, or you
    will lose the concurrency property of coroutines.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We already know that the most important benefit of coroutines is that the event
    loop is able to pause their execution while they are waiting for I/O operations
    to let other coroutines proceed. While there is currently no built-in implementation
    of HTTP protocol with support for coroutines, it's easy enough to roll out a back
    version to reproduce our example of downloading a website concurrently to track
    how long it's taking.
  prefs: []
  type: TYPE_NORMAL
- en: 'As for the `ThreadPool` example, we will need the `wait_until` function that
    will wait for any given predicate to be true:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also need a `fetch_url` function to download the content of the URL.
    As we want this to run as a coroutine, we can''t rely on `urllib`, or it would
    block forever instead of passing control back to the event loop. So, we will have
    to read the data using `asyncio.open_connection`, which works at pure TCP level
    and thus will require us to implement HTTP support ourselves:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, it''s possible to interleave the two coroutines and see that
    the download proceeds concurrently with the waiting, and that it completes in
    the expected time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Threads and coroutines are concurrency models that coexist with the Python GIL
    and the leverage execution time left available by I/O operations to allow other
    tasks to continue. With modern multicore systems, it's great to be able to use
    the full power that the system provides by involving real parallelism and distributing
    the work across all the cores that are available.
  prefs: []
  type: TYPE_NORMAL
- en: The Python standard library provides very refined tools to work with multiprocessing,
    which is a great solution to leverage parallelism on Python. As multiprocessing
    will lead to multiple separate interpreters, the GIL won't get in the way, and
    compared to threads and coroutines, it might even be easier to reason with them
    as totally isolated processes that need to cooperate, rather than to think of
    multiple threads/coroutines within same system sharing the underlying memory state.
  prefs: []
  type: TYPE_NORMAL
- en: The major cost in managing processes is usually the spawn cost and the complexity
    of having to ensure you don't fork subprocesses in any odd condition, leading
    to unwanted data in memory being copied or file descriptors being reused.
  prefs: []
  type: TYPE_NORMAL
- en: '`multiprocessing.ProcessPool` can be a very good solution to all these problems,
    as starting one at the beginning of our software will ensure that we don''t have
    to pay any particular cost when we have a task to submit to a subprocess. Furthermore,
    by creating the processes only once at the beginning, we can guarantee a predictable
    (and mostly empty) state of the software being copied to create the subprocesses.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pretty much like in the *ThreadPool* recipe, we will need two functions that
    will act as our tasks running concurrently in the processes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of processes, we don''t need to perform I/O actually to run concurrently,
    so our tasks could be doing anything. What I''m going to use is the computing
    of the Fibonacci series while printing out progress, so that we can see how the
    output of the two processes will interleave:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'So, now we need to create the multiprocessing `Pool` that will run the `fib`
    function and spawn computation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: You can see how the process IDs of the two processes interleave, and once the
    job is completed, it's possible to get the results of both of them.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When `multiprocessing.Pool` is created, a number of processes equal to the
    number of cores on the system (as stated by `os.cpu_count()`) is created through
    `os.fork` or by spawning a new Python interpreter, depending on what''s supported
    by the underlying system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the new processes are started, they will all do the same thing: execute
    the `worker` function that loops forever consuming from the queue of jobs that
    were sent to `Pool` and running them one by one.'
  prefs: []
  type: TYPE_NORMAL
- en: This means that if we create a `Pool` of two processes, we will have two workers.
    As soon as we ask `Pool` to perform something (through `Pool.apply_async`, `Pool.map`,
    or any other method), the jobs (functions and its arguments) are placed in `multiprocessing.SimpleQueue`
    from which the worker will fetch it.
  prefs: []
  type: TYPE_NORMAL
- en: Once `worker` fetches the task from the queue, it will run it. If multiple `worker`
    instances are running, each one of them will pick a task from the queue and run
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Once the task has completed, the result of the function that was executed is
    pushed back into a results queue (together with the job itself to identify which
    task the result refers to), from which `Pool` will be able to consume the results
    and provide them back to the code that originally fired the tasks.
  prefs: []
  type: TYPE_NORMAL
- en: All this communication happens across multiple processes, so it can't happen
    in memory. Instead `multiprocessing.SimpleQueue`, which is underlying, uses `pipe`,
    each producer will write into `pipe`, and each consumer will read from `pipe`.
  prefs: []
  type: TYPE_NORMAL
- en: As `pipe` is only able to read and write bytes, the arguments we submit to `pool`
    and the results of the functions executed by `pool` are converted to bytes through
    the `pickle` protocol. That is able to marshal/unmarshal Python objects in as
    far as the same modules are available on both sides (sender and receiver).
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we submit our requests to `Pool`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The `fib` function, `20`, and the empty set all get pickled and sent into the
    queue for one of the `Pool` workers to consume.
  prefs: []
  type: TYPE_NORMAL
- en: 'Meanwhile, while workers are picking up data and running the Fibonacci function,
    we join the pool, so that our primary process will block until all the processes
    on the pool have completed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: In theory, a process of the pool never completes (it runs forever, continuously
    looking for things to do in the queue). Before calling `join`, we `close` the
    pool. Closing the pool tells the pool to *exit all its processes once they finish
    what they are doing right now*.
  prefs: []
  type: TYPE_NORMAL
- en: Then, by immediately joining after `close`, we wait until the pool finishes
    what it's doing right now, which is serving our two requests.
  prefs: []
  type: TYPE_NORMAL
- en: 'As with threads, `multiprocessing.Pool` returns `AsyncResult` objects, which
    means we can check their completion through the `AsyncResult.ready()` method and
    we can grab the returned value, once it''s ready, through `AsyncResult.get()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`multiprocessing.Pool` works in nearly the same way as `multiprocessing.pool.ThreadPool`.
    In fact, they share a lot of their implementation as one is a subclass of the
    other.'
  prefs: []
  type: TYPE_NORMAL
- en: But there are some major differences that are caused by the underlying technology
    used. One is based on threads and the other on subprocesses.
  prefs: []
  type: TYPE_NORMAL
- en: The major benefit of using processes is that the Python interpreter lock won't
    limit their parallelism, and they will be able to actually run in parallel with
    each other.
  prefs: []
  type: TYPE_NORMAL
- en: On the other side, there is a cost for that. Using processes is both more expensive
    in startup time (forking a process is usually slower than spawning a thread),
    and more expensive in terms of memory used, as each process will need to have
    its own state of memory. While a lot of this cost is reduced heavily on most systems
    through techniques such as copy on write, threads usually end up being a lot cheaper
    than processes.
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, it's usually a good idea to start the process `pool` only at
    the beginning of your application, so that the additional cost of spawning processes
    is only paid once.
  prefs: []
  type: TYPE_NORMAL
- en: Processes are not only more expensive to start, but by contrast with threads,
    they don't share the state of the program; each process has its own state and
    memory. So it's not possible to share the data between `Pool` and the workers
    that will perform the tasks. All the data needs to be encoded through `pickle` and
    sent through `pipe` for the other end to consume. This has a huge cost compared
    to threads that can rely on a shared queue, especially when the data that has
    to be sent is big.
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, it's usually a good idea to avoid involving processes when
    big files or data are involved in arguments or return values, as that data will
    have to be copied multiple times to reach its final destination. In that case,
    it's better to save the data on disk and pass around the path of the file.
  prefs: []
  type: TYPE_NORMAL
- en: Futures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When a background task is spawned, it might be running concurrently with your
    main flow forever and never complete its own job (such as the worker threads of
    a `ThreadPool`), or it might be something that will return a result to you sooner
    or later and you might be waiting for that result (such as a thread that downloads
    the content of a URL in the background).
  prefs: []
  type: TYPE_NORMAL
- en: 'These second types of task all share a common behavior: their result will be
    available in `_future_`. So, a result that will be available in the future is
    commonly referred to as `Future`. Programming languages don''t all share the same
    exact definition of futures, and on Python `Future` is any function that will
    be completed in the future, typically returning a result.'
  prefs: []
  type: TYPE_NORMAL
- en: '`Future` is the callable itself, so it''s unrelated to the technology that
    will be used actually to run the callable. You will need a way to let the execution
    of the callable proceed, and in Python, that''s provided by `Executor`.'
  prefs: []
  type: TYPE_NORMAL
- en: There are executors that can run the futures into threads, processes, or coroutines
    (in the case of coroutines, the loop itself is the executor).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To run a future, we will need an executor (either `ThreadPoolExecutor`, `ProcessPoolExecutor`)
    and the futures we actually want to run. For the sake of our example, we will
    use a function that returns the time it takes to load a web page so we can benchmarks
    multiple websites to see which one is the fastest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we can create any kind of executor and have our `UrlsBenchmarker` run
    its futures within it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`UrlsBenchmarker` will fire a future for each URL through `UrlsBenchmarker._benchmark_urls`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Each future will perform `benchmark_url`, which downloads the content of the
    given URL and returns the time it took to download it, along with the URL itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Returning the URL itself is necessary, as `future` can know its return value,
    but not its arguments. So once we `submit` the function, we have lost which URL
    it is related to and by returning it together with the timing, we will always
    have the URL available whenever the timing is present.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then for each `future`, a callback is added through `future.add_done_callback`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: As soon as the future completes, it will call `UrlsBenchmarker._print_timing`,
    which prints the time it took to run the URL. This informs the user that the benchmark
    is proceeding and that it completed one of the URLs.
  prefs: []
  type: TYPE_NORMAL
- en: '`UrlsBenchmarker._benchmark_urls` will then return `futures` for all the URLs
    that we had to benchmark in a list.'
  prefs: []
  type: TYPE_NORMAL
- en: 'That list is then passed to `concurrent.futures.as_completed`. This will create
    an iterator that will return all `futures` in the order they completed and only
    when they are completed. So, we know that by iterating over it, we will only fetch
    `futures` that are already completed and we will block waiting for the completion
    of a new future as soon as the consumed all `futures` that already completed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: So, the loop will only finish when all `futures` are complete.
  prefs: []
  type: TYPE_NORMAL
- en: The list of completed `futures` is consumed by a `list` comprehension that will
    create a list containing the results of those `futures`.
  prefs: []
  type: TYPE_NORMAL
- en: As the results are all in the (`time`, `url`) form, we can use `min` to grab
    the result with the minimum time, which is the URL that took less time to download.
  prefs: []
  type: TYPE_NORMAL
- en: 'This works because comparing two tuples compares the elements in order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'So, calling `min` on a list of tuples will grab the entry with the minimum
    value in the first element of the tuple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The second element is only looked at when there are two first elements with
    the same value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'So, we grab the URL with the shortest timing (as the timing was the first of
    the entries in the tuple returned by the future) and print it as the fastest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The futures executors are very similar to the worker pools provided by `multiprocessing.pool`,
    but they have some differences that might push you toward one direction or another.
  prefs: []
  type: TYPE_NORMAL
- en: The major difference is probably the way the workers are started. The pools
    start a fixed number of workers that are created and started all at the same time
    when the pool is created. So, creating the pool early moves the cost of spawning
    the workers at the beginning of the application. This means that the application
    can be quite slow to start because it might have to fork many processes according
    to the number of workers you requested or the number of cores your system has.
    Instead, the executor creates workers only when they are needed, and it's meant
    to evolve in the future to avoid making new workers when there are available ones.
  prefs: []
  type: TYPE_NORMAL
- en: So, executors are generally faster to start up at the expense of a bit more
    delay the first time a future is sent to it, while pools focus most of their cost
    on startup time. For this reason, if you have cases where you frequently need
    to create and destroy a pool of worker processes, the `futures` executor can be
    more efficient to work with.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduled tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A common kind of background task is an action that should run by itself in the
    background at any given time. Typically, those are managed through a cron daemon
    or similar system tools by configuring the daemon to run a given Python script
    at the provided time.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you have a primary application that needs to perform tasks cyclically
    (such as expiring caches, resetting password links, flushing a queue of emails
    to send, or similar tasks), it''s not really viable to do so through a cron job
    as you would need to dump the data somewhere accessible to the other process:
    on disk, on a database, or any similarly shared storage.'
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, the Python standard library has an easy way to schedule tasks that
    are to be executed at any given time and joined with threads. It can be a very
    simple and effective solution for scheduled background tasks.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `sched` module provides a fully functioning scheduled tasks executor that
    we can mix with threads to create a background scheduler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '`BackgroundScheduler` can be started and jobs can be added to it to start their
    execution at fixed times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`BackgroundScheduler` subclasses `threading.Thread` so that it runs in the
    background while our application is doing something else. Registered tasks will
    fire and perform in a secondary thread without getting in the way of the primary
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Whenever `BackgroundScheduler` is created, the thread for it is started too,
    so it becomes immediately available. The thread will run in `daemon` mode, which
    means that it won't block the program from exiting if it's still running at the
    time the program ends.
  prefs: []
  type: TYPE_NORMAL
- en: Usually Python waits for all threads when exiting the application, so setting
    a thread as a `daemon` one makes it possible to quit without having to wait for
    them.
  prefs: []
  type: TYPE_NORMAL
- en: '`threading.Thread` executes the `run` method as the thread code. In our case,
    it''s a method that runs the tasks registered in the scheduler over and over:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '`_scheduler.run(blocking=False)` means to pick one task to run from the scheduled
    ones and run it. Then, it returns the time that it still has to be waited for
    before running the next task. If no time is returned, it means there are no tasks
    to run.'
  prefs: []
  type: TYPE_NORMAL
- en: Through `_scheduler.delayfunc(min(delta, 0.5))`, we wait for the time it takes
    before the next task needs to run, which is most half a second at most.
  prefs: []
  type: TYPE_NORMAL
- en: We wait at most half a second, because while we are waiting, the scheduled tasks
    might change. A new task might get registered and we want to ensure it won't have
    to wait more than half a second for the scheduler to catch it.
  prefs: []
  type: TYPE_NORMAL
- en: If we waited exactly the time that was pending before the next task, we might
    do a run, get back that the next task was in 60 seconds, and start waiting 60
    seconds. But what if, while we were waiting, the user registered a new task that
    had to run in 5 seconds? We would run it in 60 seconds anyway, because we were
    already waiting. By waiting at most 0.5 seconds, we know that it will take half
    a second to pick up the next task and that it will run properly in 5 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: Waiting less than the time that is pending before the next task won't make the
    tasks run any faster, because the scheduler won't run any tasks that don't already
    surpass its scheduled time. So, if there are no tasks to run, the scheduler would
    continuously tell us, *you have to wait*, and we would be waiting half a second
    for as many times as it was needed to reach the scheduled time of the next scheduled
    task.
  prefs: []
  type: TYPE_NORMAL
- en: The `run_at`, `run_after`, and `run_every` methods are the ones actually involved
    in registering functions for execution at specific times.
  prefs: []
  type: TYPE_NORMAL
- en: '`run_at` and `run_after` simply wrap the `enterabs` and `enter` methods of
    the scheduler, which allow us to register a task to run at a specific time or
    after *n* seconds.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The most interesting function is probably `run_every`, which runs a task over
    and over every *n* seconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: The method takes the callable that has to be run and wraps it into a decorator
    that actually does run the function, but once it completes, it schedules the function
    back for re-execution. This way, it will run over and over until the scheduler
    is stopped, and whenever it completes, it's scheduled again.
  prefs: []
  type: TYPE_NORMAL
- en: Sharing data between processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When working with threads or coroutines, data is shared across them by virtue
    of the fact that they share the same memory space. So, you can access any object
    from any thread, as long as attention is paid to avoiding race conditions and
    providing proper locking.
  prefs: []
  type: TYPE_NORMAL
- en: With processes, instead, things get far more complicated and no data is shared
    across them. So when using `ProcessPool` or `ProcessPoolExecutor`, we need to
    find a way to pass data across the processes and make them able to share a common
    state.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Python standard library provides many tools to create a communication channel
    between processes: `multiprocessing.Queues`, `multiprocessing.Pipe`, `multiprocessing.Value`,
    and `multiprocessing.Array` can be used to create queues that one process can
    feed and the other consume, or simply values shared between multiple processes
    in a shared memory.'
  prefs: []
  type: TYPE_NORMAL
- en: 'While all these are viable solutions, they have some limits: you must create
    all shared values before creating any process, so they are not viable if the amount
    of shared values is variable and they are limited in terms of types they can store.'
  prefs: []
  type: TYPE_NORMAL
- en: '`multiprocessing.Manager`, instead, allows us to store any number of shared
    values through a shared `Namespace`.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are the steps for this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '`manager` should be created at the beginning of your application, then all
    processes will be able to set and read values from it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have our `namespace`, any process will be able to set values to it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Any process will be able to access them all:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Without the need to create the variables early on or from the main process,
    all processes will be able to read or set any variable as far as they have access
    to `Namespace`.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `multiprocessing.Manager` class acts as a server that is able to store values
    accessible by any process that has a reference to `Manager` and to the values
    it wants to access.
  prefs: []
  type: TYPE_NORMAL
- en: '`Manager` itself is accessible by knowing the address of the socket or pipe
    where it is listening, and each process that has a reference to the `Manager`
    instance knows those:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Then, once you know how to contact the manager itself, you need to be able to
    tell the manager which object you want to access out of all that the manager is
    managing.
  prefs: []
  type: TYPE_NORMAL
- en: 'That can be done by having `Token` that represents and pinpoints that object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Particularly, `Namespace` is a kind of object that allows us to store any variable
    within it. So, it makes anything stored within `Namespace` accessible by using
    just the `namespace` token.
  prefs: []
  type: TYPE_NORMAL
- en: All processes, as they were copied from the same original process, that had
    the token of the namespace and the address of the manager are able to access `namespace`
    and thus set or read values from it.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`multiprocessing.Manager` is not constrained to work with processes that originated
    from the same process.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s possible to create a `Manager` that will listen on a network so that
    any process that is able to connect to it might be able to access its content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, once the server is started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The other processes will be able to connect to it by creating a `manager2`
    instance with the exact same arguments of the manager they want to connect to,
    and then explicitly connect:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create a `namespace` in manager and set a value into it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Knowing the token value of `namespace`, it''s possible to create a proxy object
    to access `namespace` from `manager2` through the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
