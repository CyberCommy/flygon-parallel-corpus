- en: Getting Started with Scraping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a Python development environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scraping Python.org with Requests and Beautiful Soup
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scraping Python.org with urllib3 and Beautiful Soup
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scraping Python.org with Scrapy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scraping Python.org with Selenium and PhantomJs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The amount of data available on the web is consistently growing both in quantity
    and in form.  Businesses require this data to make decisions, particularly with
    the explosive growth of machine learning tools which require large amounts of
    data for training.  Much of this data is available via Application Programming
    Interfaces, but at the same time a lot of valuable data is still only available
    through the process of web scraping.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will focus on several fundamentals of setting up a scraping environment
    and performing basic requests for data with several of the tools of the trade. 
    Python is the programing language of choice for this book, as well as amongst
    many who build systems to perform scraping.  It is an easy to use programming
    language which has a very rich ecosystem of tools for many tasks.  If you program
    in other languages, you will find it easy to pick up and you may never go back!
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a Python development environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have not used Python before, it is important to have a working development
    environment. The recipes in this book will be all in Python and be a mix of interactive
    examples, but primarily implemented as scripts to be interpreted by the Python
    interpreter. This recipe will show you how to set up an isolated development environment
    with `virtualenv` and manage project dependencies with `pip` . We also get the
    code for the book and install it into the Python virtual environment.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will exclusively be using Python 3.x, and specifically in my case 3.6.1. 
    While Mac and Linux normally have Python version 2 installed, and Windows systems
    do not. So it is likely that in any case that Python 3 will need to be installed. 
    You can find references for Python installers at www.python.org.
  prefs: []
  type: TYPE_NORMAL
- en: You can check Python's version with `python --version`
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/e9039d11-8e50-44c6-8204-3199ae5d7b1e.png)`pip` comes installed with
    Python 3.x, so we will omit instructions on its installation.  Additionally, all
    command line examples in this book are run on a Mac.  For Linux users the commands
    should be identical.  On Windows, there are alternate commands (like dir instead
    of ls), but these alternatives will not be covered.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will be installing a number of packages with `pip`.  These packages are installed
    into a Python environment.  There often can be version conflicts with other packages,
    so a good practice for following along with the recipes in the book will be to
    create a new virtual Python environment where the packages we will use will be
    ensured to work properly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Virtual Python environments are managed with the `virtualenv` tool.  This can
    be installed with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can use `virtualenv`.  But before that let''s briefly look at `pip`.
    This command installs Python packages from PyPI, a package repository with literally
    10''s of thousands of packages.  We just saw using the install subcommand to pip,
    which ensures a package is installed.  We can also see all currently installed
    packages with `pip list`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: I've truncated to the first few lines as there are quite a few.  For me there
    are 222 packages installed.
  prefs: []
  type: TYPE_NORMAL
- en: Packages can also be uninstalled using `pip uninstall` followed by the package
    name.  I'll leave it to you to give it a try.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now back to `virtualenv`. Using `virtualenv` is very simple.  Let''s use it
    to create an environment and install the code from github. Let''s walk through
    the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a directory to represent the project and enter the directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize a virtual environment folder named env:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This creates an env folder.  Let's take a look at what was installed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: New we activate the virtual environment.  This command uses the content in the
    `env` folder to configure Python. After this all python activities are relative
    to this virtual environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can check that python is indeed using this virtual environment with the
    following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: With our virtual environment created, let's clone the books sample code and
    take a look at its structure.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This created a `PythonWebScrapingCookbook` directory.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Let's change into it and examine the content.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'There are two directories.  Most the the Python code is is the `py` directory.
    `www` contains some web content that we will use from time-to-time using a local
    web server.  Let''s look at the contents of the `py` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Code for each chapter is in the numbered folder matching the chapter (there
    is no code for chapter 2 as it is all interactive Python).
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that there is a `modules` folder.  Some of the recipes throughout the
    book use code in those modules.  Make sure that your Python path points to this
    folder.  On Mac and Linux you can sets this in your `.bash_profile` file (and
    environments variables dialog on Windows):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The contents in each folder generally follows a numbering scheme matching the
    sequence of the recipe in the chapter.  The following is the contents of the chapter
    6 folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In the recipes I'll state that we'll be using the script in `<chapter directory>`/`<recipe
    filename>`.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations, you've now got a Python environment configured with the books
    code!
  prefs: []
  type: TYPE_NORMAL
- en: 'Now just the be complete, if you want to get out of the Python virtual environment,
    you can exit using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'And checking which python we can see it has switched back:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: I won't be using the virtual environment for the rest of the book. When you
    see command prompts they will be either of the form "<directory> $" or simply
    "$".
  prefs: []
  type: TYPE_NORMAL
- en: Now let's move onto doing some scraping.
  prefs: []
  type: TYPE_NORMAL
- en: Scraping Python.org with Requests and Beautiful Soup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe we will install Requests and Beautiful Soup and scrape some content
    from www.python.org.  We'll install both of the libraries and get some basic familiarity
    with them.  We'll come back to them both in subsequent chapters and dive deeper
    into each.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we will scrape the upcoming Python events from [https://www.python.org/events/pythonevents](https://www.python.org/events/pythonevents).
    The following is an an example of `The Python.org Events Page` (it changes frequently,
    so your experience will differ):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/c4caf889-b8fa-4f5e-87dc-d6d78921bddb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will need to ensure that Requests and Beautiful Soup are installed.  We
    can do that with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now let's go and learn to scrape a couple events. For this recipe we will start
    by using interactive python.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start it with the `ipython` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Next we import Requests
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We now use requests to make a GET HTTP request for the following url: [https://www.python.org/events/python-events/](https://www.python.org/events/python-events/)
    by making a `GET` request:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: That downloaded the page content but it is stored in our requests object req. 
    We can retrieve the content using the `.text` property.  This prints the first
    200 characters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We now have the raw HTML of the page.  We can now use beautiful soup to parse
    the HTML and retrieve the event data.
  prefs: []
  type: TYPE_NORMAL
- en: First import Beautiful Soup
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Now we create a `BeautifulSoup` object and pass it the HTML.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Now we tell Beautiful Soup to find the main `<ul>` tag for the recent events,
    and then to get all the `<li>` tags below it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'And finally we can loop through each of the `<li>` elements, extracting the
    event details, and print each to the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This entire example is available in the `01/01_events_with_requests.py` script
    file.  The following is its content and it pulls together all of what we just
    did step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'You can run this using the following command from the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will dive into details of both Requests and Beautiful Soup in the next chapter,
    but for now let''s just summarize a few key points about how this works.  The
    following important points about Requests:'
  prefs: []
  type: TYPE_NORMAL
- en: Requests is used to execute HTTP requests.  We used it to make a GET verb request
    of the URL for the events page.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Requests object holds the results of the request.  This is not only the
    page content, but also many other items about the result such as HTTP status codes
    and headers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Requests is used only to get the page, it does not do an parsing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We use Beautiful Soup to do the parsing of the HTML and also the finding of
    content within the HTML.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand how this worked, the content of the page has the following HTML
    to start the Upcoming Events section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/9c3b8d5a-57e7-4cab-b868-b2362f805cc8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We used the power of Beautiful Soup to:'
  prefs: []
  type: TYPE_NORMAL
- en: Find the `<ul>` element representing the section, which is found by looking
    for a `<ul>` with the a `class` attribute that has a value of `list-recent-events`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From that object, we find all the `<li>` elements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each of these `<li>` tags represent a different event.  We iterate over each
    of those making a dictionary from the event data found in child HTML tags:'
  prefs: []
  type: TYPE_NORMAL
- en: The name is extracted from the `<a>` tag that is a child of the `<h3>` tag
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The location is the text content of the `<span>` with a class of `event-location`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And the time is extracted from the `datetime` attribute of the <time> tag.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scraping Python.org in urllib3 and Beautiful Soup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe we swap out the use of requests for another library `urllib3`.
    This is another common library for retrieving data from URLs and for other functions
    involving URLs such as parsing of the parts of the actual URL and handling various
    encodings.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This recipe requires `urllib3` installed.  So install it with `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The recipe is implemented in `01/02_events_with_urllib3.py`.  The code is the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The run it with the python interpreter.  You will get identical output to the
    previous recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The only difference in this recipe is how we fetch the resource:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Unlike `Requests`, `urllib3` doesn't apply header encoding automatically. The
    reason why the code snippet works in the preceding example is because BS4 handles
    encoding beautifully.  But you should keep in mind that encoding is an important
    part of scraping. If you decide to use your own framework or use other libraries,
    make sure encoding is well handled.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Requests and urllib3 are very similar in terms of capabilities. it is generally
    recommended to use Requests when it comes to making HTTP requests. The following
    code example illustrates a few advanced features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Scraping Python.org with Scrapy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Scrapy** is a very popular open source Python scraping framework for extracting
    data. It was originally designed for only scraping, but it is has also evolved
    into a powerful web crawling solution.'
  prefs: []
  type: TYPE_NORMAL
- en: In our previous recipes, we used Requests and urllib2 to fetch data and Beautiful
    Soup to extract data. Scrapy offers all of these functionalities with many other
    built-in modules and extensions. It is also our tool of choice when it comes to
    scraping with Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scrapy offers a number of powerful features that are worth mentioning:'
  prefs: []
  type: TYPE_NORMAL
- en: Built-in extensions to make HTTP requests and handle compression, authentication,
    caching, manipulate user-agents, and HTTP headers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Built-in support for selecting and extracting data with selector languages such
    as CSS and XPath, as well as support for utilizing regular expressions for selection
    of content and links
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encoding support to deal with languages and non-standard encoding declarations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flexible APIs to reuse and write custom middleware and pipelines, which provide
    a clean and easy way to implement tasks such as automatically downloading assets
    (for example, images or media) and storing data in storage such as file systems,
    S3, databases, and others
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting ready...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several means of creating a scraper with Scrapy.  One is a programmatic
    pattern where we create the crawler and spider in our code.  It is also possible
    to configure a Scrapy project from templates or generators and then run the scraper
    from the command line using the `scrapy` command.  This book will follow the programmatic
    pattern as it contains the code in a single file more effectively.  This will
    help when we are putting together specific, targeted, recipes with Scrapy.
  prefs: []
  type: TYPE_NORMAL
- en: This isn't necessarily a better way of running a Scrapy scraper than using the
    command line execution, just one that is a design decision for this book.  Ultimately
    this book is not about Scrapy (there are other books on just Scrapy), but more
    of an exposition on various things you may need to do when scraping, and in the
    ultimate creation of a functional scraper as a service in the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The script for this recipe is `01/03_events_with_scrapy.py`. The following
    is the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The following runs the script and shows the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The same result but with another tool.  Let's go take a quick review of how
    this works.
  prefs: []
  type: TYPE_NORMAL
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will get into some details about Scrapy in later chapters, but let's just
    go through this code quick to get a feel how it is accomplishing this scrape. 
    Everything in Scrapy revolves around creating a **spider**.  Spiders crawl through
    pages on the Internet based upon rules that we provide.  This spider only processes
    one single page, so it's not really much of a spider.  But it shows the pattern
    we will use through later Scrapy examples.
  prefs: []
  type: TYPE_NORMAL
- en: The spider is created with a class definition that derives from one of the Scrapy
    spider classes.  Ours derives from the `scrapy.Spider` class.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Every spider is given a `name`, and also one or more `start_urls` which tell
    it where to start the crawling.
  prefs: []
  type: TYPE_NORMAL
- en: 'This spider has a field to store all the events that we find:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The spider then has a method names parse which will be called for every page
    the spider collects.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The implementation of this method uses and XPath selection to get the events
    from the page (XPath is the built in means of navigating HTML in Scrapy). It them
    builds the `event_details` dictionary object similarly to the other examples,
    and then adds it to the `found_events` list.
  prefs: []
  type: TYPE_NORMAL
- en: The remaining code does the programmatic execution of the Scrapy crawler.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: It starts with the creation of a CrawlerProcess which does the actual  crawling
    and a lot of other tasks.  We pass it a LOG_LEVEL of ERROR to prevent the voluminous
    Scrapy output.  Change this to DEBUG and re-run it to see the difference.
  prefs: []
  type: TYPE_NORMAL
- en: Next we tell the crawler process to use our Spider implementation.  We get the
    actual spider object from that crawler so that we can get the items when the crawl
    is complete.  And then we kick of the whole thing by calling `process.start()`.
  prefs: []
  type: TYPE_NORMAL
- en: When the crawl is completed we can then iterate and print out the items that
    were found.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: This example really didn't touch any of the power of Scrapy.  We will look more
    into some of the more advanced features later in the book.
  prefs: []
  type: TYPE_NORMAL
- en: Scraping Python.org with Selenium and PhantomJS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe will introduce Selenium and PhantomJS, two frameworks that are very
    different from the frameworks in the previous recipes. In fact, Selenium and PhantomJS
    are often used in functional/acceptance testing. We want to demonstrate these
    tools as they offer unique benefits from the scraping perspective. Several that
    we will look at later in the book are the ability to fill out forms, press buttons,
    and wait for dynamic JavaScript to be downloaded and executed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Selenium itself is a programming language neutral framework. It offers a number
    of programming language bindings, such as Python, Java, C#, and PHP (amongst others).
    The framework also provides many components that focus on testing. Three commonly
    used components are:'
  prefs: []
  type: TYPE_NORMAL
- en: IDE for recording and replaying tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Webdriver, which actually launches a web browser (such as Firefox, Chrome, or
    Internet Explorer) by sending commands and sending the results to the selected
    browser
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A grid server executes tests with a web browser on a remote server. It can run
    multiple test cases in parallel.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First we need to install Selenium.  We do this with our trusty `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: This installs the Selenium Client Driver for Python (the language bindings).
    You can find more information on it at [https://github.com/SeleniumHQ/selenium/blob/master/py/docs/source/index.rst](https://github.com/SeleniumHQ/selenium/blob/master/py/docs/source/index.rst)
    if you want to in the future.
  prefs: []
  type: TYPE_NORMAL
- en: For this recipe we also need to have the driver for Firefox in the directory
    (it's named `geckodriver`).  This file is operating system specific.  I've included
    the file for Mac in the folder. To get other versions, visit [https://github.com/mozilla/geckodriver/releases](https://github.com/mozilla/geckodriver/releases).
  prefs: []
  type: TYPE_NORMAL
- en: 'Still, when running this sample you may get the following error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: If you do, put the geckodriver file somewhere on your systems PATH, or add the
    `01` folder to your path. Oh, and you will need to have Firefox installed.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, it is required to have PhantomJS installed.  You can download and find
    installation instructions at: [http://phantomjs.org/](http://phantomjs.org/)
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The script for this recipe is `01/04_events_with_selenium.py`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'And run the script with Python.  You will see familiar output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: During this process, Firefox will pop up and open the page. We have reused the
    previous recipe and adopted Selenium.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/05feca6d-bf9f-4938-9cb7-1392310dc374.png)The Window Popped up by
    Firefox'
  prefs: []
  type: TYPE_NORMAL
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The primary difference in this recipe is the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: This gets the Firefox driver and uses it to get the content of the specified
    URL.  This works by starting Firefox and automating it to go the the page, and
    then Firefox returns the page content to our app.  This is why Firefox popped
    up.  The other difference is that to find things we need to call `find_element_by_xpath`
    to search the resulting HTML.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PhantomJS, in many ways, is very similar to Selenium. It has fast and native
    support for various web standards, with features such as DOM handling, CSS selector,
    JSON, Canvas, and SVG. It is often used in web testing, page automation, screen
    capturing, and network monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is one key difference between Selenium and PhantomJS: PhantomJS is **headless**
    and uses WebKit.  As we saw, Selenium opens and automates a browser.  This is
    not very good if we are in a continuous integration or testing environment where
    the browser is not installed, and where we also don''t want thousands of browser
    windows or tabs being opened.  Being headless, makes this faster and more efficient.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The example for PhantomJS is in the `01/05_events_with_phantomjs.py` file. 
    There is a single one line change:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: And running the script results in similar output to the Selenium / Firefox example,
    but without a browser popping up and also it takes less time to complete.
  prefs: []
  type: TYPE_NORMAL
