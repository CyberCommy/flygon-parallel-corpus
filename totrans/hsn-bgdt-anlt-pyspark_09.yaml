- en: Avoiding Shuffle and Reducing Operational Expenses
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will learn how to avoid shuffle and reduce the operational
    expense of our jobs, along with detecting a shuffle in a process. We will then
    test operations that cause a shuffle in Apache Spark to find out when we should
    be very careful and which operations we should avoid. Next, we will learn how
    to change the design of jobs with wide dependencies. After that, we will be using
    the `keyBy()` operations to reduce shuffle and, in the last section of this chapter,
    we'll see how we can use custom partitioning to reduce the shuffle of our data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Detecting a shuffle in a process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing operations that cause a shuffle in Apache Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Changing the design of jobs with wide dependencies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using `keyBy()` operations to reduce shuffle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the custom partitioner to reduce shuffle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting a shuffle in a process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn how to detect a shuffle in a process.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Loading randomly partitioned data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Issuing repartition using a meaningful partition key
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding how shuffle occurs by explaining a query
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will load randomly partitioned data to see how and where the data is loaded.
    Next, we will issue a partition using a meaningful partition key. We will then
    repartition data to the proper executors using the deterministic and meaningful
    key. In the end, we will explain our queries by using the `explain()` method and
    understand the shuffle. Here, we have a very simple test.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will create a DataFrame with some data. For example, we created an `InputRecord`
    with some random UID and `user_1`, and another input with random ID in `user_1`,
    and the last record for `user_2`. Let''s imagine that this data is loaded through
    the external data system. The data can be loaded from HDFS or from a database,
    such as Cassandra or NoSQL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the loaded data, there is no predefined or meaningful partitioning of our
    data, which means that the input record number 1 can end up in the executor first,
    and record number 2 can end up in the executor second. So, even though the data
    is from the same user, it is likely that we'll be executing operations for the
    specific user.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed in the previous chapter, [Chapter 8](fd3ec6c0-4141-4e4a-8244-3f480201b2f6.xhtml),
    *Immutable Design*, we used the `reducebyKey()` method that was taking the user
    ID or specific ID to reduce all values for the specific key. This is a very common
    operation but with some random partitioning. It is good practice to `repartition`
    the data using a meaningful key.
  prefs: []
  type: TYPE_NORMAL
- en: 'While using `userID`, we will use `repartition` in a way that the result will
    record the data that has the same user ID. So `user_1`, for example, will end
    up on the first executor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The first executor will have all the data for `userID`. If `InputRecord("1234-3456-1235-1234",
    "user_1")` is on executor 1 and `InputRecord("1123-3456-1235-1234", "user_1")` is
    on executor 2, after partitioning the data from executor 2, we will need to send
    it to executor 1, because it is a parent for this partition key. This causes a
    shuffle. A shuffle is caused by loading data that is not meaningfully partitioned,
    or not partitioned at all. We need to process our data so that we can perform
    operations on a specific key.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can `repartition` the data further, but it should be done at the beginning
    of our chain. Let''s start the test to explain our query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We are repartitioning the `userID` expression in a logical plan, but when we
    check the physical plan, it shows that a hash partition is used and that we will
    be hashing on the `userID` value. So, we scan all the RDDs and all the keys that
    have the same hash and are sent to the same executor to achieve our goal:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d4014b30-3c14-4244-8bdd-4ed9875fa4d4.png)'
  prefs: []
  type: TYPE_IMG
- en: In the next section, we'll test operations that cause a shuffle in Apache Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Testing operations that cause a shuffle in Apache Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will test the operations that cause a shuffle in Apache
    Spark. We will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Using join for two DataFrames
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using two DataFrames that are partitioned differently
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing a join that causes a shuffle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A join is a specific operation that causes shuffle, and we will use it to join
    our two DataFrames. We will first check whether it causes shuffle and then we
    will check how to avoid it. To understand this, we will use two DataFrames that
    are partitioned differently and check the operation of joining two datasets or
    DataFrames that are not partitioned or partitioned randomly. It will cause shuffle
    because there is no way to join two datasets with the same partition key if they
    are on different physical machines.
  prefs: []
  type: TYPE_NORMAL
- en: Before we join the dataset, we need to send them to the same physical machine.
    We will be using the following test.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to create `UserData`, which is a case class that we have seen already.
    It has the user ID and data. We have user IDs, that is, `user_1`, `user_2`, and
    `user_4`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We then create some transactional data similar to a user ID (`user_1`, `user_2`,
    and `user_3`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the `joinWith` transaction on `UserData` by using the `userID` column
    from `UserData` and `transactionData`. Since we have issued an `inner` join, the
    result has two elements because there is a join between the record and the transaction,
    that is, `UserData`, and `UserTransaction`. However, `UserData` has no transaction
    and `Usertransaction` has no user data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: When we were joining the data, the data was not partitioned because this was
    some random data for Spark. It was unable to know that the user ID column is the
    partition key, as it cannot guess this. Since it is not pre-partitioned, to join
    the data from two datasets, will need to send data from the user ID to the executor.
    Hence, there will be a lot of data shuffling from the executor, which is because
    the data is not partitioned.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s explain the query, perform an assert, and show the results by starting
    the test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see our result as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We have `[user_1,1]` and `[user_1,100]`, which is `userID` and `userTransaction`.
    It appears that the join worked properly, but let's look at that physical parameter.
    We had `SortMergeJoin` use `userID` for the first dataset and the second one,
    and then we used `Sort` and `hashPartitioning`.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous section, *Detecting a shuffle in a process*, we used the `partition`
    method, which uses `hashPartitioning` underneath. Although we are using `join`,
    we still need to use hash partitioning because our data is not properly partitioned.
    So, we need to partition the first dataset as there will be a lot of shuffling,
    and then we need to do exactly the same thing for the second DataFrame. Again,
    the shuffling will be done twice, and once that data is partitioned on the joined
    field, the join could be local to the executor.
  prefs: []
  type: TYPE_NORMAL
- en: There will be an assertion of records after executing the physical plan, stating
    that the `userID` user data one is on the same executor as that of user transaction
    `userID` one. Without `hashPartitioning`, there is no guarantee and hence we need
    to do the partitioning.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll learn how to change the design of jobs with wide
    dependencies, so we will see how to avoid unnecessary shuffling when performing
    a join on two datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Changing the design of jobs with wide dependencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will change the job that was performing the `join` on non-partitioned
    data. We'll be changing the design of jobs with wide dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Repartitioning DataFrames using a common partition key
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding a join with pre-partitioned data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding that we avoided shuffle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will be using the `repartition` method on the DataFrame using a common partition
    key. We saw that when issuing a join, repartitioning happens underneath. But often,
    when using Spark, we want to execute multiple operations on the DataFrame. So,
    when we perform the join with other datasets, `hashPartitioning` will need to
    be executed once again. If we do the partition at the beginning when the data
    is loaded, we will avoid partitioning again.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we have our example test case, with the data we used previously in the *Testing
    operations that cause a shuffle in Apache Spark* section. We have `UserData` with
    three records for user ID – `user_1`, `user_2`, and `user_4` – and the `UserTransaction` data
    with the user ID – that is, `user_1`, `user_2`, `user_3`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we need to `repartition` the data, which is the first very important
    thing to do. We are repartitioning our `userData` using the `userId` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will repartition our data using the `userId` column, this time for
    `transactionData`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have our data repartitioned, we have the assurance that any data that
    has the same partition key – in this example, it''s `userId` – will land on the
    same executor. Because of that, our repartitioned data will not have the shuffle,
    and the joins will be faster. In the end, we are able to join, but this time we
    are joining the pre-partitioned data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We can show our results using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/74702856-ff35-41d9-9e4b-25370715cf4c.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding screenshot, we have our physical plans for user ID and transactions.
    We perform a hash partitioning on the user ID column of the user ID data and also
    on the transaction data. After joining the data, we can see that the data is proper
    and that there is a physical plan for the join.
  prefs: []
  type: TYPE_NORMAL
- en: This time, the physical plan is a bit different.
  prefs: []
  type: TYPE_NORMAL
- en: We have a `SortMergeJoin` operation, and we are sorting our data that is already
    pre-partitioned in the previous step of our execution engine. In this way, our
    Spark engine will perform the sort-merge join, where we don't need to hash join.
    It will sort data properly and the join will be faster.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll be using `keyBy()` operations to reduce shuffle even
    further.
  prefs: []
  type: TYPE_NORMAL
- en: Using keyBy() operations to reduce shuffle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will use `keyBy()` operations to reduce shuffle. We will
    cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Loading randomly partitioned data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trying to pre-partition data in a meaningful way
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leveraging the `keyBy()` function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will load randomly partitioned data, but this time using the RDD API. We
    will repartition the data in a meaningful way and extract the information that
    is going on underneath, similar to DataFrame and the Dataset API. We will learn how
    to leverage the `keyBy()` function to give our data some structure and to cause
    the pre-partitioning in the RDD API.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the test we will be using in this section. We are creating two random
    input records. The first record has a random user ID, `user_1`, the second one
    has a random user ID, `user_1`, and the third one has a random user ID, `user_2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We will extract what is happening underneath Spark using `rdd.toDebugString`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: At this point, our data is spread randomly and the records for the user ID field
    could be on different executors because the Spark execution engine cannot guess
    whether `user_1` is a meaningful key for us or whether `1234-3456-1235-1234` is.
    We know that `1234-3456-1235-1234` is not a meaningful key, and that it is a unique
    identifier. Using that field as a partition key will give us a random distribution
    and a lot of shuffling because there is no data locality when you use the unique
    field as a partition key.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is no possibility for Spark to know that data for the same user ID will
    land on the same executor, and that''s why we need to use the user ID field, either
    `user_1`, `user_1`, or `user_2`, when partitioning the data. To achieve that in
    the RDD API, we can use `keyBy(_.userId)` in our data, but this time it will change
    the RDD type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'If we check the RDD type, we''ll see that this time, an RDD is not an input
    record, but it is an RDD of the string and input record. The string is a type
    of the field that we expected here, and it is `userId`. We will also extract information
    about the `keyBy()` function by  using `toDebugString` on the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we use `keyBy()`, all the records for the same user ID will land on the
    same executor. As we have discussed, this can be dangerous because if we have
    a skew key, it means that we have a key that has very high cardinality, and we
    can run out of memory. Also, all operations on a result will be key-wise, so we''ll
    be on the pre-partitioned data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s start this test. The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b3deb20a-fd50-456f-bd7e-30bd4a636036.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that our first debug string is a very simple one, and we have only
    the collection on the RDD, but the second one is a bit different. We have a `keyBy()`
    method and we make an RDD underneath it. We have our child RDD and parent RDD
    from the first section, *Testing operations that cause a shuffle in Apache Spark*, from
    when we extended the RDD. This a parent-child chain that's issued by the `keyBy()`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll use a custom partitioner to reduce shuffle even further.
  prefs: []
  type: TYPE_NORMAL
- en: Using a custom partitioner to reduce shuffle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will use a custom partitioner to reduce shuffle. We will
    cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a custom partitioner
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the partitioner with the `partitionBy` method on Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validating that our data was partitioned properly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will implement a custom partitioner with our custom logic, which will partition
    the data. It will inform Spark where each record should land and on which executor.
    We will be using the `partitionBy` method on Spark. In the end, we will validate
    that our data was partitioned properly. For the purposes of this test, we are
    assuming that we have two executors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s assume that we want to split our data evenly into `2` executors and
    that the instances of data with the same key will land on the same executor. So,
    our input data is a list of `UserTransactions`: `"a"`, `"b"`, `"a"`, `"b"`, and `"c"`.
    The values are not so important, but we need to keep them in mind to test the
    behavior later. The `amount` is `100`, `101`, `202`, `1`, and `55` for the given
    `UserTransactions`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'When we do a `keyBy`, `(_.userId)` is passed to our partitioner and so when
    we issue `partitionBy`, we need to extend `override` methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The `getPartition` method takes a `key`, which will be the `userId`. The key
    will be passed here and the type will be a string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The signature of these methods is `Any`, so we need to `override` it, and also
    override the number of partitions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then print our two partitions, and `numPartitions` returns the value of `2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '`getPartition` is very simple as it takes the `hashCode` and `numberOfExecutors` modules. It
    ensures that the same key will land on the same executor.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will then map every partition for the respective partition as we get an
    iterator. Here, we are taking `amount` for a test purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'In the end, we assert `55`, `100`, `202`, `101`, and `1`; the order is random,
    so there is no need to take care of the order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'If we still want to, we should use a `sortBy` method. Let''s start this test
    and see whether our custom partitioner works as expected. Now, we can start. We
    have `2` partitions, so it works as expected, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b29036a1-2b48-4d1b-a9c1-5c99d1d65299.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to detect shuffle in a process. We covered testing
    operations that cause a shuffle in Apache Spark. We also learned how to employ
    partitioning in the RDD. It is important to know how to use the API if partitioned
    data is needed, because RDD is still widely used, so we use the `keyBy` operations
    to reduce shuffle. We also learned how to use the custom partitioner to reduce
    shuffle.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll learn how to save data in the correct format using
    the Spark API.
  prefs: []
  type: TYPE_NORMAL
