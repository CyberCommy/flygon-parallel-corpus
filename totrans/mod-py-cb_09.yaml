- en: Chapter 9. Input/Output, Physical Format, and Logical Layout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll look at the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Using pathlib to work with filenames
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading and writing files with context managers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replacing a file while preserving the previous version
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading delimited files with the CSV module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading complex formats using regular expressions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading JSON documents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading XML documents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading HTML documents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Upgrading CSV from DictReader to namedtuple reader
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Upgrading CSV from DictReader to namespace reader
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using multiple contexts for reading and writing files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The term **file** is overloaded with many meanings:'
  prefs: []
  type: TYPE_NORMAL
- en: The **operating system** ( **OS** ) uses a file as a way to organize bytes of
    data. The bytes might represent an image, some sound samples, words, or even an
    executable program. All of the wildly different kinds of content are reduced to
    a collection of bytes. Application software makes sense of the bytes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are two common kinds of OS files:'
  prefs: []
  type: TYPE_NORMAL
- en: Block files exist on devices such as disks or **solid state drives** ( **SSD**
    ). These files can be read in blocks of bytes. The OS can seek any specific byte
    within the file at any time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Character files are a way to manage a device like a network connection, or a
    keyboard attached to a computer. The file is viewed as a stream of individual
    bytes, which arrive at seemingly random points of time. There's no way to seek
    forward or backwards in the stream of bytes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The word *file* also defines a data structure used by the Python runtime. The
    Python file abstraction wraps the various OS file implementations. When we open
    a file, there is a binding between a Python abstraction, an OS implementation,
    and the underlying collection of bytes on a disk or other device.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A file can also be interpreted as a collection of Python objects. From this
    viewpoint, the bytes of the file represent Python objects such as strings or numbers.
    Files of text strings are very common and easy to work with. The Unicode characters
    are often encoded to bytes using the UTF-8 encoding scheme, but there are are
    many alternatives. Python provides modules such as `shelve` and `pickle` to encode
    more complex Python objects as bytes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Often, we'll talk about how an object is serialized. When an object is written
    to a file, the Python object state information is transformed to a series of bytes.
    Deserialization is the reverse process of recovering a Python object from the
    bytes. We can also call this idea the representation of state because we generally
    serialize the state of each individual object separate from the class definition.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we process data from files, we''ll often need to make two distinctions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The physical format of the data** : This answers the fundamental question
    of what Python data structure is encoded by the bytes in a file. The bytes might
    be Unicode text. The text could represent **comma-separated values** ( **CSV**
    ) or JSON documents. The physical format is commonly handled by Python libraries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The logical layout of the data** : The layout looks at the details of the
    various CSV columns, or JSON fields within the data. In some cases, the columns
    may be labeled, or there may be data that must be interpreted by position. This
    is something that is often the responsibility of our application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both the physical format and logical layout are essential to interpreting the
    data on a file. We'll look at a number of recipes for working with different physical
    formats. We'll also look at ways to divorce our program from some aspects of logical
    layout.
  prefs: []
  type: TYPE_NORMAL
- en: Using pathlib to work with filenames
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Most operating systems use a hierarchical path to identify a file. Here''s
    an example filename:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This full pathname has the following elements:'
  prefs: []
  type: TYPE_NORMAL
- en: The leading `/` means the name is absolute. It starts from the root of the filesystem.
    In Windows, there can be an extra letter in front of the name, such as `C:` ,
    to distinguish the filesystems on each individual storage device. Linux and Mac
    OS X treat all of the devices as a single, large filesystem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The names such as `Users` , `slott` , `Documents` , `Writing` , `Python Cookbook`
    , and `code` represent the directories (or folders) of the filesystem. There must
    be a top-level `Users` directory. It must contain the `slott` subdirectory. This
    is true for each name in the path.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Windows, the OS uses `\` to separate items on the path. Python uses `/` .
    Python's standard `/` is converted to the Windows path separator character gracefully;
    we can generally ignore the Windows `\` .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no way to tell what kind of object the name `code` represents. There
    are many kinds of filesystem objects. The name `code` might be a directory that
    names other files. It could be an ordinary data file, or a link to a stream-oriented
    device. There is additional directory information that shows what kind of filesystem
    object this is.
  prefs: []
  type: TYPE_NORMAL
- en: A path without the leading `/` is relative to the current working directory.
    In Mac OS X and Linux, the `cd` command sets the current working directory. In
    Windows, the `chdir` command does this job. The current working directory is a
    feature of the login session with the OS. It's made visible by the shell.
  prefs: []
  type: TYPE_NORMAL
- en: How can we work with pathnames in a way that's independent of the specific operating
    system? How can we simplify common operations to make them as uniform as possible?
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It''s important to separate two concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: The path that identifies a file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The contents of the file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The path provides an optional sequence of directory names and the final filename.
    It may provide some information about the file contents via the filename extension.
    The directory includes the files name, information about when the file was created,
    who owns it, what the permissions are, how big it is, and other details. The contents
    of the file are separate from the directory information and the name.
  prefs: []
  type: TYPE_NORMAL
- en: Often, a filename has a suffix that can provide a hint as to what the physical
    format is. A file ending in `.csv` is likely a text file that can be interpreted
    as rows and columns of data. This binding between name and physical format is
    not absolute. File suffixes are only a hint, and can be wrong.
  prefs: []
  type: TYPE_NORMAL
- en: It's possible for the contents of a file to have more than one name. Multiple
    paths can link to a single file. The directory entries that provide additional
    names for the file's content are created with the link (`ln` ) command. Windows
    uses `mklink` . This is called a **hard link** because it's a low-level connection
    between names and content.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to hard links, we can also have **soft links** or **symbolic links**
    (or junction points). A soft link is a different kind of file, the link is easily
    seen as a reference to another file. The GUI presentation of the OS may show these
    as a different icon and call it an alias or shortcut to make it clear.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Python, the `pathlib` module handles all of the path-related processing.
    The module makes several distinctions among paths:'
  prefs: []
  type: TYPE_NORMAL
- en: Pure paths that may or may not refer to an actual file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concrete paths that are resolved and refer to an actual file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This distinction allows us to create pure paths for files that our application
    will likely create or refer to. We can also create concrete paths for those files
    that actually exist on the OS. An application can resolve a pure path to create
    a concrete path.
  prefs: []
  type: TYPE_NORMAL
- en: The `pathlib` module also makes a distinction between Linux path objects and
    Windows path objects. This distinction is rarely needed; most of the time, we
    don't want to care about the OS-level details of the path. An important reason
    for using `pathlib` is because we want processing that is identical irrespective
    of the underlying OS. The cases where we might want to work with a `PureLinuxPath`
    object are rare.
  prefs: []
  type: TYPE_NORMAL
- en: 'All of the mini recipes in this section will leverage the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We rarely need any of the other class definitions from `pathlib` .
  prefs: []
  type: TYPE_NORMAL
- en: We'll presume that `argparse` is used to gather the file or directory names.
    For more information on `argparse` , see the *Using argparse to get command line
    input* recipe in [Chapter 5](text00063.html#page "Chapter 5. User Inputs and Outputs")
    , *User Inputs and Outputs* . We'll use the `options` variable, which has the
    `input` filename or directory name that the recipe works with.
  prefs: []
  type: TYPE_NORMAL
- en: 'For demonstration purposes, a mock argument parsing is shown by providing the
    following `Namespace` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This `options` object has three mock argument values. The `input` value is
    a pure path: it doesn''t necessarily reflect an actual file. The `file1` and `file2`
    values reflect concrete paths that exist on the author''s computer. This object
    behaves the same as the options created by the `argparse` module.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ll show a number of common pathname manipulations as separate mini recipes.
    This will include the following manipulations:'
  prefs: []
  type: TYPE_NORMAL
- en: Making the output filename from the input filename
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making a number of sibling output files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a directory and a number of files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing file dates to see which is newer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removing a file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding all files that match a given pattern
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making the output filename by changing the input suffix
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Perform the following steps to make the output filename by changing the input
    suffix:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the `Path` object from the input filename string. The `Path` class will
    properly parse the string to determine the elements of the path:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the `PosixPath` class is displayed because the author is using
    Mac OS X. On a Windows machine, the class would be `WindowsPath` .
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the output `Path` object using the `with_suffix()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: All of the filename parsing is handled seamlessly by the `Path` class. The `with_suffix()`
    method saves us from manually parsing the text of the filename.
  prefs: []
  type: TYPE_NORMAL
- en: Making a number of sibling output files with distinct names
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Perform the following steps for making a number of sibling output files with
    distinct names:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a `Path` object from the input filename string. The `Path` class will
    properly parse the string to determine the elements of the path:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the `PosixPath` class is displayed because the author uses
    Linux. On a Windows machine, the class would be `WindowsPath` .
  prefs: []
  type: TYPE_NORMAL
- en: 'Extract the parent directory and the stem from the filename. The stem is the
    name without the suffix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Build the desired output name. For this example, we''ll append `_pass` to the
    filename. An input file of `file.csv` will produce an output of `file_pass.csv`
    :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Build the complete `Path` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `/` operator assembles a new path from `path` components. We need to put
    this in parentheses to be sure that it's performed first and creates a new `Path`
    object. The `input_directory` variable has the parent `Path` object, and the `output_stem_pass`
    is a simple string. After assembling a new path with the `/` operator, the `with_suffix()`
    method is used to assure a specific suffix is used.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a directory and a number of files
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following steps are for creating a directory and a number of files:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the `Path` object from the input filename string. The `Path` class will
    properly parse the string to determine the elements of the path:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the `PosixPath` class is displayed because the author uses
    Linux. On a Windows machine, the class would be `WindowsPath` .
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the `Path` object for the output directory. In this case, we''ll create
    an `output` directory as a subdirectory with the same parent directory as the
    source file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the output filename using the output `Path` object. In this example,
    the output directory will contain a file that has the same name as the input with
    a different suffix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We've used the `/` operator to assemble a new `Path` object from the parent
    `Path` and a string based on the stem of a filename. Once a `Path` object has
    been created, we can use the `with_suffix()` method to set the desired suffix
    for the file.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing file dates to see which is newer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following are the steps to see newer file dates by comparing them:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the `Path` objects from the input filename strings. The `Path` class
    will properly parse the string to determine the elements of the path:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `stat()` method of each `Path` object to get timestamps for the file.
    This method returns a `stat` object, within that `stat` object, the `st_mtime`
    attribute of that object provides the most recent modification time for the file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The values are timestamps measured in seconds. We can easily compare the two
    values to see which is newer.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want a timestamp that''s sensible to people, we can use the `datetime`
    module to create a proper `datetime` object from this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We can use the `strftime()` method to format the `datetime` object or we can
    use the `isoformat()` method to provide a standardized display. Note that the
    time will have the local time zone offset implicitly applied to the OS timestamp;
    depending on the OS configuration(s) a laptop may not show the same time as the
    server that created it because they're in different time zones.
  prefs: []
  type: TYPE_NORMAL
- en: Removing a file
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Linux term for removing a file is **unlinking** . Since a file may have
    many links, the actual data isn''t removed until all links are removed:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the `Path` object from the input filename string. The `Path` class will
    properly parse the string to determine the elements of the path:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `unlink()` method of this `Path` object to remove the directory entry.
    If this was the last directory entry for the data, then the space can be reclaimed
    by the OS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: If the file does not exist, a `FileNotFoundError` is raised. In some cases,
    this exception needs to be silenced with the `pass` statement. In other cases,
    a warning message might be important. It's also possible that a missing file represents
    a serious error.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we can rename a file using the `rename()` method of a `Path` object.
    We can create new soft links using the `symlink_to()` method. To create OS-level
    hard links, we need to use the `os.link()` function.
  prefs: []
  type: TYPE_NORMAL
- en: Finding all files that match a given pattern
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following are steps to find all files that match a given pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the `Path` object from the input directory name. The `Path` class will
    properly parse the string to determine the elements of the path:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `glob()` method of the `Path` object to locate all files that match
    a given pattern. By default, this will not recursively walk the entire directory
    tree:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Inside the OS, a path is a sequence of directories (a folder is a depiction
    of a directory). In a name such as `/Users/slott/Documents/writing` , the root
    directory, `/` , contains a directory named `Users` . This contains a subdirectory
    `slott` , which contains `Documents` , which contains `writing` .
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, a simple string representation is used to summarize the navigation
    from root to directory through to the final target directory. The string representation;
    however, makes many kinds of path operations into complex string parsing problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Path` class definition simplifies many operations on pure paths. A pure
    `Path` may or may not reflect actual filesystem resources. Operations on `Path`
    include the following examples:'
  prefs: []
  type: TYPE_NORMAL
- en: Extract the parent directory, as well as a sequence of all enclosing directory
    names.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extract the final name, the stem of the final name, and the suffix of the final
    name.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replace the suffix with a new suffix or replace the entire name with a new name.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convert a string to a `Path` . And also convert a `Path` to a string. Many OS
    functions and parts of Python prefer to use filename strings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build a new `Path` object from an existing `Path` joined with a string using
    the `/` operator.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A concrete `Path` represents an actual filesystem resource. For concrete `Paths`
    , we can do a number of additional manipulations of the directory information:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Determine what kind of directory entry this is: an ordinary file, a directory,
    a link, a socket, a named pipe (or fifo), a block device, or a character device.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get the directory details, this includes information such as timestamps, permissions,
    ownership, size, and so on. We can also modify these things.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can unlink (or remove) the directory entry.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Just about anything we might want to do with directory entries for files can
    be done with the `pathlib` module. The few exceptions are part of the `os` or
    `os.path` module.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we look at other file-related recipes in the rest of this chapter, we'll
    use `Path` objects to name the files. The objective is to avoid trying to use
    strings to represent paths.
  prefs: []
  type: TYPE_NORMAL
- en: The `pathlib` module makes a small distinction between Linux pure `Path` objects,
    and Windows pure `Path` objects. Most of the time, we don't care about the OS-level
    details of the path.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two cases where it can help to produce pure paths for a specific
    operating system:'
  prefs: []
  type: TYPE_NORMAL
- en: If we do development on a Windows laptop, but deploy web services on a Linux
    server, it may be necessary to use `PureLinuxPath` . This allows us to write test
    cases on the Windows development machine that reflects actual intended use on
    a Linux server.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we do development on a Mac OS X (or Linux) laptop, but deploy exclusively
    to Windows servers, it may be necessary to use `PureWindowsPath` .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We might have something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Note that the `/` characters are normalized from Windows to Python notation
    when displaying the `WindowsPath` object. Using the `str()` function retrieves
    a path string appropriate for the Windows OS.
  prefs: []
  type: TYPE_NORMAL
- en: If we try this using the generic `Path` class, we'll get an implementation appropriate
    to the user's environment, which may not be Windows. By using `PureWindowsPath`
    , we've bypassed the mapping to the user's actual OS.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the  *Replacing a file while preserving the previous version* recipe, we'll
    look at how to leverage the features of a `Path` to create a temporary file and
    then rename the temporary file to replace the original file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the *Using argparse to get command-line input* recipe in [Chapter 5](text00063.html#page
    "Chapter 5. User Inputs and Outputs") , *User Inputs and Outputs* , we'll look
    at one very common way to get the initial string that will be used to create a
    `Path` object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading and writing files with context managers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many programs will access external resources such as database connections, network
    connections, and OS files. It's important for a reliable, well-behaved program
    to release all external entanglements reliably and cleanly.
  prefs: []
  type: TYPE_NORMAL
- en: A program that raises an exception and eventually crashes can still properly
    release resources. This includes closing a file and being sure that any buffered
    data is properly written to the file.
  prefs: []
  type: TYPE_NORMAL
- en: This is particularly important for long-running servers. A web server may open
    and close many files. If the server did not close each file properly, then data
    objects might be left in memory, reducing the amount of room that can be used
    for ongoing web services. The loss of working memory appears like a slow leak.
    Eventually the server needs to be restarted, reducing availability.
  prefs: []
  type: TYPE_NORMAL
- en: How can we be sure that resources are acquired and released properly? How can
    we avoid resource leaks?
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One common example of expensive and important resources is an external file.
    A file that has been opened for writing is also a precious resource; after all,
    we run programs to create useful output in the form of files. It's essential that
    the OS-level resources associated with a file be cleanly released by the Python
    application. We want to be sure that buffers are flushed and the file is properly
    closed no matter what happens inside the application.
  prefs: []
  type: TYPE_NORMAL
- en: When we use a context manager, we can be sure that the files being used by our
    application are handled properly. Specifically, the file will always be closed
    even when exceptions are raised during processing.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, we'll use a script to collect some basic information about files
    in a directory. This can be used to detect file changes, the technique is often
    used to trigger processing when a file has been replaced.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll write a summary file that has the filename, modification date, size,
    and a checksum computed from the bytes in the file. We can then examine the directory
    and compare it with the previous state from the summary file. The description
    of a single file''s details can be prepared by this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This function gets a relative filename from the given `Path` object in the `path`
    parameter. We could also use the `resolve()` method to get the absolute pathname.
    The `stat()` method of a `Path` object returns a number of OS status values. The
    `st_mtime` value of the status is the last modified time. The expression `path.stat().st_mtime`
    gets the modification time for the file. This is used to create a complete `datetime`
    object. The `isoformat()` method then provides a standardized display of the modification
    time.
  prefs: []
  type: TYPE_NORMAL
- en: The value of `path.stat().st_size` is the file's current size. The value of
    `path.read_bytes()` is all of the bytes in the file, these are passed to the `md5`
    class to create a checksum using the MD5 algorithm. The `hexdigest()` function
    of the resulting `md5` object gives us a value that is sensitive enough to detect
    any single-byte change in the file.
  prefs: []
  type: TYPE_NORMAL
- en: We want to apply this to a number of files in a directory. If the directory
    is being used for example, files are being written frequently then it's possible
    that our analysis program might crash with an I/O exception while trying to read
    a file that's being written by a separate process.
  prefs: []
  type: TYPE_NORMAL
- en: We'll use a context manager to make sure the program provides good output even
    in the rare case that it crashes.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ll be working with file paths, so it''s important to import the `Path`
    class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `Path` that identifies the output file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The `with` statement creates the `file` object, and assigns it to a variable,
    `summary_file` . It also uses this `file` object as the context manager:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We can now use the `summary_file` variable as an output file. No matter what
    exceptions are raised inside the `with` statement, the file will be properly closed,
    and all OS resources released.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following statements will write information about files in the current
    working directory to the open summary file. These are indented inside the `with`
    statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This creates a `Path` for the current working directory and saves the object
    in the `base` variable. The `glob()` method of a `Path` object will generate all
    filenames that match the given pattern. The `file_facts()` function shown previously
    produces a namespace object that has useful information. We can print each summary
    to the `summary_file` .
  prefs: []
  type: TYPE_NORMAL
- en: We've omitted converting the facts to a more useful notation. It can slightly
    simplify subsequent processing if the data is serialized in JSON notation.
  prefs: []
  type: TYPE_NORMAL
- en: When the `with` statement finishes, the file will be closed. This will happen
    irrespective of any exception that might have been raised.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The context manager object and the `with` statement work together to manage
    valuable resources. In this case, the file connection is a relatively expensive
    resource because it binds OS resources with our application. It's also precious
    because it's the useful output from the script.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we write `with x:` , the object `x` is the context manager. A context
    manager object responds to two methods. These two methods are invoked by the `with`
    statement on the object provided. The significant events are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`x.__enter__()` is evaluated at the beginning of the context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`x.__exit__(*details)` is evaluated at the end of the context. The `__exit__()`
    is guaranteed irrespective of any exceptions that might have been raised within
    the context. The exception details are provided to the `__exit__()` method. The
    context manager might want to behave differently if there was an exception.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: File objects and several other kinds of objects are designed to work with this
    object manager protocol.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the sequence of events that describe how the context manager is used:'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluate `summary_path.open('w')` to create a file object. This is saved to
    `summary_file` .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate `summary_file.__enter__()` as the with context starts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do the processing inside the `with` statement context. This will write several
    lines to the given file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At the end of the `with` statement, evaluate `summary_file.__exit__()` . This
    will close the output file, and release all OS resources.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If an exception was raised inside the `with` statement and not handled, then
    reraise that exception now that the file is properly closed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The file close operations are handled automatically by the `with` statement.
    They're always performed, even if there's an exception raised. This guarantee
    is essential to preventing resource leaks.
  prefs: []
  type: TYPE_NORMAL
- en: Some people like to quibble about the word  *always* : they like to search for
    the very few situations where the context manager will not work properly. For
    example, there is a remote possibility that the entire Python runtime environment
    crashes; this will invalidate all of the language guarantees. If the Python context
    manager doesn't close the file properly, the OS will close the file, but the final
    buffer of data may be lost. There's an even more remote possibility that the entire
    OS crashes, or the hardware stops, or the computer is destroyed during a zombie
    apocalypse; the context manager won't close the files in these situations, either.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many database connections and network connections also work as context managers.
    The context manager guarantees that the connection is closed properly and the
    resources released.
  prefs: []
  type: TYPE_NORMAL
- en: We can use context managers for input files, also. The best practice is to use
    a context manager for all file operations. Most of the recipes in this chapter
    will use files and context managers.
  prefs: []
  type: TYPE_NORMAL
- en: In rare cases, we'll need to add context management capabilities to an object.
    The `contextlib` includes a function, `closing()` , which will call an object's
    `close()` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use this to wrap a database connection that lacks appropriate context
    manager capabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This assumes that the `some_database()` function creates a connection to a database.
    This connection can't be directly used as a context manager. By wrapping the connection
    in the `closing()` function, we've added the necessary features to make this into
    a proper connection manager object so that we can be assured that the database
    is properly closed.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For more information on multiple contexts, see the *Using multiple contexts
    for reading and writing files* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replacing a file while preserving the previous version
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can leverage the power of `pathlib` to support a variety of filename manipulations.
    In the *Using pathlib to work with filenames* recipe, we looked at a few of the
    most common techniques of managing directories, filenames, and file suffixes.
  prefs: []
  type: TYPE_NORMAL
- en: One common file processing requirement is to create output files in a fail-safe
    manner. That is, the application should preserve any previous output file no matter
    how or where the application fails.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: At time *t* [0] there's a valid `output.csv` file from yesterday's use of the
    `long_complex.py` application.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At time  *t* [1] we start running the `long_complex.py` application. It begins
    overwriting the `output.csv` file. It is expected to finish normally at time *t*
    [3] .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At time *t* [2] , the application crashes. The partial `output.csv` file is
    useless. Worse, the valid file from time  *t* [0] is not available either, since
    it was overwritten.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Clearly, we can make backup copies of files. This introduces an extra processing
    step. We can do better. What's a good approach to creating files that are fail-safe?
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fail-safe file output generally means that we don't overwrite the previous file.
    Instead, the application will create a new file using a temporary name. If the
    file was created successfully, then the old file can be replaced using a rename
    operation.
  prefs: []
  type: TYPE_NORMAL
- en: The goal is to create files in such a way that at any time prior to the rename,
    a crash will leave the original file in place. At any time subsequent to the rename,
    the new file is in place and is valid.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several ways to approach this. We''ll show a variation that uses
    three separate files:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output file that will be overwritten eventually: `output.csv` .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A temporary version of the file: `output.csv.tmp` . There are a variety of
    conventions for naming this file. Sometimes extra characters such as `~` or `#`
    are placed on the filename to indicate that it''s a temporary, working file. Sometimes
    it will be in the `/tmp` filesystem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The previous version of the file: `name.out.old` . Any previous `.old` file
    will be removed as part of finalizing the output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Import the `Path` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'For demonstration purposes, we''ll mock the argument parsing by providing the
    following `Namespace` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: We've provided a mock value for the `target` command-line argument. This `options`
    object behaves like the options created by the `argparse` module.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the pure `Path` for the desired output file. This file doesn''t exist
    yet, which is why this is a pure path:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the pure `Path` for a temporary output file. This will be used to create
    output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Write content to the temporary file. This is of course the heart of the application.
    It''s often quite complex. For this example, we''ve shortened it to writing just
    one literal string:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Any failure here has no impact on the original output file; the original file
    hasn't been touched.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remove any prior `.old file` :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Any failure at this point has no impact on the original output file.
  prefs: []
  type: TYPE_NORMAL
- en: 'If there''s an existing file, rename it to become the `.old file` :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Any failure after this will leave the `.old` file in place. This extra file
    can be renamed as part of a recovery process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rename the temporary file to be the new output file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: At this point, the file has been overwritten by renaming the temporary file.
    An `.old` file will be left around in case there's a need to roll back the processing
    to the previous state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This process involves three separate OS operations, an unlink, and two renames.
    This leads to a situation in which the `.old` file needs to be used to recover
    the previously good state.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a timeline that shows the state of the various files. We''ve labeled
    the content as version 1 (the previous contents) and version 2 (the revised contents):'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Time** | **Operation** | **.csv.old** | **.csv** | **.csv.tmp** |'
  prefs: []
  type: TYPE_TB
- en: '| *t* [0] |  | version 0 | version 1 |  |'
  prefs: []
  type: TYPE_TB
- en: '| *t* [1] | writing | version 0 | version 1 | in-process |'
  prefs: []
  type: TYPE_TB
- en: '| *t* [2] | close | version 0 | version 1 | version 2 |'
  prefs: []
  type: TYPE_TB
- en: '| *t* [3] | unlink `.csv.old` |  | version 1 | version 2 |'
  prefs: []
  type: TYPE_TB
- en: '| *t* [4] | rename `.csv` to `.csv.old` | version 1 |  | version 2 |'
  prefs: []
  type: TYPE_TB
- en: '| *t* [5] | rename `.csv.tmp` to `.csv` | version 1 | version 2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'While there are several opportunities for failure, there''s no ambiguity about
    which file is valid:'
  prefs: []
  type: TYPE_NORMAL
- en: If there's a `.csv` file, it's the current, valid file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If there's no `.csv` file, then the `.csv.old`  file is a backup copy, which
    can be used for recovery
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since none of these operations involved actually copying the files, they're
    all extremely fast and very reliable.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In many cases, the output files involve optionally creating a directory based
    on timestamps. This can be handled gracefully by the `pathlib` module, also. We
    might, for example, have an archive directory that we''ll put old files in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We may want to create date-stamped subdirectories for keeping temporary or
    working files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then do the following to define a working directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The `mkdir()` method will create the expected directory. Including the `parents=True`
    argument that assures that all parent directories will also be created. This can
    be handy the very first time an application is executed. The `exists_ok=True`
    is handy so that the existing directory can be reused without raising an exception.
  prefs: []
  type: TYPE_NORMAL
- en: The `parents=True` is not the default. With the default of `parents=False` ,
    when a parent directory doesn't exist, the application will crash because the
    required file doesn't exist.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the `exists_ok=True` is not the default. By default, if the directory
    exists, a `FileExistsError` exception is raised. Including options that make the
    operation silent when the directory exists.
  prefs: []
  type: TYPE_NORMAL
- en: Also, it's sometimes appropriate to use the `tempfile` module to create temporary
    files. This module can create filenames that are guaranteed to be unique. This
    allows a complex server process to create temporary files without regard to filename
    conflicts.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the *Using pathlib to work with filenames* recipe, we looked at the fundamentals
    of the `Path` class
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In [Chapter 11](text00120.html#page "Chapter 11. Testing") , *Testing* , we'll
    look at some techniques for writing unit tests that can assure that parts of this
    will behave properly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading delimited files with the CSV module
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One commonly used data format is CSV. We can easily generalize this to think
    of the comma as simply one of many candidate separator characters. We might have
    a CSV file that uses the `|` character as the separator between columns of data.
    This generalization makes CSV files particularly powerful.
  prefs: []
  type: TYPE_NORMAL
- en: How can we process data in one of the wide varieties of CSV formatting?
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A summary of a file''s content is called a schema. It''s essential to distinguish
    between two aspects of the schema:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The Physical Format of the file** : For CSV, this means the file contains
    text. The text is organized into rows and columns. There will be a row separator
    character (or characters); there will also be a column separator character. Many
    spreadsheet products will use `,` as the column separator and the `\r\n` sequence
    of characters as the row separator. Other formats are possible, though, and it''s
    easy to change the punctuation that separates columns and rows. The specific combination
    of punctuation is called the CSV dialect.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The Logical Layout of the data in the file** : This is the sequence of data
    columns that are present. There are several common cases for handling the logical
    layout in CSV files:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The file has one line of headings. This is ideal, and fits nicely with the way
    the CSV module works. The best headings are proper Python variable names.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The file has no headings, but the column positions are fixed. In this case,
    we can impose headings on the file when we open it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the file has no headings and the column positions aren't fixed, this is generally
    a serious problem. It can't easily be solved. Additional schema information is
    required; a separate list of column definitions, for example, can make the file
    useable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The file has multiple lines of headings. In this case, we have to write special
    processing to skip past these lines. We will also have to replace complex headings
    with something more useful in Python.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An even more difficult case is where the file is not in proper **First Normal
    Form** ( **1NF** ). In 1NF, each row is independent of all other rows. When a
    file is not in this normal form, we'll need to add a generator function to rearrange
    the data into 1NF. See the *Slicing and dicing a list* recipe in [Chapter 4](text00048.html#page
    "Chapter 4. Built-in Data Structures – list, set, dict") , *Built-in Data Structures
    – list, set, dict* , and *Using stacked generator expressions* recipe in [Chapter
    8](text00088.html#page "Chapter 8. Functional and Reactive Programming Features")
    , *Functional And Reactive Programming Features* for other recipes that work on
    normalizing data structures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We''ll look at a relatively simple CSV file that has some real-time data recorded
    from the log of a sailboat. This is the `waypoints.csv` file. The data looks as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: This data has four columns that need to be reformatted to create more useful
    information.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Import the `csv` module and the `Path` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'From `pathlib` import `PathExamine` from the data to confirm the following
    features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The column separator characters: `'',''` are the default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The row separator characters: `''\r\n''` are widely used in both Windows and
    Linux. This may be a feature of Excel, but it''s quite common. Python''s universal
    newlines feature means that the Linux standard `''\n''` will work just as well
    as a row separator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The presence of a single-row heading. If not present, this information can be
    provided separately.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Create a `Path` object that identifies the file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `Path` object to open the file in a `with` statement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: For more information on the with statement, see the *Reading and writing files
    with context managers* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the CSV reader from the open file object. This is indented inside the
    `with` statement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Read (and process) the various rows of data. This is properly indented inside
    the `with` statement. For this example, we''ll just print them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is a series of dictionaries that looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the row was transformed into a dictionary, the column keys are not in
    the original order. If we use `pprint()` from the `pprint` module the keys tend
    to get sorted into alphabetical order. We can now process the data by referring
    to `row[''date'']` . Using the column names is more descriptive than referring
    to the column by position: `row[0]` is hard to understand.'
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `csv` module handles physical format work of separating the rows from each
    other, and separating the columns within each row. The default rules assure that
    each input line is treated as a separate row, and the columns are separated by
    `","` .
  prefs: []
  type: TYPE_NORMAL
- en: 'What happens when we need to use the column separator character as part of
    data? We might have data like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The `notes` column has data in the first row which includes the `","` column
    separator character. The rules for CSV allow a column's value to be surrounded
    by quotes. By default, the quoting characters are `"` . Within these quoting characters,
    the column and row separator characters are ignored.
  prefs: []
  type: TYPE_NORMAL
- en: In order to embed the quote character within a quoted string, it is doubled.
    The second example row shows how the value `"blowing "like stink""` is encoded
    by doubling the quote characters when they are used inside a quoted column. These
    quoting rules mean that a CSV file can represent any combination of characters,
    including the row and column separator characters.
  prefs: []
  type: TYPE_NORMAL
- en: The values in a CSV file are always strings. A string value like `7331` may
    look like a number to us, but it's merely text when processed by the `csv` module.
    This makes the processing simple and uniform, but it can be awkward for a human
    user.
  prefs: []
  type: TYPE_NORMAL
- en: Some CSV data is exported from software such as databases or web servers. This
    data tends to be the easiest to work with because the various rows tend to be
    organized consistently.
  prefs: []
  type: TYPE_NORMAL
- en: When data is saved from a manually prepared spreadsheet, the data may reveal
    quirks of the desktop software's internal rules for data display. It's surprisingly
    common, for example, to have a column of data that is displayed as a date on the
    desktop software, but shows up as a simple floating-point number in the CSV file.
  prefs: []
  type: TYPE_NORMAL
- en: There are two solutions to the date-as-number problem. One is to add a column
    in the source spreadsheet to properly format the date as a string. Ideally, this
    is done using ISO rules so that the date is represented in YYYY-MM-DD format.
    The other solution is to recognize the spreadsheet date as a number of seconds
    past some epochal date. The epochal dates vary slightly, but they're generally
    either Jan 1, 1900 or Jan 1, 1904.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we saw in the *Combining map and reduce transformations* recipe, there's
    often a pipeline of processing that includes cleansing and transformation of the
    source data. In this specific example, there are no extra rows that need to be
    eliminated. However, each column needs to be converted into something more useful.
  prefs: []
  type: TYPE_NORMAL
- en: 'To transform the data into a more useful form, we''ll use a two-part design.
    First, we''ll define a row-level cleansing function. In this case, we''ll update
    the row-level dictionary object by adding additional column-like values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: We've created new column values, `lat_n` and `lon_n` , which have proper floating-point
    values instead of strings. We've also parsed the date and time values to create
    `datetime.date` and `datetime.time` objects. We've also combined the date and
    time into a single, useful value, which is the value of the `timestamp` column.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have a row-level function for cleaning and enriching our data, we can
    map this function to each row in the source of data. We can use `map(clean_row,
    reader)` or we can write a function that embodies this processing loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'This can be used to provide more useful data from each row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: We've injected the `cleanse()` function to create a very small stack of transformation
    rules. The stack starts with the `data_reader` , and only has one other item in
    it. This is a good beginning. As the application software is expanded to do more
    computations, the stack will expand.
  prefs: []
  type: TYPE_NORMAL
- en: 'These cleansed and enriched rows look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: We've added columns such as `lat_n` and `lon_n` , which have proper numeric
    values instead of strings. We've also added `timestamp` , which has a full date-time
    value that can be used for simple computations of elapsed time between waypoints.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: See the *Combining map and reduce transformations* recipe for more information
    on the idea of a processing pipeline or stack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See the *Slicing and dicing a list* recipe in [Chapter 4](text00048.html#page
    "Chapter 4. Built-in Data Structures – list, set, dict") , *Built-in Data Structures
    – list, set, dict* and *Using stacked generator expressions* recipe in [Chapter
    8](text00088.html#page "Chapter 8. Functional and Reactive Programming Features")
    , *Functional And Reactive Programming Features* for more information on processing
    a CSV file that isn't in a proper 1NF
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading complex formats using regular expressions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many file formats that lack the elegant regularity of a CSV file.
    One common file format that's rather difficult to parse is a web server log file.
    These files tend to have complex data without a single separator character or
    consistent quoting rules.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we looked at a simplified log file in the *Writing generator functions
    with the yield statement* recipe in  [Chapter 8](text00088.html#page "Chapter 8. Functional
    and Reactive Programming Features") , *Functional And Reactive Programming Features*
    , we saw that the rows look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: There are a variety of punctuation marks used in this file. The `csv` module
    can't handle this complexity.
  prefs: []
  type: TYPE_NORMAL
- en: How can we process this kind of data with the elegant simplicity of a CSV file?
    Can we transform these irregular rows to a more regular data structure?
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Parsing a file with a complex structure generally involves writing a function
    that behaves somewhat like the `reader()` function in the `csv` module. In some
    cases, it's slightly easier to create a small class that behaves like the `DictReader`
    class.
  prefs: []
  type: TYPE_NORMAL
- en: The core feature of the reader is a function that will transform one line of
    text into a dict or tuple of individual field values. This job can often be done
    by the `re` package.
  prefs: []
  type: TYPE_NORMAL
- en: Before we can start, we'll need to develop (and debug) the regular expression
    that properly parses each line of the input file. For more information on this,
    see the *String parsing with regular expressions* recipe in [Chapter 1](text00014.html#page
    "Chapter 1. Numbers, Strings, and Tuples") , *Numbers, Strings, and Tuples* .
  prefs: []
  type: TYPE_NORMAL
- en: 'For this example, we''ll use the following code. We''ll define a pattern string
    with a series of regular expressions for the various elements of the line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: The date-time stamp is various kinds of digits, hyphens, colons, and a comma;
    it's surrounded by `[` and `]` . We've had to use `\[` and `\]` to escape the
    normal meaning of `[` and `]` in a regular expression. The date stamp is followed
    by a severity level, which is a single run of characters. The characters `in`
    can be ignored; there are no `()` 's to capture the matching data. The module
    name is a sequence of letter characters, summarized by the character class `\w`
    , and also including `_` and `.` . There's an extra `:` character after the module
    name that can also be ignored. Finally, there's a message that extends to the
    end of the line. We've wrapped the interesting data strings in `()` to capture
    each of these as part of the regular expression processing.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we've also included the `\s+` sequence to quietly skip any number
    of space-like characters. It appears that the sample data all use a single space
    as the separator. However, when absorbing whitespace, using `\s+` seems to be
    a slightly more general approach because it permits extra spaces.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s how this pattern works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: We've provided a line of sample data. The match object, `match` , has a `groups()`
    method that returns each of the interesting fields. We can make this into a dictionary
    with named fields by using `(?P<name>...)` for each capture instead of simply
    `(...)` .
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe has two parts-defining a parse function for a single line, and using
    the parse function for each line of input.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the parse function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Perform the following steps for defining the parse function:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the compiled regular expression object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: We've used the `(?P<name>...)` regular expression construct to provide names
    for each group that's captured. The resulting dictionary will be identical with
    the results of `csv.DictReader` .
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a function that accepts a line of text as an argument:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Apply the regular expression to create a match object. We''ve assigned it to
    the `match` variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'If the match object is `None` , the line did not match the pattern. This line
    may be skipped silently. In some applications, it should be logged in some way
    to provide information useful for debugging or enhancing the application. It may
    also make sense to raise an exception for an input line that cannot be parsed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Return a useful data structure with the various pieces of data from this input
    line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: This function can be used to parse each line of input. The text is transformed
    into a dictionary with field names and values.
  prefs: []
  type: TYPE_NORMAL
- en: Using the parse function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Import the `csv` module and the `Path` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'From `pathlib` import `PathCreate` , the `Path` object that identifies the
    file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `Path` object to open the file in a `with` statement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For more information on the `with` statement, see the *Reading and writing files
    with context managers* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the log file parser from the open file object, `data_file` . In this
    case, we''ll use `map()` to apply the parser to each line from the source file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Read (and process) the various rows of data. For this example, we''ll just
    print them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is a series of dictionaries that looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: We can do more meaningful processing on these dictionaries than we can on a
    line of raw text. These allow us to filter the data by severity level, or create
    a `Counter` based on the module providing the message.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This log file is typical of files that are in First Normal Form. The data is
    organized into lines that represent independent entities or events. Each row has
    a consistent number of attributes or columns, and each column has data that is
    atomic or can't be meaningfully decomposed further. Unlike CSV files, the format
    requires a complex regular expression to parse.
  prefs: []
  type: TYPE_NORMAL
- en: In our log file example, the timestamp has a number of individual elements—year,
    month, day, hour, minute, second, and millisecond, but there's little value in
    further decomposing the timestamp. It's more helpful to use it as a single `datetime`
    object, and derive details (like hour of the day) from this object rather than
    assembling individual fields into a new piece of composite data.
  prefs: []
  type: TYPE_NORMAL
- en: In a complex log processing application, there may be several varieties of message
    fields. It may be necessary to parse these message types using separate patterns.
    When we need to do this, it reveals that the various lines in the log aren't consistent
    in the format and number of attributes, breaking one of the First Normal Form
    assumptions.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of inconsistent data, we'll have to create more sophisticated parsers.
    This may include complex filtering rules to separate out the various kinds of
    information that may appear in a web server log file. It may involve parsing part
    of the line to determine which regular expression must be used to parse the rest
    of the line.
  prefs: []
  type: TYPE_NORMAL
- en: We've relied on using the `map()` higher-order function. This applies the `log_parse()`
    function to each line of the source file. The direct simplicity of this provides
    some assurance that the number of data objects created will precisely match the
    number of lines in the log file.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ve generally followed the design pattern from the *Reading delimited files
    with the cvs module* recipe, so that reading a complex log is nearly identical
    with reading a simple CSV file. Indeed, we can see that the primary difference
    lies in one line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'As compared to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: This parallel construct allows us to reuse analysis functions across many input
    file formats. This allows us to create a library of tools that can be used on
    a number of data sources.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most common operations when reading very complex files is to rewrite
    them into an easier-to-process format. We'll often want to save the data in CSV
    format for later processing.
  prefs: []
  type: TYPE_NORMAL
- en: Some of this is similar to the *Using multiple contexts for reading and writing
    files* recipe, which also shows multiple open contexts. We'll read from one file
    and write to another file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The file writing process looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: The first portion of this script defines a CSV writer for a given file. The
    path for the output file, `target_path` , is based on the input name, `data_path`
    . The suffix changed from the original filename's suffix to `.csv` .
  prefs: []
  type: TYPE_NORMAL
- en: This file is opened with the newline character turned off by the `newline=''`
    option. This allows the `csv.DictWriter` class to insert newline characters appropriate
    for the desired CSV dialect.
  prefs: []
  type: TYPE_NORMAL
- en: A `DictWriter` object is created to write to the given file. A sequence of column
    headings is provided. These must match the keys used to write each row to the
    file. We can see that these headings match the `(?P<name>...)` parts of the regular
    expression that produces the data.
  prefs: []
  type: TYPE_NORMAL
- en: The `writeheader()` method writes the column names as the first line of output.
    This makes reading the file slightly easier because the column names are provided.
    The first row of a CSV file can be a kind of explicit schema definition that shows
    what data is present.
  prefs: []
  type: TYPE_NORMAL
- en: The source file is opened as shown in the preceding recipe. Because of the way
    the `csv` module writers work, we can provide the `reader()` generator function
    to the `writerows()` method of the writer. The `writerows()` method will consume
    all of the data produced by the `reader()` function. This will, in turn, consume
    all of the rows produced by the open file.
  prefs: []
  type: TYPE_NORMAL
- en: We don't need to write any explicit `for` statements to assure that all of the
    input rows are processed. The `writerows()` function makes this guarantee.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output file looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: The file has been transformed from the rather complex input format to a simpler
    CSV format.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Writing generator functions with the yield statement* recipe in [Chapter
    8](text00088.html#page "Chapter 8. Functional and Reactive Programming Features")
    , *Functional And Reactive Programming Features* shows other processing of this
    log format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the *Reading delimited files with the CSV module* recipe, we look at other
    applications of this general design pattern
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the *Upgrading CSV from Dictreader to namedtuple reader* and *Upgrading CSV
    from Dictreader to namespace reader* recipes we'll look at even more sophisticated
    processing techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading JSON documents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The JSON notation for serializing data is very popular. For details, see [http://json.org](http://json.org)
    . Python includes the `json` module to serialize and deserialize data in this
    notation.
  prefs: []
  type: TYPE_NORMAL
- en: JSON documents are used widely by JavaScript applications. It's common to exchange
    data between Python-based servers and JavaScript-based clients using documents
    in JSON notation. These two tiers of the application stack communicate via JSON
    documents sent via the HTTP protocol. Interestingly, a data persistence layer
    may also use HTTP protocol and JSON notation.
  prefs: []
  type: TYPE_NORMAL
- en: How do we use the `json` module to parse JSON data in Python?
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We've gathered some sailboat racing results in `race_result.json` . This file
    has information on teams, legs, and the orders in which the various teams finish
    the legs of the race.
  prefs: []
  type: TYPE_NORMAL
- en: In many cases, there are null values when a boat did not start, did not finish,
    or was disqualified from the race. In those cases, the finish position is assigned
    a score of one more than the last position. If there are seven boats, then the
    team is given eight points. This is a hefty penalty.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data has the following schema. There are two fields within the overall
    document:'
  prefs: []
  type: TYPE_NORMAL
- en: '`legs` : Array of strings that show starting port and ending port.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`teams` : Array of objects with details about each team. Within each team object,
    there are several fields of data:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`name` : String team name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`position` : Array of integers and nulls with position. The order of items
    in this array matches the order of items in the legs array.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The data looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: We've only shown the first team. There were a total of seven teams in this particular
    race.
  prefs: []
  type: TYPE_NORMAL
- en: 'The JSON-formatted data looks like a Python dictionary that contains lists
    within it. This overlap between Python syntax and JSON syntax can be thought of
    as a happy coincidence: it makes it easier to visualize the Python data structure
    that will be built from the JSON source document.'
  prefs: []
  type: TYPE_NORMAL
- en: Not all JSON structures are simply Python objects. Interestingly, the JSON document
    has a null item, which maps to Python's `None` object. The meaning is similar,
    but the syntax is different.
  prefs: []
  type: TYPE_NORMAL
- en: Also, one of the strings contains a Unicode escape sequence, `\u00cd` , instead
    of the actual Unicode character, Í. This is a common technique used to encode
    characters beyond the 128 ASCII characters.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Import the `json` module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a `Path` object that identifies the file to be processed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: The `json` module doesn't currently work directly with `Path` objects. Consequently,
    we'll read the content as a big block of text and process that text object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a Python object by parsing the JSON document:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: We've used `source_path.read_text()` to read the file named by the `Path` .
    We provided this string to the `json.loads()` function for parsing.
  prefs: []
  type: TYPE_NORMAL
- en: Once we've parsed the document to create a Python dictionary, we can see the
    various pieces. For example, the field `teams` has all of the results for each
    team. It's an array, and item 0 in that array is the first team.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data for each team will be a dictionary with two keys: `name` and `position`
    . We can combine the various keys to get the name of the first team:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'We can look inside the `legs` field to see the names of each leg of the race:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: Note that the JSON source file included a `'\u00cd'` Unicode escape sequence.
    This was parsed properly and the Unicode output shows the proper Í character.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A JSON document is a data structure in JavaScript Object Notation. JavaScript
    programs can parse the document trivially. Other languages must do a little more
    work to translate the JSON to a native data structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'A JSON document contains three kinds of structures:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Objects that map to Python dictionaries** : JSON has a syntax similar to
    Python: `{"key": "value"}` . Unlike Python, JSON only uses `"` for string quotation
    marks. JSON notation is intolerant of an extra , at the end of the dictionary
    value. Other than this, the two notations are similar.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Arrays that map to Python lists** : JSON syntax uses `[item, ...]` , which
    looks like Python. JSON is intolerant of extra , at the end of the array value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Primitive values** : There are five classes of values: string, number, `true`
    , `false` , and `null` . Strings are enclosed in `"` and use a variety of `\escape`
    sequences, which are similar to Python''s. Numbers follow the rules for floating-point
    values. The other three values are simple literals; these parallel Python''s `True`
    , `False` , and `None` literals.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no provision for any other kinds of data. This means that Python programs
    must convert complex Python objects to a simpler representation so that they can
    be serialized in JSON notation.
  prefs: []
  type: TYPE_NORMAL
- en: Conversely, we often apply additional conversions to reconstruct complex Python
    objects from the simplified JSON representation. The `json` module has places
    where we can apply additional processing to the simple structures to create more
    sophisticated Python objects.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A file, generally, contains a single JSON document. The standard doesn't provide
    an easy way to encode multiple documents in a single file. If we want to analyze
    a web log, for example, JSON may not be the best notation for preserving a huge
    volume of information.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two additional problems that we often have to tackle:'
  prefs: []
  type: TYPE_NORMAL
- en: Serializing complex objects so that we can write them to files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deserializing complex objects from the text that's read from a file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we represent a Python object's state as a string of text characters, we've
    serialized the object. Many Python objects need to be saved in a file or transmitted
    to another process. These kinds of transfers require a representation of object
    state. We'll look at serializing and deserializing separately.
  prefs: []
  type: TYPE_NORMAL
- en: Serializing a complex data structure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can also create JSON documents from Python data structures. Because Python
    is extremely sophisticated and flexible, we can easily create Python data structures
    that cannot possibly be represented in JSON.
  prefs: []
  type: TYPE_NORMAL
- en: The serialization to JSON works out the best if we create Python objects that
    are limited to simple `dict` , `list` , `str` , `int` , `float` , `bool` , and
    `None` values. If we're careful, we can build objects that serialize rapidly and
    can be used widely by a number of programs written in different languages.
  prefs: []
  type: TYPE_NORMAL
- en: None of these types of values involve Python `sets` , or other class definitions.
    This means that we're often forced to convert complex Python objects into dictionaries
    to represent them in a JSON document.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, let''s assume we''ve analyzed some data and created a resulting
    `Counter` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: We've dumped the data in JSON notation, with the keys sorted into order. This
    assures consistent output. The indent of two will show each `{}` object and each
    `[]` array indented visually to make it easier to see the document's structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can write this to a file with a relatively simple operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: When we reread this document, we will not get a `Counter` object from the JSON
    load operation. We'll only get a dictionary instance. This is a consequence of
    JSON's reduction to very simple values.
  prefs: []
  type: TYPE_NORMAL
- en: 'One commonly-used data structure that doesn''t serialize easily is a `datetime.datetime`
    object. Here''s what happens when we try:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: We've created a simple document that has a single field. The value of the field
    is a `datetime` instance. What happens when we try to serialize this in JSON?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: This shows that objects that cannot be serialized will raise a `TypeError` exception.
    Avoiding this exception can done in one of two ways. We can either convert the
    data before building the document, or we can add a hook to the JSON serialization
    process.
  prefs: []
  type: TYPE_NORMAL
- en: 'One technique is to convert the `datetime` object into a string prior to serializing
    it as JSON:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: This uses the ISO format for dates to create a string that can be serialized.
    An application that reads this data can then convert the string back into a `datetime`
    object.
  prefs: []
  type: TYPE_NORMAL
- en: 'The other technique for serializing complex data is to provide a default function
    that''s used automatically during serialization. This function must convert a
    complex object to something that can be safely serialized. Often it will create
    a simple dictionary with string and numeric values. It might also create a simple
    string value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: We've defined a function, `default_date()` , which will apply special conversion
    rules to `datetime` objects. These will be massaged into string objects that can
    be serialized by the `json.dumps()` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'We provide this function to the `dumps()` function using the `default` parameter,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: In any given application, we'll need to expand this function to handle any of
    the more complex Python objects that we might want to serialize in JSON notation.
    If there are a large number of very complex data structures, we often want a somewhat
    more general solution than meticulously converting each object to something serializable.
    There are a number of design patterns for including type information along with
    serialized details of an object's state.
  prefs: []
  type: TYPE_NORMAL
- en: Deserializing a complex data structure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When deserializing JSON to create Python objects, there's another hook that
    can be used to convert data from a JSON dictionary into a more complex Python
    object. This is called the `object_hook` and it is used during `json.loads()`
    processing to examine each complex object to see if something else should be created
    from that dict.
  prefs: []
  type: TYPE_NORMAL
- en: 'The function we provide will either create a more complex Python object, or
    it will simply leave the dict alone:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: This function will check each object that's decoded to see if the object has
    a field named `date` . If it does, the value of the entire object is replaced
    with a `datetime` object.
  prefs: []
  type: TYPE_NORMAL
- en: 'We provide a function to the `json.loads()` function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: This parses a very small JSON document that meets the criteria for containing
    a date. The resulting Python object is built from the string value found in the
    JSON serialization.
  prefs: []
  type: TYPE_NORMAL
- en: In a larger context, this particular example of handling dates isn't ideal.
    The presence of a single `'date'` field to indicate a date object could lead to
    problems with more complex objects being de-serialized using this `as_date()`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: A more general approach would either look for something unique and non-Python
    like, such as `'$date'` . An additional feature would confirm that the special
    indicator was the only key for the object. When these two criteria were met, then
    the object could be processed specially.
  prefs: []
  type: TYPE_NORMAL
- en: We may also want to design our application classes to provide additional methods
    to help with serialization. A class might include a `to_json()` method that will
    serialize the objects in a uniform way. This method might provide class information.
    It can avoid serializing any derived attributes or computed properties. Similarly,
    we might need to provide a static `from_json()` method that can be used to determine
    if a given dictionary object is actually an instance of the given class.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Reading HTML documents* recipe will show how we prepared this data from
    an HTML source
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading XML documents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The XML markup language is widely used to organize data. For details, see [http://www.w3.org/TR/REC-xml/](http://www.w3.org/TR/REC-xml/)
    . Python includes a number of libraries for parsing XML documents.
  prefs: []
  type: TYPE_NORMAL
- en: XML is called a markup language because the content of interest is marked with
    `<tag>` and `</tag>` constructs that define the structure of the data. The overall
    file includes the content plus the XML markup text.
  prefs: []
  type: TYPE_NORMAL
- en: Because the markup is intermingled with our text, there are some additional
    syntax rules that must be used. In order to include the `<` character in our data,
    we'll use XML character entity references to avoid confusion. We use `&lt;` to
    be able to include `<` in our text. Similarly, `&gt;` is used instead of `>` ,
    `&amp;` is used instead of `&` , and `&quot;` is also used to embed a `"` in an
    attribute value.
  prefs: []
  type: TYPE_NORMAL
- en: 'A document, then, will have items as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'Most XML processing allows additional `\n` and space characters in the XML
    to make the structure more obvious:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: In general, content is surrounded by the tags. The overall document forms a
    large, nested collection of containers. Viewed another way, the document forms
    a tree with a root tag that contains all of the other tags and their embedded
    content. Between tags, there is additional content entirely whitespace in this
    example that will be ignored.
  prefs: []
  type: TYPE_NORMAL
- en: It's very, very difficult to parse this with regular expressions. We need more
    sophisticated parsers to handle the nested syntax.
  prefs: []
  type: TYPE_NORMAL
- en: There are two binary libraries that are available for parsing XML-SAX and Expat.
    Python includes `xml.sax` and `xml.parsers.expat` to exploit these two modules.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to these, there's a very sophisticated set of tools in the `xml.etree`
    package. We'll focus on using the `ElementTree` module to parse and analyze XML
    documents.
  prefs: []
  type: TYPE_NORMAL
- en: How do we use the `xml.etree` module to parse XML data in Python?
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We've gathered some sailboat racing results in `race_result.xml` . This file
    has information on teams, legs, and the orders in which the various teams finished
    each leg.
  prefs: []
  type: TYPE_NORMAL
- en: In many cases, there are empty values when a boat did not start, did not finish,
    or was disqualified from the race. In those cases, the score will be one more
    than the number of boats. If there are seven boats, then the team is given eight
    points. This is a hefty penalty.
  prefs: []
  type: TYPE_NORMAL
- en: 'The root tag is the `<results>` document. This has the following schema:'
  prefs: []
  type: TYPE_NORMAL
- en: The `<legs>` tag contains individual `<leg>` tags that name each leg of the
    race. The leg names contain both a starting port and an ending port in the text.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `<teams>` tag contains a number of `<team>` tags with details of each team.
    Each team has data structured with internal tags:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `<name>` tag contains the team name.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `<position>` tag contains a number of `<leg>` tags with the finish position
    for the given leg. Each leg is numbered and the numbering matches the leg definitions
    in the `<legs>` tag.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The data looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: We've only shown the first team. There were a total of seven teams in this particular
    race.
  prefs: []
  type: TYPE_NORMAL
- en: In XML notation, the application data shows up in two kinds of places. Between
    tags; for example, `<name>Abu Dhabi Ocean Racing</name>` . The tag is `<name>`
    , the text between `<name>` and `</name>` is the value of this tag.
  prefs: []
  type: TYPE_NORMAL
- en: Also, data shows up as an attribute of a tag. For example, in `<leg n="1">`
    . The tag is `<leg>` ; the tag has an attribute, `n` , with a value of `1` . A
    tag can have an indefinite number of attributes.
  prefs: []
  type: TYPE_NORMAL
- en: The `<leg>` tags include the leg number given as an attribute, `n` , and the
    position in the leg given as the text inside the tag. The general approach is
    to put important data inside the tags, and supplemental, or clarifying data in
    the attributes. The line between the two is very blurry.
  prefs: []
  type: TYPE_NORMAL
- en: 'XML permits a **mixed content model** . This reflects the case where XML is
    mixed in with text, there will be text inside and outside XML tags. Here''s an
    example of mixed content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: Some of the text is inside the `<p>` tag, and some of the text is inside the
    `<strong>` tag. The content of the `<p>` tag is a mixture of text and tags with
    more text.
  prefs: []
  type: TYPE_NORMAL
- en: We'll use the `xml.etree` module to parse the data. This involves reading the
    data from a file and providing it to the parser. The resulting document will be
    rather complex.
  prefs: []
  type: TYPE_NORMAL
- en: We have not provided a formal schema definition for our sample data, nor have
    we provided a **Document Type Definition** ( **DTD** ). This means that the XML
    defaults to mixed content mode. Furthermore, the XML structure can't be validated
    against the schema or DTD.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ll need two modules—`xml.etree` and `pathlib` :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: We've changed the `ElementTree` module name name to `XML` to make it slightly
    easier to type. It's also common to rename this to something like `ET` .
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a `Path` object that locates the source document:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the internal `ElementTree` version of the document by parsing the source
    file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: The XML parser doesn't readily work with `Path` objects. We've elected to read
    the text from the `Path` object and then parse that text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have the document, we can then search it for the relevant pieces of
    data. In this example, we''ll use the `find()` method to locate the first instance
    of a given tag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: In this case, we located the `<teams>` tag, and then found the first instance
    of the `<team>` tag inside that list. Within the `<team>` tag, we located the
    first `<name>` tag to get the value of the team's name.
  prefs: []
  type: TYPE_NORMAL
- en: Because XML is a mixed content model, all of the `\n` , `\t` , and space characters
    in the content are perfectly preserved in the data. We rarely want any of this
    whitespace, and it makes sense to use the `strip()` method to remove all extraneous
    characters before and after the meaningful content.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The XML parser modules transform XML documents into fairly complex objects based
    on the document object model. In the case of the `etree` module, the document
    will be built from `Element` objects that generally represent tags and text.
  prefs: []
  type: TYPE_NORMAL
- en: XML also includes processing instructions and comments. These are commonly ignored
    by many XML processing applications.
  prefs: []
  type: TYPE_NORMAL
- en: Parsers for XML often have two levels of operation. At the bottom level, they
    recognize events. The events that are found by the parser include element starts,
    element ends, comment starts, comment ends, runs of text, and similar lexical
    objects. At the higher level, the events are used to build the various `Elements`
    of the document.
  prefs: []
  type: TYPE_NORMAL
- en: Each `Element` instance has a tag, text, attributes, and a tail. The tag is
    the name inside the `<tag>` . The attributes are the fields that follow the tag
    name. For example, the `<leg n="1">` tag has a tag name of `leg` and an attribute
    named `n` . Values are always strings in XML.
  prefs: []
  type: TYPE_NORMAL
- en: The text is contained between the start and end of a tag. Therefore, a tag such
    as `<name>Team SCA</name>` has `"Team SCA"` for the value of the `text` attribute
    of the `Element` that represents the `<name>` tag.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that a tag also has a tail attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: There's a `\n` character after the closing `</name>` tag and before the opening
    of the `<position>` tag. This is the tail of the `<name>` tag. The tail values
    can be important when working with a mixed content model. The tail values are
    generally whitespace when working in a non-mixed content model.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Because we can't trivially translate an XML document to a Python dictionary,
    we need a handy way to search through the document content. The `ElementTree`
    module provides a search technique that's a partial implementation of the **XML
    Path Language** ( **XPath** ) for specifying a location in an XML document. The
    XPath notation gives us considerable flexibility.
  prefs: []
  type: TYPE_NORMAL
- en: 'The XPath queries are used with the `find()` and `findall()` methods. Here''s
    how we can find all of the names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: We've looked for the top-level `<teams>` tags. Within that tag, we want `<team>`
    tags. Within those tags, we want the `<name>` tags. This will search for all instances
    of this nested tag structure.
  prefs: []
  type: TYPE_NORMAL
- en: We can search for attribute values, also. This can make it handy to find how
    all teams did on a particular leg of the race. The data is found in the `<leg>`
    tag within the `<position>` tag for each team.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, each `<leg>` has an attribute value of n that shows which of the
    race legs it represents. Here''s how we can use this to extract specific data
    from the XML document:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: This shows us the finish position of each team on leg 8 of the race. We're looking
    for all tags with `<leg n="8">` and displaying the text within that tag. We have
    to match these values with the team names to see that Team SCA finished first,
    and Dongfeng Race Team finished last on this leg.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Reading HTML documents* recipe shows how we prepared this data from an
    HTML source
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading HTML documents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A great deal of content on the Web is presented using HTML markup. A browser
    renders the data very nicely. How can we parse this data to extract the meaningful
    content from the displayed web page?
  prefs: []
  type: TYPE_NORMAL
- en: We can use the standard library `html.parser` module, but it's not helpful.
    It only provides low-level lexical scanning information, but doesn't provide a
    high-level data structure that describes the original web page.
  prefs: []
  type: TYPE_NORMAL
- en: We'll use the Beautiful Soup module to parse HTML pages. This is available from
    the **Python Package Index** ( **PyPI** ). See [https://pypi.python.org/pypi/beautifulsoup4](https://pypi.python.org/pypi/beautifulsoup4)
    .
  prefs: []
  type: TYPE_NORMAL
- en: This must be downloaded and installed to be useful. Generally, the `pip` command
    does this job very nicely.
  prefs: []
  type: TYPE_NORMAL
- en: 'Often, this is as simple as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'For Mac OS X and Linux users, the `sudo` command is required to escalate the
    user''s privileges:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: This will prompt for the user's password. The user must be able to elevate themselves
    to have root privileges.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the rare case that you have multiple versions of Python, be sure to use
    the matching version of pip. In some cases, we might have to use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: Use the `pip` that goes with Python 3.5.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We've gathered some sailboat racing results in `Volvo Ocean Race.html` . This
    file has information on teams, legs, and the order in which the various teams
    finished each leg. It's scraped from the Volvo Ocean Race website, and it looks
    wonderful when opened in a browser.
  prefs: []
  type: TYPE_NORMAL
- en: HTML notation is very similar to XML. The content is surrounded by `<tag>` marks
    that show the structure and presentation of the data. HTML predates XML, and the
    XHTML standard reconciles the two Browsers; however, must be tolerant of older
    HTML and even improperly structured HTML. The presence of damaged HTML can make
    it difficult to analyze data from the World Wide Web.
  prefs: []
  type: TYPE_NORMAL
- en: 'HTML pages include a great deal of overhead. There are often vast code and
    style sheet sections, as well as invisible metadata. The content may be surrounded
    by advertising and other information. Generally, an HTML page has the following
    overall structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: Within the `<head>` tag there will be links to JavaScript libraries, and links
    to **Cascading Style Sheet** ( **CSS** ) documents. These are generally used to
    provide interactive features and define the presentation of the content.
  prefs: []
  type: TYPE_NORMAL
- en: The bulk of the content is in the `<body>` tag. Many web pages are very busy
    and provide a tremendously complex mix of content. The design of web pages is
    a sophisticated art, and the content is designed to look good on most browsers.
    It can be difficult to track down the relevant data on a web page, because the
    focus is on how people see it more than how automated tools can process it.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the race results are in an HTML `<table>` tag, making them easy
    to find. What we see is the following overall structure to the relevant content
    in the page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: The `<thead>` tag includes the column titles for the table. There's a single
    table row tag, `<tr>` , with table heading, `<th>` , tags that include the content.
    The content has two parts; the essential display is a number for each leg of the
    race. This is the content of the tag. In addition to the displayed content, there's
    also an attribute value that's used by a JavaScript function. This attribute value
    is displayed when the cursor hovers over a column heading. The JavaScript function
    pops up the leg name.
  prefs: []
  type: TYPE_NORMAL
- en: The `<tbody>` tag includes the team name and the results for each race. The
    table row (`<tr>` ) contains the details for each team. The team name (and graphic
    and overall finish rank) is shown in the first three columns of table data, `<td>`
    . The remaining columns of table data contain the finish position for a given
    leg of the race.
  prefs: []
  type: TYPE_NORMAL
- en: Because of the relative complexity of sailboat racing, there are additional
    notes in some of the table data cells. These are included as attributes that are
    used to provide supplemental data on the reason for the cell's value. In some
    cases, teams did not start a leg, or did not finish a leg, or retired from a leg.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a typical `<tr>` row from the HTML:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: The `<tr>` tag has a class attribute that defines the style for this row. The
    CSS provides the style rules for this class of data. The `class` attribute on
    this tag helps our data gathering application locate the relevant content.
  prefs: []
  type: TYPE_NORMAL
- en: The `<td>` tags also have class attributes that define the style for the individual
    cells of data. In this case, class information clarifies what the content of the
    cell means.
  prefs: []
  type: TYPE_NORMAL
- en: One of the cells has no content. That cell has an attribute of `data-title`
    . This is used by a JavaScript function to display additional information in the
    cell.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ll need two modules: bs4 and pathlib:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: We've only imported the `BeautifulSoup` class from the `bs4` module. This class
    will provide all of the features required to parse and analyze HTML documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a `Path` object that names the source document:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the soup structure from the HTML content. We''ll assign it to a variable,
    `soup` :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: We've used a context manager to access the file. As an alternative we could
    simply read the content with `source_path.read_text(encodig='utf8')` . This works
    as well as providing an open file to the `BeautifulSoup` class.
  prefs: []
  type: TYPE_NORMAL
- en: 'The soup structure in the variable `soup` can then be processed to locate the
    various pieces of content. For example, we can extract the leg details as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: The expression `soup.table.thead.tr` will find the first `<table>` tag. Within
    that, the first `<thead>` tag; and within that, the first `<tr>` tag. We assigned
    this `<tr>` tag to a variable named, perhaps misleadingly, `thead` . We can then
    do a `findall()` to locate all `<th>` tags within this container.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll check each tag''s attributes to locate the `data-title` attribute values.
    This will have the leg name information. The leg name content looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: The `data-title` attribute value includes some additional HTML markup within
    the value. This is not a standard part of HTML and the `BeautifulSoup` parser
    doesn't look for this HTML within an attribute value.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have a small bit of HTML to parse, so we can create a small `soup` object
    just to parse that piece of text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: We create a small `BeautifulSoup` object from just the value of the `data-title`
    attribute. This soup will have information about the tag, `<strong>` , and the
    text. We used the text attribute to get all of the text without any tag information.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `BeautifulSoup` class transforms HTML documents into fairly complex objects
    based on a **document object model** ( **DOM** ). The resulting structure will
    be built from instances of the `Tag` , `NavigableString` , and `Comment` classes.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, we're interested in the tags that contain the string content of the
    web page. These are objects of the `Tag` and `NavigableString` classes.
  prefs: []
  type: TYPE_NORMAL
- en: Each `Tag` instance has a name, string, and attributes. The name is the word
    inside the `<` and `>` . The attributes are the fields that follow the tag name.
    For example, `<td class="ranking-number">1</td>` has a tag name of `td` and an
    attribute named `class` . Values are often strings, but in a few cases, the value
    can be a list of strings. The string attribute of the `Tag` object is the content
    enclosed by the tag; in this case, it's a very short string, `1` .
  prefs: []
  type: TYPE_NORMAL
- en: HTML is a mixed content model. This means that a tag can contain child tags
    in addition to navigable text. The text is mixed, it can be inside as well as
    outside any of the child tags. When looking at the children of a given tag, there
    will be a sequence of tags and text freely intermixed.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the most common features of HTML are small blocks of navigable text
    that contain only newline characters. When we have a soup like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'There are three children within the `<tr>` tag. Here''s a display of the children
    of this tag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: The two newline characters are peers to the `<td>` tag, and are preserved by
    the parser. This is navigable text that surrounds the child tag.
  prefs: []
  type: TYPE_NORMAL
- en: The `BeautifulSoup` parser depends on another, lower-level process. The lower-level
    process can be the built-in `html.parser` module. There are alternatives that
    can be installed, also. The `html.parser` is easiest to use and covers the most
    common use cases. There are alternatives available, the Beautiful Soup documentation
    lists the other low-level parsers that can be used to solve particular web parsing
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: The lower-level parser recognizes events; these include element starts, element
    ends, comment starts, comment ends, runs of text, and similar lexical objects.
    At the higher level, the events are used to build the various objects of the Beautiful
    Soup document.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `Tag` objects of Beautiful Soup represent the hierarchy of the document''s
    structure. There are several kinds of navigation among tags:'
  prefs: []
  type: TYPE_NORMAL
- en: All tags except a special root `[document]` container will have a parent. The
    top `<html>` tag will often be the only child of the root document container.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `parents` attribute is a generator for all parents of a tag. It's a path
    through the hierarchy to a given tag.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All `Tag` objects can have children. A few tags such as `<img/>` and `<hr/>`
    have no children. The `children` attribute is a generator that yields the children
    of a tag.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A tag with children may have multiple levels of tags under it. The overall `<html>`
    tag, for example, has the entire document as descendants. The `children` attribute
    has the immediate children; the `descendants` attribute generates all children
    of children.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A tag can also have siblings, which are other tags within the same container.
    Since the tags have a defined order, there's a `next_sibling` and `previous_sibling`
    attribute to help step through the peers of a tag.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In some cases, a document will have a generally straight-forward organization
    and a simple search by the `id` attribute or `class` attribute will find the relevant
    data. Here''s a typical search for a given structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: Note that we have to use `class_` in our Python query to search for the attribute
    named `class` . Given the overall document, we're searching for any `<table class="ranking-list">`
    tag. This will find the first such table in a web page. Since we know there will
    only be one of these, this attribute-based search helps distinguish between any
    other tabular data on a web page.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the parents of this `<table>` tag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: We've displayed just the tag name for each parent above the given `<table>`
    . Note that there are four nested `<div>` tags that wrap the `<section>` that
    contains the `<table>` . Each of these `<div>` tags likely has a different class
    attribute to properly define the content and the style for the content.
  prefs: []
  type: TYPE_NORMAL
- en: The `[document]` is the overall `BeautifulSoup` container that holds the various
    tags that were parsed. This is displayed distinctively to emphasize that it's
    not a real tag, but a container for the top-level `<html>` tag.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Reading JSON documents* and *Reading XML documents* recipes both use similar
    data. The example data was created for them by scraping the HTML page using these
    techniques.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Upgrading CSV from DictReader to namedtuple reader
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we read data from a CSV format file, we have two general choices for the
    resulting data structure:'
  prefs: []
  type: TYPE_NORMAL
- en: When we use `csv.reader()` , each row becomes a simple list of column values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we use `csv.DictReader` , each row becomes a dictionary. By default, the
    contents of the first row become the keys for the row dictionary. The alternative
    is to provide a list of values that will be used as the keys.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In both cases, referring to data within the row is awkward because it involves
    rather complex-looking syntax. When we use a `csv` reader, we must use `row[2]`
    : the semantics of this are completely obscure. When we use a `DictReader` , we
    can use `row[''date'']` , which is less obscure, but is still a lot of typing.'
  prefs: []
  type: TYPE_NORMAL
- en: In some real-world spreadsheets the column names are impossibly long strings.
    It's hard to work with `row['Total of all locations excluding franchisees']` .
  prefs: []
  type: TYPE_NORMAL
- en: What can we do to replace complex syntax with something simpler?
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One way to improve the readability of programs that work with spreadsheets is
    to replace a list of columns with a `namedtuple` object. This provides easy-to-use
    names defined by the `namedtuple` instead of the possibly haphazard column names
    in the `.csv` file.
  prefs: []
  type: TYPE_NORMAL
- en: More importantly, it permits much nicer syntax for referring to the various
    columns. In addition to `row[0]` , we can also use `row.date` to refer to a column
    named `date` .
  prefs: []
  type: TYPE_NORMAL
- en: The column names (and the data types for each column) are part of the schema
    for a given file of data. In some CSV files the first line of the column titles
    is a schema for the file. This schema is limited, it provides only attribute names;
    the data types aren't known and have to be treated as strings.
  prefs: []
  type: TYPE_NORMAL
- en: 'This points to two reasons for imposing an external schema on the rows of a
    spreadsheet:'
  prefs: []
  type: TYPE_NORMAL
- en: We can supply meaningful names
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can perform data conversions where necessary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We''ll look at a relatively simple CSV file that has some real-time data recorded
    from the log of a sailboat. This is the `waypoints.csv` file, and the data looks
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: The data has four columns. Two of the columns are the latitude and longitude
    of the waypoint. It has a column with the date and the time as separate values.
    This isn't ideal, and we'll look at various data cleansing steps separately.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the column titles happen to be valid Python variable names. This
    is rare, but it can lead to a slight simplification. We'll look at the alternatives
    in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: The most important step is to gather the data as `namedtuples` .
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Import the modules and definitions required. In this case, they will be from
    `collections` , `csv` , and `pathlib` :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the `namedtuple` that matches the actual data. In this case, we''ve
    called it `Waypoint` and provided names for the four columns of data. In this
    example, the attributes happen to match the column names; it''s not a requirement
    that the names match:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the `Path` object that refers to the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the processing context for the open file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a CSV reader for the data. We''ll call this a raw reader. In the long
    run, we''ll follow the *Using stacked generator expressions* recipe in [Chapter
    8](text00088.html#page "Chapter 8. Functional and Reactive Programming Features")
    , *Functional And Reactive Programming Features* and *Use a stack of generator
    expressions* recipe in [Chapter 8](text00088.html#page "Chapter 8. Functional
    and Reactive Programming Features") , *Functional And Reactive Programming Features*
    to cleanse and filter the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a generator that builds `Waypoint` objects from tuples of input data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now process rows using the `waypoints_reader` generator expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: The `waypoints_reader` object will also provide the heading row, which we want
    to ignore. We'll look at filtering and conversion in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: The expression `(Waypoint(*row) for row in raw_reader)` expands each value of
    the `row` tuple to be a positional argument value for the `Waypoint` function.
    This works because the column order in the CSV file matches the column order in
    the `namedtuple` definition.
  prefs: []
  type: TYPE_NORMAL
- en: This construction can also be performed using the `itertools` module, also.
    The `starmap()` function can be used as `starmap(Waypoint, raw_reader)` . This
    will also expand each tuple from the `raw_reader` to be a positional argument
    to the `Waypoint` function. Note that we can't use the built-in `map()` function
    for this. The `map()` function assumes that the function takes a single argument
    value. We don't want each four-item `row` tuple to be used as the only argument
    to the `Waypoint` function. We need to split the four items into four positional
    argument values.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are several parts to this recipe. Firstly, we've used the `csv` module
    for the essential parsing of rows and columns of data. We've leveraged the *Reading
    delimited files with the cvs module* recipe to process the physical format of
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, we've defined a `namedtuple()` that provides a minimal schema for
    our data. This is not very rich or detailed. It provides a sequence of column
    names. It also simplifies the syntax for accessing a particular column.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we've wrapped the `csv` reader in a generator function to build `namedtuple`
    objects for each row. This is a tiny change to the default processing, but it
    leads to a nicer style for the subsequent programming.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of `row[2]` or `row['date']` , we can now use `row.date` to refer to
    a specific column. This is a small change that can simplify the presentation of
    complex algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The initial example of processing the input has two additional problems. Firstly,
    the header row is mixed in with the useful rows of data; this header row needs
    to be rejected by a filter of some kind. Secondly, the data is all strings, and
    some conversion is necessary. We'll solve each of these by extending the recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two common techniques for discarding the unneeded header row:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use an explicit iterator and discard the first item. The general idea
    is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: This snippet shows how to create an iterator object, `waypoints_iter` , from
    the raw CSV reader. We can use the `next()` function to skip a single item from
    this reader. The remaining items can be used to build useful rows of data. We
    can also use the `itertools.islice()` function for this.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can write a generator or use the `filter()` function to exclude selected
    rows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: This example shows how to create filtered generator, `skip_header` , from the
    raw CSV reader. The filter uses a simple expression, `row[0] != 'lat'` , to determine
    if a row is a header or has useful data. Only the useful rows are passed by this
    filter. The header row is rejected.
  prefs: []
  type: TYPE_NORMAL
- en: 'The other thing we''ll need to do is to convert the various data items to more
    useful values. We''ll follow the example of the *Simplifying complex algorithms
    with immutable data structures* recipe in [Chapter 8](text00088.html#page "Chapter 8. Functional
    and Reactive Programming Features") , *Functional And Reactive Programming Features*
    and build a new `namedtuple` from the raw input data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point in most projects, it becomes clear that the original name of
    the `Waypoint namedtuple` was poorly chosen. The code will need to be refactored
    to change the names to clarify the role of the original `Waypoint` tuple. This
    renaming and refactoring will occur several times as the design evolves. It''s
    important to rename things as needed. We won''t do the renaming here: we''ll leave
    it for the reader to redesign the names.'
  prefs: []
  type: TYPE_NORMAL
- en: To do the conversions, we need a function to handle the individual fields of
    a single `Waypoint` . This will create more useful values. It will involve using
    `float()` on the latitude and longitude values. It also requires some careful
    parsing of the date values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the first part of working with the separate date and time. These are
    two lambda objects-small functions with only a single expression that convert
    date or time strings to date or time values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use these to build a new `Waypoint_data` object from the original `Waypoint`
    object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: We've applied a series of functions that build a new data structure from an
    existing data structure. The latitude and longitude values were converted with
    the `float()` function. The date and time values were converted to a `datetime`
    object using the `parse_date` and `parse_time` lambdas with the `combine()` method
    of the `datetime` class.
  prefs: []
  type: TYPE_NORMAL
- en: 'This function allows us to build a more complete stack of processing steps
    for the source data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: The original reader has been supplemented with a filter function to skip the
    header, a generator to create `Waypoint` objects, and another generator to create
    `Waypoint_Data` objects. Within the body of the `for` statement, we have a simple
    and easy-to-use data structure with pleasant names. We can refer to `row.lat`
    instead of `row[0]` or `row['lat']` .
  prefs: []
  type: TYPE_NORMAL
- en: Note that each generator function is lazy, it doesn't fetch any more input than
    is minimally required to produce some output. This stack of generator functions
    uses very little memory and can process files of unlimited size.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Upgrading CSV from dict reader to namespace reader* recipe does this with
    mutable `SimpleNamespace` data structure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Upgrading CSV from a DictReader to a namespace reader
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we read data from a CSV format file, we have two general choices for the
    resulting data structure:'
  prefs: []
  type: TYPE_NORMAL
- en: When we use `csv.reader()` , each row becomes a simple list of column values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we use `csv.DictReader` , each row becomes a dictionary. By default, the
    contents of the first row become the keys for the row dictionary. We can also
    provide a list of values that will be used as the keys.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In both cases, referring to data within the row is awkward because it involves
    rather complex-looking syntax. When we use a reader, we must use `row[0]` , the
    semantics of this are completely obscure. When we use a `DictReader` , we can
    use `row['date']` , which is less obscure, but is a lot of typing.
  prefs: []
  type: TYPE_NORMAL
- en: In some real-world spreadsheets, the column names are impossibly long strings.
    It's hard to work with `row['Total of all locations excluding franchisees']` .
  prefs: []
  type: TYPE_NORMAL
- en: What can we do to replace complex syntax with something simpler?
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The column names (and the data types for each column) are a schema for our data.
    The column titles are a schema that's embedded in the first row of the CSV data.
    This schema provides only attribute names; the data types aren't known and have
    to be treated as strings.
  prefs: []
  type: TYPE_NORMAL
- en: 'This points up two reasons for imposing an external schema on the rows of a
    spreadsheet:'
  prefs: []
  type: TYPE_NORMAL
- en: We can supply meaningful names.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can perform data conversions where necessary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can also use a schema to define data quality and cleansing processing. This
    can become quite sophisticated (and complicated). We'll limit our use of schema
    to providing column names and data conversions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll look at a relatively simple CSV file that has some real-time data recorded
    from the log of a sailboat. This is the `waypoints.csv` file. The data looks like
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: This spreadsheet has four columns. Two of them are the latitude and longitude
    of the waypoint. It has a column with the date and the time as separate values.
    This isn't ideal, and we'll look at various data cleansing steps separately.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the column titles are valid Python variable names. This leads
    to an important simplification in the processing. In the cases where there are
    no column names, or the column names aren't Python variables, we'll have to apply
    a mapping from column name to preferred attribute name.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Import the modules and definitions required. In this case, it will be from
    `types` , `csv` , and `pathlib` :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: 'Import `csv` and define a `Path` object that refers to the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the processing context for the open file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a CSV reader for the data. We''ll call this a raw reader. In the long
    run, we''ll follow the *Using stacked generator expressions* recipe in [Chapter
    8](text00088.html#page "Chapter 8. Functional and Reactive Programming Features")
    , *Functional And Reactive Programming Features* and use multiple generator expressions
    to cleanse and filter the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a generator that will convert these dictionaries into `SimpleNamespace`
    objects:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: This uses the generic `SimpleNamespace` class. When we need to use a more specific
    class, we can replace the `SimpleNamespace` with an application-specific class
    name. That class `__init__` must use keyword parameters that match the spreadsheet
    column names.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now process rows from this generator expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are several parts to this recipe. Firstly, we've used the `csv` module
    for the essential parsing of rows and columns of data. We've leveraged the *Reading
    delimited files with the cvs module* recipe to process the physical format of
    the data. The idea of the CSV format is to have columns of text that are comma
    separated in each row. There are rules for using quotes to allow the data within
    a column to contain a comma. The rules are all implemented within the `csv` module,
    saving us from writing a parser for this.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, we've wrapped the `csv` reader in a generator function to build a
    `SimpleNamespace` object for each row. This is a tiny extension to the default
    processing, but it leads to a nicer style for the subsequent programming. Instead
    of `row[2]` or `row['date']` , we can now use `row.date` to refer to a specific
    column. This is a small change that can simplify the presentation of complex algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We may have two additional problems to solve. Whether or not these are needed
    depends on the data and the use for the data:'
  prefs: []
  type: TYPE_NORMAL
- en: How do we handle spreadsheet names that aren't proper Python variables?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can we convert data from text to a Python object?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It turns out that both of these needs can be handled elegantly with a function
    that does row by row conversion of data, and also handles any necessary renaming
    of columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: 'This function is in effect the schema definition for the original spreadsheet.
    Each line in this function provides several important pieces of information:'
  prefs: []
  type: TYPE_NORMAL
- en: The attribute name in the `SimpleNamespace`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The conversion from the source data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The source column names that were mapped to the final result
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal is to define any helper or support functions required to be sure that
    each line of the conversion function is similar to the ones shown. Each line of
    this function is complete specification for a result column. As a bonus benefit,
    each line is written in Python notation.
  prefs: []
  type: TYPE_NORMAL
- en: 'This function can replace `SimpleNamespace` in the `ns_reader` statement. All
    of the conversion work is now focused into a single place:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE131]'
  prefs: []
  type: TYPE_PRE
- en: 'This row transformation function relies on a `make_timestamp()` function. This
    function converts two source columns to one resulting `datetime` object. The function
    looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: The `make_timestamp()` function breaks the timestamp creation into three parts.
    The first two parts are so simple that a lambda object was all that was needed.
    These are conversions from text to make `datetime.date` or `datetime.time` objects.
    Each conversion use the `strptime()` method to parse the date or time strings
    and return the appropriate class of object.
  prefs: []
  type: TYPE_NORMAL
- en: The third part could also have been a lambda, since it's also a single expression.
    However, it's a long expression, and it seemed slightly more clear to wrap it
    as a `def` statement. This expression uses the `combine()` method of `datetime`
    to combine a date and time into a single object.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Upgrading CSV from dict reader to namedtuple reader* recipe does this with
    an immutable `namedtuple` data structure instead of a `SimpleNamespace`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using multiple contexts for reading and writing files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It's common to need to convert data from one format to another. For example,
    we might have a complex web log that we'd like to convert to a simpler format.
  prefs: []
  type: TYPE_NORMAL
- en: See the *Reading complex formats using regular expressions* recipe for a complex
    web log format. We'd like to do this parsing just one time.
  prefs: []
  type: TYPE_NORMAL
- en: After that, we'd like to work with a simpler file format, more like the format
    shown in the *Upgrading CSV from dict reader to namedtuple reader* or *Upgrading
    CSV from dict reader to namespace reader* recipe. A file that's in CSV notation
    can be read and parsed with the `csv` module, simplifying the physical format
    considerations.
  prefs: []
  type: TYPE_NORMAL
- en: How can we convert from one format to another?
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Converting a file of data from one format to another means that the program
    will need to have two open contexts: one for reading and one for writing. Python
    makes this easy. The use of `with` statement contexts assures that the files are
    properly closed and all of the related OS resources are completely released.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll look at a common problem of summarizing many web log files. The source
    is in a format that we''ve seen in the *Writing generator functions with the yield
    statement* recipe in [Chapter 8](text00088.html#page "Chapter 8. Functional and
    Reactive Programming Features") , *Functional And Reactive Programming Features*
    and also *Reading complex formats using regular expressions* recipe in this chapter.
    The rows look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE133]'
  prefs: []
  type: TYPE_PRE
- en: These are difficult to process. The regular expression required to parse them
    is complex. For large volumes of data, it's also rather slow.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the regular expression pattern for the various elements of the line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE134]'
  prefs: []
  type: TYPE_PRE
- en: 'There are four parts to this complex regular expression:'
  prefs: []
  type: TYPE_NORMAL
- en: The date-time stamp is surrounded with `[ ]` and has a variety of digits, hyphens,
    colons, and a comma. It will be captured and assigned the name `date` by the `?P<date>`
    prefix on the `()` group.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The severity level, which is a run of characters. This is captured and given
    the name level by the `?P<level>` prefix on the next `()` group.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The module is a sequence of characters including `_` and `.` . It's sandwiched
    between `in` and a `:` . The is assigned the name `module` .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, there's a message that extends to the end of the line. This is assigned
    to the message by the `?P<message>` inside the final `()` .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pattern also includes runs of whitespace, `\s+` , which are not captured
    in any `()` groups. They're quietly ignored.
  prefs: []
  type: TYPE_NORMAL
- en: When we create a `match` object using this regular expression, the `groupdict()`
    method of that `match` object will produce a dictionary with the names and values
    from each line. This matches the way the `csv` reader works. It provides a common
    framework for processing complex data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll use this in a function that iterates through rows of log data. The function
    will apply the regular expression, and yield the group dictionaries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE135]'
  prefs: []
  type: TYPE_PRE
- en: This function looks at each line in the given input file. It applies the regular
    expression to the line. If the line matches, this will capture the relevant fields
    of data. If there is no match, the line didn't follow the expected format; this
    may deserve an error message. There's no useful data to yield, so the `continue`
    statement skips the rest of the body of the `for` statement.
  prefs: []
  type: TYPE_NORMAL
- en: The `yield` statement produces the dictionaries of matches. Each dictionary
    will have the four named fields and the captured data from the log. The data will
    be text only, so additional conversions will have to be applied separately.
  prefs: []
  type: TYPE_NORMAL
- en: We can use the `DictWriter` class from the `csv` module to emit a CSV file with
    these various data elements neatly separated. Once we've created a CSV file, we
    can process the data simply and much more quickly than the raw log rows.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This recipe will need three components:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE136]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s the pattern that matches the simple Flask logs. For other kinds of
    logs, or other formats configured into Flask, a different pattern will be required:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE137]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s the function that yields dictionaries for the matching rows. This applies
    the regular expression pattern. Non-matches are silently skipped. The matches
    will yield a dictionary of item names and their values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE138]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll define the `Path` object for the resulting log summary file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE139]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then open the results context. Because we''re using a `with` statement,
    we''re assured that the file will be properly closed no matter what else happens
    in this script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE140]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we''re writing a CSV file based on a dictionary, we''ll define a `csv.DictWriter`
    . This is indented four spaces inside the `with` statement. We must provide the
    expected keys from the input dictionary. This will define the order for the columns
    in the resulting file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE141]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll define a `Path` object for the source directory with log files. In this
    case, the log files happen to be in the directory with the script. This is rare,
    and using an environment variable might be a lot more useful:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE142]'
  prefs: []
  type: TYPE_PRE
- en: We can imagine using `os.environ.get('LOG_PATH', '/var/log')` as a more general
    solution than a hard-coded path.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll use the `glob()` method of a `Path` object to find all files that match
    the required name:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE143]'
  prefs: []
  type: TYPE_PRE
- en: This, too, could benefit from having the pattern string fetched from an environment
    variable or command-line parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll define a context for reading each source file. This context manager
    will guarantee that the input files are properly closed and the resources released.
    Note that this is indented inside the previous `with` and `for` statements, a
    total of eight spaces. This is particularly important when processing a large
    number of files:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE144]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll use the writer''s `writerows()` method to write all valid rows from
    the `extract_row_iter()` function. This is indented inside both `with` statements,
    as well as the `for` statement. This is the core of the process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE145]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also write a summary. This is indented inside the outer with and `for`
    statements. It summarizes the processing of the preceding with statement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE146]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Python works nicely with multiple context managers. We can easily have deeply-nested
    `with` statements. Each `with` statement can manage a different context object.
  prefs: []
  type: TYPE_NORMAL
- en: Since open files are context objects, it makes the most sense to wrap every
    open file in a `with` statement to be sure that the file is properly closed and
    all OS resources are released from the file.
  prefs: []
  type: TYPE_NORMAL
- en: We've used `Path` objects to represent the filesystem locations. This gives
    us the ability to easily create output names based on input names, or rename the
    files after they've been processed. For more information on this, see the *Using
    pathlib to work with filenames* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: We've used a generator function to combine two operations. Firstly, there's
    a mapping from source text to individual fields. Secondly, there's a filter that
    excludes source text that doesn't match the expected pattern. In many cases, we
    can use the `map()` and `filter()` functions to make this a little more clear.
  prefs: []
  type: TYPE_NORMAL
- en: When using regular expression matching; however, it's not as easy to separate
    the mapping and filter parts of the operation. The regular expression may not
    match some input lines, which becomes a kind of filtering that's bundled in to
    the mapping. Because of this, a generator function works out very nicely.
  prefs: []
  type: TYPE_NORMAL
- en: The `csv` writers have a `writerows()` method. This method accepts an iterator
    as it's parameter value. This makes it easy to provide a generator function to
    the writer. The writer will consume objects as they're produced by the generator.
    Very large files can be handled this way because the entire file isn't read into
    memory, just enough of the file is read to create a complete line of data.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It's often essential to have a summary count of the number of lines of log file
    read from each source, the number of lines discarded because they didn't match,
    and the number of lines finally written to the summary file.
  prefs: []
  type: TYPE_NORMAL
- en: This is challenging when using generators. The generator produces lots of rows
    of data. How can it also produce a summary?
  prefs: []
  type: TYPE_NORMAL
- en: The answer is that we can provide a mutable object as a parameter to the generator.
    The ideal kind of mutable object is an instance of `collections.Counter` . We
    can use this to count events including a valid record, an invalid record, or even
    occurrences of specific data values. The mutable object can be shared by the generator
    and the overall main program so that the main program can print the count information
    to a log.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the map-filter function that converts text to useful dictionary objects.
    We''ve written a second version called `counting_extract_row_iter()` to emphasize
    the additional feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE147]'
  prefs: []
  type: TYPE_PRE
- en: We've provided an additional argument, `counts` . When we find rows that don't
    match the regular expression, we can increment the `non-match` key in the `Counter`
    . When we find rows that do match properly, we can increment the `valid` key in
    the `Counter` . This provides a summary that shows how may rows were processed
    from the given file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The overall processing script looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE148]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ve made three small changes:'
  prefs: []
  type: TYPE_NORMAL
- en: Create an empty `Counter` object just before processing a source log file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide the `Counter` object to the `counting_extract_row_iter()` function.
    The function will update the counter as it processes rows.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Print the value of the `counter` after processing the files. The unadorned output
    isn't very pretty, but it tells an important story.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We might see output like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE149]'
  prefs: []
  type: TYPE_PRE
- en: This kind of output shows us how large the `summary_log.csv` will be, and it
    also shows that something was wrong in the `20160613.log` file.
  prefs: []
  type: TYPE_NORMAL
- en: We can easily extend this to combine all of the individual source file counters
    to produce a single large output at the end of the process. We can combine multiple
    `Counter` objects using the `+` operator to create a grand sum of all of the data.
    Details are left as an exercise for the reader.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the basics of a context, see the *Reading and writing files with context
    managers* recipe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
