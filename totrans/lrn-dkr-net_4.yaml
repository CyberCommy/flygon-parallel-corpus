- en: Chapter 4. Networking in a Docker Cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will learn how Docker containers are networked when using
    frameworks like Kubernetes, Docker Swarm, and Mesosphere.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Networked containers in a Kubernetes cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How Kubernetes networking differs from Docker networking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes on AWS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mesosphere
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker Swarm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker Swarm is a native clustering system for Docker. Docker Swarm exposes
    the standard Docker API so that any tool that communicates with the Docker daemon
    can communicate with Docker Swarm as well. The basic aim is to allow the creation
    and usage of a pool of Docker hosts together. The cluster manager of Swarm schedules
    the containers based on the availability resources in a cluster. We can also specify
    the constrained resources for a container while deploying it. Swarm is designed
    to pack containers onto a host by saving other host resources for heavier and
    bigger containers rather than scheduling them randomly to a host in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to other Docker projects, Docker Swarm uses a Plug and Play architecture.
    Docker Swarm provides backend services to maintain a list of IP addresses in your
    Swarm cluster. There are several services, such as etcd, Consul, and Zookeeper;
    even a static file can be used. Docker Hub also provides a hosted discovery service,
    which is used in the normal configuration of Docker Swarm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Docker Swarm scheduling uses multiple strategies in order to rank nodes. When
    a new container is created, Swarm places it on the node on the basis of the highest
    computed rank, using the following strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Spread**: This optimizes and schedules the containers on the nodes based
    on the number of containers running on the node at that point of time'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Binpack**: The node is selected to schedule the container on the basis of
    CPU and RAM utilization'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Random strategy**: This uses no computation; it selects the node randomly
    to schedule containers'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Docker Swarm also uses filters in order to schedule containers, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Constraints**: These use key/value pairs associated with nodes, such as `environment=production`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Affinity filter**: This is used to run a container and instruct it to locate
    and run next to another container based on the label, image, or identifier'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Port filter**: In this case, the node is selected on the basis of the ports
    available on it'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dependency filter**: This co-schedules dependent containers on the same node'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Health filter**: This prevents the scheduling of containers on unhealthy
    nodes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following figure explains various components of a Docker Swarm cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Docker Swarm](../images/00025.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Docker Swarm setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's set up our Docker Swarm setup, which will have two nodes and one master.
  prefs: []
  type: TYPE_NORMAL
- en: We will be using a Docker client in order to access the Docker Swarm cluster.
    A Docker client can be set up on a machine or laptop and should have access to
    all the machines present in the Swarm cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'After installing Docker on all three machines, we will restart the Docker service
    from a command line so that it can be accessed from TCP port 2375 on the localhost
    (`0.0.0.0:2375`) or from a specific host IP address and can allow connections
    using a Unix socket on all the Swarm nodes, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Docker Swarm images are required to be deployed as Docker containers on the
    master node. In our example, the master node''s IP address is `192.168.59.134`.
    Replace it with your Swarm''s master node. From the Docker client machine, we
    will be installing Docker Swarm on the master node using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The Swarm token generated after the execution of the command should be noted,
    as it will be used for the Swarm setup. In our case, it is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The following are the steps to set up a two-node Docker Swarm cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the Docker client node, the following `docker` command is required to
    be executed with Node 1''s IP address (in our case, `192.168.59.135`) and the
    Swarm token generated in the preceding code in order to add it to the Swarm cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Repeat the preceding steps for Node 2 by replacing Node 1's IP address with
    Node 2's.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Swarm manager is required to be set up on the master node using the following
    command on the Docker client node:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The Swarm cluster is set up and can be managed using the Swarm manager residing
    on the master node. To list all the nodes, the following command can be executed
    using a Docker client:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command can be used to get information about the cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The test `ubuntu` container can be launched onto the cluster by specifying
    the name as `swarm-ubuntu` and using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The container can be listed using the Swarm master''s IP address:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This completes the setup of a two-node Docker Swarm cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm networking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Docker Swarm networking has integration with libnetwork and even provides support
    for overlay networks. libnetwork provides a Go implementation to connect containers;
    it is a robust container network model that provides network abstraction for applications
    and the programming interface of containers. Docker Swarm is now fully compatible
    with the new networking model in Docker 1.9 (note that we will be using Docker
    1.9 in the following setup). The key-value store is required for overlay networks,
    which includes discovery, networks, IP addresses, and more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we will be using Consul to understand Docker Swarm
    networking in a better way:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will provision a VirtualBox machine called `sample-keystore` using `docker-machine`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also deploy the `progrium/consul` container on the `sample-keystore`
    machine on port `8500` with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the local environment to the `sample-keystore` machine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We can list the consul container as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a Swarm cluster using `docker-machine`. The two machines can be created
    in VirtualBox; one can act as the Swarm master. As we create each Swarm node,
    we will be passing the options required for Docker Engine to have an overlay network
    driver:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The use of the parameters used in the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--swarm`: This is used to configure a machine with Swarm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--engine-opt`: This option is used to define arbitrary daemon options required
    to be supplied. In our case, we will supply the engine daemon with the `--cluster-store`
    option during creation time, which tells the engine the location of the key-value
    store for the overlay network usability. The `--cluster-advertise` option will
    put the machine on the network at the specific port.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--swarm-discovery`: It is used to discover services to use with Swarm, in
    our case, `consul` will be that service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--swarm-master`: This is used to configure a machine as the Swarm master.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Another host can also be created and added to Swarm cluster, like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The machines can be listed as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will set the Docker environment to `swarm-master`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command can be executed on the master in order to create the
    overlay network and have multihost networking:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The network bridge can be checked on the master using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'When switching to a Swarm node, we can easily list the newly created overlay
    network, like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the network is created, we can start the container on any of the hosts,
    and it will be part of the network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Start the sample `ubuntu` container with the constraint environment set to
    the first node:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We can check using the `ifconfig` command that the container has two network
    interfaces, and it will be accessible from the container deployed using Swarm
    manager on any other host.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes is a container cluster management tool. Currently, it supports Docker
    and Rocket. It is an open source project supported by Google, and the project
    was launched in June 2014 at Google I/O. It supports deployment on various cloud
    providers such as GCE, Azure, AWS, and vSphere as well as on bare metal. The Kubernetes
    manager is lean, portable, extensible, and self-healing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes has various important components, as explained in the following
    list:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Node**: This is a physical or virtual-machine part of a Kubernetes cluster,
    running the Kubernetes and Docker services, onto which pods can be scheduled.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Master**: This maintains the runtime state of the Kubernetes server runtime.
    It is the point of entry for all the client calls to configure and manage Kubernetes
    components.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kubectl**: This is the command-line tool used to interact with the Kubernetes
    cluster to provide master access to Kubernetes APIs. Through it, the user can
    deploy, delete, and list pods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pod**: This is the smallest scheduling unit in Kubernetes. It is a collection
    of Docker containers that share volumes and don''t have port conflicts. It can
    be created by defining a simple JSON file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Replication controller**: It manages the lifecycle of a pod and ensures that
    a specified number of pods are running at a given time by creating or killing
    pods as required.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Label**: Labels are used to identify and organize pods and services based
    on key-value pairs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram shows the Kubernetes Master/Minion flow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Kubernetes](../images/00026.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Deploying Kubernetes on AWS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s get started with Kubernetes cluster deployment on AWS, which can be
    done by using the config file that already exists in the Kubernetes codebase:'
  prefs: []
  type: TYPE_NORMAL
- en: Log in to AWS Console at [http://aws.amazon.com/console/](http://aws.amazon.com/console/).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open the IAM console at [https://console.aws.amazon.com/iam/home?#home](https://console.aws.amazon.com/iam/home?#home).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose the IAM username, select the **Security Credentials** tab, and click
    on the **Create Access Key** option.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After the keys have been created, download and keep them in a secure place.
    The downloaded `.csv` file will contain an `Access Key ID` and `Secret Access
    Key`, which will be used to configure the AWS CLI.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Install and configure the AWS CLI. In this example, we have installed AWS CLI
    on Linux using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to configure the AWS CLI, use the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'After configuring the AWS CLI, we will create a profile and attach a role to
    it with full access to S3 and EC2:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The role can be created separately using the console or AWS CLI with a JSON
    file that defines the permissions the role can have:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'A role can be attached to the preceding profile, which will have complete access
    to EC2 and S3, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deploying Kubernetes on AWS](../images/00027.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'After the creation of the role, it can be attached to a policy using the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, the script uses the default profile. We can change it as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The Kubernetes cluster can be easily deployed using one command, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command will call `kube-up.sh` and, in turn, `utils.sh` using
    the `config-default.sh` script, which contains the basic configuration of a K8S
    cluster with four nodes, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The instances are `t2.micro` running Ubuntu OS. The process takes 5 to 10 minutes,
    after which the IP addresses of the master and minions get listed and can be used
    to access the Kubernetes cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kubernetes networking and its differences to Docker networking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubernetes strays from the default Docker system's networking model. The objective
    is for each pod to have an IP at a level imparted by the system's administration
    namespace, which has full correspondence with other physical machines and containers
    over the system. Allocating IPs per pod unit makes for a clean, retrogressive,
    and good model where units can be dealt with much like VMs or physical hosts from
    the point of view of port allotment, system administration, naming, administration
    disclosure, burden adjustment, application design, and movement of pods from one
    host to another. All containers in all pods can converse with all other containers
    in all other pods using their addresses. This also helps move traditional applications
    to a container-oriented approach.
  prefs: []
  type: TYPE_NORMAL
- en: As every pod gets a real IP address, they can communicate with each other without
    any need for translation. By making the same configuration of IP addresses and
    ports both inside as well as outside of the pod, we can create a NAT-less flat
    address space. This is different from the standard Docker model since there, all
    containers have a private IP address, which will allow them to be able to access
    the containers on the same host. But in the case of Kubernetes, all the containers
    inside a pod behave as if they are on the same host and can reach each other's
    ports on the localhost. This reduces the isolation between containers and provides
    simplicity, security, and performance. Port conflict can be one of the disadvantages
    of this; thus, two different containers inside one pod cannot use the same port.
  prefs: []
  type: TYPE_NORMAL
- en: In GCE, using IP forwarding and advanced routing rules, each VM in a Kubernetes
    cluster gets an extra 256 IP addresses in order to route traffic across pods easily.
  prefs: []
  type: TYPE_NORMAL
- en: Routes in GCE allow you to implement more advanced networking functions in the
    VMs, such as setting up many-to-one NAT. This is leveraged by Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: This is in addition to the main Ethernet bridge which the VM has; this bridge
    is termed as the container bridge `cbr0` in order to differentiate it from the
    Docker bridge, `docker0`. In order to transfer packets out of the GCE environment
    from a pod, it should undergo an SNAT to the VM's IP address, which GCE recognizes
    and allows.
  prefs: []
  type: TYPE_NORMAL
- en: Other implementations with the primary aim of providing an IP-per-pod model
    are Open vSwitch, Flannel, and Weave.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of a GCE-like setup of an Open vSwitch bridge for Kubernetes, the
    model where the Docker bridge gets replaced by `kbr0` to provide an extra 256
    subnet addresses is followed. Also, an OVS bridge (`ovs0`) is added, which adds
    a port to the Kubernetes bridge in order to provide GRE tunnels to transfer packets
    across different minions and connect pods residing on these hosts. The IP-per-pod
    model is also elaborated more in the upcoming diagram, where the service abstraction
    concept of Kubernetes is also explained.
  prefs: []
  type: TYPE_NORMAL
- en: A service is another type of abstraction that is widely used and suggested for
    use in Kubernetes clusters as it allows a group of pods (applications) to be accessed
    via virtual IP addresses and gets proxied to all internal pods in a service. An
    application deployed in Kubernetes could be using three replicas of the same pod,
    which have different IP addresses. However, the client can still access the application
    on the one IP address which is exposed outside, irrespective of which backend
    pod takes the request. A service acts as a load balancer between different replica
    pods and a single point of communication for clients utilizing this application.
    Kubeproxy, one of the services of Kubernetes, provides load balancing and uses
    rules to access the service IPs and redirects them to the correct backend pod.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the Kubernetes pod
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, in the following example, we will be deploying two nginx replication pods
    (`rc-pod`) and exposing them via a service in order to understand Kubernetes networking.
    Deciding where the application can be exposed via a virtual IP address and which
    replica of the pod (load balancer) the request is to be proxied to is taken care
    of by **Service Proxy**. Please refer to the following diagram for more details:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deploying the Kubernetes pod](../images/00028.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following are the steps to deploy the Kubernetes pod:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Kubernetes master, create a new folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'In the editor of your choice, create the `.yaml` file that will be used to
    deploy the nginx pods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Copy the following into the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the nginx pod using `kubectl`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding pod creation process, we created two replicas of the nginx
    pod, and its details can be listed using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'To list replication controllers on a cluster, use the `kubectl get` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The container on the deployed minion can be listed using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Deploy the nginx service using the following .`yaml` file in order to expose
    the nginx pod on host port `82`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Copy the following into the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the nginx service using the `kubectl create` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The nginx service can be listed using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the nginx server''s test page can be accessed on the following URL via
    the service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`http://192.168.3.43:82`'
  prefs: []
  type: TYPE_NORMAL
- en: Mesosphere
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mesosphere is a software solution that provides ways of managing server infrastructures
    and basically expands upon the cluster-management capabilities of Apache Mesos.
    Mesosphere has also launched the **DCOS** (**data center operating system**),
    used to manage data centers by spanning all the machines and treating them as
    a single computer, providing a highly scalable and elastic way of deploying apps
    on top of it. DCOS can be installed on any public cloud or your own private data
    center, ranging from AWS, GCE, and Microsoft Azure to VMware. Marathon is the
    framework for Mesos and is designed to launch and run applications; it serves
    as a replacement for the init system. Marathon provides various features such
    as high availability, application health check, and service discovery, which help
    you run applications in Mesos clustered environments.
  prefs: []
  type: TYPE_NORMAL
- en: This session describes how to bring up a single-node Mesos cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Docker containers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Mesos can run and manage Docker containers using the Marathon framework.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this exercise, we will use CentOS 7 to deploy a Mesos cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install Mesosphere and Marathon using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Apache Mesos uses Zookeeper to operate. Zookeeper acts as the master election
    service in the Mesosphere architecture and stores states for the Mesos nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Install Zookeeper and the Zookeeper server package by pointing to the RPM repository
    for Zookeeper, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Validate Zookeeper by stopping and restarting it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Mesos uses a simple architecture to give you intelligent task distribution across
    a cluster of machines without worrying about where they are scheduled.
  prefs: []
  type: TYPE_NORMAL
- en: 'Configure Apache Mesos by starting the `mesos-master` and `mesos-slave` processes
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Mesos will be running on port `5050`. As shown in the following screenshot,
    you can access the Mesos interface with your machine's IP address, here, `http://192.168.10.10:5050`:![Docker
    containers](../images/00029.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Test Mesos using the `mesos-execute` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'With the `mesos-execute` command running, enter *Ctrl* + *Z* to suspend the
    command. You can see how it appears in the web UI and command line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The Mesosphere stack uses Marathon to manage processes and services. It serves
    as a replacement for the traditional init system. It simplifies the running of
    applications in a clustered environment. The following figure shows the Mesosphere
    Master slave topology with Marathon:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Docker containers](../images/00030.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Marathon can be used to start other Mesos frameworks; as it is designed for
    long-running applications, it will ensure that the applications it has launched
    will continue running even if the slave nodes they are running on fail.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start the Marathon service using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: You can view the Marathon GUI at `http://192.168.10.10:8080`.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a web app using Docker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will install a simple Outyet web application:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install Docker using the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command tests the Docker file before adding it to Marathon:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Go to `http://192.168.10.10:6060/` on your browser in order to confirm it works.
    Once it does, you can hit *CTRL* + *C* to exit the Outyet Docker.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a Marathon application using Marathon Docker support, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Containers are configured and managed better with Marathon Docker, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: You can check all your applications on the Marathon GUI at `http://192.168.10.10:8080`,
    as shown in the following screenshot:![Deploying a web app using Docker](../images/00031.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploying Mesos on AWS using DCOS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this final section, we will be deploying the latest launch of DCOS by Mesosphere
    on AWS in order to manage and deploy Docker services in our data center:'
  prefs: []
  type: TYPE_NORMAL
- en: Create an AWS key pair in the region where the cluster is required to be deployed
    by going to the navigation pane and choosing **Key Pairs** under **NETWORK & SECURITY**:![Deploying
    Mesos on AWS using DCOS](../images/00032.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After being created, the key can be viewed as follows and the generated key
    pair (`.pem`) file should be stored in a secure location for future use:![Deploying
    Mesos on AWS using DCOS](../images/00033.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The DCOS cluster can be created by selecting the **1 Master** template on the
    official Mesosphere site:![Deploying Mesos on AWS using DCOS](../images/00034.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'It can also be done by providing the link for the Amazon S3 template URL in
    the stack deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deploying Mesos on AWS using DCOS](../images/00035.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Click on the **Next** button. Fill in the details such as **Stack name** and
    **KeyName**, generated in the previous step:![Deploying Mesos on AWS using DCOS](../images/00036.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the details before clicking on the **Create** button:![Deploying Mesos
    on AWS using DCOS](../images/00037.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After 5 to 10 minutes, the Mesos stack will be deployed and the Mesos UI can
    be accessed at the URL shown in the following screenshot:![Deploying Mesos on
    AWS using DCOS](../images/00038.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we will be installing the DCOS CLI on a Linux machine with Python (2.7
    or 3.4) and pip preinstalled, using the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The DCOS help file can be listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will deploy a Spark application on top of the Mesos cluster using the
    DCOS package after updating it. Get a detailed command description with `dcos
    <command> --help`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The Spark package can be installed as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: After deployment, it can be seen in the DCOS UI under the **Services** tab,
    as shown in the following screenshot:![Deploying Mesos on AWS using DCOS](../images/00039.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In order to deploy a dummy Docker application on the preceding Marathon cluster,
    we can use the JSON file to define the container image, command to execute, and
    ports to be exposed after deployment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'The app can be added to Marathon and listed as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Three instances of the preceding Docker app can be started as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: The deployed application can be seen in the DCOS UI by clicking on the **Tasks**
    tab under **Services**:![Deploying Mesos on AWS using DCOS](../images/00040.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learnt about Docker networking using various frameworks,
    such as the native Docker Swarm. Using libnetwork or out-of-the-box overlay networks,
    Swarm provides multihost networking features.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes, on the other hand, has a different perspective from Docker, in which
    each pod gets its unique IP address and communication between pods can occur with
    the help of services. Using Open vSwitch or IP forwarding and advanced routing
    rules, Kubernetes networking can be enhanced to provide connectivity between pods
    on different subnets across hosts and the ability to expose the pods to the external
    world. In the case of Mesosphere, we can see that Marathon is used as the backend
    for the networking of the deployed containers. In the case of DCOS by Mesosphere,
    the entire deployed stack of machines is treated as one machine in order to provide
    a rich networking experience between deployed container services.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about security and QoS for basic Docker networking
    by understanding kernel namespace, cgroups, and virtual firewalls.
  prefs: []
  type: TYPE_NORMAL
